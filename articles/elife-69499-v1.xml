<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">69499</article-id><article-id pub-id-type="doi">10.7554/eLife.69499</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Continuous attractors for dynamic memories</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-214306"><name><surname>Spalla</surname><given-names>Davide</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0328-6476</contrib-id><email>dspalla@sissa.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-214572"><name><surname>Cornacchia</surname><given-names>Isabel Maria</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0704-7480</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-16297"><name><surname>Treves</surname><given-names>Alessandro</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7246-5673</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>SISSA – Cognitive Neuroscience, Via Bonomea</institution><addr-line><named-content content-type="city">Trieste</named-content></addr-line><country>Italy</country></aff><aff id="aff2"><label>2</label><institution>University of Turin – Physics Department</institution><addr-line><named-content content-type="city">Torino</named-content></addr-line><country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peyrache</surname><given-names>Adrien</given-names></name><role>Reviewing Editor</role><aff><institution>McGill University</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution>University of Texas at Austin</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>09</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e69499</elocation-id><history><date date-type="received" iso-8601-date="2021-04-18"><day>18</day><month>04</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-08-12"><day>12</day><month>08</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-11-08"><day>08</day><month>11</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.11.08.373084"/></event></pub-history><permissions><copyright-statement>© 2021, Spalla et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Spalla et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-69499-v1.pdf"/><abstract><p>Episodic memory has a dynamic nature: when we recall past episodes, we retrieve not only their content, but also their temporal structure. The phenomenon of replay, in the hippocampus of mammals, offers a remarkable example of this temporal dynamics. However, most quantitative models of memory treat memories as static configurations, neglecting the temporal unfolding of the retrieval process. Here, we introduce a continuous attractor network model with a memory-dependent asymmetric component in the synaptic connectivity, which spontaneously breaks the equilibrium of the memory configurations and produces dynamic retrieval. The detailed analysis of the model with analytical calculations and numerical simulations shows that it can robustly retrieve multiple dynamical memories, and that this feature is largely independent of the details of its implementation. By calculating the storage capacity, we show that the dynamic component does not impair memory capacity, and can even enhance it in certain regimes.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>When we recall a past experience, accessing what is known as an ‘episodic memory’, it usually does not appear as a still image or a snapshot of what occurred. Instead, our memories tend to be dynamic: we remember how a sequence of events unfolded, and when we do this, we often re-experience at least part of that same sequence. If the memory includes physical movement, the sequence combines space and time to remember a trajectory. For example, a mouse might remember how it went down a hole and found cheese there.</p><p>However, mathematical models of how past experiences are stored in our brains and retrieved when we remember them have so far focused on snapshot memories. ‘Attractor network models’ are one type of mathematical model that neuroscientists use to represent how neurons communicate with each other to store memories. These models can provide insights into how circuits of neurons, for example those in the hippocampus (a part of the brain crucial for memory), may have evolved to remember the past, but so far they have only focused on how single moments, rather than sequences of events, are represented by populations of neurons.</p><p>Spalla et al. found a way to extend these models, so they could analyse how networks of neurons can store and retrieve dynamic memories. These memories are represented in the brain as ‘continuous attractors’, which can be thought of as arrows that attract mental trajectories first to the arrow itself, and once on the arrow, to the arrowhead. Each recalled event elicits the next one on the arrow, as the mental trajectory advances towards the arrowhead. Spalla et al. determined that memory networks in the hippocampus of mammals can store large numbers of these ‘arrows’, up to the same amount of ‘snapshot’ memories predicted to be stored with similar models.</p><p>Spalla et al.’s results may allow researchers to better understand memory storage and recall, since they allow for the modelling of complex and realistic aspects of episodic memories. This could provide insights into processes such as why our minds wander, as well as having implications for the study of how neurons physically interact with each other to transmit information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>continuous attractors</kwd><kwd>memory</kwd><kwd>neural dynamics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004412</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0057/2016</award-id><principal-award-recipient><name><surname>Treves</surname><given-names>Alessandro</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A neural network model with asymmetric synaptic interactions can store and retrieve multiple continuous memory sequences with their temporal structure.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The temporal unfolding of an event is an essential component of episodic memory. When we recall past events, or we imagine future ones, we do not produce static images but temporally structured movies, a phenomenon that has been referred to as 'mental time travel' (<xref ref-type="bibr" rid="bib16">Eichenbaum and Cohen, 2004</xref>; <xref ref-type="bibr" rid="bib58">Tulving, 2002</xref>).</p><p>The study of the neural activity of the hippocampus, known for its first-hand involvement in episodic memory, has provided many insights on the neural basis of memory retrieval and its temporal dynamics. An interesting example is the phenomenon of hippocampal replay, that is the reactivation, on a compressed time scale, of sequences of cells active in previous behavioral sessions. Replay takes place during sharp wave ripples, fast oscillations of the hippocampal local field potential that are particularly abundant during sleep and restful wakefulness (<xref ref-type="bibr" rid="bib9">Buzsáki et al., 1983</xref>; <xref ref-type="bibr" rid="bib10">Buzsáki et al., 1992</xref>). Indeed, replay has been observed during sleep (<xref ref-type="bibr" rid="bib47">Skaggs and McNaughton, 1996</xref>; <xref ref-type="bibr" rid="bib35">Nádasdy et al., 1999</xref>), inter-trial rest periods (<xref ref-type="bibr" rid="bib18">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib26">Jackson et al., 2006</xref>), and during still periods in navigational tasks (<xref ref-type="bibr" rid="bib15">Dupret et al., 2010</xref>; <xref ref-type="bibr" rid="bib39">Pfeiffer and Foster, 2013</xref>). Replay activity has been hypothesized to be crucial for memory consolidation (<xref ref-type="bibr" rid="bib36">O'Neill et al., 2010</xref>) and retrieval (<xref ref-type="bibr" rid="bib28">Karlsson and Frank, 2009</xref>), as well as for route planning (<xref ref-type="bibr" rid="bib39">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib37">Ólafsdóttir et al., 2018</xref>).</p><p>A temporally structured activation takes place also before the exposure to an environment (<xref ref-type="bibr" rid="bib13">Dragoi and Tonegawa, 2011</xref>), a phenomenon known as <italic>preplay</italic>, and a recent study showed that this dynamical feature emerges very early during development, preceding the appearance of theta rhythm (<xref ref-type="bibr" rid="bib17">Farooq and Dragoi, 2019</xref>) in the hippocampus. The fact that hippocampal sequences are present before the exposure to the environment suggests that their dynamical nature is not specific to a role in spatial cognition, but is inherent to hippocampal operation in general. Moreover, in a recent study <xref ref-type="bibr" rid="bib51">Stella et al., 2019</xref> have shown that the retrieved sequences of positions during slow wave sleep are not always replaying experienced trajectories, but are compatible with a random walk on the low dimensional manifold that represents the previously explored environment. This suggests that what is essential are not the sequences themselves, but the tendency to produce them: neural activity tends to move, constrained to abstract low-dimensional manifolds which can then be recycled to represent spatial environments, and possibly non-spatial ones as well. This dynamic nature extends to multiple timescales, as suggested by the observation that the neural map of the same environment progressively changes its component cells over time (<xref ref-type="bibr" rid="bib63">Ziv et al., 2013</xref>).</p><p>Low-dimensional, dynamic activity is not constrained to a single subspace: replay in sleep can reflect multiple environments (<xref ref-type="bibr" rid="bib14">Dragoi and Tonegawa, 2013</xref>; <xref ref-type="bibr" rid="bib24">Gridchyn et al., 2020</xref>), the content of awake replay reflects both the current and previous environments (<xref ref-type="bibr" rid="bib28">Karlsson and Frank, 2009</xref>), and during behavior fast hippocampal sequences appear to switch between possible future trajectories (<xref ref-type="bibr" rid="bib29">Kay et al., 2020</xref>). Further evidence comes from a recent study with human participants learning novel word pair associations (<xref ref-type="bibr" rid="bib59">Vaz et al., 2020</xref>). The study shows that the same pair-dependent neural sequences are played during the encoding and the retrieval phase.</p><p>A similar phenomenon – a dynamic activity on low dimensional manifolds – is present in memory schemata, cognitive frameworks that constrain and organize our mental activity (<xref ref-type="bibr" rid="bib22">Ghosh and Gilboa, 2014</xref>), and have been shown to have a representation in the medial temporal lobe (<xref ref-type="bibr" rid="bib4">Baraduc et al., 2019</xref>). Yet another example of dynamical, continuous memories is offered by motor programs, which have been described as low-dimensional, temporally structured neural trajectories (<xref ref-type="bibr" rid="bib45">Shenoy et al., 2013</xref>; <xref ref-type="bibr" rid="bib21">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Oztop and Arbib, 2002</xref>), or as dynamical flows on manifolds (<xref ref-type="bibr" rid="bib25">Huys et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Pillai and Jirsa, 2017</xref>).</p><p>We refer to these objects as <italic>dynamical continuous attractors</italic>, since they involve a continuous subspace that constrains and attracts the neural activity, and a dynamical evolution in this subspace. <xref ref-type="fig" rid="fig1">Figure 1</xref> schematically illustrates the concept of dynamical continuous attractors and their possible role in some of the neural processes described above.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic illustration of dynamic continuous attractors as a basis of different neural processes.</title><p>Top row: a scheme of continuous attractive manifolds, with a dynamic component in 1D (<bold>a</bold>), 2D (<bold>b</bold>) and 3D (<bold>c</bold>). The neural activity quickly converges on the attractive manifold (dotted arrows), then slides along it (full arrows), producing a dynamics that is temporally structured and constrained to a low dimensional subspace. Bottom row: multiple dynamic memories could be useful for route planning (top left), involved in mind wandering activity (bottom left) or represent multiple learned motor programs (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig1-v1.tif"/></fig><p>In most cases, computational analyses of low-dimensional neural dynamics are not concerned with memory, and focus on the description of the features of single attractors, more than on their possible coexistence. On the other hand, mechanistic models of memory usually neglect dynamical aspects, treating memories as static objects, either discrete (<xref ref-type="bibr" rid="bib2">Amit et al., 1985</xref>) or continuous (<xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>; <xref ref-type="bibr" rid="bib34">Monasson and Rosay, 2013</xref>; <xref ref-type="bibr" rid="bib43">Samsonovich and McNaughton, 1997</xref>; <xref ref-type="bibr" rid="bib49">Spalla et al., 2019</xref>). The production of sequences of discrete memories can be implemented with a heteroassociative component (<xref ref-type="bibr" rid="bib48">Sompolinsky and Kanter, 1986</xref>), usually dependent on the time integral of the instantaneous activity, that brings the network out of equilibrium and to the next step in the sequence. A similar effect can be obtained with an adaptation mechanism in a coarse grained model of cortical networks (<xref ref-type="bibr" rid="bib30">Kropff and Treves, 2005</xref>), with the difference that in this case the transitions are not imposed, but driven by the correlations between the memories in so-called latching dynamics (<xref ref-type="bibr" rid="bib42">Russo et al., 2008</xref>; <xref ref-type="bibr" rid="bib27">Kang et al., 2017</xref>). Moreover, adaptation-based mechanisms have been used to model the production of random sequences on continuous manifolds (<xref ref-type="bibr" rid="bib3">Azizi et al., 2013</xref>), and shown to be crucial in determining the balance between retrieval and prediction in a network describing CA3-CA1 interactions (<xref ref-type="bibr" rid="bib55">Treves, 2004</xref>). In the case of continuous attractor networks, movement can be induced also by mechanisms that integrate an external velocity input and make use of asymmetric synaptic strengths. Models of this kind have been used for the description of head direction cells (<xref ref-type="bibr" rid="bib61">Zhang, 1996</xref>), spatial view cells (<xref ref-type="bibr" rid="bib53">Stringer et al., 2005</xref>), and grid cells (<xref ref-type="bibr" rid="bib19">Fuhs and Touretzky, 2006</xref>; <xref ref-type="bibr" rid="bib8">Burak and Fiete, 2009</xref>), and can represent simultaneously the positions of multiple features and their temporal evolution (<xref ref-type="bibr" rid="bib52">Stringer et al., 2004</xref>). In the simplest instantiation, these systems do not necessarily reflect long-term memory storage: the activity is constrained on a single attractive manifold, which could well be experience independent.</p><p>Here, we propose a network model able to store and retrieve multiple independent dynamic continuous attractors. The model relies on a map-dependent asymmetric component in the connectivity that produces a robust shift of the activity on the retrieved attractive manifold. This connectivity profile is conceived to be the result of a learning phase in which the mechanism of spike timing dependent plasticity (STDP) (<xref ref-type="bibr" rid="bib31">Markram et al., 1997</xref>) produces the asymmetry. Crucially, the asymmetry is not treated here as a 'pathological' feature, assumed to level out in the limit of long learning, but as a defining trait of the stored memories. The balance between two components – one symmetric and trajectory-averaged, the other asymmetric and trajectory-dependent – is explicit in the formulation of the model, and allows to study their effects on memory storage.</p><p>In what follows we develop an analytical framework that allows to derive the dependence of important features of the dynamics, such as the replay speed and the asymmetry of the activity cluster, as a function of the relevant parameters of the model. We show with numerical simulations that the behavior of the model is robust with respect to its details, and depends weakly on the shape of the interactions. Finally, we estimate the storage capacity for dynamical memories and we find it to be of the same order of the capacity for static continuous attractors, and even higher in some regimes.</p><sec id="s1-1"><title>Modeling framework</title><sec id="s1-1-1"><title>A mechanistic model for dynamic retrieval</title><p>The model we consider is a continuous attractor neural network, with an additional anti-symmetric component in the connectivity strength. We consider a population of <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons, with recurrent connectivity described by an interaction matrix <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, whose entries represent the strength of the interaction between neuron <inline-formula><mml:math id="inf3"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:mi>j</mml:mi></mml:math></inline-formula>. The activity of each neuron is described by a positive real number <inline-formula><mml:math id="inf5"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> representing its instantaneous firing rate. The dynamic evolution of the network is regulated by the equations:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf6"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> is the threshold linear activation function<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with the gain <inline-formula><mml:math id="inf7"><mml:mi>g</mml:mi></mml:math></inline-formula> modulating the slope and the Heaviside step function <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> setting to zero sub-threshold inputs. The first term on the right hand side of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> represent the excitatory inputs provided to neuron <inline-formula><mml:math id="inf9"><mml:mi>i</mml:mi></mml:math></inline-formula> from the rest of the network through recurrent connections. The threshold <italic>h</italic><sub>0</sub> and the gain <inline-formula><mml:math id="inf10"><mml:mi>g</mml:mi></mml:math></inline-formula> are global parameters that regulate the average activity and the sparsity of the activity pattern (<xref ref-type="bibr" rid="bib54">Treves, 1990</xref>).</p><p>In numerical simulations, these parameters are dynamically adjusted at each time step to constrain the network to operate at a certain average activity (usually fixed to one without loss of generality) and at a certain sparsity <inline-formula><mml:math id="inf11"><mml:mi>f</mml:mi></mml:math></inline-formula>, defined as the fraction of active neuron at each time (see appendix A). The connectivity matrix <inline-formula><mml:math id="inf12"><mml:mi>J</mml:mi></mml:math></inline-formula> of the network encodes a map of a continuous parameter <inline-formula><mml:math id="inf13"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> spanning a low-dimensional manifold, e.g. the position in an environment. To do so, each neuron is assigned a preferential firing location <inline-formula><mml:math id="inf14"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> in the manifold to encode, and the strength of the interaction between pairs of neuron is given by a decreasing, symmetric function of the distance between their preferred firing locations<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This shape of the interactions is a typical one in the framework of continuous attractor neural networks (<xref ref-type="bibr" rid="bib43">Samsonovich and McNaughton, 1997</xref>; <xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>; <xref ref-type="bibr" rid="bib57">Tsodyks, 1999</xref>) and is thought to come from a time-averaged Hebbian plasticity rule: neurons with nearby firing fields will fire concurrently and strengthen their connections, while firing fields that are far apart will produce weak interactions. The symmetry of the function <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>, usually called interaction kernel, ensures that the network reaches a static equilibrium, where the activity of the neurons represents a certain position in the manifold and, if not pushed, remains still.</p></sec><sec id="s1-1-2"><title>The shift mechanism</title><p>The assumption of symmetric interactions neglects any temporal structure in the learning phase. In case of learning a spatial map, for example, the order in which recruited neurons fire along a trajectory may produce an asymmetry in the interactions as a consequence of Spike Timing Dependent Plasticity (<xref ref-type="bibr" rid="bib31">Markram et al., 1997</xref>), that requires the postsynaptic neuron to fire <italic>after</italic> the presynaptic one in order to strengthen the synapse. This phenomenon can be accounted for in the definition of the interaction kernel. Any asymmetric kernel can be decomposed in two contributions:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> is the usual symmetric component and <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> is an anti-symmetric function (<inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>). The parameter γ regulates the relative strength of the two components. The presence of <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> will generate a flow of activity along the direction of asymmetry: neuron <inline-formula><mml:math id="inf20"><mml:mi>i</mml:mi></mml:math></inline-formula> activates neuron <inline-formula><mml:math id="inf21"><mml:mi>j</mml:mi></mml:math></inline-formula> that, instead of reciprocating, will activate neurons downstream in the asymmetric direction. Mechanisms of this kind have been shown to produce a rigid shift of the encoded position along the manifold, without loss of coherence (<xref ref-type="bibr" rid="bib61">Zhang, 1996</xref>; <xref ref-type="bibr" rid="bib8">Burak and Fiete, 2009</xref>; <xref ref-type="bibr" rid="bib19">Fuhs and Touretzky, 2006</xref>). In the quantitative analysis that follows we will concentrate, when not stated otherwise, on a kernel <inline-formula><mml:math id="inf22"><mml:mi>K</mml:mi></mml:math></inline-formula> with the exponential form<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>sign</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf23"><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> is a unit vector pointing in the – constant – direction along which the asymmetry is enforced, and ξ is the spatial scale of the asymmetric component, which is fixed to one where not explicitly stated otherwise. Moreover, we make the simplifying assumption of periodic boundary conditions on the manifold spanned by <inline-formula><mml:math id="inf24"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, such that the dynamics follows a periodic cycle. Both the kernel form and the periodic boundary conditions simplify the analytical description of the model, but are not a required feature of the model. In fact, all the results presented hold for a large class of interaction kernels, and the boundary conditions can be modified (e.g. with the introduction of interactions between different memories) without compromising the functionality of the network. Both points will be addressed in the analyses of the model in the next sections.</p></sec></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Asymmetric recurrent connections produce dynamic retrieval</title><p>The spontaneous dynamics produced by the network is constrained to the low dimensional manifold codified in the connectivity matrix and spanned by the parameter <inline-formula><mml:math id="inf25"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>. The short-range interactions and the uniform inhibition enforced by the firing threshold <italic>h</italic><sub>0</sub> produce a localized 'bump' of activity in the manifold. The presence of an asymmetry in the connection strengths prevents the system from remaining in a stationary equilibrium. Instead, it generates a steady flow of activity in the direction of the asymmetry.</p><p>This flow is illustrated in <xref ref-type="fig" rid="fig2">Figure 2 (a),(b) and (c)</xref>, obtained with numerical simulation of a network encoding a one-, two-, or three-dimensional manifold, respectively. In the simulation, each neuron is assigned to a preferential firing location <inline-formula><mml:math id="inf26"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> in the manifold to encode, and the plots show the activity of the network organized according to this disposition. The activity of the population clusters in a bump around a certain position at each time point (<italic>t</italic><sub>1</sub>, <italic>t</italic><sub>2</sub>, and <italic>t</italic><sub>3</sub>), and the bump shifts by effect of the asymmetric component of the interactions. In this way, the neural population collectively encodes an evolving coordinate on the manifold spanned by <inline-formula><mml:math id="inf27"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>. The coherence of the representation is not affected by the presence of the asymmetric term: the movement of the activity bump happens without dissipation.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Dynamic retrieval of a continuous manifold.</title><p>First row: each plot presents three snapshots of the network activity at three different times (t1, t2 and t3), for a system encoding a one dimensional (<bold>a</bold>), two dimensional (<bold>b</bold>) and three dimensional (<bold>c</bold>) manifold. In (<bold>c</bold>), activity is color-coded (blue represents low activity, red is high activity, silent neurons are not plotted for better readability). In all cases, the anti-symmetric component is oriented along the x axis. (<bold>d</bold>) Dependence of the speed on γ and <inline-formula><mml:math id="inf28"><mml:mi>f</mml:mi></mml:math></inline-formula>. Dots are data from numerical simulations, full lines are the fitted curves. (<bold>e</bold>) Retrieval of two crossing trajectories. Black arrows represent the two intersecting encoded trajectories, each parallel to one of the axis. Full colored lines show the trajectories actually followed by the center of mass of the activity from the same starting point. Blue curve: low γ, the activity switches trajectories when it reaches the crossing point. Orange curve: high γ, successful crossing. In both cases <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. The blue and the orange insets show the activity bumps in the corresponding cases; the top-right inset shows the dependence of the value <inline-formula><mml:math id="inf30"><mml:msup><mml:mi>γ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, required for crossing, on ξ.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig2-v1.tif"/></fig><p>The speed of movement of the bump is modulated by the value of the parameter γ and by the sparsity level of the representation <inline-formula><mml:math id="inf31"><mml:mi>f</mml:mi></mml:math></inline-formula>, that is the fraction of neurons active at each given time during the dynamics. The dependence of the speed on these two parameters is illustrated in 2(d). Stronger asymmetry (high γ) produces a faster shift. Interestingly, the sparsity value <inline-formula><mml:math id="inf32"><mml:mi>f</mml:mi></mml:math></inline-formula> acts as a modulator of the influence of γ: sparser representations move more slowly than dense ones.</p><p>While γ describes a feature of the synaptic interactions, determined during the learning phase and relatively fixed at the short timescales of retrieval, <inline-formula><mml:math id="inf33"><mml:mi>f</mml:mi></mml:math></inline-formula> can be instantaneously modulated during retrieval dynamics. A change in the gain or the excitability of the population can be used to produce dynamic retrieval at different speeds. Thus, the model predicts an interaction between the sparsity and the speed of the reactivation of a continuous memory sequence, with increased activity leading to faster replay. It is worth noting, however, that the <italic>direction</italic> of the dynamics is fixed with <inline-formula><mml:math id="inf34"><mml:mi>J</mml:mi></mml:math></inline-formula>: the model is able to retrieve either forward or backward sequences, but not alternate between them.</p><p>The dependence of the retrieval speed <inline-formula><mml:math id="inf35"><mml:mi>s</mml:mi></mml:math></inline-formula> on γ and <inline-formula><mml:math id="inf36"><mml:mi>f</mml:mi></mml:math></inline-formula> is well described by the approximate functional form<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>This dependence is shown in 2(b), where the dots are the values obtained with numerical simulations and the full curves the fitted relationship. The full understanding of the nature of this functional form remains an open challenge for future analysis. As we will see in the next section, the analytical solution of the model yields the same form to describe the dependence of speed on γ and <inline-formula><mml:math id="inf37"><mml:mi>f</mml:mi></mml:math></inline-formula>, but a closed-form solution is still lacking.</p><p>In the model presented, the asymmetry in the interactions is enforced uniformly along a single direction also for two- and three-dimensional manifolds, representing the case in which neural dynamics follows a forced trajectory along one dimension, but is free to move without energy costs along the others. However, the same mechanism can be used to produce one-dimensional trajectories embedded in low-dimensional manifolds, with the introduction of a positional dependence in the direction of the asymmetry (<xref ref-type="bibr" rid="bib6">Blum and Abbott, 1996</xref>). In this case, an interesting problem is posed by the intersection of two trajectories embedded in the same manifold: is the network, during the retrieval of one trajectory, able to cross these intersections, or do they hinder dynamical retrieval? The investigation of the full phenomenology of position-dependent asymmetric kernels with intersecting trajectories is beyond the scope of the present work, but we present in <xref ref-type="fig" rid="fig2">Figure 2(e)</xref> a numerical study of a minimal version of this problem, with two orthogonal trajectories (<xref ref-type="fig" rid="fig2">Figure 2(e)</xref>, black arrows) embedded in a 2D manifold and memorized simultaneously in the network. Notice that in this case the two trajectories are parallel to the main axis of the square environment, but they do not need to be: any pair of orthogonal trajectories will behave in the same manner. When the network is cued to retrieve the horizontal trajectory, the behavior at the intersection depends on the strength γ and scale ξ of the asymmetric component. At low γ, the dynamics spontaneously switch trajectory at the intersection (<xref ref-type="fig" rid="fig2">Figure 2(e)</xref>, blue curve), while for γ sufficiently large the retrieval of the horizontal trajectory is successful (<xref ref-type="fig" rid="fig2">Figure 2(e)</xref>, orange curve). The value <inline-formula><mml:math id="inf38"><mml:msup><mml:mi>γ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> required for a successful crossing depends on the spatial scale ξ: larger ξ allow for crossing with lower values of γ, as shown in the top-right inset of <xref ref-type="fig" rid="fig2">Figure 2(e)</xref>, in which <inline-formula><mml:math id="inf39"><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is plotted. Intuitively, the ability of the network to retrieve crossing trajectories depends on the shape of the activity bump, which needs to be sufficiently elongated in the direction of retrieval for the successful crossing of the intersection. The blue and orange insets of <xref ref-type="fig" rid="fig2">Figure 2(e)</xref> show the difference in shape of the bump in the case of a trajectory switch (blue) and successful crossing (orange).</p></sec><sec id="s2-2"><title>Analytical solution for the single manifold case</title><p>The simplicity of continuous attractor models often allows to extract important computational principles from their analytical solution (<xref ref-type="bibr" rid="bib60">Wu et al., 2008</xref>; <xref ref-type="bibr" rid="bib20">Fung et al., 2010</xref>). In our case, the dynamic behavior of the system and its features can be fully described analytically with a generalization of the framework developed by <xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>. For this purpose, it is easier to formulate the problem in the continuum, and describe the population activity <inline-formula><mml:math id="inf40"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> by its profile <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the attractive manifold parametrized by the coordinate <inline-formula><mml:math id="inf42"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, and the dynamical evolution as a discrete step map, equivalent to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The requirement of a rigid shift of population activity is then imposed by setting the activity at time <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to be equal at the activity at time <inline-formula><mml:math id="inf44"><mml:mi>t</mml:mi></mml:math></inline-formula>, but translated by an amount <inline-formula><mml:math id="inf45"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, proportional to the speed <inline-formula><mml:math id="inf46"><mml:mi>s</mml:mi></mml:math></inline-formula> of the shift and in the direction <inline-formula><mml:math id="inf47"><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> of the asymmetry in the connections. The timescale τ sets the time unit in which the duration of the evolution is measured and does not have an impact on the behavior of the system.</p><p>The activity profile <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is then found as the self-consistent solution to the integral equation<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ9">Equation 9</xref> is valid in general. We will focus here, for the explicit derivation (reported in appendix C), on the case of a one dimensional manifold with an exponential interaction kernel<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>sign</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this case, the activity bump will take the form:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo> <mml:mtext>or </mml:mtext><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The parameters <inline-formula><mml:math id="inf49"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> determine the properties of the solution and they depend on the values of γ and speed <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><italic>k</italic><sub>2</sub> is related to the bump width by the relation<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf52"><mml:mi>R</mml:mi></mml:math></inline-formula> is the point at which <inline-formula><mml:math id="inf53"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. <italic>k</italic><sub>1</sub> is related to the asymmetry of the bump: in the limit case <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3(a)</xref>, first column) <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and we recover the cosine solution of the symmetric kernel case studied in <xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>. Larger <italic>k</italic><sub>1</sub> values result in more and more asymmetric shapes (<xref ref-type="fig" rid="fig3">Figure 3(a)</xref>, second and third columns).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Analytical solution of the model.</title><p>(<bold>a</bold>) The shape of the bump for increasing values of γ. (<bold>b</bold>) Dependence of the sparsity <inline-formula><mml:math id="inf57"><mml:mi>f</mml:mi></mml:math></inline-formula> on the gain <inline-formula><mml:math id="inf58"><mml:mi>g</mml:mi></mml:math></inline-formula> of the network. (<bold>c</bold>) Dependence of the speed of the shift on γ, at different values of sparsity. Dots show the numerical solution (note some numerical instability at low <inline-formula><mml:math id="inf59"><mml:mi>f</mml:mi></mml:math></inline-formula> and γ), full curves are the best fits.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig3-v1.tif"/></fig><p>From this analytical solution, we can determine the dependence of the speed <inline-formula><mml:math id="inf60"><mml:mi>s</mml:mi></mml:math></inline-formula> on the asymmetry strength γ and on the sparsity <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (note that in the continuum case the fraction of active neurons is given by the ratio between the bump size <inline-formula><mml:math id="inf62"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and the manifold size <inline-formula><mml:math id="inf63"><mml:mi>L</mml:mi></mml:math></inline-formula>). The sparsity <inline-formula><mml:math id="inf64"><mml:mi>f</mml:mi></mml:math></inline-formula> is modulated by the value of the gain <inline-formula><mml:math id="inf65"><mml:mi>g</mml:mi></mml:math></inline-formula>, as shown in <xref ref-type="fig" rid="fig3">Figure 3(b)</xref>: a larger gain in the transfer function corresponds to a sparser activity. The exact relation <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained from the numerical solution of a transcendental equation (see appendix C), and can be approximated with a functional shape analogous to the one used for the simulated network:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The full transcendental solution and the fitted curves are reported in <xref ref-type="fig" rid="fig3">Figure 3(c)</xref>.</p></sec><sec id="s2-3"><title>Dynamic retrieval is robust</title><p>The analytical solution of the model shows that the network performs dynamical retrieval for all values of the asymmetry strength γ, and that this parameter influences the retrieval speed and the shape of the activity bump. To further investigate the robustness of dynamical retrieval to parameter changes, we investigate with numerical simulations the behavior of the model with respect to another important parameter: the scale ξ of the anti-symmetric component. We run several dynamics of a network with interaction kernel given by<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>sign</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>/</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>varying the parameters γ and ξ and measuring the retrieval speed, the peak value of the activity bump and its skeweness. The joint effects of γ and ξ are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. In the whole range of parameters analyzed, spanning four orders of magnitude for both parameters, the network was able to produce dynamic retrieval. γ and ξ affect the speed of the shift, the peak values of the activity distribution and the skewness of the activity bump, without hindering network functionality. Moreover, all these feature vary gradually and mildly with the parameters values, producing dynamical behavior qualitatively similar in the full parameter range. This analysis shows that dynamical retrieval does not require any fine tuning of network parameters, but relies on the assumption of an exponential shape for the interaction kernel. How robust is the behavior of the network to the details of the kernel shape?</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Dynamical retrieval in a wide range of parameters.</title><p>Effect of the kernel strength γ and its spatial scale ξ, in the case of the exponential kernel <inline-formula><mml:math id="inf67"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mtext>sign</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> .(<bold>a</bold>) Retrieval speed (<bold>b</bold>) Peak value of the activity (<bold>c</bold>) Skewness of the activity bump.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig4-v1.tif"/></fig><p>We addressed this question by simulating the network dynamics with alternative kernel choices. We kept fixed the symmetric component, and explored three different anti-simmetric shapes: a gaussian-derivative shape (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), a sinusoidal shape (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) and a double step function (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). Each of these simulations produced the same retrieval dynamics (a stable bump shifting at constant speed), the only effect of the kernel shape being on the details of the shape of the activity bump (<xref ref-type="fig" rid="fig5">Figure 5</xref>, bottom row). This shows that the dynamic retrieval mechanism, much like standard continuous attractors, is robust with respect to the precise shape of the interactions. Importantly, no particular relationship is required between the shape of the symmetric and the anti-symmetric components of the kernel.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Different interaction kernels produce similar behavior.</title><p>Three examples of dynamics with the same symmetric component and three different anti-symmetric components. Top row: shape of the anti-symmetric component <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>. Bottom row: three snapshots of the retrieval dynamics for the corresponding <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>. (<bold>a</bold>) Gaussian derivative; (<bold>b</bold>) Sinusoidal; (<bold>c</bold>) Anti-symmetric step function, <inline-formula><mml:math id="inf70"><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig5-v1.tif"/></fig></sec><sec id="s2-4"><title>A dynamical memory: storing multiple manifolds</title><p>We described in detail the behavior of a neural network with asymmetric connectivity in the case of a single manifold encoded in the synaptic connectivity. For the network to behave as an autoassociative memory, however, it needs to be able to store and dynamically retrieve <italic>multiple</italic> manifolds. This is possible if we construct the interaction matrix <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the sum of the contributions from <inline-formula><mml:math id="inf72"><mml:mi>p</mml:mi></mml:math></inline-formula> different, independently encoded manifolds:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, each <inline-formula><mml:math id="inf73"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:math></inline-formula> represent the preferred firing location of neuron <inline-formula><mml:math id="inf74"><mml:mi>i</mml:mi></mml:math></inline-formula> in the manifold μ, and <inline-formula><mml:math id="inf75"><mml:mi>K</mml:mi></mml:math></inline-formula> is the same interaction kernel as in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, containing a symmetric and anti-symmetric component.</p><p>The resulting dynamics show multiple continuous attractors, corresponding to the stored manifolds. Given an initial configuration, the networks rapidly converges to the nearest (i.e. most correlated) attractor, forming a coherent bump that then moves along the manifold as a consequence of the asymmetric component of the connectivity. The same dynamics, if projected on the other unretrieved manifolds, appear as random noise. This is illustrated in <xref ref-type="fig" rid="fig6">Figure 6</xref> obtained with numerical simulations of a network encoding three different manifolds (of dimension one in (a), dimension two in (b)), and dynamically retrieving the first one. How does this shifting activity bump relate to the activity of single cells? To clarify this aspect we simulated an electrophysiological recording from the dynamical retrieval of <xref ref-type="fig" rid="fig6">Figure 6(a)</xref>. We selected a random subset of 15 of the 1000 cells of the network, and generated spike trains using a poisson point process with an instantaneous firing rate proportional to the activity level of each cell yielded by the retrieval dynamics at each time step, plus a small noise. With this procedure we obtain the spike trains of each cell, that we can visualize with a rasterplot (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). We can sort the cells according to their firing field position on each of the three manifolds (if these were, e.g., linear tracks, this would correspond to sorting according to place field positions in each of the tracks). When ordered as in the retrieved manifold, the recorded cells show a structured pattern that is considered the hallmark of sequential activity in experimental studies (<xref ref-type="fig" rid="fig6">Figure 6c</xref>, first column). If the same spikes are ordered according to the unretrieved manifolds, the pattern is lost (<xref ref-type="fig" rid="fig6">Figure 6c</xref>, second and third column), indicating that the population activity is retrieving the dynamical structure of the first manifold specifically.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Dynamic retrieval in the presence of multiple memories.</title><p>(<bold>a</bold>) In one dimension (<bold>b</bold>) In two dimensions. Each row represents a snapshot of the dynamics at a point in time. The activity is projected on each of the three attractors stored in the network. In both cases, the first attractor is retrieved, and the activity organizes in a coherent bump that shifts in time. The same activity, projected onto the two non-retrieved maps looks like incoherent noise ((<bold>a</bold>) and (<bold>b</bold>), second and third columns). (<bold>c</bold>) Spiking patterns from a simulated recording of a subset of 15 cells in the network. When cells are sorted according to their firing field on the retrieved manifold (first column), they show sequential activity. The same activity is scattered if looked from the point of view of the unretrieved manifolds (second and third columns).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig6-v1.tif"/></fig><p>Multiple dynamic manifolds can be memorized and retrieved by the network, with different speeds. <xref ref-type="fig" rid="fig7">Figure 7 (b)</xref> shows the result of the numerical simulation of a network with five different one-dimensional manifold stored in its connectivity matrix, each encoded with a different value of γ (see appendix D). These manifold are dynamically retrieved by the network at different speeds, depending on the corresponding γ. This allows the model to simultaneously store memories without the constraint of a fixed dynamical timescale, an important feature for the description of biological circuits that need to be able to operate at different temporal scales.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Retrieval speed and memory interactions.</title><p>(<bold>a</bold>) Multiple mainfolds with different velocity can be stored in a network with manifold-dependent asymmetric connectivity (<bold>b</bold>) The retrieved position at different timesteps during the retrieval dynamics of five different manifolds, stored in the same network, each with a different value of γ. (<bold>c</bold>) Manifolds memorized in the same network can be linked together (<bold>d</bold>) Sequential retrieval of five manifolds. Top row: overlap, measuring the overall coherence with the manifold, as a function of time. Bottom row: retrieved position in each manifold as a function of time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig7-v1.tif"/></fig><p>Different memories stored in the same neural population can interact with each other, building more retrieval schemes in which, for example, the retrieval of a memory cues the retrieval of another one. To investigate this possibility, we have incorporated in the model a mechanism for interaction between memories, in which the endpoint of a dynamical, one-dimensional manifold elicits the activation of the start point of a different one (see Appendix E). This results in the sequential retrieval of multiple memories, one after the other, as illustrated in <xref ref-type="fig" rid="fig7">Figure 7 (d)</xref>. The top row shows the evolution in time of the overlaps <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These order parameters quantify the coherence of the population activity <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with each of the manifolds. Localized activity in manifold μ results in a large <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, while a low <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> corresponds to an incoherent scattering of the activity. The network retrieves the manifolds in sequence, one at a time, following the instructed transitions encoded in its connectivity. The all-or-nothing behavior of the coherence parameters segments the continuous dynamics of the network into a sequence of discrete states.</p><p>The bottom row shows the evolution of the retrieved position, given in each manifold by the center of mass:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The dynamic runs across the retrieved manifold, from its beginning to its end, then jumps to next one and repeats the process. Note that the position in each of the un-retrieved manifold fluctuates around <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>L</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, as a consequence of the incoherence of the activity. Within each of the retrieved manifolds, the dynamic retains its continuous nature in the representation of the evolving position.</p><p>This sequential dynamic goes beyond the simple cued retrieval of independent memories that is the focus of most autoassociative memory models, and provides an example of a hybrid computational system, encoding both continuous and discrete features.</p><p>The interaction mechanism introduced here provides the opportunity to investigate the effect of more complex interactions than the simple memory chain presented here. We present here this first example as a proof of principle of the possibility of storing interacting dynamical memories, and will proceed to the investigation of more complex structures (e.g. interaction networks, probabilistic interactions, etc.) in future studies.</p></sec><sec id="s2-5"><title>Storage capacity</title><p>The number of maps that can be stored and retrieved by an attractor network of this kind is typically proportional to the number of inputs per neuron <inline-formula><mml:math id="inf81"><mml:mi>C</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib56">Treves and Rolls, 1991</xref>). The memory load <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> crucially determines the behavior of the system: when α is increased above a certain threshold value <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, the network is not able to retrieve any of the stored memories, falling instead into a disordered state. Therefore it is the magnitude of <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>, that is the storage capacity of the system, that determines how effectively it can operate as a memory. To estimate the storage capacity of dynamic continuous attractors, and to investigate how it is impacted by the presence of asymmetric connections, we proceed along two complementary paths.</p><p>In the case a fully connected network, where the analytical tools developed for equilibrium systems are not applicable, we take advantage of the fact that numerical simulations can be effective for the estimation of the capacity, since the number of connections per neuron <inline-formula><mml:math id="inf85"><mml:mi>C</mml:mi></mml:math></inline-formula> (the relevant parameters in the definition of the storage capacity <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) coincides with the number of neurons, minus one. For a highly diluted system, on the other hand, the number of neurons is much larger than <inline-formula><mml:math id="inf87"><mml:mi>C</mml:mi></mml:math></inline-formula>, making the simulation of the system very difficult in practice. We then resort to an analytical formulation based on a signal-to-noise analysis (<xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>), that exploits the vanishing correlations between inputs of different neurons in a highly diluted network, and does not require symmetry in the connectivity (<xref ref-type="bibr" rid="bib12">Derrida et al., 1987</xref>). The quantification of the effect of loops in the dense connectivity regime, developed in <xref ref-type="bibr" rid="bib46">Shiino and Fukai, 1992</xref> and <xref ref-type="bibr" rid="bib41">Roudi and Treves, 2004</xref> for the case of static, discrete attractors, is beyond the scope of the present work and remains an interesting open direction.</p><p>In both the fully connected and the highly diluted case we study the dependence of the capacity on two important parameters: the map sparsity, that is the ratio between the width of the connectivity kernel (fixed to one without loss of generality) and the size <inline-formula><mml:math id="inf88"><mml:mi>L</mml:mi></mml:math></inline-formula> of the stored manifolds, and the asymmetry strength γ. Note that the map sparsity <inline-formula><mml:math id="inf89"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> is different from the activity sparsity <inline-formula><mml:math id="inf90"><mml:mi>f</mml:mi></mml:math></inline-formula>: the former is a feature of the stored memories, that we will treat as a control parameter in the following analysis; the latter is a feature of the network dynamics, and its value will be fixed by an optimization procedure in the calculation of the maximal capacity.</p></sec><sec id="s2-6"><title>Analytical calculation of the capacity in the highly diluted limit</title><p>The signal-to-noise approach we follow, illustrated in details in <xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>, involves writing the local field <italic>h</italic><sub><italic>i</italic></sub> as the sum of two contributions: a signal term, due to the retrieved – ‘condensed’ – map, and a noise term consisting of the sum of the contributions of all the other, ‘uncondensed’ maps. In the diluted regime (<inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), these contributions are independent and can be summarized by a Gaussian term <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf93"><mml:mi>z</mml:mi></mml:math></inline-formula> is a random variable with zero mean and unit variance. In the continuous limit, assuming without loss of generality that map <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is retrieved, we can write:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁣</mml:mo><mml:mo mathsize="142%" stretchy="false">′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁣</mml:mo><mml:mo mathsize="142%" stretchy="false">′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁣</mml:mo><mml:mo mathsize="142%" stretchy="false">′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The noise will have variance:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf95"><mml:mi>L</mml:mi></mml:math></inline-formula> is the size of the map, <inline-formula><mml:math id="inf96"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is the spatial variance of the kernel and<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>is the average square activity.</p><p>We can write the fixed point equation for the average activity profile <inline-formula><mml:math id="inf97"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, incorporating the dynamic shift with an argument similar to the one made for the single map case:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf98"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mrow><mml:mrow><mml:msup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The average square activity <inline-formula><mml:math id="inf100"><mml:mi>y</mml:mi></mml:math></inline-formula>, entering the noise term, reads <disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>L</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Introducing the rescaled variables <disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mi>ρ</mml:mi></mml:mfrac></mml:mstyle></mml:math></disp-formula><disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>ρ</mml:mi></mml:mfrac></mml:mstyle></mml:math></disp-formula></p><p>And the functions<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the Gaussian cumulative and the Gaussian probability mass function respectively, we can rewrite the fixed-point equation as <disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Substituting <xref ref-type="disp-formula" rid="equ28">Equation 28</xref> in the expression for the noise variance 19 we obtain<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℳ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If we are able to solve <xref ref-type="disp-formula" rid="equ27">Equation 27</xref> for the rescaled activity profile <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we can use <xref ref-type="disp-formula" rid="equ29">Equation 29</xref> to calculate α. We can then maximize α with respect to <inline-formula><mml:math id="inf104"><mml:mi>g</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mi>w</mml:mi></mml:math></inline-formula>: this yields the maximal value <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> for which retrieval solutions can be found.</p><p>These equations are valid in general and have to be solved numerically. Here we present the results for the case of one-dimensional manifolds and interactions given by the exponential kernel of <xref ref-type="disp-formula" rid="equ36">Equation 36</xref>. In this case, we have<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf107"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the symmetric component of the kernel. A simple approximation, illustrated in appendix F along with the detailed solution procedure, allows to decouple the dependence of <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> on γ and <inline-formula><mml:math id="inf109"><mml:mi>L</mml:mi></mml:math></inline-formula>, with the former given by the spatial variance given by <xref ref-type="disp-formula" rid="equ30">Equation 30</xref> and the latter by the solution of <xref ref-type="disp-formula" rid="equ27 equ29">Equations 27 and 29</xref> in the <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> case. We therefore have:<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The storage capacity is plotted in <xref ref-type="fig" rid="fig8">Figure 8(a)</xref> as a function of γ and <inline-formula><mml:math id="inf111"><mml:mi>L</mml:mi></mml:math></inline-formula>.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Storage capacity.</title><p>(<bold>a</bold>) Storage capacity of a diluted network: dependence on γ and <inline-formula><mml:math id="inf112"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> (represented as <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>b</bold>) Storage capacity of a fully connected network: non monotonic dependence of the capacity on γ. Retrieval / no retrieval phase transition for different values of γ, obtained from simulations with <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, for 1D manifolds. Error bars show the standard error of the observed proportion of successful retrievals. The non-monotonic dependence of the capacity from γ can be appreciated here: the transition point moves toward the right with increasing γ up to <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, then back to the left. (<bold>c</bold>) Storage capacity of a fully connected network as a function of map sparsity <inline-formula><mml:math id="inf118"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> and asymmetry strength γ, for a one-dimensional and a two-dimensional dynamic continuous attractor.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-fig8-v1.tif"/></fig><p>For sparse maps and small values of the asymmetry, the capacity scales as<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The scaling with <inline-formula><mml:math id="inf119"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> is the same found by <xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref> in the analysis of the symmetric case, as expected: for <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> the two models are equivalent.</p><p>The presence of asymmetry decreases the capacity, but does not have a catastrophic effect: the decrease is continuous and scales with a power of γ. There is therefore a wide range of values of asymmetry and map sparsity in which a large number of dynamic manifolds can be stored and retrieved.</p></sec><sec id="s2-7"><title>Numerical estimation of the capacity for a fully connected network</title><p>To estimate the storage capacity for a fully connected network, we proceed with numerical simulations. For a network of fixed size <inline-formula><mml:math id="inf121"><mml:mi>N</mml:mi></mml:math></inline-formula>, and for given γ, <inline-formula><mml:math id="inf122"><mml:mi>L</mml:mi></mml:math></inline-formula> and number of maps <inline-formula><mml:math id="inf123"><mml:mi>p</mml:mi></mml:math></inline-formula>, we run a number of simulations <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>, letting the network evolve from a random initial configuration. We consider a simulation to have performed a successful retrieval if the global overlap <disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>that quantifies the coherence of the activity with map μ, is large for one map <inline-formula><mml:math id="inf125"><mml:msup><mml:mi>μ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> (at least 95% of the overlap value obtained in the case of a single map) and low in all others maps <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>≠</mml:mo><mml:mi>μ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></inline-formula>. We then define the retrieval probability as <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> is the number of observed retrievals.</p><p>We repeat the process varying the storage load, that is the number of stored manifolds <inline-formula><mml:math id="inf129"><mml:mi>p</mml:mi></mml:math></inline-formula>. As <inline-formula><mml:math id="inf130"><mml:mi>p</mml:mi></mml:math></inline-formula> is increased, the system reaches a transition point, at which the retrieval probability rapidly goes to zero. This transition is illustrated, for various values of γ, in <xref ref-type="fig" rid="fig8">Figure 8(b)</xref>.</p><p>The number of maps <italic>p</italic><sub><italic>c</italic></sub> at which the probability reaches zero defines the storage capacity <inline-formula><mml:math id="inf131"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Repeating this procedure for a range of values of γ and <inline-formula><mml:math id="inf132"><mml:mi>L</mml:mi></mml:math></inline-formula>, we obtain the plots shown in <xref ref-type="fig" rid="fig8">Figure 8(c)</xref>, for networks encoding one dimensional and two dimensional dynamical memories.</p><p>The first thing that can be noticed is that the network can store a large number of maps in the fully connected case as well, for a wide range of γ and <inline-formula><mml:math id="inf133"><mml:mi>L</mml:mi></mml:math></inline-formula>. A network with size in the order of ten thousand neurons could store from tens up to hundreds of dynamical memories.</p><p>The capacity for one dimensional attractors is higher than the one for their two dimensional counterparts. This is in line with what was found for symmetric networks (<xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref>).</p><p>Finally, we see that the peak of the capacity is found not only for intermediate values of map sparsity – again in line with what is known from the symmetric case – but also for intermediate values of the coefficient γ. This shows that moderate values of asymmetry can be beneficial for the storage of multiple continuous attractors, a non-trivial phenomenon that may be crucial for the memory capacity of biological networks. In particular this suggests that the natural tendency of the neural activity to show a rich spontaneous dynamics not only does not hinder the possibility for multiple memories to coexist in the same population, but can be a crucial ingredient for the correct functioning of memory mechanisms.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The results presented show how a continuous attractor neural network with memory-dependent asymmetric components in the connectivity can function as a dynamic memory. Our model is simple enough to be treated analytically, robustly produces dynamic retrieval for a large range of the relevant parameters and shows a storage capacity that is comparable to – and in some cases higher than – the capacity for static continuous attractors.</p><p>The analytical solution of the single manifold case shows that the interaction between the strength of the asymmetry and the velocity of the shift can be modulated by global features of the network activity such as its sparsity. This makes the network able to retrieve at different velocities in different regimes, without necessarily requiring short term synaptic modifications. The dependence of the retrieval speed from the sparsity of the activity yields a testable prediction in the context of hippocampal replay: faster reactivations are to be expected in association with an increase in the excitability of the population.</p><p>The insensitivity of the general features of the dynamics to the fine details of the shape of the interactions suggests that this mechanism could robustly emerge from learning or self organization processes in the presence of noise. The quantitative analysis of the learning process needed to effectively memorize low-dimensional dynamic manifolds is an interesting open direction, which goes beyond the scope of this work. However, the asymmetric Hebbian plasticity rule used here provides a simple and biologically realistic starting point.</p><p>Our analysis shows that the simple introduction of an aysmmetric Hebbian plasticity rule is sufficient to describe a dynamic memory able to store and retrieve multiple manifolds with different speeds, and it can incorporate interactions between them, producing the chained retrieval of a sequence of continuous memories.</p><p>The central result of the paper is the quantification of the storage capacity for dynamic continuous attractors, that we find to be large in magnitude, only mildly impacted by the asymmetry in diluted networks, and even higher than the capacity for static attractors in fully connected networks with moderate degrees of asymmetry. The storage capacity of out of equilibrium continuous attractors has been calculated, in a different scenario, by <xref ref-type="bibr" rid="bib62">Zhong et al., 2020</xref>. The authors considered the case of an external signal driving the activity bump along the attractor, in a network of binary neurons, and proceeded to calculate the storage capacity with several assumptions that allowed to model the interference of multiple maps as thermal noise. Interestingly, their main result is broadly compatible with what we show here: in the highly diluted regime the velocity of the external signal has a mild – detrimental – effect on the capacity. This hints that out of equilibrium effects could show some form of universality across different network models and implementations of the shift mechanism. Moreover, a high capacity for dynamical sequences has shown to be achievable also in the case of discrete items (<xref ref-type="bibr" rid="bib23">Gillett et al., 2020</xref>). Together these results suggest that the introduction of a temporal structure is compatible with the functioning of autoassociative memory in recurrent networks, and they open the way to the use of attractor models for the quantitative analysis of complex memory phenomena, such as hippocampal replay and memory schemata.</p><p>The model we propose suggests that the tendency of the activity to move in the neural population is a natural feature of networks with asymmetric connectivity, when the asymmetry is organized along a direction in a low dimensional manifold, and that static memories could be the exception rather than the rule. Indeed, <xref ref-type="bibr" rid="bib33">Mehta et al., 2000</xref> have shown that place fields can become <italic>more</italic> asymmetric in the course of spatial learning, demonstrating that the idea that symmetry emerges from an averaging of trajectory-dependent effects (<xref ref-type="bibr" rid="bib44">Sharp, 1991</xref>) does not always hold true. The structural role of the asymmetry has important implications for the analysis methods used to describe the activity of large populations of neurons, which often rely on the assumption of symmetry in the interactions (e.g. in the analysis of pairwise correlations) or equilibrium of the neural activity (e.g. the standard inverse Ising inference).</p><p>In most of the two- and three-dimensional cases analysed here, the asymmetry is constant along a single direction in each attractor. This can describe the situation in which the temporal evolution of the memory is structured along a certain dimension, and free to diffuse, without energy costs, in the remaining ones. The description of several one-dimensional trajectories embedded in a two dimensional or three dimensional space requires a position-dependent asymmetric component. A systematic analysis of this situation is left for future analysis. However, the simple case of two intersecting trajectories embedded in a 2D map, analysed here, provides a proof of concept that several intersecting trajectories can be correctly retrieved, provided that the activity bump is sufficiently elongated in the direction of the trajectory. A progressive elongation of the place fields in the running direction has been observed in rats running on a linear track (<xref ref-type="bibr" rid="bib32">Mehta et al., 1997</xref>), and our analysis predicts that an analogous effect would be observed also in open-field environments, when restricting the analysis to trajectories in the same running direction.</p><p>Another challenge is posed by the evidence that replayed sequences can be organized both forward and backward in time (<xref ref-type="bibr" rid="bib18">Foster and Wilson, 2006</xref>). The model in its current formulation can produce the retrieval of a given sequence either forward <italic>or</italic> backward, but cannot alternate between the two. This suggests that, if replay relies on asymmetric connections, the hippocampus would have to use different representations for the forward and the backward component. The fact that a change in reward uniquely modulates backward replay (<xref ref-type="bibr" rid="bib1">Ambrose et al., 2016</xref>) provides some evidence in this direction, but this question remains open to experimental investigation.</p><p>The dynamical retrieval of the model generalizes, in the framework of attractor networks, the idea of cognitive maps, incorporating a temporal organization in the low-dimensional manifold encoding the structure of the memory. This feature is reminiscent of the idea of memory schemata – constructs that can guide and constrain our mental activity when we reminisce about the past, imagine future or fictional scenarios or let our minds free to wander (<xref ref-type="bibr" rid="bib11">Ciaramelli and Treves, 2019</xref>). The use of the present model to describe memory schemata will require further steps, such as an account of the interaction between hippocampus and neocortex, and the expansion of the mechanism describing the transition between different dynamical memories. Nevertheless, the idea of dynamic retrieval of a continuous manifold and the integration of the model presented here with effective models of cortical memory networks (<xref ref-type="bibr" rid="bib7">Boboeva et al., 2018</xref>) open promising perspectives.</p><p>Finally, the full analytical description of a densely connected asymmetric attractor network is a challenge that remains open, and can yield valuable insights on the workings of the neural circuits underlying memory.</p></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Work supported by the Human Frontier Science Program RGP0057/2016 collaboration. We are grateful for inspiring exchanges with Remi Monasson and others in the collaboration, and thank Silvia Girardi for her help with <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p></ack><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Software, Validation, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Validation, Investigation, Writing - review and editing</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-69499-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>The work did not generate any experimental dataset. The code used for numerical simulations is publicly available on Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/davidespalla/CADM">https://github.com/davidespalla/CADM</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445">https://archive.softwareheritage.org/swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445</ext-link>)).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrose</surname> <given-names>RE</given-names></name><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reverse replay of hippocampal place cells is uniquely modulated by changing reward</article-title><source>Neuron</source><volume>91</volume><fpage>1124</fpage><lpage>1136</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.047</pub-id><pub-id pub-id-type="pmid">27568518</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname> <given-names>DJ</given-names></name><name><surname>Gutfreund</surname> <given-names>H</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Storing infinite numbers of patterns in a spin-glass model of neural networks</article-title><source>Physical Review Letters</source><volume>55</volume><fpage>1530</fpage><lpage>1533</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.55.1530</pub-id><pub-id pub-id-type="pmid">10031847</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azizi</surname> <given-names>AH</given-names></name><name><surname>Wiskott</surname> <given-names>L</given-names></name><name><surname>Cheng</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A computational model for preplay in the Hippocampus</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>161</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00161</pub-id><pub-id pub-id-type="pmid">24282402</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baraduc</surname> <given-names>P</given-names></name><name><surname>Duhamel</surname> <given-names>JR</given-names></name><name><surname>Wirth</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Schema cells in the macaque Hippocampus</article-title><source>Science</source><volume>363</volume><fpage>635</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1126/science.aav5404</pub-id><pub-id pub-id-type="pmid">30733419</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battaglia</surname> <given-names>FP</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Attractor neural networks storing multiple space representations: a model for hippocampal place fields</article-title><source>Physical Review E</source><volume>58</volume><fpage>7738</fpage><lpage>7753</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.58.7738</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname> <given-names>KI</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A model of spatial map formation in the Hippocampus of the rat</article-title><source>Neural Computation</source><volume>8</volume><fpage>85</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.1.85</pub-id><pub-id pub-id-type="pmid">8564805</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boboeva</surname> <given-names>V</given-names></name><name><surname>Brasselet</surname> <given-names>R</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The capacity for correlated semantic memories in the cortex</article-title><source>Entropy</source><volume>20</volume><elocation-id>824</elocation-id><pub-id pub-id-type="doi">10.3390/e20110824</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname> <given-names>Y</given-names></name><name><surname>Fiete</surname> <given-names>IR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id><pub-id pub-id-type="pmid">19229307</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Lai-Wo S.</surname> <given-names>L</given-names></name><name><surname>Vanderwolf</surname> <given-names>CH</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Cellular bases of hippocampal EEG in the behaving rat</article-title><source>Brain Research Reviews</source><volume>6</volume><fpage>139</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/0165-0173(83)90037-1</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Horváth</surname> <given-names>Z</given-names></name><name><surname>Urioste</surname> <given-names>R</given-names></name><name><surname>Hetke</surname> <given-names>J</given-names></name><name><surname>Wise</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>High-frequency network oscillation in the Hippocampus</article-title><source>Science</source><volume>256</volume><fpage>1025</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1126/science.1589772</pub-id><pub-id pub-id-type="pmid">1589772</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciaramelli</surname> <given-names>E</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A mind free to wander: neural and computational constraints on spontaneous thought</article-title><source>Frontiers in Psychology</source><volume>10</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2019.00039</pub-id><pub-id pub-id-type="pmid">30728796</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derrida</surname> <given-names>B</given-names></name><name><surname>Gardner</surname> <given-names>E</given-names></name><name><surname>Zippelius</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>An exactly solvable asymmetric neural network model</article-title><source>Europhysics Letters</source><volume>4</volume><fpage>167</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1209/0295-5075/4/2/007</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname> <given-names>G</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Preplay of future place cell sequences by hippocampal cellular assemblies</article-title><source>Nature</source><volume>469</volume><fpage>397</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1038/nature09633</pub-id><pub-id pub-id-type="pmid">21179088</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname> <given-names>G</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distinct preplay of multiple novel spatial experiences in the rat</article-title><source>PNAS</source><volume>110</volume><fpage>9100</fpage><lpage>9105</lpage><pub-id pub-id-type="doi">10.1073/pnas.1306031110</pub-id><pub-id pub-id-type="pmid">23671088</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dupret</surname> <given-names>D</given-names></name><name><surname>O'Neill</surname> <given-names>J</given-names></name><name><surname>Pleydell-Bouverie</surname> <given-names>B</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The reorganization and reactivation of hippocampal maps predict spatial memory performance</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nn.2599</pub-id><pub-id pub-id-type="pmid">20639874</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name><name><surname>Cohen</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>From Conditioning to Conscious Recollection: Memory Systems of the Brain</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farooq</surname> <given-names>U</given-names></name><name><surname>Dragoi</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emergence of preconfigured and plastic time-compressed sequences in early postnatal development</article-title><source>Science</source><volume>363</volume><fpage>168</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1126/science.aav0502</pub-id><pub-id pub-id-type="pmid">30630930</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>DJ</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhs</surname> <given-names>MC</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A spin glass model of path integration in rat medial entorhinal cortex</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>4266</fpage><lpage>4276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4353-05.2006</pub-id><pub-id pub-id-type="pmid">16624947</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname> <given-names>CC</given-names></name><name><surname>Wong</surname> <given-names>KY</given-names></name><name><surname>Wu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A moving bump in a continuous manifold: a comprehensive study of the tracking dynamics of continuous attractor neural networks</article-title><source>Neural Computation</source><volume>22</volume><fpage>752</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.07-08-824</pub-id><pub-id pub-id-type="pmid">19922292</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname> <given-names>JA</given-names></name><name><surname>Perich</surname> <given-names>MG</given-names></name><name><surname>Miller</surname> <given-names>LE</given-names></name><name><surname>Solla</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname> <given-names>VE</given-names></name><name><surname>Gilboa</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>What is a memory schema? A historical perspective on current neuroscience literature</article-title><source>Neuropsychologia</source><volume>53</volume><fpage>104</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.11.010</pub-id><pub-id pub-id-type="pmid">24280650</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillett</surname> <given-names>M</given-names></name><name><surname>Pereira</surname> <given-names>U</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Characteristics of sequential activity in networks with temporally asymmetric hebbian learning</article-title><source>PNAS</source><volume>117</volume><fpage>29948</fpage><lpage>29958</lpage><pub-id pub-id-type="doi">10.1073/pnas.1918674117</pub-id><pub-id pub-id-type="pmid">33177232</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gridchyn</surname> <given-names>I</given-names></name><name><surname>Schoenenberger</surname> <given-names>P</given-names></name><name><surname>O'Neill</surname> <given-names>J</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Assembly-Specific disruption of hippocampal replay leads to selective memory deficit</article-title><source>Neuron</source><volume>106</volume><fpage>291</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.021</pub-id><pub-id pub-id-type="pmid">32070475</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>R</given-names></name><name><surname>Perdikis</surname> <given-names>D</given-names></name><name><surname>Jirsa</surname> <given-names>VK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional architectures and structured flows on manifolds: a dynamical framework for motor behavior</article-title><source>Psychological Review</source><volume>121</volume><fpage>302</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1037/a0037014</pub-id><pub-id pub-id-type="pmid">25090422</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname> <given-names>JC</given-names></name><name><surname>Johnson</surname> <given-names>A</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name><name><surname>David Redish</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Hippocampal Sharp Waves and Reactivation during Awake States Depend on Repeated Sequential Experience</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>12415</fpage><lpage>12426</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4118-06.2006</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname> <given-names>C</given-names></name><name><surname>Naim</surname> <given-names>M</given-names></name><name><surname>Boboeva</surname> <given-names>V</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Life on the Edge: Latching Dynamics in a Potts Neural Network</article-title><source>Entropy</source><volume>19</volume><elocation-id>468</elocation-id><pub-id pub-id-type="doi">10.3390/e19090468</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Awake replay of remote experiences in the Hippocampus</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>913</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1038/nn.2344</pub-id><pub-id pub-id-type="pmid">19525943</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Chung</surname> <given-names>JE</given-names></name><name><surname>Sosa</surname> <given-names>M</given-names></name><name><surname>Schor</surname> <given-names>JS</given-names></name><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Larkin</surname> <given-names>MC</given-names></name><name><surname>Liu</surname> <given-names>DF</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Constant Sub-second cycling between representations of possible futures in the Hippocampus</article-title><source>Cell</source><volume>180</volume><fpage>552</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.01.014</pub-id><pub-id pub-id-type="pmid">32004462</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kropff</surname> <given-names>E</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The storage capacity of potts models for semantic memory retrieval</article-title><source>Journal of Statistical Mechanics: Theory and Experiment</source><volume>2005</volume><elocation-id>P08010</elocation-id><pub-id pub-id-type="doi">10.1088/1742-5468/2005/08/P08010</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Lübke</surname> <given-names>J</given-names></name><name><surname>Frotscher</surname> <given-names>M</given-names></name><name><surname>Sakmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title><source>Science</source><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>MR</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Experience-dependent, asymmetric expansion of hippocampal place fields</article-title><source>PNAS</source><volume>94</volume><fpage>8918</fpage><lpage>8921</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.16.8918</pub-id><pub-id pub-id-type="pmid">9238078</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>MR</given-names></name><name><surname>Quirk</surname> <given-names>MC</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Experience-dependent asymmetric shape of hippocampal receptive fields</article-title><source>Neuron</source><volume>25</volume><fpage>707</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81072-7</pub-id><pub-id pub-id-type="pmid">10774737</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monasson</surname> <given-names>R</given-names></name><name><surname>Rosay</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Crosstalk and transitions between multiple spatial maps in an attractor neural network model of the Hippocampus: phase diagram</article-title><source>Physical Review E</source><volume>87</volume><elocation-id>062813</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.87.062813</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nádasdy</surname> <given-names>Z</given-names></name><name><surname>Hirase</surname> <given-names>H</given-names></name><name><surname>Czurkó</surname> <given-names>A</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Replay and time compression of recurring spike sequences in the Hippocampus</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>9497</fpage><lpage>9507</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-21-09497.1999</pub-id><pub-id pub-id-type="pmid">10531452</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Neill</surname> <given-names>J</given-names></name><name><surname>Pleydell-Bouverie</surname> <given-names>B</given-names></name><name><surname>Dupret</surname> <given-names>D</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Play it again: reactivation of waking experience and memory</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>220</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.01.006</pub-id><pub-id pub-id-type="pmid">20207025</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname> <given-names>HF</given-names></name><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Role of Hippocampal Replay in Memory and Planning</article-title><source>Current Biology</source><volume>28</volume><fpage>R37</fpage><lpage>R50</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.073</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oztop</surname> <given-names>E</given-names></name><name><surname>Arbib</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Schema design and implementation of the grasp-related mirror neuron system</article-title><source>Biological Cybernetics</source><volume>87</volume><fpage>116</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1007/s00422-002-0318-1</pub-id><pub-id pub-id-type="pmid">12181587</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id><pub-id pub-id-type="pmid">23594744</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillai</surname> <given-names>AS</given-names></name><name><surname>Jirsa</surname> <given-names>VK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Symmetry breaking in Space-Time hierarchies shapes brain dynamics and behavior</article-title><source>Neuron</source><volume>94</volume><fpage>1010</fpage><lpage>1026</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.013</pub-id><pub-id pub-id-type="pmid">28595045</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roudi</surname> <given-names>Y</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>An associative network with spatially organized connectivity</article-title><source>Journal of Statistical Mechanics: Theory and Experiment</source><volume>2004</volume><elocation-id>P07010</elocation-id><pub-id pub-id-type="doi">10.1088/1742-5468/2004/07/P07010</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname> <given-names>E</given-names></name><name><surname>Namboodiri</surname> <given-names>VMK</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Kropff</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Free association transitions in models of cortical latching dynamics</article-title><source>New Journal of Physics</source><volume>10</volume><elocation-id>015008</elocation-id><pub-id pub-id-type="doi">10.1088/1367-2630/10/1/015008</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samsonovich</surname> <given-names>A</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Path integration and cognitive mapping in a continuous attractor neural network model</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>5900</fpage><lpage>5920</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-15-05900.1997</pub-id><pub-id pub-id-type="pmid">9221787</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharp</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Computer simulation of hippocampal place cells</article-title><source>Psychobiology</source><volume>19</volume><fpage>103</fpage><lpage>115</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical control of arm movements: a dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150509</pub-id><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiino</surname> <given-names>M</given-names></name><name><surname>Fukai</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Self-consistent signal-to-noise analysis and its application to analogue neural networks with asymmetric connections</article-title><source>Journal of Physics A: Mathematical and General</source><volume>25</volume><fpage>L375</fpage><lpage>L381</lpage><pub-id pub-id-type="doi">10.1088/0305-4470/25/7/017</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skaggs</surname> <given-names>WE</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Replay of neuronal firing sequences in rat Hippocampus during sleep following spatial experience</article-title><source>Science</source><volume>271</volume><fpage>1870</fpage><lpage>1873</lpage><pub-id pub-id-type="doi">10.1126/science.271.5257.1870</pub-id><pub-id pub-id-type="pmid">8596957</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Kanter</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Temporal association in asymmetric neural networks</article-title><source>Physical Review Letters</source><volume>57</volume><fpage>2861</fpage><lpage>2864</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.57.2861</pub-id><pub-id pub-id-type="pmid">10033885</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spalla</surname> <given-names>D</given-names></name><name><surname>Dubreuil</surname> <given-names>A</given-names></name><name><surname>Rosay</surname> <given-names>S</given-names></name><name><surname>Monasson</surname> <given-names>R</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Can grid cell ensembles represent multiple spaces?</article-title><source>Neural Computation</source><volume>31</volume><fpage>2324</fpage><lpage>2347</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01237</pub-id><pub-id pub-id-type="pmid">31614108</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Spalla</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>A continuous attractor network with multiple dynamic memories</data-title><source>Software Heritage</source><version designator="swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445">swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445">https://archive.softwareheritage.org/swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stella</surname> <given-names>F</given-names></name><name><surname>Baracskay</surname> <given-names>P</given-names></name><name><surname>O’Neill</surname> <given-names>J</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hippocampal Reactivation of Random Trajectories Resembling Brownian Diffusion</article-title><source>Neuron</source><volume>102</volume><fpage>450</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.052</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname> <given-names>SM</given-names></name><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Trappenberg</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Self-organising continuous attractor networks with multiple activity packets, and the representation of space</article-title><source>Neural Networks</source><volume>17</volume><fpage>5</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(03)00210-7</pub-id><pub-id pub-id-type="pmid">14690703</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname> <given-names>SM</given-names></name><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Trappenberg</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Self-organizing continuous attractor network models of hippocampal spatial view cells</article-title><source>Neurobiology of Learning and Memory</source><volume>83</volume><fpage>79</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2004.08.003</pub-id><pub-id pub-id-type="pmid">15607692</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Threshold-linear formal neurons in auto-associative nets</article-title><source>Journal of Physics A: Mathematical and General</source><volume>23</volume><fpage>2631</fpage><lpage>2650</lpage><pub-id pub-id-type="doi">10.1088/0305-4470/23/12/037</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Computational constraints between retrieving the past and predicting the future, and the CA3-CA1 differentiation</article-title><source>Hippocampus</source><volume>14</volume><fpage>539</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1002/hipo.10187</pub-id><pub-id pub-id-type="pmid">15301433</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Rolls</surname> <given-names>ET</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>What determines the capacity of autoassociative memories in the brain?</article-title><source>Network: Computation in Neural Systems</source><volume>2</volume><fpage>371</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_2_4_004</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attractor neural network models of spatial maps in Hippocampus</article-title><source>Hippocampus</source><volume>9</volume><fpage>481</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1999)9:4&lt;481::AID-HIPO14&gt;3.0.CO;2-S</pub-id><pub-id pub-id-type="pmid">10495029</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Episodic Memory: From Mind to Brain</article-title><source>Annual Review of Psychology</source><volume>53</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.53.100901.135114</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaz</surname> <given-names>AP</given-names></name><name><surname>Wittig</surname> <given-names>JH</given-names></name><name><surname>Inati</surname> <given-names>SK</given-names></name><name><surname>Zaghloul</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Replay of cortical spiking sequences during human memory retrieval</article-title><source>Science</source><volume>367</volume><fpage>1131</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1126/science.aba0672</pub-id><pub-id pub-id-type="pmid">32139543</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>S</given-names></name><name><surname>Hamaguchi</surname> <given-names>K</given-names></name><name><surname>Amari</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamics and computation of continuous attractors</article-title><source>Neural Computation</source><volume>20</volume><fpage>994</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.10-06-378</pub-id><pub-id pub-id-type="pmid">18085986</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-06-02112.1996</pub-id><pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname> <given-names>W</given-names></name><name><surname>Lu</surname> <given-names>Z</given-names></name><name><surname>Schwab</surname> <given-names>DJ</given-names></name><name><surname>Murugan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nonequilibrium statistical mechanics of continuous attractors</article-title><source>Neural Computation</source><volume>32</volume><fpage>1033</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01280</pub-id><pub-id pub-id-type="pmid">32343645</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziv</surname> <given-names>Y</given-names></name><name><surname>Burns</surname> <given-names>LD</given-names></name><name><surname>Cocker</surname> <given-names>ED</given-names></name><name><surname>Hamel</surname> <given-names>EO</given-names></name><name><surname>Ghosh</surname> <given-names>KK</given-names></name><name><surname>Kitch</surname> <given-names>LJ</given-names></name><name><surname>El Gamal</surname> <given-names>A</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Long-term dynamics of CA1 hippocampal place codes</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>264</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nn.3329</pub-id><pub-id pub-id-type="pmid">23396101</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s7" sec-type="appendix"><title>A Numerical simulations</title><p>Numerical simulations are performed with python code, available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/davidespalla/CADM">https://github.com/davidespalla/CADM</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7667e55c3b66bf91b29d0d82b4fb2aae7512ec93;origin=https://github.com/davidespalla/CADM;visit=swh:1:snp:dc72b959d17e29fefe9bef979fdb5052171824bd;anchor=swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445">swh:1:rev:65a5bdfa291840cf5bf10e1da48aadb0b316a445</ext-link>), <xref ref-type="bibr" rid="bib50">Spalla, 2021</xref>.</p><p>In the single map case, to each of the <inline-formula><mml:math id="inf134"><mml:mi>N</mml:mi></mml:math></inline-formula> units (<inline-formula><mml:math id="inf135"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> in 1D, <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1600</mml:mn></mml:mrow></mml:math></inline-formula> in 2D) is assigned a preferential firing location <italic>x</italic><sub><italic>i</italic></sub> on a regular grid spanning the environment with linear dimension <inline-formula><mml:math id="inf137"><mml:mi>L</mml:mi></mml:math></inline-formula>. From this preferred firing locations the interaction matrix <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is constructed, with the formula:<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The precise shape of the symmetric and anti-symmetric parts of the kernel are chosen differently in different simulations, according to the feature the analysis focused on, as specified in the main text. Once the network is assembled, the dynamics is initialized either with a random assignment of activity values to each unit in the range <inline-formula><mml:math id="inf139"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, or with a gaussian bump centered in the middle of the environment (note that, due to the periodic boundary conditions and the translational invariance of the connectivity, the choice of the starting point does not influence the outcome). The dynamics is then evolved in discrete time steps, with the iteration of the following operations:</p><list list-type="bullet"><list-item><p>Calculation of the local fields <inline-formula><mml:math id="inf140"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Calculation of the activity values <inline-formula><mml:math id="inf141"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Dynamic adjustment of the threshold <italic>h</italic><sub>0</sub> such that only the <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> most active neurons remain active: <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Recalculation of the activity <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the adjusted threshold</p></list-item><list-item><p>Dynamic adjustment of the gain <inline-formula><mml:math id="inf146"><mml:mi>g</mml:mi></mml:math></inline-formula> such that the mean activity <inline-formula><mml:math id="inf147"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is fixed to 1: <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Recalculation of the activity <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the adjusted gain</p></list-item></list><p>The adjustment of the parameters of the transfer function is enforced to constrain the network to operate at fixed sparsity <inline-formula><mml:math id="inf150"><mml:mi>f</mml:mi></mml:math></inline-formula> and fixed mean, set to one without loss of generality. The dynamics is iterated for a given number of steps (usually 200), large enough to assure the convergence to the attractive manifold (reached usually in &lt; five steps) and the observation of the dynamical evolution on the manifold.</p><p>In the case of multiple maps, the implemented dynamical evolution is the same, but the interaction matrix is constructed with multiple assignments of the preferred firing locations <italic>x</italic><sub><italic>i</italic></sub>, one for each of the <inline-formula><mml:math id="inf151"><mml:mi>p</mml:mi></mml:math></inline-formula> stored maps:<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The multiple assignments of the preferred firing locations are performed by a random shuffling of the labels of the units before the assignment to the position on the regular grid spanning each map.</p></sec><sec id="s8" sec-type="appendix"><title>B Simulation of electrophysiological recordings</title><p>To simulate the recording from a subset of the network during dynamical retrieval, we constructed a network of 1000 neurons with three different manifolds encoded in its connectivity matrix. We simulated a dynamic of the network from an initial condition correlated with the first manifold, for <inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math></inline-formula> time steps. Since we used circular manifolds, we then selected a chunk of the dynamics corresponding to a single lap on the circle, to have an easier scenario to compare with experimental work. We then simulated an experimental recording by selecting a random subset of 15 observed cells. We used the activity values of each cell during the dynamics as the instantaneous firing rate of a Poisson random process to generate the spiking activity, using a conversion of <inline-formula><mml:math id="inf153"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> to convert the arbitrary value of activity to a plausible range of firing rates, in the order of some Hertz. Spikes produced by this process are our simulated activity recording, and can be plotted according to the preferred firing position of each of the recorded cell in each of the three manifolds. The preferred firing position is supposed to be extracted, in an experimental setting, from the average rate maps of the recorded cells over many observations of the dynamics. In this way we obtain the plots reported in <xref ref-type="fig" rid="fig6">Figure 6(c)</xref>.</p></sec><sec id="s9" sec-type="appendix"><title>C Analytical solution of the single map model in one dimension</title><p>To solve the integral <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> in the case of a 1D manifold and the exponential kernel<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>sign</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>we start by rewriting it as<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf154"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a compact domain for which there exist a solution of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> that is zero at the boundary. This allows to exploit the fact that our threshold-linear system is, indeed, linear in the region in which <inline-formula><mml:math id="inf155"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>We then differentiate twice to obtain the differential equation<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mo>′′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This is a second order linear ODE, with constant coefficients. The presence of the shift term <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> inside the unknown function makes the equation non-trivial to solve. To solve it, we proceed in the following way: first, we look for a particular solution, that is easily found in the constant function<disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Then, we consider the associated homogeneous equation, and look for a solution in the form <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf158"><mml:mi>k</mml:mi></mml:math></inline-formula> is a solution of the characteristic equation <inline-formula><mml:math id="inf159"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, with<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="normal">e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This trascendental equation has to be solved graphically in the complex domain, as shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Analytic solution of <xref ref-type="disp-formula" rid="equ38">Equation (38)</xref>.</title><p>The top row shows the graphical procedure to find the complex zeros of the characteristic <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> given in (40), for three different values of γ. Black and red lines show the zeros of the real and imaginary part of <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. Their intersections are the complex solutions to <inline-formula><mml:math id="inf162"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The blue line represents the sparsity constraint <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The bottom row shows the corresponding solution shapes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-app1-fig1-v1.tif"/></fig><p>For each value of γ and <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>, the equation shows a pair of complex conjugate solutions<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The general solution of the equation will therefore have the form<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mo>-</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo> <mml:mtext>or </mml:mtext><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ42">Equation 42</xref> we can see that the absolute value of <italic>k</italic><sub><italic>i</italic></sub> is related to the width of the bump, and therefore to the sparsity of the solution, by the relation<disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf165"><mml:mi>R</mml:mi></mml:math></inline-formula> does in turn depend only on the free parameter <inline-formula><mml:math id="inf166"><mml:mi>g</mml:mi></mml:math></inline-formula>, through the relation that can be derived in the symmetric case (<inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf168"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>)<disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mrow><mml:mrow><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We can look for a solution with given sparsity <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf170"><mml:mi>L</mml:mi></mml:math></inline-formula> is the length of the manifold) by setting <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Requiring the the sparsity to be fixed (i.e. setting <inline-formula><mml:math id="inf172"><mml:mi>R</mml:mi></mml:math></inline-formula> constant) while varying γ constrains the zeros of 40 to lie in the subspace <inline-formula><mml:math id="inf173"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This imposes a relation between γ and both the speed <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (related to the speed of the shift) and <italic>k</italic><sub><italic>r</italic></sub> (related to the asymmetry of the shape of the solution). Varying <inline-formula><mml:math id="inf175"><mml:mi>R</mml:mi></mml:math></inline-formula> we can study the dependence of the speed on both γ and <inline-formula><mml:math id="inf176"><mml:mi>f</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s10" sec-type="appendix"><title>D Storing multiple manifolds with different retrieval speeds</title><p>To investigate the possibility to store and dynamically retrieve manifolds at different speeds, we have performed numerical simulations of a network with recurrent connectivity given by the formula<disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Each manifold μ is encoded with a different asymmetry strength <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>. The normalization factor <inline-formula><mml:math id="inf178"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is added to ensure that each manifold contributes equally to the synaptic efficacies, and does not affect the ratio of strenghts of the symmetric and asymmetric components.</p></sec><sec id="s11" sec-type="appendix"><title>E Linking multiple manifolds together</title><p>In order to model the interaction between different stored manifolds, we add to the connectivity matrix an ”heteroassociative’ term, whose strength <inline-formula><mml:math id="inf179"><mml:mmultiscripts><mml:mi>J</mml:mi><mml:mi>H</mml:mi><mml:none/><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:mmultiscripts></mml:math></inline-formula> is proportional to the distance between the preferred firing location <inline-formula><mml:math id="inf180"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf181"><mml:mi>i</mml:mi></mml:math></inline-formula> in manifold μ and the one of neuron <inline-formula><mml:math id="inf182"><mml:mi>j</mml:mi></mml:math></inline-formula> in manifold ν, shifted by the length <inline-formula><mml:math id="inf183"><mml:mi>L</mml:mi></mml:math></inline-formula> of the first manifold, which we denote by <inline-formula><mml:math id="inf184"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>ν</mml:mi></mml:msup></mml:math></inline-formula>. This shift enforces the fact that the interaction happens between the end of the first manifold and the beginning of the second. Then, the connectivity matrix will be given by<disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>ν</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>With<disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>ν</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi>ν</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>and <inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if there is a transition between μ and ν, and zero otherwise.</p></sec><sec id="s12" sec-type="appendix"><title>F Analytical calculation of <inline-formula><mml:math id="inf186"><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> in the highly diluted limit</title><p>To calculate the maximum capacity, we first need to solve <xref ref-type="disp-formula" rid="equ27">Equation 27</xref> numerically for <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for given <inline-formula><mml:math id="inf188"><mml:mi>g</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf189"><mml:mi>w</mml:mi></mml:math></inline-formula>. The procedure illustrated here focuses, for the sake of analytical simplicity, on the case of a one-dimensional, exponential kernel<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>sign</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We start from <xref ref-type="disp-formula" rid="equ27">equation 27</xref>: <disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>First, following <xref ref-type="bibr" rid="bib5">Battaglia and Treves, 1998</xref> we rewrite it with the transformation <disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>g</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>obtaining <disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mrow><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo>𝑑</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We then transform this integral equation in a differential one, by differentiating twice. We obtain<disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>′′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula>where we have used the fact that <inline-formula><mml:math id="inf190"><mml:mrow><mml:mrow><mml:msup><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ52">Equation 52</xref> is a second order, nonlinear delayed differential equation. To solve it, it is not sufficient to impose an initial condition on a single point for the solution and the first derivative (i.e. something like <inline-formula><mml:math id="inf191"><mml:mrow><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mn>0</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>): we have to specify the value of the function and its derivative in an interval <inline-formula><mml:math id="inf192"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>To do so, we reason that, if we want a bump solution, <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has to be finite for <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and cannot diverge. We then require the function to be constant (<inline-formula><mml:math id="inf195"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf196"><mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) before a certain value <italic>x</italic><sub>0</sub>, whose value can be set arbitrarily without loss of generality.</p><p>The value <italic>u</italic><sub>0</sub>, at <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf198"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> determines the shape of <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown by the numerical solution presented in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. For <inline-formula><mml:math id="inf200"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> the solution will diverge at <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, while for <inline-formula><mml:math id="inf202"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> it will oscillate. We are then left with a single value <inline-formula><mml:math id="inf203"><mml:mrow><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for which the solution has the required form.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Solutions to <xref ref-type="disp-formula" rid="equ52">Equation 52</xref> for <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf207"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</title><p>Then, keeping <italic>u</italic><sub>0</sub> fixed, we can repeat a similar procedure to find <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> for different values of γ. Also in this case, the solution either diverges or oscillates, apart from a single value <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, for which the solution has the desired shape (see <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). This eliminates the arbitrariness in the choice of <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> since it imposes, for given <inline-formula><mml:math id="inf211"><mml:mi>g</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf212"><mml:mi>w</mml:mi></mml:math></inline-formula>, a relation <inline-formula><mml:math id="inf213"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Solutions to <xref ref-type="disp-formula" rid="equ52">Equation 52</xref> for <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf215"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>.</title><p>We can then find the shape of the bump <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for given values of <inline-formula><mml:math id="inf218"><mml:mi>g</mml:mi></mml:math></inline-formula>, +<inline-formula><mml:math id="inf219"><mml:mi>w</mml:mi></mml:math></inline-formula> and γ, from which we can obtain the profile <inline-formula><mml:math id="inf220"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that we need for the calculation of the storage capacity. Some examples of the obtained profiles, for different values of γ, are shown in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-app1-fig3-v1.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Activity profile <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, obtained for the same <inline-formula><mml:math id="inf222"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, at different values of γ.</title><p>Plugging the obtained form of <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into <xref ref-type="disp-formula" rid="equ29">Equation 29</xref>, we can calculate the capacity. The dependence of the capacity on γ is shown, for <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula>, in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Dependence of the storage capacity on γ, for <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula>.</title><p>The crosses show the full solution of <xref ref-type="disp-formula" rid="equ27 equ29">Equations 27 and 29</xref>. The dashed line is obtained by taking the value of the capacity <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> obtained with full solution at <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and multiplying it by the scaling of the kernel variance <inline-formula><mml:math id="inf229"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Full dots show the value of capacity obtained with the full solution and the contribution of the kernel variance factored out.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69499-app1-fig5-v1.tif"/></fig><p>We can see from the full dots in the figure that the contribution of the integral in <xref ref-type="disp-formula" rid="equ29">Equation 29</xref> is remarkably constant in γ. This is due to the fact that the distortions of the bump shape induced by the presence of the asymmetry have a negligible effect on the average square activity <inline-formula><mml:math id="inf230"><mml:mi>y</mml:mi></mml:math></inline-formula>, whose value is dominated by the dependence on γ of the spatial variance of the kernel (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>).</p><p>This allows us to approximate the value of the integral in <xref ref-type="disp-formula" rid="equ29">Equation 29</xref> with its value in the <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> case. We can then calculate the capacity as a function of γ and <inline-formula><mml:math id="inf232"><mml:mi>L</mml:mi></mml:math></inline-formula> by solving the symmetric case for different Ls, and then incorporating the dependence on γ given by the kernel variance:<disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This approximation yields the results reported in the main text and in <xref ref-type="fig" rid="fig8">Figure 8</xref></p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69499.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Peyrache</surname><given-names>Adrien</given-names></name><role>Reviewing Editor</role><aff><institution>McGill University</institution><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Ferrari</surname><given-names>Ulisse</given-names> </name><role>Reviewer</role><aff><institution>Sorbonne Université</institution><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The study presents a new and elegant theoretical framework that generalizes memory storage and retrieval in neural networks to the dynamical case. Specifically, the authors show that imposing asymmetric interaction between units is sufficient to learn and retrieve sequential activation of these units, resembling the dynamics of hippocampal place cells during memory encoding and replay.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Continuous attractors for dynamic memories&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Ulisse Ferrari (Reviewer #3).</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that this manuscript will not be considered further for publication in <italic>eLife</italic>. However, as pointed out by the reviewers, your work is of potential interest for the field. You may thus consider to submit a new version if you think you can significantly improve the manuscript. In this case, please include the original manuscript reference number as well as the names of the senior (Laura L Colgin) and reviewer editor (Adrien Peyrache) in your cover letter.</p><p>The authors present a computational model that allows for storing and reactivating dynamical memory patterns on low dimensional manifolds. The model presented in this manuscript is simple and elegant. Synaptic couplings are decomposed into a symmetric component, which makes low dimensional manifolds attractive, and an asymmetric component, which produces dynamic retrieval. This is of particular interest as sequential activation of neurons (i.e. trajectories on a manifold) is ubiquitous in the brain, especially in the hippocampus. This work is thus an important contribution to the field, bridging the gap between theoretical and experimental studies. However, the reviewers have raised several major concerns regarding the details of the model and its presentation. Furthermore, it seems necessary to demonstrate that the present model works well beyond the one-dimensional case and/or to test predictions of the model with actual experimental data.</p><p><italic>Reviewer #2:</italic></p><p>I really like the simplicity of the learning rule. However, the article lack clarity in many fundamental sections, and it took me really long to understand important details (if I understood them). When the authors introduce the model, they introduce the inhibition term, but then they remove it as they claim it can be reabsorbed in h<sub>0</sub>. From the equations this claim is not correct, assuming that h<sub>0</sub> does not depend on the values of Vs. It's only by reading the methods that I understood that they actually choose dynamically both g and h<sub>0</sub> to enforce a certain level of sparseness. I would say that already in the main text without introducing the term multiplying b at all, it is only confusing.</p><p>Even more confusing is the introduction of x. They define x as &quot;preferential firing location in the stimulus space&quot;. They never spoke about stimuli, and I think they don't need to. I believe I finally understood what they mean, but I had to read the rest of the article (a couple of times). The description of the phenomenology is also unclear. In Figure 2 they never explain that they use x (y and z) to identify a neuron. So basically V is plotted as a function of the neuron number, parametrized by x. I guess that they discretized x and then assigned each x to a neuron. They should be more explicit about this, especially considering the broad audience of <italic>eLife</italic>. Moreover, they claim that the dynamics occurs in a low dimensional manifold, but in all the examples I see, their trajectory is always 1D, even in the cases in which they have multiple variables (e.g. figure 2c). Also in the dynamic kernel section they determine the kernel only in the case of one variable x, and obviously a 1D trajectory. The kernel they get is a for a rigid shift, under the assumption that the kernel is exponential. What would happen in a more general case? What would be the phenomenology if the dynamics really moves along a 2D or a 3D manifold? They only briefly mention in the Discussion that they consider only a single direction (they should say it from the very beginning). Also, what kind of asymmetric kernels do they consider? Is it only a rigid shift, or it could be a curvilinear trajectory when the underlying manifold has 2 or more dimensions? What is the maximal dimensionality of the trajectory? I guess it depends on the specific form of the asymmetric kernel. I have the impression that the authors have an interesting system to study, but they report only very few aspects of its dynamics (and in a rather confusing way).</p><p>Finally, they mention a few predictions in the Discussion, but they're rather generic. For a journal like <italic>eLife</italic>, in the absence of real data, quantitative predictions are important. The introduction, which is nicely written, contains several examples of phenomena that could be described by the proposed model.</p><p>There are several misprints and references missing.</p><p><italic>Reviewer #3:</italic></p><p>Spalla, Cornacchia and Treves presents a novel class of neural network models capable of storing and retrieving dynamical memory patterns. This result is a major advance in the field because it opens for many possible developments and importantly, it reduces the gap between theoretical and experimental works on memory. Previous models already showed the capacity of a neuronal network to store multiple memories, where each of them is represented by a pattern of neural activations. However, in these models when a particular memory is retrieved, neural activity aligns with the corresponding pattern, but it is static and does not flow along it. This issue prevents a clear connection between experimental and theoretical analysis of memory reactivations which are dynamical in nature, as for example in the case of the replay effect. Here instead, once neural activity aligns with a stored memory pattern, it evolves along it as a dynamical bump of activity. Therefore, here memories are dynamical objects, something closer to what has been observed in experiments.</p><p>Overall the paper presents some very relevant science and the claims are well supported by the presented analyses.</p><p>Strengths:</p><p>Having a simple but powerful model that allows for temporal evolution of reactivated memories represents a major step forward towards the construction neural network models with realistic phenomenology</p><p>Despite the added complexity of the present 'dynamical memory' model with respect to 'static memory' ones, storage capacity is not much affected, and can be even larger in certain regimes.</p><p>The authors' approach generalises previous static models, which are nicely recovered as limiting cases. This allows for encompassing previous results into a more flexible and powerful framework.</p><p>The behavior of the proposed model is robust against the fine details of its construction, meaning that a large class of models will present a similar phenomenology. This is important because it opens for considering more biologically plausible models that will still present the same phenomenological behavior.</p><p>Weaknesses:</p><p>Although the temporal evolution of the reactivated memory is the major advance of this work, their dynamics is very simple: once reactivated they evolve along a given (rigid) direction. It is not clear, for example, if the dynamics can be time-reversed, if cycles are allowed and if different memories can interact dynamically.</p><p>This work reduces the gap between computational and experimental studies of memory storing and retrieval. However it remains still very theoretical, and misses some development towards real neuronal networks. For example, it is suggested, but not shown that a biologically plausible mechanism of plasticity can result in systems that display the same phenomenology of the presented model. Additionally, the paper misses a clear discussion of how the model can empower the analysis of experimental results on memory storage and reactivation. Although it is fair to imagine that reaching these ambitious goals is a long term program that encompasses the current paper, the manuscript would benefit from a deep discussion of what are the main obstacles to be faced. As such, this weakness should be seen as suggestions for further developments, and not as limitation of the impact of the current work.</p><p>I find the dynamics of the retrieved memory either very simple, or not well explained and/or showcased. At first, it is not clear to me if the manifolds are compact and have endpoints. If it is the case, what happens when the dynamics reaches this point? Also, the velocity of the reactivation looks like a rigid property of the stored memory, but then the authors show that actually it might vary depending on the property of the systems (Figure 4, for example). Is there a way for actively tuning the velocity? This may link with the time-compressed reactivation observed in the replay? Additionally, it is not explained if the model can support more complex dynamical effects. For example, considering the effect of reverse replay (Foster and Wilson 2006), I wonder if the dynamics can be time-reversed, so that a dynamical memory can be reactivated in both directions. Also, is it possible to have cycles, so that the dynamics will run in a circle over and over? More generically, which kind of dynamics are supported by the current model? How to extend the model to increase the resulting phenomenology?</p><p>Additionally, I found the model's limitations discussed in LL 363-&gt;370 not clearly explained, and they should anyhow be expanded discussing what the model dynamics can or cannot support.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Continuous attractors for dynamic memories&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Laura Colgin (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been greatly improved since the first submission. We believe that this manuscript is potentially of great interest for the community, however we think that the manuscript would benefit from one minor (but key) revision: to add a &quot;real-life&quot; example that would illustrate the potential of this theoretical work to a broad audience. Basically, this could be as simple as a toy model with place cells spiking along a linear/circular track. This would then entail to simulate the network spontaneous activity and measure the firing rate in time of a sub-sample of the cells (recorded cells), generate some spikes from the firing rate, show that the spiking activity of the network resembles that of replay events (apart from the reverse replay), and present the results in a way that an experimental reader will immediately understand the importance of the study.</p><p>We have also made some comments regarding the general presentation of the figures and equations, detailed below in the reviewer's specific comments. The authors can of course decide not to follow these suggestions as they are not critical for the main message of the paper.</p><p><italic>Reviewer #2:</italic></p><p>Spalla, Cornacchia and Treves have largely improved the manuscript and fully addressed all my main criticisms, and in my opinion also those of the other referee. I already found the paper interesting during the first review round, and now it has also improved in clarity and completeness. I particularly appreciated:</p><p>– The results of the paper are novel and very interesting. Even if the model is very simple and easy to understand, it presents an impressive phenomenology. Also, by just adding one more feature, it nicely extends previous models (the asymmetric component of the kernel), which are recovered when γ-&gt;0.</p><p>– The efforts that the authors have made to better characterise and describe the phenomenology of the dynamical retrieval: analyses as those of Figure 7 are very intriguing and in my opinion will open for further developments of the research field.</p><p>– The improvement of the exposure, which now better fits the scope and wide audience of <italic>eLife</italic>.</p><p>For all these reasons I recommend the publication of this paper.</p><p><italic>Reviewer #3:</italic></p><p>The study by Spalla et al. presents a new and elegant theoretical framework that generalizes memory storage and retrieval in auto associative networks to the dynamical case. Specifically, while purely symmetrical synapses (as in canonical attractor networks) lead to the retrieval of static patterns, asymmetric synapses lead to sequential activation of units in the network, as observed for example during replay of hippocampal place cells. In addition, the study demonstrates that, as previous models had shown in the static case, such network can store multiple maps.</p><p>The main concerns with this study, in its present form, is the general presentation of the model and of its general properties, which could be hard to follow for a non-expert reader. The study would also benefit from concrete examples, such as simulation of replay in a minimal model of hippocampal place cells.</p><p>The study by Spalla et al. presents a new and elegant theoretical framework that generalizes memory storage and retrieval in auto associative networks to the dynamical case. Specifically, asymmetric synapses lead to sequential activation of units in the network, instead of the classical static patterns emerging from purely symmetrical synapses (e.g. attractors in which connectivity kernels depends only on the &quot;representational distance&quot; between units in the feature space). The study also demonstrates that, as previous models had shown in the static case, such network can also store multiple maps.</p><p>While the authors have already addressed many comments made during a previous round of review, the general presentation of the manuscript should be greatly improved for publication in a life science journal targeting a broad audience. Importantly, the number of main figures should be drastically reduced, and figures should focus only on the most important claims of the study.</p><p>One major concern though is that the study would benefit from a concrete example. For example, displaying simulated &quot;replayed&quot; trajectories in 2D maps (as a neurophysiologist would present it in an experimental study) in different cases influencing the speed (and general behaviour) of replay, such as different levels of asymmetry and sparsity.</p><p>The remarks below are related to the specific presentation of the manuscript.</p><p>Top panels of Figure 3 and associated text should be put in supplementary info, as this section can be fairly confusing for the non-expert reader and does not contribute much to the understanding of the phenomenon.</p><p>Figure 4 conveys an important message: the speed of the bump is related to the asymmetry of the connectivity kernel. The figure legend should be clarified: x-axis should read &quot;kernel asymmetry (\γ)&quot;, with arrows point to either left (&quot;more symmetrical&quot;) to right (&quot;more asymmetrical&quot;). |dx| should be replace by &quot;bump speed (a.u.)&quot;. The message of Figure 4b is unclear and should certainly be discarded.</p><p>While Figure 5 shows that the solution can be generalized to a broad class of asymmetric kernels, Figure 6 presents again the link between network parameters and bump features (speed, etc.) in the case of an exponential kernel. One idea would be to merge the bottom panels of Figure 3, Figure 4a and Figure 6 to convey one message about the exponential case. Then show (current) Figure 5 to show that it is valid in other cases.</p><p>Figure 7 presents an interesting simulation, illustrating that retrieval can be specific to a particular map. Here again, the presentation should be drastically improved: x_1,2,3 should be replaced by more explicit terms like &quot;feature space (map 1)&quot; or something similar. Y-axis should be labels &quot;activity level (V)&quot; or something similar, etc. Against, it is important to present figure that do not require the reader to rely too much on abbreviations and variable names.</p><p>The link between kernel asymmetry and storage capacity is important but should be made clearer. Most of the equations (17-29) should be put in appendix and be limited to equation 30 and 31. Figure 8 and 9 present overlapping data, they should be merged (or better: only one of the two should be shown) and the same remarks as for Figure 4 apply for the presentation of this figure.</p><p>The presentation of the non-monotonic dependence of storage capacity w.r.t \γ in the fully connected case is another aspect that should be improved. Figure 10 would benefit from an additional graph showing P=50% chance retrieval as a function of γ to illustrate the non-monotonic relationship. This figure should be merge in a multi-panel figure with Figure 11 so that the reader can immediately grasp the main messages regarding the fully connected case.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69499.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>I really like the simplicity of the learning rule. However, the article lack clarity in many fundamental sections, and it took me really long to understand important details (if I understood them). When the authors introduce the model, they introduce the inhibition term, but then they remove it as they claim it can be reabsorbed in h<sub>0</sub>. From the equations this claim is not correct, assuming that h<sub>0</sub> does not depend on the values of Vs. It's only by reading the methods that I understood that they actually choose dynamically both g and h<sub>0</sub> to enforce a certain level of sparseness. I would say that already in the main text without introducing the term multiplying b at all, it is only confusing.</p></disp-quote><p>We have clarified in the main text (LL 120-126) how the inhibition mechanism works, and simplified the notation, avoiding the complication of the introduction of the b(x) term.</p><disp-quote content-type="editor-comment"><p>Even more confusing is the introduction of x. They define x as &quot;preferential firing location in the stimulus space&quot;. They never spoke about stimuli, and I think they don't need to. I believe I finally understood what they mean, but I had to read the rest of the article (a couple of times).</p></disp-quote><p>We have expanded the section “A mechanistic model for dynamic retrieval” (LL 127-139), clarifying the meaning of the parameter x and avoiding references to stimuli, and striving for a clearer and more accessible explanation of the model.</p><disp-quote content-type="editor-comment"><p>The description of the phenomenology is also unclear. In Figure 2 they never explain that they use x (y and z) to identify a neuron. So basically V is plotted as a function of the neuron number, parametrized by x. I guess that they discretized x and then assigned each x to a neuron. They should be more explicit about this, especially considering the broad audience of eLife.</p></disp-quote><p>We have clarified the phenomenology presented in Figure 2, by rewriting and expanding the description of the plots in the main text (LL 174-183)</p><disp-quote content-type="editor-comment"><p>Moreover, they claim that the dynamics occurs in a low dimensional manifold, but in all the examples I see, their trajectory is always 1D, even in the cases in which they have multiple variables (e.g. figure 2c). Also in the dynamic kernel section they determine the kernel only in the case of one variable x, and obviously a 1D trajectory.</p></disp-quote><p>We clarify this aspect in LL 207-212 and in the discussion (LL 500-506), underlying how the dynamics in 2D and 3D is different from a simple 1D trajectory.</p><p>We have also added a new analysis of the case of two intersecting 1D trajectories embedded in a 2D manifold (Figure 2(e) and accompanying text), which explores the case of position-dependent asymmetry, and provides a proof of concept of the possibility to expand the model in this direction.</p><disp-quote content-type="editor-comment"><p>The kernel they get is a for a rigid shift, under the assumption that the kernel is exponential.</p></disp-quote><p>This problem is addressed in the section “dynamical retrieval is robust” (LL 266-283). There we investigate the effect of the kernel shape and its parameters on the behaviour of the model. We find this behaviour to be largely independent from the specific kernel shape, and able to produce dynamic retrieval in a remarkably broad range of parameters (see Figure 4 and Figure 5).</p><disp-quote content-type="editor-comment"><p>What would happen in a more general case? What would be the phenomenology if the dynamics really moves along a 2D or a 3D manifold? They only briefly mention in the Discussion that they consider only a single direction (they should say it from the very beginning). Also, what kind of asymmetric kernels do they consider? Is it only a rigid shift, or it could be a curvilinear trajectory when the underlying manifold has 2 or more dimensions? What is the maximal dimensionality of the trajectory? I guess it depends on the specific form of the asymmetric kernel. I have the impression that the authors have an interesting system to study, but they report only very few aspects of its dynamics (and in a rather confusing way).</p></disp-quote><p>We have added clarifications on these aspects as stated in the previous points.</p><p>In the case with a constant asymmetric direction, the presence of the asymmetry imposes no additional bound on the maximal dimensionality of the manifold.</p><p>As it is the case in continuous attractor networks of any kind, a trade-off is to be expected between the resolution and the dimensionality of the representation, if the size of the network is kept fixed.</p><disp-quote content-type="editor-comment"><p>Finally, they mention a few predictions in the Discussion, but they're rather generic. For a journal like eLife, in the absence of real data, quantitative predictions are important. The introduction, which is nicely written, contains several examples of phenomena that could be described by the proposed model.</p></disp-quote><p>We have added two key quantitative predictions by the model: the predicted elongation of place fields in the direction of the running trajectory (LL 224-227, and LL 506-513) and an interaction between the sparsity of the activity of the network and the retrieval speed (LL 191- 196 and LL 452-459).</p><p>These predictions complement the main result of the paper: that auto associative memory networks can encode continuous dynamic memory with large capacity, and that the asymmetry of connections, often considered as an afterthought in mechanistic models, can be a crucial ingredient of memory systems.</p><p>We believe these points are of interest to the broad <italic>eLife</italic> audience, beyond the technical apparatus developed to obtain them.</p><disp-quote content-type="editor-comment"><p>There are several misprints and references missing.</p></disp-quote><p>We have made an effort to correct all misprints and missing references, and we have added several more references to better put the work in the context of the existing literature.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>Spalla, Cornacchia and Treves presents a novel class of neural network models capable of storing and retrieving dynamical memory patterns. This result is a major advance in the field because it opens for many possible developments and importantly, it reduces the gap between theoretical and experimental works on memory. Previous models already showed the capacity of a neuronal network to store multiple memories, where each of them is represented by a pattern of neural activations. However, in these models when a particular memory is retrieved, neural activity aligns with the corresponding pattern, but it is static and does not flow along it. This issue prevents a clear connection between experimental and theoretical analysis of memory reactivations which are dynamical in nature, as for example in the case of the replay effect. Here instead, once neural activity aligns with a stored memory pattern, it evolves along it as a dynamical bump of activity. Therefore, here memories are dynamical objects, something closer to what has been observed in experiments.</p><p>Overall the paper presents some very relevant science and the claims are well supported by the presented analyses.</p><p>Strengths:</p><p>Having a simple but powerful model that allows for temporal evolution of reactivated memories represents a major step forward towards the construction neural network models with realistic phenomenology</p><p>Despite the added complexity of the present 'dynamical memory' model with respect to 'static memory' ones, storage capacity is not much affected, and can be even larger in certain regimes.</p><p>The authors' approach generalises previous static models, which are nicely recovered as limiting cases. This allows for encompassing previous results into a more flexible and powerful framework.</p><p>The behavior of the proposed model is robust against the fine details of its construction, meaning that a large class of models will present a similar phenomenology. This is important because it opens for considering more biologically plausible models that will still present the same phenomenological behavior.</p><p>Weaknesses:</p><p>Although the temporal evolution of the reactivated memory is the major advance of this work, their dynamics is very simple: once reactivated they evolve along a given (rigid) direction. It is not clear, for example, if the dynamics can be time-reversed, if cycles are allowed and if different memories can interact dynamically.</p><p>This work reduces the gap between computational and experimental studies of memory storing and retrieval. However it remains still very theoretical, and misses some development towards real neuronal networks. For example, it is suggested, but not shown that a biologically plausible mechanism of plasticity can result in systems that display the same phenomenology of the presented model. Additionally, the paper misses a clear discussion of how the model can empower the analysis of experimental results on memory storage and reactivation. Although it is fair to imagine that reaching these ambitious goals is a long term program that encompasses the current paper, the manuscript would benefit from a deep discussion of what are the main obstacles to be faced. As such, this weakness should be seen as suggestions for further developments, and not as limitation of the impact of the current work.</p><p>I find the dynamics of the retrieved memory either very simple, or not well explained and/or showcased. At first, it is not clear to me if the manifolds are compact and have endpoints. If it is the case, what happens when the dynamics reaches this point?</p></disp-quote><p>We have clarified in the presentation of the model (LL 158-165) that we consider manifolds with periodic boundary conditions, in which the dynamic retrieval produces periodic cycles. We highlight that this simplifying assumption is not a necessary feature of the model, and we study the alternative case in which this is replaced by a link between the endpoint of a manifold and the beginning of the next one (Figure 7 (c), (d) and accompanying text.). This analysis not only shows that dynamic retrieval does not rely on the specific boundary conditions, but also demonstrates that a chain of multiple continuous manifolds can be retrieved sequentially by the network.</p><disp-quote content-type="editor-comment"><p>Also, the velocity of the reactivation looks like a rigid property of the stored memory, but then the authors show that actually it might vary depending on the property of the systems (Figure 4, for example). Is there a way for actively tuning the velocity? This may link with the time-compressed reactivation observed in the replay?</p></disp-quote><p>We have further expanded the study of the dependence of the retrieval speed on the parameters of the network by adding an analysis of the effect of sparsity on the reactivation velocity (Figure 2 (d) and Figure 3 (b), (c)).</p><p>We find that the sparsity of the representation (a feature of the network activity, that can be regulated instantaneously during the dynamics) interacts with the asymmetry parameter \γ in determining the retrieval speed.</p><p>The possibility of instantaneously tuning the velocity by acting on the sparsity of the activity also yields a key new prediction: in a network with fixed connectivity, retrieval speed can be modulated by the activity level of the population.</p><disp-quote content-type="editor-comment"><p>Additionally, it is not explained if the model can support more complex dynamical effects. For example, considering the effect of reverse replay (Foster and Wilson 2006), I wonder if the dynamics can be time-reversed, so that a dynamical memory can be reactivated in both directions. Also, is it possible to have cycles, so that the dynamics will run in a circle over and over? More generically, which kind of dynamics are supported by the current model? How to extend the model to increase the resulting phenomenology?</p></disp-quote><p>We have added the clarification that the current model cannot handle backward and forward replay of the same trajectory (LL 196-198 and LL 514-521 in the discussion). The dynamics is naturally organized in cycles under the assumption of periodic boundary conditions, but this is not the only possibility the model can describe.</p><p>To clarify this point, we have added new analyses that demonstrate complex dynamical effects. In particular the linking of dynamical memories and the possibility to simultaneously memorize manifolds with different values of the asymmetric component, leading to different retrieval speeds (Figure 7 and accompanying text).</p><disp-quote content-type="editor-comment"><p>Additionally, I found the model's limitations discussed in LL 363-&gt;370 not clearly explained, and they should anyhow be expanded discussing what the model dynamics can or cannot support.</p></disp-quote><p>As stated in response to reviewer 2: we clarify this aspect in LL 207-212 and in the discussion (LL 500-506), underlying how the dynamics in 2D and 3D is different from a simple 1D trajectory.</p><p>We have also added a new analysis of the case of two intersection 1D trajectories embedded in a 2D manifold (Figure 2(e) and accompanying text), which explores the case of position-dependent asymmetry, and provides a proof of concept of the possibility to expand the model in this direction.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The study by Spalla et al. presents a new and elegant theoretical framework that generalizes memory storage and retrieval in auto associative networks to the dynamical case. Specifically, while purely symmetrical synapses (as in canonical attractor networks) lead to the retrieval of static patterns, asymmetric synapses lead to sequential activation of units in the network, as observed for example during replay of hippocampal place cells. In addition, the study demonstrates that, as previous models had shown in the static case, such network can store multiple maps.</p><p>The main concerns with this study, in its present form, is the general presentation of the model and of its general properties, which could be hard to follow for a non-expert reader. The study would also benefit from concrete examples, such as simulation of replay in a minimal model of hippocampal place cells.</p><p>The study by Spalla et al. presents a new and elegant theoretical framework that generalizes memory storage and retrieval in auto associative networks to the dynamical case. Specifically, asymmetric synapses lead to sequential activation of units in the network, instead of the classical static patterns emerging from purely symmetrical synapses (e.g. attractors in which connectivity kernels depends only on the &quot;representational distance&quot; between units in the feature space). The study also demonstrates that, as previous models had shown in the static case, such network can also store multiple maps.</p><p>While the authors have already addressed many comments made during a previous round of review, the general presentation of the manuscript should be greatly improved for publication in a life science journal targeting a broad audience. Importantly, the number of main figures should be drastically reduced, and figures should focus only on the most important claims of the study.</p></disp-quote><p>We thank the reviewer for the suggestion. We have reduced the number of main figures by merging the last three figures in what is now Figure 8, and have strived to improve graphical quality and figure accessibility by a general standardization of figure design and a relabeling of figure axes throughout the manuscript, with the aim to make them more consistent and more readily interpretable.</p><disp-quote content-type="editor-comment"><p>One major concern though is that the study would benefit from a concrete example. For example, displaying simulated &quot;replayed&quot; trajectories in 2D maps (as a neurophysiologist would present it in an experimental study) in different cases influencing the speed (and general behaviour) of replay, such as different levels of asymmetry and sparsity.</p></disp-quote><p>We agree with the reviewer that a concrete example is beneficial for the interpretation of the study. To this end, we have added the analysis of a simulated electrophysiological experiment probing the population activity during dynamic retrieval (see also response to the editors).</p><p>This simulation is illustrated in Figure 6(c), constructed so as to be familiar to experimental readers in its design and interpretation.</p><p>The figure and the accompanying text (LL 314-328, plus appendix B for details) serve the double purpose to connect the behaviour of the model to experimentally observable quantities, and to further illustrate the concept of dynamic auto associative memory, that Figure 6 was originally built to exemplify.</p><disp-quote content-type="editor-comment"><p>The remarks below are related to the specific presentation of the manuscript.</p><p>Top panels of Figure 3 and associated text should be put in supplementary info, as this section can be fairly confusing for the non-expert reader and does not contribute much to the understanding of the phenomenon.</p></disp-quote><p>We believe that the analytical solvability of the model is a key feature of our work that can yield quantitative insights and a deep understanding of the mechanisms behind dynamic retrieval. Therefore, we chose to maintain Figure 3 and the accompanying paragraph in the main text. However, during the first revision we moved all technical details to Appendix C, including the previous Figure 3 illustrating the analytical solution procedure, leaving only results and interpretation in the main text.</p><p>We have tried to improve the graphical design and axis labeling to make the figure more accessible to a non-technical audience.</p><disp-quote content-type="editor-comment"><p>Figure 4 conveys an important message: the speed of the bump is related to the asymmetry of the connectivity kernel. The figure legend should be clarified: x-axis should read &quot;kernel asymmetry (\γ)&quot;, with arrows point to either left (&quot;more symmetrical&quot;) to right (&quot;more asymmetrical&quot;). |dx| should be replace by &quot;bump speed (a.u.)&quot;. The message of Figure 4b is unclear and should certainly be discarded.</p></disp-quote><p>We think the reviewer is referring to the version of Figure 4 that was in the original manuscript. We indeed changed this figure during our first revision, by getting rid of Figure 4b, as the reviewer suggested, and incorporating Figure 4a, with an extended analysis on the effect of sparsity, in what is now Figure 3. We have also changed axis label names to improve clarity.</p><disp-quote content-type="editor-comment"><p>While Figure 5 shows that the solution can be generalized to a broad class of asymmetric kernels, Figure 6 presents again the link between network parameters and bump features (speed, etc.) in the case of an exponential kernel. One idea would be to merge the bottom panels of Figure 3, Figure 4a and Figure 6 to convey one message about the exponential case. Then show (current) Figure 5 to show that it is valid in other cases.</p></disp-quote><p>We agree with the reviewer and thank them for the suggestion. We swapped the two figures (they are now Figure 4, on the effects of the parameters in the exponential case and Figure 5, on different kernel shapes) and restructured the section on robustness of the asymmetric interactions by examining in depth the exponential kernel case first, and then extending the analysis to other kernel shapes.</p><disp-quote content-type="editor-comment"><p>Figure 7 presents an interesting simulation, illustrating that retrieval can be specific to a particular map. Here again, the presentation should be drastically improved: x_1,2,3 should be replaced by more explicit terms like &quot;feature space (map 1)&quot; or something similar. Y-axis should be labels &quot;activity level (V)&quot; or something similar, etc. Against, it is important to present figure that do not require the reader to rely too much on abbreviations and variable names.</p></disp-quote><p>We have changed the labels in this figure, as well as in the others throughout the manuscript, following this advice for which we are thankful to the reviewer. We have also made some changes in coloring and label sizes to improve readability, as well as adding here as a new panel the results of the new simulation of experimental recordings (see main points).</p><disp-quote content-type="editor-comment"><p>The link between kernel asymmetry and storage capacity is important but should be made clearer. Most of the equations (17-29) should be put in appendix and be limited to equation 30 and 31. Figure 8 and 9 present overlapping data, they should be merged (or better: only one of the two should be shown) and the same remarks as for Figure 4 apply for the presentation of this figure.</p></disp-quote><p>We believe that the reviewer is here referring to the first version of the manuscript.</p><p>During our first revision we heavily restructured this section putting most of the mathematical details to Appendix F. Also Figure 8 was moved to the appendix, since we agree with the reviewer that the data is overlapping with Figure 9 (now Figure 8a), and the point of the figure (to show the goodness of our approximation ansatz) is beyond the scope of the main text. Following the previous point on the number of main figures, we have aggregated all figures of this section in what is now Figure 8, that conveys in a single place the main messages on the storage capacity of the network.</p><disp-quote content-type="editor-comment"><p>The presentation of the non-monotonic dependence of storage capacity w.r.t \γ in the fully connected case is another aspect that should be improved. Figure 10 would benefit from an additional graph showing P=50% chance retrieval as a function of γ to illustrate the non-monotonic relationship. This figure should be merge in a multi-panel figure with Figure 11 so that the reader can immediately grasp the main messages regarding the fully connected case.</p></disp-quote><p>We have merged the figures in what is now Figure 8. We did not insert an additional graph, with the rationale not to show overlapping data and not to multiply further the number of figures. However, we have changed labels and other graphical aspects to improve understandability and readability. We believe that the non-monotonicity of the storage capacity is appreciable by the reader from both Figure 8b, where the transition point is shown to move back and forth with γ, and Figure 8c, in which the joint dependence of the capacity on γ and 1/L shows a clear peak in the bulk of the parameter ranges.</p></body></sub-article></article>