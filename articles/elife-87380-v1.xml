<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">87380</article-id><article-id pub-id-type="doi">10.7554/eLife.87380</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87380.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Brain areas for reversible symbolic reference, a potential singularity of the human brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>van Kerkoerle</surname><given-names>Timo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1935-8216</contrib-id><email>timo.vankerkoerle@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Pape</surname><given-names>Louise</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ekramnia</surname><given-names>Milad</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Xiaoxia</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tasserie</surname><given-names>Jordy</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dupont</surname><given-names>Morgan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xiaolian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3648-7554</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Jarraya</surname><given-names>Béchir</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0878-763X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Vanduffel</surname><given-names>Wim</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff11">11</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name><email>gdehaene@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin center</institution></institution-wrap><addr-line><named-content content-type="city">Gif sur Yvette</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Department of Neurophysics, Donders Centre for Neuroscience, Radboud University Nijmegen</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Department of Psychiatry, Radboud University Nijmegen Medical Centre</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>State Key Laboratory of Cognitive Neuroscience and Learning &amp; IDG, McGovern Institute for Brain Research, Beijing Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00zs45r78</institution-id><institution>Center for Brain Circuit Therapeutics, Department of Neurology, Brigham &amp; Women’s Hospital, Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>Department of Neurosciences, Laboratory of Neuro- and Psychophysiology, KU Leuven Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>Leuven Brain Institute, KU Leuven</institution></institution-wrap><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/058td2q88</institution-id><institution>Université Paris-Saclay (UVSQ), Hôpital Foch</institution></institution-wrap><addr-line><named-content content-type="city">Suresnes</named-content></addr-line><country>France</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Charlestown</named-content></addr-line><country>United States</country></aff><aff id="aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Radiology, Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff11"><label>11</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04ex24z53</institution-id><institution>Collège de France, Université Paris-Sciences-Lettres (PSL)</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>02</month><year>2025</year></pub-date><volume>12</volume><elocation-id>RP87380</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-03-16"><day>16</day><month>03</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-03-04"><day>04</day><month>03</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.04.531109"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-07-21"><day>21</day><month>07</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87380.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-08"><day>08</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87380.2"/></event></pub-history><permissions><copyright-statement>© 2023, van Kerkoerle et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>van Kerkoerle et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-87380-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-87380-figures-v1.pdf"/><abstract><p>The emergence of symbolic thinking has been proposed as a dominant cognitive criterion to distinguish humans from other primates during hominisation. Although the proper definition of a symbol has been the subject of much debate, one of its simplest features is bidirectional attachment: the content is accessible from the symbol, and vice versa. Behavioural observations scattered over the past four decades suggest that this criterion might not be met in non-human primates, as they fail to generalise an association learned in one temporal order (A to B) to the reverse order (B to A). Here, we designed an implicit fMRI test to investigate the neural mechanisms of arbitrary audio–visual and visual–visual pairing in monkeys and humans and probe their spontaneous reversibility. After learning a unidirectional association, humans showed surprise signals when this learned association was violated. Crucially, this effect occurred spontaneously in both learned and reversed directions, within an extended network of high-level brain areas, including, but also going beyond, the language network. In monkeys, by contrast, violations of association effects occurred solely in the learned direction and were largely confined to sensory areas. We propose that a human-specific brain network may have evolved the capacity for reversible symbolic reference.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fMRI</kwd><kwd>primate</kwd><kwd>human</kwd><kwd>learning</kwd><kwd>symbol</kwd><kwd>language</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission - Stichting Radboud Universiteit</institution></institution-wrap></funding-source><award-id>101078667</award-id><principal-award-recipient><name><surname>van Kerkoerle</surname><given-names>Timo</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission - Commissariat à l'Énergie Atomique et aux Énergies Alternatives</institution></institution-wrap></funding-source><award-id>695403</award-id><principal-award-recipient><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission - Commissariat à l'Énergie Atomique et aux Énergies Alternatives</institution></institution-wrap></funding-source><award-id>695710</award-id><principal-award-recipient><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven</institution></institution-wrap></funding-source><award-id>C14/21/111</award-id><principal-award-recipient><name><surname>Vanduffel</surname><given-names>Wim</given-names></name><name><surname>Li</surname><given-names>Xiaolian</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>FWO-Flanders</institution></institution-wrap></funding-source><award-id>G0E0520N</award-id><principal-award-recipient><name><surname>Vanduffel</surname><given-names>Wim</given-names></name><name><surname>Li</surname><given-names>Xiaolian</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004040</institution-id><institution>KU Leuven</institution></institution-wrap></funding-source><award-id>G0C1920N</award-id><principal-award-recipient><name><surname>Vanduffel</surname><given-names>Wim</given-names></name><name><surname>Li</surname><given-names>Xiaolian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Humans spontaneously reverse learned associations while macaque monkeys do not, providing a minimal test of a distinctive human capacity for symbolic representations.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>It is a longstanding question whether there is something unique about the cognitive abilities of humans relative to other animals (<xref ref-type="bibr" rid="bib40">Hauser et al., 2002</xref>; <xref ref-type="bibr" rid="bib31">Fitch et al., 2005</xref>; <xref ref-type="bibr" rid="bib49">Iriki, 2006</xref>; <xref ref-type="bibr" rid="bib46">Hopkins et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Kietzmann, 2019</xref>; <xref ref-type="bibr" rid="bib87">Penn et al., 2008</xref>; <xref ref-type="bibr" rid="bib4">Berwick and Chomsky, 2016</xref>). Symbols are ubiquitous in many domains of human cognition, underlying not only language but also mathematical, musical, or social representations and many others domains (<xref ref-type="bibr" rid="bib10">Deacon, 1998</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Kabdebon and Dehaene-Lambertz, 2019</xref>; <xref ref-type="bibr" rid="bib79">Nieder, 2009</xref>; <xref ref-type="bibr" rid="bib101">Sablé-Meyer et al., 2021</xref>). The appearance of symbolic representations, which would develop in parallel with the expansion of prefrontal and parietal associative areas, has therefore been suggested as a potential marker signalling hominisation (<xref ref-type="bibr" rid="bib10">Deacon, 1998</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib42">Henshilwood et al., 2002</xref>; <xref ref-type="bibr" rid="bib76">Neubauer et al., 2018</xref>).</p><p>This proposal, however, hinges on the definition of what a symbol is. The term symbol is often used as a synonym for a sign, which is classically defined by Ferdinand de Saussure as an arbitrary binding between a ‘signifier’ (for instance a word, a digit, but also a traffic sign, logo, etc.) and a ‘signified’ (the meaning or content to which the signifier refers) <xref ref-type="bibr" rid="bib13">de Saussure, 1995</xref>. In that respect, however, many non-human animals, including chimpanzees, macaques, but also dogs, are able to learn hundreds of such indexical relationships, even with arbitrary signs (<xref ref-type="bibr" rid="bib51">Kaminski et al., 2004</xref>; <xref ref-type="bibr" rid="bib60">Livingstone et al., 2010</xref>; <xref ref-type="bibr" rid="bib68">Matsuzawa, 1985</xref>; <xref ref-type="bibr" rid="bib95">Premack, 1971</xref>). Even bees can learn to associate arbitrary visual shapes to abstract representations such as visual quantities (two or three elements) independently of the density, size, and colour of the elements in the visual display (<xref ref-type="bibr" rid="bib47">Howard et al., 2019</xref>). More recently, it has been proposed to reserve the term ‘symbol’ for a collection of such signs that can be syntactically manipulated according to precise compositional rules (<xref ref-type="bibr" rid="bib10">Deacon, 1998</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib79">Nieder, 2009</xref>). The symbols then entertain relationships between each other that are parallel to the relationships between the objects, or concepts, they represent. For example, numerical symbols allow manipulations such as ‘2 + 3 = 5’ irrespective of whether they apply to apples, oranges, or money. Performing the ‘sum’ operation internally allows expectations about a specific outcome in the external world. Non-human animals may be conditioned to acquire iconic or indexical associations (i.e. signs which bear, respectively, a non-arbitrary or arbitrary relationships between the signifier and the signified), and even perhaps perform operations on the learned signs, such as addition (<xref ref-type="bibr" rid="bib61">Livingstone et al., 2014</xref>), but their capacities for novel symbolic composition, especially of a recursive syntactic nature, appear limited, or absent (<xref ref-type="bibr" rid="bib4">Berwick and Chomsky, 2016</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib15">Dehaene et al., 2015</xref>; <xref ref-type="bibr" rid="bib87">Penn et al., 2008</xref>; <xref ref-type="bibr" rid="bib101">Sablé-Meyer et al., 2021</xref>; <xref ref-type="bibr" rid="bib130">Yang, 2013</xref>; <xref ref-type="bibr" rid="bib131">Zhang et al., 2022</xref>).</p><p>The characterisation of the difference between humans and animals in terms of symbolic access remains controversial. Furthermore, comparing human and non-human primates is difficult in part because learning complex tasks require considerable training in animals, and a variety of factors such as motivation, learning rate, or working memory capacity may therefore explain an animal’s failure. Here, we propose to circumvent this difficulty by testing a basic element of symbolic representations, that is, the temporal reversibility of a learned arbitrary association. While the associations between indices and objects (such as those acquired during classical conditioning) are unidirectional, as in the famous example of the bell indicating the food, symbolic associations are bidirectional or symmetric (<xref ref-type="bibr" rid="bib10">Deacon, 1998</xref>; <xref ref-type="bibr" rid="bib79">Nieder, 2009</xref>). When hearing the word ‘dog’ for example, you can think of a dog, but when seeing a dog, you can also come up with the word. Such reversibility is crucial for communication (the language learner must acquire both comprehension and production skills), but also for symbolic computations, which require bidirectional exchanges between the real world (e.g. seeing three sets of four objects), the internal symbols (e.g. 3 × 4, allowing the computation 12), and back (to expect a total quantity of 12 objects). In the current work, we test the ‘reversibility hypothesis’, which proposes that because of a powerful symbolic system, humans are biassed to spontaneously form bidirectional associations between an object and an arbitrary sign. It implies that the referential function of the sign immediately operates in both directions (i.e. comprehension and production), allowing to retrieve the signified (meaning) from the signifier (symbol) and vice versa. Such reversibility is a core and necessary property of symbols, although we readily acknowledge that it is not sufficient, since genuine symbols present additional referential and compositional properties that will not be tested in the present work.</p><p>A small number of behavioural studies, spread over four decades, report that non-human animals such as bees and pigeons, but also macaques, baboons, and chimpanzees, struggle to reverse the associations that they learned in one direction (<xref ref-type="bibr" rid="bib48">Imai et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Kojima, 1984</xref>; <xref ref-type="bibr" rid="bib58">Lipkens et al., 1988</xref>; <xref ref-type="bibr" rid="bib70">Medam et al., 2016</xref>; <xref ref-type="bibr" rid="bib109">Sidman et al., 1982</xref>; <xref ref-type="bibr" rid="bib47">Howard et al., 2019</xref>; see <xref ref-type="bibr" rid="bib8">Chartier and Fagot, 2023</xref>, for a review and discussion). In a recent experiment, <xref ref-type="bibr" rid="bib8">Chartier and Fagot, 2023</xref> explored this question in 20 free-behaving baboons. After having learned to pair visual shapes (two pairs A–B) above 80% success, their performance dropped considerably when the order of presentation was subsequently reversed (B–A; 54% correct, chance = 50%), although their relearning performance was only slightly but significantly better when the reversed pairs were congruent (B1–A1; B2–A2) rather than incongruent (B1–A2; B2–A1). Even for the famous case of chimpanzee <italic>AI</italic>, who learned Arabic numerals and other arbitrary tokens for colours and objects (<xref ref-type="bibr" rid="bib69">Matsuzawa, 2009</xref>; <xref ref-type="bibr" rid="bib68">Matsuzawa, 1985</xref>), it turns out that her capacity to associate signs and their meanings was based on an explicit and sequential training in both directions, at least initially (<xref ref-type="bibr" rid="bib56">Kojima, 1984</xref>). In sharp contrast, humans as young as 8 months, even when tested under the same conditions as monkeys or baboons (<xref ref-type="bibr" rid="bib109">Sidman et al., 1982</xref>), show behavioural evidence of immediate spontaneous reversal of learned associations (<xref ref-type="bibr" rid="bib48">Imai et al., 2021</xref>; <xref ref-type="bibr" rid="bib82">Ogawa et al., 2010</xref>; <xref ref-type="bibr" rid="bib109">Sidman et al., 1982</xref>).</p><p>Still, behavioural tests depend on an explicit report which could hide an implicit understanding of symbolic representations. This confound can be alleviated by directly recording the brain responses, providing a more direct comparison between species. Here, we propose a simple brain-imaging test of reversible associations. First, the participant receives evidence of several stimulus pairings between an object (O) and an arbitrary sign or label (L) in a fixed ‘canonical order’, for example, from O<sub>1</sub> to L<sub>1</sub> and from O<sub>2</sub> to L<sub>2</sub>. Knowledge of these learned (i.e. congruent) associations is then tested using a classic violation-of-expectation paradigm, by evaluating the brain’s surprise response or ‘prediction error’ when, say, O<sub>1</sub> is followed by L<sub>2</sub>. This response can then also be evaluated in the converse direction, by switching the order of presentation of the two items within a pair. The crucial question is whether the brain shows a surprise response to an incongruent pairing presented in reversed order (e.g. L<sub>1</sub> followed by O<sub>2</sub>), relative to the corresponding congruent pairing (L<sub>1</sub> followed by O<sub>1</sub>). The reversibility hypothesis predicts that if symbolic associations are formed, pairs presented in canonical and reversed order should be similarly processed, and so a similar surprise response to incongruent pairings should be found in both cases.</p><p>A recent study from our lab used EEG to apply this approach to 4- to 5-month-old human infants (<xref ref-type="bibr" rid="bib50">Kabdebon and Dehaene-Lambertz, 2019</xref>). The infants were trained with pairs of stimuli in which a specific picture (a lion or a fish) was associated with tri-syllabic none words, depending on a rule concerning syllable-repetition in the word (e.g. xxY words such as <italic>babagu</italic> and <italic>didito</italic> were followed by the fish picture whereas xYx words such as <italic>lotilo</italic> and <italic>fudafu</italic> were followed by the lion picture). Violation-of-expectation responses were recorded in both canonical and reverse order, suggesting that preverbal human infants already have the ability to reversibly attach a symbol to an abstract rule. In human adults, an fMRI study with a more complex design using explicit reports on associations between abstract patterns also showed brain signatures suggestive of spontaneous reversal of learned associations (<xref ref-type="bibr" rid="bib82">Ogawa et al., 2010</xref>). The network of brain areas overlapped with the multiple-demand system that is ubiquitously observed in high-level cognitive tasks (<xref ref-type="bibr" rid="bib19">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2013</xref>), including bilateral inferior and middle frontal gyrus (IFG and MFG), anterior insula (AI), intra-parietal sulcus (IPS), and dorsal anterior cingulate cortex (dACC). In contrast, a human fMRI study investigating association learning between two natural visual objects found that violation effects in the learned direction were restricted to low-level visual areas (<xref ref-type="bibr" rid="bib96">Richter et al., 2018</xref>). Similarly, in macaque monkeys violation effects in the learned direction have been found selectively in visual areas, using fMRI as well as single-neuron recordings (<xref ref-type="bibr" rid="bib52">Kaposvari et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Meyer et al., 2014</xref>; <xref ref-type="bibr" rid="bib72">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib121">Vergnieux and Vogels, 2020</xref>). One of these studies (<xref ref-type="bibr" rid="bib72">Meyer and Olson, 2011</xref>) also tested, in a small subset of 17 neurons, whether the learned associations spontaneously reversed, and showed no such reversal. From these studies, it is difficult to draw a conclusion about a potential difference between species, due to important differences in recording techniques and task design.</p><p>Here, we directly compared the ability to spontaneously reverse learned associations in humans and macaque monkeys using identical training stimuli and whole-brain fMRI measures. Our goals were to (1) probe the reversibility hypothesis in an elementary passive paradigm in both species; (2) shed light on the brain mechanisms of symbolic associations in humans. Indeed, two alternative hypotheses may be formulated. First, given that symbolic learning is a defining feature of language, reversible violation-of-expectation effects might be restricted to the left-hemispheric temporal and inferior frontal language areas. Alternatively, since symbolic learning is manifest in many domains outside of language, for instance in mathematics or music, each attached to a dissociable fronto-posterior brain network (<xref ref-type="bibr" rid="bib1">Amalric and Dehaene, 2016</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Fedorenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib80">Nieder, 2019</xref>; <xref ref-type="bibr" rid="bib81">Norman-Haignere et al., 2015</xref>), reversibility could be expected to arise from a broad and bilateral network of human brain areas, including dorsal intra-parietal and middle frontal nodes. We thus tested audio–visual and visual–visual symbolic pairing in two successive experiments.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Summary of the experimental design</title><p>In the first experiment, we examined the learning and reversibility of auditory–visual pairs, that is between a visual object and an auditory label. Over the course of 3 days, we exposed humans (<italic>n</italic> = 31) and macaque monkeys (<italic>n</italic> = 2) to four pairs of visual objects and speech sounds (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for the five series of four pairs of audio–visual stimuli). Two of the pairs were presented in the auditory-to-visual direction and two in the visual-to-auditory direction, ensuring that all subjects had experience with both orders and would not be surprised by their temporal reversal per se (see discussion of the utility of this point in <xref ref-type="bibr" rid="bib70">Medam et al., 2016</xref>). After 3 consecutive days of exposure to 100% of congruent canonical trials (24 canonical trials in total per pair, presented outside the scanner), subjects were tested for learning using 3T fMRI, during which they were passively exposed to pairs that respected or violated the learned pairings (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To sustain the memory for learned pairs, the design still included 70% of congruent canonical trials (identical to the trials to which they have been exposed outside the scanner). In addition, there were 10% of incongruent canonical trials, in which the temporal order was maintained but the pairings between auditory and visual stimuli were violated. Enhanced brain responses to such incongruent pairs would indicate surprise and therefore prove that the associations had been learned. Note that all auditory and visual stimuli themselves were familiar: only their pairing was unusual. The design also included 10% of reversed congruent and 10% of reversed incongruent trials, in which the habitual (i.e. canonical) order of presentation of the pairs was reversed (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Observing an incongruity effect on such reversed trials would indicate that subjects spontaneously reversed the pairings and were surprised when they were violated. Note that the frequency of the two types of reversed trials was equal, and thus did not afford any additional learning of the reversed pairs (unlike <xref ref-type="bibr" rid="bib8">Chartier and Fagot, 2023</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental paradigm for auditory–visual label learning.</title><p>(<bold>A</bold>) Subjects were exposed to four different visual–auditory pairs during 3 days (six repetitions of each pair, 3-min video). Two pairs were always presented in the ‘visual-then-auditory’ order (object to label), and two in the ‘auditory-then-visual’ (label to object) order. During the test phase, this canonical order was kept on 80% of trials, including 10% of incongruent pairs to test memory of the learned pairs, and was reversed on 20% of the trials. On reversed trials, half the pairs were congruent and half were incongruent (each 10% of total trials), thus testing reversibility of the pairings without affording additional learning. (<bold>B, C</bold>) Activation in sensory cortices. Although each trial comprises auditory and visual stimuli, these could be separated by the temporal offsets. Images show significantly activated regions in the contrasts image &gt; sound (red-yellow) and sound &gt; image (blue-light blue), averaged across all subjects and runs for humans (<bold>B</bold>) and monkeys (<bold>C</bold>). Average finite-impulse-response (FIR) estimate of the deconvolved hemodynamic responses for humans (<bold>D</bold>) and monkeys (<bold>E</bold>) within clusters shown in B and C, respectively, separately for visual–audio (VA) and audio–visual (AV) trials. Sign flipped on <italic>y</italic>-axis for monkey responses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Stimulus sets for experiment 1.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig1-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Experiment 1 | audio–visual stimulus pairs</title><p>We first mapped the cortical regions that were activated by visual and auditory stimuli, modelling the two stimuli within each pair with separate regressors (<xref ref-type="fig" rid="fig1">Figure 1B, C</xref>). Even though the onset of the two stimuli within a pair were just 800 ms apart, the fast acquisition allowed us to separate the timing of the activation of the visual and auditory pathways in both humans and monkeys (<xref ref-type="fig" rid="fig1">Figure 1D, E</xref>). In the visual cortex, the response evoked by the pair arose earlier when the first stimulus of the pair was visual compared to when it was auditory, and the other way around for the auditory cortex (<xref ref-type="fig" rid="fig1">Figure 1D, E</xref>). In the auditory cortex of monkeys the response was relatively weak (<xref ref-type="fig" rid="fig1">Figure 1C, E</xref>), in line with previous studies (<xref ref-type="bibr" rid="bib23">Erb et al., 2019</xref>; <xref ref-type="bibr" rid="bib91">Petkov et al., 2009</xref>; <xref ref-type="bibr" rid="bib118">Uhrig et al., 2014</xref>). This might be related to the small size of auditory cortex relative to visual cortex in monkeys (<xref ref-type="bibr" rid="bib27">Felleman and Van Essen, 1991</xref>), as well as relative to the size of the human auditory cortex (<xref ref-type="bibr" rid="bib128">Woods et al., 2010</xref>).</p><p>We next investigated whether the subjects had learned the associations, whether the brain responses showed signatures of generalisation to the reversed direction, and which brain areas were involved. If participants had learned the associations, incongruent trials should evoke a surprise response relative to congruent trials, when presented in the same order as the training pairs (canonical trials). Crucially, if they spontaneously reversed the associations, a similar incongruity effect should also be seen on reversed trials. According to the reversibility hypothesis, humans should show a spontaneous reversal while monkeys should not. Only for monkeys, we should therefore find a significant interaction effect between incongruity and canonicity, indicating a significant difference between the congruity effect in the learned direction compared to the congruity effect in the reversed direction.</p><p>Indeed, in humans, a vast network was activated by incongruity on both canonical and reversed trials (voxel p &lt; 0.001, cluster p &lt; 0.05 corrected, <italic>n</italic> = 31 participants) (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="table" rid="table1">Table 1</xref>). This network included a set of high-level brain regions previously described as the multiple-demand system (<xref ref-type="bibr" rid="bib19">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2013</xref>), including bilateral IFG, MFG, AI, IPS, and dACC. It also included the language network (<xref ref-type="bibr" rid="bib83">Pallier et al., 2011</xref>), with the left superior temporal sulcus (STS), in addition to the left inferior frontal region already mentioned. However, in our case the activation was bilateral, thereby supporting the model that the language network is part of a larger symbolic network (<xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>). Furthermore, we also found activations in the precuneus, similar to the network that has been found for top–down attention to memorised visual stimuli (<xref ref-type="bibr" rid="bib105">Sestieri et al., 2010</xref>), which also included bilateral STS and IPS. Notably, we did not find any congruity effects in visually activated regions (compare to <xref ref-type="fig" rid="fig1">Figure 1B</xref>), in contrast to a previous human fMRI study (<xref ref-type="bibr" rid="bib96">Richter et al., 2018</xref>). <xref ref-type="fig" rid="fig2">Figure 2B</xref> shows the hemodynamic response within the different clusters and the different conditions. In all analyses, since there were a majority of canonical congruent trials, sensitivity was higher in the canonical direction, and thus the size of the significant clusters was larger on canonical than on reversed trials. However, no significant cluster exhibited any interaction between congruity and canonicity, indicating that there was no statistical difference between the effect of congruity for the trained and the reversed direction. Thus, the human brain fully and spontaneously reverses the auditory–visual associations that it learns.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Congruity effects in the auditory–visual task in humans (experiment 1).</title><p>(<bold>A</bold>) Areas activated by incongruent trials more than by congruent trials in canonical trials (red), reverse trials (blue), and their overlap (green). Brain maps are thresholded at p<sub>voxel</sub> &lt; 0.001 and p<sub>cluster</sub> &lt; 0.05 corrected for multiple comparisons across the brain volume. No interaction effect was observed between congruity and canonicity. (<bold>B</bold>) Average FIR estimate of the deconvolved hemodynamic responses within significant clusters in the left hemisphere, separately for VA and AV trials. Thirty-one human subjects were tested, on a single imaging session per subject after 3 days of exposure to canonical trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig2-v1.tif"/></fig><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Congruity effect in experiment 1 in 31 human subjects, with 1 imaging session per subject after 3 days of exposure to congruent canonical pairs.</title><p>The MNI coordinates indicate the location of the peak of all significant clusters in the main effect of congruity, after correction for multiple comparisons across the whole brain (corrected p<sub>cluster</sub> &lt; 0.05). Additional <italic>t</italic>-values are provided at the same peak location for the canonical and reverse congruity effects. A star is added when the voxels belong to a cluster that achieves corrected-level significance (corrected p<sub>cluster</sub> &lt; 0.05).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="3">Congruity effect (<italic>t</italic>-values)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Region</td><td align="left" valign="bottom">MNI coordinates</td><td align="left" valign="bottom">Main</td><td align="left" valign="bottom">Canonical trials</td><td align="left" valign="bottom">Reversed trials</td></tr><tr><td align="left" valign="bottom">L sup frontal</td><td align="left" valign="bottom">–26 56 24</td><td align="left" valign="bottom">4.40*</td><td align="left" valign="bottom">4.41*</td><td align="left" valign="bottom">2.10*</td></tr><tr><td align="left" valign="bottom">L precentral</td><td align="left" valign="bottom">–36 6 32</td><td align="left" valign="bottom">5.75*</td><td align="left" valign="bottom">3.57*</td><td align="left" valign="bottom">7.50*</td></tr><tr><td align="left" valign="bottom">L triangularis</td><td align="left" valign="bottom">–48 16 2</td><td align="left" valign="bottom">7.65*</td><td align="left" valign="bottom">5.45*</td><td align="left" valign="bottom">6.08*</td></tr><tr><td align="left" valign="bottom">L insula</td><td align="left" valign="bottom">–40 22 0</td><td align="left" valign="bottom">7.76*</td><td align="left" valign="bottom">5.84*</td><td align="left" valign="bottom">6.27*</td></tr><tr><td align="left" valign="bottom">L temporal pole</td><td align="left" valign="bottom">–60 2 –10</td><td align="left" valign="bottom">6.56*</td><td align="left" valign="bottom">3.95</td><td align="left" valign="bottom">5.71*</td></tr><tr><td align="left" valign="bottom">L ant STS</td><td align="left" valign="bottom">–62 –24 0</td><td align="left" valign="bottom">5.71*</td><td align="left" valign="bottom">4.28*</td><td align="left" valign="bottom">4.09*</td></tr><tr><td align="left" valign="bottom">L post STS</td><td align="left" valign="bottom">–54 –34 4</td><td align="left" valign="bottom">4.82*</td><td align="left" valign="bottom">2.78*</td><td align="left" valign="bottom">5.09*</td></tr><tr><td align="left" valign="bottom">L precuneus</td><td align="left" valign="bottom">–6 –68 40</td><td align="left" valign="bottom">4.68*</td><td align="left" valign="bottom">4.72*</td><td align="left" valign="bottom">3.39</td></tr><tr><td align="left" valign="bottom">L inf parietal</td><td align="left" valign="bottom">–28 –58 42</td><td align="left" valign="bottom">5.85*</td><td align="left" valign="bottom">3.97*</td><td align="left" valign="bottom">4.56*</td></tr><tr><td align="left" valign="bottom">L caudate</td><td align="left" valign="bottom">–10 2 14</td><td align="left" valign="bottom">5.22*</td><td align="left" valign="bottom">5.15*</td><td align="left" valign="bottom">3.03*</td></tr><tr><td align="left" valign="bottom">L cerebellum</td><td align="left" valign="bottom">–6 –82 –34</td><td align="left" valign="bottom">5.59*</td><td align="left" valign="bottom">3.98</td><td align="left" valign="bottom">3.27</td></tr><tr><td align="left" valign="bottom">R mid frontal</td><td align="left" valign="bottom">54 26 32</td><td align="left" valign="bottom">7.79*</td><td align="left" valign="bottom">5.34*</td><td align="left" valign="bottom">5.86*</td></tr><tr><td align="left" valign="bottom">R opercularis</td><td align="left" valign="bottom">50 20 32</td><td align="left" valign="bottom">7.32*</td><td align="left" valign="bottom">5.44*</td><td align="left" valign="bottom">6.74*</td></tr><tr><td align="left" valign="bottom">R insula</td><td align="left" valign="bottom">40 22 0</td><td align="left" valign="bottom">5.83*</td><td align="left" valign="bottom">4.93*</td><td align="left" valign="bottom">5.11*</td></tr><tr><td align="left" valign="bottom">R temporal pole</td><td align="left" valign="bottom">60 4 –14</td><td align="left" valign="bottom">6.89*</td><td align="left" valign="bottom">5.52*</td><td align="left" valign="bottom">4.49*</td></tr><tr><td align="left" valign="bottom">R post STS</td><td align="left" valign="bottom">48 –32 0</td><td align="left" valign="bottom">7.48*</td><td align="left" valign="bottom">5.96*</td><td align="left" valign="bottom">5.47*</td></tr><tr><td align="left" valign="bottom">R precuneus</td><td align="left" valign="bottom">4 –62 40</td><td align="left" valign="bottom">6.36*</td><td align="left" valign="bottom">5.16*</td><td align="left" valign="bottom">2.88</td></tr><tr><td align="left" valign="bottom">R inf parietal</td><td align="left" valign="bottom">34 –64 44</td><td align="left" valign="bottom">5.14*</td><td align="left" valign="bottom">3.57*</td><td align="left" valign="bottom">4.49*</td></tr><tr><td align="left" valign="bottom">R caudate</td><td align="left" valign="bottom">10 2 14</td><td align="left" valign="bottom">4.21*</td><td align="left" valign="bottom">4.35*</td><td align="left" valign="bottom">2.67*</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table><table-wrap-foot><fn><p>R: right; L: left; STS: superior temporal sulcus.</p></fn></table-wrap-foot></table-wrap><p>We next asked whether monkeys (<italic>n</italic> = 2) learned the associations and did so in both directions. We used five stimulus sets comprising four pairs in each set to train and test monkeys (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The canonical congruity effect, which indexes learning, was not significant when analysing the first imaging sessions (<italic>n</italic> = 5) after the first 3 days of exposure to the canonical pairs. As we had anticipated this based on previous work (<xref ref-type="bibr" rid="bib72">Meyer and Olson, 2011</xref>), monkeys were further exposed during 2 weeks for three of the five stimulus sets (with in total ~960 canonical trials per pair) and tested during 4 consecutive days. After this extended exposure, we found consistent effects in both monkeys (averaged over the 12 scan sessions, 4 per stimulus set per monkey), with clusters in early visual areas (V1, V2, and V4), and auditory association areas in the left temporo-parieto-occipital cortex (AV and VA trials combined, p &lt; 0.001, cluster p &lt; 0.05, <italic>n</italic> = 2) (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="table" rid="table2">Table 2</xref>). Crucially, however, this effect was confined to the canonical direction, with no significant clusters in the reversed direction at the whole-brain level, in accordance with the reversibility hypothesis. We specifically tested the difference between the congruity effect in the learned and the reversed direction by calculating the interaction effect between congruity and canonicity, which showed an activation pattern that was similar to the canonical congruity effect, which reached significance in areas V2 and V4. <xref ref-type="fig" rid="fig3">Figure 3C</xref> shows the corresponding hemodynamic signals, with an enhanced response to incongruent pairs in the canonical direction (continuous red curve) but not in the reversed direction (dashed red curve). The results thus indicated that monkey cortex could acquire audio–visual pairings, as also shown by prior visual–visual experiments (<xref ref-type="bibr" rid="bib72">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib121">Vergnieux and Vogels, 2020</xref>), but with two major differences with humans: the congruity effects did not involve a broad network of high-level cortical areas but remained restricted to early sensory areas, and the learned associations did not reverse.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Congruity effects in the auditory–visual task in monkeys (experiment 1).</title><p>(<bold>A</bold>) Significant clusters from the incongruent–congruent canonical contrast. No significant clusters were found for the reversed direction. (<bold>B</bold>) Significant clusters from the interaction between congruity and canonicity (p<sub>voxel</sub> &lt; 0.001 and p<sub>cluster</sub> &lt; 0.05 for both maps). (<bold>C, D</bold>) Average FIR estimate of the deconvolved MION responses within the clusters from the incongruent–congruent canonical contrast, averaged over VA and AV trials. All clusters in early visual areas were taken together to create figure <bold>C</bold>. The two monkeys were scanned after two additional weeks of exposure (4 imaging sessions per subject per stimulus set, three stimulus sets were used).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig3-v1.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Congruity effect in experiment 1 in two monkeys after two additional weeks of exposure to congruent canonical pairs.</title><p>Per subject, 3 stimulus sets were used, with 4 imaging sessions per stimulus set. The MNI coordinates indicate the location of the peak of all significant clusters for the canonical congruity contrast as well as the interaction between congruity and canonicity, after correction for multiple comparisons across the whole brain (corrected p<sub>cluster</sub> &lt; 0.05). Other columns provide the other contrasts at the same peak location for reference. A star is added when the voxels belong to a cluster that achieves corrected-level significance (corrected p<sub>cluster</sub> &lt; 0.05).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Region</th><th align="left" valign="bottom">MNI coordinates</th><th align="left" valign="bottom">Congruity canonical</th><th align="left" valign="bottom">Congruity reversed</th><th align="left" valign="bottom">Congruity × canonicity</th></tr></thead><tbody><tr><td align="left" valign="bottom">R V2, V4</td><td align="left" valign="bottom">17 –29 4</td><td align="left" valign="bottom">5.04<xref ref-type="table-fn" rid="table2fn3">*</xref></td><td align="left" valign="bottom">–2.24</td><td align="left" valign="bottom">4.56<xref ref-type="table-fn" rid="table2fn3">*</xref></td></tr><tr><td align="left" valign="bottom">L V2</td><td align="left" valign="bottom">–18 –30 2</td><td align="left" valign="bottom">4.6<xref ref-type="table-fn" rid="table2fn3">*</xref></td><td align="left" valign="bottom">–0.09</td><td align="left" valign="bottom">3.08</td></tr><tr><td align="left" valign="bottom">R V4</td><td align="left" valign="bottom">21 –22 0</td><td align="left" valign="bottom">4.23<xref ref-type="table-fn" rid="table2fn3">*</xref></td><td align="left" valign="bottom">–0.74</td><td align="left" valign="bottom">2.95</td></tr><tr><td align="left" valign="bottom">L TPO</td><td align="left" valign="bottom">–20 –21 11</td><td align="left" valign="bottom">4.13<xref ref-type="table-fn" rid="table2fn3">*</xref></td><td align="left" valign="bottom">0.45</td><td align="left" valign="bottom">2.26</td></tr><tr><td align="left" valign="bottom">L LGN</td><td align="left" valign="bottom">–8 –8 –5</td><td align="left" valign="bottom">0.46</td><td align="left" valign="bottom">–4.27</td><td align="left" valign="bottom">3.98</td></tr></tbody></table><table-wrap-foot><fn><p>R: right; L: left; TPO: temporo-parieto-occipital cortex; LGN: lateral geniculate nucleus.</p></fn><fn><p>For completeness, <italic>t</italic>-values are also given for non-significant clusters.</p></fn><fn id="table2fn3"><label>*</label><p>p<sub>cluster</sub> &lt; 0.05.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2-3"><title>Experiment 2 | visual–visual stimulus pairs</title><p>The non-reversal in monkeys in the above audio–visual experiment could be due to a number of methodological choices. First, although the visual stimuli were optimised for monkeys, as three out of five sets of stimuli were pictures of familiar toys, the auditory stimuli (pseudowords) might have been suboptimal for them (although note that monkeys in our lab have extensive experience with human speech). It might be argued that this choice made their discrimination difficult (although note that the canonical congruity effect is evidence of discrimination). Indeed, the auditory cortex is relatively small in monkeys compared to humans (<xref ref-type="bibr" rid="bib128">Woods et al., 2010</xref>), and there is evidence that auditory memory capacity is reduced in monkeys compared to humans (<xref ref-type="bibr" rid="bib104">Scott and Mishkin, 2016</xref>). Second, the instructions differed: while we asked human subjects to fixate a dot at the centre of the screen and to pay attention to the stimuli, monkeys were simply rewarded for fixation.</p><p>To address those concerns, we replicated the experiment with reward-dependent visual–visual associations in three macaque monkeys, one of which participated in both experiments (<xref ref-type="fig" rid="fig4">Figure 4</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). First, we replaced the spoken auditory stimuli with abstract black-and-white shapes similar to the lexigrams used to train chimpanzees to communicate with humans (<xref ref-type="bibr" rid="bib68">Matsuzawa, 1985</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Second, to enhance attention for the monkeys, we introduced a reward association paradigm that made the stimuli behaviourally relevant for them (<xref ref-type="bibr" rid="bib125">Wikman et al., 2019</xref>). Within each presentation direction, one of the two pictures of objects was associated with a high reward, and one with a low reward (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). Monkeys were still rewarded for fixation, but object identity predicted the size of the reward during the delay period following the presentation of the stimuli (two objects predicted a high reward, and two predicted a low reward). To calculate congruity effects, the two pairs within each direction were always averaged, making the reward association an orthogonal element in the design.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Visual–visual label learning in humans and monkeys (experiment 2).</title><p>(<bold>A</bold>) Experiment paradigm. Subjects were habituated to four different visual–visual pairs during 3 days. Two pairs were in the ‘object-then-label’ order and two pairs in the ‘label-then-object’ order. For the monkeys, one object in each direction was associated with a high reward while the other one was associated with a low reward, making reward size orthogonal to congruity and canonicity (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for details). (<bold>B</bold>) Monkey fMRI results. Significant clusters (p<sub>voxel</sub> &lt; 0.001 and cluster volume &gt;50) from the incongruent–congruent canonical contrast (left) and the interaction between congruity and canonicity (right). One imaging session per subject per stimulus set was performed after 3 days of exposure to canonical trials in each of the three monkeys, with 5 stimulus sets per subject. (<bold>C</bold>) Human fMRI results. Areas more activated by incongruent trials than by congruent trials in the canonical (red), and the reversed direction (blue), and their overlap (green) (right) (p<sub>voxel</sub> &lt; 0.005 and cluster volume &gt;50). No red voxels are visible because all of them figure in the overlap (green). One imaging session was performed per subject in 23 participants after 3 days of exposure to a short block of 24 canonical trials. (<bold>D</bold>) Human behavioural results. After learning, human adults rated the familiarity of different types of pairs (including a fifth category of novel, never-seen pairings). Each dot represents the mean response of a subject in each condition. Although the reversed congruent trials constituted only 10% of the trials, they were considered almost as familiar as the canonical congruent pairs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Complete description of the task paradigm for visual-visual label learning.</title><p>(<bold>A</bold>) Subjects were habituated to four different visual–visual pairs during 3 days. Two pairs were in the ‘object–label’ order and two pairs in the ‘label–object’ order. During the test phase, the same canonical order was kept in 80% of the trials, including 10% of incongruent pairs. In reversed trials (20% of trials), the pairs were either congruent (10%) or incongruent (10%) with the learning. For the monkeys, one pair in each direction was associated with a high reward while the other one was associated with a low reward, making the reward size orthogonal to congruity and canonicity. (<bold>B</bold>) Stimulus sets for experiment 2 in monkeys. Humans were tested with stimulus set 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Effect of reward for the visual–visual task in non-human primates.</title><p>(<bold>A</bold>) Significant clusters from the incongruent–congruent canonical contrast in low reward trials. (<bold>B</bold>) Significant from the incongruent–congruent canonical contrast in high reward trials. (<bold>C</bold>) Significant clusters from the interaction between congruity and reward. p<sub>voxel</sub> &lt; 0.001 and p<sub>cluster</sub> &lt; 0.05 in all panels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Analyses of all human participants in experiments 1 and 2 merged.</title><p>(<bold>A</bold>) Main effect of experiment. (<bold>B</bold>) Main effect of congruity. (<bold>C</bold>) Effect of congruity in the canonical trials and (<bold>D</bold>) in the reversed trials. (<bold>E</bold>) No significant cluster was observed for the interaction canonicity × congruity. (<bold>F</bold>) Slices in the three planes showing the only significant cluster in the experiment × congruity interaction. p<sub>voxel</sub> &lt; 0.001 and p<sub>cluster</sub> &lt; 0.05 in all panels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig4-figsupp3-v1.tif"/></fig></fig-group><p>Using this design, we obtained significant canonical congruity effects in monkeys on the first imaging day after the initial training (24 trials per pair), indicating that the animals had learned the associations (<xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="table" rid="table3">Table 3</xref>). The effect was again found in visual areas (V1, V2, and V4), also spreading to the prefrontal cortex (45B and 46v), very similar to the visually activated areas (compare to <xref ref-type="fig" rid="fig1">Figure 1C</xref>). In addition, small clusters were also found in area 6 and in STS. Crucially, the congruity effect remained restricted to the learned direction, as no area showed a significant reversed congruity effect, again in accordance with the reversibility hypothesis. The interaction between congruity and canonicity indicated that there was a significant difference between the canonical and the reversed direction in a similar set of regions (V1, V2, area 45A, 46v, and 6).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Congruity effect in experiment 2 in three monkeys after 3 days of exposure to congruent canonical pairs.</title><p>Per subject, five stimulus sets were used, with 1 imaging session per stimulus set. The MNI coordinates indicate the location of the peak of all significant clusters for the canonical congruity contrast as well as the interaction between congruity and canonicity, after correction for multiple comparisons across the whole brain (corrected p<sub>cluster</sub> &lt; 0.05). Other columns provide the other contrasts at the same peak location for reference. A star is added when the voxels belong to a cluster that achieves corrected-level significance (corrected p<sub>cluster</sub> &lt; 0.05).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Region</th><th align="left" valign="bottom">MNI coordinates</th><th align="left" valign="bottom">Congruity canonical</th><th align="left" valign="bottom">Congruity reversed</th><th align="left" valign="bottom">Congruity × canonicity</th></tr></thead><tbody><tr><td align="left" valign="bottom">L V1, V2</td><td align="left" valign="bottom">–17 –36 1</td><td align="left" valign="bottom">5.18<xref ref-type="table-fn" rid="table3fn3">*</xref></td><td align="left" valign="bottom">–2.64</td><td align="left" valign="bottom">4.82<xref ref-type="table-fn" rid="table3fn3">*</xref></td></tr><tr><td align="left" valign="bottom">R V1, V2</td><td align="left" valign="bottom">15 –35 7</td><td align="left" valign="bottom">4.76<xref ref-type="table-fn" rid="table3fn3">*</xref></td><td align="left" valign="bottom">0.98</td><td align="left" valign="bottom">1.31</td></tr><tr><td align="left" valign="bottom">L V4</td><td align="left" valign="bottom">–23 –23 8</td><td align="left" valign="bottom">3.92<xref ref-type="table-fn" rid="table3fn3">*</xref></td><td align="left" valign="bottom">0.94</td><td align="left" valign="bottom">1.61</td></tr><tr><td align="left" valign="bottom">L area 45A, 46v</td><td align="left" valign="bottom">–17 14 6</td><td align="left" valign="bottom">3.89<xref ref-type="table-fn" rid="table3fn3">*</xref></td><td align="left" valign="bottom">–2.2</td><td align="left" valign="bottom">4.00<xref ref-type="table-fn" rid="table3fn3">*</xref></td></tr><tr><td align="left" valign="bottom">R area 6/STS 6</td><td align="left" valign="bottom">22 6 –3</td><td align="left" valign="bottom">3.65<xref ref-type="table-fn" rid="table3fn3">*</xref></td><td align="left" valign="bottom">–1.26</td><td align="left" valign="bottom">3.37<xref ref-type="table-fn" rid="table3fn3">*</xref></td></tr><tr><td align="left" valign="bottom">L TPO</td><td align="left" valign="bottom">–8 –17 13</td><td align="left" valign="bottom">3.45<xref ref-type="table-fn" rid="table3fn3">*</xref></td><td align="left" valign="bottom">0.03</td><td align="left" valign="bottom">2.04</td></tr></tbody></table><table-wrap-foot><fn><p>MNI coordinates and <italic>t</italic>-values of each significant cluster at the peak voxel. R: right; L: left; STS: superior temporal sulcus; TPO: temporo-parieto-occipital cortex.</p></fn><fn><p>For completeness, <italic>t</italic>-values are also given for non-significant clusters.</p></fn><fn id="table3fn3"><label>*</label><p>p<sub>cluster</sub> &lt; 0.05.</p></fn></table-wrap-foot></table-wrap><p>The greater involvement of the frontal cortex in the congruity effect in this paradigm fits with previous reports on the impact of reward association on long-term memory for visual stimuli in macaque monkeys (<xref ref-type="bibr" rid="bib37">Ghazizadeh et al., 2018</xref>). To further investigate this, we split high versus low rewarded pairs and found that congruity effect was present only for high-reward conditions, with a significant interaction of congruity and reward in area 45 and caudate nucleus (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Overall, these results indicate that, even when stimuli were optimised and made relevant for monkeys, leading to enhanced activations and an activation of prefrontal cortex to violations of expectations, the learned associations did not reverse in monkeys.</p><p>We also ran this visual–visual paradigm in human participants (<italic>n</italic> = 24) with the goal to clarify the role of language in the reversibility process. Humans again gave evidence of reversed association, although weaker than with spoken words (<xref ref-type="fig" rid="fig4">Figure 4C</xref> and <xref ref-type="table" rid="table4">Table 4</xref>). At the normal threshold (voxel p &lt; 0.001, cluster p &lt; 0.05 corrected), the main effect of congruity was significant in a network very similar to experiment 1, including bilateral MFG, left IPS, bilateral AI, dACC, with an additional focus in left inferior temporal (IT) gyrus (<xref ref-type="fig" rid="fig4">Figure 4C</xref> and <xref ref-type="table" rid="table4">Table 4</xref>). The involvement of the language network was limited. In particular a main effect of congruity in the STS was absent, in agreement with the shift to visual symbols. Still, bilateral middle frontal gyri, STS, and the precuneus were again activated by the incongruent minus congruent contrast on reversed trials (voxel p &lt; 0.001, cluster p &lt; 0.05 corrected), thereby extending beyond the multiple-demand system (<xref ref-type="bibr" rid="bib19">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2013</xref>). While sensory activated regions were again absent, in contrast to a previous study on congruity effects in humans when using associations between two visual objects (<xref ref-type="bibr" rid="bib96">Richter et al., 2018</xref>). And crucially, no interaction effect was again found between congruity and canonicity, neither at the classical threshold (p &lt; 0.001) nor at a lower threshold (p &lt; 0.01). Those results indicate that humans can also encode pairs of visual stimuli in a symmetrical, reversible fashion, involving a network of high-level cortical areas, unlike monkeys.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Congruity effect in experiment 2 in 23 human subjects, with 1 imaging session per subject after 3 days of exposure to congruent canonical pairs.</title><p>The MNI coordinates indicate the location of the peak of all significant clusters in the main effect of congruity, after correction for multiple comparisons across the whole brain (corrected p<sub>cluster</sub> &lt; 0.05). Additional <italic>t</italic>-values are provided at the same peak location for the canonical and reverse congruity effects. A star (*) is added when the voxels belong to a cluster that achieves corrected-level significance (corrected p<sub>cluster</sub> &lt; 0.05).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="3">Congruity effect (<italic>t</italic>-values)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Region</td><td align="left" valign="bottom">MNI coordinates</td><td align="left" valign="bottom">Main</td><td align="left" valign="bottom">Canonical trials</td><td align="left" valign="bottom">Reversed trials</td></tr><tr><td align="left" valign="bottom">L triangularis</td><td align="left" valign="bottom">–44 30 24</td><td align="left" valign="bottom">5.34*</td><td align="left" valign="bottom">3.64*</td><td align="left" valign="bottom">3.91*</td></tr><tr><td align="left" valign="bottom">L operculum</td><td align="left" valign="bottom">–34 26 0</td><td align="left" valign="bottom">4.43*</td><td align="left" valign="bottom">4.36*</td><td align="left" valign="bottom">1.91</td></tr><tr><td align="left" valign="bottom">L ant cingulaire</td><td align="left" valign="bottom">–8 18 42</td><td align="left" valign="bottom">4.52*</td><td align="left" valign="bottom">3.25*</td><td align="left" valign="bottom">3.13</td></tr><tr><td align="left" valign="bottom">L suppl motor area</td><td align="left" valign="bottom">2 20 52</td><td align="left" valign="bottom">3.79*</td><td align="left" valign="bottom">3.95*</td><td align="left" valign="bottom">1.40</td></tr><tr><td align="left" valign="bottom">L precentral</td><td align="left" valign="bottom">–48 4 40</td><td align="left" valign="bottom">4.82*</td><td align="left" valign="bottom">2.56</td><td align="left" valign="bottom">4.26*</td></tr><tr><td align="left" valign="bottom">L inf parietal</td><td align="left" valign="bottom">–30 –50 44</td><td align="left" valign="bottom">5.09*</td><td align="left" valign="bottom">3.90*</td><td align="left" valign="bottom">3.30*</td></tr><tr><td align="left" valign="bottom">L mid occipital</td><td align="left" valign="bottom">–28 –70 32</td><td align="left" valign="bottom">5.05*</td><td align="left" valign="bottom">2.89</td><td align="left" valign="bottom">2.79</td></tr><tr><td align="left" valign="bottom">L visual word form area</td><td align="left" valign="bottom">–50 –60 –12</td><td align="left" valign="bottom">4.43*</td><td align="left" valign="bottom">2.62</td><td align="left" valign="bottom">3.64</td></tr><tr><td align="left" valign="bottom">R sup frontal</td><td align="left" valign="bottom">56 24 36</td><td align="left" valign="bottom">4.93*</td><td align="left" valign="bottom">3.41*</td><td align="left" valign="bottom">3.57*</td></tr><tr><td align="left" valign="bottom">R orbito frontal</td><td align="left" valign="bottom">26 26 –16</td><td align="left" valign="bottom">5.05*</td><td align="left" valign="bottom">1.92*</td><td align="left" valign="bottom">5.22</td></tr><tr><td align="left" valign="bottom" rowspan="2">R operculum</td><td align="left" valign="bottom">50 16 –2</td><td align="left" valign="bottom">3.58*</td><td align="left" valign="bottom">2.96*</td><td align="left" valign="bottom">2.11</td></tr><tr><td align="left" valign="bottom">48 10 28</td><td align="left" valign="bottom">4.74*</td><td align="left" valign="bottom">2.20*</td><td align="left" valign="bottom">4.39*</td></tr></tbody></table><table-wrap-foot><fn><p>R: right; L: left; VWFA: visual word form area.</p></fn></table-wrap-foot></table-wrap><p>Further evidence was obtained from a behavioural test, performed after imaging, where we collected familiarity ratings for each stimulus pair (see Methods, <xref ref-type="fig" rid="fig4">Figure 4</xref>). Although participants reported a higher familiarity with congruent canonical pairs (which were presented on 70% of trials) than with congruent reversed pairs (which were presented on 10% of trials, <italic>t</italic>(20) = 2.8, p = 0.01), both pairs were rated as much more familiar than their corresponding incongruent pairs (although they were also presented 10% of time), and than never-seen pairs (all <italic>t</italic>(20) &gt; 7, p &lt; 0.0001, bilateral paired <italic>t</italic>-test). This familiarity task thus confirms that humans spontaneously reverse associations and experience a memory illusion of seeing the reversed pairs.</p></sec><sec id="s2-4"><title>Joint analysis of audio–visual and visual–visual stimulus pairs</title><p>In order to better characterise the human reversible symbol learning network and its dependence on modality, we reanalysed both human experiments together (<italic>n</italic> = 55) (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). There was, unsurprisingly, a main effect of experiment with greater activation in a bilateral auditory and linguistic network in the AV experiment, and in the occipital, occipito-temporal, and occipito-parietal visual pathways in the VV experiment. A main effect of congruity was observed and was again significant in both directions, canonical and reversed, in bilateral regions: insula, MFG, precentral, IPS, precuneus, ACC, and STS. Crucially, there was still no region sensitive to the congruity × canonicity interaction, indicating that the learned associations were fully reversible. Finally, a single region, the left posterior STS, showed a significantly different congruity effect in the two experiments, as it was slightly larger in the AV relative to VV paradigm ([−60 –40 8], <italic>z</italic> = 4.51; 183 vox, pcor = 0.049), compatible with a specific role in learning of new spoken lexical items. The results therefore suggest that a broad and bilateral network, encompassing language areas but extending beyond them into dorsal parietal and prefrontal cortices, responded to violations of reversible symbolic association regardless of modality.</p><p>To interrogate more finely the role of language- and non-related areas, we turned to a sensitive subject-specific region-of-interest (ROI) analysis. We used a separate set of data acquired during a ‘localiser’ task during the same fMRI session (<xref ref-type="bibr" rid="bib94">Pinel et al., 2007</xref>) to recover, in a subject-specific manner, the coordinates of the 10% best voxels within ROIs which are considered as the main hubs of language (<xref ref-type="bibr" rid="bib83">Pallier et al., 2011</xref>), mathematics (<xref ref-type="bibr" rid="bib1">Amalric and Dehaene, 2016</xref>) and reading networks. Specifically in the conditions of that localiser, we considered activations to amodal sentence processing for the language ROIs, to simple mental arithmetic for the mathematical ROIs, and in sentence reading relative to listening for the visual word form area (VWFA). We added this last region as it is activated by written words, visual symbols <italic>par excellence</italic>. We then performed ANOVAs on the betas of the main experiment averaged over these voxels, thus the analyses were performed on a different dataset than the localiser data used to select the voxels.</p><p>A main congruity effect was observed in all ROIs (<xref ref-type="table" rid="table5">Table 5</xref>). There was also a main effect of experiment in all language ROIs, VWFA and right IT, due on the one hand to larger activations in the AV than VV experiment in frontal and superior temporal ROIs, and on the other hand to the converse trend in the VWFA and IT ROIs. A significant congruity × experiment interaction was seen only in the pSTS and IFG triangularis, because these ROIs showed a large congruity effect in the AV experiment, but no effect in the VV experiment – thus further confirming that these areas contribute specifically to the acquisition of linguistic symbols, while all other areas were engaged regardless of modality. Importantly, in all these analyses, no significant interaction canonicity × congruity nor experiment × canonicity × congruity were observed, confirming the whole-brain analyses (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> and <xref ref-type="table" rid="table5">Table 5</xref>).</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Region-of-interest (ROI) analyses of the language and mathematics localiser: <italic>F</italic>-values of ANOVAs performed on the averaged betas of the main task across different ROIs (main effect of congruity, canonicity, experiment (1 or 2), and interaction effect of congruity and canonicity, and congruity and experiment).</title><p>These ROIs correspond to the 10% best voxels selected in each participant thanks to an independent and short localiser, in regions commonly reported in the literature as activated in language and mathematical tasks. In this localiser, participants listened to and read short sentences of general content or requiring easy mental calculations. On the sagittal (<italic>x</italic> = −50 mm) and coronal (<italic>y</italic> = −58 mm) brain slices, the language and mathematical ROIs are presented as red and yellow areas, respectively. The left-lateralised white area corresponds to the visual word form area (VWFA); <italic>n</italic> = 52; df = 50.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><inline-graphic mimetype="image" mime-subtype="tif" xlink:href="elife-87380-inf001-v1.tif"/></th><th align="left" valign="bottom"><inline-graphic mimetype="image" mime-subtype="tif" xlink:href="elife-87380-inf002-v1.tif"/></th><th align="left" valign="bottom">Congruity</th><th align="left" valign="bottom">Canonicity</th><th align="left" valign="bottom">Experiment</th><th align="left" valign="bottom">Congruity × canonicity</th><th align="left" valign="bottom">Congruity × experiment</th></tr></thead><tbody><tr><td align="left" valign="bottom">ROIs language</td><td align="left" valign="bottom">Temporal pole</td><td align="char" char="." valign="bottom">11.44<xref ref-type="table-fn" rid="table5fn3"><sup>†</sup></xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">6.02<xref ref-type="table-fn" rid="table5fn4"><sup>‡</sup></xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Anterior STS</td><td align="char" char="." valign="bottom">5.41<xref ref-type="table-fn" rid="table5fn4"><sup>‡</sup></xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">42.31<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Posterior STS</td><td align="char" char="." valign="bottom">18.70<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">1.31</td><td align="char" char="." valign="bottom">50.75<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">17.01<xref ref-type="table-fn" rid="table5fn3"><sup>†</sup></xref></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Temporo-parietal junction</td><td align="char" char="." valign="bottom">20.81<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">1.85</td><td align="char" char="." valign="bottom">9.39<xref ref-type="table-fn" rid="table5fn3"><sup>†</sup></xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">IFG orbitalis</td><td align="char" char="." valign="bottom">22.47<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">11.40<xref ref-type="table-fn" rid="table5fn3"><sup>†</sup></xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">1.64</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">IFG triangularis</td><td align="char" char="." valign="bottom">16.98<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">22.42<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">10.45<xref ref-type="table-fn" rid="table5fn4"><sup>‡</sup></xref></td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">VWFA</td><td align="char" char="." valign="bottom">22.29<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">11.77<xref ref-type="table-fn" rid="table5fn3"><sup>†</sup></xref></td><td align="char" char="." valign="bottom">&lt;1</td><td align="char" char="." valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom" rowspan="8">ROIs math</td><td align="left" valign="bottom">Left precentral BA44d</td><td align="left" valign="bottom">29.71<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">4.1<xref ref-type="table-fn" rid="table5fn5"><sup>§</sup></xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom">Right precentral BA44d</td><td align="left" valign="bottom">10.44<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="left" valign="bottom">1.23</td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">1.49</td><td align="left" valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom">Left IPS</td><td align="left" valign="bottom">27.4<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">1.81</td><td align="left" valign="bottom">1.77</td><td align="left" valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom">Right IPS</td><td align="left" valign="bottom">18.19<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="left" valign="bottom">6.77</td><td align="left" valign="bottom">1.70</td><td align="left" valign="bottom">2.37</td><td align="left" valign="bottom">5.29</td></tr><tr><td align="left" valign="bottom">Left IT</td><td align="left" valign="bottom">33.43<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">4.43<xref ref-type="table-fn" rid="table5fn5"><sup>§</sup></xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom">Right IT</td><td align="left" valign="bottom">5.41<xref ref-type="table-fn" rid="table5fn4"><sup>‡</sup></xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">7.76<xref ref-type="table-fn" rid="table5fn4"><sup>‡</sup></xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td></tr><tr><td align="left" valign="bottom">Left cerebellum</td><td align="left" valign="bottom">5.51<xref ref-type="table-fn" rid="table5fn4"><sup>‡</sup></xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">2.87</td></tr><tr><td align="left" valign="bottom">Right cerebellum</td><td align="left" valign="bottom">19.20<xref ref-type="table-fn" rid="table5fn2">*</xref></td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td><td align="left" valign="bottom">&lt;1</td></tr></tbody></table><table-wrap-foot><fn><p>STS: superior temporal sulcus; IFG: inferior frontal gyrus; IPS: intra-parietal sulcus; IT: inferior temporal.</p></fn><fn id="table5fn2"><label>*</label><p>p<sub>FDRcor</sub> &lt; 0.001.</p></fn><fn id="table5fn3"><label>†</label><p>p<sub>FDRcor</sub> &lt; 0.01.</p></fn><fn id="table5fn4"><label>‡</label><p>p<sub>FDRcor</sub> &lt; 0.05.</p></fn><fn id="table5fn5"><label>§</label><p>p<sub>FDRcor</sub> &lt; 0.1.</p></fn></table-wrap-foot></table-wrap><p>Finally, in experiment 2 in which participants rated the familiarity of the pairs, we computed a within-subject behavioural index of reversibility as the difference in familiarity rating between incongruent and congruent reversed pairs. Across subjects, this index was correlated with the fMRI congruity effect (difference between incongruent and congruent trials in the ROI) on canonical trials (<italic>r</italic> = 0.49, p = 0.028) and especially on reversed trials (<italic>r</italic> = 0.64, p = 0.002) in the left dorsal part of area 44. In the right cerebellum, a similar correlation was observed but only for the reversed trials (<italic>r</italic> = 0.57, p = 0.008). No significant correlation was observed in other ROIs.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using fMRI in human and non-human primates, we studied the learning of a sequential association between either a spoken label and an object (Exp. 1), or a visual label and an object (Exp. 2). In humans, we observed no difference in brain activation between the learned and the temporally reversed associations: in both directions, violations of the learned association activated a large set of bilateral regions (insula, prefrontal, intra-parietal, and cingulate cortex) that extended beyond the language processing network. Thus, humans generalised the learned pairings across a reversal of temporal order (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In contrast, non-human primates showed evidence of remembering the pairs only in the learned direction and did not show any signature of spontaneous reversal. Crucially, we found a significant interaction between congruity and the direction of the learned association, thereby going beyond a mere negative finding. Monkey responses to incongruent pairings were entirely confined to the learned canonical order and occurred primarily within sensory areas, with propagation to the frontal cortex only for rewarded stimuli, yet still only in the forward direction (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Summary of the two experiments in humans and monkeys.</title><p>(In experiment 1, p<sub>voxel</sub> &lt; 0.001 and p<sub>cluster</sub> &lt; 0.05 for humans and monkeys. In experiment 2, p<sub>voxel</sub> &lt; 0.005 and cluster volume &gt;50 in humans and p<sub>voxel</sub> &lt; 0.001 and cluster volume &gt;50 in monkeys.).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-fig5-v1.tif"/></fig><p>Several studies previously found behavioural evidence for a uniquely human ability to spontaneously reverse a learned association (<xref ref-type="bibr" rid="bib48">Imai et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Kojima, 1984</xref>; <xref ref-type="bibr" rid="bib58">Lipkens et al., 1988</xref>; <xref ref-type="bibr" rid="bib70">Medam et al., 2016</xref>; <xref ref-type="bibr" rid="bib109">Sidman et al., 1982</xref>). Such reversibility is important because several researchers have proposed it as a defining feature of symbolic reference (<xref ref-type="bibr" rid="bib10">Deacon, 1998</xref>; <xref ref-type="bibr" rid="bib50">Kabdebon and Dehaene-Lambertz, 2019</xref>; <xref ref-type="bibr" rid="bib79">Nieder, 2009</xref>). Here, we went one step further by testing this hypothesis at the brain level. Indeed, a limit of previous behavioural studies is that animals could have understood the reversibility of a symbolic relationship, but failed to express it behaviourally because of extraneous procedural or attentional factors, or because of a conflict between different brain processes (e.g. for maintaining the specific and rewarded learned pairing vs. generalising to the reverse order). Here, we used fMRI and a passive paradigm to directly probe whether any area of the monkey brain would exhibit surprise at a violation of the reversal of a learned association. Our results show that this is not the case.</p><p>Interpretation must remain cautious, as there are also some occasional behavioural reports of spontaneous reversal of learned associations, for instance in one well-trained California sea lion and a Beluga whale (<xref ref-type="bibr" rid="bib53">Kastak et al., 2001</xref>; <xref ref-type="bibr" rid="bib75">Murayama et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Schusterman and Kastak, 1998</xref>) and possibly in 1 out of 20 baboons in <xref ref-type="bibr" rid="bib70">Medam et al., 2016</xref>. These studies may indicate that, with sufficient training, symbolic representation might eventually emerge in some animals, as also suggested by the small reversal trend in a recent behaviour study in baboons (<xref ref-type="bibr" rid="bib8">Chartier and Fagot, 2023</xref>). However, they may also merely show that animals may begin to spontaneously reverse new associations once they have received extensive training with bidirectional ones (<xref ref-type="bibr" rid="bib56">Kojima, 1984</xref>). The bulk of the literature strongly suggests that while animals easily learn indexical associations, especially monkeys and chimpanzees (<xref ref-type="bibr" rid="bib17">Diester and Nieder, 2007</xref>; <xref ref-type="bibr" rid="bib60">Livingstone et al., 2010</xref>; <xref ref-type="bibr" rid="bib68">Matsuzawa, 1985</xref>; <xref ref-type="bibr" rid="bib95">Premack, 1971</xref>), but also dogs (<xref ref-type="bibr" rid="bib35">Fugazza et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Kaminski et al., 2004</xref>), vocal birds (e.g. <xref ref-type="bibr" rid="bib88">Pepperberg, 2009</xref>), and even bees (<xref ref-type="bibr" rid="bib47">Howard et al., 2019</xref>), they exhibit little or no evidence for genuine symbolic processing. Discriminating symbolic from indexical representations can be achieved by testing for spontaneous reversibility between the labels and the objects, as in the current study, or by testing for the presence of systematic compositional relationships among the labels (<xref ref-type="bibr" rid="bib79">Nieder, 2009</xref>).</p><p>One previous study showed preliminary evidence for a lack of reversibility in macaque monkey inferotemporal cortex (<xref ref-type="bibr" rid="bib72">Meyer and Olson, 2011</xref>), but only recorded on a subset of neurons, and after extensive training on pairs of visual images (816 exposures per pair). Interestingly, a similar set of arbitrary stimuli and extensive training protocol (258 trials per pair) was used in an fMRI study of stimulus association in humans, where congruity effects were also found to be restricted to early visual areas (<xref ref-type="bibr" rid="bib96">Richter et al., 2018</xref>). Such extensive training might have led to low-level and rigid encoding in the trained direction. It is therefore instructive that, here, in monkeys, we found irreversibility after a very short training period. Indeed, in experiment 2, just 24 exposures per pair were sufficient to observe a surprise effect in the canonical direction, yet without generalisation in the reverse direction, even after longer exposures. In addition, we strived to make the objects concrete and recognisable to the monkeys (by using pictures of toys that were familiar to them, taken from various angles), while the labels were as abstract as possible to promote a symbol-referent asymmetry in the pairs. We considered using macaque vocalisations for the sounds, but these already have a defined meaning, often emotional, that could have disrupted the experiments. Furthermore, the present animals had extensive experience with human speech. Finally, while the present lab setting could be judged artificial and not easily conducive to language acquisition, previous evidence indicates that human preverbal infants easily learn labels in such a setting (<xref ref-type="bibr" rid="bib71">Mersad et al., 2021</xref>) and spontaneously reverse associations after only a short training period (<xref ref-type="bibr" rid="bib21">Ekramnia and Dehaene-Lambertz, 2019</xref>; <xref ref-type="bibr" rid="bib50">Kabdebon and Dehaene-Lambertz, 2019</xref>).</p><p>Non-human primates are often considered the animal model of choice to understand the neural correlates of high-level cognitive functions in humans (<xref ref-type="bibr" rid="bib28">Feng et al., 2020</xref>; <xref ref-type="bibr" rid="bib78">Newsome and Stein-Aviles, 1999</xref>; <xref ref-type="bibr" rid="bib100">Roelfsema and Treue, 2014</xref>). Accordingly, many studies have emphasised the similarity between human and non-human primates in terms of brain anatomy, physiology, and behaviour (<xref ref-type="bibr" rid="bib6">Caspari et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">De Valois et al., 1974</xref>; <xref ref-type="bibr" rid="bib23">Erb et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Hackett et al., 2001</xref>; <xref ref-type="bibr" rid="bib39">Harwerth and Smith, 1985</xref>; <xref ref-type="bibr" rid="bib63">Mantini et al., 2012a</xref>; <xref ref-type="bibr" rid="bib64">Mantini et al., 2012b</xref>; <xref ref-type="bibr" rid="bib62">Mantini et al., 2011</xref>; <xref ref-type="bibr" rid="bib66">Margulies et al., 2016</xref>; <xref ref-type="bibr" rid="bib92">Petrides et al., 2012</xref>; <xref ref-type="bibr" rid="bib118">Uhrig et al., 2014</xref>; <xref ref-type="bibr" rid="bib124">Warren, 1974</xref>; <xref ref-type="bibr" rid="bib126">Wilson et al., 2017</xref>; <xref ref-type="bibr" rid="bib127">Wise, 2008</xref>). At the same time, important differences between human and monkey brains have been reported as well (<xref ref-type="bibr" rid="bib85">Passingham, 2008</xref>). Using a direct comparison with fMRI, some specific functional differences have been found (<xref ref-type="bibr" rid="bib11">Denys et al., 2004a</xref>; <xref ref-type="bibr" rid="bib12">Denys et al., 2004b</xref>; <xref ref-type="bibr" rid="bib65">Mantini et al., 2013</xref>; <xref ref-type="bibr" rid="bib120">Vanduffel et al., 2002</xref>). Particularly relevant is that, in contrast to humans, monkeys show clear feature tuning in the prefrontal cortex, which is in line with the sensory activation we found in monkey PFC (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and the involvement of monkey PFC in the congruity effect in experiment 2 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Many anatomical differences have been reported between humans and monkeys using MRI as well as histological methods. In particular, the human brain is exceptionally large (<xref ref-type="bibr" rid="bib43">Herculano-Houzel, 2012</xref>), and contains a number of structural differences compared to the brains of other primates (<xref ref-type="bibr" rid="bib7">Chaplin et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Leroy et al., 2015</xref>; <xref ref-type="bibr" rid="bib77">Neubert et al., 2014</xref>; <xref ref-type="bibr" rid="bib84">Palomero-Gallagher and Zilles, 2019</xref>; <xref ref-type="bibr" rid="bib99">Rilling, 2014</xref>; <xref ref-type="bibr" rid="bib102">Schenker et al., 2010</xref>; <xref ref-type="bibr" rid="bib115">Takemura et al., 2017</xref>). Notably, while the human arcuate fasciculus provides a strong direct connection between inferior prefrontal and temporal areas involved in language processing, this bundle is reduced and does not extend as anteriorly and as ventrally in other primates, including chimpanzees (<xref ref-type="bibr" rid="bib3">Balezeau et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Eichert et al., 2020</xref>; <xref ref-type="bibr" rid="bib98">Rilling et al., 2011</xref>; <xref ref-type="bibr" rid="bib97">Rilling et al., 2008</xref>; <xref ref-type="bibr" rid="bib117">Thiebaut de Schotten et al., 2012</xref>). Also, the PFC is selectively increased in terms of tissue volume (<xref ref-type="bibr" rid="bib7">Chaplin et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Donahue et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Hill et al., 2010</xref>; <xref ref-type="bibr" rid="bib111">Smaers et al., 2017</xref>). While this may not translate to a selective increase in terms of the number of PFC neurons (<xref ref-type="bibr" rid="bib36">Gabi et al., 2016</xref>), dendritic arborisations and synaptic density are larger in human PFC (<xref ref-type="bibr" rid="bib22">Elston, 2007</xref>; <xref ref-type="bibr" rid="bib44">Hilgetag and Goulas, 2020</xref>; <xref ref-type="bibr" rid="bib107">Shibata et al., 2021</xref>). These anatomical differences may underlie the fundamental differences in language learning abilities between these species, but this is still controversial (e.g. <xref ref-type="bibr" rid="bib46">Hopkins et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Iriki, 2006</xref>). Here, we show that reversibility of associations, a crucial element in the ability to attach symbols to objects and concepts, sharply differs between human and non-human primates and offers a more tractable way to investigate potential differences between species.</p><p>The striking interspecies difference in size and extent of the violation effect as measured with fMRI, even for purely canonical stimuli, points to a more efficient species-specific learning system, that our experiment tentatively relates to a symbolic competence. The areas that specifically activated in humans when the reversed association was violated were not limited to the classical language network in the left hemisphere. They extended bilaterally to homolog areas of the right hemisphere, which are involved for instance in the acquisition of musical languages (<xref ref-type="bibr" rid="bib86">Patel, 2010</xref>). They also extend dorsally to the MFG and IPS which are involved in the acquisition of the language of numbers, geometry and higher mathematics (<xref ref-type="bibr" rid="bib1">Amalric and Dehaene, 2016</xref>; <xref ref-type="bibr" rid="bib93">Piazza, 2010</xref>; <xref ref-type="bibr" rid="bib123">Wang et al., 2019</xref>). Finally, an ROI analysis shows that they also include the VWFA and vicinity. The VWFA is known to be sensitive to letters, but also to other visual symbols such as a new learned face-like script (<xref ref-type="bibr" rid="bib74">Moore et al., 2014</xref>) or emblematic pictures of famous cities (e.g. the Eiffel tower for Paris; <xref ref-type="bibr" rid="bib112">Song et al., 2012</xref>), and the nearby lateral inferotemporal cortex responds to Arabic numerals and other mathematical symbols (<xref ref-type="bibr" rid="bib1">Amalric and Dehaene, 2016</xref>; <xref ref-type="bibr" rid="bib108">Shum et al., 2013</xref>). Strikingly, these extended areas, shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, correspond to regions whose cortical expansion and connectivity patterns are maximally different in humans compared to other primates (<xref ref-type="bibr" rid="bib7">Chaplin et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Donahue et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Hill et al., 2010</xref>; <xref ref-type="bibr" rid="bib111">Smaers et al., 2017</xref>). They also fit with a previous fMRI comparison of humans and macaque monkeys, where humans were shown to exhibit uniquely abstract and integrative representations of numerical and sequence patterns in these regions (<xref ref-type="bibr" rid="bib122">Wang et al., 2015</xref>).</p><p>In all of these studies, the observed changes are bilateral, extended, and go beyond the language network per se. Such an extended network does not fit with the hypothesis that a single localised system, such as natural language or a universal generative faculty, is the primary engine of all human-specific abstract symbolic abilities (<xref ref-type="bibr" rid="bib41">Hauser and Watumull, 2017</xref>; <xref ref-type="bibr" rid="bib113">Spelke, 2003</xref>). Rather, our results suggest that multiple parallel and partially dissociable human brain networks possess symbolic abilities and deploy them in different domains such as natural language, music and mathematics (<xref ref-type="bibr" rid="bib2">Amalric and Dehaene, 2017</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Fedorenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib26">Fedorenko and Varley, 2016</xref>).</p><p>The neurobiological mechanisms that enable reversible symbol learning in humans remain to be discovered. Interestingly, most learning rules, such as spike-time-dependent plasticity, are sensitive to temporal order and timing, a feature of fundamental importance for predictive coding. In contrast, as indicated by the behavioural results of experiment 2, humans seem to forget the temporal order in which pairs of stimuli are presented when they store them at a symbolic level. This has been interpreted as improper causal reasoning (<xref ref-type="bibr" rid="bib82">Ogawa et al., 2010</xref>). Indeed, if A repeatedly precedes B, then perceiving A predicts the appearance of B; but if B is observed, concluding to the likely presence of A is a logical fallacy. Still, brain mechanisms for temporal reversal do exist in the literature. The most prominent candidate, in both humans and non-human animals, is hippocampal-dependent neuronal replay of sequences of events, which can occur in forward and reverse temporal order (<xref ref-type="bibr" rid="bib32">Foster, 2017</xref>; <xref ref-type="bibr" rid="bib59">Liu et al., 2019</xref>). Sequence reversal may be important during learning, in order to trace back to a memorised event that led to a reward. In line with this, a retroactive gradient has been shown in memory storage in humans, where memory is strongest for stimuli that were presented close to the reward but preceding it (<xref ref-type="bibr" rid="bib5">Braun et al., 2018</xref>). This memory trace may explain the slight facilitation observed in baboons when they learn reversed congruent pairs relative to reversed incongruent pairs (<xref ref-type="bibr" rid="bib8">Chartier and Fagot, 2023</xref>). Although neuronal replay in both forward and reverse directions exists in non-human animals, it might be that this mechanism has selectively expanded to symbol-related areas of the human brain – a clear hypothesis for future work.</p><p>Obviously, even humans do not always disregard temporal order for all associations between stimulus pairs – for instance, they remember letters of the alphabet in a fixed temporal order (<xref ref-type="bibr" rid="bib55">Klahr et al., 1983</xref>). Thus, future work should also clarify which conditions promote reversible symbolic learning. Here, the pairs comprised one fixed and abstract element (either linguistic or graphical), which served as a label, paired with several different views of a concrete object. In human infants, the association of a label with the presentation of objects helps them construct the object category, as revealed by several experiments in which infants discriminate between categories (<xref ref-type="bibr" rid="bib30">Ferry et al., 2013</xref>), or correctly process the number of objects (<xref ref-type="bibr" rid="bib129">Xu et al., 2005</xref>) when the categories and objects are named, but not in the absence of a label. Interestingly, preverbal infants are flexible and accept pictures as labels for a rule (<xref ref-type="bibr" rid="bib50">Kabdebon and Dehaene-Lambertz, 2019</xref>), as well as monkey vocalisations and tones as labels for an animal category (<xref ref-type="bibr" rid="bib29">Ferguson and Waxman, 2016</xref>; <xref ref-type="bibr" rid="bib30">Ferry et al., 2013</xref>), whereas older infants who have been exposed to many social situations in which language is the primary symbolic medium to transfer information, expect symbolic labels to be in the native language (<xref ref-type="bibr" rid="bib90">Perszyk and Waxman, 2019</xref>). Later, they recover flexibility, suggesting that this transient limitation might be a contextual strategy due to the pivotal role of language in naming at this time of life.</p><p>While our results suggest a dramatic difference in the way human and non-human primates encode associations between sensory stimuli, several limitations of the present work should be kept in mind. First, due to ethical and financial reasons we only tested 4 monkeys, while we tested 55 humans in total. While it is common in primate physiological studies to report the results for two animals, this makes it challenging to extrapolate the results to the whole species (<xref ref-type="bibr" rid="bib34">Fries and Maris, 2022</xref>). To address this point, we combined the results from two different labs, collecting data from two animals in each lab.</p><p>A second limitation is that the interspecies differences that we observed could be due to a number of hard-to-control factors. While we ensured greater attention and motivation in experiment 2, other obvious differences include a lifetime of open-field experiences and education in our human adults, which was not available to monkeys and includes a strong bias towards explicit learning of symbolic systems (e.g. words, letters, digits, etc). As noted above, however, the fact that 5-month-old infants, who lack such extensive experience, also show a similar symbolic reversibility effect (<xref ref-type="bibr" rid="bib50">Kabdebon and Dehaene-Lambertz, 2019</xref>) suggest that these factors may not fully explain our findings.</p><p>One way to respond to potential differences in both lived experience and attention would be to investigate the reversal of associations in a species-relevant context, such as the recognition of conspecific identity. A large body of literature has shown that many non-human primates, including macaque monkeys, can make multi-modal associations between the faces and the vocalisations of individual animals they are familiar with (<xref ref-type="bibr" rid="bib110">Sliwa et al., 2011</xref>; <xref ref-type="bibr" rid="bib106">Seyfarth and Cheney, 2015</xref>), although the training generally takes place with a simultaneous presentation of faces and vocalisations. A future direction of research could therefore be to look for spontaneous reversal of the direction of learned pairs of faces and vocalisations of individual unfamiliar animals. The experiment would involve habituating animals with a fixed order of presentation, for example first a face and then the vocalisation of an individual animal, then testing whether they are surprised when the vocalisation is played first and then an incongruent face is shown. Note however that, even if some specific circuits such as the identity recognition system were shown to exhibit a spontaneous reversal of associations in non-human primates, the human brain may still differ in its flexible ability to associate <italic>arbitrary</italic> symbols with <italic>any</italic> mental representation in a bidirectional manner, as studied here. The distributed bilateral activation observed here in areas of human prefrontal cortex and higher temporal and parietal cortices, which are thought to form a flexible global neuronal workspace (<xref ref-type="bibr" rid="bib67">Mashour et al., 2020</xref>), suggests that, during its recent evolution and expansion, the human workspace may have acquired a capacity to process arbitrary symbol systems (<xref ref-type="bibr" rid="bib16">Dehaene et al., 2022</xref>).</p><p>A third limitation is that we only compared humans to a single species, macaque monkeys. Testing non-human primates closer to humans, such as chimpanzees, might yield different conclusions. Although chimpanzee Ai’s failure of reversibility (<xref ref-type="bibr" rid="bib56">Kojima, 1984</xref>) is striking, it may not be representative. Reversible symbolic learning should also be evaluated in vocal learners such as songbirds and parrots, as some of them demonstrate sophisticated and flexible label learning (see e.g. <xref ref-type="bibr" rid="bib89">Pepperberg and Carey, 2012</xref>). Furthermore, in dogs, social interactions between the dog and the experimenter during learning facilitate associations (<xref ref-type="bibr" rid="bib35">Fugazza et al., 2021</xref>), as is also the case in infants. Social cues were absent in our design, and whether they would favour a switch to a symbolic system might be interesting to explore. Finally, we only tested adult monkeys, yet there might be a critical period during which reversible symbolic representation might be possible with appropriate training procedures; indeed, juvenile macaques learn better and faster to associate an arbitrary label with visual quantities than adults (<xref ref-type="bibr" rid="bib114">Srihasam et al., 2012</xref>). The present work provides a simple experimental paradigm that can easily be extended to all these cases, thus offering a unique opportunity to test whether humans are unique in their ability to acquire symbols.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>We tested four adult rhesus macaques (male, 6–8 kg, 5–19 years of age). YS and JD participated in experiment 1 and JD, JC, and DN in experiment 2. All procedures were conducted in accordance with the European convention for animal care (86-406) and the NIH’s guide for the care and use of laboratory animals. They were approved by the Institutional Ethical Committee of the CEA and by the ethical committee for animal research of the KU Leuven. Animal housing and handling were according to the recommendations of the Weatherall report, allowing extensive locomotor behaviour, social interactions, and foraging. All animals were group-housed (cage size at least 16–32 m<sup>3</sup>) with diverse cage enrichment (auditory and visual stimuli, toys, foraging devices, etc.).</p><p>We also tested 55 healthy human subjects with no known neurological or psychiatric pathology (experiment 1, <italic>n</italic> = 31; experiment 2, <italic>n</italic> = 24). In experiment 2, an additional three subjects were excluded because they showed no evidence of learning the canonical pairs. Human subjects gave written informed consent to participate in this study, which was approved by the French National Ethics Committee.</p></sec><sec id="s4-2"><title>Stimuli</title><p>For the visual objects, five sets of four objects each were designed and used for both experiments 1 and 2 (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for experiment 1 and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref> for experiment 2). All five sets were used for each macaque monkey, while one set was used per human subject, alternating between sets 2 and 3 for subsequent subjects. The two first sets were 3D renderings of objects differing in their visual properties and semantic categories. As they might be considered as more familiar to humans, the other three sets of objects were photographs of monkey toys which the monkeys were exposed to in their home cages for at least 2 weeks prior to the training blocks. They were mostly geometrical 3D objects with no evident and consistent name for naive human participants. For each object eight different stimuli were generated by choosing eight different viewpoints. These stimuli are called ‘objects’ hereafter.</p><p>A label was associated with each object in each set. For experiment 1, the labels were auditory French pseudo-words with large differences in the number and identity of their syllables within each set (e.g. ‘tøjɑ̃’, ‘ɡliʃu’, ‘byɲyɲy’, and ‘kʁɛfila’). Note that monkeys were daily exposed to French radio and television as well as to French-speaking animal caretakers. Macaque monkey vocalisations were not considered as these already have a defined meaning, often emotional, that could have disrupted the experiments. In experiment 2, the labels were abstract black-and-white visual shapes, difficult to name and similar to the lexigrams used to train chimpanzees to communicate with humans (<xref ref-type="bibr" rid="bib68">Matsuzawa, 1985</xref>).</p></sec><sec id="s4-3"><title>Experimental paradigm</title><sec id="s4-3-1"><title>Stimulus presentation</title><p>Each set to be learned comprised four pairs. Two pairs were presented in the label–object direction (L1–O1 and L3–O3), and two in the object–label direction (O2–L2 and O4–L4). Labels were speech sounds in experiment 1, and black-and-white shapes in experiment 2. In each trial, the first stimulus (label or object) was presented during 700 ms, followed by an inter-stimulus-interval of 100 ms then the second stimulus during 700 ms (total trial duration: 1500 ms). The pairs were separated by a variable inter-trial-interval randomly chosen among eight different durations between 3 and 4.75 s (step = 250 ms). The series of eight intervals was randomised each time that a series was completed. The visual stimuli were ~8 degrees in diameter, centred on the screen, with an average luminance set equal to the background. At each trial, the orientation of the object was randomly chosen among the eight possibilities. A cross was present at the centre of the screen when no visual stimulus was present. Auditory stimuli were presented to both ears at ~80 dB.</p></sec><sec id="s4-3-2"><title>Training</title><p>The experiment was designed to be also tested in 3-month-old human infants (<xref ref-type="bibr" rid="bib21">Ekramnia and Dehaene-Lambertz, 2019</xref>), which explains our choice of short training sessions over 3 consecutive days because of the short attention span in infants and the reported benefit of sleep for encoding word meaning after a learning session (<xref ref-type="bibr" rid="bib33">Friedrich et al., 2017</xref>). Therefore, training consisted of observing 24 trials as described above (1 block of 24 trials for each of the 3 training days). Two pairs (one in each direction) were introduced on the first day of training (e.g. L1–O1 and O2–L2). First, one pair was shown for six trials, then the other pair for six trials, then the two pairs were randomly presented for six trials each. On the second day of training, the two other pairs (L3–O3 and O4–L4) were presented using the same procedure as on day 1. On the 3 days, all pairs were randomly presented (six presentations each). The object–label pairing was constant but the direction of presentation (O–L or L–O) and the introduction of the pair on the first or second day was counterbalanced across participants. In experiment 1, the only sounds presented were the speech labels, while no sound was present in experiment 2, the objects and labels being visual stimuli.</p></sec><sec id="s4-3-3"><title>Human protocol</title><p>In experiment 1, the participants came to the lab to watch a first video presenting the first block of trials, and on the next 2 consecutive days they received a web link on which a video was uploaded that contained the block of training for that day (24 trials, ~3 min long). For experiment 2, all three videos were sent via a web link and participants were instructed to attentively watch the video corresponding to the given day. The participants came for the fMRI session on the fourth day. Each participant saw only one set of objects–labels, either stimulus set 2 or 3, distributed equally across participants.</p><p>In experiment 2, we added a behavioural test at the end of the MRI session to measure the learning of the subjects. They were shown all 16 possible trial pairs (incongruent and congruent in canonical and non-canonical order), plus 16 never seen, one by one. For each of them, they were asked to rate how frequently they had seen them (on a five-level scale ranging from never to rarely, sometimes, often and always). The results were analysed using a five-level ANOVA which included the canonicity × congruity 2 × 2 design. A computer crash erased responses from two participants and one subject did not participate leaving 21 subjects for this analysis.</p></sec><sec id="s4-3-4"><title>Monkey protocol</title><p>Monkeys were implanted with an MR-compatible headpost under general anaesthesia. The animals were trained to sit calm in a sphinx position in a primate chair with their head fixed, inside a mock MRI setup, and trained to fixate a small dot (0.25 degrees) within a virtual window of 1.25–2 degrees diameter (<xref ref-type="bibr" rid="bib118">Uhrig et al., 2014</xref>). Then they received 1 training block per day for 3 consecutive days (24 trials per block) for each stimulus set, similar to the human participants. On the fourth day, they were scanned while being presented with the test blocks for the corresponding stimulus set. Rewards were given at regular intervals for maintaining fixation during training and testing (within a virtual window of 1.25–2 degrees diameter), asynchronous with the visual and auditory stimulus presentation.</p><p>For experiment 1, there was no congruity effect for the first imaging sessions at day 4 (i.e. no difference between congruent and incongruent pairs in the canonical direction), which consisted in total of 62 valid runs for monkey YS and 79 for monkey JD, for the 5 stimulus sets. After the first week with initial training and the first imaging session at day 4, monkeys were further trained for an additional 2 weeks (~80 blocks, with 12 trials per pair per block, so amounting to about 960 trials per pair) and then scanned during 4 days, for the last three of the five stimulus sets. This additional training was planned in advance, as we expected that pair learning in passively fixating macaque monkeys would require extensive training, based on previous literature. In particular, pair learning was observed in the temporal cortex in macaque monkeys after about 1000 exposures per pair (<xref ref-type="bibr" rid="bib72">Meyer and Olson, 2011</xref>). So, for each of the last three sets of stimuli, training and testing took four consecutive weeks.</p><p>In experiment 2, a reward association was introduced to promote monkeys’ engagement in the task. The amount of reward that the monkeys received after successfully fixating throughout the pair presentation was either increased or decreased for a duration of 1450 ms (starting 100 ms after the offset of the second stimulus), depending on the identity of the visual object. The amount of reward remained the same, but the time in between consecutive rewards was set either twice as short (for high rewards) or twice as long (for low rewards). For each temporal direction, one visual object was associated with a high reward while the other one was associated with a low reward (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). By design, the two pairs that were averaged for each of the critical tested dimensions (direction, congruity, and canonicity of the pair) therefore had opposite reward size, making reward size an orthogonal design element. The first stimulus set was used for procedural training on this reward association paradigm for 2 weeks. Stimulus sets 2–5 were used for training as in experiment 1 (with 1 block per day for 3 consecutive days) and an fMRI test session on the fourth day.</p></sec><sec id="s4-3-5"><title>Test in MRI</title><p>The MRI session comprised four test blocks in a single fMRI session in humans and between 12 and 32 blocks per day per monkey (see below for the total number of valid runs). In both humans and monkeys, each block started with four trials in the learned direction (congruent canonical trials), one trial for each of the four pairs (two O–L and two L–O pairs). The rest of the block consisted of 40 trials in which 70% of trials were identical to the training (28 trials); 10% were incongruent pairs but the direction (O–L or L–O) was correct (4 incongruent canonical trials), thus testing whether the association was learned; 10% were congruent pairs but the direction within the pairs was reversed relative to the learned pairs (4 congruent reversed trials) and 10% were incongruent pairs in reverse (4 incongruent reversed trials). As the percentage of congruent and incongruent pairs was the same in the reversed direction, a difference can only be due to a generalisation from the canonical direction. For incongruent trials, the incongruent stimulus always came from the pair presented in the same direction (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), in order to avoid that a change of position within the pair itself (first or second stimulus) induced the perception of an incongruity.</p><p>Human participants were only instructed to keep their eyes fixed on the fixation point and pay attention to the stimuli. The monkeys were rewarded for keeping their eyes fixed on the fixation point. In experiment 1, the reward was constant, whereas in experiment 2, they received the differential reward that was implemented during training, as mentioned above.</p></sec></sec><sec id="s4-4"><title>Data acquisition</title><p>For experiment 1, both humans and monkeys were scanned with the 3T Siemens Prisma at NeuroSpin using a T2*-weighted gradient echo-planar imaging (EPI) sequence, using a 64-channel head coil for humans and a customised 8-channel phased-array surface coil (KU Leuven, Belgium) for monkeys. The imaging parameters were the following: in humans, resolution: 1.75 mm isotropic, TR: 1.81 s, TE: 30.4 ms, PF: 7/8, MB3, slices: 69; in monkeys, resolution: 1.5 mm isotropic, TR: 1.08 s, TE: 13.8 ms, PF: 6/8, iPAT2, slices: 34.</p><p>MION (monocrystalline iron oxide nanoparticle, Molday Ion, BioPAL, Worchester MA) contrast agent (10 mg/kg, i.v.) was given to monkeys before scanning (<xref ref-type="bibr" rid="bib119">Vanduffel et al., 2001</xref>). Eye movements were monitored and recorded by an eye tracking system (EyeLink 1000, SR Research, Ottawa, Canada). In total, we recorded 583 valid runs, 278 for YS and 305 for JD.</p><p>For experiment 2, the settings remained the same for the humans and for one of the monkeys (JD). Two new monkeys (JC and DN) were included at the Laboratory of Neuro- and Psychophysiology of KU Leuven and scanned with a 3T Siemens Prisma using a T2*-weighted gradient EPI sequence. For JC, an external 8-channel coil was used and the imaging parameters were the following: resolution: 1.25 mm isotropic, TR: 0.9 s, T7: 15 ms, PF: 6/8, iPAT3, multi-band 2, slices: 52. For DN, an implanted 8-channel coil was used and the imaging parameters were the following: resolution: 1.25 mm isotropic, TR: 0.9 s, TE: 15 ms, PF: 6/8, iPAT3, multi-band 2, slices: 40. Monkeys were also trained to sit in a sphinx position in a primate chair with their head fixed, and MION was again injected before scanning (11 mg/kg, i.v.). Eye movements were monitored and recorded by an eye tracking system (ETL200, ISCAN inc, Woburn, MA, USA). In addition, the animals were required to keep their hands in a box in front of the chair (as verified with optical sensors), which limited body motion. In total, we recorded 279 valid runs, 81 for JD, 106 for JC, and 92 for DN.</p></sec><sec id="s4-5"><title>Preprocessing of monkey fMRI data</title><p>Functional images were reoriented, realigned, resampled (1.00 mm isotropic), and coregistered to the anatomical template of the Montreal Neurologic Institute (MNI, Montreal, Canada) monkey space using Pypreclin, which is a custom-made scripts in Python programming language (<xref ref-type="bibr" rid="bib116">Tasserie et al., 2020</xref>).</p><p>Eye-data was analysed where only the runs with more than 85% fixation (virtual window of 2–2.5 degrees diameter) were included for further analyses (<italic>n</italic> = 16 excluded in experiment 1 and <italic>n</italic> = 14 excluded in experiment 2). Moreover, a trial was excluded if the eyes were closed for more than 650 ms (out of 700) while an image was present on the screen. In experiment 1, the top 5% of runs where motion was strongest across monkeys were excluded (<italic>n</italic> = 30) because there remained significant residual motion. For experiment 1, in total 395 runs remained to be analysed, 184 for YS and 211 for JD. For experiment 2, 268 runs remained, 77 for JD, 107 for JC, and 84 for DN.</p></sec><sec id="s4-6"><title>Preprocessing of human fMRI data</title><p>SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) was used for preprocessing of human data as well as first- and second-level models. Preprocessing consisted of standard preprocessing pipeline, including slice-time correction, realign, top-up correction, segmentation, normalisation to standard MNI space, and smoothing with a 4-mm isotropic Gaussian.</p></sec><sec id="s4-7"><title>First- and second-level analyses</title><p>After image preprocessing, active brain regions were identified by performing voxel-wise GLM analyses implemented in SPM12 in both monkeys and humans. For the first experiment, in a first-level SPM model, the twelve predictors included: (1–4) the onsets of the first stimulus of the pair (four regressors consisting in the combinations of audio/visual and canonical/non-canonical factors), and (5–12) the onsets of the second stimulus (eight regressors consisting in the combinations of audio/visual, canonical/non-canonical, and congruent/incongruent factors). These 12 events were modelled as delta functions convolved with the canonical hemodynamic response function (for MION in case of monkeys). Parameters of head motion derived from realignment were also included in the model as covariates of no interest. Contrast images for the effect of congruity (incongruent minus congruent canonical, and incongruent minus congruent non-canonical) as well as the interaction between congruity and canonicity were computed. For the second experiment, the analysis was the same, except that given the two elements of the pair were in the same (visual) modality only a single predictor was used for each stimulus pair, giving four predictors: the onsets of the second stimulus of the pair, with congruent/incongruent and canonical/non-canonical as the two factors. For the monkeys, an additional factor was whether the pair was associated with a high or a low reward, giving eight predictors in total. The temporal derivative of the hemodynamic response function was added to the model as well. Before entering the second-level analysis, the data was smoothed again, using a 5-mm smoothing kernel in humans and 2-mm in monkeys.</p><p>For the second-level group analysis, subjects were taken as the statistical unit for the humans and runs were taken as statistical units for the monkeys. One-sample <italic>t</italic>-tests were performed on the contrast images to test for the effect of the condition. Results are reported at an uncorrected voxelwise threshold of p &lt; 0.001 and a cluster p &lt; 0.05 corrected for multiple comparisons (false discovery rate, FDR).</p></sec><sec id="s4-8"><title>ROI analyses</title><p>In a separate localiser, human participants listened and read short sentences. In some of the sentences, the participants were asked to compute easy mathematical operations (math sentences). Subtracting activations to math and non-math sentences allowed to separate the regions more involved in mathematical cognition than in general sentence comprehension. We selected seven left-hemispheric regions previously reported as showing a language-related activation (<xref ref-type="bibr" rid="bib83">Pallier et al., 2011</xref>), six bilateral ROIs showing mathematically related activations (<xref ref-type="bibr" rid="bib1">Amalric and Dehaene, 2016</xref>), and finally a sphere around the VWFA (of 10 mm radius, centred on [–45 –57 –12]). In these ROIs, we recovered the subject-specific coordinates of each participant’s 10% best voxels in the following comparisons: sentences versus rest for the six language ROIs; reading versus listening for the VWFA; and numerical versus non-numerical sentences for the eight mathematical ROIs. We extracted the beta of these voxels and performed ANOVAs with congruity and canonicity as within-subject factors, and experiment as the between-subject factor. Two participants in experiment 1, and one participant in experiment 2 had no localiser, leaving 52 participants (<italic>n</italic> = 29 and <italic>n</italic> = 23) for these analyses. p-values were FDR corrected considering all 15 ROIs in each comparison.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing, Data acquisition</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Project administration, Data acquisition</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con4"><p>Data acquisistion</p></fn><fn fn-type="con" id="con5"><p>Data acquisistion</p></fn><fn fn-type="con" id="con6"><p>Data acquisition</p></fn><fn fn-type="con" id="con7"><p>Data acquisition</p></fn><fn fn-type="con" id="con8"><p>Data acquisition</p></fn><fn fn-type="con" id="con9"><p>Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Supervision, Funding acquisition, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Visualization, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects gave written informed consent to participate in this study, which was approved by the French National Ethics Committee.</p></fn><fn fn-type="other"><p>All procedures were conducted in accordance with the European convention for animal care (86-406) and the NIH's guide for the care and use of laboratory animals. They were approved by the Institutional Ethical Committee (CETEA protocol # 16-043) and by the ethical committee for animal research of the KU Leuven. Animal housing and handling were according to the recommendations of the Weatherall report, allowing extensive locomotor behaviour, social interactions, and foraging. All animals were group-housed (cage size at least 16–32 m<sup>3</sup>) with diverse cage enrichment (auditory and visual stimuli, toys, foraging devices, etc.).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-87380-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and analysis code used in this study are available on the Radboud Data Repository without access restriction, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/vps4-qs29">https://doi.org/10.34973/vps4-qs29</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Van Kerkoerle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Brain areas for reversible symbolic reference, a potential singularity of the human brain</data-title><source>Radboud Data Repository</source><pub-id pub-id-type="doi">10.34973/vps4-qs29</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Julien Lemaitre for providing veterinary care for the animals at NeuroSpin, and Chantal Franssen for help with training the animals at the KUL. We thank the NeuroSpin support cells for help in recruiting human participants and help in scanning the participants. We also thank the anonymous reviewers for their constructive comments on an earlier version of this manuscript. This research was supported by INSERM, CEA, Université Paris Saclay, Collège de France. During the course of this work, TvK, GD, and SD received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement numbers 101078667, 695710, and 695403). XL and WV were supported by grants from KU Leuven (grant agreement number C14/21/111) and from FWO-Flanders (grant agreement numbers G0E0520N and G0C1920N).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Origins of the brain networks for advanced mathematics in expert mathematicians</article-title><source>PNAS</source><volume>113</volume><fpage>4909</fpage><lpage>4917</lpage><pub-id pub-id-type="doi">10.1073/pnas.1603205113</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cortical circuits for mathematical knowledge: evidence for a major subdivision within the brain’s semantic networks</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>373</volume><elocation-id>20160515</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0515</pub-id><pub-id pub-id-type="pmid">29292362</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balezeau</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>B</given-names></name><name><surname>Gallardo</surname><given-names>G</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Hopkins</surname><given-names>W</given-names></name><name><surname>Anwander</surname><given-names>A</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Primate auditory prototype in the evolution of the arcuate fasciculus</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>611</fpage><lpage>614</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0623-9</pub-id><pub-id pub-id-type="pmid">32313267</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berwick</surname><given-names>RC</given-names></name><name><surname>Chomsky</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Why Only Us: Language and Evolution</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>EK</given-names></name><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Retroactive and graded prioritization of memory by reward</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4886</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-07280-0</pub-id><pub-id pub-id-type="pmid">30459310</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspari</surname><given-names>N</given-names></name><name><surname>Arsenault</surname><given-names>JT</given-names></name><name><surname>Vandenberghe</surname><given-names>R</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional similarity of medial superior parietal areas for shift-selective attention signals in humans and monkeys</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>2085</fpage><lpage>2099</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx114</pub-id><pub-id pub-id-type="pmid">28472289</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaplin</surname><given-names>TA</given-names></name><name><surname>Yu</surname><given-names>HH</given-names></name><name><surname>Soares</surname><given-names>JGM</given-names></name><name><surname>Gattass</surname><given-names>R</given-names></name><name><surname>Rosa</surname><given-names>MGP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A conserved pattern of differential expansion of cortical areas in simian primates</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>15120</fpage><lpage>15125</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2909-13.2013</pub-id><pub-id pub-id-type="pmid">24048842</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chartier</surname><given-names>TF</given-names></name><name><surname>Fagot</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Simultaneous learning of directional and non-directional stimulus relations in baboons (Papio papio)</article-title><source>Learning &amp; Behavior</source><volume>51</volume><fpage>166</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.3758/s13420-022-00522-8</pub-id><pub-id pub-id-type="pmid">35449392</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Affourtit</surname><given-names>J</given-names></name><name><surname>Ryskin</surname><given-names>R</given-names></name><name><surname>Regev</surname><given-names>TI</given-names></name><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Jouravlev</surname><given-names>O</given-names></name><name><surname>Malik-Moraleda</surname><given-names>S</given-names></name><name><surname>Kean</surname><given-names>H</given-names></name><name><surname>Varley</surname><given-names>R</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The human language system does not support music processing</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.01.446439</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Deacon</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>The Symbolic Species – The Co–Evolution of Language &amp; the Brain</source><publisher-name>W. W. Norton &amp; Company</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denys</surname><given-names>K</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Nelissen</surname><given-names>K</given-names></name><name><surname>Peuskens</surname><given-names>H</given-names></name><collab>ED</collab><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2004">2004a</year><article-title>The processing of visual shape in the cerebral cortex of human and nonhuman primates: a functional magnetic resonance imaging study</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>2551</fpage><lpage>2565</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3569-03.2004</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denys</surname><given-names>K</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Nelissen</surname><given-names>K</given-names></name><name><surname>Sawamura</surname><given-names>H</given-names></name><name><surname>Georgieva</surname><given-names>S</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Van Essen</surname><given-names>D</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2004">2004b</year><article-title>Visual activation in prefrontal cortex is stronger in monkeys than in humans</article-title><source>Journal of Cognitive Neuroscience</source><volume>16</volume><fpage>1505</fpage><lpage>1516</lpage><pub-id pub-id-type="doi">10.1162/0898929042568505</pub-id><pub-id pub-id-type="pmid">15601515</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>de Saussure</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>Cours de Linguistique Générale, Grande Bibliothèque Payot</source><publisher-loc>Paris</publisher-loc><publisher-name>Payot</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname><given-names>RL</given-names></name><name><surname>Morgan</surname><given-names>H</given-names></name><name><surname>Snodderly</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Psychophysical studies of monkey vision. 3. Spatial luminance contrast sensitivity tests of macaque and human observers</article-title><source>Vision Research</source><volume>14</volume><fpage>75</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(74)90118-7</pub-id><pub-id pub-id-type="pmid">4204839</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees</article-title><source>Neuron</source><volume>88</volume><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.019</pub-id><pub-id pub-id-type="pmid">26447569</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Symbols and mental programs: a hypothesis about human singularity</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>751</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.06.010</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diester</surname><given-names>I</given-names></name><name><surname>Nieder</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Semantic associations between signs and numerical categories in the prefrontal cortex</article-title><source>PLOS Biology</source><volume>5</volume><elocation-id>e294</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0050294</pub-id><pub-id pub-id-type="pmid">17973578</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>CJ</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Preuss</surname><given-names>TM</given-names></name><name><surname>Rilling</surname><given-names>JK</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Quantitative assessment of prefrontal cortex in humans relative to nonhuman primates</article-title><source>PNAS</source><volume>115</volume><fpage>E5183</fpage><lpage>E5192</lpage><pub-id pub-id-type="doi">10.1073/pnas.1721653115</pub-id><pub-id pub-id-type="pmid">29739891</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.004</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichert</surname><given-names>N</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Bryant</surname><given-names>KL</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Krug</surname><given-names>K</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cross-species cortical alignment identifies different types of anatomical reorganization in the primate temporal lobe</article-title><source>eLife</source><volume>9</volume><elocation-id>e53232</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53232</pub-id><pub-id pub-id-type="pmid">32202497</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekramnia</surname><given-names>M</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Investigating Bidirectionality of Associations in Young Infants as an Approach to the Symbolic System</source><publisher-name>Semantic Scholar</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Elston</surname><given-names>GN</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>4.13 - specialization of the neocortical pyramidal cell during primate evolution in</chapter-title><person-group person-group-type="editor"><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><source>Evolution of Nervous Systems</source><publisher-name>Academic Press</publisher-name><fpage>191</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/B0-12-370878-8/00164-6</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Armendariz</surname><given-names>M</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Homology and specificity of natural sound-encoding in human and monkey auditory cortex</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3636</fpage><lpage>3650</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy243</pub-id><pub-id pub-id-type="pmid">30395192</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Behr</surname><given-names>MK</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specificity for high-level linguistic processing in the human brain</article-title><source>PNAS</source><volume>108</volume><fpage>16428</fpage><lpage>16433</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id><pub-id pub-id-type="pmid">21885736</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Broad domain generality in focal regions of frontal and parietal cortex</article-title><source>PNAS</source><volume>110</volume><fpage>16616</fpage><lpage>16621</lpage><pub-id pub-id-type="doi">10.1073/pnas.1315235110</pub-id><pub-id pub-id-type="pmid">24062451</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Varley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Language and thought are not the same thing: evidence from neuroimaging and neurological patients</article-title><source>Annals of the New York Academy of Sciences</source><volume>1369</volume><fpage>132</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1111/nyas.13046</pub-id><pub-id pub-id-type="pmid">27096882</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Jensen</surname><given-names>FE</given-names></name><name><surname>Greely</surname><given-names>HT</given-names></name><name><surname>Okano</surname><given-names>H</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Roberts</surname><given-names>AC</given-names></name><name><surname>Fox</surname><given-names>JG</given-names></name><name><surname>Caddick</surname><given-names>S</given-names></name><name><surname>Poo</surname><given-names>M</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Morrison</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Opportunities and limitations of genetically modified nonhuman primate models for neuroscience research</article-title><source>PNAS</source><volume>117</volume><fpage>24022</fpage><lpage>24031</lpage><pub-id pub-id-type="doi">10.1073/pnas.2006515117</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>B</given-names></name><name><surname>Waxman</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What the [beep]? Six-month-olds link novel communicative signals to meaning</article-title><source>Cognition</source><volume>146</volume><fpage>185</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2015.09.020</pub-id><pub-id pub-id-type="pmid">26433024</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferry</surname><given-names>AL</given-names></name><name><surname>Hespos</surname><given-names>SJ</given-names></name><name><surname>Waxman</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Nonhuman primate vocalizations support categorization in very young human infants</article-title><source>PNAS</source><volume>110</volume><fpage>15231</fpage><lpage>15235</lpage><pub-id pub-id-type="doi">10.1073/pnas.1221166110</pub-id><pub-id pub-id-type="pmid">24003164</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>WT</given-names></name><name><surname>Hauser</surname><given-names>MD</given-names></name><name><surname>Chomsky</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The evolution of the language faculty: clarifications and implications</article-title><source>Cognition</source><volume>97</volume><fpage>179</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2005.02.005</pub-id><pub-id pub-id-type="pmid">16112662</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Replay Comes of Age</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>581</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031538</pub-id><pub-id pub-id-type="pmid">28772098</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>M</given-names></name><name><surname>Wilhelm</surname><given-names>I</given-names></name><name><surname>Mölle</surname><given-names>M</given-names></name><name><surname>Born</surname><given-names>J</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The sleeping infant brain anticipates development</article-title><source>Current Biology</source><volume>27</volume><fpage>2374</fpage><lpage>2380</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.070</pub-id><pub-id pub-id-type="pmid">28756948</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>What to do if n is two?</article-title><source>Journal of Cognitive Neuroscience</source><volume>34</volume><fpage>1114</fpage><lpage>1118</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01857</pub-id><pub-id pub-id-type="pmid">35468209</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fugazza</surname><given-names>C</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name><name><surname>Magyari</surname><given-names>L</given-names></name><name><surname>Dror</surname><given-names>S</given-names></name><name><surname>Zempléni</surname><given-names>A</given-names></name><name><surname>Miklósi</surname><given-names>Á</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rapid learning of object names in dogs</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>2222</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-81699-2</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabi</surname><given-names>M</given-names></name><name><surname>Neves</surname><given-names>K</given-names></name><name><surname>Masseron</surname><given-names>C</given-names></name><name><surname>Ribeiro</surname><given-names>PFM</given-names></name><name><surname>Ventura-Antunes</surname><given-names>L</given-names></name><name><surname>Torres</surname><given-names>L</given-names></name><name><surname>Mota</surname><given-names>B</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name><name><surname>Herculano-Houzel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>No relative expansion of the number of prefrontal neurons in primate and human evolution</article-title><source>PNAS</source><volume>113</volume><fpage>9617</fpage><lpage>9622</lpage><pub-id pub-id-type="doi">10.1073/pnas.1610178113</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazizadeh</surname><given-names>A</given-names></name><name><surname>Griggs</surname><given-names>W</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Temporal-prefrontal cortical network for discrimination of valuable objects in long-term memory</article-title><source>PNAS</source><volume>115</volume><fpage>E2135</fpage><lpage>E2144</lpage><pub-id pub-id-type="doi">10.1073/pnas.1707695115</pub-id><pub-id pub-id-type="pmid">29437980</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackett</surname><given-names>TA</given-names></name><name><surname>Preuss</surname><given-names>TM</given-names></name><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Architectonic identification of the core region in auditory cortex of macaques, chimpanzees, and humans</article-title><source>The Journal of Comparative Neurology</source><volume>441</volume><fpage>197</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1002/cne.1407</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harwerth</surname><given-names>RS</given-names></name><name><surname>Smith</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Rhesus monkey as a model for normal vision of humans</article-title><source>American Journal of Optometry and Physiological Optics</source><volume>62</volume><fpage>633</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1097/00006324-198509000-00009</pub-id><pub-id pub-id-type="pmid">4050966</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname><given-names>MD</given-names></name><name><surname>Chomsky</surname><given-names>N</given-names></name><name><surname>Fitch</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The faculty of language: what is it, who has it, and how did it evolve?</article-title><source>Science</source><volume>298</volume><fpage>1569</fpage><lpage>1579</lpage><pub-id pub-id-type="doi">10.1126/science.298.5598.1569</pub-id><pub-id pub-id-type="pmid">12446899</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname><given-names>MD</given-names></name><name><surname>Watumull</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Universal Generative Faculty: The source of our expressive power in language, mathematics, morality, and music</article-title><source>Journal of Neurolinguistics</source><volume>43</volume><fpage>78</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.jneuroling.2016.10.005</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henshilwood</surname><given-names>CS</given-names></name><name><surname>d’Errico</surname><given-names>F</given-names></name><name><surname>Yates</surname><given-names>R</given-names></name><name><surname>Jacobs</surname><given-names>Z</given-names></name><name><surname>Tribolo</surname><given-names>C</given-names></name><name><surname>Duller</surname><given-names>GAT</given-names></name><name><surname>Mercier</surname><given-names>N</given-names></name><name><surname>Sealy</surname><given-names>JC</given-names></name><name><surname>Valladas</surname><given-names>H</given-names></name><name><surname>Watts</surname><given-names>I</given-names></name><name><surname>Wintle</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Emergence of modern human behavior: middle stone age engravings from South Africa</article-title><source>Science</source><volume>295</volume><fpage>1278</fpage><lpage>1280</lpage><pub-id pub-id-type="doi">10.1126/science.1067575</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herculano-Houzel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The remarkable, yet not extraordinary, human brain as a scaled-up primate brain and its associated cost</article-title><source>PNAS</source><volume>109</volume><fpage>10661</fpage><lpage>10668</lpage><pub-id pub-id-type="doi">10.1073/pnas.1201895109</pub-id><pub-id pub-id-type="pmid">22723358</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Goulas</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>“Hierarchy” in the organization of brain networks</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>375</volume><elocation-id>20190319</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0319</pub-id><pub-id pub-id-type="pmid">32089116</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>J</given-names></name><name><surname>Inder</surname><given-names>T</given-names></name><name><surname>Neil</surname><given-names>J</given-names></name><name><surname>Dierker</surname><given-names>D</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Van Essen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Similar patterns of cortical expansion during human development and evolution</article-title><source>PNAS</source><volume>107</volume><fpage>13135</fpage><lpage>13140</lpage><pub-id pub-id-type="doi">10.1073/pnas.1001229107</pub-id><pub-id pub-id-type="pmid">20624964</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopkins</surname><given-names>WD</given-names></name><name><surname>Russell</surname><given-names>JL</given-names></name><name><surname>Schaeffer</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The neural and cognitive correlates of aimed throwing in chimpanzees: a magnetic resonance image and behavioural study on a unique form of social tool use</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>367</volume><fpage>37</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1098/rstb.2011.0195</pub-id><pub-id pub-id-type="pmid">22106425</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>SR</given-names></name><name><surname>Avarguès-Weber</surname><given-names>A</given-names></name><name><surname>Garcia</surname><given-names>JE</given-names></name><name><surname>Greentree</surname><given-names>AD</given-names></name><name><surname>Dyer</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Symbolic representation of numerosity by honeybees (<italic>Apis mellifera</italic> ): matching characters to small quantities</article-title><source>Proceedings of the Royal Society B</source><volume>286</volume><elocation-id>20190238</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2019.0238</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imai</surname><given-names>M</given-names></name><name><surname>Murai</surname><given-names>C</given-names></name><name><surname>Miyazaki</surname><given-names>M</given-names></name><name><surname>Okada</surname><given-names>H</given-names></name><name><surname>Tomonaga</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The contingency symmetry bias (affirming the consequent fallacy) as A prerequisite for word learning: A comparative study of pre-linguistic human infants and chimpanzees</article-title><source>Cognition</source><volume>214</volume><elocation-id>104755</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2021.104755</pub-id><pub-id pub-id-type="pmid">33957427</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iriki</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The neural origins and implications of imitation, mirror neurons and tool use</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>660</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.10.008</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabdebon</surname><given-names>C</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Symbolic labeling in 5-month-old human infants</article-title><source>PNAS</source><volume>116</volume><fpage>5805</fpage><lpage>5810</lpage><pub-id pub-id-type="doi">10.1073/pnas.1809144116</pub-id><pub-id pub-id-type="pmid">30837317</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaminski</surname><given-names>J</given-names></name><name><surname>Call</surname><given-names>J</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Word learning in a domestic dog: evidence for “fast mapping”</article-title><source>Science</source><volume>304</volume><fpage>1682</fpage><lpage>1683</lpage><pub-id pub-id-type="doi">10.1126/science.1097859</pub-id><pub-id pub-id-type="pmid">15192233</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaposvari</surname><given-names>P</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning signals in macaque inferior temporal cortex</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>250</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw374</pub-id><pub-id pub-id-type="pmid">27909007</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastak</surname><given-names>CR</given-names></name><name><surname>Schusterman</surname><given-names>RJ</given-names></name><name><surname>Kastak</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Equivalence classification by California sea lions using class-specific reinforcers</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>76</volume><fpage>131</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1901/jeab.2001.76-131</pub-id><pub-id pub-id-type="pmid">11599636</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Aristotle on the definition of what it is to be human in</chapter-title><person-group person-group-type="editor"><name><surname>Keil</surname><given-names>G</given-names></name><name><surname>Kreft</surname><given-names>N</given-names></name></person-group><source>Aristotle’s Anthropology</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><fpage>25</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1017/9781108131643.002</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klahr</surname><given-names>D</given-names></name><name><surname>Chase</surname><given-names>WG</given-names></name><name><surname>Lovelace</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Structure and process in alphabetic retrieval</article-title><source>Journal of Experimental Psychology</source><volume>9</volume><fpage>462</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.9.3.462</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kojima</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Generalization between productive use and receptive discrimination of names in an artificial visual language by a chimpanzee</article-title><source>International Journal of Primatology</source><volume>5</volume><fpage>161</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1007/BF02735739</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leroy</surname><given-names>F</given-names></name><name><surname>Cai</surname><given-names>Q</given-names></name><name><surname>Bogart</surname><given-names>SL</given-names></name><name><surname>Dubois</surname><given-names>J</given-names></name><name><surname>Coulon</surname><given-names>O</given-names></name><name><surname>Monzalvo</surname><given-names>K</given-names></name><name><surname>Fischer</surname><given-names>C</given-names></name><name><surname>Glasel</surname><given-names>H</given-names></name><name><surname>Van der Haegen</surname><given-names>L</given-names></name><name><surname>Bénézit</surname><given-names>A</given-names></name><name><surname>Lin</surname><given-names>C-P</given-names></name><name><surname>Kennedy</surname><given-names>DN</given-names></name><name><surname>Ihara</surname><given-names>AS</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name><name><surname>Moutard</surname><given-names>M-L</given-names></name><name><surname>Poupon</surname><given-names>C</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>Roberts</surname><given-names>N</given-names></name><name><surname>Hopkins</surname><given-names>WD</given-names></name><name><surname>Mangin</surname><given-names>J-F</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>New human-specific brain landmark: The depth asymmetry of superior temporal sulcus</article-title><source>PNAS</source><volume>112</volume><fpage>1208</fpage><lpage>1213</lpage><pub-id pub-id-type="doi">10.1073/pnas.1412389112</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipkens</surname><given-names>R</given-names></name><name><surname>Kop</surname><given-names>PFM</given-names></name><name><surname>Matthijs</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A test of symmetry and transitivity in the conditional discrimination performances of pigeons</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>49</volume><fpage>395</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1901/jeab.1988.49-395</pub-id><pub-id pub-id-type="pmid">16812547</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human replay spontaneously reorganizes experience</article-title><source>Cell</source><volume>178</volume><fpage>640</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.06.012</pub-id><pub-id pub-id-type="pmid">31280961</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Morocz</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The benefit of symbols: monkeys show linear, human-like, accuracy when using symbols to represent scalar value</article-title><source>Animal Cognition</source><volume>13</volume><fpage>711</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1007/s10071-010-0321-1</pub-id><pub-id pub-id-type="pmid">20443126</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Pettine</surname><given-names>WW</given-names></name><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Moore</surname><given-names>B</given-names></name><name><surname>Morocz</surname><given-names>IA</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Symbol addition by monkeys provides evidence for normalized quantity coding</article-title><source>PNAS</source><volume>111</volume><fpage>6822</fpage><lpage>6827</lpage><pub-id pub-id-type="doi">10.1073/pnas.1404208111</pub-id><pub-id pub-id-type="pmid">24753600</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mantini</surname><given-names>D</given-names></name><name><surname>Gerits</surname><given-names>A</given-names></name><name><surname>Nelissen</surname><given-names>K</given-names></name><name><surname>Durand</surname><given-names>JB</given-names></name><name><surname>Joly</surname><given-names>O</given-names></name><name><surname>Simone</surname><given-names>L</given-names></name><name><surname>Sawamura</surname><given-names>H</given-names></name><name><surname>Wardak</surname><given-names>C</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Default mode of brain function in monkeys</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12954</fpage><lpage>12962</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2318-11.2011</pub-id><pub-id pub-id-type="pmid">21900574</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mantini</surname><given-names>D</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Romani</surname><given-names>GL</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Data-driven analysis of analogous brain networks in monkeys and humans during natural vision</article-title><source>NeuroImage</source><volume>63</volume><fpage>1107</fpage><lpage>1118</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.042</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mantini</surname><given-names>D</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Betti</surname><given-names>V</given-names></name><name><surname>Perrucci</surname><given-names>MG</given-names></name><name><surname>Romani</surname><given-names>GL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Interspecies activity correlations reveal functional correspondence between monkey and human brain areas</article-title><source>Nature Methods</source><volume>9</volume><fpage>277</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1868</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mantini</surname><given-names>D</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Romani</surname><given-names>GL</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evolutionarily novel functional networks in the human brain?</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>3259</fpage><lpage>3275</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4392-12.2013</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Goulas</surname><given-names>A</given-names></name><name><surname>Falkiewicz</surname><given-names>M</given-names></name><name><surname>Huntenburg</surname><given-names>JM</given-names></name><name><surname>Langs</surname><given-names>G</given-names></name><name><surname>Bezgin</surname><given-names>G</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Situating the default-mode network along a principal gradient of macroscale cortical organization</article-title><source>PNAS</source><volume>113</volume><fpage>12574</fpage><lpage>12579</lpage><pub-id pub-id-type="doi">10.1073/pnas.1608282113</pub-id><pub-id pub-id-type="pmid">27791099</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mashour</surname><given-names>GA</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Changeux</surname><given-names>JP</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Conscious processing and the global neuronal workspace hypothesis</article-title><source>Neuron</source><volume>105</volume><fpage>776</fpage><lpage>798</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.026</pub-id><pub-id pub-id-type="pmid">32135090</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuzawa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Use of numbers by a chimpanzee</article-title><source>Nature</source><volume>315</volume><fpage>57</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1038/315057a0</pub-id><pub-id pub-id-type="pmid">3990808</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuzawa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Symbolic representation of number in chimpanzees</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>92</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.04.007</pub-id><pub-id pub-id-type="pmid">19447029</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medam</surname><given-names>T</given-names></name><name><surname>Marzouki</surname><given-names>Y</given-names></name><name><surname>Montant</surname><given-names>M</given-names></name><name><surname>Fagot</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Categorization does not promote symmetry in Guinea baboons (Papio papio)</article-title><source>Animal Cognition</source><volume>19</volume><fpage>987</fpage><lpage>998</lpage><pub-id pub-id-type="doi">10.1007/s10071-016-1003-4</pub-id><pub-id pub-id-type="pmid">27278368</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mersad</surname><given-names>K</given-names></name><name><surname>Kabdebon</surname><given-names>C</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Explicit access to phonetic representations in 3-month-old infants</article-title><source>Cognition</source><volume>213</volume><elocation-id>104613</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2021.104613</pub-id><pub-id pub-id-type="pmid">33568329</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Statistical learning of visual transitions in monkey inferotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>19401</fpage><lpage>19406</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112895108</pub-id><pub-id pub-id-type="pmid">22084090</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Ramachandran</surname><given-names>S</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Statistical learning of serial visual transitions by neurons in monkey inferotemporal cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>9332</fpage><lpage>9337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1215-14.2014</pub-id><pub-id pub-id-type="pmid">25009266</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>MW</given-names></name><name><surname>Durisko</surname><given-names>C</given-names></name><name><surname>Perfetti</surname><given-names>CA</given-names></name><name><surname>Fiez</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning to read an alphabet of human faces produces left-lateralized training effects in the fusiform gyrus</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>896</fpage><lpage>913</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00506</pub-id><pub-id pub-id-type="pmid">24168219</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murayama</surname><given-names>T</given-names></name><name><surname>Suzuki</surname><given-names>R</given-names></name><name><surname>Kondo</surname><given-names>Y</given-names></name><name><surname>Koshikawa</surname><given-names>M</given-names></name><name><surname>Katsumata</surname><given-names>H</given-names></name><name><surname>Arai</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spontaneous establishing of cross-modal stimulus equivalence in a beluga whale</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>9914</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-09925-4</pub-id><pub-id pub-id-type="pmid">28855548</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubauer</surname><given-names>S</given-names></name><name><surname>Hublin</surname><given-names>JJ</given-names></name><name><surname>Gunz</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The evolution of modern human brain shape</article-title><source>Science Advances</source><volume>4</volume><elocation-id>eaao5961</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aao5961</pub-id><pub-id pub-id-type="pmid">29376123</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubert</surname><given-names>FX</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Thomas</surname><given-names>AG</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparison of human ventral frontal cortex areas for cognitive control and language with areas in monkey frontal cortex</article-title><source>Neuron</source><volume>81</volume><fpage>700</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.012</pub-id><pub-id pub-id-type="pmid">24485097</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Stein-Aviles</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Nonhuman primate models of visually based cognition</article-title><source>ILAR Journal</source><volume>39</volume><fpage>78</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1093/ilar.40.2.78</pub-id><pub-id pub-id-type="pmid">11533513</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Prefrontal cortex and the evolution of symbolic reference</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>99</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.04.008</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nieder</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>A Brain for Numbers: The Biology of the Number Instinct</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical pathways for music and speech revealed by hypothesis-free voxel decomposition</article-title><source>Neuron</source><volume>88</volume><fpage>1281</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id><pub-id pub-id-type="pmid">26687225</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogawa</surname><given-names>A</given-names></name><name><surname>Yamazaki</surname><given-names>Y</given-names></name><name><surname>Ueno</surname><given-names>K</given-names></name><name><surname>Cheng</surname><given-names>K</given-names></name><name><surname>Iriki</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural correlates of species-typical illogical cognitive bias in human inference</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>2120</fpage><lpage>2130</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21330</pub-id><pub-id pub-id-type="pmid">19702470</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pallier</surname><given-names>C</given-names></name><name><surname>Devauchelle</surname><given-names>AD</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical representation of the constituent structure of sentences</article-title><source>PNAS</source><volume>108</volume><fpage>2522</fpage><lpage>2527</lpage><pub-id pub-id-type="doi">10.1073/pnas.1018711108</pub-id><pub-id pub-id-type="pmid">21224415</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palomero-Gallagher</surname><given-names>N</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Differences in cytoarchitecture of Broca’s region between human, ape and macaque brains</article-title><source>Cortex</source><volume>118</volume><fpage>132</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.09.008</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Passingham</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>What Is Special About the Human Brain</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>Music, Language, and the Brain</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195123753.001.0001</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penn</surname><given-names>DC</given-names></name><name><surname>Holyoak</surname><given-names>KJ</given-names></name><name><surname>Povinelli</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Darwin’s mistake: explaining the discontinuity between human and nonhuman minds</article-title><source>The Behavioral and Brain Sciences</source><volume>31</volume><fpage>109</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1017/S0140525X08003543</pub-id><pub-id pub-id-type="pmid">18479531</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pepperberg</surname><given-names>IM</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>The Alex Studies: Cognitive and Communicative Abilities of Grey Parrots</source><publisher-name>Harvard University Press</publisher-name><pub-id pub-id-type="doi">10.1016/j.applanim.2006.04.005</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pepperberg</surname><given-names>IM</given-names></name><name><surname>Carey</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Grey parrot number acquisition: the inference of cardinal value from ordinal position on the numeral list</article-title><source>Cognition</source><volume>125</volume><fpage>219</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2012.07.003</pub-id><pub-id pub-id-type="pmid">22878117</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perszyk</surname><given-names>DR</given-names></name><name><surname>Waxman</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Infants’ advances in speech perception shape their earliest links between language and cognition</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>3293</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-39511-9</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Optimizing the imaging of the monkey auditory cortex: sparse vs. continuous fMRI</article-title><source>Magnetic Resonance Imaging</source><volume>27</volume><fpage>1065</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/j.mri.2009.01.018</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Tomaiuolo</surname><given-names>F</given-names></name><name><surname>Yeterian</surname><given-names>EH</given-names></name><name><surname>Pandya</surname><given-names>DN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The prefrontal cortex: comparative architectonic organization in the human and the macaque monkey brains</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>48</volume><fpage>46</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2011.07.002</pub-id><pub-id pub-id-type="pmid">21872854</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piazza</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neurocognitive start-up tools for symbolic number representations</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>542</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.09.008</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinel</surname><given-names>P</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Meriaux</surname><given-names>S</given-names></name><name><surname>Jobert</surname><given-names>A</given-names></name><name><surname>Serres</surname><given-names>J</given-names></name><name><surname>Le Bihan</surname><given-names>D</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Fast reproducible identification and large-scale databasing of individual functional cognitive networks</article-title><source>BMC Neuroscience</source><volume>8</volume><elocation-id>91</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-8-91</pub-id><pub-id pub-id-type="pmid">17973998</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Premack</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Language in chimpanzee?</article-title><source>Science</source><volume>172</volume><fpage>808</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1126/science.172.3985.808</pub-id><pub-id pub-id-type="pmid">5572906</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>D</given-names></name><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Suppressed sensory response to predictable object stimuli throughout the ventral visual stream</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7452</fpage><lpage>7461</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3421-17.2018</pub-id><pub-id pub-id-type="pmid">30030402</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rilling</surname><given-names>JK</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Preuss</surname><given-names>TM</given-names></name><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Hu</surname><given-names>X</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The evolution of the arcuate fasciculus revealed with comparative DTI</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>426</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/nn2072</pub-id><pub-id pub-id-type="pmid">18344993</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rilling</surname><given-names>JK</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Preuss</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Continuity, divergence, and the evolution of brain language pathways</article-title><source>Frontiers in Evolutionary Neuroscience</source><volume>3</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/fnevo.2011.00011</pub-id><pub-id pub-id-type="pmid">22319495</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rilling</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparative primate neuroimaging: insights into human brain evolution</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>46</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.09.013</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Basic neuroscience research with nonhuman primates: a small but indispensable component of biomedical research</article-title><source>Neuron</source><volume>82</volume><fpage>1200</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.06.003</pub-id><pub-id pub-id-type="pmid">24945764</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Fagot</surname><given-names>J</given-names></name><name><surname>Caparos</surname><given-names>S</given-names></name><name><surname>Van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensitivity to geometric shape regularity in humans and baboons: A putative signature of human singularity</article-title><source>PNAS</source><volume>118</volume><elocation-id>2023123118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2023123118</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schenker</surname><given-names>NM</given-names></name><name><surname>Hopkins</surname><given-names>WD</given-names></name><name><surname>Spocter</surname><given-names>MA</given-names></name><name><surname>Garrison</surname><given-names>AR</given-names></name><name><surname>Stimpson</surname><given-names>CD</given-names></name><name><surname>Erwin</surname><given-names>JM</given-names></name><name><surname>Hof</surname><given-names>PR</given-names></name><name><surname>Sherwood</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Broca’s area homologue in chimpanzees (<italic>Pan troglodytes</italic>): probabilistic mapping, asymmetry, and comparison to humans</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>730</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp138</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schusterman</surname><given-names>RJ</given-names></name><name><surname>Kastak</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Functional equivalence in a California sea lion: relevance to animal social and communicative interactions</article-title><source>Animal Behaviour</source><volume>55</volume><fpage>1087</fpage><lpage>1095</lpage><pub-id pub-id-type="doi">10.1006/anbe.1997.0654</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>BH</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Auditory short-term memory in the primate auditory cortex</article-title><source>Brain Research</source><volume>1640</volume><fpage>264</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2015.10.048</pub-id><pub-id pub-id-type="pmid">26541581</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sestieri</surname><given-names>C</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention to memory and the environment: functional specialization and dynamic competition in human posterior parietal cortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>8445</fpage><lpage>8456</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4719-09.2010</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seyfarth</surname><given-names>RM</given-names></name><name><surname>Cheney</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>The evolution of concepts about agents: or, what do animals recognize when they recognize an individual</chapter-title><person-group person-group-type="editor"><name><surname>Margolis</surname><given-names>E</given-names></name><name><surname>Laurence</surname><given-names>S</given-names></name></person-group><source>The Conceptual Mind</source><publisher-name>The MIT Press</publisher-name><fpage>57</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9383.003.0007</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shibata</surname><given-names>M</given-names></name><name><surname>Pattabiraman</surname><given-names>K</given-names></name><name><surname>Lorente-Galdos</surname><given-names>B</given-names></name><name><surname>Andrijevic</surname><given-names>D</given-names></name><name><surname>Kim</surname><given-names>SK</given-names></name><name><surname>Kaur</surname><given-names>N</given-names></name><name><surname>Muchnik</surname><given-names>SK</given-names></name><name><surname>Xing</surname><given-names>X</given-names></name><name><surname>Santpere</surname><given-names>G</given-names></name><name><surname>Sousa</surname><given-names>AMM</given-names></name><name><surname>Sestan</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Regulation of prefrontal patterning and connectivity by retinoic acid</article-title><source>Nature</source><volume>598</volume><fpage>483</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03953-x</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shum</surname><given-names>J</given-names></name><name><surname>Hermes</surname><given-names>D</given-names></name><name><surname>Foster</surname><given-names>BL</given-names></name><name><surname>Dastjerdi</surname><given-names>M</given-names></name><name><surname>Rangarajan</surname><given-names>V</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A brain area for visual numerals</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>6709</fpage><lpage>6715</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4558-12.2013</pub-id><pub-id pub-id-type="pmid">23595729</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sidman</surname><given-names>M</given-names></name><name><surname>Rauzin</surname><given-names>R</given-names></name><name><surname>Lazar</surname><given-names>R</given-names></name><name><surname>Cunningham</surname><given-names>S</given-names></name><name><surname>Tailby</surname><given-names>W</given-names></name><name><surname>Carrigan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>A search for symmetry in the conditional discriminations of rhesus monkeys, baboons, and children</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>37</volume><fpage>23</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1901/jeab.1982.37-23</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sliwa</surname><given-names>J</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Pascalis</surname><given-names>O</given-names></name><name><surname>Wirth</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spontaneous voice–face identity matching by rhesus monkeys for familiar conspecifics and humans</article-title><source>PNAS</source><volume>108</volume><fpage>1735</fpage><lpage>1740</lpage><pub-id pub-id-type="doi">10.1073/pnas.1008169108</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smaers</surname><given-names>JB</given-names></name><name><surname>Gómez-Robles</surname><given-names>A</given-names></name><name><surname>Parks</surname><given-names>AN</given-names></name><name><surname>Sherwood</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Exceptional evolutionary expansion of prefrontal cortex in great apes and humans</article-title><source>Current Biology</source><volume>27</volume><fpage>714</fpage><lpage>720</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.01.020</pub-id><pub-id pub-id-type="pmid">28162899</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Tian</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Top-down processing of symbolic meanings modulates the visual word form area</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12277</fpage><lpage>12283</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1874-12.2012</pub-id><pub-id pub-id-type="pmid">22933809</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spelke</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><chapter-title>What makes us smart? core knowledge and natural language</chapter-title><person-group person-group-type="editor"><name><surname>Gentner</surname><given-names>D</given-names></name><name><surname>Goldin-Meadow</surname><given-names>S</given-names></name></person-group><source>Language in Mind</source><publisher-name>MIT Press</publisher-name><fpage>277</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.7551/mitpress/4117.003.0017</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Mandeville</surname><given-names>JB</given-names></name><name><surname>Morocz</surname><given-names>IA</given-names></name><name><surname>Sullivan</surname><given-names>KJ</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Behavioral and anatomical consequences of early versus late symbol training in macaques</article-title><source>Neuron</source><volume>73</volume><fpage>608</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.022</pub-id><pub-id pub-id-type="pmid">22325210</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takemura</surname><given-names>H</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Keliris</surname><given-names>GA</given-names></name><name><surname>Landi</surname><given-names>SM</given-names></name><name><surname>Sliwa</surname><given-names>J</given-names></name><name><surname>Ye</surname><given-names>FQ</given-names></name><name><surname>Barnett</surname><given-names>MA</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Occipital white matter tracts in human and macaque</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>3346</fpage><lpage>3359</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx070</pub-id><pub-id pub-id-type="pmid">28369290</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tasserie</surname><given-names>J</given-names></name><name><surname>Grigis</surname><given-names>A</given-names></name><name><surname>Uhrig</surname><given-names>L</given-names></name><name><surname>Dupont</surname><given-names>M</given-names></name><name><surname>Amadon</surname><given-names>A</given-names></name><name><surname>Jarraya</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pypreclin: An automatic pipeline for macaque functional MRI preprocessing</article-title><source>NeuroImage</source><volume>207</volume><elocation-id>116353</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116353</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiebaut de Schotten</surname><given-names>M</given-names></name><name><surname>Dell’Acqua</surname><given-names>F</given-names></name><name><surname>Valabregue</surname><given-names>R</given-names></name><name><surname>Catani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Monkey to human comparative anatomy of the frontal lobe association tracts</article-title><source>Cortex</source><volume>48</volume><fpage>82</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2011.10.001</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhrig</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Jarraya</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A hierarchy of responses to auditory regularities in the macaque brain</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>1127</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3165-13.2014</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Mandeville</surname><given-names>JB</given-names></name><name><surname>Nelissen</surname><given-names>K</given-names></name><name><surname>Van Hecke</surname><given-names>P</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual motion processing investigated using contrast agent-enhanced fMRI in awake behaving monkeys</article-title><source>Neuron</source><volume>32</volume><fpage>565</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00502-5</pub-id><pub-id pub-id-type="pmid">11719199</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Peuskens</surname><given-names>H</given-names></name><name><surname>Denys</surname><given-names>K</given-names></name><name><surname>Sunaert</surname><given-names>S</given-names></name><name><surname>Todd</surname><given-names>JT</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Extracting 3D from motion: differences in human and monkey intraparietal cortex</article-title><source>Science</source><volume>298</volume><fpage>413</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1126/science.1073574</pub-id><pub-id pub-id-type="pmid">12376701</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergnieux</surname><given-names>V</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Statistical learning signals for complex visual images in macaque early visual cortex</article-title><source>Frontiers in Neuroscience</source><volume>14</volume><elocation-id>789</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00789</pub-id><pub-id pub-id-type="pmid">32848562</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Uhrig</surname><given-names>L</given-names></name><name><surname>Jarraya</surname><given-names>B</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Representation of numerical and sequential patterns in macaque and human brains</article-title><source>Current Biology</source><volume>25</volume><fpage>1966</fpage><lpage>1974</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.06.035</pub-id><pub-id pub-id-type="pmid">26212883</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name><name><surname>Figueira</surname><given-names>S</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Representation of spatial sequences using nested rules in human prefrontal cortex</article-title><source>NeuroImage</source><volume>186</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.10.061</pub-id><pub-id pub-id-type="pmid">30449729</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Possibly unique characteristics of learning by Primates</article-title><source>Journal of Human Evolution</source><volume>3</volume><fpage>445</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/0047-2484(74)90004-9</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikman</surname><given-names>P</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reward cues readily direct monkeys’ auditory performance resulting in broad auditory cortex modulation and interaction with sites along cholinergic and dopaminergic pathways</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>3055</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-38833-y</pub-id><pub-id pub-id-type="pmid">30816142</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>B</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Conserved sequence processing in primate frontal cortex</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>72</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2016.11.004</pub-id><pub-id pub-id-type="pmid">28063612</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Forward frontal fields: phylogeny and fundamental function</article-title><source>Trends in Neurosciences</source><volume>31</volume><fpage>599</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.08.008</pub-id><pub-id pub-id-type="pmid">18835649</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>DL</given-names></name><name><surname>Herron</surname><given-names>TJ</given-names></name><name><surname>Cate</surname><given-names>AD</given-names></name><name><surname>Yund</surname><given-names>EW</given-names></name><name><surname>Stecker</surname><given-names>GC</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Kang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional properties of human auditory cortical fields</article-title><source>Frontiers in Systems Neuroscience</source><volume>4</volume><elocation-id>155</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2010.00155</pub-id><pub-id pub-id-type="pmid">21160558</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>F</given-names></name><name><surname>Cote</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Labeling guides object individuation in 12-month-old infants</article-title><source>Psychological Science</source><volume>16</volume><fpage>372</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.01543.x</pub-id><pub-id pub-id-type="pmid">15869696</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ontogeny and phylogeny of language</article-title><source>PNAS</source><volume>110</volume><fpage>6324</fpage><lpage>6327</lpage><pub-id pub-id-type="doi">10.1073/pnas.1216803110</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Zhen</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Long</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Working memory for spatial sequences: developmental and evolutionary factors in encoding ordinal and relational structures</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>850</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0603-21.2021</pub-id><pub-id pub-id-type="pmid">34862186</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87380.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Max Planck Institute for Psycholinguistics</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>fMRI was used to address an <bold>important</bold> aspect of human cognition - the capacity for structured representations and symbolic processing - in a cross-species comparison with macaques; the experimental design probed implicit symbolic processing through reversal of learned stimulus pairs. The authors present <bold>solid</bold> evidence in humans that helps elucidate the role of brain networks in symbolic processing, however the evidence from macaques was necessarily incomplete (e.g., hard-to-quantify differences in learning trajectories and lived experience between species).</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87380.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Kerkoerle and colleagues present a very interesting comparative fMRI study in humans and monkeys, assessing neural responses to surprise reactions at the reversal of a previously learned association. The implicit nature of this task, assessing how this information is represented without requiring explicit decision making, is an elegant design. The paper reports that both humans and monkeys show neural responses across a range of areas when presented with incongruous stimulus pairs. Monkeys also show a surprise response when the stimuli are presented in the reversed direction. However, humans show no such surprise response based on this reversal, suggesting that they encode the relationship reversibly and bidirectionally, unlike the monkeys. This has been suggested as a hallmark of symbolic representation, that might be absent in nonhuman animals.</p><p>I find this experiment and the results quite compelling, and the data do support the hypothesis that humans are somewhat unique in their tendency to form reversible, symbolic associations. I think that an important strength of the results is that the critical finding is the presence of an interaction between congruity and canonicity in macaques, which does not appear in humans. These results go a long way to allay concerns I have about the comparison of many human participants to a very small number of macaques.</p><p>The results do appear to show that macaques show the predicted interaction effect (even despite the sample size), while humans do not. I think this is quite convincing. (Although had the results turned out differently (for example an effect in humans that was absent in macaques), I think this difference in sample size would be considerably more concerning.)</p><p>I would also note that while I agree with the authors conclusions, it is also notable to me that the congruity effect observed in humans (red vs blue lines in Fig. 2B) appears to be far more pronounced than any effect observed in the macaques (Fig. 3C-3). Again, this does not challenge the core finding of this paper but does suggest methodological or possibly motivational/attentional differences between the humans and the monkeys (or, for example, that the monkeys had learned the associations less strongly and clearly than the humans). The authors now discuss this more fully.</p><p>This is a strong paper with elegant methods and makes a worthwhile contribution to our understanding of the neural systems supporting symbolic representations in humans, as opposed to other animals.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87380.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In their article titled, van Kerkoerle et al address the timely question of whether non-human primates (rhesus macaques) possess the ability for reverse symbolic inference as observed in humans. Through an fMRI experiment in both humans and monkeys, they analyzed the bold signal in both species while observing audio-visual and visual-visual stimuli pairs that had been previously learned in a particular direction. Remarkably, the findings pertaining to humans revealed that a broad brain network exhibited increased activity in response to surprises occurring in both the learned and reverse directions. Conversely, in monkeys, the study uncovered that the brain activity within sensory areas only responded to the learned direction but failed to exhibit any discernible response to the reverse direction. These compelling results indicate that the capacity for reversible symbolic inference may be specific to humans, even though it remains to be tested in other species.</p><p>In general, the manuscript is skillfully crafted and highly accessible to readers. The experimental design exhibits originality, and the analyses are tailored to effectively address the central question at hand. Although the first experiment raised a number of methodological inquiries, the subsequent second experiment thoroughly addresses these concerns and effectively replicates the initial findings, thereby significantly strengthening the overall study. Overall, this article is of high quality and brings new insight into human cognition.</p><p>The main limitation of the studies is the sample size of the non-human primate group (n=2 and n=3). Nevertheless, this limitation is carefully addressed and discussed in the manuscript.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87380.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Original review</p><p>This study investigates the hypothesis that humans (but not non-human primates) spontaneously learn reversible temporal associations (i.e., learning a B-A association after only being exposed to A-B sequences), which the authors consider to be a foundational property of symbolic cognition. To do so, they expose humans and macaques to 2-item sequences (in a visual-auditory experiment, pairs of images and spoken nonwords, and in a visual-visual experiment, pairs of images and abstract geometric shapes) in a fixed temporal order, then measure the brain response during a test phase to congruent vs. incongruent pairs (relative to the trained associations) in canonical vs. reversed order (relative to the presentation order used in training). The advantage of neuroimaging for this question is that it removes the need for a behavioral test, which non-human primates can fail for reasons unrelated to the cognitive construct being investigated. In humans, the researchers find statistically indistinguishable incongruity effects in both directions (supporting a spontaneous reversible association), whereas in monkeys they only find incongruity effects in the canonical direction (supporting an association but a lack of spontaneous reversal). Although the precise pattern of activation varies by experiment type (visual-auditory vs. visual-visual) in both species, the authors point out that some of the regions involved are also those that are most anatomically different between humans and other primates. The authors interpret their findings to support the hypothesis that reversible associations, and by extension symbolic cognition, is uniquely human.</p><p>This study is a valuable complement to prior behavioral work on this question. However, I have some concerns about methods and framing.</p><p>Methods - Design issues:</p><p>(1) The authors originally planned to use the same training/testing protocol for both species but the monkeys did not learn anything, so they dramatically increased the amount of training and evaluation. By my calculation from the methods section, humans were trained on 96 trials and tested on 176, whereas the monkeys got an additional 3,840 training trials and 1,408 testing trials. The authors are explicit that they continued training the monkeys until they got a congruity effect. On the one hand, it is commendable that they are honest about this in their write-up, given that this detail could easily be framed as deliberate after the fact. On the other hand, it is still a form of p-hacking, given that it's critical for their result that the monkeys learn the canonical association (otherwise, the critical comparison to the non-canonical association is meaningless).</p><p>(2) Between-species comparisons are challenging. In addition to having differences in their DNA, human participants have spent many years living in a very different culture than that of NHPs, including years of formal education. As a result, attributing the observed differences to biology is challenging. One approach that has been adopted in some past studies is to examine either young children or adults from cultures that don't have formal educational structures. This is not the approach the authors take. This major confound needs to minimally be explicitly acknowledged up front.</p><p>(3) Humans have big advantages in processing and discriminating spoken stimuli and associating them to visual stimuli (after all, this is what words are in spoken human languages). Experiment 2 ameliorates these concerns to some degree, but still it is difficult to attribute the failure of NHPs to show reversible associations in Experiment 1 to cognitive differences rather than the relative importance of sound string to meaning associations in the human vs. NHP experiences.</p><p>(4) More minor: The localizer task (math sentences vs. other sentences) makes sense for math but seems to make less sense for language: why would a language region respond more to sentences that don't describe math vs. ones that do?</p><p>Methods - Analysis issues:</p><p>(5) The analyses appear to &quot;double dip&quot; by using the same data to define the clusters and to statistically test the average cluster activation (Kriegeskorte et al., 2009). The resulting effect sizes are therefore likely inflated, and the p-values are anticonservative.</p><p>FRAMING:</p><p>(6) The framing (&quot;Brain mechanisms of reversible symbolic reference: A potential singularity of the human brain&quot;) is bigger than the finding (monkeys don't spontaneously reverse a temporal association but humans do). The title and discussion are full of buzzy terms (&quot;brain mechanisms&quot;, &quot;symbolic&quot;, and &quot;singularity&quot;) that are only connected to the experiments by a debatable chain of assumptions.</p><p>First, this study shows relatively little about brain &quot;mechanisms&quot; of reversible symbolic associations, which implies insights about how these associations are learned, recognized, and represented. But we're only given standard fMRI analyses that are quite inconsistent across similar experimental paradigms, with purely suggestive connections between these spatial patterns and prior work on comparative brain anatomy.</p><p>Second, it's not clear what the relationship is between symbolic cognition and a propensity to spontaneously reverse a temporal association. Certainly if there are inter-species differences in learning preferences this is important to know about, but why is this construed as a difference in the presence or absence of symbols? Because the associations aren't used in any downstream computation, there is not even any way for participants to know which is the sign and which is the signified: these are merely labels imposed by the researchers on a sequential task.</p><p>Third, the word &quot;singularity&quot; is both problematically ambiguous and not well supported by the results. &quot;Singularity&quot; is a highly loaded word that the authors are simply using to mean &quot;that which is uniquely human&quot;. Rather than picking a term with diverse technical meanings across fields and then trying to restrict the definition, it would be better to use a different term. Furthermore, even under the stated definition, this study performed a single pairwise comparison between humans and one other species (macaques), so it is a stretch to then conclude (or insinuate) that the &quot;singularity&quot; has been found (see also pt. 2 above).</p><p>(7) Related to pt. 6, there is circularity in the framing whereby the authors say they are setting out to find out what is uniquely human, hypothesizing that the uniquely human thing is symbols, and then selecting a defining trait of symbols (spontaneous reversible association) *because* it seems to be uniquely human (see e.g., &quot;Several studies previously found behavioral evidence for a uniquely human ability to spontaneously reverse a learned association (Imai et al., 2021; Kojima, 1984; Lipkens et al., 1988; Medam et al., 2016; Sidman et al., 1982), and such reversibility was therefore proposed as a defining feature of symbol representation reference (Deacon, 1998; Kabdebon and Dehaene-Lambertz, 2019; Nieder, 2009).&quot;, line 335). They can't have it both ways. Either &quot;symbol&quot; is an independently motivated construct whose presence can be independently tested in humans and other species, or it is by fiat synonymous with the &quot;singularity&quot;. This circularity can be broken by a more modest framing that focuses on the core research question (e.g., &quot;What is uniquely human? One possibility is spontaneous reversal of temporal associations.&quot;) and then connects (speculatively) to the bigger conceptual landscape in the discussion (&quot;Spontaneous reversal of temporal associations may be a core ability underlying the acquisition of mental symbols&quot;).</p><p>Comments on revised version:</p><p>I thank the authors for engaging constructively with my comments. I'm convinced by the responses to my original points 1, 2, 3, and 4. I'm also partially convinced by the response to point 6 (with qualifications discussed below). I do want to clear the record on points 1 and 6 (about which the authors expressed offense at aspects of my original comments), and to press on points 5 and 7.</p><p>(1) It's very helpful to know that the plan was always to extend training in Expt 1. The rationale is now clear in the methods, although I'd encourage the authors to also emphasize this if space permits in the vicinity of lines 211-216, which still read as if the extended training was a post hoc decision (&quot;the canonical congruity effect... was not significant... after 3 days of exposure... Thus... monkeys were further exposed...&quot;). The authors have objected to my original use of &quot;p hacking&quot;, which I agree was too strong (my apologies). My intention was only to point out that *if it were the case that training duration was conditional on the monkeys' success at learning the canonical association* (which the authors have now clarified was not the case), then this would be steering the study post hoc to achieve a desired outcome. I recognize the authors' point that the canonical direction was a sanity check, not the effect of interest (reversed association), but it's still true that they needed to achieve this sanity check in order for the absence of a reversed effect to be meaningful. This was the source of my original concern. This point is only clarificational (no action is recommended).</p><p>(5) The authors have said they don't understand my concern about &quot;double-dipping&quot; in the statistical analyses, so I will attempt to clarify. First, I should stress that this concern applies only to the whole-brain results (Tables 1-4), not the fROI results. As the authors point out, this was indeed unclear, and I apologize. My concern about Tables 1-4 is that they seem to be derived using the classical technique of thresholding contrasts at some significance level to define clusters and then reporting cluster statistics (in this case, t-values) derived from *the same contrast in the same activation maps*. If this is not what was done (i.e., if orthogonal data and/or contrasts were used to define clusters and quantify contrasts within clusters, as in the fROI analyses), then this point is moot (and clarification in the paper would be helpful). But if this is what was done, then this procedure is known to be distortionary (e.g., Kriegeskorte et al 2009, &quot;Nonindependent selective analysis is incorrect and should not be acceptable in neuroscientific publications&quot;).</p><p>(6) The authors have objected to my use of the term &quot;insinuate&quot; as pejorative. I don't share this impression (and insult was certainly not my intent) but I'm happy to concede that a less loaded term (e.g., &quot;suggest&quot;) would have been a better choice. I apologize. In any case, I stand by my intended original concern that a key idea in this piece (that reversible symbolic inference is a singularity of the human brain) is being advanced rhetorically rather than empirically, by repeatedly supplying it to readers (albeit with qualifiers like &quot;potential&quot;) as an interpretive lens through which to view empirical results that only directly support a more modest claim (that macaques spontaneously reverse sequential associations less readily than humans do). To be clear, it is good that the authors don't make this stronger claim outright, and it is fine to motivate a more modest research question (e.g., do species differ in spontaneous reversal of associations) on the grounds that it is a stepping stone to a bigger one (what is the singularity). But by placing the bigger framing front and center in this way, there's a risk that this paper will be received by the community as establishing a conclusion that it does not actually establish.</p><p>(7) The authors have said they don't understand the circularity I'm alleging. Having read the revision, I believe the issue is still there, so I'll make another attempt. The problem is most clearly apparent in the Discussion text quoted in my original comment (lines 347-350 of the revision, emphasis mine): &quot;Several studies previously found behavioural evidence for a *uniquely human* ability to spontaneously reverse a learned association (Imai et al., 2021; Kojima, 1984; Lipkens et al., 1988; Medam et al., 2016; Sidman et al., 1982), and such reversibility was *therefore* proposed as a defining feature of symbol representation reference (Deacon, 1998; Kabdebon and Dehaene-Lambertz, 2019; Nieder, 2009).&quot; In other words, reversal of associations is selected as a defining feature of symbols and targeted by this study *because* it is thought to be uniquely human. This is fine, but it prohibits you from then advocating the hypothesis that symbolic cognition is the singularity (lines 49-52), because &quot;symbol&quot; is being defined such that this is necessarily the case. To minimally paraphrase what I perceive to be the circular logic in the framing, the argument seems to go: &quot;What is uniquely human? Symbols. What are symbols? That which is uniquely human.&quot; In my original comment, I suggested a reframing that would fix this issue, namely: &quot;What is uniquely human? Spontaneous reversal of temporal associations.&quot; The authors say they don't see the difference between this framing and their own, so I'll try to clarify: the difference is that it sidesteps the notion of &quot;symbol&quot;, and in so doing removes the circular definitions of &quot;symbol&quot; and &quot;singularity&quot; in terms of each other. This suggestion was given not as a prescription but as an example to show that the issue can be remedied by revisions to the framing without doing damage to the empirical claims. If the authors prefer a different remedy that avoids circular definitions of terms, that's fine.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87380.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kerkoerle</surname><given-names>van Timo</given-names></name><role specific-use="author">Author</role><aff><institution>Radboud University Nijmegen</institution><addr-line><named-content content-type="city">Gif sur Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Pape</surname><given-names>Louise</given-names></name><role specific-use="author">Author</role><aff><institution>Radboud University Nijmegen</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Ekramnia</surname><given-names>Milad</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin center</institution><addr-line><named-content content-type="city">Gif/Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Xiaoxia</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin center</institution><addr-line><named-content content-type="city">Gif/Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Tasserie</surname><given-names>Jordy</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin center</institution><addr-line><named-content content-type="city">Gif/Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Dupont</surname><given-names>Morgan</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin center</institution><addr-line><named-content content-type="city">Gif/Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xiaolian</given-names></name><role specific-use="author">Author</role><aff><institution>Laboratory for Neuro-and Psychophysiology</institution><addr-line><named-content content-type="city">3000 Leuven</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Jarraya</surname><given-names>Béchir</given-names></name><role specific-use="author">Author</role><aff><institution>CEA - Université Paris-Saclay</institution><addr-line><named-content content-type="city">Saclay</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Vanduffel</surname><given-names>Vim</given-names></name><role specific-use="author">Author</role><aff><institution>KU Leuven Medical School</institution><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff></contrib><contrib contrib-type="author"><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name><role specific-use="author">Author</role><aff><institution>Inserm</institution><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name><role specific-use="author">Author</role><aff><institution>Université Paris- Saclay</institution><addr-line><named-content content-type="city">Paris-Saclay</named-content></addr-line><country>France</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>fMRI was used to address an important aspect of human cognition - the capacity for structured representations and symbolic processing - in a cross-species comparison with non-human primates (macaques); the experimental design probed implicit symbolic processing through reversal of learned stimulus pairs. The authors present solid evidence in humans that helps elucidate the role of brain networks in symbolic processing, however the evidence from macaques was incomplete (e.g., sample size constraints, potential and hard-to-quantify differences in attention allocation, motivation, and lived experience between species).</p></disp-quote><p>Thank you very much for your assessment. We would like to address the potential issues that you raise point-by-point below.</p><p>We agree that for macaque monkey physiology, sample size is always a constraint, due to both financial and ethical reasons. We addressed this concern by combining the results from two different labs, which allowed us to test 4 animals in total, which is twice as much as what is common practice in the field of primate physiology. (We discuss this now on lines 473-478.)</p><p>Interspecies differences in motivation, attention allocation, task strategies etc. could also be limiting factors. Note that we did address the potential lack of attention allocation directly in Experiment 2 using implicit reward association, which was successful as evidenced by the activation of attentional control areas in the prefrontal cortex. We cannot guarantee that the strategies that the two species deploy are identical, but we tentatively suggest that this might be a less important factor in the present study than in other interspecies comparisons that use explicit behavioral reports. In the current study, we directly measured surprise responses in the brain in the absence of any explicit instructions in either species, which allowed us to measure the spontaneous reversal of learned associations, which is a very basic element of symbolic representation. Our reasoning is that such spontaneous responses should be less dependent on attention allocation and task strategies. (We discuss this now in more detail on lines 478-485.)</p><p>Finally, lived experience could be a major factor. Indeed, obvious differences include a lifetime of open-field experiences and education in our human adult subjects, which was not available to the monkey subjects, and includes a strong bias towards explicit learning of symbolic systems (e.g. words, letters, digits, etc). However, we have previously shown that 5-month-old human infants spontaneously generalize learning to the reversed pairs after a short learning in the lab using EEG (Kabdebon et al, PNAS, 2019). This indicates that also with very limited experience, humans spontaneously reverse learned associations. (We discuss this now in more detail on lines 478-485.) It could be very interesting to investigate whether spontaneous reversal could be present in infant macaque monkeys, as there might be a critical period for this effect. Although neurophysiology in awake infant monkeys is highly challenging, it would be very relevant for future work. (We discuss this in more detail on lines 493-498.)</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Kerkoerle and colleagues present a very interesting comparative fMRI study in humans and monkeys, assessing neural responses to surprise reactions at the reversal of a previously learned association. The implicit nature of this task, assessing how this information is represented without requiring explicit decision-making, is an elegant design. The paper reports that both humans and monkeys show neural responses across a range of areas when presented with incongruous stimulus pairs. Monkeys also show a surprise response when the stimuli are presented in a reversed direction. However, humans show no such surprise response based on this reversal, suggesting that they encode the relationship reversibly and bidirectionally, unlike the monkeys. This has been suggested as a hallmark of symbolic representation, that might be absent in nonhuman animals.</p><p>I find this experiment and the results quite compelling, and the data do support the hypothesis that humans are somewhat unique in their tendency to form reversible, symbolic associations. I think that an important strength of the results is that the critical finding is the presence of an interaction between congruity and canonicity in macaques, which does not appear in humans. These results go a long way to allay concerns I have about the comparison of many human participants to a very small number of macaques.</p></disp-quote><p>We thank the reviewer for the positive assessment. We also very much appreciate the point about the interaction effect in macaque monkeys – indeed, we do not report just a negative finding.</p><disp-quote content-type="editor-comment"><p>I understand the impossibility of testing 30+ macaques in an fMRI experiment. However, I think it is important to note that differences necessarily arise in the analysis of such datasets. The authors report that they use '...identical training, stimuli, and whole-brain fMRI measures'. However, the monkeys (in experiment 1) actually required 10 times more training.</p></disp-quote><p>We agree that this description was imprecise. We have changed it to “identical training stimuli” (line 151), indeed the movies used for training were strictly identical. Furthermore, please note that we do report the fMRI results after the same training duration. In experiment 1, after 3 days of training, the monkeys did not show any significant results, even in the canonical direction. However, in experiment 2, with increased attention and motivation, a significant effect was observed on the first day of scanning after training, as was found in human subjects (see Figure 4 and Table 3).</p><disp-quote content-type="editor-comment"><p>More importantly, while the fMRI measures are the same, group analysis over 30+ individuals is inherently different from comparing only 2 macaques (including smoothing and averaging away individual differences that might be more present in the monkeys, due to the much smaller sample size).</p></disp-quote><p>Thank you for understanding that a limited sampling size is intrinsic to macaque monkey physiology. We also agree that data analysis in humans and monkeys is necessarily different. As suggested by the reviewer, we added an analysis to address this, see the corresponding reply to the ‘Recommendations for the authors’ section below.</p><disp-quote content-type="editor-comment"><p>Despite this, the results do appear to show that macaques show the predicted interaction effect (even despite the sample size), while humans do not. I think this is quite convincing, although had the results turned out differently (for example an effect in humans that was absent in macaques), I think this difference in sample size would be considerably more concerning.</p></disp-quote><p>Thank you for noting this. Indeed, the interaction effect is crucial, and the task design was explicitly made to test this precise prediction, described in our manuscript as the “reversibility hypothesis”. The congruity effect in the learned direction served as a control for learning, while the corresponding congruity effect in the reversed direction tested for spontaneous reversal. The reversibility hypothesis stipulates that in humans there should not be a difference between the learned and the reversed direction, while there should be for monkeys. We already wrote about that in the result section of the original manuscript and now also describe this more explicitly in the introduction and beginning of the result section.</p><disp-quote content-type="editor-comment"><p>I would also note that while I agree with the authors' conclusions, it is notable to me that the congruity effect observed in humans (red vs blue lines in Fig. 2B) appears to be far more pronounced than any effect observed in the macaques (Fig. 3C-3). Again, this does not challenge the core finding of this paper but does suggest methodological or possibly motivational/attentional differences between the humans and the monkeys (or, for example, that the monkeys had learned the associations less strongly and clearly than the humans).</p></disp-quote><p>As also explained in response to the eLife assessment above, we expanded the “limitations” section of the discussion, with a deeper description of the possible methodological differences between the two species (see lines 478-485).</p><p>With the same worry in mind, we did increase the attention and motivation of monkeys in experiment 2, and indeed obtained a greater activation to the canonical pairs and their violation, -notably in the prefrontal cortex – but crucially still without reversibility.</p><p>In the end, we believe that the striking interspecies difference in size and extent of the violation effect, even for purely canonical stimuli, is an important part of our findings and points to a more efficient species-specific learning system, that our experiment tentatively relates to a symbolic competence.</p><disp-quote content-type="editor-comment"><p>This is a strong paper with elegant methods and makes a worthwhile contribution to our understanding of the neural systems supporting symbolic representations in humans, as opposed to other animals.</p></disp-quote><p>We again thank the reviewer for the positive review.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>In their article titled &quot;Brain mechanisms of reversible symbolic reference: a potential singularity of the human brain&quot;, van Kerkoerle et al address the timely question of whether non-human primates (rhesus macaques) possess the ability for reverse symbolic inference as observed in humans. Through an fMRI experiment in both humans and monkeys, they analyzed the bold signal in both species while observing audio-visual and visual-visual stimuli pairs that had been previously learned in a particular direction. Remarkably, the findings pertaining to humans revealed that a broad brain network exhibited increased activity in response to surprises occurring in both the learned and reverse directions. Conversely, in monkeys, the study uncovered that the brain activity within sensory areas only responded to the learned direction but failed to exhibit any discernible response to the reverse direction. These compelling results indicate that the capacity for reversible symbolic inference may be unique to humans.</p><p>In general, the manuscript is skillfully crafted and highly accessible to readers. The experimental design exhibits originality, and the analyses are tailored to effectively address the central question at hand.</p><p>Although the first experiment raised a number of methodological inquiries, the subsequent second experiment thoroughly addresses these concerns and effectively replicates the initial findings, thereby significantly strengthening the overall study. Overall, this article is already of high quality and brings new insight into human cognition.</p></disp-quote><p>We sincerely thank the reviewer for the positive comments.</p><disp-quote content-type="editor-comment"><p>I identified three weaknesses in the manuscript:</p><p>- One major issue in the study is the absence of significant results in monkeys. Indeed, authors draw conclusions regarding the lack of significant difference in activity related to surprise in the multidemand network (MDN) in the reverse congruent versus reverse incongruent conditions. Although the results are convincing (especially with the significant interaction between congruency and canonicity), the article could be improved by including additional analyses in a priori ROI for the MDN in monkeys (as well as in humans, for comparison).</p></disp-quote><p>First, we disagree with the statement about “absence of significant results in monkeys”. We do report a significant interaction which, as noted by the referee, is a crucial positive finding.</p><p>Second, we performed the suggested analysis for experiment 2, using the bilateral ROIs of the putative monkey MDN from previous literature (Mitchell, et al. 2016), which are based on the human study by Fedorenko et al. (PNAS, 2013).</p><table-wrap id="sa4table1" position="float"><label>Author response table 1.</label><caption><title>Congruity effect for monkeys in Experiment 2 within the ROIs of the MDN (n=3).</title><p>Significance was assessed with one-sided one-sample t-tests.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom"/><th valign="bottom">Congruity effect canonical trials</th><th valign="bottom"/><th valign="bottom"/><th valign="bottom">Congruity effect reversed trials</th><th valign="bottom"/><th valign="bottom"/><th valign="bottom">Interaction effect congruity X canonicity</th><th valign="bottom"/><th valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom">region</td><td align="left" valign="bottom">t-values</td><td align="left" valign="bottom">p -values</td><td align="left" valign="bottom">FDR p-values</td><td align="left" valign="bottom">t-values</td><td align="left" valign="bottom">p-values</td><td align="left" valign="bottom">FDR p-values</td><td align="left" valign="bottom">t-values</td><td align="left" valign="bottom">p -values</td><td align="left" valign="bottom">FDR p-values</td></tr><tr><td align="left" valign="bottom">insular</td><td align="char" char="." valign="bottom">0.84</td><td align="char" char="." valign="bottom">0.20</td><td align="char" char="." valign="bottom">0.23</td><td align="char" char="." valign="bottom">-1.04</td><td align="char" char="." valign="bottom">0.85</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">1.36</td><td align="char" char="." valign="bottom">0.09</td><td align="char" char="." valign="bottom">0.17</td></tr><tr><td align="left" valign="bottom">lateral fissure</td><td align="char" char="." valign="bottom">1.67</td><td align="char" char="." valign="bottom">0.05</td><td align="char" char="." valign="bottom">0.19</td><td align="char" char="." valign="bottom">-0.28</td><td align="char" char="." valign="bottom">0.61</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">1.25</td><td align="char" char="." valign="bottom">0.11</td><td align="char" char="." valign="bottom">0.17</td></tr><tr><td align="left" valign="bottom">lateral frontal middle</td><td align="char" char="." valign="bottom">1.02</td><td align="char" char="." valign="bottom">0.15</td><td align="char" char="." valign="bottom">0.23</td><td align="char" char="." valign="bottom">-1.36</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">1.66</td><td align="char" char="." valign="bottom">0.05</td><td align="char" char="." valign="bottom">0.17</td></tr><tr><td align="left" valign="bottom">lateral frontal anterior</td><td align="char" char="." valign="bottom">1.36</td><td align="char" char="." valign="bottom">0.09</td><td align="char" char="." valign="bottom">0.23</td><td align="char" char="." valign="bottom">-1.02</td><td align="char" char="." valign="bottom">0.85</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">1.58</td><td align="char" char="." valign="bottom">0.06</td><td align="char" char="." valign="bottom">0.17</td></tr><tr><td align="left" valign="bottom">lateral frontal posterior</td><td align="char" char="." valign="bottom">1.84</td><td align="char" char="." valign="bottom">0.03</td><td align="char" char="." valign="bottom">0.19</td><td align="char" char="." valign="bottom">-0.32</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">1.36</td><td align="char" char="." valign="bottom">0.09</td><td align="char" char="." valign="bottom">0.17</td></tr><tr><td align="left" valign="bottom">anterior intraparietal</td><td align="char" char="." valign="bottom">1.19</td><td align="char" char="." valign="bottom">0.12</td><td align="char" char="." valign="bottom">0.23</td><td align="char" char="." valign="bottom">0.38</td><td align="char" char="." valign="bottom">0.35</td><td align="char" char="." valign="bottom">0.91</td><td align="char" char="." valign="bottom">0.43</td><td align="char" char="." valign="bottom">0.33</td><td align="char" char="." valign="bottom">0.44</td></tr><tr><td align="left" valign="bottom">anterior cingular rostral</td><td align="char" char="." valign="bottom">-0.23</td><td align="char" char="." valign="bottom">0.59</td><td align="char" char="." valign="bottom">0.59</td><td align="char" char="." valign="bottom">0.99</td><td align="char" char="." valign="bottom">0.16</td><td align="char" char="." valign="bottom">0.82</td><td align="char" char="." valign="bottom">-0.89</td><td align="char" char="." valign="bottom">0.81</td><td align="char" char="." valign="bottom">0.81</td></tr><tr><td align="left" valign="bottom">anterior cingular caudal</td><td align="char" char="." valign="bottom">0.86</td><td align="char" char="." valign="bottom">0.19</td><td align="char" char="." valign="bottom">0.23</td><td align="char" char="." valign="bottom">0.83</td><td align="char" char="." valign="bottom">0.20</td><td align="char" char="." valign="bottom">0.82</td><td align="char" char="." valign="bottom">-0.19</td><td align="char" char="." valign="bottom">0.58</td><td align="char" char="." valign="bottom">0.66</td></tr></tbody></table></table-wrap><p>As can be seen, none of the regions within the monkey MDN showed an FDR-corrected significant difference or interaction. Although the absence of a canonical congruity effect makes it difficult to draw strong conclusions, it did approach significance at an uncorrected level in the lateral frontal posterior region, similar to the large prefrontal effect we report in Figures 4 and 5. Furthermore, for the reversed congruity effect there was never even a trend at the uncorrected level, and the crucial interaction of canonicity and congruity again approached significance in the lateral prefrontal cortex.</p><p>We also performed an ANOVA in the human participants of the VV experiment on the average betas across the 7 different fronto-parietal ROIs as used by Mitchell et al to define their equivalent to the monkey brain (Fig 1a, right in Mitchell et al. 2016) with congruity, canonicity and hemisphere (except for the anterior cingulate which is a bilateral ROI) as within-subject factors. We confirmed the results presented in the manuscript (Figure 4C) with notably no significant interaction between congruity and canonicity in any of these ROIs (all F-values (except insula) &lt;1). A significant main effect of congruity was observed in the posterior middle frontal gyrus (MFG) and inferior precentral sulcus at the FDR corrected level. Analyses restricted to the canonical trials found a congruity effect in these two regions plus the anterior insula and anterior cingulate/presupplementary motor area, whereas no ROIs were significant at a FDR corrected level for reverse trials. There was a trend in the middle MFG and inferior precentral region for reversed trials. Crucially, there was not even a trend for the interaction between congruity and canonicity at the uncorrected level. The difference in the effect size between the canonical and reversed direction can therefore be explained by the larger statistical power due to the larger number of congruent trials (70%, versus 10% for the other trial conditions), not by a significant effect by the canonical and the reversed direction.</p><table-wrap id="sa4table2" position="float"><label>Author response table 2.</label><caption><title>Congruity effect for humans in Experiment 2 within the ROIs of the MDN (n=23).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="right" valign="bottom"/><th align="right" valign="bottom"/><th align="right" valign="bottom"/><th align="right" valign="bottom"/><th align="center" valign="bottom" colspan="3">Congruity effect</th><th align="center" valign="bottom" colspan="3">Congruity effect</th><th align="center" valign="bottom" rowspan="2" colspan="3">Interaction Congruity X Canonicity</th></tr><tr><th align="right" valign="bottom"/><th align="center" valign="middle" colspan="3">Main Congruity effect</th><th align="center" valign="bottom" colspan="3">Canonical trials</th><th align="center" valign="bottom" colspan="3">Reverse trials</th></tr></thead><tbody><tr><td align="left" valign="bottom"/><td align="right" valign="bottom">F-val</td><td align="right" valign="bottom">p-val</td><td align="right" valign="bottom">FDR p-val</td><td align="right" valign="bottom">F -val</td><td align="right" valign="bottom">p-val</td><td align="right" valign="bottom">FDR p-val</td><td align="right" valign="bottom">F-val</td><td align="right" valign="bottom">p-val</td><td align="right" valign="bottom">FDR p-val</td><td align="right" valign="bottom">F -val</td><td align="right" valign="bottom">p-val</td><td align="right" valign="bottom">FDR p-val</td></tr><tr><td align="left" valign="bottom">Insula</td><td align="right" valign="bottom">6.04</td><td align="right" valign="bottom">0.022</td><td align="right" valign="bottom">0.051</td><td align="right" valign="bottom">11.68</td><td align="right" valign="bottom">0.002</td><td align="right" valign="bottom">0.014*</td><td align="right" valign="bottom">1.27</td><td align="right" valign="bottom">0.27</td><td align="right" valign="bottom">0.31</td><td align="right" valign="bottom">1.86</td><td align="right" valign="bottom">0.19</td><td align="right" valign="bottom">n.s</td></tr><tr><td align="left" valign="bottom">antMFG</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">1.01</td><td align="right" valign="bottom">0.32</td><td align="right" valign="bottom">0.37</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td></tr><tr><td align="left" valign="bottom">postMFG</td><td align="right" valign="bottom">8.53</td><td align="right" valign="bottom">0.008</td><td align="right" valign="bottom">0.028*</td><td align="right" valign="bottom">9.25</td><td align="right" valign="bottom">0.006</td><td align="right" valign="bottom">0.019*</td><td align="right" valign="bottom">5.11</td><td align="right" valign="bottom">0.034</td><td align="right" valign="bottom">0.12</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td></tr><tr><td align="left" valign="bottom">ipreCentral</td><td align="right" valign="bottom">10.75</td><td align="right" valign="bottom">0.003</td><td align="right" valign="bottom">0.021*</td><td align="right" valign="bottom">7.03</td><td align="right" valign="bottom">0.014</td><td align="right" valign="bottom">0.024*</td><td align="right" valign="bottom">6.2</td><td align="right" valign="bottom">0.02</td><td align="right" valign="bottom">0.12</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td></tr><tr><td align="left" valign="bottom">SpreCentral</td><td align="right" valign="bottom">3.05</td><td align="right" valign="bottom">0.094</td><td align="right" valign="bottom">0.11</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">1.75</td><td align="right" valign="bottom">0.2</td><td align="right" valign="bottom">0.28</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td></tr><tr><td align="left" valign="bottom">Intra-parietal sulcus</td><td align="right" valign="bottom">4.23</td><td align="right" valign="bottom">0.051</td><td align="right" valign="bottom">0.071</td><td align="right" valign="bottom">3.7</td><td align="right" valign="bottom">0.067</td><td align="right" valign="bottom">0.094</td><td align="right" valign="bottom">1.97</td><td align="right" valign="bottom">0.17</td><td align="right" valign="bottom">0.28</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td></tr><tr><td align="left" valign="bottom">Anterior Cingulate</td><td align="right" valign="bottom">5.44</td><td align="right" valign="bottom">0.029</td><td align="right" valign="bottom">0.051</td><td align="right" valign="bottom">8.43</td><td align="right" valign="bottom">0.008</td><td align="right" valign="bottom">0.019*</td><td align="right" valign="bottom">1.72</td><td align="right" valign="bottom">0.2</td><td align="right" valign="bottom">0.28</td><td align="right" valign="bottom">&lt;1</td><td align="right" valign="bottom">n.s</td><td align="right" valign="bottom">n.s</td></tr></tbody></table></table-wrap><p>These results support our contention that the type of learning of the stimulus pairs was very different in the two species. We thank the reviewer for suggesting these relevant additional analyses.</p><disp-quote content-type="editor-comment"><p>- While the authors acknowledge in the discussion that the number of monkeys included in the study is considerably lower compared to humans, it would be informative to know the variability of the results among human participants.</p></disp-quote><p>We agree that this is an interesting question, although it is also very open-ended. For instance, we could report each subjects’ individual whole-brain results, but this would take too much space (and the interested reader will be able to do so from the data that we make available as part of this publication). As a step in this direction, we provide below a figure showing the individual congruity effects, separately for each experiment and for each ROI of table 5, and for each of the 52 participants for whom an fMRI localizer was available:</p><p>Author response image 1.</p><p>Difference in mean betas between congruent and incongruent conditions in a-priori linguistic and mathematical ROIs (see definition and analyses in Table 5) in both experiments (experiment 1 = AV, left panel; experiment 2 = VV, right panel). Dots correspond to participants (red: canonical trials, green reversed trials).The boxplot notch is located at the median and the lower and upper box hinges at the 25th and 75th centiles. Whiskers extend to 1.5 inter-quartile ranges on either side of the hinges. ROIs are ranked by the median of the Incongruent-Congruent difference across canonical and reversed order,</p><p>within a given experiment. For purposes of comparison between the two experiments, we have underlined with colors the top-five common ROIs between the two experiments. N.s.: non-significant congruity effect (p&gt;0.05)</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87380-sa4-fig1-v1.tif"/></fig><p>Several regions show a rather consistent difference across subjects (see, for instance, the posterior STS in experiment 1, left panel). Overall, only 3 of the 52 participants did not show any beta superior to 2 in canonical or reversed in any ROIs. The consistency is quite striking, given the limited number of test trials (in total only 16 incongruent trials per direction per participant), and the fact that these ROIs were selected for their responses to spoken or written sentences, as part of a subsidiary task quite different from the main task.</p><disp-quote content-type="editor-comment"><p>- Some details are missing in the methods.</p></disp-quote><p>Thank you for these comments, we reply to them point-by-point below.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>This study investigates the hypothesis that humans (but not non-human primates) spontaneously learn reversible temporal associations (i.e., learning a B-A association after only being exposed to A-B sequences), which the authors consider to be a foundational property of symbolic cognition. To do so, they expose humans and macaques to 2-item sequences (in a visual-auditory experiment, pairs of images and spoken nonwords, and in a visual-visual experiment, pairs of images and abstract geometric shapes) in a fixed temporal order, then measure the brain response during a test phase to congruent vs. incongruent pairs (relative to the trained associations) in canonical vs. reversed order (relative to the presentation order used in training). The advantage of neuroimaging for this question is that it removes the need for a behavioral test, which non-human primates can fail for reasons unrelated to the cognitive construct being investigated. In humans, the researchers find statistically indistinguishable incongruity effects in both directions (supporting a spontaneous reversible association), whereas in monkeys they only find incongruity effects in the canonical direction (supporting an association but a lack of spontaneous reversal). Although the precise pattern of activation varies by experiment type (visual-auditory vs. visual-visual) in both species, the authors point out that some of the regions involved are also those that are most anatomically different between humans and other primates. The authors interpret their finding to support the hypothesis that reversible associations, and by extension symbolic cognition, is uniquely human.</p><p>This study is a valuable complement to prior behavioral work on this question. However, I have some concerns about methods and framing.</p></disp-quote><p>We thank the reviewer for the careful summary of the manuscript, and the positive comments.</p><disp-quote content-type="editor-comment"><p>Methods - Design issues:</p><p>The authors originally planned to use the same training/testing protocol for both species but the monkeys did not learn anything, so they dramatically increased the amount of training and evaluation. By my calculation from the methods section, humans were trained on 96 trials and tested on 176, whereas the monkeys got an additional 3,840 training trials and 1,408 testing trials. The authors are explicit that they continued training the monkeys until they got a congruity effect. On the one hand, it is commendable that they are honest about this in their write-up, given that this detail could easily be framed as deliberate after the fact. On the other hand, it is still a form of p-hacking, given that it's critical for their result that the monkeys learn the canonical association (otherwise, the critical comparison to the non-canonical association is meaningless).</p></disp-quote><p>Thank you for this comment.</p><p>Indeed, for experiment 1, the amount of training and testing was not equal for the humans and monkeys, as also mentioned by reviewer 2. We now describe in more detail how many training and imaging days we used for each experiment and each species, as well as the number of blocks per day and the number of trials per block (see lines 572-577). We also added the information on the amount of training receives to all of the legends of the Tables.</p><p>We are sorry for giving the impression that we trained until the monkeys learned this. This was not the case. Based on previous literature, we actually anticipated that the short training would not be sufficient, and therefore planned additional training in advance. Specifically, Meyer &amp; Olson (2011) had observed pair learning in the inferior temporal cortex of macaque monkeys after 816 exposures per pair. This is similar to the additional training we gave, about 80 blocks with 12 trials per pair per block. This is now explained in more detail (lines 577-580).</p><p>Furthermore, we strongly disagree with the pejorative term p-hacking. The aim of the experiment was not to show a congruency effect in the canonical direction in monkeys, but to track and compare their behavior in the same paradigm as that of humans for the reverse direction. It would have been unwise to stop after human-identical training and only show that humans learn better, which is a given. Instead, we looked at brain activations at both times, at the end of human-identical training and when the monkeys had learned the pairs in the canonical direction.</p><p>Finally, in experiment 2, monkeys were tested after the same 3 days of training as humans. We wrote: “Using this design, we obtained significant canonical congruity effects in monkeys on the first imaging day after the initial training (24 trials per pair), indicating that the animals had learned the associations” (lines 252-253).</p><disp-quote content-type="editor-comment"><p>(2) Between-species comparisons are challenging. In addition to having differences in their DNA, human participants have spent many years living in a very different culture than that of NHPs, including years of formal education. As a result, attributing the observed differences to biology is challenging. One approach that has been adopted in some past studies is to examine either young children or adults from cultures that don't have formal educational structures. This is not the approach the authors take. This major confound needs to minimally be explicitly acknowledged up front.</p></disp-quote><p>Thank you for raising this important point. We already had a section on “limitations” in the manuscript, which we now extended (line 478-485). Indeed, this study is following a previous study in 5-month-old infants using EEG, in which we already showed that after learning associations between labels and categories, infants spontaneously generalize learning to the reversed pairs after a short learning period in the lab (Kabdebon et al, PNAS, 2019). We also cited preliminary results of the same paradigm as used in the current study but using EEG in 4-month-old infants (Ekramnia and Dehaene-Lambertz, 2019), where we replicated the results obtained by Kabdebon et al. 2019 showing that preverbal infants spontaneously generalize learning to the reversed pairs.</p><p>Functional MRI in awake infants remains a challenge at this age (but see our own work, DehaeneLambertz et al, Science, 2002), especially because the experimental design means only a few trials in the conditions of interest (10%) and thus a long experimental duration that exceed infants’ quietness and attentional capacities in the noisy MRI environment. (We discuss this on lines 493-496.)</p><disp-quote content-type="editor-comment"><p>(3) Humans have big advantages in processing and discriminating spoken stimuli and associating them with visual stimuli (after all, this is what words are in spoken human languages). Experiment 2 ameliorates these concerns to some degree, but still, it is difficult to attribute the failure of NHPs to show reversible associations in Experiment 1 to cognitive differences rather than the relative importance of sound string to meaning associations in the human vs. NHP experiences.</p></disp-quote><p>As the reviewer wrote, we deliberately performed Experiment 2 with visual shapes to control for various factors that might have explained the monkeys' failure in Experiment 1.</p><disp-quote content-type="editor-comment"><p>(4) More minor: The localizer task (math sentences vs. other sentences) makes sense for math but seems to make less sense for language: why would a language region respond more to sentences that don't describe math vs. ones that do?</p></disp-quote><p>The referee is correct: our use of the word “reciprocally” was improper (although see Amalric et Dehaene, 2016 for significant differences in both directions when non-mathematical sentences concern specific knowledge). We changed the formulation to clarify this as follows: “In these ROIs, we recovered the subject-specific coordinates of each participant’s 10% best voxels in the following comparisons: sentences vs rest for the 6 language Rois ; reading vs listening for the VWFA ; and numerical vs non-numerical sentences for the 8 mathematical ROIs.” (lines 678-680).</p><disp-quote content-type="editor-comment"><p>Methods - Analysis issues:</p><p>(5) The analyses appear to &quot;double dip&quot; by using the same data to define the clusters and to statistically test the average cluster activation (Kriegeskorte et al., 2009). The resulting effect sizes are therefore likely inflated, and the p-values are anticonservative.</p></disp-quote><p>It is not clear to us which result the reviewer is referring to. In Tables 1-4, we report the values that we found significant in the whole brain analysis, we do not report additional statistical tests for this data. For Table 5, the subject-specific voxels were identified through a separate localizer experiment, which was designed to pinpoint the precise activation areas for each subject in the domains of oral and written language-processing and math. Subsequently, we compared the activation at these voxel locations across different conditions of the main experiment. Thus, the two datasets were distinct, and there was no double dipping. In both interpretations of the comment, we therefore disagree with the reviewer.</p><disp-quote content-type="editor-comment"><p>Framing:</p><p>(6) The framing (&quot;Brain mechanisms of reversible symbolic reference: A potential singularity of the human brain&quot;) is bigger than the finding (monkeys don't spontaneously reverse a temporal association but humans do). The title and discussion are full of buzzy terms (&quot;brain mechanisms&quot;, &quot;symbolic&quot;, and &quot;singularity&quot;) that are only connected to the experiments by a debatable chain of assumptions.</p><p>First, this study shows relatively little about brain &quot;mechanisms&quot; of reversible symbolic associations, which implies insights into how these associations are learned, recognized, and represented. But we're only given standard fMRI analyses that are quite inconsistent across similar experimental paradigms, with purely suggestive connections between these spatial patterns and prior work on comparative brain anatomy.</p></disp-quote><p>We agree with the referee that the term “mechanism” is ambiguous and, for systems neuroscientists, may suggest more than we are able to do here with functional MRI. We changed the title to “Brain areas for reversible symbolic reference, a potential singularity of the human brain”. This title better describes our specific contribution: mapping out the areas involved in reversibility in humans, and showing that they do not seem to respond similarly in macaque monkeys.</p><disp-quote content-type="editor-comment"><p>Second, it's not clear what the relationship is between symbolic cognition and a propensity to spontaneously reverse a temporal association. Certainly, if there are inter-species differences in learning preferences this is important to know about, but why is this construed as a difference in the presence or absence of symbols? Because the associations aren't used in any downstream computation, there is not even any way for participants to know which is the sign and which is the signified: these are merely labels imposed by the researchers on a sequential task.</p></disp-quote><p>As explained in the introduction, the reversibility test addressed a very minimal core property of symbolic reference. There cannot be a symbol if its attachment doesn’t operate in both directions. Thus, this property is necessary – but we agree that it is not sufficient. Indeed, more tests are needed to establish whether and how the learned symbols are used in further downstream compositional tasks (as discussed in our recent TICS papers, Dehaene et al. 2022). We added a sentence in the introduction to acknowledge this fact:</p><p>“Such reversibility is a core and necessary property of symbols, although we readily acknowledge that it is not sufficient, since genuine symbols present additional referential and compositional properties that will not be tested in the present work.” (lines 89-92).</p><disp-quote content-type="editor-comment"><p>Third, the word &quot;singularity&quot; is both problematically ambiguous and not well supported by the results. &quot;Singularity&quot; is a highly loaded word that the authors are simply using to mean &quot;that which is uniquely human&quot;. Rather than picking a term with diverse technical meanings across fields and then trying to restrict the definition, it would be better to use a different term. Furthermore, even under the stated definition, this study performed a single pairwise comparison between humans and one other species (macaques), so it is a stretch to then conclude (or insinuate) that the &quot;singularity&quot; has been found (see also pt. 2 above).</p></disp-quote><p>We have published an extensive review including a description of our use of the term “singularity” (Dehaene et al., TICS 2022). Here is a short except: “Humans are different even in domains such as drawing and geometry that do not involve communicative language. We refer to this observation using the term “human cognitive singularity”, the word singularity being used here in its standard meaning (the condition of being singular) as well as its mathematical sense (a point of sudden change). Hominization was certainly a singularity in biological evolution, so much so that it opened up a new geological age (the Anthropocene). Even if evolution works by small continuous change (and sometimes it doesn’t [4]), it led to a drastic cognitive change in humans.”</p><p>We find the referee’s use of the pejorative term ”insinuate” quite inappropriate. From the title on, we are quite nuanced and refer only to a “potential singularity”. Furthermore, as noted above, we explicitly mention in the discussion the limitations of our study, and in particular the fact that only a single non-human species was tested (see lines 486-493). We are working hard to get chimpanzee data, but this is remarkably difficult for us, and we hope that our paper will incite other groups to collect more evidence on this point.</p><disp-quote content-type="editor-comment"><p>(7) Related to pt. 6, there is circularity in the framing whereby the authors say they are setting out to find out what is uniquely human, hypothesizing that the uniquely human thing is symbols, and then selecting a defining trait of symbols (spontaneous reversible association) *because* it seems to be uniquely human (see e.g., &quot;Several studies previously found behavioral evidence for a uniquely human ability to spontaneously reverse a learned association (Imai et al., 2021; Kojima, 1984; Lipkens et al., 1988; Medam et al., 2016; Sidman et al., 1982), and such reversibility was therefore proposed as a defining feature of symbol representation reference (Deacon, 1998; Kabdebon and DehaeneLambertz, 2019; Nieder, 2009).&quot;, line 335). They can't have it both ways. Either &quot;symbol&quot; is an independently motivated construct whose presence can be independently tested in humans and other species, or it is by fiat synonymous with the &quot;singularity&quot;. This circularity can be broken by a more modest framing that focuses on the core research question (e.g., &quot;What is uniquely human? One possibility is spontaneous reversal of temporal associations.&quot;) and then connects (speculatively) to the bigger conceptual landscape in the discussion (&quot;Spontaneous reversal of temporal associations may be a core ability underlying the acquisition of mental symbols&quot;).</p></disp-quote><p>We fail to understand the putative circularity that the referee sees in our introduction. We urge him/her to re-read it, and hope that, with the changes that we introduced, it does boil down to his/her summary, i.e. “What is uniquely human? One possibility is spontaneous reversal of temporal associations.&quot;</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>In general, the manuscript was very clear, easy to read, and compelling. I would recommend the authors carefully check the text for consistency and minor typos. For example:</p><p>The sample size for the monkeys kept changing throughout the paper. E.g., Experiment 1: n = 2 (line 149); n = 3 (line 205).</p></disp-quote><p>Thank you for catching this error, we corrected it. The number of animals was indeed 2 for experiment 1, and 3 for experiment 2. (Animals JD and YS participated in experiment 1 and JD, JC and DN in experiment 2. So only JD participated in both experiments.)</p><disp-quote content-type="editor-comment"><p>Similarly, the number of stimulus pairs is reported inconsistently (4 on line 149, 5 pairs later in the paper).</p></disp-quote><p>We’re sorry that this was unclear. We used 5 sets of 4 audio-visual pairs each. We now clarify this, on line 157 and on lines 514-516.</p><disp-quote content-type="editor-comment"><p>At least one case of p&gt;0.0001, rather than p &lt; 0.0001 (I assume).</p></disp-quote><p>Thank you once again, we now corrected this.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>One major issue in the study is the absence of significant results in monkeys. Indeed, the authors draw conclusions regarding the lack of significant difference in activity related to surprise in the multidemand network (MDN) in the reverse congruent versus reverse incongruent conditions. Although the results are convincing (especially with the significant interaction between congruency and canonicity), the article could be improved by including additional analyses in a priori ROI for the MDN in monkeys (as well as in humans, for comparison). In other words: what are the statistics for the MDN regarding congruity, canonicity, and interaction in both species? Since the authors have already performed this type of analysis for language and Math ROIs (table 5), it should be relatively easy for them to extend it to the MDN. Demonstrating that results in monkeys are far from significant could further convince the reader.</p><p>Furthermore, while the authors acknowledge in the discussion that the number of monkeys included in the study is considerably lower compared to humans, it would be informative to know the variability of the results among human participants. Specifically, it would be valuable to describe the proportion of human participants in which the effects of congruency, canonicity, and their interaction are significant. Additionally, stating the variability of the F-values for each effect would provide reassurance to the reader regarding the distinctiveness of humans in comparison to monkeys. Low variability in the results would serve to mitigate concerns that the observed disparity is merely a consequence of testing a unique subset of monkeys, which may differ from the general population. Indeed, this would be a greater support to the notion that the dissimilarity stems from a genuine distinction between the two species.</p></disp-quote><p>We responded to both of these points above.</p><disp-quote content-type="editor-comment"><p>In terms of methods, details are missing:</p><p>- How many trials of each condition are there exactly? (10% of 44 trials is 4.4) :</p></disp-quote><p>We wrote: “In both humans and monkeys, each block started with 4 trials in the learned direction (congruent canonical trials), one trial for each of the 4 pairs (2 O-L and 2 L-O pairs). The rest of the block consisted of 40 trials in which 70% of trials were identical to the training; 10% were incongruent pairs but the direction (O-L or L-O) was correct (incongruent canonical trials), thus testing whether the association was learned; 10% were congruent pairs but the direction within the pairs was reversed relative to the learned pairs (congruent reversed trials) and 10% were incongruent pairs in reverse (incongruent reversed trials).”(See lines 596-600.)</p><p>Thus, each block comprised 4 initial trials, 28 canonical congruent trials, 4 canonical incongruent, 4 reverse congruent and 4 reverse incongruent trials, i.e. 4+28+3x4=40 trials.</p><disp-quote content-type="editor-comment"><p>- How long is one trial?</p></disp-quote><p>As written in the method section: “In each trial, the first stimulus (label or object) was presented during 700ms, followed by an inter-stimulus-interval of 100ms then the second stimulus during 700ms. The pairs were separated by a variable inter-trial-interval of 3-5 seconds” i.e. 700+100+700=1500, plus 3 to 4.75 seconds of blank between the trials (see lines 531-533).</p><disp-quote content-type="editor-comment"><p>- How are the stimulus presentations jittered?</p></disp-quote><p>See : “The pairs were separated by a variable inter-trial-interval randomly chosen among eight different durations between 3 and 4.75 seconds (step=250 ms). The series of 8 intervals was randomized again each time it was completed.”(lines 533-535).</p><disp-quote content-type="editor-comment"><p>- What is the statistical power achieved for humans? And for monkeys?</p></disp-quote><p>We know of no standard way to define power for fMRI experiments. Power will depend on so many parameters, including the fMRI signal-to-noise ratio, the attention of the subject, the areas being considered, the type of analysis (whole-brain versus ROIs), etc.</p><disp-quote content-type="editor-comment"><p>- Videos are mentioned in the methods, is it the image and sound? It is not clear.</p></disp-quote><p>We’re sorry that it was unclear. Video’s were only used for the training of the human subjects. We now corrected this in the method section (lines 552-554).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>The main recommendations are to adjust the framing (making it less bold and more connected to the empirical evidence) and to ensure independence in the statistical analyses of the fMRI data.</p></disp-quote><p>See our replies to the reviewer’s comments on “Framing” above. In particular, we changed the title of the paper from “Brain mechanisms of reversible symbolic reference” to “Brain areas for reversible symbolic reference”.</p><p>References cited in this response</p><p>Dehaene, S., Al Roumi, F., Lakretz, Y., Planton, S., &amp; Sablé-Meyer, M. (2022). Symbols and mental programs : A hypothesis about human singularity. Trends in Cognitive Sciences, 26(9), 751‑766. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2022.06.010">https://doi.org/10.1016/j.tics.2022.06.010</ext-link>.</p><p>Dehaene-Lambertz, Ghislaine, Stanislas Dehaene, et Lucie Hertz-Pannier. Functional Neuroimaging of Speech Perception in Infants. Science 298, no 5600 (2002): 2013-15. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1077066">https://doi.org/10.1126/science.1077066</ext-link>.</p><p>Ekramnia M, Dehaene-Lambertz G. 2019. Investigating bidirectionality of associations in young infants as an approach to the symbolic system. Presented at the CogSci. p. 3449.</p><p>Fedorenko E, Duncan J, Kanwisher N (2013) Broad domain generality in focal regions of frontal and parietal cortex. Proc Natl Acad Sci U S A 110:16616-16621.</p><p>Kabdebon, Claire, et Ghislaine Dehaene-Lambertz. « Symbolic Labeling in 5-Month-Old Human Infants ». Proceedings of the National Academy of Sciences 116, no 12 (2019): 5805-10. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1809144116">https://doi.org/10.1073/pnas.1809144116</ext-link>.</p><p>Mitchell, D. J., Bell, A. H., Buckley, M. J., Mitchell, A. S., Sallet, J., &amp; Duncan, J. (2016). A Putative Multiple-Demand System in the Macaque Brain. Journal of Neuroscience, 36(33), 8574‑8585. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0810-16.2016">https://doi.org/10.1523/JNEUROSCI.0810-16.2016</ext-link></p></body></sub-article></article>