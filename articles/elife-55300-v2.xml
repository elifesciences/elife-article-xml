<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">55300</article-id><article-id pub-id-type="doi">10.7554/eLife.55300</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Temporal selectivity declines in the aging human auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-79072"><name><surname>Erb</surname><given-names>Julia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3440-7269</contrib-id><email>julia.erb@uni-luebeck.de</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-172475"><name><surname>Schmitt</surname><given-names>Lea-Maria</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9356-2234</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-21838"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7619-0459</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><institution>Department of Psychology, University of Lübeck</institution><addr-line><named-content content-type="city">Lübeck</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Johnsrude</surname><given-names>Ingrid S</given-names></name><role>Reviewing Editor</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>03</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e55300</elocation-id><history><date date-type="received" iso-8601-date="2020-01-20"><day>20</day><month>01</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-07-02"><day>02</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Erb et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Erb et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-55300-v2.pdf"/><abstract><p>Current models successfully describe the auditory cortical response to natural sounds with a set of spectro-temporal features. However, these models have hardly been linked to the ill-understood neurobiological changes that occur in the aging auditory cortex. Modelling the hemodynamic response to a rich natural sound mixture in N = 64 listeners of varying age, we here show that in older listeners’ auditory cortex, the key feature of temporal rate is represented with a markedly broader tuning. This loss of temporal selectivity is most prominent in primary auditory cortex and planum temporale, with no such changes in adjacent auditory or other brain areas. Amongst older listeners, we observe a direct relationship between chronological age and temporal-rate tuning, unconfounded by auditory acuity or model goodness of fit. In line with senescent neural dedifferentiation more generally, our results highlight decreased selectivity to temporal information as a hallmark of the aging auditory cortex.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>It can often be difficult for an older person to understand what someone is saying, particularly in noisy environments. Exactly how and why this age-related change occurs is not clear, but it is thought that older individuals may become less able to tune in to certain features of sound.</p><p>Newer tools are making it easier to study age-related changes in hearing in the brain. For example, functional magnetic resonance imaging (fMRI) can allow scientists to ‘see’ and measure how certain parts of the brain react to different features of sound. Using fMRI data, researchers can compare how younger and older people process speech. They can also track how speech processing in the brain changes with age.</p><p>Now, Erb et al. show that older individuals have a harder time tuning into the rhythm of speech. In the experiments, 64 people between the ages of 18 to 78 were asked to listen to speech in a noisy setting while they underwent fMRI. The researchers then tested a computer model using the data. In the older individuals, the brain’s tuning to the timing or rhythm of speech was broader, while the younger participants were more able to finely tune into this feature of sound. The older a person was the less able their brain was to distinguish rhythms in speech, likely making it harder to understand what had been said.</p><p>This hearing change likely occurs because brain cells become less specialised overtime, which can contribute to many kinds of age-related cognitive decline. This new information about why understanding speech becomes more difficult with age may help scientists develop better hearing aids that are individualised to a person’s specific needs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>functional mri</kwd><kwd>healthy aging</kwd><kwd>spectro-temporal modulations</kwd><kwd>hearing loss</kwd><kwd>presbycusis</kwd><kwd>temporal rate coding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-CoG-2014-646696 &quot;AUDADAPT&quot;</award-id><principal-award-recipient><name><surname>Obleser</surname><given-names>Jonas</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>OB 352/2-1</award-id><principal-award-recipient><name><surname>Obleser</surname><given-names>Jonas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The aged human auditory cortex shows preserved tonotopy, but temporal modulations are represented with a markedly broader tuning, highlighting decreased temporal selectivity as a hallmark of the aging auditory cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Age-related hearing loss is a frequent cause for a decline in speech comprehension, particularly in complex acoustic environments. As the temporal envelope of speech is crucial for speech comprehension (<xref ref-type="bibr" rid="bib63">Shannon et al., 1995</xref>), the ability to accurately encode temporal features in the auditory system is pivotal for successful perception of speech and conspecific vocalizations (<xref ref-type="bibr" rid="bib53">Peelle and Wingfield, 2016</xref>). Age-related deficits have been observed in different temporal psychoacoustic tasks (e.g. gap detection; <xref ref-type="bibr" rid="bib64">Snell et al., 2002</xref>). Those psychoacoustic studies suggest that the precision of processing of temporal cues declines with age (e.g. <xref ref-type="bibr" rid="bib24">Gordon-Salant et al., 2006</xref>).</p><p>While age-related hearing loss has been attributed to a variety of peripheral declines in auditory coding (e.g. <xref ref-type="bibr" rid="bib21">Gates and Mills, 2005</xref>), fewer studies have looked at more central levels of auditory processing. Yet, declines in temporal processing are hypothesized to be not exclusively peripheral in origin, but more central than at least the mid-brain (<xref ref-type="bibr" rid="bib58">Recanzone, 2018</xref>). In the aged macaque auditory cortex, neurons become more broadly tuned to temporal modulations and temporal fidelity of cortical responses decreases (<xref ref-type="bibr" rid="bib49">Ng and Recanzone, 2018</xref>). Topographically, a neural dedifferentiation is suggested to take place in the older brain: In young macaques, the neuronal response to the inter-stimulus interval in primary field A1 and caudolateral belt area (CL) differs in that A1 neurons have shorter response latencies. In aged animals, however, no difference was observed between A1 and CL such that aged A1 neurons had an equivalent response latency to CL neurons (<xref ref-type="bibr" rid="bib49">Ng and Recanzone, 2018</xref>). Further, in aged neurons, a shift in neural coding strategy from less temporal coding towards more rate coding of temporal modulations was evident (<xref ref-type="bibr" rid="bib51">Overton and Recanzone, 2016</xref>; for a review of both coding strategies see <xref ref-type="bibr" rid="bib33">Joris et al., 2004</xref>). For those neurons that still adhered to a temporal coding strategy, the temporal fidelity decreased, although the absolute number of neurons responsive to temporal modulations was unaffected by age (<xref ref-type="bibr" rid="bib51">Overton and Recanzone, 2016</xref>).</p><p>At higher processing levels, only a number of relatively unspecific changes have been reported in the older human listener (for review see <xref ref-type="bibr" rid="bib53">Peelle and Wingfield, 2016</xref>). For example, the frontal and cingulo-opercular cortical response is elevated during a challenging speech comprehension or gap detection task (<xref ref-type="bibr" rid="bib16">Erb and Obleser, 2013</xref>; <xref ref-type="bibr" rid="bib68">Vaden et al., 2015</xref>; <xref ref-type="bibr" rid="bib69">Vaden et al., 2020</xref>). This has been interpreted as compensatory mechanism in response to loss of sensory acuity. Aging has been hypothesized to lead to a neural dedifferentiation in sensory cortices (e.g., <xref ref-type="bibr" rid="bib52">Park et al., 2004</xref>). A more recent unspecific observation is that with age, decoding accuracy of stimulus conditions from auditory cortical fMRI responses declines (<xref ref-type="bibr" rid="bib39">Lalwani et al., 2019</xref>; but see discussion for contrasting findings in MEG speech tracking, for example <xref ref-type="bibr" rid="bib55">Presacco et al., 2016</xref>). This finding in turn has been linked to an imbalance of excitation and inhibition in older adults, more specifically a reduction of GABA levels in auditory cortex (<xref ref-type="bibr" rid="bib39">Lalwani et al., 2019</xref>). Accumulating evidence supports the theory of an age-related loss of inhibition in sensory cortices (<xref ref-type="bibr" rid="bib10">Caspary et al., 2008</xref>).</p><p>A recent model of cortical processing (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>) has led to substantial progress in our understanding of how natural sounds become represented in the auditory cortex. A series of studies have shown that cortical processing of sounds is optimized to represent the spectro-temporal modulations which are typically present in conspecific vocalizations such as speech or animal calls (<xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Hullett et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Erb et al., 2019a</xref>). However, those studies have not looked at the pressing case of the aged human cortex. It remains unknown whether — and if so, which — general response properties are altered in aged auditory cortex.</p><p>The present study applies an fMRI encoding and decoding approach to compare auditory cortical responses to natural sounds in young and older humans. We focus on the age-group comparison at the level of representation of fundamental acoustic features (spectro-temporal modulations). Results show that while the large-scale topographic organization of acoustic features is preserved in the older auditory cortex, age groups differ in tuning to temporal modulations: Human aging thus appears to be accompanied by an anatomically and functionally specific broadening of temporal-rate tuning in auditory cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>fMRI responses to sounds from a total of N = 64 younger and older listeners were modelled using both model-based encoding (<xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref>) and decoding (<xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>). In brief, sounds were decomposed into acoustic features, that is, the frequency-dependent spectro-temporal modulation content (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>). Analyses were restricted to voxels in an anatomic mask of the auditory cortex. In a fourfold cross-validation procedure, we used ridge regression to derive single- and multi-voxel MTFs.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and analysis pipeline.</title><p>Participants listened to a story embedded in sound textures at a signal-to-noise ratio(SNR) of 0 dB while we acquired 3T-fMRI data. For audiograms of the participants see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The sounds were decomposed into their modulation spectrum (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>; for modulation spectra of the sounds see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). In a first <italic>univariate encoding</italic> approach, we calculated a modulation transfer function (MTF) for each individual voxel. In a second <italic>multivariate decoding</italic> approach, linear decoders were trained on the response patterns in auditory cortex for each feature of the modulation representation. Predictions were then tested on a left-out testing data set.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Pure-tone audiometry and behavioural results.</title><p>(<bold>a</bold>) Audiograms for older participants (averaged over left and right ear). (<bold>b</bold>) Average behavioural performance (mean ± SEM) in the scanner in a four-choice task (three questions on the story content after each run) for young (green) and older participants (violet). Chance level was 0.25 [proportion correct]. Young participants performed significantly better than older participants (mean proportion correct difference = 0.1, p=0.009 [permutation test], Cohen’s <italic>d</italic> = 0.65).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Modulation spectra of the stimuli.</title><p>(<bold>a</bold>) Average of the marginalized modulation spectra for the stimulus streams separately and their mixture. (<bold>b</bold>) Standard deviation (SD) of the marginalized modulation spectra. (<bold>c</bold>) Pairwise correlations (Pearson’s <italic>r</italic>) between the different streams in the modulation representation. While speech and texture are mostly uncorrelated over time, the speech–texture mixture is highly correlated with the speech stream in the low frequency range and with textures in the high frequency range. Note that the left plot has a different scaling than the middle and right plot.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig1-figsupp2-v2.tif"/></fig></fig-group><sec id="s2-1"><title>Encoding results</title><p>We compared three models of hemodynamic responses to sound. The models respectively describe the fMRI responses to the (1) speech stream (foreground), (2) textures (background), or (3) the mixture of both streams (for modulation spectra of the different streams see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><sec id="s2-1-1"><title>Sound identification accuracies</title><p>In a first encoding analysis, we estimated a modulation transfer function (MTF) for each voxel based on a subset of fMRI data (training) for each model. We then assessed the ability of these models to accurately predict the fMRI responses to sounds of a new, independent data set (testing data). We quantified prediction accuracy by means of a sound identification analysis (see Materials and methods).</p><p>Sound identification accuracy was higher than chance level (0.5) for all models and age groups (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The sound mixture model had the highest sound identification accuracies (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) and thus described most accurately the fMRI responses to sounds. All subsequent encoding and decoding analyses were based on this mixture model. Young participants had significantly higher identification accuracies for mixture than old participants (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). There was no significant correlation between chronological age and sound identification accuracy for mixture within the young or older group (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Sound identification accuracies for the different encoding models.</title><p>(<bold>a</bold>) Accuracies for sound mixture, texture (background stream) and speech (foreground stream), for young (left, green) and older participants (right, violet). Bars indicate the mean (± standard error of the mean, SEM) of all participants per age group. Accuracies are normalized between 0 and 1; zero denotes that the predicted activity pattern for a given stimulus was least similar to the measured one among all test stimuli; one denotes correct identification; chance level is 0.5. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.005, exact permutation test. (<bold>b</bold>) Identification accuracies for sound mixture were uncorrelated with chronological age within each age group.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig2-v2.tif"/></fig></sec><sec id="s2-1-2"><title>Topographical best-feature maps</title><p>Best feature maps were obtained from univariate encoding models by marginalizing the single-voxel MTFs for the dimension of interest. In particular, tonotopic maps were obtained by averaging over the rate and scale dimension and assigning the frequency with the maximal response to a given voxel. Tonotopic maps showed the typical mirror-symmetric frequency gradients along Heschl’s gyrus in both young and aged auditory cortex, irrespective of age (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Tonotopic gradients were most obvious in the young group (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, upper panel). Consistent with previous human fMRI data (<xref ref-type="bibr" rid="bib45">Moerel et al., 2012</xref>), a low frequency region was observed in the central region of Heschl’s gyrus (HG), presumably marking the boundary between the human homologue of primary auditory fields A1 and R (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, red-yellow). This low frequency region was surrounded antero-medially and posteriorly by high frequency regions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, green-blue). The antero-medial high frequency areas clustered on the planum polare (PP). The posterior regions preferring high frequencies covered planum temporale (PT). We did not find significant age-group differences using two-sample voxel-wise <italic>t</italic>-tests (corrected for multiple comparisons). In fact, tonotopic group maps were correlated, indicating that topography for best frequency was preserved in older adults (<xref ref-type="fig" rid="fig3">Figure 3a</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Tonotopic maps. Best frequency maps were derived by marginalizing modulation transfer functions (MTFs) for frequency and assigning the feature with the maximal response to a given voxel.</title><p>(<bold>a</bold>) Group maps were obtained as median across subjects and are shown on FreeSurfer’s <italic>fsaverage5</italic> template. Tonotopic maps in the young (upper panel) and older group (bottom panel) are correlated in both the right and left hemisphere. (<bold>b</bold>) Exemplary individual tonotopic maps for the left hemisphere (LH) of a young (upper panel) and older participant (bottom panel) are displayed on individual surfaces. Black outlines indicate Heschl’s gyrus. ***p&lt;0.005.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig3-v2.tif"/></fig><p>Topographical best-feature maps for rate and scale were more complex. Similarly, we did not find significant voxel-wise age-group differences using two-sample <italic>t</italic>-tests (corrected for multiple comparisons). On the contrary, both for temporal and spectral modulations, group maps were correlated between age groups (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For best temporal modulation maps, we observed a medial-to-lateral gradient for increasing temporal rate (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). For best spectral modulation maps, the locus of highest spectral resolution coincided with the low frequency region on Heschl’s gyrus (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, <xref ref-type="fig" rid="fig4">Figure 4b</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Group best-feature maps for temporal rate (<bold>a</bold>) and spectral scale (<bold>b</bold>).</title><p>Group maps were obtained as median across subjects and shown on FreeSurfer’s <italic>fsaverage5</italic> template. Black outlines indicate Heschl’s gyrus. Topographic organization in the young (upper panel) and older group (bottom panel) is significantly correlated (Pearson’s correlation coefficient) for best temporal rate (<bold>a</bold>) and best spectral scale (<bold>b</bold>). Histograms of best feature values are shown for the young (green) and older group (violet) above the colour bars. Note the shift of the distribution in older relative to young adults for temporal rate (<bold>a</bold>) but not spectral scale (<bold>b</bold>), indicating that older adults’ voxel-wise preferred temporal rates are shifted towards higher modulation rates. ***p&lt;0.005.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig4-v2.tif"/></fig></sec></sec><sec id="s2-2"><title>Decoding results</title><sec id="s2-2-1"><title>Feature reconstruction from auditory cortex</title><p>Cortical sensitivity to acoustic features was quantified using model-based decoding. Decoders were trained on a subset of the data (training) for each acoustic feature separately (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Reconstruction accuracies were obtained as Pearson’s <italic>r</italic> between predicted and actual acoustic features in the testing data. Results were compared across age groups using exact permutation tests.</p><p>Our main hypothesis was that cortical sensitivity is highest for slow temporal modulations, based on previous observations of human (but not monkey) auditory cortex being most sensitive to the modulations present speech (<xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Erb et al., 2019a</xref>). Decoding yielded highest accuracies at frequencies of 230–580 Hz (mean <italic>r</italic> = 0.47) and spectral scales of 0.25 cyc/oct (mean <italic>r</italic> = 0.47), irrespective of age (<xref ref-type="fig" rid="fig5">Figure 5a,b</xref>), indicating that brain responses followed the frequency and spectral modulation content of the stimuli (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>). Conversely, for temporal modulations, reconstruction accuracies peaked at rates of 4–8 Hz (mean <italic>r</italic> = 0.4) in both age groups (<xref ref-type="fig" rid="fig5">Figure 5a,b</xref>). Those peaks were not present in the stimuli (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>) and imply preferred processing of temporal rates in the speech-relevant range (<xref ref-type="bibr" rid="bib54">Poeppel and Assaneo, 2020</xref>), corroborating our hypothesis (see also discussion).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Decoding from auditory cortex.</title><p>(<bold>a</bold>) Auditory cortical modulation transfer functions (MTFs) for the young and older group quantified by reconstruction accuracies (Pearson’s <italic>r</italic> between predicted an actual acoustic features). (<bold>b</bold>) Mean ± SEM of the MTFs’ marginal profiles for rate, scale and frequency for young (green) and older (violet) participants separately. (<bold>c</bold>) We quantified tuning selectivity by the selectivity index (<italic>SI;</italic> see <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>). For illustration purposes, we show the marginal profiles for rate in a young (green) and older individual (violet) with extreme <italic>SI</italic> values. (<bold>d</bold>) <italic>SI</italic> was compared for each acoustic dimension (rate, scale, frequency) between age groups using an exact permutation test. Black line indicates the median <italic>SI</italic>. Black line indicates the median <italic>SI</italic>. Note that when removing the extreme case in the young group (exceeding the grand average by ±2 SD) for rate <italic>SI</italic>, the mean age-group difference remains significant (p=0.026). Removing the two extreme cases in the older group for scale <italic>SI</italic> leads to a significant age-group difference (p=0.024). Note the different scaling of the plots. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for decoding from auditory cortex with ICA (AROMA)-cleaned data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Decoding from auditory cortex with ICA(AROMA)-cleaned data.</title><p>(<bold>a</bold>) Mean ± SEM of the MTFs’ marginal profiles for rate, scale and frequency for young (green) and older (violet) participants separately. (<bold>b</bold>) The selectivity index <italic>SI</italic> (see <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) was compared per acoustic dimension between age groups using an exact permutation test. Note that irrespective of preprocessing (with or without ICA-cleaning, <xref ref-type="fig" rid="fig5">Figure 5</xref>), reconstruction accuracies peak at temporal rates of 4–8 Hz. Likewise, for AROMA-cleaned data <italic>SI</italic> for temporal rates is higher in young compared to older participants. Removing the young-subject outlier for rate <italic>SI</italic> (exceeding the grand average by ±2 SD) does not alter the <italic>SI</italic> mean age difference qualitatively (p=0.027).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig5-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-2-2"><title>Selectivity index</title><p>A key observation was the age difference in selectivity of tuning to temporal modulations (as quantified by the selectivity index <italic>SI</italic>, see Materials and methods). Selectivity of temporal rate coding was higher in young than in older participants (mean <italic>SI</italic> age difference = 0.01, p=0.004 [permutation test], Cohen’s <italic>d</italic> = 0.68, <xref ref-type="fig" rid="fig5">Figure 5d</xref>, left). Note that removing the extreme case in the young group (outliers were defined as exceeding the grand average by ±2 SD) for rate <italic>SI</italic> does not alter results qualitatively: It reduces the mean age-group difference to a still significant <italic>SI</italic> difference = 0.009, p=0.026. Removing the two extreme cases in the older group for scale <italic>SI</italic> increases the mean age-group difference to a significant <italic>SI</italic> difference = 0.037, p=0.024. A lower selectivity index is consistent with a broadening of tuning in the aged auditory cortex.</p><p>Careful artefact removal from fMRI data is particularly important for age comparisons. Therefore, we re-ran the decoding analysis on ICA-cleaned data (using AROMA, <xref ref-type="bibr" rid="bib56">Pruim et al., 2015</xref>). Irrespective of preprocessing without (<xref ref-type="fig" rid="fig5">Figure 5</xref>) or with ICA-cleaning (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), the key results remained unchanged, that is, reconstruction accuracies peaking at temporal rates of 4–8 Hz as well as selectivity for temporal rates being higher in young than in older participants.</p><p>Also, this key result of a broadened tuning to temporal rate was corroborated by an even simpler measure of selectivity: The variance in the reconstruction accuracy profiles across the six temporal rate bins (1–32 Hz, ‘rate variance’) was higher in the young than in the older participants (mean difference in variance = 3.7×10<sup>−4</sup>, p&lt;0.001 [permutation test], Cohen’s <italic>d</italic> = 0.93, <xref ref-type="fig" rid="fig6">Figure 6</xref> a, left).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Variance along reconstruction accuracy profiles as alternative measure for cortical tuning selectivity.</title><p>(<bold>a</bold>) Variance of the MTFs’ marginal profiles compared between age groups. <italic>P</italic>-values are based on an exact permutation test. (<bold>b</bold>) Amongst the older group, the variance of the rate profiles (‘rate variance’) correlates with the <italic>SI</italic> for rate (left panel) and chronological age (middle panel), but not with hearing loss (right panel; Spearman’s correlation coefficient). MTF: modulation transfer function, <italic>SI</italic>: selectivity index, PTA: pure tone average.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig6-v2.tif"/></fig><p>In the older group, the rate variance was significantly correlated with the rate selectivity index and chronological age but was uncorrelated with hearing loss (<xref ref-type="fig" rid="fig6">Figure 6</xref>, b). Note however that the rate selectivity index (see above) in the older group was not significantly correlated with chronological age (Spearman’s rho = −0.2, p=0.289), nor with hearing loss (Spearman’s rho = 0.34, p=0.062).</p><p>In a control analysis, we ensured that these results cannot be explained by the overall slightly worse model fits in the older group as quantified by sound identification accuracy (as evident in <xref ref-type="fig" rid="fig2">Figure 2</xref>): In a multiple regression predicting older participants’ variance in rate from chronological age with covariates PTA and sound identification accuracy, neither PTA (<italic>t(26)</italic> = 0.73, p=0.473, permutation test) nor sound identification accuracy (<italic>t(26)</italic> = 1.02, p=0.312) were significant, while age was significant (<italic>t(26)</italic> = −2.8, p=0.003; <italic>adjusted R<sup>2</sup></italic> = 0.39). Age remained significant when excluding two multivariate outliers identified based on Cook’s distance (<italic>t</italic>(24) = −2.25, p=0.021).</p></sec><sec id="s2-2-3"><title>Feature reconstruction from auditory subregions</title><p>To test the differential sensitivity of auditory cortical subregions to acoustic features, we re-ran the decoding analyses in the following regions of interest (ROIs) derived from the FreeSurfer atlas (<xref ref-type="bibr" rid="bib13">Destrieux et al., 2010</xref>): Heschl’s gyrus and sulcus (HG/HS), planum polare (PP), planum temporale (PT), superior temporal gyrus (STG) and superior temporal sulcus (STS, <xref ref-type="fig" rid="fig7">Figure 7a - f</xref>)). As control regions, we chose primary visual cortex (V1, calcarine sulcus) as another primary sensory area, and two higher-level areas, namely the middle frontal gyrus (MFG) and superior parietal gyrus (SPG; <xref ref-type="fig" rid="fig7">Figure 7a, g-i</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Decoding of temporal rate from regions of interest (ROIs).</title><p>(<bold>a</bold>) ROIs derived from the FreeSurfer atlas (<xref ref-type="bibr" rid="bib13">Destrieux et al., 2010</xref>) are displayed on FreeSurfer’s <italic>fsaverage</italic> template. Mean (± SEM) reconstruction accuracy profiles (left) and selectivity index (right) per age group for temporal rate in (<bold>b</bold>) Heschl’s gyrus and sulcus, (<bold>c</bold>) planum temporale, (<bold>d</bold>) planum polare, (<bold>e</bold>) superior temporal gyrus, (<bold>f</bold>) superior temporal sulcus and the control regions (<bold>g</bold>) calcarine sulcus, (<bold>h</bold>) superior parietal gyrus, (<bold>i</bold>) middle frontal gyrus. The selectivity index (<italic>SI</italic>) was compared per ROI between age groups using an exact permutation test. When removing the young-subject <italic>SI</italic> exceeding the grand average by ±2 SD, the <italic>SI</italic> age difference remains significant in HG/HS (p=0.008), and in PT (p=0.04). Consistently, our alternative measure of selectivity, variance across rate bins, was significantly higher in young than older adults in HG/HS (p=0.0004) and PT (p=0.0003). See <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for the reconstruction accuracies z-scored with respect to the empirical null distribution in an exemplary region HG/HS. See <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> for group maps of the rate selectivity index obtained from encoding models. See <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref> for decoding of spectral scale and frequency from ROIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Z-scored reconstruction accuracies from Heschl’s gyrus/sulcus.</title><p>Single subject and median reconstruction accuracy profiles for temporal rate, spectral scale and frequency z-scored with respect to the empirical null distribution for young (<bold>a</bold>) and older (<bold>b</bold>) participants. Note that all z-values exceed 1.96 and are thus significant at p&lt;0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Topographic maps for the rate selectivity index.</title><p>Maps were derived by calculating the <italic>SI</italic> across the rate profile for a given voxel and averaged in the young group (upper panel) and older group (bottom panel). Higher <italic>SI</italic> (red) indicates higher selectivity. Black outlines indicate Heschl’s gyrus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig7-figsupp2-v2.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Decoding of spectral scale and frequency in ROIs.</title><p>Mean (± SE) reconstruction accuracy profiles (left) and selectivity index (right) per age group for spectral scale and frequency in (<bold>a</bold>) Heschl’s gyrus and sulcus (HG/HS), (<bold>b</bold>) planum temporale (PT), (<bold>c</bold>) planum polare (PP), (<bold>d</bold>) superior temporal gyrus (STG), (<bold>e</bold>) superior temporal sulcus (STS) and the control regions (<bold>f</bold>) calcarine sulcus (V1), (<bold>g</bold>) superior parietal gyrus (SPG) and (h) middle frontal gyrus (MFG). Selectivity index was compared per ROI between age groups using exact permutation test. Black line indicates the median <italic>SI</italic>. Note that removing the young outlier participant (&gt;2 SD away from the mean) for scale in PT reduces the mean age difference to a still significant <italic>SI</italic> difference = 0.015, p=0.026.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-fig7-figsupp3-v2.tif"/></fig></fig-group><p>A decrease in the rate selectivity index with age was confirmed for HG/HS (mean <italic>SI</italic> difference = 0.012, p=0.009 [permutation test]) and PT (mean <italic>SI</italic> difference = 0.016, p=0.005 [permutation test]; see <xref ref-type="fig" rid="fig7">Figure 7b,c</xref>). Removing the extreme cases in the young group exceeding the grand average by ±2 SD reduces the mean <italic>SI</italic> age difference to 0.012 (p=0.008) in HG/HS, and to 0.012 (p=0.04) in PT. The alternative measure of selectivity, variance of the rate profiles, was also significantly higher in young than older adults in HG/HS (variance difference = 4×10<sup>−4</sup>, p=0.0004) and PT (variance difference = 5×10<sup>−4</sup>, p=0.0003).</p><p>Note that reconstruction accuracies in all ROIs but the control regions (V1, SPG, MFG) were significant (permutation tests; for the z-scored reconstruction accuracies in an exemplary region HG/HS see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><p>In contrast, there were no age differences in temporal selectivity in PP, STG, STS or the control regions V1, SPG and MFG (<xref ref-type="fig" rid="fig7">Figure 7d</xref> – i).</p><p>Qualitatively, this observation was confirmed by a control analysis, where we derived topographic maps of the rate selectivity index from the encoding models. To this end, we marginalized MTFs for rate and calculated the <italic>SI</italic> for a given voxel. The topographic distribution confirmed highest voxel-wise <italic>SI</italic> for rate in area PT of the younger group (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>).</p><p>These results indicate that age-related changes in temporal selectivity selectively occur in primary auditory cortex and the planum temporale, but not surrounding belt areas. For spectral scale, we also found a decreased <italic>SI</italic> in older adults in HG/HS (<italic>SI</italic> difference = 0.012, p=0.008) and PT (<italic>SI</italic> difference = 0.016, p=0.021), but none of the other ROIs (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>). For frequency, we did not find any age differences in selectivity index at all (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This cross-sectional approach used encoding and decoding models of hemodynamic responses to compare the spectro-temporal fidelity in auditory cortex of younger and older listeners. We pursued the long-standing but largely understudied question whether temporal and/or spectral modulations, which are key acoustic dimensions of behaviourally relevant signals such as speech and music, are represented in a less differentiated fashion in the aging auditory cortex.</p><p>We find the large-scale topographic organization of acoustic features entirely preserved in the aged auditory cortex, and best feature maps for frequency, scale and rate accordingly were correlated very highly between age groups.</p><p>However, age-related differences are evident in the cortical sensitivity to mainly temporal modulations as quantified by the decoding analysis. Decoding of temporal rate is most accurate at slow rates of 4–8 Hz irrespective of age, but the tuning to temporal rate is significantly sharper in young than in older participants. Also, amongst older adults, broadening of rate tuning correlates with chronological age. Importantly, the age-related rate selectivity difference was driven by primary auditory (HG) and adjacent area PT, while secondary auditory areas PP, STG, and STS did not show age differences.</p><sec id="s3-1"><title>Preserved topographic organization in aging auditory cortex</title><p>Using natural stimuli, we revealed topographic maps of acoustic feature preference for frequency, spectral and temporal modulations. Thus, we show that topographic maps of feature preference can be observed under approximately natural hearing situations. Tonotopic maps exhibited the well-established mirror-symmetric high-low-high frequency gradients across Heschl’s gyrus and adjacent areas (<xref ref-type="fig" rid="fig3">Figure 3</xref>) consistent with abundant evidence in human (<xref ref-type="bibr" rid="bib19">Formisano et al., 2003</xref>; <xref ref-type="bibr" rid="bib66">Talavage et al., 2004</xref>; <xref ref-type="bibr" rid="bib31">Humphries et al., 2010</xref>; <xref ref-type="bibr" rid="bib72">Woods et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Da Costa et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Striem-Amit et al., 2011</xref>; <xref ref-type="bibr" rid="bib40">Langers and van Dijk, 2012</xref>; <xref ref-type="bibr" rid="bib45">Moerel et al., 2012</xref>; <xref ref-type="bibr" rid="bib47">Moerel et al., 2014</xref>) and non-human primates (<xref ref-type="bibr" rid="bib44">Merzenich and Brugge, 1973</xref>; <xref ref-type="bibr" rid="bib48">Morel et al., 1993</xref>; <xref ref-type="bibr" rid="bib37">Kosaki et al., 1997</xref>; <xref ref-type="bibr" rid="bib57">Rauschecker et al., 1997</xref>; <xref ref-type="bibr" rid="bib7">Bendor and Wang, 2008</xref>; <xref ref-type="bibr" rid="bib5">Baumann et al., 2013</xref>; <xref ref-type="bibr" rid="bib32">Joly et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Baumann et al., 2015</xref>). Notably, tonotopic maps in younger versus older adults were highly correlated, indicating topographical stability of tonotopic organization across age groups.</p><p>The well-demonstrated existence of tonotopic maps notwithstanding, a topographic organization for temporal or spectral modulations in auditory cortex is less clear (<xref ref-type="bibr" rid="bib62">Schönwiesner and Zatorre, 2009</xref>). Here, in both young and older adults we observed a medial-to-lateral low-to-high gradient for temporal modulations across the superior temporal plane (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), while for spectral modulations, the locus with the highest resolution (~1.5 cyc/oct) was observed in lateral Heschl’s gyrus (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). A topographic representation for modulation rate has previously been observed in the primate auditory cortex (<xref ref-type="bibr" rid="bib6">Baumann et al., 2015</xref>) where fast temporal acoustic information was preferably encoded in caudal auditory regions (<xref ref-type="bibr" rid="bib9">Camalier et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Kusmierek and Rauschecker, 2014</xref>) and slow temporal information in rostral areas R and RT (<xref ref-type="bibr" rid="bib42">Liang et al., 2002</xref>; <xref ref-type="bibr" rid="bib7">Bendor and Wang, 2008</xref>).</p><p>Whereas earlier human fMRI studies had failed to show a clear topography for modulation rate (<xref ref-type="bibr" rid="bib22">Giraud et al., 2000</xref>; <xref ref-type="bibr" rid="bib62">Schönwiesner and Zatorre, 2009</xref>; <xref ref-type="bibr" rid="bib50">Overath et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Leaver and Rauschecker, 2016</xref>), recent human fMRI (<xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref>) and electrocorticography (ECoG) studies (<xref ref-type="bibr" rid="bib30">Hullett et al., 2016</xref>) proposed the presence of a posterior-to-anterior high-to-low rate gradient in human auditory cortex. However, comparison of those studies is hampered by the differences in coverage and ranges of temporal modulations: While <xref ref-type="bibr" rid="bib30">Hullett et al., 2016</xref> examined representation of slow temporal modulation rates present in speech (1–2 Hz) only along the STG with ECoG, <xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref> analyzed rates of 1–27 Hz, whereas <xref ref-type="bibr" rid="bib6">Baumann et al., 2015</xref> presented rates of up to 512 Hz with more comprehensive coverage using fMRI.</p></sec><sec id="s3-2"><title>Functional mapping from natural sounds is feasible using conventional MR field strength in aging populations</title><p>While previous studies had presented single natural sounds (<xref ref-type="bibr" rid="bib14">Erb et al., 2019a</xref>), or even synthetic sounds such as AM stimuli (<xref ref-type="bibr" rid="bib6">Baumann et al., 2015</xref>) or dynamic ripples (<xref ref-type="bibr" rid="bib62">Schönwiesner and Zatorre, 2009</xref>), here, we presented a continuous stream of speech embedded in an acoustically rich background of sound textures. As an important foundation for the main conclusions of this manuscript, we here demonstrate the capacity to derive meaningful tonotopic maps from natural stimulus conditions, at conventional field strengths (3 T) and in special populations (older participants).</p><p>Although an inherent caveat of the use of natural stimulus conditions is the difficulty of measuring psychoacoustic performance, this experimental design has the advantage that it closely approximates natural listening conditions (<xref ref-type="bibr" rid="bib25">Hamilton and Huth, 2020</xref>). As synthetic sounds lack both the behavioural relevance and the statistical structure of natural sounds, they activate auditory cortex differently than natural stimuli (<xref ref-type="bibr" rid="bib67">Theunissen et al., 2000</xref>; <xref ref-type="bibr" rid="bib8">Bitterman et al., 2008</xref>).</p><p>Methodologically, our encoding approach relies on the assumption that acoustic features that maximally activate single voxels are more accurately encoded. However, higher responses may not necessarily mean better encoding. The second, complementary decoding analysis incorporates the whole range of values rather than only the maximal values. The reconstruction accuracy explicitly quantifies the amount of information about a set of stimulus features that is available in a region of interest. Accurate reconstruction of acoustic features in test sounds indicates that these features are reproducibly mapped into distinct spatial patterns. As data from individual voxels are jointly modelled, this combined analysis of signals from multiple voxels increases the sensitivity for stimulus information that may be represented in patterns of responses, rather than in individual voxels (<xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref>).</p></sec><sec id="s3-3"><title>Broadened temporal tuning in aging auditory cortex</title><p>Under the assumption that the reconstruction accuracy reflects tuning properties of neuronal populations, the observed decrease in selectivity reveals a broadening of temporal tuning functions with age. This finding provides mechanistic insights into the changes of temporal processing in the aging auditory cortex: The broadened temporal tuning, reminiscent of an age-related decrease in temporal precision in the midbrain (<xref ref-type="bibr" rid="bib3">Anderson et al., 2012</xref>), may underlie the difficulties older adults experience in speech comprehension in noisy environments (<xref ref-type="bibr" rid="bib2">Anderson et al., 2011</xref>). While the present study lacks more direct measures of encoding along the auditory pathway, it is important to note that rate selectivity in the older listeners was statistically unrelated to their pure-tone audiograms (this holds true for both presented measures of tuning, the selectivity index [Figure 5] and the variance across temporal rates [<xref ref-type="fig" rid="fig6">Figure 6</xref>]). Thus, the broadening of temporal tuning is not explicable merely by peripheral hearing loss. Future studies need to carefully assess the consequences of altered cortical temporal tuning on behavioural and psychoacoustic performance (<xref ref-type="bibr" rid="bib15">Erb et al., 2019b</xref>; <xref ref-type="bibr" rid="bib18">Flinker et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Holmes and Griffiths, 2019</xref>), a measure that was precluded by the use of natural sounds in the current study.</p><p>The observation of decreased temporal selectivity may seem at odds with previous accounts of elevated cortical responses to the temporal speech envelope in aging. Evidence for better reconstruction of temporal envelopes from older listeners’ MEG signals (<xref ref-type="bibr" rid="bib55">Presacco et al., 2016</xref>) and larger N1 amplitudes in older adults with widened auditory filters (<xref ref-type="bibr" rid="bib27">Herrmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Henry et al., 2017</xref>) suggest an enhanced cortical envelope representation. However, an inflated cortical representation of the speech envelope has been argued to rather be a maladaptive response as compensation of the lack of temporal precision in the midbrain (<xref ref-type="bibr" rid="bib3">Anderson et al., 2012</xref>).</p><p>Neuronal coding of temporal modulations follows two distinct principles, that is rate and temporal coding (<xref ref-type="bibr" rid="bib33">Joris et al., 2004</xref>). While rate coding constitutes a variation of overall spike rate as a function of modulation frequency, temporal coding is achieved through phase-locking to the stimulus envelope. Fast temporal modulations (&gt;50 Hz) are typically encoded through the rate code, while neurons tuned to slow temporal rates (&lt;50 Hz) mostly synchronize to the sounds’ modulations (temporal code, <xref ref-type="bibr" rid="bib59">Sachs, 1984</xref>; <xref ref-type="bibr" rid="bib33">Joris et al., 2004</xref>). It remains unclear how hemodynamic responses reflect these two types of neuronal coding and how those are in turn related to the observed age-related broadening of tuning functions. Evidence from electrophysiology suggests a change in coding strategy across neuronal populations in A1 from less temporal coding towards more rate coding of temporal modulations in aged animals (<xref ref-type="bibr" rid="bib49">Ng and Recanzone, 2018</xref>).</p><p>Interestingly, the observed age-related decrease in selectivity to temporal modulations was restricted to areas HG/HS and PT, the putative homologues of areas A1 and CL in the macaque (<xref ref-type="bibr" rid="bib34">Kaas and Hackett, 2000</xref>). Both areas have been shown to exhibit altered temporal encoding in older macaques (<xref ref-type="bibr" rid="bib49">Ng and Recanzone, 2018</xref>). In a direct comparison of the vector strengths (a metric of periodicity of the neuronal response to a modulated signal) as a function of the inter-stimulus interval, responses differed in young A1 and CL neurons such that A1 neurons had higher vector strength. This is consistent with the notion that CL neurons process predominantly spatial (rather than temporal) information. Therefore, so the theory, CL neurons would not necessarily need to pass on the temporal coding of A1 neurons. This difference between belt and core areas was abandoned in the older animals, where aged A1 were similar to both the aged and young CL neurons, indicating that the temporal fidelity of A1 responses decreases with age.</p><p>However, a good temporal fidelity of A1 neurons is thought to be critical for temporal processing. We speculate that such an age-related decline in temporal processing may be linked to an imbalance of excitation and inhibition observed in aging (<xref ref-type="bibr" rid="bib70">Voytek et al., 2015</xref>). Inhibitory (e.g. GABAergic or glycinergic) neurotransmitters have been shown to increase response synchrony to modulated stimuli in both the cochlear nucleus and the inferior colliculus (<xref ref-type="bibr" rid="bib36">Koch and Grothe, 1998</xref>; <xref ref-type="bibr" rid="bib4">Backoff et al., 1999</xref>, for review see <xref ref-type="bibr" rid="bib10">Caspary et al., 2008</xref>), suggesting that inhibitory neurotransmission is of particular importance to precise neural timing and, thus, the adequate tracking of temporal sound features.</p><p>An inherent challenge to age group comparisons is to distinguish any unspecific decrease of SNR in the data of older participants (e.g., due to vascular changes that impact the BOLD response <xref ref-type="bibr" rid="bib20">Garrett et al., 2017</xref> or due to the typical increase in movement artefacts) from specific age-related sensory processing changes. Most reassuringly, however, a multiple regression model taking into account overall goodness of the encoding model fit (i.e., sound identification accuracy) did not alter the observed relationship between age and temporal rate tuning.</p></sec><sec id="s3-4"><title>Conclusions</title><p>The present study aimed to narrow the gap between recent progress in modelling the neural processing of acoustic features from natural stimuli on the one hand, and the understanding of potential senescent changes in these cortical stimulus representations on the other hand. Although the large-scale topographic organization of acoustic features appears preserved in the auditory cortex of older compared to younger listeners, age-related differences in the marginal profiles of multi-voxel MTFs were evident. Tuning to slow temporal rates which abounds in natural sounds and especially in speech (<xref ref-type="bibr" rid="bib14">Erb et al., 2019a</xref>) was markedly sharper in young compared to older participants. Consistent with previous findings in the macaque, these results suggest that temporal rate selectivity in auditory cortex declines in normal aging. The specificity of this decline, confined to primary auditory and adjacent areas and sparing tonotopic representations, opens a new lead in the ongoing search for tractable neurobiological signatures of older adults’ widely observed deficits in speech comprehension in noisy environments.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We invited a total of <italic>n = 75</italic> participants for scanning from which we had to exclude one participant due to excessive movement, three participants due to incidental neurological findings, five participants due to their inability to understand speech in noise, one participant due to broken headphones, one participant due to age (43 years, i.e., could not be assigned to young or older group). The remaining <italic>n</italic> = 64 participants were right-handed, young (<italic>n</italic> = 33; aged 18–32, mean 24.7 years18 female) and older (<italic>n</italic> = 31; aged 51–78, mean 63.8 years, 15 female) native German speakers. Simulations (custom matlab code) showed that a two-sample permutation test at a conventional type I error rate of 5% with a sample size of <italic>N</italic> = 27 per group can detect medium to large effects of Cohen’s <italic>d</italic> = 0.75 (as can be expected for the neural fMRI measures under consideration, for example <xref ref-type="bibr" rid="bib1">Alavash et al., 2019</xref>) with a satisfactory power of 80%. The power of our procedure reduces accordingly if the true effect in the population is smaller but remains over 60% also for true effects closer to Cohen’s <italic>d</italic> = 0.5.</p><p>Younger participants had self-reported normal hearing. Older participants’ hearing ranged from normal hearing to mild hearing loss. The older participants were part of a cohort which is regularly tested in the lab (e.g., <xref ref-type="bibr" rid="bib1">Alavash et al., 2019</xref>). They were recruited based on their audiograms that had been acquired within the two years prior to the experiment (for audiograms see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>) and were excluded from the study if the pure-tone-average (PTA) of the better ear exceeded 30 dB HL. On average, hearing thresholds are expected to increase by approximately 0.25–1.7 dB HL per year at the age range of 48–79 years (<xref ref-type="bibr" rid="bib71">Wiley et al., 2008</xref>). Thus, we considered 2-year-old audiograms to be valid. All participants gave informed consent and were financially compensated or received course credit. All procedures were approved by the local ethics committee of the University of Lübeck (ethical approval AZ 16–107).</p></sec><sec id="s4-2"><title>Stimuli and task</title><p>Participants listened to 64 min of a freely narrated audiobook (Hertha Müller, ‘<italic>Die Nacht ist aus Tinte gemacht’</italic>) presented against a competing stream of resynthesized natural sounds (‘sound textures’; <xref ref-type="bibr" rid="bib43">McDermott and Simoncelli, 2011</xref>) at 0 dB SNR. Textures were synthesized from the spectro-temporal modulation content of a large set of real-life sounds (<italic>n</italic> = 192), including speech and vocal samples, music pieces, animal cries, scenes from nature and tool sounds that had been used in previous studies (<xref ref-type="bibr" rid="bib46">Moerel et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref>). Texture synthesis parameters were as follows: Frequency range = 0.02–10 kHz, number of frequency bands = 30, sampling rate = 20 kHz, temporal modulation range = 0.5–200 Hz, sampling rate = 400 Hz; maximum number of iterations = 60. Texture exemplars of 5 s length were concatenated to form the background stream. The order of exemplars was pseudo-randomized across participants (in four different sound orders). In total, each exemplar was repeated four times.</p><p>Participants were asked to listen to the story and answer three four-choice questions on its semantic content after each run (24 questions in total); chance level was thus proportion correct = 0.25. Four-answer choice questions were displayed on a screen and participants responded via a button box in their right hand. Both the young and older group performed on average above chance (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>), but young participants performed significantly better than older participants (mean proportion correct difference = 0.1, p=0.009 [permutation test], Cohen’s <italic>d</italic> = 0.65).</p></sec><sec id="s4-3"><title>Acquisition of MRI data</title><p>We acquired functional and structural MRI at 3T (Siemens Magnetom Skyra) with a 64-channel RF head array coil.</p><p>Functional T2*-weighted data were acquired using an echo planar imaging sequence. We collected eight runs of eight minutes each (each run contained 519 volumes) using continuous scanning. The acquisition parameters were as follows: repetition time (TR) = acquisition time (TA) = 947 ms, echo time (TE) = 28 ms, acceleration factor = 4, flip angle = 60°, field of view (FOV) = 200×200 mm, 52 slices; voxel size = 2.5 mm isotropic (whole-brain coverage). Field maps for intensity inhomogeneity correction were acquired after every second run (TR = 610 ms, TE1 = 4.92, TE2 = 7.38, flip angle = 60°, FOV = 200×200 mm, 62 slices, voxel size = 2.5 mm isotropic).</p><p>Anatomical T1-weighted images were acquired using an MPRAGE sequence (TR = 2400 ms, time to inversion [TI]=1000 ms, TE = 3.16 ms, flip angle = 8°, FOV = 256×256 mm, number of slices = 176, voxel size = 1 mm isotropic, GRAPPA acceleration factor = 2). T2-weighted images were collected at the end of the session (TR = 3200 ms, TE = 449 ms, FOV = 256×256 mm, number of slices = 176, voxel size = 1 mm isotropic, GRAPPA acceleration factor = 2).</p></sec><sec id="s4-4"><title>Preprocessing of MR data</title><p>Results included in this manuscript are based on a preprocessing pipeline of <italic>fMRIPprep</italic> 1.2.4 which is based on <italic>Nipype</italic> 1.1.6 (<xref ref-type="bibr" rid="bib17">Esteban et al., 2019</xref>). The following two paragraphs on preprocessing are based on an automatically generated output of <italic>fMRIPprep</italic>.</p><sec id="s4-4-1"><title>Anatomical data preprocessing</title><p>The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) using N4BiasFieldCorrection (ANTs 2.2.0) and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped using antsBrainExtraction.sh (ANTs 2.2.0), using OASIS as target template. Brain surfaces were reconstructed using recon-all (FreeSurfer 6.0.1), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle. Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template version 2009c was performed through nonlinear registration with antsRegistration (ANTs 2.2.0), using brain-extracted versions of both T1w volume and template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (FSL 5.0.9).</p></sec><sec id="s4-4-2"><title>Functional data preprocessing</title><p>For each of the eight BOLD runs, the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of <italic>fMRIPrep</italic>. A deformation field to correct for susceptibility distortions was estimated based on a field map that was co-registered to the BOLD reference, using a custom workflow of <italic>fMRIPrep</italic>. Based on the estimated susceptibility distortion, an unwarped BOLD reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration. Co-registration was configured with nine degrees of freedom to account for distortions remaining in the BOLD reference. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9). BOLD runs were slice-time corrected using 3dTshift from AFNI 20160207. The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. In a control analysis, ICA-based Automatic Removal Of Motion Artifacts (AROMA) was used to generate a variant of data that is non-aggressively denoised (<xref ref-type="bibr" rid="bib56">Pruim et al., 2015</xref>). All analyses (see below) were performed in native space. Subsequently, results (best feature maps) were resampled to surfaces on the <italic>fsaverage5</italic> template using mri_vol2surf (FreeSurfer).</p></sec></sec><sec id="s4-5"><title>Modulation representation</title><p>The modulation content of the stimuli was obtained by filtering the sounds within a biologically plausible model of auditory processing (<xref ref-type="bibr" rid="bib11">Chi et al., 2005</xref>). This auditory model consists of an early stage that models the transformations that acoustic signals undergo from the cochlea to the midbrain; and a cortical stage that accounts for the processing of the sounds at the level of the auditory cortex. We derived the spectrogram and its modulation content using the ‘NSL Tools’ package (available at <ext-link ext-link-type="uri" xlink:href="http://www.isr.umd.edu/Labs/NSL/Software.htm">http://www.isr.umd.edu/Labs/NSL/Software.htm</ext-link>) and customized Matlab code (The MathWorks Inc, Matlab 2014b/2018a).</p><p>Spectrograms for all sounds were obtained using a bank of 128 overlapping bandpass filters with equal width (Q<sub>10dB</sub> = 3), spaced along a logarithmic frequency axis over a range of <italic>f</italic> = 180–7040 Hz. The output of the filter bank was band-pass filtered (hair cell stage). A midbrain stage modelled the enhancement of frequency selectivity as a first-order derivative with respect to the frequency axis, followed by a half-wave rectification and a short-term temporal integration (time constant <italic>τ</italic> = 8 ms).</p><p>Next, the auditory spectrogram was further analyzed by the cortical stage, where the modulation content of the auditory spectrogram was computed through a bank of 2-dimensional filters selective for a combination of spectral and temporal modulations. The filter bank performs a complex wavelet decomposition of the auditory spectrogram. The magnitude of such decomposition yields a phase-invariant measure of modulation content. The modulation selective filters have joint selectivity for spectral and temporal modulations, and are directional, that is they respond either to upward or downward frequency sweeps.</p><p>Filters were tuned to spectral modulation frequencies of Ω = [0.3, 0.4, 0.8, 1.3, 2.3, 4] cyc/oct, temporal modulation frequencies of ω = [1, 2, 4, 8, 16, 32] Hz, and centre frequencies of <italic>f</italic> = [232, 367, 580, 918, 1452, 2297, 3633, 5746] Hz. Our rationale for this choice of values was to use a decomposition roughly covering the temporal and spectral modulations present in the acoustic energy of natural sounds we used (for spectro-temporal modulation content of the sounds see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). To avoid overfitting, for the decoding analyses in regions of interest (ROIs), we reduced the number of features such that filters were tuned to spectral modulation frequencies of Ω = [0.25, 0.5, 1, 2, 4] cyc/oct, temporal modulation frequencies of ω = [1, 2.4, 5.7, 13.5, 32] Hz, and centre frequencies of <italic>f</italic> = [277, 576, 1201, 2502, 5213] Hz.</p><p>The filter bank output was computed at each frequency along the tonotopic axis and then averaged over time. This resulted for the encoding (ROI decoding) analysis in a representation with 6 (5) spectral modulation frequencies × 6 (5) temporal modulation frequencies × 8 (5) tonotopic frequencies = 288 (125) parameters to learn. The time-averaged output of the filter bank was averaged across the upward and downward filter directions (<xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref>). Those processing steps were applied to all stimuli, resulting in an [<italic>N x F</italic>] feature matrix <italic>S</italic> of modulation energy, where <italic>N</italic> is the number of sounds, and <italic>F</italic> is the number of features in the modulation representation.</p><p>Prior to acoustic feature extraction, the continuous sounds (~8 min per run) were cut into snippets of the length of the TR (947 ms) resulting in a total of <italic>n</italic> = 4152 sounds which were subdivided into training (n = 3114) and test sounds (n = 1038) for the cross-validation procedure (see below). This resulted in an equivalent temporal resolution of the feature matrix and the fMRI data.</p><p>Note that for the encoding analysis (see below), the number of parameters to estimate is thus smaller than the number of observations in the training set (<italic>n</italic> = 3114 training sounds). In the decoding analysis, the number of fitted features is limited by the number of voxels instead (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). Each feature was convolved with the standard double gamma model for the hemodynamic response function peaking at 4 s.</p></sec><sec id="s4-6"><title>Encoding and decoding models</title><p>We applied two modelling approaches to fMRI data as described in <xref ref-type="bibr" rid="bib15">Erb et al., 2019b</xref>; <xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref>; <xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref> (<xref ref-type="fig" rid="fig1">Figure 1</xref>). In a first <italic>univariate encoding</italic> approach, we calculated a modulation transfer function (MTF) for each individual voxel. The MTF characterizes how faithfully the modulation content of the stimulus gets transferred to the voxel. By assigning the feature value with the maximal response to each voxel, we obtained maps of a voxel’s best features across the auditory cortex.</p><p>In a second <italic>multivariate decoding</italic> approach, data from voxels were jointly modelled within a model-based decoding framework. The combined analysis of signals from multiple voxels increases the sensitivity for stimulus information that is represented in patterns of activity, rather than in individual voxels. Further, the accuracy with which those features can be reconstructed provides an explicit measure of the amount of information about sound features available in cortex.</p><sec id="s4-6-1"><title>Univariate encoding analysis: Model estimation</title><p>Based on the training data only, the fMRI activity <italic>Y<sub>i</sub> [N<sub>train</sub> ×1]</italic> at voxel <italic>i</italic> was modeled as a linear transformation of the feature matrix <italic>S<sub>train</sub> [N<sub>train</sub> ×F]</italic> plus a noise term <italic>n [N<sub>train</sub> ×1]</italic>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:math></disp-formula>where <italic>N<sub>train</sub></italic> is the number of sounds in the training set, and <italic>C<sub>i</sub></italic> is an [<italic>F</italic> × 1] vector of model parameters, whose elements <italic>c<sub>ij</sub></italic> quantify the contribution of feature <italic>j</italic> to the overall response of voxel <italic>i</italic>. Columns of matrices <italic>S<sub>train</sub></italic> and <italic>Y<sub>i</sub></italic> were converted to standardized z-scores. Therefore, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> does not include a constant term. The solution to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> was computed using kernel ridge regression (<xref ref-type="bibr" rid="bib28">Hoerl and Kennard, 1970</xref>). The regularization parameter λ was selected independently for each voxel via generalized cross-validation (<xref ref-type="bibr" rid="bib23">Golub et al., 1979</xref>). The search grid included 25 values between 10<sup>−6</sup> and 10<sup>6</sup> logarithmically spaced with a grid grain of 10<sup>0.5</sup>.</p><p>To obtain more stable estimates of the voxels’ feature profiles, this computation was performed in a fourfold cross-validation procedure within each participant using different subsets of the eight runs. For each iteration, two out of the eight runs were selected and left out for testing, resulting in a subset of 3114 training sounds on which the estimation was performed and 1038 test sounds. In this way, we obtained four estimates of each voxel’s feature profile which were averaged across iterations.</p></sec><sec id="s4-6-2"><title>Model evaluation</title><p>To evaluate the model’s prediction accuracy, we performed a sound identification analysis (<xref ref-type="bibr" rid="bib35">Kay et al., 2008</xref>). To this end, we used the fMRI activity patterns predicted by the model to identify which of the test sounds had been heard. Given the trained model <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> [<italic>F</italic> × <italic>V</italic>], and the feature matrix <italic>S<sub>test</sub></italic> [<italic>N<sub>test</sub> × F</italic>] for the test set, the predicted fMRI activity <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><sub>test</sub> [<italic>N<sub>test</sub> × V</italic>] for the test sounds was obtained as:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Then, we computed for each stimulus <italic>s<sub>k</sub></italic> the correlation <italic>r<sub>s</sub></italic> between its predicted fMRI activity <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><sub>test</sub>(<italic>s</italic>) [<italic>1 × V</italic>] and all measured fMRI responses <italic>Y<sub>test</sub></italic>(<italic>s</italic>) [<italic>1 × V</italic>]. The rank of the correlation between predicted and observed activity for stimulus <italic>s<sub>k</sub></italic> was used as a measure of the model’s ability to correctly match <italic>Y</italic><sub>test</sub>(<italic>s</italic>) with its prediction <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><sub>test</sub>(<italic>s</italic>). The rank was then normalized between 0 and 1 as follows to obtain the sound identification score <italic>m</italic> for stimulus <italic>s</italic> (<xref ref-type="bibr" rid="bib60">Santoro et al., 2014</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that <italic>m</italic> = 1 indicates a correct match; <italic>m</italic> = 0 indicates that the predicted activity pattern for stimulus <italic>s<sub>i</sub></italic> was least similar to the measured one among all stimuli. Normalized ranks (sound identification scores) were computed for all stimuli in the test set, and the overall model’s accuracy was obtained as the mean of the sound identification scores across stimuli.</p></sec><sec id="s4-6-3"><title>Topographical best-feature maps</title><p>The response profiles for temporal modulation, spectral modulation and frequency were computed as marginal sums of the estimated stimulus-activity mapping function <italic>C</italic> of the frequency-specific modulation model by summing across irrelevant dimensions. For example, to obtain the temporal modulation transfer function (tMTF), we summed across the spectral modulation and frequency dimension:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>M</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mi>f</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To calculate profiles for the spectral modulation transfer function (sMTF) and frequency transfer function (fTF), we correspondingly summed across irrelevant dimensions. The voxels’ best features were defined as the maximum of the tMTF, sMTF and fTF, respectively. Cortical maps were generated by colour-coding the voxels’ preferred values and projecting them onto an inflated representation of the cortex. To obtain group maps, individual maps were transformed to FreeSurfer’s <italic>fsaverage5</italic> space and averaged.</p></sec><sec id="s4-6-4"><title>Multivariate decoding analysis: Model estimation</title><p>In the multivariate decoding analysis, we evaluated the fidelity with which regions of interest (ROIs) in auditory cortex encode acoustic features by estimating decoders. In addition to a region including the whole auditory cortex, we selected five ROIs based on anatomical criteria (using the FreeSurfer labels): Heschl’s gyrus/sulcus (HG/HS), planum temporale (PT), planum polare (PP), superior temporal gyrus (STG) and sulcus (STS). Additionally, we selected three control ROIs, namely the visual cortex (V1, calcarine sulcus), middle frontal (MFG) and superior parietal gyrus (SPG). For each individual in each ROI, a linear decoder was trained for every feature of the modulation space based on the training data only (<xref ref-type="bibr" rid="bib61">Santoro et al., 2017</xref>). Each stimulus feature <italic>S<sub>j</sub></italic> [<italic>N<sub>train</sub> ×1</italic>] was modelled as a linear transformation of the multi-voxel response pattern <italic>Y<sub>train</sub> [N<sub>train</sub> ×V</italic>] plus a bias term <italic>b<sub>j</sub></italic> and a noise term <italic>n</italic> [<italic>N<sub>train</sub> ×1</italic>] as follows:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>N<sub>train</sub></italic> is the number of sounds in the training set, V is the number of voxels, one is a [<italic>N<sub>train</sub> × 1</italic>] vector of ones, and <italic>C<sub>j</sub></italic> is a [<italic>V × 1</italic>] vector of model parameters, whose elements <italic>c<sub>ji</sub></italic> quantify the contribution of voxel <italic>i</italic> to the encoding of feature <italic>j</italic>. All parameters in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> were estimated within the same ridge-regression cross-validation scheme as <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. Here, the regularization parameter λ was determined independently for each feature by generalized cross validation using the identical search grid as in the univariate encoding model (see above).</p></sec><sec id="s4-6-5"><title>Estimation of multi-voxel modulation transfer functions (MTFs)</title><p>Decoders were estimated on the train runs (see above) and tested on the test runs. Given the trained model <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> [<italic>V</italic> × <italic>F</italic>], and the patterns of fMRI activity for the test sounds <italic>Y<sub>test</sub></italic> [<italic>N<sub>test</sub> × V</italic>], the predicted feature matrix activity S<sub>test</sub> [<italic>N<sub>test</sub> × F</italic>] for the test sounds was calculated as:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>C</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Predictions for all features from the test sets were concatenated and decoders were assessed individually by computing the Pearson’s correlation coefficient (<italic>r</italic>) between a predicted and a given actual stimulus feature. For decoding from the whole auditory cortex (ROIs), this resulted in 288 (125) correlation coefficients, which represented the MTF. To obtain marginal profiles of the MTFs, we averaged across the momentarily irrelevant dimensions, respectively.</p><p>Statistical significance of the reconstruction accuracy was assessed with permutation testing. For each stimulus feature we computed the null-distribution of accuracies at the single-subject level by randomly permuting (1,000 times) the stimulus labels of the test sounds and computing the correlation coefficient with the predicted features for each permutation. Note that to reduce computing time and resources, we limited the number of permutations to 1,000 whenever encoding and decoding models had to be estimated for each permutation at the single-subject level. For less computationally intense calculations (group comparisons), we used the default of <italic>n</italic> = 10,000 permutations.</p><p>In order to preserve the temporal correlations among sounds, the predictors were phase scrambled by using the fast Fourier transform (fft), randomly permuting the phase angles of the fft and calculating the inverse fft before convolution with the hemodynamic response function. The empirical chance level of correlation <italic>r<sub>chance</sub></italic> was defined as the mean of the null distribution. The <italic>p</italic>-value was computed as the proportion of permutations that yielded a correlation equal to or more extreme than the empirical one. The reconstruction accuracies were z-scored relative to the empirical null distribution.</p></sec><sec id="s4-6-6"><title>Post-hoc statistical analysis of marginal MTFs</title><p>Group marginal profiles of MTFs were obtained as the mean of all individual marginal MTFs. To assess the statistical significance of the observations on the MTF’s marginal profiles, we performed the following post-hoc analyses.</p></sec><sec id="s4-6-7"><title>Selectivity index</title><p>Based on the marginal profiles of the MTFs, we quantified how selectively a brain region is tuned towards processing the acoustic feature with the highest reconstruction accuracy relative to all other features using a selectivity index<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>r<sub>j</sub></italic> is the reconstruction accuracy at feature <italic>j, k</italic> is the number of features, <italic>j<sub>max</sub></italic> is the index of the maximal reconstruction accuracy. <italic>SI</italic> was calculated for each acoustic dimension separately (i.e., rate, scale or frequency) and compared between age groups using an exact permutation test by calculating the mean group difference and permuting the age group labels 10,000 times.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was funded by an ERC consolidator Grant (ERC-CoG-2014–646696 ‘AUDADAPT’ to JO) and the German Research Foundation (DFG; OB 352/2–1). Martin Göttlich helped with MR sequences. Anne Herrmann, Malte Naujokat, Clara Mergner, and Anne Ruhe helped acquire the data. We are grateful for the methods and analysis tools developed at the Department for Cognitive Neuroscience, Maastricht University, in particular by Federico De Martino, Roberta Santoro and Elia Formisano which were central for the current project. We thank the members of the Auditory Cognition group as well as the editors and reviewers for very constructive feedback on the present study.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Software, Investigation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants gave informed consent and were financially compensated or received course credit. All procedures were approved by the local ethics committee of the University of Lübeck (ethical approval AZ 16-107).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-55300-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>MRI data and custom code to reproduce all essential findings are publicly available on the Open Science Framework (OSF).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Schmitt</surname><given-names>L-M</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>GRASP</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/zbuah/">10.17605/OSF.IO/28R57</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alavash</surname> <given-names>M</given-names></name><name><surname>Tune</surname> <given-names>S</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modular reconfiguration of an auditory control brain network supports adaptive listening behavior</article-title><source>PNAS</source><volume>116</volume><fpage>660</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1073/pnas.1815321116</pub-id><pub-id pub-id-type="pmid">30587584</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>S</given-names></name><name><surname>Parbery-Clark</surname> <given-names>A</given-names></name><name><surname>Yi</surname> <given-names>HG</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A neural basis of speech-in-noise perception in older adults</article-title><source>Ear and Hearing</source><volume>32</volume><fpage>750</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e31822229d3</pub-id><pub-id pub-id-type="pmid">21730859</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>S</given-names></name><name><surname>Parbery-Clark</surname> <given-names>A</given-names></name><name><surname>White-Schwoch</surname> <given-names>T</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Aging affects neural precision of speech encoding</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>14156</fpage><lpage>14164</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2176-12.2012</pub-id><pub-id pub-id-type="pmid">23055485</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Backoff</surname> <given-names>PM</given-names></name><name><surname>Shadduck Palombi</surname> <given-names>P</given-names></name><name><surname>Caspary</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>γ-aminobutyric acidergic and glycinergic inputs shape coding of amplitude modulation in the Chinchilla cochlear nucleus</article-title><source>Hearing Research</source><volume>134</volume><fpage>77</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(99)00071-4</pub-id><pub-id pub-id-type="pmid">10452378</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumann</surname> <given-names>S</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A unified framework for the organization of the primate auditory cortex</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00011</pub-id><pub-id pub-id-type="pmid">23641203</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumann</surname> <given-names>S</given-names></name><name><surname>Joly</surname> <given-names>O</given-names></name><name><surname>Rees</surname> <given-names>A</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Sun</surname> <given-names>L</given-names></name><name><surname>Thiele</surname> <given-names>A</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The topography of frequency and time representation in primate auditory cortices</article-title><source>eLife</source><volume>4</volume><elocation-id>e03256</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03256</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bendor</surname> <given-names>D</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural response properties of primary, Rostral, and rostrotemporal core fields in the auditory cortex of marmoset monkeys</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>888</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1152/jn.00884.2007</pub-id><pub-id pub-id-type="pmid">18525020</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitterman</surname> <given-names>Y</given-names></name><name><surname>Mukamel</surname> <given-names>R</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name><name><surname>Nelken</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Ultra-fine frequency tuning revealed in single neurons of human auditory cortex</article-title><source>Nature</source><volume>451</volume><fpage>197</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1038/nature06476</pub-id><pub-id pub-id-type="pmid">18185589</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camalier</surname> <given-names>CR</given-names></name><name><surname>D'Angelo</surname> <given-names>WR</given-names></name><name><surname>Sterbing-D'Angelo</surname> <given-names>SJ</given-names></name><name><surname>de la Mothe</surname> <given-names>LA</given-names></name><name><surname>Hackett</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural latencies across auditory cortex of macaque support a dorsal stream supramodal timing advantage in primates</article-title><source>PNAS</source><volume>109</volume><fpage>18168</fpage><lpage>18173</lpage><pub-id pub-id-type="doi">10.1073/pnas.1206387109</pub-id><pub-id pub-id-type="pmid">23074251</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspary</surname> <given-names>DM</given-names></name><name><surname>Ling</surname> <given-names>L</given-names></name><name><surname>Turner</surname> <given-names>JG</given-names></name><name><surname>Hughes</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inhibitory neurotransmission, plasticity and aging in the mammalian central auditory system</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1781</fpage><lpage>1791</lpage><pub-id pub-id-type="doi">10.1242/jeb.013581</pub-id><pub-id pub-id-type="pmid">18490394</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname> <given-names>T</given-names></name><name><surname>Ru</surname> <given-names>P</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Da Costa</surname> <given-names>S</given-names></name><name><surname>van der Zwaag</surname> <given-names>W</given-names></name><name><surname>Marques</surname> <given-names>JP</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name><name><surname>Clarke</surname> <given-names>S</given-names></name><name><surname>Saenz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Human primary auditory cortex follows the shape of heschl's gyrus</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>14067</fpage><lpage>14075</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2000-11.2011</pub-id><pub-id pub-id-type="pmid">21976491</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname> <given-names>C</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>A</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>NeuroImage</source><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id><pub-id pub-id-type="pmid">20547229</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erb</surname> <given-names>J</given-names></name><name><surname>Armendariz</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Homology and specificity of natural Sound-Encoding in human and monkey auditory cortex</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3636</fpage><lpage>3650</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy243</pub-id><pub-id pub-id-type="pmid">30395192</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erb</surname> <given-names>J</given-names></name><name><surname>Ludwig</surname> <given-names>AA</given-names></name><name><surname>Kunke</surname> <given-names>D</given-names></name><name><surname>Fuchs</surname> <given-names>M</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Temporal sensitivity measured shortly after cochlear implantation predicts 6-Month speech recognition outcome</article-title><source>Ear and Hearing</source><volume>40</volume><fpage>27</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000588</pub-id><pub-id pub-id-type="pmid">29697465</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erb</surname> <given-names>J</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Upregulation of cognitive control networks in older adults' speech comprehension</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>116</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00116</pub-id><pub-id pub-id-type="pmid">24399939</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname> <given-names>O</given-names></name><name><surname>Markiewicz</surname> <given-names>CJ</given-names></name><name><surname>Blair</surname> <given-names>RW</given-names></name><name><surname>Moodie</surname> <given-names>CA</given-names></name><name><surname>Isik</surname> <given-names>AI</given-names></name><name><surname>Erramuzpe</surname> <given-names>A</given-names></name><name><surname>Kent</surname> <given-names>JD</given-names></name><name><surname>Goncalves</surname> <given-names>M</given-names></name><name><surname>DuPre</surname> <given-names>E</given-names></name><name><surname>Snyder</surname> <given-names>M</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name><name><surname>Wright</surname> <given-names>J</given-names></name><name><surname>Durnez</surname> <given-names>J</given-names></name><name><surname>Poldrack</surname> <given-names>RA</given-names></name><name><surname>Gorgolewski</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flinker</surname> <given-names>A</given-names></name><name><surname>Doyle</surname> <given-names>WK</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Devinsky</surname> <given-names>O</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spectrotemporal modulation provides a unifying framework for auditory cortical asymmetries</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>393</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0548-z</pub-id><pub-id pub-id-type="pmid">30971792</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name><name><surname>Di Salle</surname> <given-names>F</given-names></name><name><surname>van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mirror-symmetric tonotopic maps in human primary auditory cortex</article-title><source>Neuron</source><volume>40</volume><fpage>859</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00669-X</pub-id><pub-id pub-id-type="pmid">14622588</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname> <given-names>DD</given-names></name><name><surname>Lindenberger</surname> <given-names>U</given-names></name><name><surname>Hoge</surname> <given-names>RD</given-names></name><name><surname>Gauthier</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Age differences in brain signal variability are robust to multiple vascular controls</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>10149</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-09752-7</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gates</surname> <given-names>GA</given-names></name><name><surname>Mills</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Presbycusis</article-title><source>The Lancet</source><volume>366</volume><fpage>1111</fpage><lpage>1120</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(05)67423-5</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Wable</surname> <given-names>J</given-names></name><name><surname>Johnsrude</surname> <given-names>I</given-names></name><name><surname>Frackowiak</surname> <given-names>R</given-names></name><name><surname>Kleinschmidt</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Representation of the temporal envelope of sounds in the human brain</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>1588</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.3.1588</pub-id><pub-id pub-id-type="pmid">10980029</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname> <given-names>GH</given-names></name><name><surname>Heath</surname> <given-names>M</given-names></name><name><surname>Wahba</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Generalized Cross-Validation as a method for choosing a good ridge parameter</article-title><source>Technometrics</source><volume>21</volume><fpage>215</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1080/00401706.1979.10489751</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon-Salant</surname> <given-names>S</given-names></name><name><surname>Yeni-Komshian</surname> <given-names>GH</given-names></name><name><surname>Fitzgibbons</surname> <given-names>PJ</given-names></name><name><surname>Barrett</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Age-related differences in identification and discrimination of temporal cues in speech segments</article-title><source>The Journal of the Acoustical Society of America</source><volume>119</volume><fpage>2455</fpage><lpage>2466</lpage><pub-id pub-id-type="doi">10.1121/1.2171527</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The revolution will not be controlled: natural stimuli in speech neuroscience</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>573</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1080/23273798.2018.1499946</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname> <given-names>MJ</given-names></name><name><surname>Herrmann</surname> <given-names>B</given-names></name><name><surname>Kunke</surname> <given-names>D</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Aging affects the balance of neural entrainment and top-down neural modulation in the listening brain</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15801</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15801</pub-id><pub-id pub-id-type="pmid">28654081</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname> <given-names>B</given-names></name><name><surname>Henry</surname> <given-names>MJ</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Altered temporal dynamics of neural adaptation in the aging human auditory cortex</article-title><source>Neurobiology of Aging</source><volume>45</volume><fpage>10</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2016.05.006</pub-id><pub-id pub-id-type="pmid">27459921</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoerl</surname> <given-names>AE</given-names></name><name><surname>Kennard</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Ridge regression: biased estimation for nonorthogonal problems</article-title><source>Technometrics</source><volume>12</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1080/00401706.1970.10488634</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>E</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>'Normal' hearing thresholds and fundamental auditory grouping processes predict difficulties with speech-in-noise perception</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>16771</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-53353-5</pub-id><pub-id pub-id-type="pmid">31728002</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hullett</surname> <given-names>PW</given-names></name><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human superior temporal gyrus organization of spectrotemporal modulation tuning derived from speech stimuli</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>2014</fpage><lpage>2026</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1779-15.2016</pub-id><pub-id pub-id-type="pmid">26865624</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname> <given-names>C</given-names></name><name><surname>Liebenthal</surname> <given-names>E</given-names></name><name><surname>Binder</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Tonotopic organization of human auditory cortex</article-title><source>NeuroImage</source><volume>50</volume><fpage>1202</fpage><lpage>1211</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.01.046</pub-id><pub-id pub-id-type="pmid">20096790</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joly</surname> <given-names>O</given-names></name><name><surname>Baumann</surname> <given-names>S</given-names></name><name><surname>Balezeau</surname> <given-names>F</given-names></name><name><surname>Thiele</surname> <given-names>A</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Merging functional and structural properties of the monkey auditory cortex</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>198</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00198</pub-id><pub-id pub-id-type="pmid">25100930</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joris</surname> <given-names>PX</given-names></name><name><surname>Schreiner</surname> <given-names>CE</given-names></name><name><surname>Rees</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neural processing of amplitude-modulated sounds</article-title><source>Physiological Reviews</source><volume>84</volume><fpage>541</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1152/physrev.00029.2003</pub-id><pub-id pub-id-type="pmid">15044682</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname> <given-names>JH</given-names></name><name><surname>Hackett</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Subdivisions of auditory cortex and processing streams in primates</article-title><source>PNAS</source><volume>97</volume><fpage>11793</fpage><lpage>11799</lpage><pub-id pub-id-type="doi">10.1073/pnas.97.22.11793</pub-id><pub-id pub-id-type="pmid">11050211</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Naselaris</surname> <given-names>T</given-names></name><name><surname>Prenger</surname> <given-names>RJ</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><volume>452</volume><fpage>352</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature06713</pub-id><pub-id pub-id-type="pmid">18322462</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname> <given-names>U</given-names></name><name><surname>Grothe</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>GABAergic and glycinergic inhibition sharpens tuning for frequency modulations in the inferior colliculus of the big Brown bat</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>71</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.80.1.71</pub-id><pub-id pub-id-type="pmid">9658029</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosaki</surname> <given-names>H</given-names></name><name><surname>Hashikawa</surname> <given-names>T</given-names></name><name><surname>He</surname> <given-names>J</given-names></name><name><surname>Jones</surname> <given-names>EG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Tonotopic organization of auditory cortical fields delineated by parvalbumin immunoreactivity in macaque monkeys</article-title><source>The Journal of Comparative Neurology</source><volume>386</volume><fpage>304</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19970922)386:2&lt;304::AID-CNE10&gt;3.0.CO;2-K</pub-id><pub-id pub-id-type="pmid">9295154</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusmierek</surname> <given-names>P</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Selectivity for space and time in early Areas of the auditory dorsal stream in the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>1671</fpage><lpage>1685</lpage><pub-id pub-id-type="doi">10.1152/jn.00436.2013</pub-id><pub-id pub-id-type="pmid">24501260</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalwani</surname> <given-names>P</given-names></name><name><surname>Gagnon</surname> <given-names>H</given-names></name><name><surname>Cassady</surname> <given-names>K</given-names></name><name><surname>Simmonite</surname> <given-names>M</given-names></name><name><surname>Peltier</surname> <given-names>S</given-names></name><name><surname>Seidler</surname> <given-names>RD</given-names></name><name><surname>Taylor</surname> <given-names>SF</given-names></name><name><surname>Weissman</surname> <given-names>DH</given-names></name><name><surname>Polk</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural distinctiveness declines with age in auditory cortex and is associated with auditory GABA levels</article-title><source>NeuroImage</source><volume>201</volume><elocation-id>116033</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116033</pub-id><pub-id pub-id-type="pmid">31326572</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langers</surname> <given-names>DR</given-names></name><name><surname>van Dijk</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mapping the tonotopic organization in human auditory cortex with minimally salient acoustic stimulation</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>2024</fpage><lpage>2038</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr282</pub-id><pub-id pub-id-type="pmid">21980020</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver</surname> <given-names>AM</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional topography of human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1416</fpage><lpage>1428</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0226-15.2016</pub-id><pub-id pub-id-type="pmid">26818527</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname> <given-names>L</given-names></name><name><surname>Lu</surname> <given-names>T</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural representations of sinusoidal amplitude and frequency modulations in the primary auditory cortex of awake primates</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>2237</fpage><lpage>2261</lpage><pub-id pub-id-type="doi">10.1152/jn.2002.87.5.2237</pub-id><pub-id pub-id-type="pmid">11976364</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname> <given-names>JH</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title><source>Neuron</source><volume>71</volume><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.032</pub-id><pub-id pub-id-type="pmid">21903084</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merzenich</surname> <given-names>MM</given-names></name><name><surname>Brugge</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Representation of the cochlear partition of the superior temporal plane of the macaque monkey</article-title><source>Brain Research</source><volume>50</volume><fpage>275</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(73)90731-2</pub-id><pub-id pub-id-type="pmid">4196192</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Processing of natural sounds in human auditory cortex: Tonotopy, spectral tuning, and relation to voice sensitivity</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>14205</fpage><lpage>14216</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1388-12.2012</pub-id><pub-id pub-id-type="pmid">23055490</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Santoro</surname> <given-names>R</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Processing of natural sounds: characterization of multipeak spectral tuning in human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>11888</fpage><lpage>11898</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5306-12.2013</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An anatomical and functional topography of human auditory cortical areas</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>225</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00225</pub-id><pub-id pub-id-type="pmid">25120426</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morel</surname> <given-names>A</given-names></name><name><surname>Garraghty</surname> <given-names>PE</given-names></name><name><surname>Kaas</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Tonotopic organization, architectonic fields, and connections of auditory cortex in macaque monkeys</article-title><source>The Journal of Comparative Neurology</source><volume>335</volume><fpage>437</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1002/cne.903350312</pub-id><pub-id pub-id-type="pmid">7693772</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ng</surname> <given-names>CW</given-names></name><name><surname>Recanzone</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Age-Related changes in temporal processing of Rapidly-Presented sound sequences in the macaque auditory cortex</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>3775</fpage><lpage>3796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx240</pub-id><pub-id pub-id-type="pmid">29040403</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overath</surname> <given-names>T</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Sanes</surname> <given-names>DH</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sensitivity to temporal modulation rate and spectral bandwidth in the human auditory system: fMRI evidence</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>2042</fpage><lpage>2056</lpage><pub-id pub-id-type="doi">10.1152/jn.00308.2011</pub-id><pub-id pub-id-type="pmid">22298830</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overton</surname> <given-names>JA</given-names></name><name><surname>Recanzone</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Effects of aging on the response of single neurons to amplitude-modulated noise in primary auditory cortex of rhesus macaque</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2911</fpage><lpage>2923</lpage><pub-id pub-id-type="doi">10.1152/jn.01098.2015</pub-id><pub-id pub-id-type="pmid">26936987</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>DC</given-names></name><name><surname>Polk</surname> <given-names>TA</given-names></name><name><surname>Park</surname> <given-names>R</given-names></name><name><surname>Minear</surname> <given-names>M</given-names></name><name><surname>Savage</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Aging reduces neural specialization in ventral visual cortex</article-title><source>PNAS</source><volume>101</volume><fpage>13091</fpage><lpage>13095</lpage><pub-id pub-id-type="doi">10.1073/pnas.0405148101</pub-id><pub-id pub-id-type="pmid">15322270</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Wingfield</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural consequences of Age-Related hearing loss</article-title><source>Trends in Neurosciences</source><volume>39</volume><fpage>486</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2016.05.001</pub-id><pub-id pub-id-type="pmid">27262177</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Assaneo</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Speech rhythms and their neural foundations</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>322</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0304-4</pub-id><pub-id pub-id-type="pmid">32376899</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Presacco</surname> <given-names>A</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Anderson</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evidence of degraded representation of speech in noise, in the aging midbrain and cortex</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>2346</fpage><lpage>2355</lpage><pub-id pub-id-type="doi">10.1152/jn.00372.2016</pub-id><pub-id pub-id-type="pmid">27535374</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pruim</surname> <given-names>RHR</given-names></name><name><surname>Mennes</surname> <given-names>M</given-names></name><name><surname>van Rooij</surname> <given-names>D</given-names></name><name><surname>Llera</surname> <given-names>A</given-names></name><name><surname>Buitelaar</surname> <given-names>JK</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ICA-AROMA: a robust ICA-based strategy for removing motion artifacts from fMRI data</article-title><source>NeuroImage</source><volume>112</volume><fpage>267</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.02.064</pub-id><pub-id pub-id-type="pmid">25770991</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname> <given-names>JP</given-names></name><name><surname>Tian</surname> <given-names>B</given-names></name><name><surname>Pons</surname> <given-names>T</given-names></name><name><surname>Mishkin</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Serial and parallel processing in rhesus monkey auditory cortex</article-title><source>The Journal of Comparative Neurology</source><volume>382</volume><fpage>89</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19970526)382:1&lt;89::AID-CNE6&gt;3.0.CO;2-G</pub-id><pub-id pub-id-type="pmid">9136813</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The effects of aging on auditory cortical function</article-title><source>Hearing Research</source><volume>366</volume><fpage>99</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2018.05.013</pub-id><pub-id pub-id-type="pmid">29853323</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sachs</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Neural coding of complex sounds: speech</article-title><source>Annual Review of Physiology</source><volume>46</volume><fpage>261</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1146/annurev.ph.46.030184.001401</pub-id><pub-id pub-id-type="pmid">6370109</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santoro</surname> <given-names>R</given-names></name><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding of natural sounds at multiple spectral and temporal resolutions in the human auditory cortex</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003412</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003412</pub-id><pub-id pub-id-type="pmid">24391486</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santoro</surname> <given-names>R</given-names></name><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Valente</surname> <given-names>G</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reconstructing the spectrotemporal modulations of real-life sounds from fMRI response patterns</article-title><source>PNAS</source><volume>114</volume><fpage>4799</fpage><lpage>4804</lpage><pub-id pub-id-type="doi">10.1073/pnas.1617622114</pub-id><pub-id pub-id-type="pmid">28420788</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönwiesner</surname> <given-names>M</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spectro-temporal modulation transfer function of single voxels in the human auditory cortex measured with high-resolution fMRI</article-title><source>PNAS</source><volume>106</volume><fpage>14611</fpage><lpage>14616</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907682106</pub-id><pub-id pub-id-type="pmid">19667199</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>RV</given-names></name><name><surname>Zeng</surname> <given-names>FG</given-names></name><name><surname>Kamath</surname> <given-names>V</given-names></name><name><surname>Wygonski</surname> <given-names>J</given-names></name><name><surname>Ekelid</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname> <given-names>KB</given-names></name><name><surname>Mapes</surname> <given-names>FM</given-names></name><name><surname>Hickman</surname> <given-names>ED</given-names></name><name><surname>Frisina</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Word recognition in competing babble and the effects of age, temporal processing, and absolute sensitivity</article-title><source>The Journal of the Acoustical Society of America</source><volume>112</volume><fpage>720</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1121/1.1487841</pub-id><pub-id pub-id-type="pmid">12186051</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Hertz</surname> <given-names>U</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Extensive cochleotopic mapping of human auditory cortical fields obtained with phase-encoding fMRI</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e17832</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0017832</pub-id><pub-id pub-id-type="pmid">21448274</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talavage</surname> <given-names>TM</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Melcher</surname> <given-names>JR</given-names></name><name><surname>Ledden</surname> <given-names>PJ</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Tonotopic organization in human auditory cortex revealed by progressions of frequency sensitivity</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>1282</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1152/jn.01125.2002</pub-id><pub-id pub-id-type="pmid">14614108</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Sen</surname> <given-names>K</given-names></name><name><surname>Doupe</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>2315</fpage><lpage>2331</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-02315.2000</pub-id><pub-id pub-id-type="pmid">10704507</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaden</surname> <given-names>KI</given-names></name><name><surname>Kuchinsky</surname> <given-names>SE</given-names></name><name><surname>Ahlstrom</surname> <given-names>JB</given-names></name><name><surname>Dubno</surname> <given-names>JR</given-names></name><name><surname>Eckert</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical activity predicts which older adults recognize speech in noise and when</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>3929</fpage><lpage>3937</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2908-14.2015</pub-id><pub-id pub-id-type="pmid">25740521</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaden</surname> <given-names>KI</given-names></name><name><surname>Eckert</surname> <given-names>MA</given-names></name><name><surname>Dubno</surname> <given-names>JR</given-names></name><name><surname>Harris</surname> <given-names>KC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cingulo‐opercular adaptive control for younger and older adults during a challenging gap detection task</article-title><source>Journal of Neuroscience Research</source><volume>98</volume><fpage>680</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1002/jnr.24506</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voytek</surname> <given-names>B</given-names></name><name><surname>Kramer</surname> <given-names>MA</given-names></name><name><surname>Case</surname> <given-names>J</given-names></name><name><surname>Lepage</surname> <given-names>KQ</given-names></name><name><surname>Tempesta</surname> <given-names>ZR</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Gazzaley</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Age-Related changes in 1/f neural electrophysiological noise</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>13257</fpage><lpage>13265</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2332-14.2015</pub-id><pub-id pub-id-type="pmid">26400953</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiley</surname> <given-names>TL</given-names></name><name><surname>Chappell</surname> <given-names>R</given-names></name><name><surname>Carmichael</surname> <given-names>L</given-names></name><name><surname>Nondahl</surname> <given-names>DM</given-names></name><name><surname>Cruickshanks</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Changes in hearing thresholds over 10 years in older adults</article-title><source>Journal of the American Academy of Audiology</source><volume>19</volume><fpage>281</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.3766/jaaa.19.4.2</pub-id><pub-id pub-id-type="pmid">18795468</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname> <given-names>DL</given-names></name><name><surname>Herron</surname> <given-names>TJ</given-names></name><name><surname>Cate</surname> <given-names>AD</given-names></name><name><surname>Yund</surname> <given-names>EW</given-names></name><name><surname>Stecker</surname> <given-names>GC</given-names></name><name><surname>Rinne</surname> <given-names>T</given-names></name><name><surname>Kang</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional properties of human auditory cortical fields</article-title><source>Frontiers in Systems Neuroscience</source><volume>4</volume><elocation-id>155</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2010.00155</pub-id><pub-id pub-id-type="pmid">21160558</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55300.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Johnsrude</surname><given-names>Ingrid S</given-names></name><role>Reviewing Editor</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name><role>Reviewer</role><aff><institution>University of Maryland</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Using fMRI and computation modeling, this study explores how age affects cortical responses to natural sounds with different spectrotemporal properties. Using both encoding and decoding analyses, the authors demonstrate that older listeners have broadened temporal rate tuning compared to younger listeners, but show no difference in spectral tuning.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Temporal selectivity declines in the aging human auditory cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Barbara Shinn-Cunningham as the Senior Editor, Ingrid Johnsrude as Reviewing Editor, and two reviewers. The following individual involved in review of your submission has agreed to reveal their identity: Jonathan Z Simon (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. In recognition of the fact that revisions may take longer than the time we typically allow, until the research enterprise restarts in full, we will give authors as much time as they need to submit revised manuscripts.</p><p>Summary:</p><p>The researchers use fMRI and computational modeling to compare auditory cortical responses to natural sounds across subjects of varying ages (cross-sectional design), according to the spectrotemporal properties of the sounds.</p><p>Approaching age-based neural dedifferentiation (or neural &quot;tuning&quot;) in the auditory domain has not been well handled in the literature to date. In that sense, reviewers found this work to be interesting, particularly with respect to the detailed and nuanced treatment the authors give to the analysis of auditory stimuli (i.e., rates, scales, frequencies).</p><p>The reviewers found the study to be well designed.</p><p>The analysis proceeds in two steps: first using an encoding framework, and then using a decoding framework. The authors find that older listeners have broadened temporal rate tuning compared to younger listeners, in contrast to spectral tuning which does not show such a difference.</p><p>The reviewers commented that the analysis methods are employed well and described clearly. It is evident that there is great attention to detail in the analysis, both in the formalism and the statistics.</p><p>The benefits and tradeoffs between the encoding and decoding approaches are handled well, and both reviewers commented on how nice it was to see both frameworks used on the same data.</p><p>Essential revisions:</p><p>The reviewers raise a number of concerns that must be adequately addressed before the paper can be accepted. Some of the required revisions may require further experimentation within the framework of the presented studies and techniques.</p><p>1) Subsection “Univariate encoding analysis: Model estimation”: How were subject data incorporated in the cross-validation scheme? It is clear how sounds were split, but not how subject data were handled. If all subjects are included and you simply split on stimuli, then the data are technically not &quot;independent&quot; per se in the classic between-subjects sense…one can always argue bias is present by having all subjects contribute at each cross-validation stage.</p><p>2) Subsection “Multivariate decoding analysis: Model estimation”: It seems to be a major choice to have a single sensory control region (calcarine sulcus) given the task type. Please justify this more.</p><p>3) Subsection “Sound identification accuracies”: the joint stimuli (texture + speech) are somehow &quot;uniquely&quot; identified despite texture being repeated 4x. Would results likely be even better without this repetition? Why was texture repetition required at all?</p><p>4) Subsection “Decoding results”: It was not clear why decoding accuracy should be highest at these values of freq, scale, and rate. Was there a hypothesis regarding these levels?</p><p>5) The age-based result in Figure 5E is questionable. This was plotted as a scatterplot in WebPlotDigitizer to examine the impact of data on the leftmost side of the plot (which visually are pulling the slope negative). Using a Spearman corr reduces the corr value to -.21 (p=.27). Holding out a single high leverage case (only the most extreme value detected by computing Cook's distance) also reduces the Pearson correlation to -.28 (p=.14) and Spearman to -.13 (p=.50). And taking out the three cases identified by a Cook's rule of thumb (Standard Cook's cutoff = 4/n), Pearson and Spearman corrs are reduced to -.19 and -.05 respectively.</p><p>6) The uni- and multivariate outliers need to be better addressed in Figure 5B/C as well. The distributions are not terribly well behaved. For example, without what seems like a young adult outlier at the highest SI level in Figure 5C, does the group effect remain?</p><p>7) The same should be investigated for Figure 6B also; the right tail of the young group appears to pull up the young adult mean in that case too. It does appear that Figure 6A may hold up however, but this needs to be verified.</p><p>8) Because of a lack of clarity in the results as they currently sit, little can be made of the current Discussion section, various aspects of what is discussed may fall away once the data are reanalyzed. The Discussion section (and Abstract) should therefore be revised in light of what the reanalysis shows.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55300.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>The researchers use fMRI and computational modeling to compare auditory cortical responses to natural sounds across subjects of varying ages (cross-sectional design), according to the spectrotemporal properties of the sounds.</p><p>Approaching age-based neural dedifferentiation (or neural &quot;tuning&quot;) in the auditory domain has not been well handled in the literature to date. In that sense, reviewers found this work to be interesting, particularly with respect to the detailed and nuanced treatment the authors give to the analysis of auditory stimuli (i.e., rates, scales, frequencies).</p><p>The reviewers found the study to be well designed.</p><p>The analysis proceeds in two steps: first using an encoding framework, and then using a decoding framework. The authors find that older listeners have broadened temporal rate tuning compared to younger listeners, in contrast to spectral tuning which does not show such a difference.</p><p>The reviewers commented that the analysis methods are employed well and described clearly. It is evident that there is great attention to detail in the analysis, both in the formalism and the statistics.</p><p>The benefits and tradeoffs between the encoding and decoding approaches are handled well, and both reviewers commented on how nice it was to see both frameworks used on the same data.</p><p>Essential revisions:</p><p>The reviewers raise a number of concerns that must be adequately addressed before the paper can be accepted. Some of the required revisions may require further experimentation within the framework of the presented studies and techniques.</p><p>1) Subsection “Univariate encoding analysis: Model estimation”: How were subject data incorporated in the cross-validation scheme? It is clear how sounds were split, but not how subject data were handled. If all subjects are included and you simply split on stimuli, then the data are technically not &quot;independent&quot; per se in the classic between-subjects sense…one can always argue bias is present by having all subjects contribute at each cross-validation stage.</p></disp-quote><p>We thank the reviewers for this comment and would like to clarify the cross-validation scheme. Both encoding and decoding models were run at the single-subject level. Therefore, second-level splits are not feasible for the en-/decoding approach applied here. We now also explicitly state in the manuscript that encoding and decoding were not run on the group level: “… this computation was performed in a fourfold cross-validation procedure within each participant…” (subsection “Multivariate decoding analysis: Model estimation”). For review only, we have added <xref ref-type="fig" rid="respfig1">Author response image 1</xref> showing the cross-validation scheme:</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Cross-validation scheme per subject.</title><p>In a four-fold cross-validation (CV), the eight fMRI runs of each subject were split into six training and two testing runs, such that each run served once as testing run.Legend.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55300-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>2) Subsection “Multivariate decoding analysis: Model estimation”: It seems to be a major choice to have a single sensory control region (calcarine sulcus) given the task type. Please justify this more.</p></disp-quote><p>We had originally chosen primary visual cortex as a control region because it constitutes another primary sensory area. However, we fully agree with the reviewers that a single sensory control region may not be sufficient. In fact, this comment has now helped us a great deal in substantiating further the specificity of the results to auditory regions HG and PT.</p><p>In the revised manuscript we have added two more higher-level control areas that are not primary sensory regions, namely the middle frontal and superior parietal gyrus (Figure 7 and Figure 7—figure supplement 3). Note that for all control regions, reconstruction accuracies were low and no age differences in selectivity index (<italic>SI</italic>) were observed (subsection “Feature reconstruction from auditory subregions”):</p><p>“As control regions, we chose primary visual cortex (V1, calcarine sulcus) as another primary sensory area, and two higher-level areas, namely the middle frontal (MFG) and superior parietal gyrus (SPG; Figure 7A,G-I).”</p><disp-quote content-type="editor-comment"><p>3) Subsection “Sound identification accuracies”: the joint stimuli (texture + speech) are somehow &quot;uniquely&quot; identified despite texture being repeated 4x. Would results likely be even better without this repetition? Why was texture repetition required at all?</p></disp-quote><p>We believe that the reviewer questions the necessity of the repetition of sound textures. The main reason for the repetition was that we only had a limited number of textures based on natural sounds (<italic>N</italic> = 192) that we had used in previous experiments (Erb et al., 2019). As recurring sound textures were embedded in non-repetitive speech, each sample of the sound mixture had its unique acoustic properties. Also, the sound mixture was cut in snippets of 947 ms ( = 1 TR) and acoustic features were extracted from these snippets for the sound identification analysis. As the length of snippets was not aligned to the length of sound textures, acoustic features were derived from the different fragments of a recurring sound texture. Therefore, we deem sounds uniquely identifiable.</p><disp-quote content-type="editor-comment"><p>4) Subsection “Decoding results”: It was not clear why decoding accuracy should be highest at these values of freq, scale, and rate. Was there a hypothesis regarding these levels?</p></disp-quote><p>We thank the reviewer for this comment which we take as opportunity to clarify our hypothesis on the decoding results (subsection “Feature reconstruction from auditory cortex”):</p><p>“Our main hypothesis was that cortical sensitivity is highest for slow temporal modulations, based on previous observations of human (but not monkey) auditory cortex being most sensitive to the modulations present speech (Santoro et al., 2017, Erb et al., 2019). Decoding yielded highest accuracies at frequencies of 230–580 Hz (mean <italic>r</italic> = 0.47) and spectral scales of 0.25 cyc/oct (mean <italic>r</italic> = 0.47), irrespective of age (Figure 5A,B), indicating that brain responses followed the frequency and spectral modulation content of the stimuli (Figure 1—figure supplement 2A). Conversely, for temporal modulations, reconstruction accuracies peaked at rates of 4–8 Hz (mean r = 0.4) in both age groups (Figure 5A,B). Those peaks were not present in the stimuli (Figure 1—figure supplement 2A) and imply preferred processing of temporal rates in the speech-relevant range (Poeppel and Assaneo, 2020), corroborating our hypothesis (see also Discussion section).”</p><disp-quote content-type="editor-comment"><p>5) The age-based result in Figure 5E is questionable. This was plotted as a scatterplot in WebPlotDigitizer to examine the impact of data on the leftmost side of the plot (which visually are pulling the slope negative). Using a Spearman corr reduces the corr value to -.21 (p=.27). Holding out a single high leverage case (only the most extreme value detected by computing Cook's distance) also reduces the Pearson correlation to -.28 (p=.14) and Spearman to -.13 (p=.50). And taking out the three cases identified by a Cook's rule of thumb (Standard Cook's cutoff = 4/n), Pearson and Spearman corrs are reduced to -.19 and -.05 respectively.</p></disp-quote><p>We thank the reviewer for these additional analyses. We agree with the reviewer that either reporting rank-based (Spearman’s) correlations or identifying and excluding outliers is appropriate here.</p><p>Therefore, in the revised version, univariate outliers were identified as being outside of the range mean ± 2 standard deviations (SD). Multivariate outliers were identified based on Cook’s distance. Please also note that for group comparisons we used permutation tests, which are robust against distributional assumptions while being more sensitive than rank-based procedures.</p><p>The correlation between the rate selectivity index and age proved indeed not robust to outlier control; we have therefore removed this specific result from the manuscript and Figure 5.</p><p>Most important to the overall conclusion, however, note that our second, and arguably less noisy measure of tuning selectivity (i.e., variance of reconstruction accuracy across feature bins) proved robust to outliers (Figure 6 in the revised version).</p><p>Note further that in a multiple regression conservatively excluding two multivariate outliers based on Cook’s distance, the prediction of rate variance based on age remains significant (Subsection “Selectivity index”):</p><p>“In a control analysis, we ensured that these results cannot be explained by the overall slightly worse model fits in the older group as quantified by sound identification accuracy (as evident in Figure 2): In a multiple regression predicting older participants’ variance in rate from chronological age with covariates PTA and sound identification accuracy, neither PTA (<italic>t(26)</italic> = 0.73, <italic>p</italic> = 0.473, permutation test) nor sound identification accuracy (<italic>t(26)</italic> = 1.02, <italic>p</italic> = 0.312) were significant, while age was significant (<italic>t(26)</italic> = -2.8, <italic>p</italic> = 0.003; <italic>adjusted R<sup>2</sup></italic> = 0.39). Age remained significant when excluding two multivariate outliers identified based on Cook’s distance (<italic>t</italic>(24) = -2.25, <italic>p</italic> = 0.021).”</p><disp-quote content-type="editor-comment"><p>6) The uni- and multivariate outliers need to be better addressed in Figure 5B/C as well. The distributions are not terribly well behaved. For example, without what seems like a young adult outlier at the highest SI level in Figure 5C, does the group effect remain?</p></disp-quote><p>We thank the reviewer for this comment and have looked at the <italic>SI</italic> age comparison with and without outliers in all respective figures (Figure 5, Figure 5—figure supplement 1, Figure 7). Please note that we used permutation tests for group comparisons which are robust against distributional assumptions. This age group comparison for rate <italic>SI</italic> is not affected by outliers as stated in caption of Figure 5 (see comment # 5 above and Subsection “Selectivity index):</p><p>“Note that removing the extreme cases in the young group (outliers were defined as exceeding the grand average by ± 2 SD) for rate <italic>SI</italic> does not alter results qualitatively: It reduces the mean age group difference to a still significant <italic>SI</italic> difference = 0.009, <italic>p</italic> = 0.026.”</p><disp-quote content-type="editor-comment"><p>7) The same should be investigated for 6B also; the right tail of the young group appears to pull up the young adult mean in that case too. It does appear that 6A may hold up however, but this needs to be verified.</p></disp-quote><p>We re-ran the group comparisons for the rate <italic>SI</italic> in HG/HS and PT with and without outliers (Figure 7B,C under comment # 2). Note that the age difference in both ROIs remains significant when excluding outliers (see caption of Figure 7 and subsection “Feature reconstruction from auditory subregions”):</p><p>“Removing the extreme cases in the young group exceeding the grand average by ± 2 SD reduces the mean <italic>SI</italic> age difference to 0.012 (<italic>p</italic> = 0.008) in HG/HS, and to 0.012 (<italic>p</italic> = 0.04) in PT.”</p><disp-quote content-type="editor-comment"><p>8) Because of a lack of clarity in the results as they currently sit, little can be made of the current Discussion section, various aspects of what is discussed may fall away once the data are reanalyzed. The Discussion section (and Abstract) should therefore be revised in light of what the reanalysis shows.</p></disp-quote><p>We thank the reviewers for all suggested analyses which – we believe – have substantially strengthened the manuscript. In summary, those analyses have led us to remove the correlation of age and rate SI which did not prove robust to outliers. On the other hand, neither the age group results are nor the correlation of rate variance with age were affected by the suggested analyses. Further, adding two control regions of interest to the decoding analysis has substantiated the result of specificity of age group differences in rate tuning to early auditory areas.</p><p>We again thank the reviewers for probing our approach with these comments (#5–#8 above) but feel that the revised, much more thoroughly outlier-controlled analyses justify the conclusion of a noteworthy relationship between chronological age and temporal-rate selectivity.</p></body></sub-article></article>