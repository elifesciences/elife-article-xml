<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82587</article-id><article-id pub-id-type="doi">10.7554/eLife.82587</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Visual and motor signatures of locomotion dynamically shape a population code for feature detection in <italic>Drosophila</italic></article-title></title-group><contrib-group><contrib contrib-type="author" id="author-114305"><name><surname>Turner</surname><given-names>Maxwell H</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4164-9995</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290521"><name><surname>Krieger</surname><given-names>Avery</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290522"><name><surname>Pang</surname><given-names>Michelle M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-4720"><name><surname>Clandinin</surname><given-names>Thomas R</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6277-6849</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Neurobiology</institution>, <institution>Stanford University</institution>, <addr-line><named-content content-type="city">Stanford</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-4023"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role>Reviewing editor</role><aff><institution>University of California, Santa Barbara</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>trc@stanford.edu</email> (TC);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>28</day><month>10</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e82587</elocation-id><history><date date-type="received"><day>10</day><month>08</month><year>2022</year></date><date date-type="accepted"><day>25</day><month>10</month><year>2022</year></date></history><permissions><copyright-statement>Â© 2022, Turner et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Turner et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82587-v2.pdf"/><abstract><p>Natural vision is dynamic: as an animal moves, its visual input changes dramatically. How can the visual system reliably extract local features from an input dominated by self-generated signals? In Drosophila, diverse local visual features are represented by a group of projection neurons with distinct tuning properties. Here we describe a connectome-based volumetric imaging strategy to measure visually evoked neural activity across this population. We show that local visual features are jointly represented across the population, and that a shared gain factor improves trial-to-trial coding fidelity. A subset of these neurons, tuned to small objects, is modulated by two independent signals associated with self-movement, a motor-related signal and a visual motion signal associated with rotation of the animal. These two inputs adjust the sensitivity of these feature detectors across the locomotor cycle, selectively reducing their gain during saccades and restoring it during intersaccadic intervals. This work reveals a strategy for reliable feature detection during locomotion.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32-MH118707</award-id><principal-award-recipient><name><surname>Turner</surname><given-names>Maxwell H</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K99-EY032549</award-id><principal-award-recipient><name><surname>Turner</surname><given-names>Maxwell H</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY022638</award-id><principal-award-recipient><name><surname>Clandinin</surname><given-names>Thomas R</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS110060</award-id><principal-award-recipient><name><surname>Clandinin</surname><given-names>Thomas R</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>GRFP</award-id><principal-award-recipient><name><surname>Krieger</surname><given-names>Avery</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014037</institution-id><institution>National Defense Science and Engineering Graduate</institution></institution-wrap></funding-source><award-id>Fellowship</award-id><principal-award-recipient><name><surname>Pang</surname><given-names>Michelle M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All software and code is available on GitHub. Main analysis, modeling and figure generation code can be found here: https://github.com/mhturner/glom_pop; Visual stimulus code can be found here: https://github.com/ClandininLab/visanalysis and here: https://github.com/ClandininLab/flystim. Extracted ROI responses and associated stimulus metadata, along with raw imaging data, can be found in a Dryad repository here: https://doi.org/10.5061/dryad.h44j0zpp8.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Turner MH</collab></person-group><year iso-8601-date="2022">2022</year><source>Data from: Visual and motor signatures of locomotion dynamically shape a population code for feature detection in Drosophila</source><ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.5061/dryad.h44j0zpp8">https://dx.doi.org/10.5061/dryad.h44j0zpp8</ext-link><comment>Dryad Digital Repository, doi:10.5061/dryad.h44j0zpp8</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-82587-supp-v2.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>