<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">73097</article-id><article-id pub-id-type="doi">10.7554/eLife.73097</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Eye movements reveal spatiotemporal dynamics of visually-informed planning in navigation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-249937"><name><surname>Zhu</surname><given-names>Seren</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0555-9690</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-211035"><name><surname>Lakshminarasimhan</surname><given-names>Kaushik Janakiraman</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-251591"><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-138210"><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9650-8962</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Center for Neural Science</institution>, <institution>New York University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution content-type="dept">Center for Theoretical Neuroscience</institution>, <institution>Columbia University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><institution content-type="dept">Department of Psychology</institution>, <institution>New York University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-134740"><name><surname>Zhang</surname><given-names>Hang</given-names></name><role>Reviewing editor</role><aff><institution>Peking University</institution>, <country>China</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>lt1686@nyu.edu</email> (SZ);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>03</day><month>05</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e73097</elocation-id><history><date date-type="received"><day>16</day><month>08</month><year>2021</year></date><date date-type="accepted"><day>01</day><month>05</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Zhu et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Zhu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-73097-v1.pdf"/><abstract><p>Goal-oriented navigation is widely understood to depend upon internal maps. Although this may be the case in many settings, humans tend to rely on vision in complex, unfamiliar environments. To study the nature of gaze during visually-guided navigation, we tasked humans to navigate to transiently visible goals in virtual mazes of varying levels of difficulty, observing that they took near-optimal trajectories in all arenas. By analyzing participants’ eye movements, we gained insights into how they performed visually-informed planning. The spatial distribution of gaze revealed that environmental complexity mediated a striking tradeoff in the extent to which attention was directed towards two complimentary aspects of the world model: the reward location and task-relevant transitions. The temporal evolution of gaze revealed rapid, sequential prospection of the future path, evocative of neural replay. These findings suggest that the spatiotemporal characteristics of gaze during navigation are significantly shaped by the unique cognitive computations underlying real-world, sequential decision making.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19-NS118246</award-id><principal-award-recipient><name><surname>Zhu</surname><given-names>Seren</given-names></name><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY022538</award-id><principal-award-recipient><name><surname>Zhu</surname><given-names>Seren</given-names></name><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI-1707398</award-id><principal-award-recipient><name><surname>Lakshminarasimhan</surname><given-names>Kaushik Janakiraman</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lakshminarasimhan</surname><given-names>Kaushik Janakiraman</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, nor the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experimental procedures were approved by the Institutional Review Board at New York University and all participants signed an informed consent form (IRB-FY2019-2599).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Links to data and code are included in the manuscript.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Zhu SL</collab><collab>Lakshminarasimhan KJ</collab><collab>Arfaei N</collab><collab>Angelaki DE</collab></person-group><year iso-8601-date="2022">2022</year><source>Gaze-navigation</source><ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/neuro-sci/gaze-navigation">https://gin.g-node.org/neuro-sci/gaze-navigation</ext-link><comment>G-node, gaze-navigation</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-73097-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>