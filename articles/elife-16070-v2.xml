<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">16070</article-id><article-id pub-id-type="doi">10.7554/eLife.16070</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Neural pattern change during encoding of a narrative predicts retrospective duration estimates</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-37422"><name><surname>Lositsky</surname><given-names>Olga</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7089-4474</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-37916"><name><surname>Chen</surname><given-names>Janice</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4511-4725</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-37917"><name><surname>Toker</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-37918"><name><surname>Honey</surname><given-names>Christopher J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0745-5089</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-68817"><name><surname>Shvartsman</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-20748"><name><surname>Poppenk</surname><given-names>Jordan L</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3315-5098</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-16326"><name><surname>Hasson</surname><given-names>Uri</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3599-7168</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-19764"><name><surname>Norman</surname><given-names>Kenneth A</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5887-9682</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Princeton Neuroscience Institute</institution>, <institution>Princeton University</institution>, <addr-line><named-content content-type="city">Princeton</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Helen Wills Neuroscience Institute</institution>, <institution>University of California, Berkeley</institution>, <addr-line><named-content content-type="city">Berkeley</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Psychology</institution>, <institution>University of Toronto</institution>, <addr-line><named-content content-type="city">Toronto</named-content></addr-line>, <country>Canada</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Psychology</institution>, <institution>Queen's University</institution>, <addr-line><named-content content-type="city">Kingston</named-content></addr-line>, <country>Canada</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Psychology</institution>, <institution>Princeton University</institution>, <addr-line><named-content content-type="city">Princeton</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name><role>Reviewing editor</role><aff id="aff6"><institution>Boston University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>lositsky@princeton.edu</email> (OL);</corresp><corresp id="cor2"><email>knorman@princeton.edu</email> (KAN)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>01</day><month>11</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e16070</elocation-id><history><date date-type="received"><day>16</day><month>03</month><year>2016</year></date><date date-type="accepted"><day>17</day><month>10</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Lositsky et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Lositsky et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-16070-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.16070.001</object-id><p>What mechanisms support our ability to estimate durations on the order of minutes? Behavioral studies in humans have shown that changes in contextual features lead to overestimation of past durations. Based on evidence that the medial temporal lobes and prefrontal cortex represent contextual features, we related the degree of fMRI pattern change in these regions with people’s subsequent duration estimates. After listening to a radio story in the scanner, participants were asked how much time had elapsed between pairs of clips from the story. Our ROI analyses found that duration estimates were correlated with the neural pattern distance between two clips at encoding in the right entorhinal cortex. Moreover, whole-brain searchlight analyses revealed a cluster spanning the right anterior temporal lobe. Our findings provide convergent support for the hypothesis that retrospective time judgments are driven by 'drift' in contextual representations supported by these regions.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.001">http://dx.doi.org/10.7554/eLife.16070.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.16070.002</object-id><title>eLife digest</title><p>How do humans judge how much time has passed during daily life, such as when waiting for the bus? Psychology studies have shown that people remember events to have lasted longer when more changes occurred during that time period. These changes can occur either in the environment (such as changes in location) or in the individual’s internal state (such as changes in goals and emotions).</p><p>Brain activity changes from moment to moment. Lositsky et al. hypothesized that when patterns of activity in a person’s brain change a lot across an interval of time, that person will judge that a long time has passed. On the other hand, if brain activity changes less over that interval, individuals will judge that less time has passed.</p><p>Some regions of the brain are sensitive to information that unfolds over several minutes; many of these regions are vital for forming memories of episodes from our lives. Using a technique called functional magnetic resonance imaging (fMRI), Lositsky et al. specifically looked at the activity of these regions while volunteers listened to a 25-minute radio drama. Afterwards, the volunteers listened to clips from different events in the story and judged how much time passed between those events.</p><p>Even though each pair of audio clips occurred exactly two minutes apart in the original story, people’s time judgments were strongly influenced by how many scene changes happened in the story between the two clips. In a part of the brain called the right anterior temporal lobe – and especially in a region of it called the entorhinal cortex – Lositsky et al. found that brain activity changed more when audio clips were judged to be further apart in time. Activity in this region fluctuated more slowly overall than in the rest of the brain. This could mean that it combines sensory information (about images, sounds, smells and so on) across minutes of time, in order to form a representation of the current situation.</p><p>Future research could focus on several unanswered questions. Exactly which environmental and internal changes influence our perception of time? What form does this information take in the entorhinal cortex? Studies show that the entorhinal cortex contains “grid cells” that track our location in space. Could these cells also help judge the passage of time?</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.002">http://dx.doi.org/10.7554/eLife.16070.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>retrospective time estimates</kwd><kwd>episodic memory</kwd><kwd>temporal context model</kwd><kwd>multi-voxel pattern analysis</kwd><kwd>fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>Early Stage Investigator, R01-MH094480</award-id><principal-award-recipient><name><surname>Hasson</surname><given-names>Uri</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000925</institution-id><institution>John Templeton Foundation</institution></institution-wrap></funding-source><award-id>Proposal 36751</award-id><principal-award-recipient><name><surname>Lositsky</surname><given-names>Olga</given-names></name><name><surname>Norman</surname><given-names>Kenneth A</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>Training Grant, 2T32MH065214</award-id><principal-award-recipient><name><surname>Lositsky</surname><given-names>Olga</given-names></name><name><surname>Chen</surname><given-names>Janice</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Judgments of how much time elapsed between two events in a story are predicted by changes in fMRI activity patterns.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Imagine that you are at the bus stop when you run into a colleague and the two of you become engrossed in a conversation about memory research. After a few minutes, you realize that the bus still has not arrived. Without looking at your watch, you have some sense of how long you have been waiting. Where does this intuition come from?</p><p>Estimation of durations lasting a few seconds has been probed in the neuroimaging, neuropsychology and neuropharmacology literatures (see <xref ref-type="bibr" rid="bib99">Wittmann, 2013</xref>, for a review). On the other hand, the neural mechanisms underlying time perception on the scale of minutes have remained unexplored. This is particularly true of <italic>retrospective</italic> judgments, where individuals experience an interval without paying attention to time and must subsequently estimate the interval’s duration. In such cases, individuals must rely on information stored in memory to estimate duration. How is this accomplished?</p><p>Memory scholars have long posited that the same contextual cues that help us to retrieve an item from memory can also help us determine its recency. According to extant theories of context and memory (see <xref ref-type="bibr" rid="bib68">Manning et al., 2014</xref>, for a review), <italic>mental context</italic> refers to aspects of our mental state that tend to persist over a relatively long time scale; this encompasses our representation of slowly-changing aspects of the external world (e.g., what room we are in) as well as other slowly-changing aspects of our internal mental state (e.g., our current plans). Crucially, these theories posit that slowly-changing contextual features can be episodically associated with more quickly-changing aspects of the world (e.g., stimuli that appear at a particular moment in time; <xref ref-type="bibr" rid="bib72">Mensink and Raaijmakers, 1988</xref>; <xref ref-type="bibr" rid="bib57">Howard and Kahana, 2002</xref>).</p><p><xref ref-type="bibr" rid="bib17">Bower (1972)</xref> first proposed that we could determine how long ago an item occurred by comparing our current context with the context associated with the remembered item. The similarity of these two context representations would reflect their temporal distance, with more similar representations associated with events that happened closer together in time. Thus, a slowly varying mental context could serve as a temporal tag (<xref ref-type="bibr" rid="bib78">Polyn and Kahana, 2008</xref>). In parallel, researchers in the domain of retrospective time estimation have shown that the degree of context change is a better predictor of duration judgments than alternative explanations, such as the number of items remembered from the interval (<xref ref-type="bibr" rid="bib10">Block and Reed, 1978</xref>; <xref ref-type="bibr" rid="bib15">Block, 1990</xref>, <xref ref-type="bibr" rid="bib16">1992</xref>). Indeed, changes in task processing (<xref ref-type="bibr" rid="bib10">Block and Reed, 1978</xref>; <xref ref-type="bibr" rid="bib83">Sahakyan and Smith, 2014</xref>), environmental context (<xref ref-type="bibr" rid="bib12">Block, 1982</xref>), and emotions (<xref ref-type="bibr" rid="bib77">Pollatos et al., 2014</xref>), as well as event boundaries (<xref ref-type="bibr" rid="bib81">Poynter, 1983</xref>; <xref ref-type="bibr" rid="bib102">Zakay et al., 1994</xref>; <xref ref-type="bibr" rid="bib39">Faber and Gennari, 2015</xref>), lead to overestimation of past durations.</p><p>In our study, we set out to obtain neural evidence in support of the hypothesis that mental context change drives duration estimates. Specifically, we hypothesized that, in brain regions representing mental context, the degree of neural pattern change between two events (operationalized as change in multi-voxel patterns of fMRI activity) should predict participants’ estimates of how much time passed between those events.</p><p>Extensive prior work has implicated the medial temporal lobe (MTL) and lateral prefrontal cortex (PFC) in representing contextual information (<xref ref-type="bibr" rid="bib78">Polyn and Kahana, 2008</xref>; for reviews of MTL contributions to representing context, see <xref ref-type="bibr" rid="bib36">Eichenbaum et al., 2007</xref>, and <xref ref-type="bibr" rid="bib82">Ritchey and Ranganath, 2012</xref>; for related computational modeling work, see <xref ref-type="bibr" rid="bib55">Howard and Eichenbaum, 2013</xref>). In keeping with our hypothesis, multiple studies have obtained evidence linking neural pattern change in these regions to temporal memory judgments. <xref ref-type="bibr" rid="bib69">Manns et al. (2007)</xref> recorded from rat hippocampus during an odor memory task; they found that greater change in hippocampal activity patterns between two stimuli predicted better memory for the order in which the stimuli occurred. In the human neuroimaging literature, <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref> found that the degree to which activity patterns in rostrolateral prefrontal cortex changed during the encoding of a stimulus predicted better memory for the temporal position of that stimulus in the experiment. <xref ref-type="bibr" rid="bib60">Jenkins and Ranganath (2016)</xref> also showed that greater pattern distance between two stimuli at encoding in the hippocampus, medial and anterior prefrontal cortex predicted better order memory. Only one study has directly related neural pattern drift to judgments of elapsed time in humans: <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref> found that patterns of fMRI activity in left hippocampus were more similar for pairs of stimuli that were later estimated to have occurred closer together in time, despite equivalent time passage between all pairs (a little less than a minute).</p><p>While the <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref> study provides support for our hypothesis, it has some limitations. First, in <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref>, participants estimated the temporal distance of stimuli that were linked to their contexts in an artificial way (by placing pictures of objects or famous faces on unrelated scene backgrounds); it is unclear whether these results will generalize to more naturalistic situations where events are linked through a narrative. Second, since participants performed the temporal memory test after each encoding run, they were not entirely naïve to the manipulation. Knowing that they would have to estimate durations between stimuli could have changed participants’ strategy and enhanced their attention to time (for evidence that estimating time prospectively engages different mechanisms, see <xref ref-type="bibr" rid="bib53">Hicks et al., 1976</xref>, and <xref ref-type="bibr" rid="bib101">Zakay and Block, 2004</xref>). In the current study, we sought to address the above issues by eliciting temporal distance judgments for pairs of events that had occurred several minutes apart and that were embedded in the context of a rich naturalistic story; participants listened to the entire story before being informed about the temporal judgment task.</p><p>Based on the studies reviewed above, we predicted that neural pattern drift in medial temporal and lateral prefrontal regions might support duration estimation. In our study, we examined these regions of interest (ROIs), as well as a broader set of regions that have been implicated in fMRI studies of time estimation, including the inferior parietal cortex, putamen, insula and frontal operculum (see <xref ref-type="box" rid="B1">Box 1</xref> for a review). In addition to the ROI analysis, which examined activity patterns in masks that were anatomically defined, we performed a searchlight analysis, which examined activity patterns within small cubes over the whole brain.</p><p>Participants were scanned while they listened to a 25-minute science fiction radio story. Outside the scanner, they were surprised with a time perception test, in which they had to estimate how much time had passed between pairs of auditory clips from the story. Controlling for objective time, we found that the degree of neural pattern distance between two clips at the time of encoding predicted how much time an individual would later estimate passed between them. The effect was significant in the right entorhinal cortex ROI. Extending the anatomical analysis to all masks in cortex revealed an additional effect in the left caudal anterior cingulate cortex (ACC). Moreover, whole-brain searchlight analyses yielded significant clusters spanning the right anterior temporal lobe. Our results suggest that patterns of neural activity in these regions may carry contextual information that helps us make retrospective time judgments on the order of minutes.</p><boxed-text id="B1"><object-id pub-id-type="doi">10.7554/eLife.16070.003</object-id><label>Box 1.</label><caption><title>fMRI literature on prospective time estimation.</title></caption><p>As noted in the main text, only one study (<xref ref-type="bibr" rid="bib38">Ezzyat and Davachi, 2014</xref>) has used fMRI to study retrospective estimation of time intervals lasting more than a few seconds. The vast majority of fMRI studies of time estimation have used prospective tasks, in which participants are asked to deliberately track the duration of a short stimulus or compare the duration of two stimuli. Such studies have repeatedly shown that activity in the putamen, insula, inferior frontal cortex (frontal operculum), and inferior parietal cortex increases as participants pay more attention to the duration of stimuli, as opposed to another time-varying attribute (<xref ref-type="bibr" rid="bib31">Coull, 2004</xref>; <xref ref-type="bibr" rid="bib30">Coull et al., 2004</xref>; <xref ref-type="bibr" rid="bib66">Livesey et al., 2007</xref>; <xref ref-type="bibr" rid="bib94">Wiener et al., 2010</xref>; <xref ref-type="bibr" rid="bib98">Wittmann et al., 2010</xref>). <xref ref-type="bibr" rid="bib34">Dirnberger et al. (2012)</xref> showed that greater activity in the putamen and insula during encoding of aversive emotional pictures predicted better subsequent memory for those pictures, but only when their duration was overestimated relative to neutral images. This suggests that the putamen and insula might mediate the relationship between enhanced processing for emotional stimuli and subjective time dilation. Given the established role of these regions in time processing (albeit of a different sort), we included these regions in the set of <italic>a priori</italic> ROIs for our main fMRI analysis.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.003">http://dx.doi.org/10.7554/eLife.16070.003</ext-link></p></boxed-text></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral results</title><sec id="s2-1-1"><title>Participants were sensitive to the duration of story intervals</title><p><xref ref-type="fig" rid="fig1">Figure 1</xref> shows the experimental design, which consisted of an fMRI session, followed immediately by a behavioral session. After listening to a 25-min radio story in the scanner, participants were asked how much time had passed between 43 pairs of clips from the story. In actuality, 24 of the clip pairs had been presented 2 minutes apart in the story, while 19 of the clip pairs had been presented 6 minutes apart in the story (participants were not informed of this). Participants were able to estimate the duration of experienced minutes-long intervals far above chance, albeit with substantial intra- and inter-individual variability. On average, across participants, the 6-min intervals (<italic>M</italic>=5.70 min, <italic>SD</italic>=3.06) were judged to be significantly longer than the 2-min intervals (<italic>M</italic>=3.69 min, <italic>SD</italic>=1.96), <italic>t</italic>(17) = 5.20, <italic>p</italic>&lt;10<sup>-4</sup> (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.004</object-id><label>Figure 1.</label><caption><title>Experimental design.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.004">http://dx.doi.org/10.7554/eLife.16070.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig1-v2"/></fig><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.005</object-id><label>Figure 2.</label><caption><title>Mean duration estimates for all intervals (<bold>A</bold>) and confident intervals (<bold>B</bold>) as a function of their actual duration.</title><p>Each blue circle represents the mean duration estimate for an individual participant within a given interval duration (2 or 6 min). The blue bar heights represent the global means for 2 and 6-min intervals across intervals and participants.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.005">http://dx.doi.org/10.7554/eLife.16070.005</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.16070.006</object-id><label>Figure 2—source data 1.</label><caption><title>Duration estimates and confidence ratings for all participants and intervals.</title><p>To generate the plot in <xref ref-type="fig" rid="fig2">Figure 2</xref>, duration estimates for an objective duration (2 or 6 min) were first averaged within participants, for all intervals (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and for confident intervals only (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The global means (represented by the heights of the blue bars) were then obtained by averaging again across participants. Confidence ratings in this table are binary: 1 reflects a high-confidence interval and 0 reflects a low-confidence interval (see <italic>Removing low-confidence intervals</italic> in Materials and methods).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.006">http://dx.doi.org/10.7554/eLife.16070.006</ext-link></p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-16070-fig2-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="png" mimetype="image" xlink:href="elife-16070-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16070.007</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Reliability of duration estimates across participants.</title><p>Between-group correlations were obtained by splitting the participants randomly into two equal groups and averaging the duration estimates for each interval (across participants) within a group. Each dot in the scatterplot represents a particular temporal interval; its <italic>x</italic> and <italic>y</italic> coordinates indicate the mean estimated duration of that interval for Group 1 and Group 2 participants, respectively. We repeated this procedure 1000 times to ensure that we sampled a variety of group splits. The average correlation between the two groups was 0.64 (<italic>SD</italic>=0.09) for 2-min intervals and 0.54 (<italic>SD</italic>=0.15) for 6-min intervals. The above plot shows the grouping that was most representative of the mean.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.007">http://dx.doi.org/10.7554/eLife.16070.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig2-figsupp1-v2"/></fig></fig-group></p><p>As described in the Materials and methods (see <italic>Removing low-confidence intervals</italic>), participants also provided confidence ratings reflecting their certainty about each clip’s place in the story. Based on this measure, we grouped each participant’s duration estimates into high-confidence and low-confidence intervals. To verify that participants were better at distinguishing 6-min intervals from 2-min intervals when they were confident, we calculated the difference between the mean duration estimates for 6-min intervals and the mean duration estimates for 2-min intervals for every participant. The difference score was significantly higher for high-confidence intervals (<italic>M</italic>=2.43, <italic>SD</italic>=1.82) than for all intervals (<italic>M</italic>=2.01, <italic>SD</italic>=1.64), <italic>t</italic>(17)=2.33, <italic>p</italic>=0.0324. Thus, participants were significantly more accurate at estimating an interval’s duration when they confidently remembered the temporal position of both clips delimiting that interval in the story (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>For a given interval duration, some intervals were consistently judged to be longer than other intervals across participants, although the actual amount of elapsed time was held constant. To test the reliability of duration estimates across participants, we split the subjects randomly into two groups, averaged the duration estimates within each group, and correlated the two averages with each other. We repeated this procedure 1000 times to ensure that we sampled a variety of group splits. The average correlation between the two groups was 0.64 (<italic>SD</italic>=0.09) for 2-min intervals and 0.54 (<italic>SD</italic>=0.15) for 6-min intervals (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). This analysis suggests that features of the story made some intervals appear consistently shorter and other intervals appear consistently longer across participants.</p></sec><sec id="s2-1-2"><title>Duration estimates are influenced by memory of the story</title><p>We found that participants estimated six-minute intervals to be significantly longer than two-minute intervals (<xref ref-type="fig" rid="fig2">Figure 2</xref>), and that some intervals in the story tended to be systematically over-estimated by participants (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). However, it is possible that participants could judge the temporal distance between two clips purely based on the similarity between them (e.g. Are the same characters speaking? Is the background music the same? Is the topic of conversation similar?).</p><p>To ensure that participants were using their memory of the story to judge temporal distance, we ran a control experiment in which 17 participants who had never heard the story were given the exact same memory test. They were asked to try to estimate the amount of time that had elapsed between each pair of clips during the original telling of the story. During debriefing, participants reported making duration estimates based on the perceptual and semantic similarity between the two clips (e.g., which character voices were present, which background music was playing, the topic of conversation).</p><p>We found that naïve participants estimated 6-min intervals (<italic>M</italic>=6.21 min, <italic>SD</italic>=1.91) to be longer than 2-min intervals (<italic>M</italic>=5.63 min, <italic>SD</italic>=1.74; <italic>t</italic>(16)=2.62, <italic>p</italic>=0.019), suggesting that the similarity between two clips carried some information about the temporal distance between them. However, naïve participants were significantly less accurate at distinguishing 6-min intervals from 2-min intervals than our original participants who had heard the story. To quantify this, we calculated the difference between the mean duration estimates for 6-min intervals and the mean duration estimates for 2-min intervals for every participant (exactly as above). The difference score was significantly higher for our original participants (<italic>M</italic>=2.01 min, <italic>SD</italic>=1.64 min) than for naïve participants (<italic>M</italic>=0.59 min, <italic>SD</italic>=0.91 min), <italic>t</italic>(26.86)=−3.22, <italic>p</italic>&lt;0.005. Thus, having memory of the story enabled our participants to estimate durations with significantly higher accuracy.</p><p>We hypothesized that both our original participants and the naïve participants would use consistent strategies to estimate the temporal distance between two clips, but that these strategies would differ across groups. If this is the case, duration estimates should be more correlated across participants within groups than across participants between groups. The correlation in duration estimates across participants within a group (see Materials and methods) in duration estimates was as strong for naïve participants (<italic>M</italic>=0.43, <italic>SD</italic>=0.18, 95% CI [0.40, 0.56]) as for our original participants (<italic>M</italic>=0.43, <italic>SD</italic>=0.25, 95% CI=[0.37, 0.58]), suggesting that both groups used a consistent strategy to estimate the distance between two clips. When we correlated duration estimates from our original group of participants with those of our naïve participants, we found that the between-group correlations (<italic>M</italic>=0.18, <italic>SD</italic>=0.22, 95% CI=[0.04, 0.28]) were significantly above 0, suggesting that a component of the original duration estimates was influenced by the similarity in content between clips. However, the between-group correlations were significantly lower than the within-group correlations (<italic>p</italic>&lt;0.0001, as assessed by a permutation test described in the <italic>Materials and methods</italic>). In other words, there is a reliable component of our original participants’ behavior that cannot be captured by accounting for the perceptual and semantic similarity between clips. In summary, having memory of the story induced a qualitatively different pattern of behavior and produced significantly more accurate duration estimates.</p></sec><sec id="s2-1-3"><title>Correlation between number of event boundaries and duration estimates</title><p>To gain additional evidence that duration estimates were related to contextual change, we looked at the correlation between estimated duration and the number of event boundaries in the interval between the clips. The number of intervening event boundaries can be viewed as a proxy for contextual change, insofar as event boundaries often encompass changes in scene, characters and conversation topic (<xref ref-type="bibr" rid="bib62">Kurby and Zacks, 2008</xref>; <xref ref-type="bibr" rid="bib100">Zacks et al., 2009</xref>). As reviewed in the Introduction, numerous studies have found a relationship between changes in contextual features during an interval and duration estimates for that interval.</p><p>A separate group of participants (n=9) listened to the story and was asked to press a button every time they felt an event boundary was occurring. These data were then averaged across participants to obtain the mean number of event boundaries inside each two-minute interval. We found that the mean number of boundaries in an interval was significantly correlated with the mean duration estimates from our original experiment (<italic>r </italic>= 0.49, 95% CI [0.27, 0.57]; <xref ref-type="fig" rid="fig3">Figure 3</xref>). This suggests that our participants’ retrospective duration estimates were influenced by the number of contextual changes that had occurred during an interval.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.008</object-id><label>Figure 3.</label><caption><title>Mean duration estimates for 2-minute intervals as a function of the number of event boundaries in each interval.</title><p>The number of event boundaries in an interval predicted retrospective duration estimates in our original experiment (<bold>A</bold>), but did not significantly predict duration estimates of naïve participants (<bold>B</bold>) who had never heard the story. This suggests that the number of contextual changes between two clips influenced temporal distance judgments significantly more when the content of the story between the two clips could be recalled.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.008">http://dx.doi.org/10.7554/eLife.16070.008</ext-link></p><p><supplementary-material id="SD2-data"><object-id pub-id-type="doi">10.7554/eLife.16070.009</object-id><label>Figure 3—source data 1.</label><caption><title>Mean number of event boundaries and mean duration estimates from both original and naïve participants.</title><p>Intervals appear in chronological order and the 'position in story' indicates the middle time point between the two clips delimiting the interval. Mean duration estimates were obtained by averaging the duration estimates for a specific interval across participants. The mean number of event boundaries in an interval was obtained by averaging data from a separate group of participants who pressed the spacebar every time a boundary was occurring.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.009">http://dx.doi.org/10.7554/eLife.16070.009</ext-link></p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-16070-fig3-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="SD3-data"><object-id pub-id-type="doi">10.7554/eLife.16070.010</object-id><label>Figure 3—source data 2.</label><caption><title>Duration estimates from the naïve experiment, including both 2 and 6-min intervals.</title><p>As above, Intervals appear in chronological order and the 'position in story' indicates the middle time point between the two clips delimiting the interval.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.010">http://dx.doi.org/10.7554/eLife.16070.010</ext-link></p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-16070-fig3-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig3-v2"/></fig></p><p>However, it is important to note that the number of event boundaries between two clips also influences the perceptual and semantic similarity between them (e.g., clips from the same scene might sound more similar than clips from different scenes). Thus, our participants’ duration estimates could correlate with the number of event boundaries, even if they were basing their estimates purely on the perceptual similarity between clips. To explore this possibility, we tested whether the number of event boundaries would correlate with duration estimates from naïve participants, who could <italic>only</italic> judge temporal distance based on the similarity between clips, given that they had never heard the story.</p><p>Importantly, we found that the number of event boundaries in an interval did not significantly correlate with duration estimates of naïve participants (<italic>r</italic>=0.09, 95% CI [−0.05, 0.21]; <xref ref-type="fig" rid="fig3">Figure 3</xref>). Of course, we cannot definitively prove the null hypothesis that naïve duration estimates do not correlate with the number of event boundaries. However, the correlation between the number of boundaries and duration estimates was significantly higher for our original participants than for naïve participants (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>=0.40, 95% CI [0.15 0.56]). In other words, duration estimates from participants who remembered the story were significantly more correlated with the number of contextual changes between two clips than duration estimates from participants who were judging temporal distance based merely on the similarity between the two clips. This suggests that the number of event boundaries carries information about temporal context that is not contained within the clips alone, and that our original participants’ estimates were influenced by their memory of this contextual information.</p></sec></sec><sec id="s2-2"><title>fMRI results</title><p>We tested whether BOLD pattern change between two clips correlated with temporal distance estimates, using both ROI and whole-brain searchlight analyses. Each type of analysis was performed both within-participants across intervals and within-intervals across participants.</p><p>In the within-participant analysis, we correlated each participant’s duration estimates with that participant’s neural pattern distances (see <italic>Within-Participant Correlation between Pattern Change and Duration Estimates</italic> and <italic>Within-Participant Whole-brain Searchlight</italic>). In the within-interval analysis, we correlated individual differences in subjective duration for a given interval with individual differences in neural pattern distance for that interval (see <italic>Within-Interval Correlation between Pattern Change and Duration Estimates</italic> and <italic>Within-Interval Whole-brain Searchlight</italic>). The two versions of each analysis were performed in order to rule out the possibility that our effects were driven either by participant or interval random effects. In particular, we were concerned that correlations between neural pattern distance and behavior could reflect sensitivity to perceptual or semantic features of the clips (i.e., clip pairs with similar perceptual/semantic features might be associated with shorter duration estimates and greater neural similarity, relative to clip pairs with more dissimilar features). The within-interval analysis addresses this concern by holding clip identity constant.</p><p>Next, we fit a mixed-effects model for each ROI (see <italic>Mixed-Effects Model Accounting for Naïve Duration Estimates</italic>), in which we estimated whether pattern distance in that ROI could predict duration estimates, even when accounting for participant random effects, item (interval) random effects, as well as naïve duration estimates (which are a proxy for the perceptual and semantic similarity between two clips, see <italic>Behavioral results</italic>).</p><p>Finally, we discuss the brain regions that showed significant effects across all analyses (see <italic>Comparing Results from ROI and Searchlight Analyses</italic>).</p><p>As noted in the Materials and methods, the ROI and searchlight analyses were conducted only on high-confidence two-minute intervals. Six-minute intervals were excluded from the fMRI analysis, since we could not successfully dissociate neural pattern change at this timescale from low-frequency scanner noise (see <italic>Methodological challenges with analyzing pattern distance over long time scales</italic> in the Materials and methods).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.011</object-id><label>Figure 4.</label><caption><title>Correlating pattern distance with duration estimates within participants.</title><p>For each ROI in each participant, the pattern distance between each pair of clips at encoding was correlated with the participant’s retrospective duration estimate (<bold>A–B</bold>). The top panel (<bold>A</bold>) shows two example intervals. The neural distance (1-Pearson’s <italic>r</italic>) between clips 2 and 4 (second interval) is greater than the neural distance between clips 1 and 3 (first interval), as is the subjective duration estimate. (<bold>B</bold>) shows the correlation between neural distance and duration estimates in a hypothetical region and participant. (<bold>C</bold>) We used a permutation test to generate 10,000 surrogate pattern distance vectors (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), which we then used to obtain a distribution of null correlations between neural distances and duration estimates. For each ROI in each participant, we calculated the z-scored correlation value, which reflects the strength of the empirical correlation relative to the distribution of null correlations. For each ROI, we performed a random effects t-test to assess whether the z-score was reliably positive across participants. P-values from this t-test were then subjected to multiple comparisons correction using False Discovery Rate (FDR).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.011">http://dx.doi.org/10.7554/eLife.16070.011</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16070.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Permutation test assessing the temporal specificity of correlations between pattern change and behavior.</title><p>This procedure is described in the Materials and methods (see <italic>Statistical analysis of correlations between pattern change and behavior</italic>). (<bold>A</bold>,<bold>B</bold>) The time course of pattern change is constructed using the distance (1 - Pearson’s <italic>r</italic>) between each pattern and the pattern 80 TRs (2 min) after it. As in the main analysis, we averaged over the 5 consecutive TRs surrounding each pattern (for simplicity, this is not shown in the above figure). (<bold>C</bold>) 10,000 surrogate pattern distance time courses are generated by randomizing the phases of the original time course, thus conserving the amplitude of each frequency component. (<bold>D</bold>) Surrogate pattern distances are correlated with time estimates, generating 10,000 null correlations. A Z-value for each ROI / searchlight in each participant is computed to compare the strength of the empirical correlation with the distribution of null correlations. The p-value for a given ROI is obtained using a right-tailed t-test on the Z-values across participants.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.012">http://dx.doi.org/10.7554/eLife.16070.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig4-figsupp1-v2"/></fig></fig-group></p><sec id="s2-2-1"><title>Anatomical ROI analyses</title><p>We first tested whether pattern change in regions suggested by the literature to be important for representing temporal context (see <italic>ROI Selection</italic>) correlated with retrospective duration estimates. Anatomical ROIs were derived from FreeSurfer cortical parcellation (<xref ref-type="bibr" rid="bib32">Desikan et al., 2006</xref>) and from a probabilistic MTL atlas (<xref ref-type="bibr" rid="bib54">Hindy and Turk-Browne, 2015</xref>).</p><sec id="s9"><title>Within-participant correlation between pattern change and duration estimates</title><p>The within-participant analysis procedure is outlined in <xref ref-type="fig" rid="fig4">Figure 4</xref>. We calculated the correlation between neural pattern distance and duration estimates within participants (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) in each of the 32 ROIs shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. To assess the likelihood of obtaining a correlation of that magnitude by chance, we used a phase randomization procedure (described in Materials and methods) to obtain 10,000 null correlations for each ROI in every participant. This enabled us to calculate a Z-value for every ROI in every participant, which reflects the strength of the actual correlation between pattern distance and duration estimates relative to the distribution of null correlations (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Here we report the regions whose Z-values were consistently positive across participants, corrected for multiple comparisons using False Discovery Rate (<xref ref-type="bibr" rid="bib8">Benjamini et al., 2006</xref>).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.013</object-id><label>Figure 5.</label><caption><title>Within-participant ROI analysis: mean Z-values (across all 18 participants) of correlations between pattern distance and duration estimates for the 16 <italic>a priori</italic> ROIs.</title><p>Z-values were obtained from the phase randomization procedure and reflect the strength of the empirical correlation relative to the distribution of null correlations. Error bars represent standard errors of the mean. The blue dots over the right entorhinal cortex and right pars orbitalis indicate that these ROIs survived FDR correction at q&lt;0.05.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.013">http://dx.doi.org/10.7554/eLife.16070.013</ext-link></p><p><supplementary-material id="SD4-data"><object-id pub-id-type="doi">10.7554/eLife.16070.014</object-id><label>Figure 5—source data 1.</label><caption><title>Within-participant analysis Z-values and Pearson’s <italic>r</italic> values for all participants and grey matter regions derived from FreeSurfer segmentation and the probabilistic MTL atlas.</title><p><bold>Excel sheet 1</bold> contains the Z-values for each participant and region, reflecting the strength of the empirical correlation between pattern distance and duration estimates relative to the distribution of null correlations. NaNs signify that a participant had fewer than 10 voxels in a given brain region, most likely due to signal dropout (this was only an issue for the frontal pole). The bar plots in <xref ref-type="fig" rid="fig5">Figure 5</xref> were generated by plotting the mean Z-value (and standard error of the mean) across participants for each of the <italic>a priori</italic> ROIs. <bold>Excel sheet 2</bold>: T-values were obtained from a right-tailed t-test verifying whether the Z-values for a region were reliably positive across participants. The p-values from this t-test were then subjected to multiple comparisons correction using FDR. The three regions in bold survived whole-brain FDR correction at <italic>q</italic>&lt;0.1 and are shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. <bold>Excel sheet 3</bold> contains the Fisher-transformed Pearson’s <italic>r</italic> values for each participant and region.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.014">http://dx.doi.org/10.7554/eLife.16070.014</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16070-fig5-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.16070.015</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Anatomical ROIs that showed a significant correlation between pattern change and duration estimates within participants, after whole-brain FDR correction.</title><p>In red are regions with q&lt;0.1: the right entorhinal cortex, right pars orbitalis and left caudal ACC. This analysis was performed in native space on participant-specific ROIs. ROIs were transformed from native functional space to MNI space for display purposes.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.015">http://dx.doi.org/10.7554/eLife.16070.015</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig5-figsupp1-v2"/></fig></fig-group></p><p>Out of the regions selected <italic>a priori</italic>, the right entorhinal cortex and right pars orbitalis showed a significant positive correlation between pattern change and duration estimates for high-confidence 2-minute intervals (<italic>q</italic>&lt;0.05). <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the mean Z-values across participants for all <italic>a priori</italic> ROIs (16 in each hemisphere), including lateral prefrontal regions (top panel A), medial temporal lobe regions, insula, putamen, and inferior parietal cortex (bottom panel B). While a large number of these regions had Z-values that were positive across participants (e.g., left hippocampus, left entorhinal cortex, right perirhinal cortex, right amygdala, bilateral insula, and right caudal middle frontal cortex, <italic>p</italic>&lt;0.05 uncorrected), we report only those that survived FDR correction.</p><p>As part of an exploratory search, we also performed this analysis on the other brain regions derived from FreeSurfer cortical parcellation. This included the 16 ROIs mentioned above, in addition to regions in the occipital lobe, parietal lobe, medial prefrontal cortex, lateral temporal lobe, basal ganglia, thalamus and brainstem (the complete list of regions can be found in <xref ref-type="supplementary-material" rid="SD4-data">Figure 5—source data 1</xref>). Out of the 84 regions tested (42 in each hemisphere), the right entorhinal cortex, right pars orbitalis, and left caudal anterior cingulate cortex (ACC) showed significant positive correlations between pattern change and duration estimates (<italic>q</italic>&lt;0.1). This suggests that the right entorhinal cortex and right pars orbitalis, which were part of our list of <italic>a priori</italic> ROIs, contained effects that were apparent even after whole-brain correction, and reveals an additional effect in the left caudal ACC that we had not anticipated. <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> displays the locations of these three regions in MNI space.</p></sec><sec id="s10"><title>Within-interval correlation between pattern change and duration estimates</title><p>Above, in the within-participants analysis, we found that the neural pattern distance between two clips at encoding was correlated with retrospective duration judgments in the right entorhinal cortex, right pars orbitalis and left caudal ACC. However, in the <italic>Behavioral results</italic>, we found that the perceptual and semantic similarity between two clips could explain some of the variance in subjective duration across intervals, even though it could not explain all the variance. Thus, it is possible that neural pattern change in the regions we found correlates with the component of duration estimates that is driven by perceptual and semantic content, rather than the component that is driven by abstract, slowly varying contextual features.</p><p>To rule out this concern, we performed a within-interval (across participants) version of the ROI analysis. For each ROI, we correlated (1) duration estimates for a given interval across participants with (2) the neural pattern distances for that interval across participants; results were then aggregated across all 2-min intervals. Rather than capturing variance within an individual across intervals of the story, this analysis captures variance across individuals for a given interval of the story. By performing the correlation within a given interval, we hold constant the perceptual and semantic content of the two clips and only leverage individual differences in how long the interval appeared retrospectively.</p><p>As described in the Materials and methods, a permutation test was used to assess the statistical significance of each correlation. Duration estimates were scrambled across participants 10,000 times to obtain a distribution of null correlations for every interval in every ROI. This enabled us to calculate a Z-value, which reflects the strength of the actual correlation between pattern distance and duration estimates relative to the distribution of null correlations. Finally, a right-tailed t-test was performed to assess whether the Z-values for a region were reliably above 0 across intervals. The p-values from this t-test were subjected to multiple comparisons correction using FDR.</p><p>Out of the regions selected <italic>a priori</italic>, the right entorhinal cortex, right amygdala, and right insula showed a significant positive correlation between pattern change and duration estimates for high-confidence 2-minute intervals (<italic>q</italic>&lt;0.05). <xref ref-type="fig" rid="fig6">Figure 6</xref> shows the mean Z-values across intervals for all <italic>a priori</italic> ROIs (16 in each hemisphere).<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.016</object-id><label>Figure 6.</label><caption><title>Within-interval ROI analysis: mean Z-values (across all 2-min intervals) of correlations between pattern distance and duration estimates for the 16 <italic>a priori</italic> ROIs.</title><p>Error bars represent standard errors of the mean. Correlations between pattern change and duration estimates were performed across participants, separately for each interval.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.016">http://dx.doi.org/10.7554/eLife.16070.016</ext-link></p><p><supplementary-material id="SD5-data"><object-id pub-id-type="doi">10.7554/eLife.16070.017</object-id><label>Figure 6—source data 1.</label><caption><title>Within-interval analysis Z-values and Pearson’s <italic>r</italic> values for all intervals and regions in the FreeSurfer and MTL atlases.</title><p>NaNs for a given interval and region indicate that there were not enough participants who rated that interval as confident and who had at least 10 voxels in the specific region to calculate a correlation (this was only an issue for the frontal pole). The bar plots in <xref ref-type="fig" rid="fig6">Figure 6</xref> were generated by plotting the mean Z-value (and standard error of the mean) across intervals for each of the <italic>a priori</italic> ROIs. The t-values were obtained from a right-tailed t-test on the Z-values for each region. The p-values from this t-test were then subjected to multiple comparisons correction using FDR.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.017">http://dx.doi.org/10.7554/eLife.16070.017</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16070-fig6-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig6-v2"/></fig></p><p>Extending this analysis to the whole brain (same anatomical masks as in <xref ref-type="supplementary-material" rid="SD4-data">Figure 5—source data 1</xref>) revealed only the right entorhinal cortex (<italic>q</italic>&lt;0.05), suggesting that the effect in this region was strong enough to survive whole-brain correction.</p><p>Importantly, the right entorhinal cortex is the only region with significant effects in both the within-interval analysis (Cohen’s <italic>d</italic> = 0.83) and the within-participant analysis (Cohen’s <italic>d</italic> = 0.79). If neural pattern distance between two clips in entorhinal cortex were driven solely by changes in clip content, we would have expected the correlation with duration estimates to be larger for the within-participant analysis (where story content differed across intervals) than for the within-interval analysis (where story content is held constant). The fact that the effect sizes are similar shows that perceptual or semantic differences in content between the two clips are not the main factor driving the correlation between duration estimates and neural pattern change in this region.</p></sec><sec id="s11"><title>Mixed-effects model accounting for naïve duration estimates</title><p>We analyzed our data using a hierarchical linear regression model (<xref ref-type="bibr" rid="bib45">Gelman and Hill, 2006</xref>; see Materials and methods for additional detail). This analysis estimates population-level effects of interest, while controlling for the possibility of individual variability between subjects and between clip pairs. In other words, this approach leverages the power of the within-interval analysis to control for the objective content similarity between two clips, while also taking into account variability in the effect across participants. In addition, we included the mean duration estimates from our naïve participants as a covariate in the model (see <italic>Behavioral results</italic>). Since naïve participants had estimated the temporal distance between each pair of clips without hearing the story, this covariate is a further control for the inherent guessability of the temporal distance between two clips. Both controls strengthen our interpretation that the remaining effect of neural pattern distance on duration estimates is driven by the contextual dissimilarity (rather than perceptual or content dissimilarity) between two clips.</p><p>For each anatomical region derived from FreeSurfer and MTL segmentation (42 in each hemisphere), we fit a model where duration estimates were predicted by naïve duration estimates as well as the neural pattern distance in that region (see Materials and methods for the complete formula). We then computed 95% confidence intervals of the fixed-effects parameter estimates using the asymptotic Gaussian approximation (see Materials and methods).</p><p>The fixed effect of naïve estimates was positive in all models and its confidence intervals did not include zero in 80% of the models. This reproduced our finding that naïve duration estimates are correlated with the original duration estimates (see <italic>Behavioral results</italic>), suggesting that interval durations are partially guessable based on the similarity between clips. However, even under this control, the fixed effect of neural pattern distance in left caudal ACC and right entorhinal cortex exhibited confidence intervals that did not include zero (<xref ref-type="fig" rid="fig7">Figure 7</xref>). <xref ref-type="supplementary-material" rid="SD6-data">Figure 7—source data 1</xref> contains the parameter estimates and 95% confidence intervals for all 84 anatomical regions.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.018</object-id><label>Figure 7.</label><caption><title>Parameter estimates and 95% confidence intervals for the fixed effect of neural pattern distance on duration estimates.</title><p>We also included the right amygdala and right superior temporal cortex in the figure, because their confidence intervals did not include 0 when a slightly less conservative fitting procedure was used (see <xref ref-type="supplementary-material" rid="SD6-data">Figure 7—source data 1</xref> and Materials and methods).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.018">http://dx.doi.org/10.7554/eLife.16070.018</ext-link></p><p><supplementary-material id="SD6-data"><object-id pub-id-type="doi">10.7554/eLife.16070.019</object-id><label>Figure 7—source data 1.</label><caption><title>Parameter estimates (betas) and 95% confidence intervals for the fixed effects of neural pattern distance on duration estimates for all 84 anatomical regions.</title><p>Parameter estimates are provided for four variants of the mixed-effects ROI analysis: 1) full model (with naïve estimates) using the <xref ref-type="bibr" rid="bib28">Chung et al., 2015</xref> blme fitting procedure and Box-Cox transform of duration estimates (see Materials and methods), 2) model without naïve estimates, using the <xref ref-type="bibr" rid="bib28">Chung et al., 2015</xref> blme fitting procedure and Box-Cox transform of duration estimates, 3) full model (with naïve estimates) using the <xref ref-type="bibr" rid="bib3">Bates et al., 2015</xref> lme4 fitting procedure and Box-Cox transform of duration estimates, and 4) full model (with naïve estimates) using the <xref ref-type="bibr" rid="bib28">Chung et al., 2015</xref> blme fitting procedure, but without any transform of duration estimates. The first analysis variant, which is the most conservative, is the one reported in the Results and plotted in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.019">http://dx.doi.org/10.7554/eLife.16070.019</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16070-fig7-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig7-v2"/></fig></p><p>Importantly, including the naïve duration estimates as a covariate in the model did not significantly weaken the relationship between neural pattern distance and duration estimates in these regions (though the effects were slightly lower numerically). <xref ref-type="fig" rid="fig7">Figure 7</xref> shows in green the 95% confidence intervals for the same ROIs when naïve duration estimates are excluded from the model.</p></sec></sec><sec id="s2-2-2"><title>Whole-brain searchlights</title><p>As with the Anatomical ROI analyses, both within-participant and within-interval analyses were performed for the Whole-Brain Searchlight analyses, in order to rule out the possibility that our effects were driven either by participant or interval random effects.</p><sec id="s13"><title>Within-participant whole-brain searchlight</title><p>We ran a cubic searchlight with 3x3x3 (27) voxels (972 mm<sup>3</sup>) through the entire brain and tested for a correlation between pattern change and duration estimates in each searchlight. The same phase-randomization procedure that was used for the within-participant anatomical ROI analysis was also applied here; this procedure generates Z-values that reflect how likely we are to get this strong of a correlation by chance, given the frequency spectrum of the fMRI data. When excluding low-confidence intervals, we found a significant cluster in the right anterior temporal lobe (<italic>p</italic>=0.034, FWE-corrected; Center of Gravity MNI coordinates (x, y, z) in mm: [45.6, −5.53, −21.7]; cluster size=572 voxels in 3 mm MNI space). Small parts of the cluster also extended to the right posterior insula and right putamen (see <xref ref-type="fig" rid="fig8">Figure 8</xref>).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.020</object-id><label>Figure 8.</label><caption><title>Results of within-participant whole-brain searchlight.</title><p>Voxels in orange represent centers of searchlights that exhibited significant correlations between pattern change and duration estimates within participants across intervals (<italic>p</italic>&lt;0.05, FWE). The significant cluster had peak MNI coordinates (in mm): <italic>x</italic> = 45.6, <italic>y</italic> = -5.53, <italic>z</italic> = -21.7.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.020">http://dx.doi.org/10.7554/eLife.16070.020</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig8-v2"/></fig></p></sec><sec id="s14"><title>Within-interval Whole-brain searchlight</title><p>We also ran a searchlight version of the within-interval analysis. In order to match searchlights across participants, functional data were transformed to 3 mm MNI space. Since this transformation approximately doubles the number of brain voxels, we ran cubic searchlights of radius 2 with 5x5x5 (125) voxels through the entire brain.</p><p>As with the ROI analysis, this analysis was performed on high-confidence duration estimates. For each interval, we only included participants who had confidently recollected the temporal position of the two clips delimiting that interval.</p><p>To assess the significance of each correlation score, we used the same permutation test as for the ROI analysis. Duration estimates were scrambled across participants 10,000 times to obtain a distribution of null correlations, and Z-values were calculated for each interval. We thus obtained a brain map of Z-values for each of the 24 intervals, and FSL’s <italic>randomise</italic> function was used to control the family-wise error rate, as above.</p><p>Similarly to the within-participant searchlight, we found a significant cluster in the right anterior temporal lobe (<italic>p</italic>=0.019, FWE-corrected; Center of Gravity MNI coordinates (x, y, z) in mm: [32.1, −10.2, −18.7]; cluster size=535 voxels in 3 mm MNI space). The cluster extended from the right parahippocampal gyrus, hippocampus and amygdala medially to the middle temporal gyrus and temporal pole laterally (see <xref ref-type="fig" rid="fig9">Figure 9</xref>).<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.021</object-id><label>Figure 9.</label><caption><title>Results of within-interval whole-brain searchlight.</title><p>Voxels in orange represent centers of searchlights that exhibited significant correlations between pattern change and duration estimates across participants (<italic>p</italic>&lt;0.05, FWE). The significant cluster had center of gravity MNI coordinates (in mm): <italic>x</italic> = 32.1, <italic>y</italic> = −10.2, <italic>z</italic> = −18.7.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.021">http://dx.doi.org/10.7554/eLife.16070.021</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig9-v2"/></fig></p></sec></sec><sec id="s2-2-3"><title>Comparing results from ROI and searchlight analyses</title><p>The within-participant ROI analysis revealed significant effects in the right entorhinal cortex, right pars orbitalis and left caudal ACC. The within-interval ROI analysis revealed significant effects in the right entorhinal cortex, right amygdala and right insula. The mixed-effects ROI analysis showed that the right entorhinal cortex and left caudal ACC had confidence intervals above 0, even when naïve duration estimates were accounted for. Both the within-participant and within-interval searchlights revealed significant clusters in the right anterior temporal lobe. <xref ref-type="fig" rid="fig10">Figure 10</xref> enables a comparison of the two searchlight analyses; the right entorhinal cortex ROI that emerged in all three ROI analyses is also overlaid. The within-interval searchlight cluster was located more medially than the within-participant searchlight cluster, though the two overlapped in the right amygdala, right temporal pole, and the cerebral white matter of the right anterior temporal lobe. Moreover, the within-interval searchlight cluster overlapped with the right entorhinal cortex ROI (see green voxels, <xref ref-type="fig" rid="fig10">Figure 10</xref>).<fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.022</object-id><label>Figure 10.</label><caption><title>Comparison of ROI and Searchlight results.</title><p>The within-participant searchlight cluster (<italic>p</italic>&lt;0.05, FWE) is displayed in blue; the within-interval searchlight cluster (<italic>p</italic>&lt;0.05, FWE) is displayed in yellow; voxels that overlap between the searchlights are shown in green. The right entorhinal cortex (<italic>q</italic>&lt;0.05 FDR in both ROI analyses) is displayed in red; voxels that overlap between the within-interval searchlight and the right entorhinal ROI are shown in green.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.022">http://dx.doi.org/10.7554/eLife.16070.022</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig10-v2"/></fig></p><p>The difference in the set of regions that passed the significance threshold between the ROI and searchlight analyses is very likely due to the difference in shapes between the searchlight cube and the anatomical masks. Following the anatomy is particularly important for small, elongated regions like entorhinal cortex and caudal ACC, which are unlikely to be perfectly aligned across participants. For the searchlight analyses, the data needed to be transformed to MNI space in order to aggregate the results; consequently, imperfections in alignment can reduce the significance of searchlight results in these regions. On the other hand, anatomical ROI analyses were performed entirely in native space, making them more suitable for idiosyncratically shaped regions.</p></sec><sec id="s2-2-4"><title>Patterns of activity in entorhinal cortex change slowly over time</title><p>To further probe the idea that the regions we found represent slowly changing contextual features, we assessed whether their patterns of activity change slowly over time relative to the rest of the brain. We focused this analysis on the right entorhinal cortex and left caudal ACC, both of which were significant in the mixed-effects ROI analysis.</p><p>We quantified the speed of BOLD signal change in three different ways: (1) a multivariate procedure, (2) a multivariate procedure in which we regressed out ROI size, and (3) a univariate procedure. (1) For the multivariate procedure, we obtained the mean auto-correlation function of the pattern in every region, and took the full-width half-maximum (FWHM) of this function as a measure of how slowly the pattern moves away from itself on average (see Materials and methods). (2) Since this analysis was performed on anatomical masks derived from FreeSurfer parcellation, they varied substantially in size. To ensure that differences in the speed of pattern change were not due to differences in ROI size, we also performed the multivariate procedure after regressing the vector of ROI sizes (number of voxels) out of the vector of FWHM values for each participant. (3) Finally, we performed the above analysis for every voxel individually. Rather than calculating the mean auto-correlation function of the pattern in every region, we calculated the auto-correlation function of every voxel’s time course and averaged the auto-correlation functions across all the voxels in a given region. The FWHM was then computed for this mean auto-correlation derived from individual voxel time courses.</p><p>Using these three procedures, we compared the FWHMs in the right entorhinal cortex and left caudal ACC with FWHMs in three regions known to be involved in auditory and language processing: the right transverse temporal cortex, which encompasses primary auditory cortex (<xref ref-type="bibr" rid="bib33">Destrieux et al., 2010</xref>; <xref ref-type="bibr" rid="bib85">Shapleske et al., 1999</xref>), the right banks of the superior temporal sulcus and the right superior temporal cortex, which are involved in auditory processing and the early cortical stages of speech perception (<xref ref-type="bibr" rid="bib9">Binder et al., 2000</xref>; <xref ref-type="bibr" rid="bib52">Hickok and Poeppel, 2004</xref>).</p><p><xref ref-type="table" rid="tbl1">Table 1</xref> shows the FWHMs in the above regions derived using the three procedures, as well as the ranking of the right entorhinal cortex and left caudal ACC mean FWHMs relative to all the other masks in the brain (84 in total).<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.023</object-id><label>Table 1.</label><caption><p>Speed of pattern change in the right entorhinal cortex and left caudal ACC relative to the rest of the brain. Full-Width Half-Maximum (FWHM) values reflect how slowly patterns of activity (multivariate) or individual voxels (univariate) change over time. The Multivariate (-ROI size) column reflects the slowness of pattern change when controlling for the effect of ROI size.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.023">http://dx.doi.org/10.7554/eLife.16070.023</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle"/><th align="center" colspan="2" valign="middle"><bold>Multivariate</bold></th><th align="center" colspan="2" valign="middle"><bold>Multivariate (-ROI size)</bold> </th><th align="center" colspan="2" valign="middle"><bold>Univariate</bold></th></tr></thead><tbody><tr><td align="left" valign="middle">Region</td><td align="left" valign="middle">FWHM (TRs)</td><td align="left" valign="middle">Ranking</td><td align="left" valign="middle">FWHM (TRs)</td><td align="left" valign="middle">Ranking</td><td align="left" valign="middle">FWHM (TRs)</td><td align="left" valign="middle">Ranking</td></tr><tr><td align="left" valign="middle">Right entorhinal</td><td align="left" valign="middle"><italic>M=18.9, SD=13.8</italic> </td><td align="left" valign="middle">3rd</td><td align="left" valign="middle"><italic>M=1.2, SD=1.9</italic></td><td align="left" valign="middle">4th</td><td align="left" valign="middle"><italic>M=23, SD=15.6</italic></td><td align="left" valign="middle">1st</td></tr><tr><td align="left" valign="middle">Left caudal ACC</td><td align="left" valign="middle"><italic>M=8.3, SD=1.8</italic></td><td align="left" valign="middle">66th</td><td align="left" valign="middle"><italic>M=-0.5, SD=0.5</italic></td><td align="left" valign="middle">67th</td><td align="left" valign="middle"><italic>M=9.2, SD=3.8</italic></td><td align="left" valign="middle">46th</td></tr><tr><td align="left" valign="middle">Right transverse temporal cortex</td><td align="left" valign="middle"><italic>M=7.3, SD=1.2</italic></td><td align="left" valign="middle">80th</td><td align="left" valign="middle"><italic>M=-0.8, SD=0.5</italic></td><td align="left" valign="middle">83rd</td><td align="left" valign="middle"><italic>M=7.9, SD=1.2</italic></td><td align="left" valign="middle">68th</td></tr><tr><td align="left" valign="middle">Right banks of superior temporal sulcus</td><td align="left" valign="middle"><italic>M=9.0, SD=2.1</italic></td><td align="left" valign="middle">48th</td><td align="left" valign="middle"><italic>M=-0.3, SD=0.4</italic></td><td align="left" valign="middle">49th</td><td align="left" valign="middle"><italic>M=8.8, SD=1.7</italic></td><td align="left" valign="middle">61st</td></tr><tr><td align="left" valign="middle">Right superior temporal cortex</td><td align="left" valign="middle"><italic>M=11.0, SD=3.1</italic></td><td align="left" valign="middle">28th</td><td align="left" valign="middle"><italic>M=0.4, SD=0.6</italic></td><td align="left" valign="middle">18th</td><td align="left" valign="middle"><italic>M=10.3, SD=2.4</italic></td><td align="left" valign="middle">34th</td></tr></tbody></table></table-wrap></p><p>Across all three procedures, a right-tailed Wilcoxon signed-rank test indicated that the FWHMs in the right entorhinal cortex were consistently larger across participants than the FWHMs in the right transverse temporal cortex (<italic>p</italic>&lt;0.00005, <italic>p</italic>&lt;0.0005 and <italic>p</italic>&lt;0.00005), the right banks of the superior temporal sulcus (<italic>p</italic>&lt;0.001, <italic>p</italic>&lt;0.001 and <italic>p</italic>&lt;0.0005) and the right superior temporal cortex (<italic>p</italic>&lt;0.005, <italic>p</italic>=0.06 and <italic>p</italic>&lt;0.0005). Thus, single voxels and multivariate patterns in entorhinal cortex changed consistently more slowly than those in regions involved in auditory and language processing. Moreover, the mean FWHM in the right entorhinal cortex was one of the largest among all 84 regions, ranking 3rd, 4th and 1st in the brain across the three procedures. The other regions with the slowest voxel and pattern change included the temporal pole, medial and lateral orbitofrontal cortex, frontal pole, perirhinal cortex, pars orbitalis and inferior temporal cortex.</p><p>On the other hand, the left caudal ACC ranked 66th, 67th and 46th out of 84 regions across the three procedures, suggesting that it did not exhibit slow signal change relative to the rest of the brain. Across the three procedures, the FWHMs in the left caudal ACC were larger than those in the right transverse temporal cortex (<italic>p</italic>&lt;0.01, <italic>p</italic>&lt;0.005, and <italic>p</italic>=0.059), but generally smaller than those in the right banks of the superior temporal sulcus (<italic>p</italic>=0.97, <italic>p</italic>=0.96, and <italic>p</italic>=0.42) and the right superior temporal cortex (<italic>p</italic>=1.0, <italic>p</italic>=1.0, <italic>p</italic>=0.98). Thus, patterns in the left caudal ACC changed only slightly more slowly than those in primary auditory cortex.</p><p>Taken together, all three variants of the analysis showed that the right entorhinal cortex, along with other regions of the anterior and medial temporal lobe, orbitofrontal cortex and frontal pole, had the slowest pattern change in the brain. These results do not seem to be due to differences in the sizes of the anatomical masks and suggest that the right anterior MTL regions found most consistently in our ROI and searchlight analyses process information that changes slowly over time. Our findings are consistent with those of <xref ref-type="bibr" rid="bib90">Stephens et al. (2013)</xref>, who showed that auditory cortex regions processing momentary stimulus features had intrinsically faster dynamics than higher-order regions that integrated information over longer time scales (see also <xref ref-type="bibr" rid="bib63">Lerner et al., 2011</xref>).</p></sec><sec id="s2-2-5"><title>Story position effects cannot explain the correlation between duration estimates and neural pattern change</title><p>We found that duration estimates systematically decreased as a function of position in the story, with earlier intervals being estimated as longer than later intervals (<xref ref-type="fig" rid="fig11">Figure 11</xref>). The correlation between the estimated duration of an interval and its time in the story was consistently negative across participants (<italic>M</italic>=−0.40, <italic>SD</italic>= 0.22; <italic>t</italic>(16)=−7.59, <italic>p</italic>&lt;0.00001).<fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.024</object-id><label>Figure 11.</label><caption><title>Mean duration estimates and pattern distances (across participants) for all 2-minute intervals as a function of the interval’s position in the story.</title><p>The middle time point of each 2-min interval (half-way between the two clips delimiting it) was chosen as the x-coordinate.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.024">http://dx.doi.org/10.7554/eLife.16070.024</ext-link></p><p><supplementary-material id="SD7-data"><object-id pub-id-type="doi">10.7554/eLife.16070.025</object-id><label>Figure 11—source data 1.</label><caption><title>Duration estimates and pattern distances in all FreeSurfer and MTL ROIs for each 2-minute interval in every participant.</title><p>Data prior to high-pass filtering and after high-pass filtering (cut-off = 480 s) are provided. The unfiltered neural pattern distances tend to increase with time in story, even in the CSF and white matter. To generate the plots in <xref ref-type="fig" rid="fig11">Figure 11</xref>, duration estimates and pattern distances were averaged across participants for each interval and plotted as a function of the interval’s position in the story. The interval’s position in the story (in minutes) was set as the middle time point between the two clips delimiting it.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.025">http://dx.doi.org/10.7554/eLife.16070.025</ext-link></p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-16070-fig11-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig11-v2"/></fig></p><p>This result may be a replication of the positive time-order effect: the finding that people judge earlier durations in a series of durations to be longer than later durations (<xref ref-type="bibr" rid="bib12">Block, 1982</xref>, <xref ref-type="bibr" rid="bib13">1985</xref>; <xref ref-type="bibr" rid="bib20">Brown and Stubbs, 1988</xref>). The effect has been interpreted to mean that context usually changes more rapidly at the start of a novel episode (<xref ref-type="bibr" rid="bib12">Block, 1982</xref>, <xref ref-type="bibr" rid="bib14">1986</xref>). However, another possibility is that the characteristics of the particular story we picked are driving this result. In our story, there was a strong negative correlation between the mean number of event boundaries per interval and the position of the interval in the story (<italic>r</italic>=−0.77). Thus, the decrease in mean duration estimates with story position may be due to the relationship between the number of event boundaries and duration estimates (see <italic>Behavioral results</italic>).</p><p>If the decrease in duration estimates over time is due to a decrease in the amount of contextual change over the course of the story, we might expect BOLD pattern dissimilarity to decrease over time in the brain regions yielded by our ROI analyses. However, there was no consistent correlation between pattern change during an interval and its time in the story for the right entorhinal cortex (<italic>M</italic>=0.03, <italic>SD</italic>=0.21; <italic>t</italic>(16)= 0.65; <italic>p</italic>=0.53), the right pars orbitalis (<italic>M</italic>=−0.10, <italic>SD</italic>=0.22; <italic>t</italic>(16)=−1.83, <italic>p</italic>=0.09), the left caudal ACC (<italic>M</italic>=−0.05, <italic>SD</italic>=0.18; <italic>t</italic>(16)=−1.15, <italic>p</italic>=0.27), the right amygdala (<italic>M</italic>=−0.02, <italic>SD</italic>=0.23; <italic>t</italic>(16)=−0.28, <italic>p</italic>=0.78) or the right insula (<italic>M</italic>=−0.08, <italic>SD</italic>=0.25; <italic>t</italic>(16)=−1.34, <italic>p</italic>=0.20). These results suggest that the relationship between duration estimates and pattern dissimilarity in these regions was not driven by a shared effect of story position. Rather, it seems that pattern dissimilarity in these regions correlated with more fine-grained variations in the estimated durations of nearby intervals (<xref ref-type="fig" rid="fig11">Figure 11</xref>).</p><p>To investigate why the above regions did not show the expected decrease in pattern dissimilarity over time, we assessed whether any brain region in the FreeSurfer or MTL atlas might show this effect. There was no brain region whose pattern of activity changed more at the beginning than at the end of the story. Given that we were looking for a slow change in neural signal (unfolding over the entire course of the story), we thought that our high-pass filter might be removing this slow change; to address this possibility, we analyzed the unfiltered data. When we did this, we found that neural pattern change in the unfiltered data showed a consistent correlation in the <italic>opposite</italic> direction: almost all brain patterns changed more at the end of the story than at the beginning, including the CSF and white matter (<italic>q</italic>&lt;0.05, FDR), suggesting that a signal unrelated to neural processing, such as scanner drift or motion, may cause activity patterns to change more as time passes (see <xref ref-type="supplementary-material" rid="SD7-data">Figure 11—source data 1</xref>). Thus, even if the degree of neural pattern change were decreasing over time, we might not be able to detect this effect, as it would have to overcome a global signal in the opposite direction that is not due to neural activity and that is present everywhere, including the CSF.</p></sec><sec id="s2-2-6"><title>Replication of <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref>: activity at encoding predicts accuracy of temporal context memory</title><p>As described in the Materials and methods (<italic>Time perception test</italic> section), besides estimating the elapsed duration between pairs of clips from the story, participants were given an additional test, where they indicated each clip’s position on the timeline of the story. The mean correlation (across participants) between the actual and estimated temporal position on the timeline of the story was <italic>r</italic>=0.885 (<italic>SD</italic>=0.05), suggesting that participants remembered the temporal position of each clip extremely well (p&lt;10<sup>–21</sup>). <xref ref-type="fig" rid="fig12">Figure 12</xref> shows the timeline estimates for a representative participant (top left panel), as well as the absolute residual error associated with each clip (top right panel), group averaged and plotted against time in the story.<fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.026</object-id><label>Figure 12.</label><caption><title>Replication of <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref>: activity at encoding predicts accuracy of temporal context memory.</title><p><bold>Top left panel</bold>: Timeline estimates for a representative participant. The estimated temporal position of each clip is plotted against its actual position in the story. <bold>Top right panel</bold>: Group-averaged residual error for each clip plotted against its time in the story. Our behavioral results mimic those of Figure 2 in <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref> showing that accuracy increases for clips that occurred later in the story. <bold>Bottom panels</bold>: Clusters that showed a significant correlation between activity at encoding and subsequent accuracy at placing a clip on the timeline of the story. The prefrontal cluster in light blue was significant (<italic>p</italic>=0.008, FWE), while the medial parietal cluster (<italic>p</italic>=0.058, FWE) and the lateral temporal cluster in dark blue (<italic>p</italic>=0.098, FWE) were trending.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.026">http://dx.doi.org/10.7554/eLife.16070.026</ext-link></p></caption><graphic mime-subtype="png" mimetype="image" xlink:href="elife-16070-fig12-v2"/></fig></p><p>This behavioral dataset enabled us reproduce an fMRI analysis from <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref>, where voxel activity at encoding was correlated with subsequent accuracy in remembering when a trial occurred in the experiment. For each participant, we regressed the estimated timeline position against the actual position and used the absolute value of the residual as a measure of error. We found that the accuracy (negative error) of timeline placements was significantly correlated with encoding activity in large clusters of the left dorsolateral prefrontal cortex and medial prefrontal cortex, including dorsomedial PFC and anterior cingulate (<italic>p</italic>=0.008, FWE-corrected; Center of Gravity MNI coordinates (x, y, z) in mm: [−20, 34.8, 28.4]; cluster size = 1121 voxels in 3 mm MNI space), as well as sub-threshold clusters in the medial parietal cortex, including precuneus and posterior cingulate (<italic>p</italic>=0.058, FWE-corrected; Center of Gravity MNI coordinates (x, y, z) in mm: [−10.5, −54, 16.1]; cluster size = 419 voxels), and left superior temporal gyrus (<italic>p</italic>=0.098, FWE-corrected; Center of Gravity MNI coordinates (x, y, z) in mm: [−56.9, −19.1, −3.72]; cluster size = 270 voxels).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While human and animal time perception has been a subject of intense empirical investigation (see <xref ref-type="bibr" rid="bib99">Wittmann, 2013</xref>), most neuroimaging studies have tested its mechanisms on the scale of milliseconds to seconds and neglected paradigms in which long-term memory plays an important role. Such studies have typically employed <italic>prospective paradigms</italic>, in which participants must deliberately attend to the duration of a stimulus. However, behavioral studies in humans have consistently demonstrated that <italic>retrospective paradigms</italic>, in which participants are asked to estimate the duration of an elapsed interval from memory, tap into different cognitive mechanisms from prospective ones (<xref ref-type="bibr" rid="bib53">Hicks et al., 1976</xref>; <xref ref-type="bibr" rid="bib101">Zakay and Block, 2004</xref>; <xref ref-type="bibr" rid="bib11">Block and Zakay, 2008</xref>). In retrospective paradigms, changes in spatial, emotional and cognitive context tend to modulate estimates of elapsed time (<xref ref-type="bibr" rid="bib16">Block, 1992</xref>; <xref ref-type="bibr" rid="bib10">Block and Reed, 1978</xref>; <xref ref-type="bibr" rid="bib83">Sahakyan and Smith, 2014</xref>; <xref ref-type="bibr" rid="bib77">Pollatos et al., 2014</xref>).</p><p>In the present study, we used changes in patterns of BOLD activity as a proxy for mental context change. We sought to extend previous neuroimaging work by testing whether neural pattern change predicts duration estimates on the scale of several minutes and in a more naturalistic setting, where spatial location, situational inference, characters, and emotional elements can all drive contextual change.</p><p>Participants were scanned while they listened to a 25-minute radio story and were subsequently asked how much time (in minutes and seconds) had elapsed between pairs of clips from the story (all pairs were in fact two minutes apart). Using this approach, we were able to probe retrospective duration memory repeatedly within participants without needing to interrupt the encoding of the story. This allowed us to leverage within-participant variability in neural pattern change and relate it to a participant’s retrospective duration estimates.</p><p>Using a within-participant anatomical ROI analysis (encompassing 16 regions selected <italic>a priori</italic>), we found that neural pattern distance in the right entorhinal cortex and right pars orbitalis at the time of encoding was correlated with subsequent duration estimates. Extending this analysis to all anatomical ROIs in cortex revealed an additional effect in the left caudal anterior cingulate cortex (ACC). These results converged qualitatively with the results of our whole-brain searchlight analysis, which revealed a significant cluster spanning the right anterior temporal lobe.</p><p>To test our interpretation that duration estimates were driven by contextual change, we asked a separate group of participants to identify event boundaries in the story. We found that the number of event boundaries between two clips was very highly correlated with participants’ subsequent duration estimates. Importantly, the number of event boundaries was significantly less correlated with duration estimates for a separate group of 'naïve' participants, who had been asked to estimate the elapsed time between clips without first hearing the story. These behavioral experiments provide evidence that retrospective duration estimates were indeed influenced by memory for intervening contextual changes between clips.</p><p>In addition, we sought to rule out the possibility that neural pattern distance between two clips reflected only the perceptual or semantic similarity between them, rather than the degree of mental context change. We performed a within-interval analysis, in which pattern distances for the same pair of clips were correlated with duration estimates <italic>across participants</italic>. The within-interval ROI analysis yielded effects of the same size in the right entorhinal cortex, right amygdala and right insula. The within-interval whole-brain searchlight revealed a significant cluster in the right anterior temporal lobe. Thus, pattern distance in the right anterior temporal lobe, particularly the right entorhinal cortex, predicted variability in duration estimates even when the perceptual and semantic distance of the clips was controlled as much as possible, suggesting that pattern change in these regions may capture idiosyncratic differences in mental context that cannot be predicted from the stimulus alone.</p><p>Finally, if neural pattern distance between two clips reflected only the similarity in content between them, rather than abstract contextual similarity, we would expect the correlation between pattern distance and duration estimates to be weakened when controlling for naïve duration estimates, which were based solely on the perceptual and semantic similarity between two clips. Fitting a mixed-effects model to each ROI showed that neural pattern distance in the right entorhinal cortex, along with the left caudal ACC, exhibited a significant effect on duration estimates even when all other factors, including random effects of participants and intervals, as well as naïve duration estimates, were controlled for.</p><p>In support of the hypothesis that these regions represent slowly varying contextual information, we found that the right entorhinal cortex, as well as adjacent regions of the MTL, temporal pole and orbitofrontal cortex, had some of the slowest neural pattern change in the entire brain. This is in line with findings that brain regions at the top of the processing hierarchy (furthest from the primary perceptual areas) integrate information over longer time scales and are therefore best suited for representing abstract information extracted from multiple streams of sensory observations (<xref ref-type="bibr" rid="bib90">Stephens et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Lerner et al., 2011</xref>).</p><p>Our results implicating the right entorhinal cortex in representing context fit well with other results in the literature. Multiple lines of evidence have suggested an important role for the entorhinal cortex in representing relationships between the spatial environment, task and incoming stimuli. Lesions of the lateral entorhinal cortex in rodents have shown that this region is necessary for discriminating between novel and familiar associations of object and place, object and non-spatial context, or place and context, while leaving non-associative forms of memory unaffected (<xref ref-type="bibr" rid="bib22">Buckmaster et al., 2004</xref>; <xref ref-type="bibr" rid="bib96">Wilson et al., 2013a</xref>; <xref ref-type="bibr" rid="bib95">2013b</xref>). Moreover, electrophysiological recordings in rats performing a spatial memory task showed that neurons in the medial entorhinal cortex exhibited greater context sensitivity and greater modulation by task-relevant mnemonic information than hippocampal neurons, while hippocampal neurons carried more specific spatial information (<xref ref-type="bibr" rid="bib65">Lipton et al., 2007</xref>). Medial entorhinal neurons also exhibited longer firing periods, which led the authors to propose that they could bind a series of hippocampal representations of distinct events (<xref ref-type="bibr" rid="bib64">Lipton and Eichenbaum, 2008</xref>). Thus, changes in distributed entorhinal activity patterns on the scale of minutes might represent changes in contextual elements that are later retrieved to make duration judgments (for theoretical discussion of the role of entorhinal cortex in contextual representation, see <xref ref-type="bibr" rid="bib56">Howard et al., 2005</xref>).</p><p>While the right entorhinal cortex was the only medial temporal lobe region that survived FDR correction in both our within-participant and within-interval ROI analyses, our whole-brain searchlights found a significant relationship between pattern change and duration estimates in two extensive clusters that overlapped in the right hippocampus, the right perirhinal cortex, right amygdala and right temporal pole.</p><p>Two previous studies, <xref ref-type="bibr" rid="bib74">Noulhiane et al. (2007)</xref> and <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref>, have directly implicated the MTL in retrospective time estimation in humans. <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref> scanned participants while they were presented with trial-unique faces and objects on a scene background, which changed every four trials. After each run, participants were asked whether pairs of stimuli had occurred close together or far apart in time (all pairs were about 50 s apart). They found that neural pattern distance in the left hippocampus at the time of encoding was greater for pairs of stimuli later rated as 'far apart', though only when the stimuli were separated by a scene change. <xref ref-type="bibr" rid="bib74">Noulhiane et al. (2007)</xref> used a retrospective behavioral paradigm similar to ours in patients with unilateral MTL lesions. In that study, participants were asked to estimate the temporal distance between object pictures that had been randomly inserted into a silent documentary film. They found that the degree of left entorhinal, left perirhinal and left temporopolar cortex damage correlated with the degree to which patients overestimated minutes-long intervals in retrospect. (For related evidence from the animal literature, see <xref ref-type="bibr" rid="bib58">Jacobs et al., 2013</xref>, who showed that bilateral inactivation of the hippocampus impaired rats’ ability to discriminate between similarly long durations, such as 8 and 12 minutes, but not between less similar intervals, such as 3 and 12 minutes.)</p><p>Our ROI and searchlight results are in line with the above set of findings, and suggest that patients with anterior MTL lesions might be impaired in retrospective time estimation because patterns of activity in entorhinal, perirhinal, and temporopolar cortex encode contextual changes on the scale of minutes. The set of regions we found is more extensive than those in <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref> and mostly right-lateralized. It is possible that the difference in the extent of our effects could be explained by differences in the paradigms that were used. In both the <xref ref-type="bibr" rid="bib74">Noulhiane et al. (2007)</xref> and <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi (2014)</xref> studies, the links between objects and their context had to be deliberately constructed. In our study, the clips whose temporal distance participants estimated were excerpts from a story, and therefore strongly linked with a situational, spatial, and emotional context. Thus, it is possible that activity patterns in a more extensive cluster tracked temporal distance estimates because our auditory story caused changes in a broader set of contextual features.</p><p>Extending our anatomical ROI analysis to the entire brain showed that pattern change in the left caudal anterior cingulate cortex (ACC) predicted subsequent duration estimates, and this region remained significant in a mixed-effects model controlling for the effect of naïve duration estimates. However, caudal ACC exhibited more rapid pattern change than the anterior and medial temporal lobe, suggesting that it may represent a qualitatively different, faster-changing signal. Caudal ACC activity has been shown to increase in response to shifts in task contingencies (see <xref ref-type="bibr" rid="bib86">Shenhav et al., 2013</xref>, for a review) and there is converging evidence that ACC responses are important for adjusting behavior to unexpected changes by increasing attention and learning rate (<xref ref-type="bibr" rid="bib21">Bryden et al., 2011</xref>; <xref ref-type="bibr" rid="bib5">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">McGuire et al., 2014</xref>). <xref ref-type="bibr" rid="bib75">O’Reilly et al. (2013)</xref> have provided evidence that the ACC only responds to surprising outcomes when they necessitate updating beliefs about the current state of the world. Although the present study was not designed to test such accounts, our findings are consistent with a role for ACC in updating predictive models. Events in the story that prompt participants to update their beliefs about the characters’ situation are also likely to cause changes in cognitive context and therefore overestimation of duration. However, future studies are needed to test this interpretation, for instance by manipulating belief updating independently of surprise and measuring its effect on retrospective duration estimates.</p><p>In addition to the anatomical ROI analysis, we performed a whole-brain searchlight that yielded an extensive cluster covering the right anterior temporal lobe, extending from the medial temporal regions described above to the middle temporal gyrus and temporal pole. Prior work has suggested that the middle temporal gyrus and temporal pole are involved in narrative comprehension (<xref ref-type="bibr" rid="bib40">Ferstl et al., 2008</xref>; <xref ref-type="bibr" rid="bib70">Mar, 2004</xref>) and narrative item memory (<xref ref-type="bibr" rid="bib49">Hasson et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Maguire et al., 1999</xref>). <xref ref-type="bibr" rid="bib37">Ezzyat and Davachi (2011)</xref> found a similarly located cluster (extending from the right perirhinal cortex to the right middle temporal gyrus) to be involved in integrating information within narrative events. In particular, they showed that activity within these regions gradually increases within events and that this increase predicts the degree to which memories become clustered within events. Retrospective time judgments have been shown to increase with the number of events an interval contains (<xref ref-type="bibr" rid="bib81">Poynter, 1983</xref>; <xref ref-type="bibr" rid="bib102">Zakay et al., 1994</xref>; <xref ref-type="bibr" rid="bib39">Faber and Gennari, 2015</xref>), suggesting that brain regions involved in clustering memories by events may carry important information for estimating durations.</p><p>Finally, we were able to replicate an analysis by <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref>, who showed that activity during encoding in the left lateral prefrontal cortex and right anterior hippocampus predicted accuracy in remembering when a trial had occurred in the experiment. Our analysis revealed a cluster in the left dorsolateral prefrontal cortex that is similar to that found in their study. However, we also found significant clusters in the medial prefrontal and medial parietal cortex. These regions may be important for maintaining narrative information over minutes-long timescales (<xref ref-type="bibr" rid="bib63">Lerner et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Hasson et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Chen et al., 2015</xref>), which might explain why their activity predicted temporal context memory for clips from an auditory story, but did not appear in <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref>, where participants recalled the timing of trials which were not linked by a narrative. Moreover, our clusters overlap highly with the 'posterior medial network' (<xref ref-type="bibr" rid="bib82">Ritchey and Ranganath, 2012</xref>), which has been consistently implicated in episodic memory, episodic simulation and theory of mind.</p><sec id="s3-1"><title>Conclusion</title><p>After probing human participants’ time perception for intervals from an auditory story they had just heard, we found substantial variability in subjective estimates of the passage of time. This variability was significantly correlated with changes in BOLD activity patterns in the right anterior temporal lobe, particularly the right entorhinal cortex, between the start and end of each interval. Control experiments demonstrated that duration estimates were strongly driven by contextual boundaries and that the relationship between neural distance and behavior still held when we controlled for the perceptual and semantic similarity of the clips. Our findings suggest that patterns of activity in these regions might encode contextual information that participants can later retrieve to infer the durations of intervals on the scale of minutes. Additional work is needed to assess how these regions contribute to representing particular contextual features (such as physical environment, abstract task states, and emotional states) and whether changes in each of these features affect retrospective duration estimates differently.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>18 participants (13 female) took part in the study. All participants were recruited from the Princeton undergraduate and graduate student population and were between 18 and 31 years of age (mean = 22 years). All participants were screened to ensure no neurological or psychiatric disorders. Written informed consent was obtained for all participants in accordance with the Princeton Institutional Review Board regulations. Participants were compensated $20/hr for the scanning session, and $12/hr for the behavioral session.</p><p>Given that no previous studies had related neural pattern change during a naturalistic stimulus to subsequent duration estimates for minutes-long intervals, we could not <italic>a priori</italic> estimate the variance in the pattern change signal, the variance in duration estimates, or the correlation between them. Therefore, rather than performing a power analysis, we chose a sample size that was in the same range as previous fMRI studies that had used naturalistic stimuli to study memory (<xref ref-type="bibr" rid="bib63">Lerner et al., 2011</xref>, <italic>n</italic>=11 per condition; <xref ref-type="bibr" rid="bib25">Chen et al., 2015</xref>, <italic>n</italic>=13, 14 and 24 per condition; <xref ref-type="bibr" rid="bib26">Chen et al., 2016</xref>, <italic>n</italic>=22 [5 excluded]), as well as fMRI studies that had related neural pattern distance to mnemonic judgments (<xref ref-type="bibr" rid="bib37">Ezzyat and Davachi, 2011</xref>, <italic>n</italic>=19; <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath, 2010</xref>, <italic>n</italic>=16 (1 excluded); <xref ref-type="bibr" rid="bib38">Ezzyat and Davachi, 2014</xref>, <italic>n</italic>=21 (3 excluded), <xref ref-type="bibr" rid="bib60">Jenkins and Ranganath, 2016</xref>, <italic>n</italic>=17).</p></sec><sec id="s4-2"><title>Experimental design and stimuli</title><p>The experiment consisted of two parts: an approximately 40-min session in the MRI scanner, during which participants listened to the auditory story, followed immediately by a 1-hr behavioral session, during which participants completed a time perception test on the story they had just heard. <xref ref-type="fig" rid="fig1">Figure 1</xref> illustrates the experimental procedure.</p><sec id="s4-2-1"><title>fMRI session</title><p>Prior to the fMRI session, participants were instructed to listen carefully to the auditory story while in the scanner, because they might be asked questions about it later. The nature of the follow-up questions was unknown to the participants. While in the scanner, participants listened to a 25-minute-long radio adaptation of a science fiction story called 'Tunnel Under the World' (written by Frederik Pohl), originally aired on the radio drama series, 'X Minus One', in 1956.</p></sec><sec id="s4-2-2"><title>Time perception test</title><p>After leaving the scanner, participants were surprised with a time perception test, presented on a laptop with the Psychophysics toolbox (<xref ref-type="bibr" rid="bib19">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib76">Pelli, 1997</xref>) for MATLAB (The MathWorks Inc., Natick, MA). For each of 43 questions, participants listened to a 10 s clip from the story, followed by another 10 s clip, and were asked to estimate how much time had passed between the first and second clips when they initially heard the story. Participants were specifically asked to estimate how much time had passed in their own lives, rather than how much narrative time had passed in the story. They were also asked to make the judgments as intuitively as possible, without resorting to deductive reasoning about the sequence of events that unfolded in between the two excerpts.</p><p>Participants had complete control over the pacing of the test. On each question, they initiated the playing of the clips, and were able to replay the clips if they missed them the first time. They could take as long as they wished to enter their duration estimates (in minutes and seconds), using the keyboard. Clip pairs were identical across participants, but the order in which the pairs were presented was randomized.</p><p>To control for the objective passage of time, we ensured that 24 of the clip pairs were 2 minutes apart and 19 of the pairs were 6 minutes apart. Debriefing showed that participants were unaware of this manipulation, and the high variability of duration estimates for both the 2 and 6-min intervals further confirmed that they were unaware of the fixed interval durations.</p><p>After participants had provided duration estimates for all 43 intervals, the 86 clips that had delimited those intervals were replayed in a random order (unpaired), and participants were asked to place each clip on the timeline of the story. For each of the 86 questions, a white line appeared on a black background, representing the full length of the story. Participants could place their cursor at any point on that line, followed by the Enter key. After each placement, they were asked to provide a confidence rating on a scale of 1 to 5, reflecting their confidence about that clip’s place in the story. Participants were instructed to base the confidence rating on their certainty of when that clip occurred in the story, rather than on the vividness of the memory for that clip.</p><p>Please note: the first of our 18 participants completed a version of the time perception test that differed only in the following way: the specific intervals in the story whose duration was asked about were different. In all other respects (half of the intervals were 2 min while the other half were 6 min apart), the behavioral test was identical to the subsequent 17 participants. For this reason, however, any analyses where duration estimates are compared across participants were performed on 17 rather than 18 participants. Any within-participant analyses were performed on all 18 data sets.</p></sec><sec id="s4-2-3"><title>Naïve time perception test</title><p>To address the concern that participants were estimating temporal distance between two clips based purely on the content of the clips (rather than their memory of when the clips had occurred in the story), we administered an identical time perception test to a separate group of 17 participants who had never heard the story. Naïve participants were asked to try their best to guess how much time passed between each pair of clips during the original telling of the story, even though they had never heard the story. Participants were told the length of the story (25 min, 33 s) and informed that the maximum distance between two clips could not exceed that duration.</p></sec><sec id="s4-2-4"><title>Event boundary test</title><p>A separate group of 9 participants were asked to listen to the same story and to press the space bar every time they thought an event had ended and a new event was beginning. This test was purely behavioral and fMRI data were not collected for these participants.</p></sec></sec><sec id="s4-3"><title>Behavioral data analysis</title><sec id="s4-3-1"><title>Significance of correlation between duration estimates and event boundaries</title><p>To assess whether the number of event boundaries in an interval predicted duration estimates for that interval, we related our original participants’ duration estimates with event boundary data collected from a separate group of 9 participants. For each 2-min interval from the time perception test, we counted the number of event boundaries that a participant had indicated during that interval and averaged that number across the 9 participants. This resulted in a mean number of event boundaries per interval, which was then correlated with the mean estimated duration of that interval from our original participants.</p><p>To assess the statistical significance of this correlation, we performed a bootstrapping procedure on the duration estimates. We obtained 1000 bootstrap samples, each time selecting with replacement a different subset of <italic>n</italic> individuals from our pool of <italic>n</italic> participants. The duration estimates for each subset were averaged across participants and correlated with the mean number of event boundaries. The upper limit (<italic>ul</italic>) for an <italic>x</italic>% confidence interval was set to the value of the Pearson correlation in percentile <italic>x%</italic> of the bootstrap distribution; the lower limit (<italic>ll</italic>) for the confidence interval was set to the value of the Pearson correlation in percentile 100-<italic>x</italic> of this distribution. Confidence intervals that did not encompass zero were considered reliable at the given level of confidence.</p></sec><sec id="s4-3-2"><title>Significance of difference in correlations with event boundaries between original duration estimates and naïve duration estimates</title><p>We hypothesized that duration estimates from our original participants (who had actually heard the story) would be significantly more correlated with the number of event boundaries between two clips than duration estimates from our naïve participants, who had never heard the story. To assess the significance of the difference in correlations, we computed the <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (empirical difference), as well as the upper confidence limits (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and lower confidence limits (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) for the difference between the two correlations. We used the following formulae (<xref ref-type="bibr" rid="bib104">Zou, 2007</xref>; <xref ref-type="bibr" rid="bib79">Poppenk and Norman, 2012</xref>) for two bootstrapped correlation confidence intervals:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The upper (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and lower limits (<inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) for a 95% confidence interval of each group’s correlation were calculated as described above.</p></sec><sec id="s4-3-3"><title>Reliability of duration estimates across participants within and between groups</title><p>We hypothesized that both our original participants and the naïve participants (who had never heard the story) would use consistent strategies to estimate the temporal distance between two clips, but that these strategies would differ across groups. If this is the case, duration estimates should be more reliable across participants within groups than across participants between groups.</p><p>To assess within-group reliability, we correlated each participant’s duration estimates with the mean of the other participants’ estimates. These correlations were then averaged across participants within a group to obtain a mean within-group ISC (inter-subject correlation). The between-group reliability was calculated by correlating each participant’s duration estimates from one group (e.g., the original participants) with the mean duration estimates from the other group (e.g., the naïve participants). These correlations were then also averaged across participants to obtain a mean between-group ISC. Confidence intervals for the mean between-group ISC were calculated by bootstrapping the duration estimates from both groups 10,000 times, each time selecting with replacement a different subset of <italic>n</italic> individuals from our pool of <italic>n</italic> participants. The between-group ISCs were calculated for each bootstrap sample and averaged across participants, resulting in a distribution of 10,000 mean between-group ISCs. Confidence intervals for the within-group ISC were obtained in a similar manner.</p><p>To assess the significance of the difference between the mean within-group ISC and the mean between-group ISC, we compared the empirical difference with a null distribution of differences. Group labels (naïve participants vs. original participants) were scrambled 10,000 times, such that each participant’s duration estimates were randomly assigned to either the naïve group or to the original group. The difference between the mean within-group ISC and the mean between-group ISC was then computed for these two random groups. Using this null distribution of ISC differences, we calculated a p-value based on the number of permutations that yielded a greater difference than the empirical difference.</p><p>Please note that the within-group and between-group correlations could be compared only because the group sizes were identical (17 participants in each) and because the within-group correlations were equally strong for the original and naïve groups (<italic>M</italic>=0.43, <italic>SD</italic>=0.25, 95% CI=[0.37, 0.58] vs. <italic>M</italic>=0.43, <italic>SD</italic>=0.18, 95% CI [0.40, 0.56]). Since the within-group ISCs are comparable, we can infer that the significant difference between the within-group and between-group reliability reflects a difference in the signals (strategies) underlying the two groups of duration estimates (<xref ref-type="bibr" rid="bib27">Chow et al., 2015</xref>), rather than a difference in within-group reliability.</p></sec></sec><sec id="s4-4"><title>MRI acquisition</title><p>Participants were scanned in a 3T full-body Skyra MRI scanner (Siemens, Munich, Germany) with a 20-channel head coil. Functional images were acquired using a T2*-weighted echo planer imaging (EPI) pulse sequence (repetition time [TR], 1500 ms; echo time [TE], 28 ms; flip angle, 64°), each volume comprising 27 slices of 4 mm thickness. In-plane resolution was 3×3 mm<sup>2</sup> (field of view [FOV], 192×192 mm<sup>2</sup>). Slice acquisition order was interleaved. Anatomical images were acquired using a T1-weighted magnetization-prepared rapid-acquisition gradient echo (MPRAGE) pulse sequence (TR, 2300 ms; TE, 3.08 ms; flip angle 9°; 0.89 mm<sup>3</sup> resolution; FOV, 256 mm<sup>2</sup>). Participants’ heads were stabilized with foam padding to minimize head movement. Auditory stimuli were presented using the Psychophysics toolbox (<xref ref-type="bibr" rid="bib19">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib76">Pelli, 1997</xref>). Participants were provided with MRI compatible in-ear mono earbuds (Model S14, Sensimetrics Corporation, Malden, MA), which provided the same audio input to each ear. MRI-safe passive noise-canceling headphones were placed over the earbuds for additional protection against noise.</p></sec><sec id="s4-5"><title>fMRI data preprocessing</title><p>FMRI data processing was carried out using FEAT (FMRI Expert Analysis Tool) Version 5.98, part of FSL (FMRIB's Software Library, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">www.fmrib.ox.ac.uk/fsl</ext-link>). The following procedure was applied: motion correction using MCFLIRT (<xref ref-type="bibr" rid="bib61">Jenkinson et al., 2002</xref>); slice-timing correction using Fourier-space time-series phase-shifting; non-brain removal using BET (<xref ref-type="bibr" rid="bib89">Smith, 2002</xref>); spatial smoothing using a Gaussian kernel of FWHM 6.0 mm; grand-mean intensity normalization of the entire 4D dataset by a single multiplicative factor; and high-pass temporal filtering (Gaussian-weighted least-squares straight line fitting, with sigma=240.0 s). The procedure for selecting the high-pass filter is described below. Preprocessed data were kept in the native functional space for all analyses, except for the within-interval searchlight analysis, which was performed across participants.</p><p>Preprocessed data were then despiked using the following procedure: for each voxel, data points that deviated from the mean by more than 5 times the inter-quartile range were removed and replaced using cubic interpolation.</p><sec id="s4-5-1"><title>Procedure for obtaining anatomical masks: FreeSurfer and MTL segmentation</title><p>Segmentation was performed in a semi-automated fashion using the FreeSurfer image analysis suite, which is documented and available online (version 5.1; <ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</ext-link>) with details described previously (e.g. <xref ref-type="bibr" rid="bib43">Fischl et al., 2004</xref>; <xref ref-type="bibr" rid="bib80">Poppenk and Norman, 2014</xref>). Briefly, this processing includes removal of non-brain tissue using a hybrid watershed/surface deformation procedure (<xref ref-type="bibr" rid="bib91">Ségonne et al., 2004</xref>), automated Talairach transformation, intensity normalization (<xref ref-type="bibr" rid="bib88">Sled et al., 1998</xref>), tessellation of the grey matter / white matter boundary, automated topology correction (<xref ref-type="bibr" rid="bib42">Fischl et al., 2001</xref>; <xref ref-type="bibr" rid="bib92">Segonne et al., 2007</xref>), surface deformation following intensity gradients (<xref ref-type="bibr" rid="bib41">Fischl and Dale, 2000</xref>), parcellation of cortex into units based on gyral and sulcal structure (<xref ref-type="bibr" rid="bib32">Desikan et al., 2006</xref>; <xref ref-type="bibr" rid="bib43">Fischl et al., 2004</xref>), and creation of a variety of surface-based data, including maps of curvature and sulcal depth.</p><p>We resampled and aligned FreeSurfer segmentations of all grey matter, white matter, and cerebrospinal fluid (CSF) regions to native functional image space for use as anatomical masks. Anatomical regions were segmented according to the Desikan-Killiany Atlas (<xref ref-type="bibr" rid="bib32">Desikan et al., 2006</xref>).</p><p>It is important to note that the medial temporal lobe (MTL) masks in the Desikan-Killiany Atlas do not match the canonical anatomical distinctions in the literature. For example, the parahippocampal gyrus mask comprises the medial part of the parahippocampal cortex and the posterior part of the entorhinal cortex. Therefore, instead of the FreeSurfer MTL masks, we used a probabilistic MTL atlas developed by <xref ref-type="bibr" rid="bib54">Hindy and Turk-Browne (2015)</xref>. MTL regions, including perirhinal cortex, entorhinal cortex and parahippocampal cortex were defined probabilistically in MNI space, based on a database of manual MTL segmentations from a separate set of 24 participants. Manual segmentations were created on <italic>T</italic><sub>2</sub>-weighted turbo spin-echo images using anatomical landmarks (<xref ref-type="bibr" rid="bib35">Duvernoy, 2005</xref>; <xref ref-type="bibr" rid="bib23">Carr et al., 2010</xref>; <xref ref-type="bibr" rid="bib84">Schapiro et al., 2012</xref>) and then registered to an MNI template. Finally, nonlinear registration (FNIRT; <xref ref-type="bibr" rid="bib1">Andersson et al., 2007</xref>) was used to register the masks from MNI space to each participant's native space. After registration, voxels with a probability greater than 0.3 of being in a region were assigned to that ROI.</p></sec><sec id="s4-5-2"><title>Residualization of non-neuronal signal sources</title><p>Slow changes of respiration over time (RV) have been shown to induce robust changes in the BOLD signal (<xref ref-type="bibr" rid="bib24">Chang et al., 2009</xref>) in many areas around the cerebral midline. To minimize signal change unrelated to neural activity, we used multiple linear regression to project out 3 nuisance variables from the BOLD data (<xref ref-type="bibr" rid="bib6">Behzadi et al., 2007</xref>; <xref ref-type="bibr" rid="bib87">Silbert et al., 2014</xref>). Nuisance regressors were:</p><list list-type="order"><list-item><p>the average time course of high standard deviation voxels (voxels with the top 1% largest standard deviation), as these voxels tend to have the highest fractional variance of physiological noise (e.g., cardiac and respiratory components) and are likely near blood vessels (<xref ref-type="bibr" rid="bib6">Behzadi et al., 2007</xref>),</p></list-item><list-item><p>the average BOLD signal measured in CSF,</p></list-item><list-item><p>the average white matter signal.</p></list-item></list><p>All masks (grey matter, white matter and CSF) were obtained from the FreeSurfer segmentation procedure described above. The beneficial effects of this residualization procedure on the signal-to-noise ratio are shown in <xref ref-type="fig" rid="fig13">Figure 13</xref>. Note that this procedure was always applied after removal of low-frequency components using the high-pass filter (see below).<fig id="fig13" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.027</object-id><label>Figure 13.</label><caption><title>Mean inter-subject correlations (ISCs) for 6 representative brain regions as a function of the high-pass filter cut-off.</title><p>Shaded error bars represent standard errors of the mean (across participants). Top panel (<bold>A</bold>) shows the mean ISCs after the residualization procedure has been applied (see <italic>Residualization of non-neuronal signal sources</italic>). The 480 s cut-off was the gentlest filter for which all of the grey matter regions listed above showed ISC values significantly above those in the CSF. Bottom panel (<bold>B</bold>) shows the mean ISCs prior to the residualization procedure. Without residualization, the ISCs of some grey matter regions never rise significantly above those in the white matter and CSF. Note that without high-pass filtering ('none') or residualization, all brain regions displayed spuriously high ISCs.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.027">http://dx.doi.org/10.7554/eLife.16070.027</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-16070-fig13-v2"/></fig></p></sec><sec id="s4-5-3"><title>Methodological challenges with analyzing pattern distance over long time scales: Selection of temporal high-pass filter cut-off</title><p>Because we were interested in the aspect of neural activity that changes slowly over time (reflecting gradual changes in context), we could not use a standard high-pass filter (with a cut-off period on the order of 120 s), as it would remove components of the signal that evolve on the scale of minutes. Thus, we were faced with the challenge of preserving slower components of the BOLD signal that reflect neural activity, while removing low-frequency components attributable to non-neuronal noise, including scanner drift and physiological noise (such as low-frequency respiratory variation and heart rate variation). Physiological noise (and a substantial component of scanner noise) was factored out using the residualization procedure described above. This enabled us to select a gentler high-pass filter than is generally used in the literature.</p><p>We then performed a separate analysis to determine the optimal high-pass filter cut-off period, i.e. the lowest frequency cut-off that still enabled us to remove most of the non-neuronal noise. This analysis relies on the idea that, when participants listen to the same story or watch the same film, the signal in brain regions processing the story is highly correlated across participants (<xref ref-type="bibr" rid="bib48">Hasson et al., 2004</xref>). While such correlations should not be present in CSF or white matter, spurious inter-subject correlations in these regions can arise due to low-frequency noise. In addition, listening to the same story could induce correlated motion across participants, but these correlations would also be present in CSF and white matter. Thus, we searched for a high-pass filter that could remove nonspecific correlations in CSF and white matter, while preserving correlations in brain regions known to be important for processing the stimulus. For each participant, the inter-subject correlation (ISC) of a brain region was defined as the correlation between that participant’s ROI time course (averaged over voxels in that region) with the average time course of all the other participants (<xref ref-type="bibr" rid="bib50">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="bib63">Lerner et al., 2011</xref>).</p><p>Since the functional scan length was 1560 s (26 min), high-pass filter cut-off periods of 140 s, 240 s, 300 s, 400 s, 480 s, 600 s and 720 s were attempted. The minimal cut-off attempted, 140 s, was the cut-off used in several previous studies with naturalistic stimuli (e.g. <xref ref-type="bibr" rid="bib63">Lerner et al., 2011</xref>), while 720 s represented approximately half of the scan duration and was the longest cut-off that could reasonably make a difference to data quality.</p><p>Given that roughly half the clip pairs in our time perception test were 2 min apart and the other half were 6 min apart, we hoped to find a filter that would allow us to measure pattern distances at both of these time scales. However, we were unable to find a high-pass filter that would allow us to examine activity patterns that were 6 min (360 s) apart. In order to meaningfully measure distances between neural patterns that are 360 s apart, the Nyquist theorem suggests we would need a high-pass filter cut-off of 720 s or larger. However, plotting ISC as a function of high-pass filter (<xref ref-type="fig" rid="fig13">Figure 13</xref>) showed that a cut-off like 720 s was not able to remove inter-subject correlations in the CSF, which remained of the same magnitude as those in some grey matter regions. We concluded that pattern distances at the 6-minute time scale are too confounded with low-frequency noise (as reflected in spurious correlations in the CSF), and therefore restricted our analysis to intervals that were 2 min long.</p><p>According to the Nyquist theorem, we need a filter cut-off of 4 min (240 s) or longer in order to measure distances between patterns that are 2 min apart (120 s). Out of the filters tested (240 s – 720 s), a cut-off of 480 s was selected to be the gentlest (i.e. the longest) filter that reduced the magnitude of inter-subject correlations in ventricles and CSF, such that they were significantly below the correlations in most grey matter regions.</p><p><xref ref-type="fig" rid="fig13">Figure 13</xref> illustrates that, even for regions like the hippocampus – with relatively low inter-subject correlations – the 480 s filter cut-off, combined with the residualization procedure, succeeded at raising the grey matter ISCs significantly above those of the white matter and CSF.</p></sec></sec><sec id="s4-6"><title>fMRI data analysis</title><sec id="s4-6-1"><title>Within-participant correlation between pattern change and duration estimates</title><p>Our primary hypothesis was that greater pattern dissimilarity between two clips (at the time of encoding) would correlate with greater subsequent duration estimates. For each pair of clips from the time perception test, we located the TRs (volumes) corresponding to when the participant first heard those clips and extracted the activity patterns for each ROI at those time points. Since the auditory clips were between 5 s and 10 s in duration (corresponding to about 5 volumes), we averaged the patterns over 5 consecutive TRs for every clip, with the 5-TR window centered on the middle of each clip.</p><p>We then related the pattern distance between the two clips at encoding to how much time the participant thought passed between them. Specifically, we calculated the dissimilarity (1 – Pearson correlation) between the two averaged activity patterns. The pattern dissimilarity scores for a given region were then correlated with that participant’s subsequent duration estimates. This was performed separately for every ROI and searchlight (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We thus obtained a Pearson correlation score for every ROI in every participant. All Pearson correlation coefficients were Fisher-transformed prior to statistical testing (<xref ref-type="bibr" rid="bib44">Fisher, 1915</xref>).</p><p>To assess the reliability of the correlation across participants for a given ROI, we ran a phase-randomization procedure, which is described in detail below. The results of the phase-randomization procedure were then subjected to multiple comparisons correction.</p></sec><sec id="s4-6-2"><title>Removing low-confidence intervals</title><p>If a participant could not remember when in the story a particular clip had occurred, it would be difficult for them to estimate the temporal distance between that clip and another clip. It is possible that participants would invoke different retrieval strategies in such cases (for instance, they might base their duration estimates purely on the content of the clips, without recollecting their context). It is also possible that such estimates could be random guesses. To filter out guesses, we used the confidence ratings collected after the time perception test, in which participants rated how well they could remember when in the story each individual clip had occurred. Specifically, we located the participant’s confidence for the two clips delimiting each temporal interval, and took the smaller of the two ratings as the confidence for that interval. We performed the main analysis relating neural drift to time estimation only on high-confidence intervals, removing pairs of clips with the lowest confidence. Since participants calibrated their confidence ratings differently (some were more prone to rate their confidence as 4/5, while others were more prone to rate it as 2/5), we picked the confidence threshold for each participant that removed at least 33% of the intervals with the lowest confidence, while preserving at least 33% of the intervals with the highest confidence. Our behavioral analysis (see <italic>Behavioral results</italic>) shows that participants’ duration estimates were significantly more accurate for high-confidence intervals than when all intervals were included.</p></sec><sec id="s4-6-3"><title>Statistical analysis of correlations between pattern change and behavior</title><p>Because of the presence of long-range temporal autocorrelation in the BOLD signal (<xref ref-type="bibr" rid="bib103">Zarahn, 1997</xref>), the statistical likelihood of each observed correlation (between neural distance and duration estimates) was assessed using a permutation procedure based on surrogate data. The surrogate data were generated using phase randomization (<xref ref-type="bibr" rid="bib93">Theiler et al., 1992</xref>). Phase-randomized surrogates have the same autocorrelation as the original signal.</p><p>Since our analysis measures pattern change over multiple voxels, rather than the time course of a single voxel, we generated surrogate time courses of pattern change (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows how that time course was obtained). Having extracted the time course of pattern change for each ROI, we applied a Fourier transform to that signal. To randomize its phases, we multiplied each complex amplitude by <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>ϕ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is independently chosen for each frequency from the interval [0, 2π]. In order for the inverse Fourier transform to be real (no imaginary components), we symmetrized the phases, so that <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, we took the inverse Fourier transform to produce the surrogate time courses.</p><p>Each surrogate dataset was analyzed in the same manner as the empirical data: pattern dissimilarity between each pair of clips was correlated with duration estimates. Thus, we generated a distribution of 10,000 null correlations for every ROI in every participant (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). As above, all correlation coefficients were Fisher-transformed to ensure that they follow a Gaussian distribution. For every ROI, we were then able to compare the empirical Pearson correlation with the distribution of null correlations. We calculated a Z-value for every participant:<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A large positive Z-value implies that the empirical correlation is large relative to the distribution of null correlations. To assess whether the Z-values for a given ROI were reliably positive across participants, we performed a right-tailed t-test against 0. The p-values from the above t-test were then subjected to multiple comparisons correction. For anatomical ROIs (derived from the FreeSurfer and MTL atlases), we used MATLAB’s fdr_bky.m function, which executes the 'two-stage' <xref ref-type="bibr" rid="bib8">Benjamini et al. (2006)</xref> procedure for controlling the false discovery rate (FDR) of a family of hypothesis tests. The procedure implemented by this function is more powerful than the original <xref ref-type="bibr" rid="bib7">Benjamini and Hochberg (1995)</xref> procedure when a considerable percentage of the hypotheses in the family are false. For the searchlight analysis, we controlled the family-wise error (FWE) rate, as described below.</p></sec><sec id="s4-6-4"><title>ROI selection</title><p>The literature reviewed above suggests that the MTL, lateral prefrontal cortex, insula, putamen and inferior parietal cortex might all process information important for inferring the duration of past events. We therefore performed an ROI analysis on the following regions, derived from both the FreeSurfer and MTL atlases: hippocampus, parahippocampal cortex, entorhinal cortex, perirhinal cortex, amygdala, superior frontal cortex, caudal and rostral middle frontal gyrus (dorsolateral prefrontal cortex), pars opercularis (frontal operculum), pars triangularis, pars orbitalis, lateral orbitofrontal cortex, frontal pole, insula, putamen and inferior parietal cortex. This resulted in an analysis on 16 regions of interest (in each hemisphere) motivated by the literature. ROIs with q-values &lt; 0.05 (FDR) are reported as significant.</p><p>As part of an exploratory, whole-brain search, we also ran the same analysis on all grey matter regions in the Desikan-Killiany Atlas, which contained 42 regions in each hemisphere, including the ones mentioned above (see <italic>Procedure for obtaining anatomical masks: FreeSurfer and MTL segmentation</italic>). The complete list of regions can be found in <xref ref-type="supplementary-material" rid="SD4-data">Figure 5—source data 1</xref>. For the exploratory analysis, we report regions with q-values &lt; 0.1 (FDR).</p></sec><sec id="s4-6-5"><title>Within-interval correlation between pattern change and duration estimates</title><p>Our main analysis verified whether the pattern distance between two clips was correlated with duration estimates in a given participant and then aggregated the results across participants. To address the concern that pattern distance between two clips might reflect only the difference in story content between those clips (rather than change in abstract factors like mental context), we performed the same analysis for a given interval across participants and aggregated the results across intervals. Since this analysis is performed within intervals, it ensures that story content is held constant across participants, such that differences in pattern distances and duration estimates are due to individual differences only. To ensure that pattern distances and duration estimates were comparable across participants, all vectors were z-scored within participants. The Pearson correlation between pattern distances and duration estimates across participants was then calculated for every 2 min interval in every ROI.</p><p>As for the within-participant analysis, this procedure was performed on high-confidence intervals. For each interval, we only included participants who had confidently recollected the temporal position of the two clips delimiting that particular interval.</p><p>The significance of each correlation score was assessed using a permutation test: 10,000 null correlations were obtained by scrambling the duration estimates across participants, such that a given participant’s duration estimate was matched with a different participant’s pattern distance. (Since this analysis was performed across participants, it was not necessary to generate phase-randomized pattern distance time courses – the auto-correlation in the BOLD signal for a given participant only represents a concern for the within-participant analysis.)</p><p>As above, a Z-value was obtained for every interval, reflecting the degree to which the empirical correlation was higher than the distribution of null correlations. Finally, a right-tailed t-test was performed to assess whether a given ROI’s Z-values were reliably positive across intervals. The p-values from this t-test were subjected to multiple comparisons correction using FDR.</p><p>To compare effect sizes between the within-interval and within-participants analyses, we calculated Cohen’s <italic>d</italic> for a region as:<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the Pearson’s correlation between pattern distance and duration estimates. (Using the Z-values derived from the permutation procedures rather than the raw correlation coefficients yielded practically identical results.)</p></sec><sec id="s4-6-6"><title>Mixed-effects model accounting for naïve duration estimates</title><p>We analyzed our data using a hierarchical linear regression model (<xref ref-type="bibr" rid="bib45">Gelman and Hill, 2006</xref>). Known in different fields as hierarchical, mixed, or multi-level models, such regressions correctly account for non-independence of repeated observations of the same subject and stimulus (in our case, interval). In doing this, they estimate the population effects (coefficients) of interest, even assuming that individual subjects or items (henceforth, collectively 'groups') may have idiosyncratic perturbations from the population and that those perturbations may be correlated within a group. They are a generalization of approaches that treat all observations as independent (e.g. t-test, ANOVA, linear regression), as well as of approaches that can take into account the non-independence across a single grouping factor (e.g. repeated-measures ANOVA), and are more conservative than any of the above (<xref ref-type="bibr" rid="bib2">Barr et al., 2013</xref>). (More precisely, methods that do assume observation independence are anti-conservative in the presence of correlated observations.)</p><p>Formally, the model is the following:<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <italic>i</italic>th observed duration judgment, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix of predictors (neural pattern distance) and covariates (naïve duration estimates), <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector of coefficients (as in conventional linear regression), <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the subject of the <italic>i</italic>th observation, so that <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a subject-specific perturbation of all of the coefficients, and <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is similarly an item-specific perturbation of the coefficients.</p><p>This model is undefined when either the subject or item effects approach zero (either because there is truly no variability, or more realistically when there is insufficient data to estimate this variability). Since such rich models often fail to converge or approach singularity given typical psychological datasets (<xref ref-type="bibr" rid="bib3">Bates et al., 2015a</xref>), we imposed a weak Wishart prior on the group covariances, which regularizes the model away from singularity (<xref ref-type="bibr" rid="bib28">Chung et al., 2015</xref>). This weak, boundary-avoiding prior on our random effects covariance structure regularizes the model towards simpler random effects structures unless the data suggests otherwise (<xref ref-type="bibr" rid="bib28">Chung et al., 2015</xref>). All models converged under this prior. This fitting procedure was implemented using the R package blme (<xref ref-type="bibr" rid="bib29">Chung et al., 2013</xref>), which extends the lme4 package (<xref ref-type="bibr" rid="bib4">Bates et al., 2015b</xref>) and performs maximum-a-posteriori estimation of linear mixed-effects models.</p><p>Please note that we also verified that our results were replicable using an alternative fitting procedure suggested by <xref ref-type="bibr" rid="bib3">Bates et al. (2015a)</xref>. We used the lme4 package to fit the ‘maximal’ model (in the sense of <xref ref-type="bibr" rid="bib2">Barr et al., 2013</xref>) and removed zero-variance random effects terms until the model converged and until the estimated random effects covariance matrix was full-rank, indicating a non-degenerate estimate. We obtained highly consistent results using both fitting procedures. In the <italic>Results</italic> section, we report only the first procedure, which has been found to be more conservative (<xref ref-type="bibr" rid="bib28">Chung et al., 2015</xref>). <xref ref-type="bibr" rid="bib28">Chung et al. (2015)</xref> report: 'Uncertainty for the fixed coefficients is less underestimated than under classical ML or restricted maximum likelihood estimation.' Indeed, our effects were very slightly stronger using the second procedure (<xref ref-type="bibr" rid="bib3">Bates et al., 2015a</xref>). Both sets of results can be found in <xref ref-type="supplementary-material" rid="SD6-data">Figure 7—source data 1</xref>.</p><p>Finally, the duration estimates are bounded at zero and positively skewed, which resulted in heteroskedastic residuals. To mitigate this, we power-transformed the duration estimates using the Box-Cox power transformation (<xref ref-type="bibr" rid="bib18">Box and Cox, 1964</xref>). We picked the exponent <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for each model by maximizing the profile likelihood in a model without group effects (though see e.g. <xref ref-type="bibr" rid="bib46">Gurka et al. (2006)</xref> for an extension to the hierarchical case).</p><p>In R formula notation, a model of the following form was fit to the data from each region of interest:<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>E</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>∣</mml:mo><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>∣</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Please note that participants from the original experiment could not be 'matched' with participants from the naïve experiment. For this reason, naïve duration estimates were group-averaged and the mean vector of naïve estimates was placed as a covariate in the model. The above formula shows that the slope of the relationship between naïve estimates and original duration estimates was allowed to vary by subject (i.e. each participant’s duration estimates might be differently related to the naïve group mean). On the other hand, the slope for naïve estimates could not vary by interval, since naïve estimates did not vary by subject.</p><p>We computed 0.95 confidence intervals of <italic>β</italic> using the asymptotic Gaussian approximation (called the 'Wald approximation' in lme4) based on the estimated local curvature of the likelihood surface. Since this approximation is anti-conservative (it assumes infinite data and no model misspecification), we then computed a more conservative parametric bootstrap interval for the intervals that did not include zero. Effects whose interval does not overlap with 0 are significant at the conventional <italic>α</italic>=0.05 level.</p><p>Note that all of the above choices (including the choice of fitting procedure and the power transform of the data) are conservative relative to their alternatives. For instance, prior to power-transforming the duration estimates, the fixed effects of neural pattern distance were estimated to be stronger (as reported in <xref ref-type="supplementary-material" rid="SD6-data">Figure 7—source data 1</xref>.) These alternative analyses revealed additional significant regions that are either false positives or regions we lack the power to detect.</p></sec><sec id="s4-6-7"><title>Whole-brain searchlights</title><p>In addition to using anatomical ROIs, we ran a cubic searchlight throughout the entire brain. The same analysis as described above was performed for every searchlight, and the Z-value for each searchlight was assigned to the center voxel.</p><p>The within-participant analysis was performed in native functional space, and each cubic searchlight contained 3x3x3 (27) voxels. To aggregate the results across participants, each participant’s Z-value map was transformed to standard MNI space and down-sampled to 3 mm to reflect the resolution of the original data.</p><p>The within-interval analysis was performed in 3 mm MNI space, in order to match the searchlights across participants. Since this transformation approximately doubles the number of brain voxels, we ran cubic searchlights of radius 2 with 5x5x5 (125) voxels through the entire brain. Neural pattern distance was not calculated for searchlights on the very edge of the brain with fewer than 25 voxels, in order to reduce noise from overly small patterns. We also excluded a searchlight location if fewer than 5 participants had brain voxels in that location.</p><p>Family-wise error rate was controlled using FSL’s <italic>randomise</italic> function (version 5.0.4, <xref ref-type="bibr" rid="bib97">Winkler et al., 2014</xref>). An uncorrected p-value image was first generated, reflecting voxel-wise (searchlight) reliability across participants or intervals. The significance of supra-threshold clusters (defined by the cluster-forming threshold, p&lt;0.01) was then assessed by cluster mass. Specifically, a corrected p-value was assigned to each cluster by assessing its cluster mass with respect to the null distribution of the maximum cluster mass during 10,000 permutation simulations (<xref ref-type="bibr" rid="bib51">Hayasaka and Nichols, 2003</xref>; <xref ref-type="bibr" rid="bib73">Nichols and Holmes, 2002</xref>). Cluster coordinates are reported in MNI space, and cluster size reflects the number of voxels in 3x3x3mm MNI space.</p></sec><sec id="s4-6-8"><title>Comparing speed of pattern change across brain regions</title><p>If the brain regions that showed significant effects in our main analysis represent mental context, then the pattern of activity in these regions should change more slowly over time than the patterns in regions representing sensory information. To quantify the speed of pattern change in a given ROI, we obtained the correlation of the pattern at every time point (TR) with itself at every other time point. (As for our main analysis, the BOLD time course of every voxel was smoothed using a moving average filter of 5 TRs. This temporal smoothing was used as a de-noising technique and did not affect the results.) We then averaged the auto-correlation curves across TRs to obtain a mean auto-correlation function for every region in every participant. The more rapidly a pattern changes over time, the more sharply the auto-correlation should decrease as we move away from 0. To quantify this, we defined the Full-Width Half-Maximum (FWHM) of the auto-correlation curve as the number of time points (TRs) for which the auto-correlation was equal to or greater than half its maximum value (the maximum was always 1.)</p><p>To compare the speed of pattern change in the regions we found (right entorhinal cortex and left caudal ACC) with regions involved in auditory and language processing, we performed a paired Wilcoxon signed rank test on the FWHM values across participants. The p-values from this test were subjected to multiple comparisons correction using FDR.</p><p>Since the anatomical masks we used varied substantially in size, we sought to ensure that differences in the speed of pattern change were not due to differences in ROI size. For this purpose, we performed the same analysis after regressing the vector of ROI sizes out of the vector of FWHM values for every participant.</p><p>Since the above regression would only account for a linear effect of ROI size on the speed of pattern change, we additionally performed a univariate analysis that calculated the auto-correlation function for each voxel individually. The auto-correlation curve was obtained by correlating the BOLD time course of every voxel with itself at all possible lags. The mean auto-correlation for an ROI was obtained by averaging the auto-correlation curves across all the voxels in that ROI. The FWHM values were then calculated in the same manner as above for every ROI in every participant.</p></sec><sec id="s4-6-9"><title>Replication of <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref> 'coarse temporal memory' fMRI analysis</title><p>As in <xref ref-type="bibr" rid="bib59">Jenkins and Ranganath (2010)</xref>, we correlated each voxel’s activity during encoding of a clip with the accuracy of a participant’s placement of that clip on the timeline. Voxel activity was averaged over a 5-TR window centered on the mid-point of the clip. For each participant, the estimated clip position on the timeline was regressed against actual position. Accuracy was defined as the negative error, which was the absolute value of the residual for a clip. Within participants, voxel activity was then correlated with accuracy across all clips, and the Pearson’s <italic>r</italic> score was Fisher-transformed. As for the within-participant searchlight analysis, transformed <italic>r</italic> score maps were registered to 3mm MNI space, and FSL’s <italic>randomise</italic> was used to control the FWE rate.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Lucy Lin for her assistance with data collection for the event boundary experiment. We would like to thank Erez Simony, Lili Sahakyan, Mariam Aly, Anna Schapiro and Michael Chow for their advice on data analysis and preprocessing, as well as helpful discussion.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>OL, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>JC, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>DT, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>CJH, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con5"><p>MS, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con6"><p>JLP, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con7"><p>UH, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con8"><p>KAN, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All parts of the experimental procedure were approved by the Princeton Institutional Review Board under Protocol #5516. All participants were screened to ensure no neurological or psychiatric disorders. Written informed consent, and consent to publish, was obtained for all participants in accordance with the Princeton Institutional Review Board regulations.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><sec id="s7" sec-type="datasets"><title>Major datasets</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="data-ro1" source-id="http://dataspace.princeton.edu/jspui/handle/88435/dsp011n79h6771" source-id-type="uri"><collab>Lositsky O</collab><x>,</x> <collab>Chen J</collab><x>,</x> <collab>Toker D</collab><x>,</x> <collab>Honey CJ</collab><x>,</x> <collab>Hasson U</collab><x>,</x> <collab>Norman KA</collab><x>,</x> <year>2016</year><x>,</x><source>Neural pattern change during encoding of a narrative predicts retrospective duration estimates</source><x>,</x> <ext-link ext-link-type="uri" xlink:href="http://dataspace.princeton.edu/jspui/handle/88435/dsp011n79h6771">http://dataspace.princeton.edu/jspui/handle/88435/dsp011n79h6771</ext-link><x>,</x> <comment>Publicly available at the Princeton dataspace</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title><italic>Non-linear registration aka Spatial normalisation FMRIB Technial Report TR07JA2</italic></article-title><source>In Practice</source><ext-link ext-link-type="uri" xlink:href="http://fmrib.medsci.ox.ac.uk/analysis/techrep/tr07ja2/tr07ja2.pdf">http://fmrib.medsci.ox.ac.uk/analysis/techrep/tr07ja2/tr07ja2.pdf</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>DJ</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Scheepers</surname><given-names>C</given-names></name><name><surname>Tily</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title><source>Journal of Memory and Language</source><volume>68</volume><fpage>255</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2012.11.001</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>DM</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Vasishth</surname><given-names>S</given-names></name><name><surname>Baayen</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Parsimonious mixed models</article-title><source>arXiv Preprint arXiv:1506.04967</source></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>DM</given-names></name><name><surname>Mächler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>BM</given-names></name><name><surname>Walker</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: A oractical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B (Methodological)</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Krieger</surname><given-names>AM</given-names></name><name><surname>Yekutieli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Adaptive linear step-up procedures that control the false discovery rate</article-title><source>Biometrika</source><volume>93</volume><fpage>491</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1093/biomet/93.3.491</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Frost</surname><given-names>JA</given-names></name><name><surname>Hammeke</surname><given-names>TA</given-names></name><name><surname>Bellgowan</surname><given-names>PS</given-names></name><name><surname>Springer</surname><given-names>JA</given-names></name><name><surname>Kaufman</surname><given-names>JN</given-names></name><name><surname>Possing</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>512</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.5.512</pub-id><pub-id pub-id-type="pmid">10847601</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name><name><surname>Reed</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Remembered duration: Evidence for a contextual-change hypothesis</article-title><source>Journal of Experimental Psychology: Human Learning &amp; Memory</source><volume>4</volume><fpage>656</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.4.6.656</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name><name><surname>Zakay</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><chapter-title>Timing and remembering the past, the present, and the future</chapter-title><source>Psychology of Time</source><pub-id pub-id-type="doi">10.1016/B978-0-08046-977-5.00012-0</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Temporal judgments and contextual change</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>8</volume><fpage>530</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.8.6.530</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1985">1985</year><chapter-title>Contextual coding in memory: Studies of remembered duration</chapter-title><source>Time, Mind, and Behavior</source><fpage>169</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-70491-8_11</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Remembered duration: imagery processes and contextual encoding</article-title><source>Acta Psychologica</source><volume>62</volume><fpage>103</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(86)90063-6</pub-id><pub-id pub-id-type="pmid">3766191</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1990">1990</year><chapter-title>Models of psychological time</chapter-title><source>Cognitive Models of Psychological Time</source><fpage>1</fpage><lpage>35</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Block</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1992">1992</year><chapter-title>Prospective and retrospective duration judgment: The role of information processing and memory</chapter-title><person-group person-group-type="editor"><name><surname>Macar</surname> <given-names>F</given-names></name><name><surname>Pouthas</surname> <given-names>V</given-names></name><name><surname>Friedman</surname> <given-names>W. J</given-names></name></person-group><source>Time, Actions and Cognition: Towards Bridging the Gap</source><publisher-loc>Dordrecht, The Netherlands</publisher-loc><publisher-name>Kluwer Academic</publisher-name><fpage>141</fpage><lpage>152</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bower</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>Stimulus-sampling theory of encoding variability</chapter-title><person-group person-group-type="editor"><name><surname>Melton</surname> <given-names>AW</given-names></name><name><surname>Martin</surname> <given-names>E</given-names></name></person-group><source>Coding Processes in Human Memory</source><publisher-loc>Washington, DC</publisher-loc><publisher-name>V. H. Winston</publisher-name><fpage>85</fpage><lpage>123</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Box</surname><given-names>GEP</given-names></name><name><surname>Cox</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>An analysis of transformations</article-title><source>Journal of the Royal Statistical Society. Series B (Methodological</source><volume>26</volume><fpage>211</fpage><lpage>252</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>SW</given-names></name><name><surname>Stubbs</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The psychophysics of retrospective and prospective timing</article-title><source>Perception</source><volume>17</volume><fpage>297</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1068/p170297</pub-id><pub-id pub-id-type="pmid">3226871</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bryden</surname><given-names>DW</given-names></name><name><surname>Johnson</surname><given-names>EE</given-names></name><name><surname>Tobia</surname><given-names>SC</given-names></name><name><surname>Kashtelyan</surname><given-names>V</given-names></name><name><surname>Roesch</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attention for learning signals in anterior cingulate cortex</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>18266</fpage><lpage>18274</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4715-11.2011</pub-id><pub-id pub-id-type="pmid">22171031</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckmaster</surname><given-names>CA</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name><name><surname>Amaral</surname><given-names>DG</given-names></name><name><surname>Suzuki</surname><given-names>WA</given-names></name><name><surname>Rapp</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Entorhinal cortex lesions disrupt the relational organization of memory in monkeys</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>9811</fpage><lpage>9825</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1532-04.2004</pub-id><pub-id pub-id-type="pmid">15525766</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname><given-names>VA</given-names></name><name><surname>Rissman</surname><given-names>J</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Imaging the human medial temporal lobe with high-resolution fMRI</article-title><source>Neuron</source><volume>65</volume><fpage>298</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.12.022</pub-id><pub-id pub-id-type="pmid">20159444</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Influence of heart rate on the BOLD signal: the cardiac response function</article-title><source>NeuroImage</source><volume>44</volume><fpage>857</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.029</pub-id><pub-id pub-id-type="pmid">18951982</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Simony</surname><given-names>E</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Accessing real-life episodic information from minutes versus hours earlier modulates Hippocampal and high-order cortical dynamics</article-title><source>Cerebral Cortex </source><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv155</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shared experience, shared memory: a common structure for brain activity during naturalistic recall</article-title><source>bioRxiv</source><ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/content/early/2016/01/05/035931.abstract">http://biorxiv.org/content/early/2016/01/05/035931.abstract</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chow</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Latent variable modeling of temporal profiles of neural activity during the processing of continuous natural stimuli</chapter-title><source>Society for Neuroscience Annual Meeting</source><publisher-name>Chicago, IL</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>Y</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rabe-Hesketh</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Dorie</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Weakly informative prior for point estimation of covariance matrices in hierarchical models</article-title><source>Journal of Educational and Behavioral Statistics</source><volume>40</volume><fpage>136</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.3102/1076998615570945</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>Y</given-names></name><name><surname>Rabe-Hesketh</surname><given-names>S</given-names></name><name><surname>Dorie</surname><given-names>V</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A nondegenerate penalized likelihood estimator for variance parameters in multilevel models</article-title><source>Psychometrika</source><volume>78</volume><fpage>685</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1007/s11336-013-9328-2</pub-id><pub-id pub-id-type="pmid">24092484</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coull</surname><given-names>JT</given-names></name><name><surname>Vidal</surname><given-names>F</given-names></name><name><surname>Nazarian</surname><given-names>B</given-names></name><name><surname>Macar</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Functional anatomy of the attentional modulation of time estimation</article-title><source>Science</source><volume>303</volume><fpage>1506</fpage><lpage>1508</lpage><pub-id pub-id-type="doi">10.1126/science.1091573</pub-id><pub-id pub-id-type="pmid">15001776</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coull</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>fMRI studies of temporal attention: allocating attention within, or towards, time</article-title><source>Brain Research. Cognitive Brain Research</source><volume>21</volume><fpage>216</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.02.011</pub-id><pub-id pub-id-type="pmid">15464353</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><name><surname>Killiany</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname><given-names>C</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>A</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>NeuroImage</source><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id><pub-id pub-id-type="pmid">20547229</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dirnberger</surname><given-names>G</given-names></name><name><surname>Hesselmann</surname><given-names>G</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Preminger</surname><given-names>S</given-names></name><name><surname>Jahanshahi</surname><given-names>M</given-names></name><name><surname>Paz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Give it time: neural evidence for distorted time perception and enhanced memory encoding in emotional situations</article-title><source>NeuroImage</source><volume>63</volume><fpage>591</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.06.041</pub-id><pub-id pub-id-type="pmid">22750720</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Duvernoy</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>The Human Hippocampus: Functional Anatomy, Vascularization and Serial Sections with MRI</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name><name><surname>Yonelinas</surname><given-names>AP</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The medial temporal lobe and recognition memory</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>123</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094328</pub-id><pub-id pub-id-type="pmid">17417939</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ezzyat</surname><given-names>Y</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What constitutes an episode in episodic memory?</article-title><source>Psychological Science</source><volume>22</volume><fpage>243</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1177/0956797610393742</pub-id><pub-id pub-id-type="pmid">21178116</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ezzyat</surname><given-names>Y</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Similarity breeds proximity: pattern similarity within and across contexts is related to later mnemonic judgments of temporal proximity</article-title><source>Neuron</source><volume>81</volume><fpage>1179</fpage><lpage>1189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.042</pub-id><pub-id pub-id-type="pmid">24607235</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faber</surname><given-names>M</given-names></name><name><surname>Gennari</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>In search of lost time: Reconstructing the unfolding of events from memory</article-title><source>Cognition</source><volume>143</volume><fpage>193</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2015.06.014</pub-id><pub-id pub-id-type="pmid">26188683</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferstl</surname><given-names>EC</given-names></name><name><surname>Neumann</surname><given-names>J</given-names></name><name><surname>Bogler</surname><given-names>C</given-names></name><name><surname>von Cramon</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The extended language network: a meta-analysis of neuroimaging studies on text comprehension</article-title><source>Human Brain Mapping</source><volume>29</volume><fpage>581</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1002/hbm.20422</pub-id><pub-id pub-id-type="pmid">17557297</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Measuring the thickness of the human cerebral cortex from magnetic resonance images</article-title><source>PNAS</source><volume>97</volume><fpage>11050</fpage><lpage>11055</lpage><pub-id pub-id-type="doi">10.1073/pnas.200033797</pub-id><pub-id pub-id-type="pmid">10984517</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Automated manifold surgery: constructing geometrically accurate and topologically correct models of the human cerebral cortex</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>70</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1109/42.906426</pub-id><pub-id pub-id-type="pmid">11293693</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>van der Kouwe</surname><given-names>A</given-names></name><name><surname>Destrieux</surname><given-names>C</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Salat</surname><given-names>DH</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Seidman</surname><given-names>LJ</given-names></name><name><surname>Goldstein</surname><given-names>J</given-names></name><name><surname>Kennedy</surname><given-names>D</given-names></name><name><surname>Caviness</surname><given-names>V</given-names></name><name><surname>Makris</surname><given-names>N</given-names></name><name><surname>Rosen</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Automatically parcellating the human cerebral cortex</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg087</pub-id><pub-id pub-id-type="pmid">14654453</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1915">1915</year><article-title>Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population</article-title><source>Biometrika</source><volume>10</volume><elocation-id>507</elocation-id><pub-id pub-id-type="doi">10.2307/2331838</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hill</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Data Analysis Using Regression and Multilevel/Hierarchical Models</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurka</surname><given-names>MJ</given-names></name><name><surname>Edwards</surname><given-names>LJ</given-names></name><name><surname>Muller</surname><given-names>KE</given-names></name><name><surname>Kupper</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Extending the Box-Cox transformation to the linear mixed model</article-title><source>Journal of the Royal Statistical Society: Series A</source><volume>169</volume><fpage>273</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1111/j.1467-985X.2005.00391.x</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical process memory: memory as an integral component of information processing</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>304</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.04.006</pub-id><pub-id pub-id-type="pmid">25980649</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Fuhrmann</surname><given-names>G</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Intersubject synchronization of cortical activity during natural vision</article-title><source>Science</source><volume>303</volume><fpage>1634</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1126/science.1089506</pub-id><pub-id pub-id-type="pmid">15016991</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Brain networks subserving the extraction of sentence information and its encoding to memory</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2899</fpage><lpage>2913</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm016</pub-id><pub-id pub-id-type="pmid">17372276</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Vallines</surname><given-names>I</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of temporal receptive windows in human cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>2539</fpage><lpage>2550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5487-07.2008</pub-id><pub-id pub-id-type="pmid">18322098</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayasaka</surname><given-names>S</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Validating cluster size inference: random field and permutation methods</article-title><source>NeuroImage</source><volume>20</volume><fpage>2343</fpage><lpage>2356</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.08.003</pub-id><pub-id pub-id-type="pmid">14683734</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dorsal and ventral streams: a framework for understanding aspects of the functional anatomy of language</article-title><source>Cognition</source><volume>92</volume><fpage>67</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2003.10.011</pub-id><pub-id pub-id-type="pmid">15037127</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hicks</surname><given-names>RE</given-names></name><name><surname>Miller</surname><given-names>GW</given-names></name><name><surname>Kinsbourne</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Prospective and retrospective judgments of time as a function of amount of information processed</article-title><source>The American Journal of Psychology</source><volume>89</volume><fpage>719</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.2307/1421469</pub-id><pub-id pub-id-type="pmid">1020767</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindy</surname><given-names>NC</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Action-based learning of multistate objects in the medial temporal Lobe</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv030</pub-id><pub-id pub-id-type="pmid">25754517</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The hippocampus, time, and memory across scales</article-title><source>Journal of Experimental Psychology</source><volume>142</volume><fpage>1211</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1037/a0033621</pub-id><pub-id pub-id-type="pmid">23915126</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Fotedar</surname><given-names>MS</given-names></name><name><surname>Datey</surname><given-names>AV</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The temporal context model in spatial navigation and relational learning: toward a common explanation of medial temporal lobe function across domains</article-title><source>Psychological Review</source><volume>112</volume><fpage>75</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.1.75</pub-id><pub-id pub-id-type="pmid">15631589</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A distributed representation of temporal context</article-title><source>Journal of Mathematical Psychology</source><volume>46</volume><fpage>269</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1006/jmps.2001.1388</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>NS</given-names></name><name><surname>Allen</surname><given-names>TA</given-names></name><name><surname>Nguyen</surname><given-names>N</given-names></name><name><surname>Fortin</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Critical role of the hippocampus in memory for elapsed time</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>13888</fpage><lpage>13893</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1733-13.2013</pub-id><pub-id pub-id-type="pmid">23966708</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkins</surname><given-names>LJ</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Prefrontal and medial temporal lobe activity at encoding predicts temporal context memory</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>15558</fpage><lpage>15565</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1337-10.2010</pub-id><pub-id pub-id-type="pmid">21084610</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkins</surname><given-names>LJ</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct neural mechanisms for remembering when an event occurred</article-title><source>Hippocampus</source><volume>26</volume><fpage>554</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1002/hipo.22571</pub-id><pub-id pub-id-type="pmid">26845069</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurby</surname><given-names>CA</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Segmentation in the perception and memory of events</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>72</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.11.004</pub-id><pub-id pub-id-type="pmid">18178125</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname><given-names>Y</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2906</fpage><lpage>2915</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3684-10.2011</pub-id><pub-id pub-id-type="pmid">21414912</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>PA</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Complementary roles of hippocampus and medial entorhinal cortex in episodic memory</article-title><source>Neural Plasticity</source><volume>2008</volume><elocation-id>258467</elocation-id><pub-id pub-id-type="doi">10.1155/2008/258467</pub-id><pub-id pub-id-type="pmid">18615199</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>PA</given-names></name><name><surname>White</surname><given-names>JA</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Disambiguation of overlapping experiences by neurons in the medial entorhinal cortex</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>5787</fpage><lpage>5795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1063-07.2007</pub-id><pub-id pub-id-type="pmid">17522322</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livesey</surname><given-names>AC</given-names></name><name><surname>Wall</surname><given-names>MB</given-names></name><name><surname>Smith</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Time perception: manipulation of task difficulty dissociates clock functions from other cognitive demands</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>321</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.06.033</pub-id><pub-id pub-id-type="pmid">16934301</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>EA</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Morris</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The functional neuroanatomy of comprehension and memory: the importance of prior knowledge</article-title><source>Brain</source><volume>122 (Pt 10)</volume><fpage>1839</fpage><lpage>1850</lpage><pub-id pub-id-type="doi">10.1093/brain/122.10.1839</pub-id><pub-id pub-id-type="pmid">10506087</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Manning</surname><given-names>JR</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>The role of context in episodic memory</chapter-title><person-group person-group-type="editor"><name><surname>Gazzaniga</surname> <given-names>M. S</given-names></name><name><surname>Mangun</surname> <given-names>G. R</given-names></name></person-group><source>The Cognitive Neurosciences</source><publisher-loc>Cambridge</publisher-loc><fpage>557</fpage><lpage>566</lpage><comment>In press</comment></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manns</surname><given-names>JR</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Gradual changes in hippocampal activity support remembering the order of events</article-title><source>Neuron</source><volume>56</volume><fpage>530</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.017</pub-id><pub-id pub-id-type="pmid">17988635</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mar</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neuropsychology of narrative: story comprehension, story production and their interrelation</article-title><source>Neuropsychologia</source><volume>42</volume><fpage>1414</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2003.12.016</pub-id><pub-id pub-id-type="pmid">15193948</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functionally dissociable influences on learning rate in a dynamic environment</article-title><source>Neuron</source><volume>84</volume><fpage>870</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.013</pub-id><pub-id pub-id-type="pmid">25459409</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mensink</surname><given-names>G-J</given-names></name><name><surname>Raaijmakers</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A model for interference and forgetting</article-title><source>Psychological Review</source><volume>95</volume><fpage>434</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.95.4.434</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric permutation tests for functional neuroimaging: a primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noulhiane</surname><given-names>M</given-names></name><name><surname>Pouthas</surname><given-names>V</given-names></name><name><surname>Hasboun</surname><given-names>D</given-names></name><name><surname>Baulac</surname><given-names>M</given-names></name><name><surname>Samson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Role of the medial temporal lobe in time estimation in the range of minutes</article-title><source>Neuroreport</source><volume>18</volume><fpage>1035</fpage><lpage>1038</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e3281668be1</pub-id><pub-id pub-id-type="pmid">17558291</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname><given-names>JX</given-names></name><name><surname>Schüffelgen</surname><given-names>U</given-names></name><name><surname>Cuell</surname><given-names>SF</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Rushworth</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title><source>PNAS</source><volume>110</volume><fpage>E3660</fpage><lpage>3669</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id><pub-id pub-id-type="pmid">23986499</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollatos</surname><given-names>O</given-names></name><name><surname>Laubrock</surname><given-names>J</given-names></name><name><surname>Wittmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interoceptive focus shapes the experience of time</article-title><source>PLoS ONE</source><volume>9</volume><elocation-id>e86934</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0086934</pub-id><pub-id pub-id-type="pmid">24489807</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polyn</surname><given-names>SM</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Memory search and the neural representation of context</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>24</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.10.010</pub-id><pub-id pub-id-type="pmid">18069046</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poppenk</surname><given-names>J</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms supporting superior source memory for familiar items: a multi-voxel pattern analysis study</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>3015</fpage><lpage>3026</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.07.010</pub-id><pub-id pub-id-type="pmid">22820636</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poppenk</surname><given-names>J</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Briefly cuing memories leads to suppression of their neural representations</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>8010</fpage><lpage>8020</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4584-13.2014</pub-id><pub-id pub-id-type="pmid">24899722</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poynter</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Duration judgment and the segmentation of experience</article-title><source>Memory &amp; Cognition</source><volume>11</volume><fpage>77</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.3758/BF03197664</pub-id><pub-id pub-id-type="pmid">6855562</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Ritchey</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Two cortical systems for memory-guided behaviour</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/nrn3338</pub-id><pub-id pub-id-type="pmid">22992647</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahakyan</surname><given-names>L</given-names></name><name><surname>Smith</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>“A long time ago, in a context far, far away”: Retrospective time estimates and internal context change</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>40</volume><fpage>86</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1037/a0034250</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Kustner</surname><given-names>LV</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Shaping of object representations in the human medial temporal lobe based on temporal regularities</article-title><source>Current Biology</source><volume>22</volume><fpage>1622</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.06.056</pub-id><pub-id pub-id-type="pmid">22885059</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapleske</surname><given-names>J</given-names></name><name><surname>Rossell</surname><given-names>SL</given-names></name><name><surname>Woodruff</surname><given-names>PW</given-names></name><name><surname>David</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The planum temporale: a systematic, quantitative review of its structural, functional and clinical significance</article-title><source>Brain Research Reviews</source><volume>29</volume><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/S0165-0173(98)00047-2</pub-id><pub-id pub-id-type="pmid">9974150</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The expected value of control: an integrative theory of anterior cingulate cortex function</article-title><source>Neuron</source><volume>79</volume><fpage>217</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.007</pub-id><pub-id pub-id-type="pmid">23889930</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Simony</surname><given-names>E</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Coupled neural systems underlie the production and comprehension of naturalistic narrative speech</article-title><source>PNAS</source><volume>111</volume><fpage>E4687</fpage><lpage>E4696</lpage><pub-id pub-id-type="doi">10.1073/pnas.1323812111</pub-id><pub-id pub-id-type="pmid">25267658</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sled</surname><given-names>JG</given-names></name><name><surname>Zijdenbos</surname><given-names>AP</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A nonparametric method for automatic correction of intensity nonuniformity in MRI data</article-title><source>IEEE Transactions on Medical Imaging</source><volume>17</volume><fpage>87</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1109/42.668698</pub-id><pub-id pub-id-type="pmid">9617910</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fast robust automated brain extraction</article-title><source>Human Brain Mapping</source><volume>17</volume><fpage>143</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1002/hbm.10062</pub-id><pub-id pub-id-type="pmid">12391568</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname><given-names>GJ</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A place for time: the spatiotemporal structure of neural dynamics during natural audition</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>2019</fpage><lpage>2026</lpage><pub-id pub-id-type="doi">10.1152/jn.00268.2013</pub-id><pub-id pub-id-type="pmid">23926041</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Glessner</surname><given-names>M</given-names></name><name><surname>Salat</surname><given-names>D</given-names></name><name><surname>Hahn</surname><given-names>HK</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A hybrid approach to the skull stripping problem in MRI</article-title><source>NeuroImage</source><volume>22</volume><fpage>1060</fpage><lpage>1075</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.032</pub-id><pub-id pub-id-type="pmid">15219578</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Pacheco</surname><given-names>J</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Geometrically accurate topology-correction of cortical surfaces using nonseparating loops</article-title><source>IEEE Transactions on Medical Imaging</source><volume>26</volume><fpage>518</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1109/TMI.2006.887364</pub-id><pub-id pub-id-type="pmid">17427739</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theiler</surname><given-names>J</given-names></name><name><surname>Eubank</surname><given-names>S</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Galdrikian</surname><given-names>B</given-names></name><name><surname>Doyne Farmer</surname><given-names>J</given-names></name><name><surname>Farmer</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Testing for nonlinearity in time series: the method of surrogate data</article-title><source>Physica D: Nonlinear Phenomena</source><volume>58</volume><fpage>77</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(92)90102-S</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiener</surname><given-names>M</given-names></name><name><surname>Turkeltaub</surname><given-names>P</given-names></name><name><surname>Coslett</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The image of time: a voxel-wise meta-analysis</article-title><source>NeuroImage</source><volume>49</volume><fpage>1728</fpage><lpage>1740</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.09.064</pub-id><pub-id pub-id-type="pmid">19800975</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>DI</given-names></name><name><surname>Langston</surname><given-names>RF</given-names></name><name><surname>Schlesiger</surname><given-names>MI</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Watanabe</surname><given-names>S</given-names></name><name><surname>Ainge</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Lateral entorhinal cortex is critical for novel object-context recognition</article-title><source>Hippocampus</source><volume>23</volume><fpage>352</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1002/hipo.22095</pub-id><pub-id pub-id-type="pmid">23389958</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>DI</given-names></name><name><surname>Watanabe</surname><given-names>S</given-names></name><name><surname>Milner</surname><given-names>H</given-names></name><name><surname>Ainge</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Lateral entorhinal cortex is necessary for associative but not nonassociative recognition memory</article-title><source>Hippocampus</source><volume>23</volume><fpage>1280</fpage><lpage>1290</lpage><pub-id pub-id-type="doi">10.1002/hipo.22165</pub-id><pub-id pub-id-type="pmid">23836525</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Ridgway</surname><given-names>GR</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Permutation inference for the general linear model</article-title><source>NeuroImage</source><volume>92</volume><fpage>381</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.01.060</pub-id><pub-id pub-id-type="pmid">24530839</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittmann</surname><given-names>M</given-names></name><name><surname>Simmons</surname><given-names>AN</given-names></name><name><surname>Aron</surname><given-names>JL</given-names></name><name><surname>Paulus</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Accumulation of neural activity in the posterior insula encodes the passage of time</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>3110</fpage><lpage>3120</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2010.06.023</pub-id><pub-id pub-id-type="pmid">20600186</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The inner sense of time: how the brain creates a representation of duration</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>217</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1038/nrn3452</pub-id><pub-id pub-id-type="pmid">23403747</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacks</surname><given-names>JM</given-names></name><name><surname>Speer</surname><given-names>NK</given-names></name><name><surname>Reynolds</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Segmentation in reading and film comprehension</article-title><source>Journal of Experimental Psychology: General</source><volume>138</volume><fpage>307</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1037/a0015305</pub-id><pub-id pub-id-type="pmid">19397386</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zakay</surname><given-names>D</given-names></name><name><surname>Block</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Prospective and retrospective duration judgments: an executive-control perspective</article-title><source>Acta Neurobiologiae Experimentalis</source><volume>64</volume><fpage>319</fpage><lpage>328</lpage><pub-id pub-id-type="pmid">15283475</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zakay</surname><given-names>D</given-names></name><name><surname>Tsal</surname><given-names>Y</given-names></name><name><surname>Moses</surname><given-names>M</given-names></name><name><surname>Shahar</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The role of segmentation in prospective and retrospective time estimation processes</article-title><source>Memory &amp; Cognition</source><volume>22</volume><fpage>344</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.3758/BF03200861</pub-id><pub-id pub-id-type="pmid">8007836</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zarahn</surname><given-names>E</given-names></name><name><surname>Aguirre</surname><given-names>GK</given-names></name><name><surname>D'Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Empirical Analyses of BOLD fMRI Statistics</article-title><source>NeuroImage</source><volume>5</volume><fpage>179</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1006/nimg.1997.0263</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>GY</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Toward using confidence intervals to compare correlations</article-title><source>Psychological Methods</source><volume>12</volume><fpage>399</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1037/1082-989X.12.4.399</pub-id><pub-id pub-id-type="pmid">18179351</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.16070.031</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name><role>Reviewing editor</role><aff id="aff7"><institution>Boston University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors’ note: a previous version of this study was rejected after peer review, but the authors submitted for reconsideration. The first decision letter after peer review is shown below.]</p><p>Thank you for choosing to send your work entitled &quot;Neural pattern change during encoding of a narrative predicts retrospective duration estimates&quot; for consideration at <italic>eLife</italic>. Your full submission has been evaluated by Timothy Behrens (Senior editor) and two peer reviewers and a member of our Board of Reviewing Editors, and the decision was reached after discussions between the reviewers. Based on our discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>Reviewer #1:</p><p>The logic of the paper is that in some regions (notably EC and pars orbitalis) the RSA distance between the multivoxel response at two moments predicts the time subjects judge between those two events later on. The authors argue that this result suggests that retrospective time judgements depend on a gradually changing state of temporal context that resides in these regions. If we take its conclusions at face value, this paper would make an important contribution. The paper is potentially an important advance over previous studies because it uses realistic stimuli.</p><p>Unfortunately, I don't quite accept the conclusions. The fundamental problem is that I can imagine obtaining the result without any memory demands whatsoever. Imagine that the participants were played audio clips from a radio drama and asked to judge how far apart they were in the show. Let's say for one pair of clips both have the sound of the ocean in the background and the same speakers speaking. The other pair of clips does not sound alike---different people are speaking in different locations. Which of these pairs would be judged to be closer in time? Now, insofar as an aspect of brain activity measures any property of the clips (power spectrum, semantic content, etc), we would readily expect that brain activity to correctly categorize the pairs of clips. But this is certainly not a memory effect as by construction, there is no actual memory for anything. This account seems to naturally account for the finding that many many brain regions show a tendency towards a correlation (<xref ref-type="fig" rid="fig5">Figure 5</xref>), although most of them do not reach significance.</p><p>I would feel much better about accepting the conclusions if, rather than assessing significance relative to chance, the analysis was done relative to some control region that ought to be sensitive to auditory and/or semantic similarity. I am not enough of an expert in the auditory system (or fMRI more broadly) to suggest a specific comparison region, but as it is I think the conclusions either need to be significantly moderated or the empirical support for those conclusions needs to be stronger.</p><p>Reviewer #2:</p><p>Using multivoxel pattern similarity, the authors find that right entorhinal cortex, right ATL, right pars orbitalis and left ACC show patterns of activity that correlate with cued retrospective duration judgments while keeping objective duration constant. They show this using both an ROI and searchlight approach and find an overlapping, though not completely identical, set of regions which they attribute to differences between the two methods in size, shape, and respect of anatomical boundaries. The experiment is interesting and methodologically sound but would be more impactful if the authors did more work to understand what is driving their effect. As is, the authors don't do much to make the reader excited about the findings or to better differentiate it from prior related work.</p><p>For example, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> suggests that certain pairs of story clips are consistently rated as closer together versus further apart. There appears to be no attempt to characterize what features of the story drive this effect. Moreover, it is unclear how much such features may produce the effect of neural dissimilarity correlating with greater distance judgments. For instance, if two clips with two different sets of characters are rated as further apart that two clips with the same set of characters, the regions that show dissimilarity scaling with subjective duration may be those sensitive specifically to characters rather than context more broadly. Thus an alternate explanation for their results is that the regions that show their effect are just sensitive to the content of the story that can produce both neural dissimilarity and greater duration ratings. The authors should discuss/examine this alternative.</p><p>One analysis that might support their account of effects being due to gradual change in context tracking regions would be to split their 2 min interval into 20-30s chunks and see if the change is indeed gradual. Otherwise the dissimilarity measure could be simply a result in differences in evoked activity between the two time points, which would be more likely if the effects were due to content sensitivity.</p><p>It would be informative to know whether the pattern similarity values in their regions correlate with each other as they should if they are tracking the same context state representation?</p><p>Did duration ratings change as a function of position in the story? One could imagine that overall recency would have an effect on duration judgments. And if so, did pattern similarity values change?</p><p>[Editors’ note: what now follows is the decision letter after the authors submitted for further consideration.]</p><p>Thank you for submitting your article &quot;Neural pattern change during encoding of a narrative predicts retrospective duration estimates&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Marc Howard (Reviewer #1); Lila Davachi (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>1) Please analyze and report univariate data – to see if they replicate Jenkins and Ranganath – or not.</p><p>2) Focus the paper more on the host of regions that show a slowly changing neural signal over time – instead of focusing on entorhinal and pars orbitalis (area 47).</p><p>3) Please discuss the divergence in their neural data from the primacy and recency effects in behavior.</p><p>4) Please test control duration judgments vs experimental duration judgments directly. If there's a reliable effect, then at least part of what they're calling mental context is most likely not mental context. This outcome probably does not lead to an <italic>eLife</italic> paper.</p><p>5) Given that there isn't a reliable effect, the reviewers need to be convinced that this lack of an effect is meaningful. One way to do this would be to do a power analysis. Another way would be to place a confidence interval on the correlation and show that the correlation, while possibly non-zero, would have to be so small we wouldn't care about it. Even better if you can argue it would have to be so small it couldn't account for the correlation between the experimental judgments and the drift. There are also fancy ways to approach this (e.g., Bayesian inference). In sum, the authors need to make a positive case for the null if they want to argue that this is a memory effect rather than some property of their stimuli.</p><p>Reviewer #1</p><p>On the previous round of review, my major concern was that the change attributed to putative contextual drift that correlates with duration judgments could more simply be attributed to perceptual/semantic differences in the patterns themselves. Real world stimuli that unfold in time are autocorrelated over just about every time scale. Compounding the problem, it is impossible to measure the similarity on all relevant dimensions. This revision makes a substantive attempt to argue against the perceptual hypothesis, there are two behavioral controls that attempt to address the question of whether the results attributed to contextual change could be driven by perceptual effects. To summarize my reaction to the revision, while the controls make for a stronger case than the previous submission, I am not convinced that these controls address the concern in a satisfactory way. I suggest additional analyses with the existing data that could clarify this point. If this concern were resolved (which is not at all clear) the manuscript would result in a very nice contribution.</p><p>The first control asks subjects to describe event boundaries during presentation of the story. This allows a rough estimate of the change in context between the two clips. Indeed the number of event boundaries predicted duration judgments by the original participants. The assumption seems to be that there is no perceptual/semantic similarity across event boundaries, but is not an assumption that I can accept. Imagine a story where Alice and Betty have a discussion at the beach. Then there's an event boundary and Alice and Betty move to the coffee shop. Then there's another event boundary as Alice leaves and Betty and Chris have a conversation in the coffee shop. The number of event boundaries correlates with the overlap of perceptual/semantic features present in the scene. More broadly, if the perceptual/semantic content is autocorrelated over long time scales (and it almost surely is) and if event boundaries are a proxy for abrupt drops in the autocorrelation, then number of event boundaries really ought to predict perceptual/semantic similarity of the available features. So this control is not at all convincing.</p><p>In the second control, a group of naive subjects are asked to rate the similarity of the clips. There is no evidence that their ratings correspond to the number of event boundaries between the clips. The suggestion is that because number of event boundaries indexes contextual change (but not presumably perceptual/semantic similarity), the null result requires us to accept that there is no difference in the similarity of the two clips. Leaving aside for a moment the issue of asking the reader to accept the null (which is a really serious problem!), this is kind of an indirect test of what we're really after. The finding is that duration judgments of the fMRI subjects correlate with number of event boundaries whereas the judgments of the naive controls do not correlate with number of event boundaries. Why not just ask whether the judgments of the fMRI subjects correlate with the judgments of the naive subjects? If they do, then there is no way to argue that the change in the multivoxel signal is due to contextual drift per se. It might be possible to partial out the effect attributable to the naive subjects' judgments. If there is not a correlation, then the authors still have to successfully argue for the null, but it's at least a clean and direct (and much more sensitive!) test of the question of interest.</p><p>Reviewer #2</p><p>This revision has been responsive to prior concerns about whether visual or semantic dissimilarity during temporal memory judgments could be used to infer how far apart the clips had been presented during encoding. The authors conducted behavioral analyses to show that distance judgments were related to listening to the story and could not be deduced based on the test stimuli alone. They show that the number of event boundaries experienced in between the test stimuli also modulated distance judgments. These new results remove any doubts that the reported effects are not driven by visual confounds.</p><p>However, I am somehow not that excited about the new ms as it now provides a list of more regions that show pattern change related to distance judgments – much of the medial temporal lobe, frontal cortex, anterior temporal cortex, ACC… given the effects are more widespread, the laser focus on entorhinal and pars orbitalis makes the paper not easy to digest. Is this a general broad signal? Or is it focused?</p><p>Also new data that has been added in response to other concerns that now raise some skepticism. The most intrusive is the fact that distance judgments vary predictably by 'list' position – events early in the audiovisual recording are remembered as farther apart than those later in the tape. (see <xref ref-type="fig" rid="fig11">Figure 11</xref> – top panel). However, pattern similarity estimates do not track this behavioral effect. This result raises questions about why, if entorhinal and frontal cortex are representing temporal context signal, would they not also some what mirror the behavioral judgments. I could imagine that context representations may play less of a role as item memory fades? This is not discussed but should be for the authors' views on this to be clear. Otherwise, the impact of the final result is unclear.</p><p>In my original review, I had requested that they examine whether univariate activity was related to temporal memory success. The did run that analysis but only in entorhinal cortex and pars orbitalis -and do not see any effects but beg off reporting it since they did not have <italic>a priori</italic> predictions about it. however, published work (Jenkins and Ranganath) has shown that univariate activity in more dorsal parts of lateral frontal cortex is related to coarse temporal memory judgments. so there is a clear precedent for this effect. I am not sure why they say they did not have that prediction but this analysis, even if the results DO NOT show a univariate effect would be informative and could even bolster their conclusions that patterns across time, rather than activity to any single event, is a better predictor of temporal memory judgments.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.16070.032</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the author responses to the first round of peer review follow.]</p><p><italic>Reviewer #1:</italic> </p><p><italic>The logic of the paper is that in some regions (notably EC and pars orbitalis) the RSA distance between the multivoxel response at two moments predicts the time subjects judge between those two events later on. The authors argue that this result suggests that retrospective time judgements depend on a gradually changing state of temporal context that resides in these regions. If we take its conclusions at face value, this paper would make an important contribution. The paper is potentially an important advance over previous studies because it uses realistic stimuli.</italic> </p><p><italic>[…]</italic></p><p><italic>I would feel much better about accepting the conclusions if, rather than assessing significance relative to chance, the analysis was done relative to some control region that ought to be sensitive to auditory and/or semantic similarity. I am not enough of an expert in the auditory system (or fMRI more broadly) to suggest a specific comparison region, but as it is I think the conclusions either need to be significantly moderated or the empirical support for those conclusions needs to be stronger.</italic> </p><p>We thank the reviewer for raising these essential questions. In order to address them, we conducted two new behavioral experiments, as well as two new analyses of the neural data:</p><p>1) We show that our participants' duration estimates correlate strongly with the number of event boundaries between two clips, suggesting that their estimates were influenced by their memory for the content of the story in between two clips (rather than the similarity between the two clips alone).</p><p>2) A separate group of participants was asked to complete the same time perception test without first listening to the story. Since these &quot;naive participants&quot; had no memory of the story, they could only base their duration estimates on the similarity between the two clips. We show that duration estimates from these naive participants do not correlate with the number of event boundaries between two clips, proving that the intervening content between clips does not influence duration estimates when participants have no memory of the story.</p><p>3) A related concern was that neural pattern change might be driven by the perceptual and semantic dissimilarity of the clips, rather than the degree of contextual drift between the clips. To address this, we performed a within-interval version of our main ROI analysis. This analysis holds constant the two clips whose pattern distance is being measured. We show that individual differences in neural pattern change for a given pair of clips correlate with individual differences in duration estimates in the right entorhinal cortex and right pars orbitalis, as well as other regions that had been sub-threshold in our main analysis. Thus, pattern change in these regions correlates with duration estimates even when the perceptual and semantic content of the two clips is held constant. If neural pattern change were being driven by story content, we would have expected the effect to be larger for the across-interval, within-participants analysis (where story content differed across intervals) than for the across-participants, within- interval version of the analysis (where story content is held constant). The fact that the effect was similar in size for the two analyses suggests that story content is not a major factor driving the observed correlation between neural pattern change and duration estimates.</p><p>4) We show that patterns of activity in entorhinal cortex and pars orbitalis change significantly more slowly over time than patterns in cortical regions implicated in auditory and language processing, suggesting that they may integrate information over longer time scales.</p><p>We believe, and we hope the reviewer will agree, that these analyses directly address and alleviate the concern that our results could be obtained &quot;without any memory demands whatsoever&quot;.</p><p>A summary of the new analyses is now presented in a section titled &quot;Factors Driving the Correlation between Pattern Change and Duration Estimates&quot;:</p><p>“[…]we conducted two control behavioral studies. One group of participants indicated when event boundaries were occurring in the story. […]Moreover, pattern change in the right entorhinal cortex correlates highly with pattern change in the right pars orbitalis, suggesting that the two regions may cooperate to represent different facets of a unified, slowly changing context signal.”</p><p>A more detailed description of each analysis follows this section.</p><p><italic>Reviewer #2:</italic> </p><p><italic>Using multivoxel pattern similarity, the authors find that right entorhinal cortex, right ATL, right pars orbitalis and left ACC show patterns of activity that correlate with cued retrospective duration judgments while keeping objective duration constant. They show this using both an ROI and searchlight approach and find an overlapping, though not completely identical, set of regions which they attribute to differences between the two methods in size, shape, and respect of anatomical boundaries. The experiment is interesting and methodologically sound but would be more impactful if the authors did more work to understand what is driving their effect. As is, the authors don't do much to make the reader excited about the findings or to better differentiate it from prior related work.</italic> </p><p>We are grateful to the reviewer for their comments on how to increase the impact of the work. As described below, we have made several substantial changes to the paper to further specify what is driving our effect.</p><p><italic>For example, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> suggests that certain pairs of story clips are consistently rated as closer together versus further apart. There appears to be no attempt to characterize what features of the story drive this effect. Moreover, it is unclear how much such features may produce the effect of neural dissimilarity correlating with greater distance judgments. For instance, if two clips with two different sets of characters are rated as further apart that two clips with the same set of characters, the regions that show dissimilarity scaling with subjective duration may be those sensitive specifically to characters rather than context more broadly. Thus an alternate explanation for their results is that the regions that show their effect are just sensitive to the content of the story that can produce both neural dissimilarity and greater duration ratings. The authors should discuss/examine this alternative.</italic></p><p>We thank the reviewer for pointing out this important concern. Reviewer #1 raised essentially the same question: are the regions that show dissimilarity scaling with subjective duration sensitive to story content (e.g., which characters are present), rather than to context more broadly? Please see our response to Reviewer # 1 above (Major comment #1), in which we describe new behavioral control experiments and new neural analyses. The behavioral controls suggest that the number of event boundaries between two clips is a strong driver of duration estimates (but only for participants who have heard the story), and may be the reason why duration estimates are so consistent across participants. We also present a new neural analysis showing that all of the regions we found display a significant correlation between neural pattern dissimilarity and duration estimates across participants for a given pair of clips, in other words, even when the (objective) content of the story is held constant.</p><p>A summary of the new analyses is now presented in the section titled &quot;Factors Driving the Correlation between Pattern Change and Duration Estimates&quot;; a more detailed description of each analysis follows this section.</p><p><italic>One analysis that might support their account of effects being due to gradual change in context tracking regions would be to split their 2 min interval into 20-30s chunks and see if the change is indeed gradual. Otherwise the dissimilarity measure could be simply a result in differences in evoked activity between the two time points, which would be more likely if the effects were due to content sensitivity.</italic> </p><p>We greatly appreciated the reviewer's suggestion to &quot;split the 2 min interval into 20-30s chunks and see if the change is indeed gradual&quot;, and decided to expand on this idea to analyze the speed of pattern change across the entire story timecourse.</p><p>We quantified the speed of pattern change by calculating the auto-correlation of the pattern in a given region for every time point (TR) and averaging across time points to obtain a mean auto- correlation curve. The full-width half-maximum (FWHM) of this curve was taken as a measure of pattern change speed (the wider the peak of this curve is, the more gradually the pattern changes over time). We found that patterns of activity in the right entorhinal cortex and right pars orbitalis changed significantly more slowly than patterns in the right transverse temporal cortex (primary auditory cortex), right banks of the superior temporal sulcus and right superior temporal cortex (involved in auditory and language processing). Importantly, we also found that the right entorhinal cortex and right pars orbitalis, along with neighboring regions in the temporal pole, medial temporal lobe, orbitofrontal cortex and frontal pole, had the highest FWHMs (slowest pattern change) in the entire brain.</p><p>These results are now presented in the manuscript (section titled &quot;Patterns of activity in entorhinal cortex and pars orbitalis change slowly over time&quot;).</p><p>Since the anatomical masks used in the above analysis were of different sizes, we performed two control analyses to ensure that differences in the speed of pattern change were not due to differences in ROI size.</p><p>First, we regressed the vector of ROI size out of the vector of FWHM values across regions for every participant. This modified analysis replicated the results reported above: the entorhinal cortex, pars orbitalis, as well as other ROIs in the anterior temporal lobe, medial temporal lobe and orbitofrontal cortex, still had the slowest pattern change in the brain, and significantly slower than in primary auditory cortex. These results are reported in the manuscript.</p><p>Second, we performed a univariate version of the above analysis by calculating the auto- correlation function of each voxel individually, averaging the auto-correlation curves across all voxels of a given ROI and then computing the FWHM value for the average curve. The univariate analysis replicated the above findings, and showed that the right entorhinal cortex ROI had the slowest changing voxels of all the regions in our atlas. These results are reported in the manuscript.</p><p>Taken together, we feel that these new analyses provide strong support for our interpretation that entorhinal cortex and pars orbitalis process information that changes gradually over time.</p><p><italic>It would be informative to know whether the pattern similarity values in their regions correlate with each other as they should if they are tracking the same context state representation?</italic> </p><p>We agree with the reviewer that regions tracking the same context state representation should have correlated pattern change values across two-minute intervals. To explore this, we extracted the pattern dissimilarity for each of the 24 pairs of clips (which were 2 min apart) and averaged the vectors across participants. The correlation between the mean pattern distance vectors in the right entorhinal cortex and right pars orbitalis was r = 0.73. In order to interpret the magnitude of this correlation, we also calculated the correlation between every possible pair of mean pattern distance vectors (for all 84 anatomical masks). This resulted in a distribution of 3486 correlations for every possible pair of regions ((84 x 84 – 84) I 2 = 3486).</p><p>Out of 3486 pairs of regions, only 242 exhibited a correlation that was higher than the one observed between the right entorhinal and right pars orbitalis. Thus, the correlation between the pattern distances in these two regions is higher than for 93% of region pairs.</p><p>A phase randomization procedure showed that the likelihood of obtaining a correlation of this magnitude by chance – given the auto-correlation in the pattern change vectors – was p=0.0011.</p><p>The strong correlation in pattern change between the two regions suggests that they may cooperate to represent different facets of a unified, slowly changing context signal.</p><p>This analysis is now reported in greater detail in the manuscript.</p><p><italic>Did duration ratings change as a function of position in the story? One could imagine that overall recency would have an effect on duration judgments. And if so, did pattern similarity values change?</italic> </p><p>Duration estimates did change as a function of position in the story, with earlier intervals being estimated as longer than later intervals (<xref ref-type="fig" rid="fig11">Figure 11</xref>). The correlation between the estimated duration of an interval and its time in the story was consistently negative across participants (M= -0.40, SD= 0.22; t(16)= -7.59, p&lt;0.00001). These results replicate the positive time-order effect, which is the finding that people judge earlier durations in a series of durations to be longer than later durations (Block, 1982, 1985; Brown &amp; Stubbs, 1988). The effect has been interpreted to mean that context changes more rapidly at the start of a novel episode (Block, 1982, 1986).</p><p>Interestingly, the pattern dissimilarity values in right entorhinal cortex and right pars orbitalis did not exhibit the same overall decrease across time. In fact, there was no consistent correlation between pattern change during an interval and its time in the story for the right entorhinal cortex (M=0.03, SD=0.21; t(16)= 0.65; p=0.53) or the right pars orbitalis (M=-0.10, SD=0.22; t(16)= -1.83, p=0.09). These results suggest that the relationship between duration estimates and pattern dissimilarity in these regions was not driven by a shared linear trend. Rather, it seems that pattern dissimilarity in these regions correlated with more fine-grained variations in the estimated durations of nearby intervals (<xref ref-type="fig" rid="fig11">Figure 11</xref>).</p><p>This analysis is now presented in the manuscript.</p><p>[Editors' note: the author responses to the re-review follow.]</p><p><italic>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</italic> </p><p><italic>1) Please analyze and report univariate data – to see if they replicate Jenkins and Ranganath – or not.</italic> </p><p><italic>2) Focus the paper more on the host of regions that show a slowly changing neural signal over time – instead of focusing on entorhinal and pars orbitalis (area 47).</italic> </p><p><italic>3) Please discuss the divergence in their neural data from the primacy and recency effects in behavior.</italic> </p><p><italic>4) Please test control duration judgments vs experimental duration judgments directly. If there's a reliable effect, then at least part of what they're calling mental context is most likely not mental context. This outcome probably does not lead to an eLife paper.</italic> </p><p><italic>5) Given that there isn't a reliable effect, the reviewers need to be convinced that this lack of an effect is meaningful. One way to do this would be to do a power analysis. Another way would be to place a confidence interval on the correlation and show that the correlation, while possibly non-zero, would have to be so small we wouldn't care about it. Even better if you can argue it would have to be so small it couldn't account for the correlation between the experimental judgments and the drift. There are also fancy ways to approach this (e.g., Bayesian inference). In sum, the authors need to make a positive case for the null if they want to argue that this is a memory effect rather than some property of their stimuli.</italic> </p><p>We would like to thank the reviewers for their tremendously helpful comments, which have guided our revisions and inspired us to add new analyses that we feel have substantially improved the rigor of our contribution. These revisions have also enabled us to reorganize our manuscript into a far more coherent structure that helps highlight the consistency of our findings across analyses.</p><p>The following is a summary of the most important changes.</p><p>Reviewer 1 was concerned that, if our original behavioral data is correlated with the behavioral data from the control (naïve) group who had not heard the story, then a component of our original behavior could be correlated with the perceptual or semantic similarity between clip pairs and that this component could be driving the correlation with neural pattern change. We address this concern in the following ways:</p><p>First, we emphasize the importance of the within-interval analysis, which correlates individual differences in subjective duration for a pair of clips with individual differences in neural pattern drift. This analysis holds constant the objective similarity of the two clips and leverages variance across participants. In addition to the ROI analysis from the previous version of the manuscript, we report a new searchlight version of the within-interval analysis, demonstrating that a cluster in the right anterior temporal lobe, overlapping with the right entorhinal region found in the ROI analysis, is significant even when the objective similarity between two clips is controlled for.</p><p>Second, we perform a highly conservative mixed-effects version of the ROI analysis. For each ROI, we fit a model that estimates the population-level effect of neural pattern distance on duration estimates, while controlling for individual variability between participants and between clip pairs. This analysis combines the virtues of both the within-participant and within-interval analyses. We show that the right entorhinal cortex and left caudal ACC exhibit confidence intervals that do not include 0, even when the most conservative fitting procedure and power transform of the behavioral data are applied. Moreover, we show that these effects are not weakened by including the mean duration estimates from the control (naïve) group in the model.</p><p>The new analyses above also help us to address the concern from Reviewer 2 that, while we reported effects in a distributed set of brain regions, our discussion focused on only two or three of those regions. The revised manuscript explicitly synthesizes the findings from all of the above analyses in a section entitled “Comparing Results from ROI and Searchlight Analyses”. The synthesis shows that regions of the right anterior temporal lobe, peaking in the right entorhinal cortex, emerge consistently across the within-participant and within-interval versions of the ROI and searchlight analyses, as well as the mixed-effects ROI analysis.</p><p>As requested by Reviewer 2, in the new manuscript, we discuss in detail the lack of linear decrease in neural pattern change with time in the story. Raw fMRI data prior to high-pass filtering shows that if such a trend were present, it would be obscured by an existing linear trend in the opposite direction, which seems to be caused by non-neuronal scanner artifacts.</p><p>Finally, we report an exciting new replication of the Jenkins and Ranganath (2010) coarse temporal memory analysis. After performing a whole-brain univariate analysis, we find a significant correlation between activity during encoding of a clip and the participant’s later accuracy in placing that clip on the timeline of the story in extensive clusters of the lateral prefrontal cortex (DL-PFC) and dorsomedial PFC, as well as sub-threshold clusters in the medial parietal. Our results suggest the importance of the default mode network in subsequent memory for the temporal context of a clip, especially when the clip is part of a coherent narrative.</p><p><italic>Reviewer #1</italic></p><p><italic>On the previous round of review, my major concern was that the change attributed to putative contextual drift that correlates with duration judgments could more simply be attributed to perceptual/semantic differences in the patterns themselves. Real world stimuli that unfold in time are autocorrelated over just about every time scale. Compounding the problem, it is impossible to measure the similarity on all relevant dimensions. This revision makes a substantive attempt to argue against the perceptual hypothesis, there are two behavioral controls that attempt to address the question of whether the results attributed to contextual change could be driven by perceptual effects. To summarize my reaction to the revision, while the controls make for a stronger case than the previous submission, I am not convinced that these controls address the concern in a satisfactory way. I suggest additional analyses with the existing data that could clarify this point. If this concern were resolved (which is not at all clear) the manuscript would result in a very nice contribution.</italic> </p><p> <italic>The first control asks subjects to describe event boundaries during presentation of the story. This allows a rough estimate of the change in context between the two clips. Indeed the number of event boundaries predicted duration judgments by the original participants. The assumption seems to be that there is no perceptual/semantic similarity across event boundaries, but is not an assumption that I can accept. Imagine a story where Alice and Betty have a discussion at the beach. Then there's an event boundary and Alice and Betty move to the coffee shop. Then there's another event boundary as Alice leaves and Betty and Chris have a conversation in the coffee shop. The number of event boundaries correlates with the overlap of perceptual/semantic features present in the scene. More broadly, if the perceptual/semantic content is autocorrelated over long time scales (and it almost surely is) and if event boundaries are a proxy for abrupt drops in the autocorrelation, then number of event boundaries really ought to predict perceptual/semantic similarity of the available features. So this control is not at all convincing.</italic> </p><p>We thank the reviewer for pointing out this important concern and completely agree that the number of event boundaries between two clips should correlate with the degree of perceptual and semantic dissimilarity between them. In fact, we discussed this possibility in the previous version of the manuscript but did not make it sufficiently explicit in our argument:</p><p>In the previous manuscript, we showed that the number of event boundaries between two clips was significantly more correlated with original duration estimates than with naïve duration estimates. Based on this result, we concluded that having memory of the story caused the original participants to be more influenced by the number of event boundaries. In other words, we hoped to infer that our original participants were influenced by their memory of events that had occurred between the two clips when estimating durations.</p><p>We have modified this section in order to better emphasize this logic and placed it in the Behavioral Results section of the revised manuscript:</p><p>“However, it is important to note that the number of event boundaries between two clips also influences the perceptual and semantic similarity between them (e.g., clips from the same scene might sound more similar than clips from different scenes). […] This suggests that the number of event boundaries carries information about temporal context that is not contained within the clips alone, and that our original participants’ estimates were influenced by their memory of this contextual information.”</p><p><italic>In the second control, a group of naive subjects are asked to rate the similarity of the clips. There is no evidence that their ratings correspond to the number of event boundaries between the clips. The suggestion is that because number of event boundaries indexes contextual change (but not presumably perceptual/semantic similarity), the null result requires us to accept that there is no difference in the similarity of the two clips. Leaving aside for a moment the issue of asking the reader to accept the null (which is a really serious problem!), this is kind of an indirect test of what we're really after. The finding is that duration judgments of the fMRI subjects correlate with number of event boundaries whereas the judgments of the naive controls do not correlate with number of event boundaries. Why not just ask whether the judgments of the fMRI subjects correlate with the judgments of the naive subjects? If they do, then there is no way to argue that the change in the multivoxel signal is due to contextual drift per se. It might be possible to partial out the effect attributable to the naive subjects' judgments. If there is not a correlation, then the authors still have to successfully argue for the null, but it's at least a clean and direct (and much more sensitive!) test of the question of interest.</italic></p><p>We thank the reviewer for highlighting the importance of discussing the correlation between original behavior and naïve behavior. In the previous manuscript, we did correlate the naïve duration judgments directly with the original duration judgments and reported the results:</p><p>“The inter-subject correlation in duration estimates was as strong for naïve participants (M=0.43, SD=0.18) as for our original participants (M=0.43, SD=0.25), suggesting that they used a consistent strategy to estimate durations. However, when we correlated duration estimates from our original group of participants with those of our naïve participants, we found that between-group correlations (M=0.18, SD=0.22) were significantly lower than the within-group correlations (p&lt;0.0001, as assessed by a permutation test described in the Materials and methods). This suggests that while both groups used a consistent strategy to estimate durations, the nature of the strategy differed across groups.”</p><p>Using these results, we never meant to argue that <italic>none</italic> of the original duration estimates could be explained by the perceptual or semantic similarity of the clips. In other words, we never meant to argue for the null. In fact, we feel it would be surprising if the duration estimates did not correlate with perceptual and semantic similarity, since presumably a large component of mental context change is driven by changes in perceptual and semantic changes.</p><p>By showing that the within-group correlations were significantly stronger than the between-group correlations, we hoped only to show that the two groups were using qualitatively different strategies. Together with the significantly greater correlation between original estimates and event boundaries, we hope these results show that there is a component of the duration estimates that was driven by memory and that could not be explained by perceptual similarity alone.</p><p>To avoid a misinterpretation of the argument, we have made these claims more explicit in the revised manuscript. We have also added confidence intervals for the within-group and between-group correlations.</p><p>“When we correlated duration estimates from our original group of participants with those of our naïve participants, we found that the between-group correlations (M=0.18, SD=0.22, 95% CI=[0.04, 0.28]) were significantly above 0, suggesting that a component of the original duration estimates was influenced by the similarity in content between clips. However, the between-group correlations were significantly lower than the within-group correlations (p&lt;0.0001, as assessed by a permutation test described in the Materials and methods). In other words, there is a reliable component of our original participants’ behavior that cannot be captured by accounting for the perceptual and semantic similarity between clips. In summary, having memory of the story induced a qualitatively different pattern of behavior and produced significantly more accurate duration estimates.”</p><p>Most importantly, we did not mean to argue that these behavioral results show anything about the neural data. In the previous manuscript, we attempted to delimit the implications of the behavioral data, and to point out that only the within-interval neural analysis enables us to rule out perceptual similarity as an explanation of the neural effects:</p><p>“These results suggest that duration estimates do not correlate with the number of contextual changes when participants are judging temporal distance based purely on the content of the clips. […]</p><p>However, it is still possible that pattern distance in the brain regions we found correlates with the component of duration estimates that is driven by the perceptual and semantic similarity between clips, rather than by contextual changes. To rule out this possibility, we performed a version of our main analysis that holds constant the perceptual and semantic similarity between two clips.”</p><p>In the within-interval analysis, we correlated individual differences in subjective duration for a given interval with individual differences in neural pattern distance for that interval. By performing the correlation within a given interval, we hold constant the perceptual and semantic content of the two clips and only leverage individual differences in how long the interval appeared retrospectively.</p><p>To better emphasize the importance of the within-interval analysis as a control for the objective similarity between two clips, we have placed the within-interval ROI analysis in the Results section of the revised manuscript, directly after the within-participant ROI analysis. We have also added a whole-brain searchlight version of the within-interval analysis, which we have placed directly after within-participant whole-brain searchlight, in the revised manuscript. Throughout the Results and Discussion of the revised manuscript, we have tried to highlight the importance of showing that a brain region is significant in both the within-participant analysis, which controls for subject random effects, and the within-interval analysis, which controls for item random effects. This is particularly evident in the section entitled “Comparing Results from ROI and Searchlight Analyses” (pp. 33-35), where we highlight that the right entorhinal cortex was the only ROI that survived both types of analyses, whereas the searchlight clusters from both types of analyses overlapped in areas like the right amygdala, right temporal pole and right posterior parahippocampal gyrus.</p><p>Mixed-Effects Modeling</p><p>In addition to the within-interval analysis, we sought to more thoroughly address the concern that patterns of activity in the regions we found might represent the perceptual or semantic content of the clips, rather than abstract contextual information. For this purpose, we fit a mixed-effects model to the data from each ROI, controlling for the effect of naïve duration estimates. For each ROI, we fit a model of this form:</p><p><italic>SubjectiveDuration ~ 1 + NeuralDistance + NaiveDuration + (1 + NeuralDistance | Interval) + (1 + NeuralDistance + NaiveDuration | Subject)</italic> </p><p>As described in the revised manuscript,</p><p>“This analysis estimates population-level effects of interest, while controlling for the possibility of individual variability between subjects and between clip pairs. In other words, this approach leverages the power of the within-interval analysis to control for the objective content similarity between two clips, while also taking into account variability in the effect across participants. In addition, we included the mean duration estimates from our naïve participants as a covariate in the model (see Behavioral Results). Since naïve participants had estimated the temporal distance between each pair of clips without hearing the story, this covariate is a further control for the inherent guessability of the temporal distance between two clips. Both controls strengthen our interpretation that the remaining effect of neural pattern distance on duration estimates is driven by the contextual dissimilarity (rather than perceptual or content dissimilarity) between two clips.”</p><p>Out of the 84 anatomical ROIs, we found that the fixed effect of neural pattern distance on duration estimates was positive (i.e., had bootstrapped confidence intervals that did not include 0) in the right entorhinal cortex and left caudal anterior cingulate cortex (ACC). We also found near-significant effects in the right amygdala and right superior temporal cortex. In this model, a significant fixed effect means that the effect generalizes to the population, even after variability across participants and intervals, as well as the other covariates (i.e., naïve duration estimates) have been accounted for.</p><p>Importantly, including the naïve duration estimates in the model did not have a significant impact on the size of the fixed effect in these regions, suggesting that the relationship between neural distance and duration estimates was not driven solely by the perceptual or semantic dissimilarity between clips. These results are detailed in subsection “Mixed-Effects Model Accounting for Naïve Duration Estimates” of the revised manuscript.</p><p><italic>Reviewer #2</italic></p><p><italic>This revision has been responsive to prior concerns about whether visual or semantic dissimilarity during temporal memory judgments could be used to infer how far apart the clips had been presented during encoding. The authors conducted behavioral analyses to show that distance judgments were related to listening to the story and could not be deduced based on the test stimuli alone. They show that the number of event boundaries experienced in between the test stimuli also modulated distance judgments. These new results remove any doubts that the reported effects are not driven by visual confounds.</italic> </p><p> <italic>However, I am somehow not that excited about the new ms as it now provides a list of more regions that show pattern change related to distance judgments – much of the medial temporal lobe, frontal cortex, anterior temporal cortex, ACC… given the effects are more widespread, the laser focus on entorhinal and pars orbitalis makes the paper not easy to digest. Is this a general broad signal? Or is it focused?</italic> </p><p>We thank the reviewer for pointing out this inconsistency between the breadth of our results and the focus on two specific regions in our discussion. Our Results section has undergone substantial revisions, which now make it easier to compare and synthesize the results across analyses.</p><p>We have restructured the Results section to have the following sub-sections:</p><p>1) Within-participant ROI analysis</p><p>2) Within-interval ROI analysis</p><p>3) Mixed-Effects ROI analysis</p><p>4) Within-participant Searchlight analysis</p><p>5) Within-interval Searchlight analysis</p><p>6) Comparing ROI and Searchlight analyses</p><p>Sub-section 1 reports significant effects in the right entorhinal, right pars orbitalis and left caudal ACC. Sub-section 2 reports significant effects in the right entorhinal, right amygdala and right insula. Sub-section 3 reports significant effects in the right entorhinal cortex and left caudal ACC. Sub-sections 4 and 5 both report significant clusters in the right anterior temporal lobe.</p><p>Sub-section 6 summarizes all the above results and highlights the fact that only the right entorhinal cortex reliably survived all versions of the ROI analysis, whereas the searchlight clusters overlap with this region as well as parts of the right amygdala, temporal pole, anterior middle temporal gyrus and posterior parahippocampal gyrus. Thus, our final results are localized to the right anterior temporal lobe, peaking in the right entorhinal cortex.</p><p>Please note that the results for the within-interval ROI analysis (sub-section 2) have changed very slightly in the revised manuscript. When we were reproducing these results, we noticed a mistake in the MATLAB code, which used a slightly different threshold to determine which intervals were labeled as “confident”. In the rest of the paper, we ensured that the confidence threshold for each participant would keep at least <italic>1/3</italic> of the behavioral data. However, for the within-interval analysis, we had mistakenly used a confidence threshold for each participant that would keep at least <italic>1/2</italic> of the behavioral data. Thus, the within-interval analysis was mistakenly using more of the behavioral data (a less stringent confidence threshold) than the other analyses in the paper. Correcting this error resulted in a very minor change in the Z-values for this analysis, though this change was sufficient to bring several of the ROIs below the q&lt;0.05 FDR threshold. Using the correct confidence threshold, the new results show that only the right entorhinal, right amygdala and right insula pass FDR correction (q&lt;0.05) for the within-interval ROI analysis, and the right entorhinal cortex even survives whole-brain correction (among 84 anatomical regions) at q&lt;0.05.</p><p><italic>Also new data that has been added in response to other concerns that now raise some skepticism. The most intrusive is the fact that distance judgments vary predictably by 'list' position – events early in the audiovisual recording are remembered as farther apart than those later in the tape. (see <xref ref-type="fig" rid="fig11">Figure 11</xref> – top panel). However, pattern similarity estimates do not track this behavioral effect. This result raises questions about why, if entorhinal and frontal cortex are representing temporal context signal, would they not also some what mirror the behavioral judgments. I could imagine that context representations may play less of a role as item memory fades? This is not discussed but should be for the authors' views on this to be clear. Otherwise, the impact of the final result is unclear.</italic></p><p>We thank the reviewer for pointing out the importance of investigating this discrepancy between the neural and behavioral data. We have modified this section of the manuscript and included an extensive discussion of the possible reasons for this discrepancy.</p><p>First, we discuss the positive time-order effect, the canonical finding that duration estimates are larger at the start of a new “episode” and decrease over time within the episode (i.e., duration estimates might be longer at the beginning of the story because context changes more at the start of a novel episode). Second, we show that there are significantly more event boundaries in the beginning of our story, and that the number of event boundaries decreases with time in story. Both of these factors might explain why duration estimates decrease with time in story.</p><p>Importantly, we then discuss why neural pattern change did not decrease over time in the story. Since this trend was not present in any of the regions uncovered by our ROI analyses (right entorhinal, right OFC, left caudal ACC, right amygdala, right insula), we performed a whole-brain search, to check whether any anatomical region exhibits this decrease in pattern change over time. Surprisingly, we did not find any region exhibiting this pattern significantly. Given that we were looking for a slow change in neural signal (unfolding over the entire time course of the story), we thought that our high-pass filter might be removing this slow change; to address this possibility, we analyzed the unfiltered data. When we did this, we found an overwhelming trend in the opposite direction, with most brain patterns changing <italic>more</italic> with time in the experiment. This increase in pattern change over time was even present in the CSF and white matter, suggesting that it was not reflective of neuronal activity, but was probably caused by a non-neuronal artifact, such as scanner drift or motion, that increased slowly with time.</p><p>In conclusion, we argue that even if neural activity patterns were changing less and less as the story unfolds, in concert with the behavior, we might not be able to see this effect, as it would have to overcome a global signal in the opposite direction that is not due to neural activity and is present everywhere, including the CSF.</p><p> <italic>In my original review, I had requested that they examine whether univariate activity was related to temporal memory success. The did run that analysis but only in entorhinal cortex and pars orbitalis -and do not see any effects but beg off reporting it since they did not have a priori predictions about it. however, published work (Jenkins and Ranganath) has shown that univariate activity in more dorsal parts of lateral frontal cortex is related to coarse temporal memory judgments. so there is a clear precedent for this effect. I am not sure why they say they did not have that prediction but this analysis, even if the results DO NOT show a univariate effect would be informative and could even bolster their conclusions that patterns across time, rather than activity to any single event, is a better predictor of temporal memory judgments.</italic></p><p>We apologize for not having addressed the reviewer’s question more thoroughly in our previous round of revisions, and we appreciate the suggestion of performing a whole-brain analysis to search for the relationship found by Jenkins and Ranganath (2010) between univariate activity at encoding and subsequent coarse temporal memory judgments.</p><p>In addressing this request, we found that (in our previous analysis) we had quantified the accuracy of participants’ timeline judgments in a slightly different manner from the Jenkins and Ranganath (2010) analysis. Jenkins and Ranganath (2010) had performed a linear regression of estimated temporal position against actual temporal position, and used the absolute value of the residuals as their measure of “error”. In contrast, we had used the absolute value of the distance between the estimated place in the story and the actual place in the story as our measure of error. Using this absolute distance method, we reported null effects in the right entorhinal cortex and right pars orbitalis, and in fact we did not find significant effects in any ROI in the brain.</p><p>In a new version of this analysis, we have now followed the Jenkins and Ranganath procedure more closely by performing a linear regression on the behavior. We then correlated the negative of the error (our measure of accuracy) with the activity of each brain voxel at encoding. We found highly significant clusters in the left dorsolateral prefrontal cortex (replicating the above report), as well as the medial prefrontal cortex, and slightly sub-threshold clusters in the medial parietal (precuneus and retrosplenial) and left superior temporal gyrus (<xref ref-type="fig" rid="fig14">Author response image 1</xref>, left panel, blue clusters). Thus, it seems that the linear regression procedure mattered for the final results.</p><p>Importantly, in this analysis, we performed a full correlation between a voxel’s activity when a clip was encoded and the participant’s accuracy in placing that clip on the timeline. Jenkins and Ranganath had binarized the behavior into “Hits” (bottom 1/3 of residual errors) and “Misses” (top 1/3 of residual errors). When we binned the behavior in a similar way, we found that the medial parietal, medial prefrontal and left superior temporal clusters were all highly significant, whereas the left dorsolateral PFC cluster was no longer significant (<xref ref-type="fig" rid="fig14">Author response image 1</xref>, right panel, red-yellow clusters).<fig id="fig14" position="float"><object-id pub-id-type="doi">10.7554/eLife.16070.028</object-id><label>Author response image 1.</label><caption><title>Clusters whose activity at encoding correlated with subsequent accuracy at placing clips on the timeline of the story.</title><p>The left panel (in blue) shows the results of the Pearson’s correlation between accuracy on a clip and the encoding activity for that clip. The right panel (red-yellow) shows the results of the contrast between activity for Hits (bottom 1/3 of residual error) and activity for Misses (top 1/3 of residual error) when placing the clip on the timeline of the story.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.16070.028">http://dx.doi.org/10.7554/eLife.16070.028</ext-link></p></caption><graphic mime-subtype="png" mimetype="image" xlink:href="elife-16070-resp-fig1-v2"/></fig></p><p>In subsection “Replication of Jenkins and Ranganath 2010: activity at encoding predicts accuracy of temporal context memory”, we report the analysis where a full correlation between voxel activity and behavioral accuracy is performed (rather than the contrast between Hits and Misses). We do not report both versions for the sake of brevity.</p><p>To summarize, both versions of the whole-brain analysis revealed that regions of the Default Mode Network (DMN) were significantly more active during the encoding of clips whose place in the timeline of the story participants later recalled more accurately.</p><p>In the manuscript’s discussion, we propose that one reason for the discrepancy between the Jenkins &amp; Ranganath results and our results may be due to the narrative structure of our stimulus, which seems to elicit strong inter-subject correlations in regions of the DMN (Lerner et al., 2011). It is possible that this network is particularly important for encoding the temporal context of stimuli that are part of a narrative (Chen et al., 2015), but that another strategy is used when the stimuli whose timing is recalled were not related to one another.</p><p>Regarding the reviewer’s comment:</p><p> <italic>“even if the results DO NOT show a univariate effect would be informative and could even bolster their conclusions that patterns across time, rather than activity to any single event, is a better predictor of temporal memory judgments.”</italic> </p><p>The reviewer is suggesting that the results of the univariate analysis could potentially bolster our conclusions from the multivariate analysis. However, please note that it is not necessarily surprising that our multivariate analyses (which constitute the bulk of the manuscript) reveal different results from this univariate analysis, given that the behavioral tests used are different. In the bulk of the manuscript, we relate multivariate pattern change to duration estimates, where participants explicitly estimated the relative distance between two clips. On the other hand, the univariate analysis leverages data from a separate behavioral test where participants placed each clip, individually, on the timeline of the story (not a comparison between two clips).</p></body></sub-article></article>