<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75485</article-id><article-id pub-id-type="doi">10.7554/eLife.75485</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>One-shot generalization in humans revealed through a drawing task</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-170901"><name><surname>Tiedemann</surname><given-names>Henning</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2637-7888</contrib-id><email>henning-tiedemann@hotmail.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-170898"><name><surname>Morgenstern</surname><given-names>Yaniv</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6902-667X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-170900"><name><surname>Schmidt</surname><given-names>Filipp</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8023-7304</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-50193"><name><surname>Fleming</surname><given-names>Roland W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5033-5069</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Department of Experimental Psychology, Justus Liebig University Giessen</institution></institution-wrap><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>Laboratory of Experimental Psychology, University of Leuven (KU Leuven)</institution></institution-wrap><addr-line><named-content content-type="city">Leuven</named-content></addr-line><country>Belgium</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Center for Mind, Brain and Behavior (CMBB), University of Marburg and Justus Liebig University Giessen</institution></institution-wrap><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75485</elocation-id><history><date date-type="received" iso-8601-date="2021-11-11"><day>11</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-01"><day>01</day><month>05</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-05-31"><day>31</day><month>05</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.05.31.446461"/></event></pub-history><permissions><copyright-statement>© 2022, Tiedemann et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Tiedemann et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75485-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75485-figures-v1.pdf"/><abstract><p>Humans have the amazing ability to learn new visual concepts from just a single exemplar. How we achieve this remains mysterious. State-of-the-art theories suggest observers rely on internal ‘generative models’, which not only describe observed objects, but can also synthesize novel variations. However, compelling evidence for generative models in human one-shot learning remains sparse. In most studies, participants merely compare candidate objects created by the experimenters, rather than generating their own ideas. Here, we overcame this key limitation by presenting participants with 2D ‘Exemplar’ shapes and asking them to draw their own ‘Variations’ belonging to the same class. The drawings reveal that participants inferred—and synthesized—genuine novel categories that were far more varied than mere copies. Yet, there was striking agreement between participants about which shape features were most distinctive, and these tended to be preserved in the drawn Variations. Indeed, swapping distinctive parts caused objects to swap apparent category. Our findings suggest that internal generative models are key to how humans generalize from single exemplars. When observers see a novel object for the first time, they identify its most distinctive features and infer a generative model of its shape, allowing them to mentally synthesize plausible variants.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual perception</kwd><kwd>categorization</kwd><kwd>shape perception</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>222641018-SFB/TRR 135 TP C1</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Roland W</given-names></name><name><surname>Schmidt</surname><given-names>Filipp</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2015-CoG-682859</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Roland W</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Hessian Ministry of Higher Education, Science, Research and Art</institution></institution-wrap></funding-source><award-id>The Adaptive Mind</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Roland W</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Humans are able to generate a complete category of varied objects from just one exemplar shape, identifying and utilizing its most distinctive parts to create a coherent group, even for other observers.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Visual recognition and categorization of objects are vital for practically every visual task, from identifying food to locating potential predators. Humans can rapidly classify objects (<xref ref-type="bibr" rid="bib78">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="bib49">Mack and Palmeri, 2015</xref>; <xref ref-type="bibr" rid="bib66">Serre et al., 2007</xref>), complex scenes (<xref ref-type="bibr" rid="bib82">Wilder et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Fei-Fei et al., 2007</xref>), and materials (<xref ref-type="bibr" rid="bib68">Sharan et al., 2010</xref>) into familiar classes, as well as build new classes when presented with sufficient examples (<xref ref-type="bibr" rid="bib58">Op de Beeck and Baker, 2010</xref>; <xref ref-type="bibr" rid="bib20">Gauthier et al., 1998</xref>). When large numbers of examples are provided, the mathematical basis of human pattern recognition and categorization is well described, and computational models trained with large training sets can emulate many of the human abilities in these areas (<xref ref-type="bibr" rid="bib35">Huang et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Szegedy et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Szegedy et al., 2015</xref>; <xref ref-type="bibr" rid="bib76">Szegedy et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Radford, 2021</xref>; <xref ref-type="bibr" rid="bib44">Kubilius et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Jozwik et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Jozwik et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Jozwik et al., 2019</xref>). However, humans also have the amazing ability to infer new classes when presented with only a small number or even just one single example, as typically occurs when first encountering new concepts (‘one-shot categorization’; <xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>; <xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>; <xref ref-type="bibr" rid="bib56">Ons and Wagemans, 2012</xref>; <xref ref-type="bibr" rid="bib61">Richards et al., 1992</xref>; <xref ref-type="bibr" rid="bib53">Morgenstern et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Ayzenberg and Lourenco, 2021</xref>). Such one-shot learning—which we here use interchangeably with one-shot categorization—is crucial to the formation of human perceptual categories, particularly at early stages of visual development (<xref ref-type="bibr" rid="bib22">Gelman and Markman, 1986</xref>; <xref ref-type="bibr" rid="bib23">Gelman and Meyer, 2011</xref>; <xref ref-type="bibr" rid="bib29">Gopnik and Sobel, 2000</xref>; <xref ref-type="bibr" rid="bib72">Smith and Slone, 2017</xref>; <xref ref-type="bibr" rid="bib85">Yuan et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Gershkoff-Stowe et al., 1997</xref>; <xref ref-type="bibr" rid="bib46">Landau et al., 1988</xref>; <xref ref-type="bibr" rid="bib47">Landau et al., 1998</xref>; <xref ref-type="bibr" rid="bib59">Pereira and Smith, 2009</xref>). Yet how we achieve this remains mysterious and is a significant challenge for artificial learning systems (<xref ref-type="bibr" rid="bib21">Geirhos et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">Zhang et al., 2019</xref>; <xref ref-type="bibr" rid="bib87">Zhang et al., 2021</xref>; <xref ref-type="bibr" rid="bib5">Baker et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Michaelis et al., 2020</xref>).</p><p>From a computational perspective, the ability to infer a new category from just a single exemplar seems virtually impossible: Given only a single exemplar of a category, there is an infinite number of sets containing that exemplar, any of which could in principle be the true category from which the exemplar was drawn. How can we predict the scope of a category without having witnessed any variability?</p><p>The state-of-the-art in psychology and computer science for understanding how humans generalize from few samples suggest that they infer a <italic>generative model</italic>, which considers the observed exemplar as a single sample from a statistical generative process (<xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>; <xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>; <xref ref-type="bibr" rid="bib12">Fei-Fei et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Goodman et al., 2008a</xref>; <xref ref-type="bibr" rid="bib28">Goodman et al., 2008b</xref>; <xref ref-type="bibr" rid="bib45">Lake et al., 2015</xref>; <xref ref-type="bibr" rid="bib74">Stuhlmuller et al., 2010</xref>). In intuitive terms, it is assumed that observers have a ‘deep’ understanding of objects, such that they infer the causes or processes that generated the object (<xref ref-type="bibr" rid="bib19">Fleming and Schmidt, 2019</xref>; <xref ref-type="bibr" rid="bib65">Schmidt et al., 2019</xref>). Importantly, generative models can not only be used to identify behaviourally significant features to judge new samples but can also be used to synthesize or imagine new (i.e., never observed) objects from the same class.</p><p>A key step in the inference of a generative model for a given exemplar is the identification of <italic>diagnostic features</italic> that are informative about the underlying generative processes and therefore category membership. Some features—such as the bilaterally symmetric arrangement of animal limbs—are evidence of lawful processes that structure and describe valid members of the category. At the same time, other features of the observed exemplar—such as the specific pose of the limbs—are free to vary across class members. Differentiating between these generic and non-generic (or ‘non-transverse’), diagnostic features, such as particular relationships between elements in objects (e.g., symmetry) is one important cue to underlying generative processes (<xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>). Other cue features might include statistical outliers in the shape (e.g., a sharp protrusion, or an angle), that makes a local feature stand out within the shape (<xref ref-type="bibr" rid="bib16">Feldman and Singh, 2005</xref>; <xref ref-type="bibr" rid="bib17">Feldman and Singh, 2006</xref>; <xref ref-type="bibr" rid="bib18">Feldman, 2013</xref>; <xref ref-type="bibr" rid="bib57">Op de Beeck et al., 2008</xref>; <xref ref-type="bibr" rid="bib41">Kayaert et al., 2005</xref>), or compared to others seen previously. However, how we identify and interpret such category-defining features and use them for generalization remains a matter of debate (<xref ref-type="bibr" rid="bib67">Serre, 2016</xref>).</p><p>A stumbling block in testing whether humans infer generative models of objects lies in current experimental methods in category learning. Typically, tasks exploring categorization and generalization ask observers to <italic>discriminate</italic> between multiple presented objects (<xref ref-type="bibr" rid="bib1">Ashby and Maddox, 2005</xref>) (with rare exceptions, e.g., <xref ref-type="bibr" rid="bib74">Stuhlmuller et al., 2010</xref>) often varying along binary dimensions (e.g., thin vs. thick, square vs. circle; although see <xref ref-type="bibr" rid="bib32">Hegdé et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Kromrey et al., 2010</xref>). Yet, the very process of presenting multiple objects potentially interferes with how observers perform the task. Rather than using a generative model, observers can solve these types of discrimination tasks simply by comparing how similar objects are to one another, without establishing preferences for particular features, and without synthesizing new variants. Another key limitation is that the experimenter—rather than the observer—determines the range of possible options the observers can choose from, thereby crucially constraining the range of responses. Thus, a question of central theoretical importance about whether humans use internal generative models— and if so, how consistent they are across observers—remains unanswered.</p><p>To overcome these shortcomings by tapping into generative rather than discriminative processes, we used a task in which observers were presented with single Exemplar objects and were asked to explicitly generate (draw) new objects (‘Variations’) from the same category, on a tablet computer. With only one Exemplar present, participants could not derive category-defining features by looking for commonalities between category members. Indeed, as their task did not involve comparing objects at all, they could not rely solely on internal discriminative models. Instead, to create new objects, participants were forced to utilize a generative model extracted from the Exemplar, to derive new category members, unless they chose to simply copy the Exemplar with minor deviations, which was discouraged in the instructions. By analyzing the drawings relative to copies of Exemplars, and by asking other participants to (1) categorize drawings, (2) identify their distinctive features, and (3) compare them with Exemplar shapes, we test whether the participants that drew the Variations truly generalized from single exemplars, and determine which features they relied on to do so, giving us insight into what these generative models look like and how similar they are between observers. The strength of unconstrained drawing tasks has been shown in areas such as memory (<xref ref-type="bibr" rid="bib4">Bainbridge et al., 2019</xref>), recognition (<xref ref-type="bibr" rid="bib11">Fan et al., 2020</xref>), efficient representation of both scenes (<xref ref-type="bibr" rid="bib69">Sheng et al., 2021</xref>) and part structure of objects (<xref ref-type="bibr" rid="bib55">Mukherjee et al., 2019</xref>), and developmental changes in children (<xref ref-type="bibr" rid="bib48">Long et al., 2019</xref>), making it an ideal tool to investigate categorization and one-shot learning.</p></sec><sec id="s2" sec-type="results|discussion"><title>Results and discussion</title><sec id="s2-1"><title>Systematic generalization in a generative one-shot categorization task (Experiments 1 and 2)</title><p>Our first major finding is that participants can synthesize categories of complex objects by generalizing from single exemplars. On each trial, one of eight Exemplar shapes (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) was presented in the upper half of a tablet computer’s screen. Participants were instructed to ‘draw a novel object that is not a copy of the Exemplar, yet which belongs to the same category’ using a digital pencil (<bold>Experiment 1</bold>, <xref ref-type="fig" rid="fig1">Figure 1b</xref>; see Materials and methods). Exemplars were created by the experimenters by combining a main body with varying numbers of parts (ranging from 0 to 5) and displaying a wide range of visual features, such as positive and negative parts (e.g., the indentation in Exemplar 2), curvatures (polygonal and curved shapes), and complexity of parts (e.g., the ‘spike’ in Exemplar 4 vs. the ‘twirl’ in Exemplar 6). The aim was to create Exemplars that are diverse without resembling real-world objects, to maximize generalizations as well as limit the impact of semantic knowledge. While hand-crafting stimuli runs the risk of introducing biases, the intended variety of Exemplars made an algorithmic creation approach unfeasible. For each of the 8 Exemplars, 17 participants (‘drawers’) each drew 12 Variations (yielding a total of 204 Variations per Exemplar, i.e., 1632 drawings overall). To minimize potential carry-over effects of previously seen Exemplars, shapes were shown in randomized order for each participant. As a baseline, another group of participants (<italic>n</italic> = 15) was asked to copy the Exemplars three times as accurately as possible (<bold>Experiment 2b</bold>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Generative one-shot categorization task and results.</title><p>(<bold>a</bold>) Exemplars presented in the Experiments. (<bold>b</bold>) In the task, a group of participants was presented with an Exemplar and asked to draw new objects from the same category (blue). As a measure of baseline performance, another group of participants was asked to copy the Exemplar as accurately as possible (green). (<bold>c</bold>) Examples of drawn Variations (blue) generated in response to one of the Exemplars (yellow). Variations are plotted according to their perceptual similarity to the Exemplar, with more similar Variations closer to the centre. (<bold>d</bold>) Individual differences in ‘creativity’, defined by the average perceived similarity of participant’s drawings to the respective Exemplar. Error bars indicate standard errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>A subset of Variations (dark blue) created for each Exemplar (yellow, centre).</title><p>Variations shown closer to the centre were deemed more similar to the respective Exemplar (<bold>Experiment 2</bold>). The complete set of all Variations and their corresponding similarities to the originating Exemplar can be downloaded from Zenodo.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig1-figsupp1-v1.tif"/></fig></fig-group><p>To assess the range and variety of the generated shapes, a new group of 12 participants rated the similarity of each Variation to the corresponding Exemplar (<bold>Experiment 2</bold>, note that number of data points per participant vary by experiment, for details see Materials and methods). <xref ref-type="fig" rid="fig1">Figure 1c</xref> shows examples of Variations for one Exemplar, with perceptually more similar drawings plotted closer to the Exemplar (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for Variations of other Exemplars). The drawings illustrate the range of responses, with some shapes being very similar to the Exemplar while others differ considerably. <xref ref-type="fig" rid="fig1">Figure 1d</xref> shows the average similarity of produced Variations for each participant, demonstrating that participants varied substantially in how much their drawings deviated from the Exemplar, that is, in their creativity when producing Variations. Linear regression showed that age of drawing (i.e., whether a shape was drawn earlier or later within the sequence of 12 shapes for each Exemplar), did not predict similarity (<italic>R</italic>² &lt; 0.01, <italic>F</italic>(1,10) = 0.08, p = 0.77). Individual participants tended to stay within a constrained similarity spectrum with only small trends in the direction of less or more similar shapes over time with slopes ranging from −0.13 to 0.12 (mean = 0.07).</p><p>To test whether the generated shapes were genuinely new objects or just slightly altered copies of the Exemplars, the copies were compared to a subset of Variations using a 2-AFC task (<bold>Experiment 2b</bold>, see Materials and methods for details), in which a new group of participants (<italic>n</italic> = 15) was asked to pick the shape that looked more like a copy of the Exemplar. In 95% of trials (chance = 50%) the copies were chosen, showing that the vast majority of a representative subset of Variations were perceived to be more different from the Exemplars than a mere copy (one-sided binomial test: 50%; <italic>N</italic> = 16,200, i.e., number of judgements; <italic>K</italic> (correct responses) = 15,388, p &lt;0.001).</p></sec><sec id="s2-2"><title>Responses represent distinct perceptual categories (Experiment 3)</title><p>Next, we tested whether the drawings represented distinct perceptual object categories or were mere random variations that did not form coherent groups. The similarity ratings for each Exemplar’s Variations from <bold>Experiment 2</bold> were split into subsets (40 bins) that spanned the full similarity continuum (see Materials and methods). A new group of participants (<italic>n</italic> = 15) classified one Variation from each bin for each Exemplar (320 stimuli in total), by sorting in each trial a randomly selected Variation into one of the eight Exemplars’ categories. The average percentage of correct classifications was high (86%) and well above chance (one-sided binomial test: 12.5%; <italic>N</italic> = 4800, i.e., number of judgements; <italic>K</italic> (correct responses) = 4111, p &lt; 0.001). <xref ref-type="fig" rid="fig2">Figure 2a</xref> shows the confusion matrix for true classes and participant responses. In almost all instances, observers sorted the Variations correctly into the originating class. Except for a single cell (row 4, column 1), all cells are significantly different from chance with all diagonal cells above—and all others below—chance (one-sided binomial tests with Bonferroni-adjusted p value of 0.015, according to the eight possible outcomes in each row). To investigate whether performance in this task was influenced by the similarity between Variations and Exemplars, the Variations were divided into four similarity bins ranging from very similar to very dissimilar. Performance in the three bins most similar to the Exemplars was virtually identical (overall 89%, 87%, and 88% correct responses), with a maximum of 14% misclassifications in single cells. For the least similar bin, performance dropped to 79%, suggesting that highly dissimilar shapes were significantly more often misclassified, while overall classification accuracy was still way above chance. The maximum of misclassification here was 28% in a single cell (row 4, column 1). This shows that there are almost no systematic miscategorizations between Exemplar categories and demonstrates that the Variations produced in the generative task are samples of robust perceptual categories. Specifically, our results tend to suggest that drawers identified diagnostic features in the Exemplars and reproduced them in the Variations, allowing other observers to identify them as belonging to a common class. Thus, drawers effectively derived distinct novel categories from just a single object.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Drawings constitute a real perceptual category.</title><p>(<bold>a</bold>) The confusion matrix for true classes and participant responses shows that the vast majority of Variations were classified correctly. A subset of Variations that had to be categorized is shown on the right of the matrix with the Exemplars on the bottom. (<bold>b</bold>) Variations largely reproduce the global curvature characteristics of the Exemplar. Curvature similarity across Variations and their Exemplars: Exemplars (yellow) ordered by percentage of perimeter comprised of straight lines, together with average percentage of straight lines in all Variations of that category (blue). Error bars indicate standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig2-v1.tif"/></fig></sec><sec id="s2-3"><title>Identifying category-defining features</title><p>So far, we have shown that drawers generated genuinely novel objects, which other observers could nevertheless assign to their corresponding category. This suggests that drawers identified and reproduced those features of the object that were most diagnostic of the class. Thus, we next investigated which features were preserved, the extent participants agreed about the most significant features, and the importance of these features for category assignment.</p><sec id="s2-3-1"><title>Global curvature features</title><p>One significant feature preserved in almost all Variations was the global curvature, namely whether the Exemplar consisted of straight or curved lines. We find that polygonal Exemplars tended to lead to polygonal Variations while curvaceous Exemplars led to curvaceous Variations (<italic>r</italic> = 0.99, p &lt; 0.001; <xref ref-type="fig" rid="fig2">Figure 2b</xref>). This finding is broadly in line with the concept of non-random features indicating generative processes (<xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>; <xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>): A pencil tracing a random walk is unlikely to draw a straight line, so straight lines are considered evidence of a significant (i.e., non-random) generative process, which are therefore preserved in Variations.</p></sec><sec id="s2-3-2"><title>Part-related features (Experiment 4)</title><p>Considering the Variations in <xref ref-type="fig" rid="fig1">Figure 1c</xref>, another significant feature that seems to be approximately preserved in many drawings is the part structure—that is, arrangement and number of parts—of the Exemplars (this is especially salient when looking at the most similar Variations; see also <bold>Ideas and Speculations</bold>). At the same time, the parts themselves were often modified in size, orientation, or elongation. This signifies a potential strategy that participants might have used in the drawing task, highlighting their generative approach: Starting from perceptual segmentation of the Exemplar into parts, they may have modified these parts (to varying degrees) and ‘put them back together’, either in the original or a different order.</p><p>To test whether participants used this strategy, another group of participants (<italic>n</italic> = 15) was shown one Exemplar at a time, paired with a Variation of either the same or a different category, and asked to identify any corresponding parts between the two shapes. This allowed us to test whether part correspondence is stronger within categories than across categories—and how much of the Exemplar’s part structure was retained in its Variations. When participants perceived parts as corresponding, they delineated each part by drawing a line, thereby ‘cutting’ the shape into two parts on either side of the line, and then choosing one of the parts to indicate the correspondence to a part in the Exemplar.</p><p><xref ref-type="fig" rid="fig3">Figure 3a</xref> shows aggregated mappings of correspondence for two of the categories, with the Exemplar on the left and rows of corresponding Variations in descending similarity to the Exemplar (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1b</xref> for complete set). Each point of the Variations’ contour is coloured the same as the Exemplar’s contour to which it was perceived to correspond the most. If a point was perceived to correspond to a whole section of the Exemplar (e.g., the green ‘nose’ of the Exemplar in the bottom row of <xref ref-type="fig" rid="fig3">Figure 3a</xref>), then it was coloured the same as the circular median point of that section. Colour saturation indicates how often correspondence was seen for each point, with higher saturation indicating stronger correspondence. This visualization shows the generally high level of agreement about corresponding parts between participants. <xref ref-type="fig" rid="fig3">Figure 3b</xref> compares the correspondence in same-category pairs to that in different-category pairs. On average, 71% of an Exemplar’s area corresponded to some part of its Variations, with only 18% correspondence to Variations of other categories (one-sided <italic>t</italic>-test: <italic>t</italic>(1199) = 38.84, p &lt; 0.001, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>). To test whether less similar shapes share fewer parts, we analyzed the corresponding area as a function of the similarity of Variations to Exemplars by linear regression. We find that the extent to which the Variation was seen to correspond was well predicted by similarity (<italic>R</italic>² = 0.92, <italic>F</italic>(1,8) = 96.69, p &lt; 0.001, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>). This also suggests that the extent to which parts or area corresponded might have been used by observers in <bold>Experiment 2</bold> to assess similarity.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Variations and Exemplars of the same category share most of their parts.</title><p>(<bold>a</bold>) Aggregated mapping of correspondences for two categories, showing high agreement between participants about corresponding parts. Colours are explained in main text. (<bold>b</bold>) Comparison of percentage of correspondences seen within (blue) and across (green) categories, showing that significantly more correspondence was perceived within categories. Error bars indicate standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Correspondence data for whole subset.</title><p>(<bold>a</bold>) Left panel: mean area seen to correspond for each category’s Exemplar with Variations from other categories (green bars) and from the same category (blue), showing both the large difference in area corresponding within and across categories and that on average a large majority of each within-category shape pair was seen to correspond. Right panel: mean corresponding area as a function of similarity to the Exemplar (blue, linear regression model in red), showing that corresponding area decreases with decreasing similarity. (<bold>b</bold>) Correspondence mapping for each Exemplar and Variation used in Experiment 4. Variations are ordered from left to right by decreasing similarity to Exemplar (similarities from Experiment 2).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Analysing part order and part changes in the correspondence task (Experiment 4).</title><p>(<bold>a</bold>) Part order analysis and part order types. The left-most panel shows that part order was extracted by moving clockwise from the left-most point along the contour and creating a simplified ‘part circle’ below. Creating such a circle for each shape in a shape pair and comparing them allows to see whether part order was the same, reversed or shuffled, as shown columnwise here. (<bold>b</bold>) Part changes types. Part changes were defined by ‘gaps’ in the circle, that is, sections of the shape not seen to correspond. These resulted in additions (a gap existed in the Variation but not Exemplar), omissions (a gap existed in the Exemplar but not Variation), and substitutions (gaps occupied the same spot in the part order but were not seen to correspond). (<bold>c</bold>) Part order versus similarity. Percentages of part order types along with shapes not used in the analysis (pink line, includes all trials where 0–2 parts were seen to correspond and thus being insufficient to create an alterable part order) with respect to the similarity of the Variation to the Exemplar (as derived from Experiment 2), showing that most shapes with more than 2 corresponding parts had the same part order as the Exemplar. (<bold>d</bold>) Part changes versus similarity. Percentage of part order types, along with shapes with no correspondence (pink line), compared to similarity to the Exemplar, showing that substitutions were the most common part change, increasing with dissimilarity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig3-figsupp2-v1.tif"/></fig></fig-group><p>A significant correlation between the percentage of correct categorizations from <bold>Experiment 3</bold> and the percentage of corresponding area (<italic>r</italic> = 0.53, p &lt; 0.01) indicates that Variations that share more parts with the Exemplar are grouped more often with their Exemplar compared to those sharing fewer parts. Notably, out of the 80 Variations used in <bold>Experiment 4</bold>, 49 were always classified correctly, with a range in corresponding area between 30% and 100%, and a mean of 81%.</p><p>Visual inspection of Variations suggested that Exemplars’ part orderings were often retained in Variations, that is, visually different corresponding parts were arranged in the same sequence. However, in some Variations this order was changed, as if the parts were ‘shuffled’. To test the prevalence of different part orderings in the Variations, we created a compressed representation of part order for each shape pair’s correspondences (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>): Moving clockwise around the shape’s silhouette, we listed each part indicated by the participant, as well as gaps with no correspondences, resulting in a ‘part circle’ describing the order of labelled parts for each shape (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a</xref>). This representation was chosen because Exemplars were created to comprise a main body with parts arranged roughly circularly around it, making the resulting part circles approximately analogous to perceived part ordering. To analyze part order, part circles were then compared to quantify the extent to which orders were identical, reversed, or shuffled (i.e., non-identical). There is an important limitation to this analysis, however. By this definition, part order could only be changed with more than two corresponding parts, amounting to 30% of all trials. For the remaining 70% of trials, this analysis is not possible as with two or fewer corresponding parts, part order is automatically conserved or cannot be defined. This relatively low number is explained by participants often aggregating corresponding parts in their responses, for example, by considering the three ‘non-bitten’ spikes in the top Exemplar in <xref ref-type="fig" rid="fig3">Figure 3a</xref> as a single corresponding part—resulting in only two correspondences even though the shape may have featured more perceptual parts. Of those trials with more than two correspondences, part order was identical in 77%, reversed in 7%, and shuffled in 17% of cases. Analyzing gaps in part circles also provided a tentative measure of how often parts of the Exemplar were omitted (4%), substituted with another non-corresponding part (25%), or new parts were added that did not exist in the Exemplar (8%). Notably, more part substitutions occurred for less similar Variations to the Exemplar (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2d</xref>). For further details of this analysis, see Materials and methods.</p><p>While we have to be careful to generalize to the full set of Variations from this limited number of data points, this analysis nevertheless provides some insight into potential strategies used in the creation of new shapes: Many Variations retained the Exemplar’s part order, sometimes in reversed form, suggesting that the shape was changed from its original form (as opposed to being created completely from scratch) either globally or part by part to create an appreciably different object displaying the same part order. In contrast, Variations with a shuffled part order, or omitted and added parts, point to a ‘building block’ approach where parts are treated as independent, re-combinable elements.</p><p>Together, these findings demonstrate that Exemplars and associated Variations share a considerable portion of their parts, indicating that drawers preserve identifiable parts in their Variations even though they varied specific geometrical properties of these parts. Furthermore, Variations also tended to preserve the order of corresponding parts—with some exceptions where part order was changed—overall pointing to a highly part-based creation approach.</p></sec><sec id="s2-3-3"><title>Identification and preservation of distinctive parts (Experiment 5)</title><p>Another notable feature of the generated shape categories is that certain parts of the Exemplars seem to be more distinctive than others, and that these often also appear in the Variations. We suggest that these distinctive parts are a major driving force for correct categorization (<bold>Experiment 3</bold>). To address whether participants agreed about which shape features are most distinctive, we showed a new group of participants (<italic>n</italic> = 10) all 8 Exemplars together with 39 Variations from each of the categories, in random order. They were asked to mark up to three parts of each object’s silhouette through a painting interface, starting with the most distinctive part, followed by the second and third most distinctive parts. <xref ref-type="fig" rid="fig4">Figure 4a</xref> shows all responses for one shape, together with the aggregated response, which indicates a high level of agreement across observers (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for the complete set). <xref ref-type="fig" rid="fig4">Figure 4b</xref> shows aggregated responses for a subset of Variations per category. Contrasting these data with randomized responses mimicking the number of areas painted and the lengths of consecutive areas painted, but with randomized placement of those areas (see Methods for details), shows significantly higher agreement between humans compared to chance: The mean consecutive area of a shapes’ silhouette with a high distinctiveness score (above 75% of the highest score) for the human data made up 19% of a shape’s perimeter compared to &lt;1% for the randomized data (two-sample Kolmogorov–Smirnov test, p &lt; 0.001). This high agreement among observers is especially noteworthy given that the quite vague concept of ‘distinctiveness’ could be interpreted differently by different observers, pointing to just how important and characteristic these parts are considered compared to the rest of the shape.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Observers agree on the most distinctive parts.</title><p>(<bold>a</bold>) Individual responses of all participants for an example Variation, showing which parts were marked as most distinctive (red), second most distinctive (orange), and third most distinctive (yellow). Aggregating these responses results in the shape on the right. The redder each point in the contour, the more distinctive it was evaluated across all participants. (<bold>b</bold>) Comparing aggregated responses between Exemplars and corresponding Variations (a subset shown here) suggests that in most categories similar parts (e.g., the indentation, spike or twirl) were identified as distinctive across most Variations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Aggregated distinctiveness scores for all shapes.</title><p>The first (top left) shape in each category is the Exemplar, with the following shapes representing all other drawings used in <bold>Experiment 5</bold> in descending order of similarity to the respective Exemplar (similarity derived from <bold>Experiment 2</bold>). Distinctiveness is represented by colour of each point from black (completely indistinctive) through yellow and orange to red (highly distinctive), as shown at the bottom.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Visual inspection suggests that for most categories, participants tend to consistently indicate specific parts as being the most distinctive across different Variations (e.g., indentation for category 2, spike for category 4, jagged feature for category 5, and twirl for category 6). Cross-referencing these data with the part correspondences from <bold>Experiment 4</bold> allowed us to test whether distinctive parts within a category are indeed conserved (i.e., whether the most distinctive parts in each shape are those that are judged to correspond to each other). For each corresponding part pair, average distinctiveness scores of both parts were calculated. Comparing the scores of all Exemplar parts with the scores of all Variation parts of the same category shows a high correlation (<italic>r</italic> = 0.63; p &lt; 0.001), suggesting that distinctive parts remain distinctive even when modified or shuffled within the overall part structure. Equivalently, indistinctive parts in Exemplars tend to remain indistinctive in Variations, lending further support to the finding that participants modify individual parts to create new shapes. This raises the possibility that parts that are perceived to be distinctive are particularly important in determining category membership.</p></sec></sec><sec id="s2-4"><title>Causal role of distinctive parts in categorization (Experiment 6)</title><p>We next sought to test the impact of distinctive parts on category membership more directly. To do this, we created new stimuli from Variations by replacing the most distinctive part—as determined from <bold>Experiment 5</bold>—with the most distinctive part of the Exemplar of another category. For comparison, we created stimuli where we replaced the same Variation’s least distinctive part with the least distinctive part of the Exemplar of another category (<xref ref-type="fig" rid="fig5">Figure 5a</xref>; controlling for perimeter length—see Materials and methods). A new group of participants (<italic>n</italic> = 15) grouped each of these newly generated stimuli (280 with the distinctive, 280 with the indistinctive part swapped) with one of the Exemplars, similar to <bold>Experiment 2</bold>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Distinctive parts are the main categorization cue.</title><p>(<bold>a</bold>) Original drawings (top row) were altered so that either the least distinctive part was swapped with the least distinctive part of a different category (second row) or the most distinctive part was swapped with the most distinctive part of another category (third row). (<bold>b</bold>) Comparison of the percentage of correct categorizations for the original shapes (from <bold>Experiment 2</bold>), swapped indistinctive parts and swapped distinctive parts (error bars indicate standard errors). (<bold>c</bold>) Bar plots showing how often the category of the swapped-in part determined the categorization choice. The indistinctive (left) and distinctive (right) parts are shown in red on the shapes’ silhouette. Bars significantly different from chance (p &lt; 0.01) are marked with an asterisk.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig5-v1.tif"/></fig><p><xref ref-type="fig" rid="fig5">Figure 5b</xref> compares the results with the percentage of correct categorizations in <bold>Experiment 2</bold>. Binomial tests indicate that the proportion of correct responses from <bold>Experiment 2</bold> (86%) was significantly higher than both swap conditions (69% for indistinctive swap, p &lt; 0.001, and 32% for distinctive swap, p &lt; 0.001). Further, we find a large difference in effect sizes: while swapping an indistinctive part does have a small effect (Cohen’s <italic>h</italic> = 0.29), swapping a distinctive part has an immense effect on correct categorizations (Cohen’s <italic>h</italic> = 1.05). This shows that distinctive parts were a driving force in categorization decisions, even though they often comprised only a small percentage of shapes’ contours. <xref ref-type="fig" rid="fig5">Figure 5c</xref> summarizes how often the category of the new, swapped-in part determined the categorization decision, separately for indistinctive and distinctive parts. Performing one-sided binomial tests on each of swapped-part conditions (with a Bonferroni-adjusted p level for 16 tests) shows that most distinctive parts strongly biased participant’s responses toward their categories. In contrast, only one indistinctive part had a significantly positive impact on its category choice, while most indistinctive parts resulted in choices of their category even significantly below chance.</p><p>Previous studies showed that small-to-intermediate fragments of an image are sufficient for correct categorization (<xref ref-type="bibr" rid="bib32">Hegdé et al., 2008</xref>; <xref ref-type="bibr" rid="bib80">Ullman et al., 2002</xref>). These informative fragments were defined by implicit analysis of the statistical distribution of features across the complete object category or categories (i.e., the fragments were learned during a training phase). In contrast, in our experiments, parts were identified as distinctive from just a single piece of data (i.e., one object), consistently across many observers. We suggest that observers may use statistical outliers within shapes (<xref ref-type="bibr" rid="bib51">McCarthy and Warrington, 1986</xref>), or outliers relative to previously seen objects to identify distinctive parts.</p><p>In summary, observers agreed on the most distinctive parts of shapes and used this information as one of the main cues for category membership. In line with this, when creating new shapes, these distinctive parts were reproduced with modifications that retained their specific characteristics making them both distinctive and signifiers of their category.</p></sec><sec id="s2-5"><title>General discussion</title><p>Our ability to organize objects into categories at a glance is of fundamental importance to everyday activities. It allows us to access a wealth of knowledge about objects from previous experiences, rather than having to discover each newly encountered item’s properties de novo. Visual object categorization is typically so effortless that we easily take it for granted. Yet disorders of object perception—such as visual agnosia (<xref ref-type="bibr" rid="bib51">McCarthy and Warrington, 1986</xref>; <xref ref-type="bibr" rid="bib62">Riddoch and Humphreys, 1987</xref>; <xref ref-type="bibr" rid="bib25">Goodale et al., 1991</xref>; <xref ref-type="bibr" rid="bib8">Behrmann and Nishimura, 2010</xref>) —lead to profound deficits. A particularly striking characteristic of healthy human object perception is how rapidly observers learn new categories. Whereas machine-learning systems typically require thousands of examples per category to rival human performance at recognizing objects in photographs (<xref ref-type="bibr" rid="bib35">Huang et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Szegedy et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Szegedy et al., 2015</xref>; <xref ref-type="bibr" rid="bib76">Szegedy et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Radford, 2021</xref>; <xref ref-type="bibr" rid="bib44">Kubilius et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Jozwik et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Jozwik et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Jozwik et al., 2019</xref>), human infants and adults appear to be able to generalize successfully from just a single example of a new category (<xref ref-type="bibr" rid="bib22">Gelman and Markman, 1986</xref>; <xref ref-type="bibr" rid="bib23">Gelman and Meyer, 2011</xref>; <xref ref-type="bibr" rid="bib29">Gopnik and Sobel, 2000</xref>; <xref ref-type="bibr" rid="bib72">Smith and Slone, 2017</xref>; <xref ref-type="bibr" rid="bib85">Yuan et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Gershkoff-Stowe et al., 1997</xref>; <xref ref-type="bibr" rid="bib46">Landau et al., 1988</xref>; <xref ref-type="bibr" rid="bib47">Landau et al., 1998</xref>; <xref ref-type="bibr" rid="bib59">Pereira and Smith, 2009</xref>)—so-called ‘one-shot learning’ (<xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>; <xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>; <xref ref-type="bibr" rid="bib56">Ons and Wagemans, 2012</xref>; <xref ref-type="bibr" rid="bib61">Richards et al., 1992</xref>; <xref ref-type="bibr" rid="bib53">Morgenstern et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Ayzenberg and Lourenco, 2021</xref>).</p><p>One-shot categorization is a formally under-constrained inference problem (<xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>). There are infinitely many sets containing any given exemplar, any of which could be the true category. It is thus remarkable that humans seem to be able to draw consistent conclusions about category membership from such sparse data (<xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>; <xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>). Their judgements presumably reflect assumptions about how—and by how much—objects within a category tend to differ from one another, which constrains the space of variants that are deemed likely. Yet how this occurs remains elusive.</p><p>We suggest that generative statistical models of shape play a key role in one-shot categorization. However, to date, direct evidence for generative models has been scant. Most experimental studies on categorization use sets of pre-selected stimuli to define categories and <italic>discriminative</italic> tasks to test their hypotheses. The main drawback of this approach is that the assumed category-defining features are determined by the experimenter and might therefore differ from the features used in unrestricted categorization decisions. Moreover, discriminative tasks allow for simple strategies based on comparisons between objects, rather than probing the visual system’s internal generative models of objects which are thought to be central to how humans and machines can learn categories from sparse data (e.g., <xref ref-type="bibr" rid="bib19">Fleming and Schmidt, 2019</xref>).</p><p>Here, by contrast, we used a generative one-shot categorization task to tap directly into generative models and human creativity—and to identify category-defining features—by allowing participants to generate their own new category samples, rather than merely selecting between experimenter-generated alternatives. The resulting Variations are thus shaped by the features that participants consider important in the context of their previous experiences and by their internal visual imagery processes. In principle, different observers might consider different features as significant so that across observers no coherent object categories would emerge. However, in our findings this was clearly not the case. We found a high degree of agreement between observers, suggesting general principles of how humans analyze single objects and extrapolate new category members from its features. Participants created Variations for different Exemplars varying in curvature characteristics and number of parts, resulting in a large dataset of over 1600 drawings, which were significantly more variable than mere copies of the Exemplars. Yet, despite the wide variety of Variations, the overwhelming majority of a representative subset of these shapes was correctly grouped with the originating Exemplar by independent observers, showing that our task yielded genuine distinct perceptual categories.</p><p>There are, of course, some limitations to our approach. By asking participants to draw variants, the responses are necessarily limited by participant’s skills with the digital drawing interface. Some people have lesser confidence or ability in drawing, and thus it is possible that some of drawings might not accurately reflect their mental image of a given variant. However, this is somewhat mitigated by the relatively straightforward 2D forms they were asked to draw. No ability to render perspective, shading or other more advanced artistic skills were required. It is also worth noting that there was a substantial difference between copies (which were quite accurate) and variants (which were highly diverse). This suggests that artistic ability did not prevent participants from depicting at least a subset of the varied and novel instances brought to mind by the exemplar.</p><p>Another potential bias introduced by our approach is possible carry-over effects of Exemplars seen in previous trials influencing the drawings of later ones. While we aimed to minimize these effects by randomizing the order of Exemplars, future studies could reduce this further by presenting just a single Exemplar per participant, for example in an online experiment with larger numbers of participants.</p><p>A third potential issue with the method was our use of hand-created experimental stimuli. The presence of distinctive features in some of the stimuli (e.g., the ‘twirl’ in Exemplar 6) was therefore not accidental but the result of a decision by the experimenter, and it could be that such features occur less frequently in natural objects. This limitation could be mitigated in future experiments by creating stimuli through a stochastic process that is less under experimenter control. For example, by training a generative algorithm (e.g., Generative Adversarial Networks <xref ref-type="bibr" rid="bib26">Goodfellow et al., 2014</xref>) on a dataset of natural shapes, it is possible to create stimuli that share statistical properties with natural shapes (<xref ref-type="bibr" rid="bib54">Morgenstern et al., 2021</xref>), although care would need to be taken to rule out stimuli that resemble recognizable items (which often result from such methods). However, even algorithmic stimulus generation does not fully solve the problem. Through their decisions about the algorithm and training set, experimenters necessarily exert some degree of control over the kinds of stimuli presented to participants, even if they do not hand-create or hand-select the individual instances, as we did. Despite our use of hand-drawn Exemplars, the finding that participants could consistently identify distinctive features without any explicit training—and treated these as defining class membership—suggests that both experimenter and participants shared assumptions about which features are important. More broadly, the finding that participants created Variations that other participants recognized as belonging to the same class provides an existence proof for systematic generative processes, irrespective of how frequently such distinctive features occur in natural objects.</p></sec><sec id="s2-6"><title>Ideas and speculation</title><p>How might a generative model operate in practice to allow a participant to synthesize new objects? While it is difficult to probe the exact strategies used to create each Variation, visual inspection of the drawings, as well as the part correspondence task in <bold>Experiment 4,</bold> provide some tentative insights. We summarize our speculations about some of the possible strategies in <xref ref-type="fig" rid="fig6">Figure 6</xref>. For example, we call one strategy ‘Warping’, as many variants appear to be related to the original Exemplar by a relatively straightforward (non-linear) spatial distortion operation that preserves the ordering of points, but alters their relative distances. Through such warping, a wide variety of shapes can be synthesized while largely retaining one-to-one correspondence with features of the prototype from which they are derived.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Examples of proposed strategies used by participants.</title><p>Selected examples representing part-based strategies like warp, shuffle, and other strategies, both successful (i.e., shapes were correctly classified almost all the time) and unsuccessful strategies (i.e., shapes were correctly classified far below average).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig6-v1.tif"/></fig><p>Some strategies seem to involve segmenting the Exemplar into parts, which are then modified to various degrees and recombined to create a Variation (<xref ref-type="fig" rid="fig7">Figure 7</xref>). There is considerable evidence for perceptual organization processes that segment shapes into distinct subcomponents based on their geometrical properties (<xref ref-type="bibr" rid="bib33">Hoffman and Richards, 1984</xref>; <xref ref-type="bibr" rid="bib34">Hoffman and Singh, 1997</xref>; <xref ref-type="bibr" rid="bib70">Siddiqi and Kimia, 1995</xref>; <xref ref-type="bibr" rid="bib9">Biederman, 1987</xref>; <xref ref-type="bibr" rid="bib71">Singh et al., 1999</xref>; <xref ref-type="bibr" rid="bib50">Marr and Nishihara, 1978</xref>) and causal origin (<xref ref-type="bibr" rid="bib73">Spröte et al., 2016</xref>). It thus seems intuitively plausible that parts might play a significant role in the generative process too. This idea is further supported by the following observations: (1) Variations’ parts often corresponded to parts of the Exemplar (<bold>Experiment 4</bold>); (2) part structure was retained or merely shuffled in a significant number of shapes (<bold>Experiment 4</bold>); and (3) distinctive parts were crucial for categorization (<bold>Experiment 6</bold>). As a result, even highly dissimilar shapes still shared 34% of their area with the originating Exemplar (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref> right panel). Importantly, some parts were identified as being highly characteristic of the Exemplar’s category and accordingly were varied in the Variations in ways to remain category defining (<bold>Experiment 6</bold>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Proposed steps of shape creation using a predominantly part-based approach.</title><p>After analyzing shape features and identifying distinctive parts, the individual parts get transformed and re-assembled to form a new shape. In the transformation process, distinctive parts are changed to a lesser extent than other, non-distinctive parts. The creation of a new shape from the individual parts might involve adding, merging, or removing parts, as well as changing their order. Note that this is an illustration of only one strategy (albeit an important one); however, Variations might also be created using non-part-based strategies, where other features such as curvature are varied.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-fig7-v1.tif"/></fig><p>In addition to reusing and changing individual parts, the order of parts also seems to have been retained in many Variations—showing that drawers relied on both individual parts as well as their relationships to create a new object. Indeed, even shapes that appear to be created by a straightforward ‘Warp’ strategy are not inconsistent with a part-based analysis and recombination.</p><p>In other Variations, however, part order was shuffled, illustrating a local strategy in which parts were considered independent and recombined in different order. This part-based approach is also reflected by the omission or addition of parts to the Exemplar, as revealed by our ‘part circle’ analysis. One limitation of the analyses supporting these suggestions is that in <bold>Experiment 4</bold>, observers often (70% of trials) indicated only two or fewer corresponding parts. For these shapes, it is not possible to identify part-order relationships using the ‘part circle’ analysis. This was in part because participants often appear to have bundled together multiple elements—which a part-detection algorithm would treat as discrete—into composite shape elements, effectively processing the parts in concert. Moreover, although the drawings themselves were entirely unconstrained, it should be noted that <bold>Experiment 4</bold> was explicitly focussed on corresponding parts by design, which only allows us to assess part-based strategies.</p><p>Visual inspection of the dataset also suggests additional strategies that are harder to quantify and less decidedly part focussed. Some of these are shown in <xref ref-type="fig" rid="fig6">Figure 6</xref> (top right), which seem to change the Exemplar’s part structure substantially while nevertheless retaining and exaggerating the (statistical) motif of ‘spikiness’. The exact nature of the features that define abstract concepts such as ‘spikiness’—and thus allow observers to relate the Variations to the Exemplar—remain elusive. However, presumably it is necessary to retain certain statistical properties of the shape, such as the prevalence of sharp angles and straight edges. The bottom right panel shows Variations which were less often linked to the original category in <bold>Experiment 3</bold> than the average Variation. Very few if any of the parts in the Exemplar are retained, and curved elements abound despite the original shape being entirely angular. Evidently, eliminating core features—such as parts or straightness—was not a successful strategy for generating legal Variations. Consequently, we suggest that observers analyze the Exemplar with respect to a number of features, ranging from basic properties like curvature to more high-level properties like ‘spikiness’, and then vary one or multiple of these features to create a new object. This is consistent with recent work showing that perceived similarity between shapes can be predicted by considering a large number of shape features (<xref ref-type="bibr" rid="bib54">Morgenstern et al., 2021</xref>). Yet, in general, observers seemed to share assumptions about the extent to which features could be varied while still retaining the categories’ identity, as indicated by the small number of misclassifications in <bold>Experiment 3</bold>. It is interesting to speculate that these assumptions may therefore be derived from the statistics of variations between items in the natural world.</p><p>Many of the generated shapes not only shared most of their parts, but, strikingly, specific parts were also reliably perceived as more distinctive than others and were the main catalyst driving categorization decisions, even if they comprised only a minority of the shapes’ area. This suggests that a crucial stage in one-shot categorization is the identification of those features within an object that are most likely to be ‘distinctive’ for defining the category.</p><p>What is the basis of such inferences? Given only a single exemplar, how are the ‘most distinctive’ features determined? We suggest that observers’ decisions are related to processes that identify signatures of the underlying <italic>generative processes</italic> responsible for creating the observed shape and which thus define the category. This is closely related to Feldman's (<xref ref-type="bibr" rid="bib14">Feldman, 1992</xref>; <xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>) theory of non-accidental features. Statistical relations that are unlikely to occur under a random model (e.g., collinearity of features) are evidence of the operation of non-random generative processes. Similarly, parts that have statistically distinct properties from the rest of the shape (<xref ref-type="bibr" rid="bib65">Schmidt et al., 2019</xref>)—or which are statistically distinct from parts of objects seen previously (<xref ref-type="bibr" rid="bib45">Lake et al., 2015</xref>)—are likely evidence of a category-defining process. Consistent with this idea, we find that parts that were deemed distinctive were also those that had the most significant effect of categorization (e.g., the ‘twirl’ feature in category 6; see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). On a more global scale, curvature characteristics (straight or curved lines) of Exemplars were preserved almost perfectly in most drawings, showing that these features were also deemed non-random. Conversely, random features were altered more freely: Since the presented shapes were abstract, the part arrangement held no meaning and was therefore free to be modified (in contrast to familiar objects like a chair or a human), resulting in some Variations with changed part-ordering from the Exemplar. Yet an important open question is which properties of parts or features are used to determine their status as outliers, and how these properties are determined, based on familiar objects and processing constraints of the visual system. For example, even though distinctiveness is an intuitive concept—as demonstrated by the high agreement by observers in <bold>Experiment 5</bold>—it is challenging to define it formally. A large number of factors can affect the distinctiveness of a part or feature, ranging from purely geometric qualities such as frequency of curvature changes (<xref ref-type="bibr" rid="bib6">Baker et al., 2021</xref>; <xref ref-type="bibr" rid="bib2">Attneave, 1957</xref>; e.g., a tentacle vs. a stub) to comparative analyses within the object (difference between part and other object parts) or across other objects seen before (difference between part and other objects’ parts). More generally, parts might be compared to an internal taxonomy of parts, each weighted with its probability of appearance, potentially informed by frequencies of parts in real-world objects or by their function (<xref ref-type="bibr" rid="bib55">Mukherjee et al., 2019</xref>; <xref ref-type="bibr" rid="bib79">Tversky, 1989</xref>). Another important open question is how parts are parameterized so that identity-preserving variations can be generated. Skeletal representations (e.g., <xref ref-type="bibr" rid="bib17">Feldman and Singh, 2006</xref>) offer a promising avenue for potential representations (<xref ref-type="bibr" rid="bib10">Destler et al., 2019</xref>; <xref ref-type="bibr" rid="bib81">Wilder et al., 2011</xref>; <xref ref-type="bibr" rid="bib82">Wilder et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Wilder et al., 2019</xref>).</p></sec><sec id="s2-7"><title>Conclusions and future directions</title><p>Taken together, the results of our experiments suggest that humans are not merely passive observers, who assign objects to categories based on their relative similarities in a fixed feature space. Instead, a key aspect of human generalization is our ability to identify important signatures of generative processes and then to run internal routines that actively synthesize new objects that we have never actually observed. Thus, although drawing tasks present experimenters with challenges—particularly in terms of analyzing the resulting drawings—they also provide a promising avenue into research not only about object categorization but also human creativity. As seen in <xref ref-type="fig" rid="fig1">Figure 1d</xref>, there is a notable difference between participants as to how much they tended to deviate from the Exemplars, raising the question of what drives these individual differences. Another particularly fascinating open question is the extent to which the synthesis of novel objects by observers recruits physical simulation processes (<xref ref-type="bibr" rid="bib7">Battaglia et al., 2013</xref>). It seems plausible that experience with the natural ways that objects, materials, animals, and plants move and change shape over time (<xref ref-type="bibr" rid="bib63">Schmidt and Fleming, 2016a</xref>; <xref ref-type="bibr" rid="bib73">Spröte et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">Schmidt et al., 2016b</xref>) might influence the types of variations that we tend to imagine (e.g., articulating limbs into different poses). The fact that observers can derive diverse but constrained variations from a single exemplar suggests a deep perceptual understanding about the ways things in the natural world tend to vary.</p><p>In future work, it would also be interesting to analyze the similarities between human drawings (and their similarities to the exemplars) using artificial neural networks trained on datasets of line drawings, such as the ‘Quick Draw!’ dataset (<xref ref-type="bibr" rid="bib36">Jongejan et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">Ha and Eck, 2017</xref>; <xref ref-type="bibr" rid="bib84">Xu et al., 2021</xref>; <xref ref-type="bibr" rid="bib40">Kabakus, 2020</xref>). In particular, each of the human drawings (or exemplars) could be fed into such a network, and a feature vector describing the shape derived, allowing an automated quantitative similarity analysis to be performed. Here, we used human-based ratings to perform such analyses, but this approach does not scale well to larger datasets. Even more intriguing is the possibility of training a neural network model to reproduce the types of generalizations that humans produce when presented with a single exemplar. This would answer questions, such as whether it is necessary to be exposed to human-created line drawings in order to generalize like humans, or whether a visual diet consisting entirely of natural images is sufficient. It would also allow a more rigorous test of whether part-based representations are necessary to capture the full range of human generalization. However, to date, one-shot generalization as exhibited by our observers remains a significant challenge for artificial vision systems.</p></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><p>Participants were recruited through university mailing lists. All participants gave informed consent before the experiments in accordance with the Declaration of Helsinki. The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the Department of Psychology and Sports Sciences of the Justus-Liebig University Giessen (LEK-FB06; protocol code 2016-0007, approved 18 April 2016). <bold>Experiment 1</bold> was conducted on a touchpad computer, all others on a computer.</p><sec id="s3-1"><title>Sample sizes</title><p>In the only studies known by the authors that employed a generative one-shot categorization task (<xref ref-type="bibr" rid="bib15">Feldman, 1997</xref>), the number of objects drawn per Exemplar ranged from 96 to 168. Given that these Exemplars were comparatively simple (e.g., a dot on a line, two connected lines), we aimed for a substantially larger number of drawings per Exemplar (204) to allow all potentially relevant features of our Exemplars to be expressed in the generated objects. Given the novelty of this paradigm, estimating effect sizes for follow-up studies using the drawn shapes proved difficult. For this reason, all follow-up experiments in this study aimed for very high sample sizes to allow the detection of potentially very small effects. For example, in <bold>Experiment 3</bold>, in which chance level is at 12.5%, to detect a significant difference from that level of 2.5% using a one-sided binomial test (with Bonferroni-adjusted alpha = 0.00625 and power = 0.95) we would need a sample size of 3197 trials, a number we have exceeded (4800 trials) for additional sensitivity. Concerning <italic>t</italic>-tests, to detect a small Cohen’s <italic>d</italic> of 0.2 (with alpha = 0.05 and power = 0.95), a minimum sample size of 1084 is necessary (for a one-sided <italic>t</italic>-test), which is exceeded, for example, in <bold>Experiment 4</bold> with 1200 data points per condition.</p></sec><sec id="s3-2"><title>Experiment 1: generative one-shot categorization task</title><p>Seventeen participants drew 12 new shapes for each of the 8 Exemplars (204 shapes per participant, overall 1632 shapes). They were instructed to draw a new object, belonging to the Exemplar’s class, that was not just a mere copy of the Exemplar. The Exemplars were shown in randomized order in the upper part of an iPad Pro (12.9″) oriented in portrait mode and observers’ drawings were on the lower part of the screen. While drawing, participants could switch between drawing freely and drawing straight lines. Participants could clear the current drawing or undo the last drawn segment. After putting down the pen they could only continue drawing from the end-point of the last segment to prevent gaps between lines. After finishing a shape, the drawing area was cleared and a new object could be drawn. A shape could only be finished if the contour was closed, that is, the first and last point drawn were on the same spot. Participants were instructed not to draw any overlapping or crossing lines.</p></sec><sec id="s3-3"><title>Experiment 2: perceived similarity to Exemplars</title><p>For each category, 12 new participants judged the perceived similarity of all drawings created in Experiment 1 to their originating Exemplar. Before making any judgements, all Variations of a category were shown in grids of 3 by 4 shapes consecutively to give the observers an idea of the range of shapes to be judged. In total, participants viewed 17 grids of 12 shapes. They were then presented with the Exemplar on the far left of the screen. On the screen bottom, participants used the mouse to drag and drop 3 randomly chosen Variations from that category into the upper part of the screen and arrange them based on similarity to the Exemplar: the closer the shape was placed on the <italic>x</italic>-axis towards the Exemplar, the more similar it was judged to be. Participants were instructed to try and keep a consistent scale of similarity, meaning that equally similar shapes relative to the Exemplar should be placed in the same area of the screen, across trials and irrespective of how similar the shapes were to each other. After placing three shapes relative to the Exemplar, a button press revealed the next three shapes on the screen’s bottom portion. Already placed shapes were greyed out but could still be adjusted in position if so desired. To prevent the screen from getting cluttered, old shapes disappeared after 4 trials so that only 12 shapes were shown at one time. For each of the participants responses, the similarity values were normalized between 0 (the Exemplar) and 1 (the least similar drawing). The final similarity value of each drawing was averaged across observers over these normalized responses.</p></sec><sec id="s3-4"><title>Picking stimuli from similarity space</title><p>In <bold>Experiments 2b</bold>, <bold>3</bold>, <bold>4</bold>, <bold>5</bold>, and <bold>6</bold>, we selected a subset of shapes from each category ensuring that the subset spanned the whole range of similarities. As an example, a subset of 20 shapes was created by taking all shapes of a category (minus the Exemplar) and dividing them into 20 equally sized bins spanning the perceived similarity space derived in <bold>Experiment 2</bold>. For each bin, we selected the shape with the lowest between-participant variance in similarity judgements. If a bin was empty, neighbouring bins were searched for a shape not yet used.</p></sec><sec id="s3-5"><title>Experiment 2b: comparing copies and new drawings</title><p>Fifteen participants were instructed to copy each Exemplar three times as best as possible with the drawing interface used in <bold>Experiment 1</bold>, resulting in 45 copies of each Exemplar. Then, 15 new participants were shown one of these copies, along with the Exemplar and one Variation of that category per trial. The task was to pick the shape that was a copy of the Exemplar. The 45 Variations per category were selected to span the range of similarities (see <bold>Picking stimuli from similarity space</bold>) and randomly paired with one of the copies. Each category comprised one block of trials with blocks being randomized in order, resulting in 45 × 8 = 360 trials. These trials were repeated three times resulting in 1080 trials overall. In each repeat, the order of blocks and pairings of copies and Variations was randomized.</p></sec><sec id="s3-6"><title>Experiment 3: perceptual category membership</title><p>Fifteen participants sequentially judged which Exemplar category 40 drawings of each category (chosen as described in <bold>Picking stimuli from similarity space</bold>) belonged to, resulting in 320 data points per participant. If participants were unsure about the category membership, they were instructed to pick the category they thought the shape belonged to the most.</p></sec><sec id="s3-7"><title>Experiment 4: corresponding parts experiment</title><p>Fifteen participants participated in this experiment. For each Exemplar, a subset of 10 drawings spanning the range of similarities was chosen (see <bold>Picking stimuli from similarity space</bold>). Participants were shown an Exemplar on the left and a drawing on the right, either belonging to the same or a different category. They were then asked if they saw any corresponding parts between these shapes. If not, the next shape pair was shown. Otherwise they were asked to pick the corresponding parts by first picking the part in the Exemplar and then the corresponding part in the drawing. To pick a part, the two delineating points of the part were to be clicked in succession, creating a line between these points within the shape. After that, the polygon on either side of the line could be chosen as the final part. A part could only be picked if the resulting line between the points did not intersect with other parts of the shape. Instead of picking a part, the rest of the shape (comprised of anything not yet picked) could also be picked, indicated by a red dot at the centroid of the remaining shape. Any section already picked could not be used for another part. Any number of part pairs could be picked, unless the remaining shape was picked, after which the trial was ended, since no more unpicked parts remained.</p><p>Each Exemplar was paired with 10 corresponding and 10 Variations of another randomly chosen category, resulting in 8 × 20 = 160 trials per participant.</p></sec><sec id="s3-8"><title>Part order analysis</title><p>To compare part ordering in Exemplar/Variation pairs we created a simplified representation of parts (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2a</xref>): Starting with the left-most point of a shape and moving in clockwise order, each point was tested whether it was within a designated part, or not. Accordingly, a value or an empty value (or ‘gap’) was added to an array. The array values only changed when encountering a new part (or non-part; i.e., if the current part was not the same as for the previous point). While this is a simplified representation of part order, we believe it to be a sensible approach for the shapes of this study, given that they mainly consisted of one central main body with a differing number of parts attached. This analysis results in a circular array showing the clockwise ordering of parts for each shape (‘part circle’). To compare part order, non-corresponding parts (i.e., gap segments) were ignored, since we were only interested in the order of corresponding parts. By comparing these arrays, we could see whether part ordering was identical, reversed, or shuffled. Since order between two circular arrays could only be different when they consisted of more than two values (i.e., the array [A,B] is considered the same as [B,A]), only trials with more than two correspondences were considered, amounting to 30% of trials. Part changes, for example, omitted or substituted parts of an Exemplar or parts added to the Variation that did not exist in the Exemplar, were defined by gaps in the part order array: A gap in the Exemplar with no counterpart in the Variation at that same spot was an omission. A gap in the Variation but not the Exemplar was considered an addition. If there was a gap at the same spot in both shapes it was defined as a substitution. For this part-change analysis all stimuli where any correspondence was seen was used (85% of trials).</p></sec><sec id="s3-9"><title>Experiment 5: marking distinctive parts</title><p>Ten participants were sequentially shown a subset of 39 shapes (chosen as described in <bold>Picking stimuli from similarity space</bold>) for each category, plus the Exemplar, resulting in 320 trials overall. They were asked to mark the ‘most distinctive areas’ of each shape. They could freely paint on the shape’s silhouette and were not constrained to paint only consecutive parts. After painting the most distinctive part or parts in red they could switch to the next lower distinctiveness tier (orange) and paint the second most distinctive areas. After that they could switch to yellow for the third most distinctive area. At least one part had to be painted red, the lower distinctiveness tiers were optional.</p><p>To aggregate these responses (as in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), each point of the contour was given a score, with each red response adding 3, each orange 2 and each yellow adding 1 to the score of that point. After summing the individual scores, the resultant scores were normalized between 0 (score = 0) and 100 (highest possible score).</p></sec><sec id="s3-10"><title>Creation of randomized responses for comparison with data from Experiment 5</title><p>For each categories’ responses, we computed how often each distinctiveness tier was used (except the first tier, which was always used). In addition, we computed the distribution of lengths of each tier’s consecutive painted areas and the distribution of number of non-consecutive areas per tier and category. With these distributions 10 (the number of participants in <bold>Experiment 5</bold>) randomized responses were created for each of the shapes from <bold>Experiment 5</bold>, meaning the average number of areas and consecutive lengths mimicked the human distribution, while the placement on the shape was random.</p></sec><sec id="s3-11"><title>Experiment 6: swapping distinctive parts experiment. Stimuli</title><p>First, we segmented each shape of <bold>Experiment 5</bold> into one distinctive and one indistinctive part. The distinctive part was the largest consecutive part of the shape with an aggregated distinctiveness score higher than 75 in each point. The indistinctive part was the rest of the shape.</p><p>In ‘distinctive part swapped’ shapes, we swapped the distinctive part with a distinctive part of another categories’ Exemplar. In ‘indistinctive part swapped’ shapes, we swapped an indistinctive part of the shape with the same number of points as the distinctive part that had the lowest average distinctiveness score. Since points were equally spaced along the contour both parts had the same perimeter length. In either case the swapped-in part was rotated and either uniformly compressed or stretched to fit the gap as best as possible.</p><p>In each case a drawing from the subset from <bold>Experiment 5</bold> (excluding the Exemplars) was used as the base shape. In this way for each of the 8 categories, 5 shapes were created with parts swapped from one of each of the other 7 categories both once with the distinctive part and once with the indistinctive part swapped, resulting in 8 × 7 × 2 × 5 = 560 shapes.</p><p>Because this process sometimes created shapes with large self-intersections, the final five shapes for each condition were hand-picked to create well-formed, artefact-free shapes.</p></sec><sec id="s3-12"><title>Procedure</title><p>Fifteen participants then conducted a repeat of the ‘Perceptual category-membership’ experiment with the 560 stimuli (see <bold>Experiment 3</bold>).</p></sec><sec id="s3-13"><title>Perceived Exemplar complexity</title><p>Twenty participants arranged the eight Exemplars in order of perceived complexity from simple to complex. The average rank of these responses was used to order the Exemplars and their categories in most plots of this paper.</p></sec></sec></body><back><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Resources, Software, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Methodology, Resources, Software, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Methodology, Resources, Software, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Funding acquisition, Project administration, Resources, Software, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the Department of Psychology and Sports Sciences of the Justus-Liebig University Giessen (LEK-FB06; protocol code 2016-0007, approved 18 April 2016).</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-75485-transrepform1-v1.pdf"/></supplementary-material></sec><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>All data generated and analyzed are available at zenodo at: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/5230306">https://zenodo.org/record/5230306</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Tiedemann</surname><given-names>H</given-names></name><name><surname>Morgenstern</surname><given-names>Y</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>One shot generalization in humans revealed through a drawing task - Dataset</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5230306</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Research funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation—project number 222641018—SFB/TRR 135 TP C1), by the European Research Council (ERC) Consolidator Award 'SHAPE'—project number ERC-2015-CoG-682859 and by 'The Adaptive Mind', funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Human category learning</article-title><source>Annual Review of Psychology</source><volume>56</volume><fpage>149</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.56.091103.070217</pub-id><pub-id pub-id-type="pmid">15709932</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Physical determinants of the judged complexity of shapes</article-title><source>Journal of Experimental Psychology</source><volume>53</volume><fpage>221</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1037/h0043921</pub-id><pub-id pub-id-type="pmid">13416488</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Lourenco</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The shape skeleton supports one-shot categorization in human infants</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>2316</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.9.2316</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bainbridge</surname><given-names>WA</given-names></name><name><surname>Hall</surname><given-names>EH</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Drawings of real-world scenes during free recall reveal detailed object and spatial information in memory</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-018-07830-6</pub-id><pub-id pub-id-type="pmid">30602785</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>N</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Erlikhman</surname><given-names>G</given-names></name><name><surname>Kellman</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep convolutional networks do not classify based on global object shape</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006613</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006613</pub-id><pub-id pub-id-type="pmid">30532273</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>N</given-names></name><name><surname>Garrigan</surname><given-names>P</given-names></name><name><surname>Kellman</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Constant curvature segments as building blocks of 2D shape representation</article-title><source>Journal of Experimental Psychology. General</source><volume>150</volume><fpage>1556</fpage><lpage>1580</lpage><pub-id pub-id-type="doi">10.1037/xge0001007</pub-id><pub-id pub-id-type="pmid">33332142</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battaglia</surname><given-names>PW</given-names></name><name><surname>Hamrick</surname><given-names>JB</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Simulation as an engine of physical scene understanding</article-title><source>PNAS</source><volume>110</volume><fpage>18327</fpage><lpage>18332</lpage><pub-id pub-id-type="doi">10.1073/pnas.1306572110</pub-id><pub-id pub-id-type="pmid">24145417</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrmann</surname><given-names>M</given-names></name><name><surname>Nishimura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Agnosias</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>1</volume><fpage>203</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1002/wcs.42</pub-id><pub-id pub-id-type="pmid">26271235</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><pub-id pub-id-type="pmid">3575582</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destler</surname><given-names>N</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shape discrimination along morph-spaces</article-title><source>Vision Research</source><volume>158</volume><fpage>189</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2019.03.002</pub-id><pub-id pub-id-type="pmid">30878276</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>JE</given-names></name><name><surname>Wammes</surname><given-names>JD</given-names></name><name><surname>Gunn</surname><given-names>JB</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Relating Visual Production and Recognition of Objects in Human Visual Cortex</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>1710</fpage><lpage>1721</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1843-19.2019</pub-id><pub-id pub-id-type="pmid">31871278</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>One-shot learning of object categories</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>28</volume><fpage>594</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2006.79</pub-id><pub-id pub-id-type="pmid">16566508</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Iyer</surname><given-names>A</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What do we perceive in a glance of a real-world scene?</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/7.1.10</pub-id><pub-id pub-id-type="pmid">17461678</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Constructing perceptual categories</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.1992.223268</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Structure of Perceptual Categories</article-title><source>Journal of Mathematical Psychology</source><volume>41</volume><fpage>145</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1006/jmps.1997.1154</pub-id><pub-id pub-id-type="pmid">9237918</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Information along contours and object boundaries</article-title><source>Psychological Review</source><volume>112</volume><fpage>243</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.1.243</pub-id><pub-id pub-id-type="pmid">15631595</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian estimation of the shape skeleton</article-title><source>PNAS</source><volume>103</volume><fpage>18014</fpage><lpage>18019</lpage><pub-id pub-id-type="doi">10.1073/pnas.0608811103</pub-id><pub-id pub-id-type="pmid">17101989</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><chapter-title>An integrated Bayesian approach to shape representation and perceptual organization</chapter-title><person-group person-group-type="editor"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><source>In Shape Perception in Human and Computer Vision</source><publisher-name>Springer</publisher-name><fpage>55</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1007/978-1-4471-5195-1_4</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Getting “fumpered”: Classifying objects by what has been done to them</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1167/19.4.15</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I</given-names></name><name><surname>Williams</surname><given-names>P</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Tanaka</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Training “greeble” experts: a framework for studying expert object recognition processes</article-title><source>Vision Research</source><volume>38</volume><fpage>2401</fpage><lpage>2428</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00442-2</pub-id><pub-id pub-id-type="pmid">9798007</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><name><surname>Temme</surname><given-names>CRM</given-names></name><name><surname>Rauber</surname><given-names>J</given-names></name><name><surname>Schütt</surname><given-names>HH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Generalisation in Humans and Deep Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1808.08750">https://arxiv.org/abs/1808.08750</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>SA</given-names></name><name><surname>Markman</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Categories and induction in young children</article-title><source>Cognition</source><volume>23</volume><fpage>183</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(86)90034-x</pub-id><pub-id pub-id-type="pmid">3791915</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>SA</given-names></name><name><surname>Meyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Child categorization</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>2</volume><fpage>95</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1002/wcs.96</pub-id><pub-id pub-id-type="pmid">23440312</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershkoff-Stowe</surname><given-names>L</given-names></name><name><surname>Thal</surname><given-names>DJ</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Namy</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Categorization and Its Developmental Relation to Early Language</article-title><source>Child Development</source><volume>68</volume><fpage>843</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8624.1997.tb01966.x</pub-id><pub-id pub-id-type="pmid">29106722</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name><name><surname>Jakobson</surname><given-names>LS</given-names></name><name><surname>Carey</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A neurological dissociation between perceiving objects and grasping them</article-title><source>Nature</source><volume>349</volume><fpage>154</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1038/349154a0</pub-id><pub-id pub-id-type="pmid">1986306</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Pouget-Abadie</surname><given-names>J</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Warde-Farley</surname><given-names>D</given-names></name><name><surname>Ozair</surname><given-names>S</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generative adversarial nets</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>ND</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>A rational analysis of rule-based concept learning</article-title><source>Cognitive Science</source><volume>32</volume><fpage>108</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1080/03640210701802071</pub-id><pub-id pub-id-type="pmid">21635333</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>ND</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Compositionality in rational analysis: Grammar-based induction for concept learning</article-title><conf-name>The Probabilistic Mind: Prospects for Bayesian Cognitive Science</conf-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199216093.001.0001</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gopnik</surname><given-names>A</given-names></name><name><surname>Sobel</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Detecting blickets: how young children use information about novel causal powers in categorization and induction</article-title><source>Child Development</source><volume>71</volume><fpage>1205</fpage><lpage>1222</lpage><pub-id pub-id-type="doi">10.1111/1467-8624.00224</pub-id><pub-id pub-id-type="pmid">11108092</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ha</surname><given-names>D</given-names></name><name><surname>Eck</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Neural Representation of Sketch Drawings</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1704.03477.pdf">https://arxiv.org/pdf/1704.03477.pdf</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegdé</surname><given-names>J</given-names></name><name><surname>Bart</surname><given-names>E</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Fragment-based learning of visual object categories</article-title><source>Current Biology</source><volume>18</volume><fpage>597</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.03.058</pub-id><pub-id pub-id-type="pmid">18424145</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>DD</given-names></name><name><surname>Richards</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Parts of recognition</article-title><source>Cognition</source><volume>18</volume><fpage>65</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(84)90022-2</pub-id><pub-id pub-id-type="pmid">6543164</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>DD</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Salience of visual parts</article-title><source>Cognition</source><volume>63</volume><fpage>29</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(96)00791-3</pub-id><pub-id pub-id-type="pmid">9187064</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Van Der Maaten</surname><given-names>L</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Densely Connected Convolutional Networks</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>4700</fpage><lpage>4708</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Jongejan</surname><given-names>J</given-names></name><name><surname>Rowley</surname><given-names>H</given-names></name><name><surname>Kawashima</surname><given-names>T</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Fox-Gieg</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Quick, 953 Draw!</article-title><source>Experiment</source><ext-link ext-link-type="uri" xlink:href="https://quickdraw.withgoogle.com">https://quickdraw.withgoogle.com</ext-link><date-in-citation iso-8601-date="2022-05-06">May 6, 2022</date-in-citation></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>K</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Cichy</surname><given-names>R</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual features versus categories: Explaining object representations in primate IT and deep neural networks with weighted representational modeling</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>511</elocation-id><pub-id pub-id-type="doi">10.1167/16.12.511</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep convolutional neural networks, features, and categories perform similarly at explaining primate high-level visual representations</article-title><conf-name>2018 Conference on Cognitive Computational Neuroscience</conf-name><pub-id pub-id-type="doi">10.32470/CCN.2018.1232-0</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Marques</surname><given-names>T</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large-Scale Hyperparameter Search for Predicting Human Brain Responses in the Algonauts Challenge</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/689844</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kabakus</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Novel Sketch Recognition Model based on Convolutional Neural Networks</article-title><conf-name>2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA</conf-name><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/HORA49412.2020.9152911</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayaert</surname><given-names>G</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Tuning for shape dimensions in macaque inferior temporal cortex</article-title><source>The European Journal of Neuroscience</source><volume>22</volume><fpage>212</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04202.x</pub-id><pub-id pub-id-type="pmid">16029211</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><conf-name>Advances in neural information processing systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kromrey</surname><given-names>S</given-names></name><name><surname>Maestri</surname><given-names>M</given-names></name><name><surname>Hauffen</surname><given-names>K</given-names></name><name><surname>Bart</surname><given-names>E</given-names></name><name><surname>Hegdé</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Fragment-based learning of visual object categories in non-human primates</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e15444</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0015444</pub-id><pub-id pub-id-type="pmid">21124837</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Neural Networks as a Computational Model for Human Shape Sensitivity</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004896</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id><pub-id pub-id-type="pmid">27124699</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level concept learning through probabilistic program induction</article-title><source>Science (New York, N.Y.)</source><volume>350</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1126/science.aab3050</pub-id><pub-id pub-id-type="pmid">26659050</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Jones</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The importance of shape in early lexical learning</article-title><source>Cognitive Development</source><volume>3</volume><fpage>299</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1016/0885-2014(88)90014-7</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>B</given-names></name><name><surname>Smith</surname><given-names>L</given-names></name><name><surname>Jones</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Object perception and object naming in early development</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(97)01111-x</pub-id><pub-id pub-id-type="pmid">21244958</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Fan</surname><given-names>J</given-names></name><name><surname>Chai</surname><given-names>Z</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Developmental Changes in the Ability to Draw Distinctive Features of Object Categories</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/8rzku</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Palmeri</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The dynamics of categorization: Unraveling rapid categorization</article-title><source>Journal of Experimental Psychology. General</source><volume>144</volume><fpage>551</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1037/a0039184</pub-id><pub-id pub-id-type="pmid">25938178</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name><name><surname>Nishihara</surname><given-names>HK</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>200</volume><fpage>269</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id><pub-id pub-id-type="pmid">24223</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>RA</given-names></name><name><surname>Warrington</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Visual associative agnosia: a clinico-anatomical study of a single case</article-title><source>Journal of Neurology, Neurosurgery, and Psychiatry</source><volume>49</volume><fpage>1233</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1136/jnnp.49.11.1233</pub-id><pub-id pub-id-type="pmid">3794729</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Closing the Generalization Gap in One-Shot Object Detection</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.04267">https://arxiv.org/abs/2011.04267</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgenstern</surname><given-names>Y</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>One-shot categorization of novel object classes in humans</article-title><source>Vision Research</source><volume>165</volume><fpage>98</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2019.09.005</pub-id><pub-id pub-id-type="pmid">31707254</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgenstern</surname><given-names>Y</given-names></name><name><surname>Hartmann</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Tiedemann</surname><given-names>H</given-names></name><name><surname>Prokott</surname><given-names>E</given-names></name><name><surname>Maiello</surname><given-names>G</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An image-computable model of human visual shape similarity</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008981</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008981</pub-id><pub-id pub-id-type="pmid">34061825</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mukherjee</surname><given-names>K</given-names></name><name><surname>Hawkins</surname><given-names>RX</given-names></name><name><surname>Fan</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Communicating semantic part information in drawings</article-title><conf-name>CogSci... Annual Conference of the Cognitive Science Society. Cognitive Science Society (U.S.). Conference</conf-name><fpage>2413</fpage><lpage>2419</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ons</surname><given-names>B</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Generalization of visual shapes by flexible and simple rules</article-title><source>Seeing and Perceiving</source><volume>25</volume><fpage>237</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1163/187847511X571519</pub-id><pub-id pub-id-type="pmid">21771394</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perceived shape similarity among unfamiliar objects and the organization of the human object vision pathway</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>10111</fpage><lpage>10123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2511-08.2008</pub-id><pub-id pub-id-type="pmid">18829969</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural basis of visual object learning</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>22</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.11.002</pub-id><pub-id pub-id-type="pmid">19945336</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>AF</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Developmental changes in visual object recognition between 18 and 24 months of age</article-title><source>Developmental Science</source><volume>12</volume><fpage>67</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2008.00747.x</pub-id><pub-id pub-id-type="pmid">19120414</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning Transferable Visual Models from Natural Language Supervision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>W</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Jepson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>From Features to Perceptual Categories</article-title><conf-name>British Machine Vision Conference 1992</conf-name><fpage>99</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1007/978-1-4471-3201-1</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riddoch</surname><given-names>MJ</given-names></name><name><surname>Humphreys</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>A case of integrative visual agnosia</article-title><source>Brain</source><volume>110 (Pt 6)</volume><fpage>1431</fpage><lpage>1462</lpage><pub-id pub-id-type="doi">10.1093/brain/110.6.1431</pub-id><pub-id pub-id-type="pmid">3427396</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Visual perception of complex shape-transforming processes</article-title><source>Cognitive Psychology</source><volume>90</volume><fpage>48</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2016.08.002</pub-id><pub-id pub-id-type="pmid">27631704</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Spröte</surname><given-names>P</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Perception of shape and space across rigid transformations</article-title><source>Vision Research</source><volume>126</volume><fpage>318</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2015.04.011</pub-id><pub-id pub-id-type="pmid">25937375</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Phillips</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visual perception of shape-transforming processes: “Shape Scission.”</article-title><source>Cognition</source><volume>189</volume><fpage>167</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2019.04.006</pub-id><pub-id pub-id-type="pmid">30986590</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Models of visual categorization</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>7</volume><fpage>197</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1002/wcs.1385</pub-id><pub-id pub-id-type="pmid">26997155</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharan</surname><given-names>L</given-names></name><name><surname>Rosenholtz</surname><given-names>R</given-names></name><name><surname>Adelson</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Material perception: What can you see in a brief glance?</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>784</elocation-id><pub-id pub-id-type="doi">10.1167/9.8.784</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheng</surname><given-names>H</given-names></name><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Where to draw the line?</article-title><source>PLOS ONE</source><volume>16</volume><elocation-id>e0258376</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0258376</pub-id><pub-id pub-id-type="pmid">34748556</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siddiqi</surname><given-names>K</given-names></name><name><surname>Kimia</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Parts of visual form: computational aspects</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>17</volume><fpage>239</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1109/34.368189</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>M</given-names></name><name><surname>Seyranian</surname><given-names>GD</given-names></name><name><surname>Hoffman</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Parsing silhouettes: the short-cut rule</article-title><source>Perception &amp; Psychophysics</source><volume>61</volume><fpage>636</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.3758/bf03205536</pub-id><pub-id pub-id-type="pmid">10370334</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Slone</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Developmental Approach to Machine Learning?</article-title><source>Frontiers in Psychology</source><volume>8</volume><elocation-id>2124</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.02124</pub-id><pub-id pub-id-type="pmid">29259573</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spröte</surname><given-names>P</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual perception of shape altered by inferred causal history</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/srep36245</pub-id><pub-id pub-id-type="pmid">27824094</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stuhlmuller</surname><given-names>A</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Learning structured generative concepts</article-title><conf-name>Cognitive Science Society</conf-name></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Wei</surname><given-names>L</given-names></name><name><surname>Yangqing</surname><given-names>J</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Rabinovich</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Going deeper with convolutions</article-title><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Wojna</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rethinking the Inception Architecture for Computer Vision</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><fpage>2818</fpage><lpage>2826</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.308</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title><conf-name>in Thirty-first AAAI conference on artificial intelligence</conf-name></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tversky</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Parts, partonomies, and taxonomies</article-title><source>Developmental Psychology</source><volume>25</volume><fpage>983</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1037/0012-1649.25.6.983</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname><given-names>S</given-names></name><name><surname>Vidal-Naquet</surname><given-names>M</given-names></name><name><surname>Sali</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual features of intermediate complexity and their use in classification</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>682</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1038/nn870</pub-id><pub-id pub-id-type="pmid">12055634</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Superordinate shape classification using natural shape statistics</article-title><source>Cognition</source><volume>119</volume><fpage>325</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2011.01.009</pub-id><pub-id pub-id-type="pmid">21440250</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Dickinson</surname><given-names>S</given-names></name><name><surname>Jepson</surname><given-names>A</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial relationships between contours impact rapid scene classification</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/18.8.1</pub-id><pub-id pub-id-type="pmid">30073270</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Rezanejad</surname><given-names>M</given-names></name><name><surname>Dickinson</surname><given-names>S</given-names></name><name><surname>Siddiqi</surname><given-names>K</given-names></name><name><surname>Jepson</surname><given-names>A</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local contour symmetry facilitates scene categorization</article-title><source>Cognition</source><volume>182</volume><fpage>307</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.09.014</pub-id><pub-id pub-id-type="pmid">30415132</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>P</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Yin</surname><given-names>Q</given-names></name><name><surname>Song</surname><given-names>YZ</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep Self-Supervised Representation Learning for Free-Hand Sketch</article-title><source>IEEE Transactions on Circuits and Systems for Video Technology</source><volume>31</volume><fpage>1503</fpage><lpage>1513</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2020.3003048</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>L</given-names></name><name><surname>Xiang</surname><given-names>V</given-names></name><name><surname>Crandall</surname><given-names>D</given-names></name><name><surname>Smith</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning the generative principles of a symbol system from limited examples</article-title><source>Cognition</source><volume>200</volume><elocation-id>104243</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104243</pub-id><pub-id pub-id-type="pmid">32151856</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Jiao</surname><given-names>J</given-names></name><name><surname>Xing</surname><given-names>E</given-names></name><name><surname>El Ghaoui</surname><given-names>L</given-names></name><name><surname>Jordan</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theoretically principled trade-off between robustness and accuracy</article-title><conf-name>In International Conference on Machine Learning</conf-name><fpage>7472</fpage><lpage>7482</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Hardt</surname><given-names>M</given-names></name><name><surname>Recht</surname><given-names>B</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Understanding deep learning (still) requires rethinking generalization</article-title><source>Communications of the ACM</source><volume>64</volume><fpage>107</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1145/3446776</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75485.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.05.31.446461" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.31.446461"/></front-stub><body><p>This paper employs innovative approaches to elegantly tackle the question of how we are able to learn an object category with just a single example, and what features we use to distinguish that category. Through a collection of rigorous experiments and analytical methods, the paper demonstrates people's impressive abilities at rapid category learning and highlights the important role of distinctive features for determining category membership. This paper and its approach will be of interest to those who study learning, memory, and perception, while also contributing to a growing field which uses naturalistic drawing as a window into high-level cognition.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75485.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bainbridge</surname><given-names>Wilma</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.31.446461">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.05.31.446461v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;One shot generalization in humans revealed through a drawing task&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Chris Baker as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Wilma Bainbridge (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I really enjoyed the work, and my comments are predominantly ways I think the authors could strengthen their existing claims and better justify their conclusions. I think the experiments were generally well designed and the data interesting.</p><p>1. Associated with public review point 1 – how were the other strategies in Figure 7 defined? At one point, this excerpt is included: &quot;a range of other, more complex strategies were also used to generate shapes that grouped with the Exemplar, and even some that did not&quot;, and it's not particularly clear what this means, or whether these other strategies were participant-identified, data-driven, or speculation on the part of the researchers.</p><p>2. Associated with public review point 1 – &quot;At times the parts were shuffled, resulting in a shape with a different part ordering. Sometimes only a subset of the original parts was used, or parts were added with respect to the Exemplar.&quot; Is this backed by data? Potentially, you could take participant reports of overlapping features from Exp 4, then find the angular distance between the part's location on the Exemplar and its location on the Variation. Similarly, how often were parts omitted? How often were new parts added?</p><p>3. Associated with public review point 2 – I believe that Exp 3 and 4 fill some of this gap, but in E2, the similarity ratings do not tell us much about categorical membership, and to a certain extent, the finding that some Variations are rated more similar to the Exemplar than others is a necessary outcome of the task design. I believe a quick experiment that replicates this task but includes intermixed Variations from multiple exemplars would be useful, It would provide a more valid, continuous metric of similarity, and you would be able to test whether Variations are rated as more similar to their own Exemplar than another.</p><p>4. Associated with public review point 2 – The claim is made that &quot;even when the part-structure was altered substantially, other observers were able to identify corresponding parts between Variations and Exemplar, with a high degree of consistency&quot;. I assume that when the part structure was altered substantially, these would also be the Variations rated as least similar to the Exemplar in E2. To make this claim then, would require an analysis of the low similarity Variations in particular. I imagine the authors have the data to establish this, but if not, the claim should be tempered or clarified. A similar claim is made in another section, that similarly requires empirical support or tempering: &quot;…could be varied while still retaining its identity. Importantly, these assumptions seem to be shared by other observers.&quot;</p><p>5. Associated with public review point 3 – In E5, it is suggested that &quot;these distinctive parts are a major driving force for correct categorization (Experiment 3)&quot;. This is something that could be explicitly tested via cross-experiment comparisons.</p><p>6. Associated with public review point 4 – How were the initial exemplars generated? Do we know anything about their overlap or similarity?</p><p>7. I was tripped up a bit by a few elements in the figures. In Figure 1, 'creativity' is probably a construct that is characterized by more than variance in drawings, so maybe it should just be labelled as something more precise like &quot;Intra-exemplar Similarity&quot;. Similarly in Figure 2, these aren't predicted categories per se, but rather participant responses. In Figure 4, what are the numerical values that correspond to points on the color bar? In Figure 5, &quot;Categorization equals category of swapped in part&quot; could be subtly changed to something like &quot;Categorization consistent with category of swapped in part&quot;.</p><p>8. It would be useful to see some discussion of what exactly defines an individual feature, or what makes a person categorize a feature as distinct. Could it be a function of the set of Exemplars? That is, a feature is distinct if it is not present in the other Exemplars? Is it some metric of deviation from a rounded shape?</p><p>9. This is not necessary for the current paper, but it may be worth feeding these shapes through a neural network pre-trained on line drawings (e.g. sketchRNN, or anything trained on &quot;Quick, Draw&quot;). Then, you could derive a feature vector for each and compute a RDM for all shapes, comparing across all categories.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75485.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I really enjoyed the work, and my comments are predominantly ways I think the authors could strengthen their existing claims and better justify their conclusions. I think the experiments were generally well designed and the data interesting.</p><p>1. Associated with public review point 1 – how were the other strategies in Figure 7 defined? At one point, this excerpt is included: &quot;a range of other, more complex strategies were also used to generate shapes that grouped with the Exemplar, and even some that did not&quot;, and it's not particularly clear what this means, or whether these other strategies were participant-identified, data-driven, or speculation on the part of the researchers.</p></disp-quote><p>We have now significantly expanded and clarified the speculative nature of these additional strategies, and clarify that they might only be a subset of many potential strategies. See the new ‘Ideas and Speculations’ sub-section of the manuscript.</p><disp-quote content-type="editor-comment"><p>2. Associated with public review point 1 – &quot;At times the parts were shuffled, resulting in a shape with a different part ordering. Sometimes only a subset of the original parts was used, or parts were added with respect to the Exemplar.&quot; Is this backed by data? Potentially, you could take participant reports of overlapping features from Exp 4, then find the angular distance between the part's location on the Exemplar and its location on the Variation. Similarly, how often were parts omitted? How often were new parts added?</p></disp-quote><p>As described above, we have added a new part-order analysis to Experiment 4 (also see Figure S 4) and discuss the its results and limitations in the Discussion section. Overall, most Variations analysed for part-order retained the part order of the Exemplar. Some Variations, however, had a shuffled order, with exact percentages per similarity to Exemplar shown in Figure S 4.</p><disp-quote content-type="editor-comment"><p>3. Associated with public review point 2 – I believe that Exp 3 and 4 fill some of this gap, but in E2, the similarity ratings do not tell us much about categorical membership, and to a certain extent, the finding that some Variations are rated more similar to the Exemplar than others is a necessary outcome of the task design. I believe a quick experiment that replicates this task but includes intermixed Variations from multiple exemplars would be useful, It would provide a more valid, continuous metric of similarity, and you would be able to test whether Variations are rated as more similar to their own Exemplar than another.</p></disp-quote><p>We agree that the suggested task would result in a more robust similarity metric, and especially the comparison across categories would be very useful. However, given how often Variations were classified correctly in Experiment 3, we believe that the proposed task would at the same time strongly reduce the number of shapes considered of the same category—resulting in a well-populated stimulus space close to the Exemplar, surrounded by a large, sparsely-populated gap, and more remote locations with all shapes from the other categories. Therefore, this task would be an effective way to filter out ‘unsuccessful’ Variations (which clearly do not seem to belong to the respective Exemplar), however, at the cost of resolution for the more similar shapes. Consequently, we believe that our intra-category similarity space combined with the inter-category discrimination of Experiment 3 does provide sufficient data to support our conclusions. However, we agree with the reviewer that the alternative task they propose would be a useful extension for follow-up studies that are focused on cross-category boundaries or confusions.</p><disp-quote content-type="editor-comment"><p>4. Associated with public review point 2 – The claim is made that &quot;even when the part-structure was altered substantially, other observers were able to identify corresponding parts between Variations and Exemplar, with a high degree of consistency&quot;. I assume that when the part structure was altered substantially, these would also be the Variations rated as least similar to the Exemplar in E2. To make this claim then, would require an analysis of the low similarity Variations in particular. I imagine the authors have the data to establish this, but if not, the claim should be tempered or clarified. A similar claim is made in another section, that similarly requires empirical support or tempering: &quot;…could be varied while still retaining its identity. Importantly, these assumptions seem to be shared by other observers.&quot;</p></disp-quote><p>We addressed these points by an extensive rewriting of this part of the discussion, now taking into account the results of the new part order analysis (Experiment 4, Figure S 4) as well as the analysis targeting the relationship between the percentage of corresponding area with similarity (Figure S 2) (line 462 onwards).</p><disp-quote content-type="editor-comment"><p>5. Associated with public review point 3 – In E5, it is suggested that &quot;these distinctive parts are a major driving force for correct categorization (Experiment 3)&quot;. This is something that could be explicitly tested via cross-experiment comparisons.</p></disp-quote><p>While we agree that cross-experiment comparisons could potentially provide additional evidence to support this claim, we believe we already have stronger evidence than such analyses would provide. To assess the causal role of distinctive parts with cross-experiment comparisons would be challenging, and we believe less informative than what we have done. First, since errors in Experiment 3 were rare, we would have only few data points to analyse mis-categorizations. Second, any comparative analysis would only provide correlational rather than causal evidence. This is why, instead of performing cross-experiment comparisons, we conducted Experiment 6 in which we directly manipulate the presence or absence of distinctive parts. We believe this provides a more direct test of the causal role of distinctive parts in categorization.</p><disp-quote content-type="editor-comment"><p>6. How were the initial exemplars generated? Do we know anything about their overlap or similarity?</p></disp-quote><p>We added a more in-depth explanation how and why we hand-crafted the Exemplars to Experiment 1. It is not trivial to quantify shape similarity, however for the purpose of review, here we have included a very straightforward dissimilarity analysis based on the 1-“Intersection-over-Union” (IOU) metric that is widely used in computer vision. Values of 0 indicate identity (intersection and union of the two shapes are the same), values of 1 indicate no overlap between the shapes. As can be seen, most shapes are substantially different from one another, with 20 out of 28 cross-category comparisons having a dissimilarity value &gt;=0.5. We take this as evidence that the exemplars were substantially distinct from one another, certainly far above discrimination thresholds. However, given the limitations of using IOU-based metrics to capture human shape-similarity, we decided not to include these more detailed quantitative evaluations of the shape in the manuscript.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75485-sa2-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>7. I was tripped up a bit by a few elements in the figures. In Figure 1, 'creativity' is probably a construct that is characterized by more than variance in drawings, so maybe it should just be labelled as something more precise like &quot;Intra-exemplar Similarity&quot;. Similarly in Figure 2, these aren't predicted categories per se, but rather participant responses. In Figure 4, what are the numerical values that correspond to points on the color bar? In Figure 5, &quot;Categorization equals category of swapped in part&quot; could be subtly changed to something like &quot;Categorization consistent with category of swapped in part&quot;.</p></disp-quote><p>We agree and have added quotation marks to point out that we follow a particular definition of ‘creativity’ which is given in the figure captions of Figure 1. Also, we annotated the color bars in Figure 4 to clarify the range and changed the wording in Figure 5.</p><disp-quote content-type="editor-comment"><p>8. It would be useful to see some discussion of what exactly defines an individual feature, or what makes a person categorize a feature as distinct. Could it be a function of the set of Exemplars? That is, a feature is distinct if it is not present in the other Exemplars? Is it some metric of deviation from a rounded shape?</p></disp-quote><p>We have extended the Discussion section and now discussing the concept of distinctive parts as well as theories of what makes a part or feature distinctive in greater detail. See the new “Ideas and Speculation” sub-section.</p><disp-quote content-type="editor-comment"><p>9. This is not necessary for the current paper, but it may be worth feeding these shapes through a neural network pre-trained on line drawings (e.g. sketchRNN, or anything trained on &quot;Quick, Draw&quot;). Then, you could derive a feature vector for each and compute a RDM for all shapes, comparing across all categories.</p></disp-quote><p>This is a fantastic suggestion for a future paper and indeed, in collaboration with deep learning specialists, we are making some initial explorations in this direction. For the purposes of our claims in the current study, we believe that the human-based similarity ratings are sufficient, but it would certainly be exciting to automate the process. To address the comment, we have now added a paragraph discussing DNNs to the end of the manuscript (now called “Conclusions and future work”).</p></body></sub-article></article>