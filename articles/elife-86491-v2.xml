<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86491</article-id><article-id pub-id-type="doi">10.7554/eLife.86491</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Tracking subjects’ strategies in behavioural choice experiments at trial resolution</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-202301"><name><surname>Maggi</surname><given-names>Silvia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6533-3509</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-291807"><name><surname>Hock</surname><given-names>Rebecca M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0917-570X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-291808"><name><surname>O'Neill</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-123524"><name><surname>Buckley</surname><given-names>Mark</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7455-8486</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-291809"><name><surname>Moran</surname><given-names>Paula M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-291810"><name><surname>Bast</surname><given-names>Tobias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6163-3229</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-291811"><name><surname>Sami</surname><given-names>Musa</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-59115"><name><surname>Humphries</surname><given-names>Mark D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1906-2581</contrib-id><email>mark.humphries@nottingham.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ee9ar58</institution-id><institution>School of Psychology, University of Nottingham</institution></institution-wrap><addr-line><named-content content-type="city">Nottingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0458dap48</institution-id><institution>Department of Health &amp; Nutritional Sciences, Atlantic Technological University</institution></institution-wrap><addr-line><named-content content-type="city">Sligo</named-content></addr-line><country>Ireland</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Experimental Psychology, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ee9ar58</institution-id><institution>Department of Neuroscience, University of Nottingham</institution></institution-wrap><addr-line><named-content content-type="city">Nottingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ee9ar58</institution-id><institution>Institute of Mental Health, University of Nottingham</institution></institution-wrap><addr-line><named-content content-type="city">Nottingham</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>01</day><month>03</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e86491</elocation-id><history><date date-type="received" iso-8601-date="2023-01-29"><day>29</day><month>01</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2024-02-23"><day>23</day><month>02</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-08-31"><day>31</day><month>08</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.30.505807"/></event></pub-history><permissions><copyright-statement>© 2024, Maggi et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Maggi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86491-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86491-figures-v2.pdf"/><abstract><p>Investigating how, when, and what subjects learn during decision-making tasks requires tracking their choice strategies on a trial-by-trial basis. Here, we present a simple but effective probabilistic approach to tracking choice strategies at trial resolution using Bayesian evidence accumulation. We show this approach identifies both successful learning and the exploratory strategies used in decision tasks performed by humans, non-human primates, rats, and synthetic agents. Both when subjects learn and when rules change the exploratory strategies of win-stay and lose-shift, often considered complementary, are consistently used independently. Indeed, we find the use of lose-shift is strong evidence that subjects have latently learnt the salient features of a new rewarded rule. Our approach can be extended to any discrete choice strategy, and its low computational cost is ideally suited for real-time analysis and closed-loop control.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision making</kwd><kwd>behavioural strategy</kwd><kwd>Bayesian inference</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rat</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/J008648/1</award-id><principal-award-recipient><name><surname>Humphries</surname><given-names>Mark D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/P005659/1</award-id><principal-award-recipient><name><surname>Humphries</surname><given-names>Mark D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/S025944/1</award-id><principal-award-recipient><name><surname>Humphries</surname><given-names>Mark D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/K005480/1</award-id><principal-award-recipient><name><surname>Buckley</surname><given-names>Mark</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/T00598X/1</award-id><principal-award-recipient><name><surname>Buckley</surname><given-names>Mark</given-names></name><name><surname>Humphries</surname><given-names>Mark D</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/M008770/1</award-id><principal-award-recipient><name><surname>Hock</surname><given-names>Rebecca M</given-names></name><name><surname>Moran</surname><given-names>Paula M</given-names></name><name><surname>Bast</surname><given-names>Tobias</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000837</institution-id><institution>University of Nottingham</institution></institution-wrap></funding-source><award-id>Anne McLaren Fellowship</award-id><principal-award-recipient><name><surname>Maggi</surname><given-names>Silvia</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007155</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Sami</surname><given-names>Musa</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000832</institution-id><institution>Dowager Countess Eleanor Peel Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Sami</surname><given-names>Musa</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new Bayesian algorithm for tracking subjects’ choice strategies on every trial reveals when subjects learn and what they tried while doing so, providing strong evidence that reward- and loss-driven exploration change independently.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Experiments on decision making typically take the form of discrete trials in which a subject is required to make a choice between two and more alternatives (<xref ref-type="bibr" rid="bib49">Packard and McGaugh, 1996</xref>; <xref ref-type="bibr" rid="bib73">Yin and Knowlton, 2004</xref>; <xref ref-type="bibr" rid="bib9">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib17">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Buckley et al., 2009</xref>; <xref ref-type="bibr" rid="bib28">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="bib35">Izquierdo et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Juavinett et al., 2018</xref>). The subject’s goal is to learn by trial and error the target rule leading to the correct choice (<xref ref-type="bibr" rid="bib45">Mansouri et al., 2020</xref>), guided by feedback alone (reward and/or error signals; <xref ref-type="bibr" rid="bib34">Ito et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Amarante et al., 2017</xref>) or with additional predictive stimuli (such as texture or odour; <xref ref-type="bibr" rid="bib70">van Wingerden et al., 2010</xref>; <xref ref-type="bibr" rid="bib71">Wang et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Banerjee et al., 2020</xref>). Such experimental designs can be equally applied whether the subject is primate (<xref ref-type="bibr" rid="bib61">Rudebeck et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Buckley et al., 2009</xref>; <xref ref-type="bibr" rid="bib43">Leeson et al., 2009</xref>; <xref ref-type="bibr" rid="bib63">Shiner et al., 2015</xref>), rodent (<xref ref-type="bibr" rid="bib57">Raposo et al., 2012</xref>; <xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>; <xref ref-type="bibr" rid="bib68">Tait et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Campagner et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Harris et al., 2021</xref>), or insect (<xref ref-type="bibr" rid="bib26">Giurfa and Sandoz, 2012</xref>).</p><p>Whatever the species, neuroscience is increasingly looking to fine-grained analyses of decision-making behaviour (<xref ref-type="bibr" rid="bib41">Krakauer et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Pereira et al., 2020</xref>), seeking to characterise not just the variation between subjects but also a subject’s variability across trials (<xref ref-type="bibr" rid="bib65">Smith et al., 2004</xref>; <xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Ashwood et al., 2022</xref>). When analysing the behaviour of each subject in a choice task, we ideally want to know not only when the subject has learnt the correct rule but also what the subject tried while learning. Rules correspond to particular choice strategies, like ‘turn right’ or ‘press the cued lever’; exploratory decisions made while learning are also typically characterised as choice strategies, like ‘win-stay’. Fully characterising the variation within and between individuals on choice tasks requires that we can ask of any trial: what choice strategy are they using now?</p><p>Classic approaches to analysing performance on decision-making tasks, like psychometric curves, are computed post hoc and implicitly assume a fixed strategy throughout (<xref ref-type="bibr" rid="bib39">Kim and Shadlen, 1999</xref>; <xref ref-type="bibr" rid="bib16">Carandini and Churchland, 2013</xref>). Choice behaviour is though inherently non-stationary: learning, by definition, requires changes to behaviour, humans and other animals switch strategies as they explore tasks, and experimenters switch target rules, introducing new rules or switching previous reward contingencies (<xref ref-type="bibr" rid="bib10">Birrell and Brown, 2000</xref>; <xref ref-type="bibr" rid="bib25">Genovesio et al., 2005</xref>; <xref ref-type="bibr" rid="bib58">Rich and Shapiro, 2007</xref>; <xref ref-type="bibr" rid="bib43">Leeson et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Donoso et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>; <xref ref-type="bibr" rid="bib36">Jang et al., 2015</xref>; <xref ref-type="bibr" rid="bib56">Powell and Redish, 2016</xref>; <xref ref-type="bibr" rid="bib8">Bartolo and Averbeck, 2020</xref>; <xref ref-type="bibr" rid="bib62">Russo et al., 2021</xref>). Some algorithms can detect changes in choice behaviour that correspond to learning (<xref ref-type="bibr" rid="bib65">Smith et al., 2004</xref>; <xref ref-type="bibr" rid="bib67">Suzuki and Brown, 2005</xref>), but they lack information on what strategies subjects use to learn or whether the subject’s learnt strategy matches the target rule.</p><p>We introduce a simple but effective Bayesian approach to inferring the probability of different choice strategies at trial resolution. This can be used both for inferring when subjects learn, by tracking the probability of the strategy matching the target rule, and for inferring subjects’ use of exploratory strategies during learning. We show that our inference algorithm successfully tracks strategies used by synthetic agents, and then show how our approach can infer both successful learning and exploratory strategies across decision-making tasks performed by rats, non-human primates, and humans. The analysis provides general insights about learning: that win-stay and lose-shift, commonly discussed as though yoked together (e.g. <xref ref-type="bibr" rid="bib35">Izquierdo et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Miller et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>), are dissociable strategies; and that subjects’ lose-shift choices show latent learning of new rules. Our approach is computationally efficient, could be used in real time for triggering changes to task parameters or neural stimulation, and is easily extended to more complex decision-making tasks: to aid this we provide an open source toolbox in MATLAB (<ext-link ext-link-type="uri" xlink:href="https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_MATLAB">GitHub</ext-link>, copy archived at <xref ref-type="bibr" rid="bib31">Humphries, 2023a</xref>) and Python (<ext-link ext-link-type="uri" xlink:href="https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_Python">GitHub</ext-link>, copy archived at <xref ref-type="bibr" rid="bib32">Humphries and Powell, 2023b</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Given a subject performing a decision-making task with two or more choices, and the subject’s observed history of choices up to and including trial <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, our goal is to compute the probability <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> that a particular strategy <italic>i</italic> has been used on current trial <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>With this probability, we ask two questions about choice strategies:</p><list list-type="order"><list-item><p>What have the subjects learnt? By defining strategy <italic>i</italic> as one of the available rules in the task, high values for <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> are evidence of successfully learning that rule. This approach can be equally applied to naive subjects during training or to well-trained subjects across rule switches (e.g. to see which other rules they switch between).</p></list-item><list-item><p>What strategies are being tried in order to learn the rule? Subjects use choice strategies to explore the structure of the environment. For example, animals often show some combination of win-stay or lose-shift behaviour at the start of learning (<xref ref-type="bibr" rid="bib36">Jang et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Akrami et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Constantinople et al., 2019</xref>). Potential exploratory strategies include random guessing, history dependence, and information seeking. By choosing strategy <italic>i</italic> to be one of these exploratory strategies, we can track its occurrence.</p></list-item></list><sec id="s2-1"><title>Computing strategy probabilities at trial resolution</title><p>We compute <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> using Bayes theorem to provide an estimate of each probability and the uncertainty of that estimate, given the available evidence up to trial <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mtext>posterior</mml:mtext></mml:mrow></mml:mrow></mml:munder><mml:mo>∝</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mtext>likelihood</mml:mtext></mml:mrow></mml:mrow></mml:munder><mml:mo>×</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mtext>prior</mml:mtext></mml:mrow></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the posterior is the observer’s estimate of the probability of strategy <italic>i</italic> being executed, which we want to obtain; the likelihood is the consistency of the subject’s choices with the strategy we are assessing; and the prior is the initial estimate of the probability that the subject uses strategy <italic>i</italic>. We now define each of the likelihood, prior, and posterior distributions.</p><p>On each trial, there are two observable outcomes for a given strategy <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>: the subject’s choice is either consistent with that strategy (a success, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) or it is not (a failure, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>). The sequence of successes or failures to execute the strategy is then the history of choices <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. As each trial in the sequence has two outcomes the likelihood <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is a binomial distribution.</p><p>Setting the prior as the Beta distribution <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, defined in the range [0, 1], allows us to treat the unknown probability of strategy <italic>i</italic> as a random variable. The Beta distribution is the so-called conjugate prior for the binomial distribution, meaning that the posterior is also a Beta distribution, dramatically simplifying the calculations in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.</p><p>Indeed, so simple that updating the posterior distribution <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> becomes, in practice, elementary arithmetic (Methods): given the parameters of the Beta distribution <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for strategy <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> on the previous trial, and that the choice on trial <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is either consistent (<inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) or not (<inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>) with strategy <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, then update <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The initial prior is defined by the values set for <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. This, then, uses the accumulated evidence up to trial <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> to provide a full trial-by-trial estimate of the posterior probability of strategy <italic>i</italic>, in the Beta distribution <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><p>Using Bayes theorem in this way assumes that the parameter being estimated – here <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> – is stationary. But choice behaviour is often non-stationary, as when a subject switches exploratory strategies while learning, or when an experimenter changes the target rule after learning, forcing a subject to switch away from the learnt strategy. As we show further below, naive use of the Bayesian approach will fail to track these changes.</p><p>We solve the non-stationarity problem by weighting the evidence entered into the Bayesian update by its recency. We keep a running total of past successes to execute strategy <italic>i</italic> up to trial <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, exponentially weighted by how far in the past each success occurred: <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> is the rate of evidence decay. Similarly, we keep a running total of past failures to execute strategy <italic>i</italic>: <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We then obtain the following algorithm for trial <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> (Methods):</p><list list-type="bullet"><list-item><p>Observe whether the choice on trial <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> was consistent (<inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) or not (<inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>) with the execution of strategy <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Update the running totals of evidence, decaying the prior evidence: <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>; and <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Update the parameters for the posterior distribution: <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>The posterior distribution for <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is then the Beta distribution <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. With <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> we recover standard Bayesian inference; the lower <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>, the stronger recent history is weighted. Throughout we use <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula> unless otherwise noted; we show below how the choice of <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> affects strategy inference.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> demonstrates the algorithm using example data from a naive rat performing consecutive sessions on a Y-maze task. In every trial, the rat had to choose either the left or right arm (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), and received reward based on four rules that were applied in sequence: right arm, cued arm, left arm, and uncued arm. These rules switched after 10 consecutive correct trials or 11 correct out of 12 trials. <xref ref-type="fig" rid="fig1">Figure 1b</xref> plots the variables needed to infer the rat’s strategy: its choices, rewards, and the cue location. We used these to compute <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the strategies corresponding to the first three rules, ‘go right’, ‘go cued’, and ‘go left’. <xref ref-type="fig" rid="fig1">Figure 1c</xref> plots the resulting posterior distributions for those three strategies across three sequential trials during the learning of the ‘go left’ rule. These show how the rat consistently choosing the left arm updates <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>go left</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to shift its posterior distribution rightward, while the rat choosing not to go down the right arm when it is cued updates <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>go cued</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to shift its posterior distribution leftward; as a consequence, ‘go left’ becomes the most probable strategy.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>A Bayesian approach to tracking strategies.</title><p>(<bold>a</bold>) Schematic of the Y-maze task. The rat received a reward at the arm end if it chose the correct arm according to the currently enforced rule. A light cue was randomly switched on at the end of one of the two arms on each trial. Rules were in the sequence ‘go right’, ‘go cue’, ‘go left’, ‘go uncued’, and switched after 10 consecutive correct trials or 11 out of 12. (<bold>b</bold>) Example rat performance on the Y-maze task. We plot performance across 14 consecutive sessions: vertical grey dashed lines indicate session separation, while coloured bars at the top give the target rule in each trial. Performance is quantified by the cumulative distributions of obtained reward (grey) and arm choice (black). We also plot the cumulative cue location (brown), to show it is randomised effectively. Choice and cue distributions increase by +1 for right and decrease by −1 for left. Small black arrow indicates the trial <italic>t</italic> shown in panel c. (<bold>c</bold>) Example posterior distributions of three rule strategies for three sequential trials. Vertical dashed coloured lines identify the maximum a posteriori (MAP) probability estimate for each strategy. (<bold>d</bold>) Time-series of MAP probabilities on each trial for the three main rule strategies, for the same subject in panel b. Horizontal grey dashed line indicates chance. We omit ‘go uncued’ for clarity: as it is the complementary strategy of ‘go cue’ (<italic>P</italic>(go uncued) = 1 - <italic>P</italic>(go cued)), so it is below chance for almost all trials. (<bold>e</bold>) Precision (1/variance) for each trial and each of the tested strategies in panel d; note the precisions of mutually exclusive strategies (e.g. go left and go right) are identical by definition. (<bold>f</bold>) Time-series of MAP probabilities for three exploratory strategies. Staying or shifting is defined with respect to the choice of arm (Spatial) or the state of the light in the chosen arm (Cued) – for example if the rat initially chose the unlit arm and was unrewarded, then chose the lit arm on the next trial, this would be a successful occurrence of ‘Lose-Shift-Cued’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>A stationary Bayesian approach fails to track behavioural changes.</title><p>(<bold>a</bold>) Same as <xref ref-type="fig" rid="fig1">Figure 1b</xref>. Example rat performance on the Y-maze task. Each curve shows the cumulative distribution of reward (grey), choice (black), and cue location (brown). Vertical grey dashed lines indicate sessions. Colours at the top indicate the target rule. (<bold>b</bold>) Maximum a posteriori (MAP) probabilities for three strategies across the 14 sessions for the example animal in panel a, when using Bayesian estimation without evidence decay. Note how the estimated probability of ‘go right’ remains the highest through both the switches to the ‘go cued’ and ‘go left’ rules, despite the animal clearly altering its behaviour, as it required 10 consecutive trials (or 11 out of 12) of the correct choice to switch the rule. Horizontal grey dashed line indicates chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Robustness of the Bayesian approach to changes in the initial prior.</title><p>(<bold>a</bold>) Example of uniform and Jeffrey’s distributions used as the prior probability for the Bayesian algorithm. (<bold>b</bold>) Similar to <xref ref-type="fig" rid="fig1">Figure 1d</xref>, maximum a posteriori (MAP) probabilities over trials for the example rat in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. The model is initialised with the Jeffrey’s prior. (<bold>c</bold>) Difference between the MAP probabilities for each strategy when initialised with either the uniform or the Jeffrey’s prior.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig1-figsupp2-v2.tif"/></fig></fig-group><p>To track changes in <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> across all trials we take the best estimate of the probability that strategy <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> is being executed as the maximum a posteriori (MAP) value of <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Plotting the MAP probabilities for each trial in <xref ref-type="fig" rid="fig1">Figure 1d</xref> shows how they capture three key dynamics of choice behaviour: first, learning when the probability the subject is using the target-rule’s strategy becomes large and dominant; second, perseverance, as when learnt strategies persist after the target rule has been changed – for example, ‘go right’ persists as the dominant strategy for over a hundred trials after the reinforced rule has switched to ‘go cued’ around trial 25; and, third, switching, as when the rat switches from ‘go right’ to ‘go cued’ in the session beginning at trial 152, emphasising that choice strategy is non-stationary. Indeed, in these example data, a Bayesian approach without the decay of evidence cannot track rule learning or strategy switching (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). With the decay of evidence, the estimated MAP probabilities, and hence inferred changes in strategy, are robust to the choice of prior distribution (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>We can also summarise the amount of evidence we have for our MAP estimate of <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> by computing the precision – the inverse of the variance – of the full probability distribution: the higher the precision, the more concentrated the Beta distribution is around the MAP estimate of the probability of strategy <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p><p>The choice of scalar summaries (MAP and precision) here was made to facilitate ease of plotting (<xref ref-type="fig" rid="fig1">Figure 1</xref> onwards), quantifying (<xref ref-type="fig" rid="fig2">Figure 2</xref> onwards), and combining across subjects (<xref ref-type="fig" rid="fig3">Figure 3</xref> onwards) the changes to the posteriors <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. As we show below, these scalar summaries were sufficient to reveal striking behavioural changes during learning and exploration; but we emphasise that the full posterior is calculated for each trial, offering rich potential for further applications of this algorithm.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Robust tracking of strategies in synthetic data.</title><p>(<bold>a</bold>) Cumulative distribution of raw behavioural data for the synthetic agent, performing a two-alternative task of choosing the randomly cued option. Curves are as in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. Vertical dashed lines indicate strategy switches by the agent. (<bold>b</bold>) Maximum a posteriori (MAP) values of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the five implanted strategies across trials, for <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula>. Chance is 0.5. (<bold>c</bold>) Precision for the five implanted strategies. (<bold>d</bold>) A decision rule to quantify the effect of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. For every tested strategy, listed on the left, we plot for each trial the strategies with the maximum MAP probability (black dots) and maximum precision (grey circles) across all strategies, and the agent’s actual strategy (orange bars). Maximising both the MAP probability and precision uniquely identifies one tested strategy, which matches the agent’s strategy. (<bold>e</bold>) Algorithm performance across evidence decay rates. Left: successful inference (black) of the agent’s strategy using the decision rule. Right: proportion of trials with the correct detected strategy; grey shading shows <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> values within the top 5%. Standard Bayesian inference is <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. (<bold>f</bold>) Number of trials until a strategy switch is detected, as a function of the number of trials using the first strategy. One line per decay rate (<inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>). Detection was the first trial for which the algorithm’s MAP probability estimate of the new strategy was greater than the MAP probability estimate of the first strategy. (<bold>g</bold>) The decay rate (<inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>) sets an asymptotic limit on the posterior distribution of <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We plot the posterior distribution here for an infinite run of successes in performing strategy <italic>i</italic> at a range of <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.95</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. (<bold>h</bold>) The asymptotic limits on the precision of the posterior distributions set by the choice of decay rate (<inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>). (<bold>i</bold>) The asymptotic limits on the probability that <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> exceeds chance (p = 0.5) in a two-choice task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Maximum a posteriori (MAP) probabilities for agent strategies across changes in decay rate.</title><p>For the synthetic agent performing the five blocks of strategies, each panel plots the MAP values for one strategy, across each trial and each value of the decay rate between 0.5 and 1, in steps of 0.01. Vertical black dashed lines indicate block switches. Horizontal arrows are γ = 0.9.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Slower evidence decay tracks gradual switches in strategy more accurately and robustly.</title><p>(<bold>a</bold>) Model for the gradual switch between two strategies after trial <italic>T</italic> . The probabilities of executing the two strategies change linearly at some rate <italic>m</italic>. Trial <italic>T</italic> is 500 in panels b and c, so that past-history effects have plateaued, giving worst-case performance for detection. Detection of the switch is measured from when the probabilities crossover: when the probability of executing strategy 2 exceeds that of executing strategy 1. (<bold>b</bold>) Accuracy of tracking gradual strategy switches increases with slower evidence decay. We plot the number of trials after the crossover trial until detection as a function of <italic>m</italic>, the rate of change in the execution probabilities. Negative values indicate detection before the crossover of probabilities; these false positive detections were due to a chance run of trials using the new strategy 2. Plots show means and standard deviation (SD) over 50 repeats; γ = 1 omitted as detection scales with <italic>T</italic> when there is no evidence decay. (<bold>c</bold>) Stability of the detected switch increases with slower evidence decay: the proportion of trials after the detection for which the algorithm reported the new strategy (strategy 2) was dominant. Plots show means and SD over 50 repeats.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Slower evidence decay gives lower variability when tracking stochastic use of a strategy.</title><p>(<bold>a</bold>) Estimating the stochastic probability of using a strategy. For three different probabilities of using a strategy (<italic>p</italic>(use)), we plot the mean and standard deviation (S.D.) of the algorithm’s MAP estimate of that test strategy, over 50 simulations of 50 trials. Results shown for γ = 0.5. (<bold>b</bold>) As for (<bold>a</bold>), for γ = 0.9. (<bold>c</bold>) Variation in the MAP estimates of <italic>p</italic>(use) decreases with slower rates of evidence decay (higher γ). S.D. taken from trial 50 of 50 simulations similar to those in panels a and b.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Algorithm performance for tracking a new conditional strategy.</title><p>Each panel plots the number of trials needed to track <italic>P</italic>(strategy<italic>i</italic>(<italic>t</italic>)|choices(1 : <italic>t</italic>)) above chance (0.5) after the appearance of a new conditional strategy whose condition occurs with probability p(meet condition). The probability of executing the new conditional strategy when its condition is met linearly increases from 0 to 1 with rate <italic>m</italic>. The right-most panel is the abrupt case (<italic>m</italic> = ∞) where the strategy’s probability is immediately 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Limits on the probability of the tested strategy matching a true strategy used by an agent during exploration.</title><p>(<bold>a</bold>) The cumulative distribution of <italic>p</italic>(match) between the tested strategy and a true strategy when both are unconditional. For a task with <italic>n</italic> options <italic>p</italic>(match) can take one of three values: 0 (tested and true are mutually exclusive); 1/<italic>n</italic> (tested strategy chooses one option); 1 −1/<italic>n</italic> (the tested strategy chooses any but one option). (<bold>b</bold>) The distribution of <italic>p</italic>(match) when the tested strategy is unconditional, but one true strategy is conditional (blue: ‘All’). We evaluated <italic>p</italic>(match) using probabilities between 0.1 and 1 that this true strategy’s condition is met; and we allowed that the tested strategy could exactly match this true strategy either when the condition is met or when it is not. Not allowing this exact matching gives the distribution in purple, with a maximum at 1 − 1/<italic>n</italic>. (<bold>c</bold>) The full distribution of <italic>p</italic>(match) for when the tested strategy is unconditional, given by the combination of the relevant distributions in panels (<bold>a, b</bold>). Because we know the type of tested strategy, but not the type of true strategy used by a subject, these distributions place bounds on what we can infer from the maximum a posteriori (MAP) probability of a tested unconditional strategy. (<bold>d–f</bold>) As for (<bold>a–c</bold>), for when the tested strategy is conditional. (<bold>g–l</bold>) As for (<bold>a–f</bold>), for a task with <italic>n</italic> = 3 options. Note the maximum <italic>p</italic>(match) at 1 − 1/<italic>n</italic> (vertical dashed grey line) when the logical equivalence between tested and true strategies is omitted.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig2-figsupp5-v2.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Detecting learning.</title><p>(<bold>a</bold>) Schematic of Y-maze task, from <xref ref-type="fig" rid="fig1">Figure 1a</xref>. (<bold>b</bold>) Maximum a posteriori (MAP) estimate of <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the correct strategy for the spatial (left panel) and cued (right panel) rules on the Y-maze, aligned to the learning trial for each subject (trial 0, vertical dashed line). Learning trials are defined by the experimenters’ original criterion (grey) or by our strategy criterion (orange). Curves represent means (bold line) ± standard error of the mean (SEM; shading) for <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> sessions meeting each criterion across the four rats (Spatial rule: original criterion <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mstyle></mml:math></inline-formula>, strategy criterion <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mstyle></mml:math></inline-formula>; Cued rules: original criterion <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mstyle></mml:math></inline-formula>, strategy criterion <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mstyle></mml:math></inline-formula>). Horizontal dashed line is chance. (<bold>c</bold>) Comparison between learning trials in the Y-maze task identified using the original criterion or our strategy criterion. Left: the identified learning trials for each rule. Trial zero was the first trial with that rule enforced, and rules are labelled in the order they were presented to the rats. Right: number of identified learning trials per rat. (<bold>d</bold>) Schematic of the lever-press task. Rats experienced both spatial (e.g. choose left lever) and cued (e.g. choose lit lever) rules. (<bold>e</bold>) As for panel b, for the lever-press task. <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>28</mml:mn></mml:mstyle></mml:math></inline-formula> rats. (<bold>f</bold>) Comparing the learning trials identified in the lever-press task by the original criterion or our strategy criterion for the first (left) and second (right) rule learnt by each rat. Solid symbols show mean and SEM; open symbols show individuals’ learning trials. p-values are from a Wilcoxon signed rank test between the learning trials identified by the original and by the strategy criterion (<inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>28</mml:mn></mml:mstyle></mml:math></inline-formula> rats). For the first rule, the learning trials based on the original and strategy criterion are 88.9 ± 12.3 and 59 ± 11.3 (mean ± SEM), respectively. For the second rule, the learning trials based on the original and strategy criterion are 224.3 ± 33 and 272 ± 33 (mean ± SEM), respectively. Note the second rule’s <italic>y</italic>-axis scale is a factor of 2 larger than for the first rule, because of the time taken to learn the cued rule second.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Performance of further learning criteria.</title><p>‘Criterion’ and ‘Strategy’ criteria are those shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> for classic trial-counting and maximum a posteriori (MAP) estimates contiguously above chance, respectively. We compare those here to two more stringent criteria: ‘Strategy 2’: the first trial at which the MAP estimate for the rule strategy is above chance and the precision of the posterior distribution is greater than all others; ‘Strategy 3’: the first trial at which the probability of the posterior distribution <italic>P</italic>(strategy<italic><sub>i</sub></italic>(<italic>t</italic>)|choices(1 : <italic>t</italic>)) containing chance fell below some threshold θ = 0.05. (<bold>a</bold>) For the Y-maze task, the MAP estimate of <italic>P</italic>(strategy<italic><sub>i</sub></italic>(<italic>t</italic>)|choices(1 : <italic>t</italic>)) for the target-rule strategy around learning defined by the four tested learning criteria. Lines represent means over sessions; shading shows standard error of the mean (SEM). (<bold>b</bold>) The number of learning trials per animal identified by each criterion. (<bold>c</bold>) As for panel a, for the lever-press task. (<bold>d</bold>) The identified learning trials in the lever-press task according to each criterion. The task had equal numbers of animals learn an egocentric or cued lever-press rule first, then switched to the other rule; the plots group animals by their experienced rule sequence. A three-factor analysis of variance (ANOVA) on criterion, rule type, and rule order confirmed a significant effect on learning trials of the choice of learning criterion (<italic>F</italic> = 8.91, p &lt; 0.001); it confirmed that learning trials depended on rule type (<italic>F</italic> = 297.18, p &lt; 0.001) and rule order (<italic>F</italic> = 270.12, p &lt; 0.001); and it confirmed that the sequence of rules affected the speed of learning for all tested criteria, with cued rules taking substantially longer to learn after first learning a spatial rule (the interaction of rule order × rule type, <italic>F</italic> = 250.05, p &lt; 0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig3-figsupp1-v2.tif"/></fig></fig-group><p>The above focused on strategies corresponding to the target rules to learn. We can equally define models for strategies that are explorative, to track the strategies the subject engages while learning the rule. For example, a subject’s strategy of win-stay based on their previous choice would be: count a success if trial <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> was correct and the subject made the same choice on trial <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>; count a failure if trial <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> was correct and the subject made a different choice on trial <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig1">Figure 1f</xref>, we plot MAP probabilities for three such explorative strategies, win-stay and lose-shift for the prior choice of left or right arm, and lose-shift based on the prior cue position. This demonstrates when exploratory strategies are used to guide behaviour; for example, of these strategies, lose-shift in response to the choice of arm begins to dominate from around trial 50–150 (see <xref ref-type="fig" rid="fig1">Figure 1f</xref>).</p><p>Throughout we distinguish rule strategies, which correspond to one of the reinforced rules available in the task, from exploratory strategies, which are all other possible choice strategies given task information. The choice of strategies is down to the user (Methods).</p></sec><sec id="s2-2"><title>Robust tracking of strategies in synthetic data</title><p>We turn now to three questions about the algorithm and its use: first, why do we need to decay evidence? Second, how does the choice of decay rate <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> affect our inferences of <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>? And, third, what happens if our chosen strategies do not include a true strategy? We show that decaying evidence is necessary to track strategy changes; that there is a range of <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> over which performance is robust; and that the absence of a true strategy sets upper bounds on the algorithm’s estimate of <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><p>First, to demonstrate the need to decay evidence, we simulated an agent doing a two-alternative forced-choice task. The agent was rewarded when it chose the randomly cued option (brown curve in <xref ref-type="fig" rid="fig2">Figure 2a</xref>). The agent switched its strategy every 100 trials, across five strategies, with only the fourth strategy corresponding to the rewarded rule (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). This set of implanted strategies included a range of options examined in behavioural studies, often individually, and we used many consecutive trials with the same strategy to explore the full effects of the decay rate <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. We inferred <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for eight strategies in total, including the five implanted ones, using the agent’s choice data. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows how the algorithm’s MAP estimate of <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (panel b) and its precision (panel c) tracked these strategy switches when using a decay rate of <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula>. <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> plots every strategy’s MAP estimates at all tested decay rates.</p><p>We used a simple decision rule to quantify how the decay rate <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> affected the algorithm’s tracking of strategies (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). This rule selected the most likely strategy being executed at trial <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> by choosing strategy <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> with the highest MAP estimate of <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, resolving ties by choosing the strategy with the maximum precision (<xref ref-type="fig" rid="fig2">Figure 2d</xref>).</p><p><xref ref-type="fig" rid="fig2">Figure 2e</xref> plots the trials on which the algorithm was correct using this decision rule. We see standard Bayesian inference (<inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) could not detect any switches in strategy. But with <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula> or less, the algorithm detected switches in 10 trials or fewer, and tracked the correct strategy in more than 88% of trials (grey shading in <xref ref-type="fig" rid="fig2">Figure 2e</xref>).</p><p>We then used extensive simulations to quantify how the number of trials <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> for which a first strategy is used affects the algorithm’s ability to track an abrupt switch to a second strategy (Methods). Standard Bayesian inference scaled linearly, taking another <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> trials to detect the switch (<xref ref-type="fig" rid="fig2">Figure 2f</xref>, <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>). Using evidence decay, our algorithm detects abrupt switches orders-of-magnitude faster, and at a rate that rapidly plateaued with increasing <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). Decaying evidence thus ensures that how long a first strategy was used little impacts the algorithm’s ability to track a switch to a new strategy. And while stronger evidence decay (smaller <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>) increased the speed at which the abrupt switch was detected, even for <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula> the worst-case performance was 10 trials (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). Collectively, these results show how decaying evidence is necessary to detect switches in strategy, and that the algorithm can rapidly track abrupt switches in strategy for a wide range of <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>We could then turn to our second question: If the decay of evidence is essential, how rapid should it be? Choosing <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> defines asymptotic limits on the posterior distribution because it limits the values that <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> can take when there is a long run of successes or failures to execute a strategy (Methods, <xref ref-type="disp-formula" rid="equ16 equ17">Equations 15 and 16</xref>). These limits do not stop the algorithm correctly estimating the MAP value of <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>; for example, for a long run of successes, the MAP estimate of <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> will always be 1 (<xref ref-type="fig" rid="fig2">Figure 2g</xref>). But these limits do mean that, in contrast to standard Bayesian inference, rapid decay <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> places bounds on how concentrated the posterior distribution of <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> can get around its maximum value (<xref ref-type="fig" rid="fig2">Figure 2g–i</xref>).</p><p>The main consequence of these bounds is that slower decay of evidence makes the algorithm more robust to noise. We see this robustness in two crucial uses of the algorithm, tracking gradual switches between two strategies and tracking the stochastic use of a strategy. In <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, we show that for gradual switches slowly decaying evidence (<inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>∼</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula>) correctly identifies when the new strategy has become dominant, and consistently tracks its use thereafter; by contrast, rapidly decaying evidence (<inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) prematurely identifies when the new strategy has become dominant and inconsistently tracks it use thereafter. In <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, we show that for the stochastic use of a strategy <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> with some probability <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the average MAP estimate <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> will converge to <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; but we also show that the more rapidly evidence is decayed the greater the trial-to-trial variation in <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. Choosing the decay rate <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> is thus a trade-off between the speed (<xref ref-type="fig" rid="fig2">Figure 2f</xref>) of tracking changes in strategy and the accuracy of that tracking (<xref ref-type="fig" rid="fig2">Figure 2g</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Our use of <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mstyle></mml:math></inline-formula> here is thus motivated by favouring robust yet still rapid tracking.</p><p>Tracking exploratory strategies presents a further challenge, our third question above: The set of exploratory strategies we test may not include a true strategy used by a subject. Exploratory strategies come in two forms: like rule strategies they can be unconditional, available for use at every decision, such as repeating a previous choice; but unlike rule strategies they can be conditional, only available for use when their condition or conditions or met, such as win-stay. We first confirmed that our algorithm could rapidly track the use of conditional strategies (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). We then sought to place bounds on how the inference algorithm behaves if a true exploratory strategy is missing.</p><p>What then would the algorithm’s MAP estimate of <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> be for any kind of strategy <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> that is not the true exploratory strategy? The general answer to this is the probability <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>match</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> that the choice made under strategy <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> matches the choice made under the true strategy. By definition <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>match</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, otherwise strategy <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> and the true strategy are indistinguishable to the observer. What we are interested in is how close <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>match</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> could grow to 1, and thus how hard it would be to tell when we do not have the true strategy. Exhaustively enumerating the possible values of <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>match</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for a task with <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> choices revealed three bounds (we derived these from the last column in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>; see Methods for details of enumeration). First, if the algorithm estimates the MAP probability of all tested exploratory strategies to be <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> then the true strategy or strategies are missing. Second, an MAP probability <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a tested exploratory strategy is evidence for at least a partial match to the true strategy (e.g. in <xref ref-type="fig" rid="fig2">Figure 2b</xref>, when the true strategy adopted by the agent was ‘lose-shift-spatial’ the tested strategy of ‘alternate’ had an MAP probability of around 0.75, because both strategies will change choice on the trial after a loss, thus matching on all post-loss trials but not all post-reward trials). Indeed, our preferred way of interpreting exploratory strategies is that they provide evidence about what task features, such as obtaining reward (or not), and the subject’s response to them, such as staying or shifting, are driving exploration; the algorithm’s estimate of <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is thus a read-out of the evidence. Third, we find that an MAP probability approaching 1 is strong evidence that the tested strategy is, or is equivalent to, the true strategy.</p><p>Collectively these results show that our approach, combining Bayesian inference with a rate of forgetting evidence used in that inference, can track non-stationary probabilities of strategy use at trial resolution. While the forgetting rate is essential (<xref ref-type="fig" rid="fig2">Figure 2a–f</xref>), the effects it has on the behaviour of the posterior distribution (<xref ref-type="fig" rid="fig2">Figure 2g–i</xref>) and on the ability to correctly track switches in strategy (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), give us a principled range of forgetting rates to consider, and over which the algorithm’s performance is robust. With these insights to hand, we now apply our inference algorithm to uncover the choice strategies used in tasks performed by rats, humans, and monkeys.</p></sec><sec id="s2-3"><title>Trial-resolution inference reveals evidence of earlier learning</title><p>Classic approaches to detecting learning depend on arbitrary criteria based on counts of correct trials (e.g. <xref ref-type="bibr" rid="bib10">Birrell and Brown, 2000</xref>; <xref ref-type="bibr" rid="bib11">Boulougouris et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Floresco et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Leeson et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Peyrache et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>). Such criteria are typically conservative, requiring long sequences or high percentages of correct trials to give unambiguous evidence of learning. We thus asked whether our trial-resolution algorithm could provide a more principled approach to detecting learning.</p><p>To investigate this question, we used data from rats learning cross-modal rule-switch tasks. We compared data from two conceptually similar tasks, the first involving the choice of one of two arms in a Y-maze (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, data from <xref ref-type="bibr" rid="bib53">Peyrache et al., 2009</xref>) and the other the choice of one of two levers in an operant box (<xref ref-type="fig" rid="fig3">Figure 3d</xref>; task design from <xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>). In both tasks, the rat had to learn at least one spatial rule based on the position of the choice option (arm or lever) and at least one cue-driven rule, based on the position of a randomly illuminated cue light. As noted above, the Y-maze task contained a sequence of up to four rules, two spatial (left/right) and two cue-driven. In the data available to us, three rats experienced all four rules and one rat experienced the first two rules, giving a total of seven switches from spatial to cue-driven rules and three switches from cue-driven to spatial rules. In the lever-press task, rats were trained on one spatial and one cue-driven rule: 16 rats experienced a switch from a spatial to a cue-driven rule, and 12 experienced the switch from a cue-driven to a spatial rule (Methods).</p><p>Classic definitions of learning the target rule were originally used for both tasks. For the Y-maze study, learning was defined post hoc by three consecutive correct trials followed by 80% correct until the end of the session, with the first of the three trials then labelled as the learning trial (<xref ref-type="bibr" rid="bib53">Peyrache et al., 2009</xref>). For the lever-press task, learning was defined on-line as 10 consecutive correct trials, after which the session ended (<xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>) – the first of these ten we label as the learning trial. To assess how conservative these criteria were, we computed the probability <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the strategy <italic>i</italic> corresponding to the target rule and found it was consistently above chance before these classically defined learning trials for both the Y-maze (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, ‘Criterion’) and lever-press tasks (<xref ref-type="fig" rid="fig3">Figure 3e</xref>, ‘Criterion’). This suggests rules were learnt notably earlier than detected.</p><p>We thus tested a sequence-based criterion using <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to identify the learning trial: we found the first trial at which the MAP probability for the target-rule’s strategy remained above chance (0.5) until the end of the session (Methods). Using this criterion, the change in probability around the learning trial showed clearer steps from chance to above-chance performance in both tasks (<xref ref-type="fig" rid="fig3">Figure 3b, e</xref>, ‘Strategy’).</p><p>In the Y-maze task, this strategy-based criterion identified that learning occurred 12 times across the four rats, compared to 10 times using the original criterion (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). In particular, unlike the original criterion, the strategy criterion suggested that all three rats which experienced the ‘go left’ rule learnt it, and one rat learnt the ‘go uncued’ rule.</p><p>For the lever-press task, we could compare the learning trials identified by both approaches because every rat was trained to the original criterion. The strategy criterion robustly identified learning earlier, on average 30 ± 5.8 trials earlier, during learning of the first target rule (<xref ref-type="fig" rid="fig3">Figure 3f</xref>, left). In contrast, both criteria identified similar learning trials during the second target rule each rat experienced (<xref ref-type="fig" rid="fig3">Figure 3f</xref>, right). They thus both agreed that learning the cued rule after a spatial rule took longer than learning a spatial rule after the cued rule (<xref ref-type="fig" rid="fig3">Figure 3f</xref>; a two-factor analysis of variance on the strategy criterion showing significant effects of rule type [<italic>F</italic> = 74.73, <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>], rule order [<italic>F</italic> = 45.36, <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>], and their interaction [<italic>F</italic> = 54.74, <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>]).</p><p>The strategy-based criterion was analogous to classical criteria for learning as it used only a scalar estimate of performance. We thus explored two other criteria for learning (Methods) that also incorporated the uncertainty in the estimate of <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The first extended the sequence-based definition to also require that the precision of the target-rule’s strategy was greater than all other tested rule strategies, indicating greater evidence for that strategy. The second defined the learning trial as the first at which the probability the posterior distribution <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> did not contain chance was above some high threshold (here 0.95) until the end of the session. We found, as expected, that learning took longer to detect with these increasingly stringent criteria for the level of uncertainty (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Indeed, the full posterior criterion seemed to identify expert performance in both tasks (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a and c</xref>), so could itself be a useful criterion for making rule switches. Consequently, for the lever-press task, both these more stringent criteria did not identify learning markedly earlier than classic sequence criterion (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1d</xref>). Nonetheless, these more stringent criterion still showed that animals were significantly slower to learn when switching from a spatial to a cued rule than vice versa (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1d</xref>).</p></sec><sec id="s2-4"><title>Flexibility in responses to rule changes depends on rule type</title><p>Learning a target rule is one of two key events in decision-making tasks. The other is responding to a change in the target rule. Changes in behaviour around rule switches are useful to understand the flexibility of behaviour, and the relative dominance of one mode of responding (e.g. spatial) over another (e.g. cue-driven) (<xref ref-type="bibr" rid="bib10">Birrell and Brown, 2000</xref>; <xref ref-type="bibr" rid="bib23">Floresco et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Buckley et al., 2009</xref>; <xref ref-type="bibr" rid="bib71">Wang et al., 2019</xref>). To show how our approach can quantify behavioural flexibility, we thus used our inference algorithm to examine trial-resolution changes in choice strategy around rule-switches in the same Y-maze and lever-press tasks.</p><p>In both tasks, rats which had been training on a spatial rule continued to have a high probability of executing a spatial strategy on each trial long after the rule had switched to a cue-based target rule (<xref ref-type="fig" rid="fig4">Figure 4a, b</xref>, left panels). Thus, rats showed little behavioural response during switches from spatial to cued rules.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Detecting responses to task changes.</title><p>(<bold>a</bold>) Responses to rule switches in the Y-maze task. The left panel plots the maximum a posteriori (MAP) estimates of <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the target spatial and cue strategies aligned to the switch from a spatial to a cued rule (<italic>N</italic> = 7 sessions, from four rats). The right panel plots the MAP estimates aligned to the switch from a cued to a spatial rule (<italic>N</italic> = 3 sessions, from three rats). For both panels, curves plot means ± standard error of the mean (SEM; shading) across sessions, aligned to the rule-switch trial (vertical dashed line). The horizontal dashed line indicates chance. (<bold>b</bold>) Same as panel a, but for the lever-press task. <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mstyle></mml:math></inline-formula> rats for spatial → cued; <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mstyle></mml:math></inline-formula> cued → spatial. (<bold>c</bold>) Response to each rule-switch in the Y-maze task. Each dot shows the proportion of trials after a rule switch in which the labelled strategy was above chance. Proportions calculated for a window of 30 trials after the rule switch. Lines join datapoints from the same switch occurrence. (<bold>d</bold>) Same as panel c, for the lever-press task. Proportions calculated for a window of 60 trials after the rule switch, to take advantage of the longer sessions in this task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig4-v2.tif"/></fig><p>The switch from a cued to a spatial rule in both tasks evoked a considerably faster change in strategy. Over the same time window as the spatial → cued switch, while the probability of selecting the cued strategy declined slowly, the probability of selecting the now-correct spatial strategy increased above chance within 20 trials of the rule switch (<xref ref-type="fig" rid="fig4">Figure 4a, b</xref>, right panels).</p><p>To demonstrate the fine-grained analysis possible with our Bayesian algorithm, we also quantified the variability in rule-switch responses across rats by calculating for each rat the proportion of trials in which each rule strategy was above chance after the switch (<xref ref-type="fig" rid="fig4">Figure 4c, d</xref>). Individual rats sustained their use of the now-incorrect strategy after both spatial → cued and cued → spatial rule switches (<xref ref-type="fig" rid="fig4">Figure 4c, d</xref>). The reduction in use of a cued strategy after cued → spatial rule switches was also evidenced by that strategy reverting to chance for a few trials so that the proportion of trials above chance was less than one (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, right). Rats in both tasks also showed considerable variation in how often a spatial strategy was above chance after the switch to the spatial rule (<xref ref-type="fig" rid="fig4">Figure 4c, d</xref>, right), suggesting varying rates of adapting to the rule switch.</p><p>Our trial-resolution algorithm has thus provided evidence from both learning (<xref ref-type="fig" rid="fig3">Figure 3f</xref>) and responding to rule switches (<xref ref-type="fig" rid="fig4">Figure 4</xref>) that the spatial → cued rule switch is harder for rats to detect than the cued → spatial switch, across two different tasks. Moreover, the behavioural response to the cued → spatial rule switch (<xref ref-type="fig" rid="fig4">Figure 4a, b</xref>, right panels) shows that, as both were above chance, the newly correct strategy (go spatial) can be acquired even while the now incorrect strategy (go cued) is still actively used.</p></sec><sec id="s2-5"><title>Independent changes in lose-shift and win-stay around learning and rule switches</title><p>Having used the changes in rule strategies to characterise both learning and the response to rule switches, we then sought to understand the exploratory strategies that were driving these changes.</p><p>To probe this, we computed <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for four exploratory strategies based on the task features pertinent to the target rules. For the spatial rules, these were win-stay and lose-shift based on the choice of arm or lever; for example, repeating the choice of left arm or lever after being rewarded for it on the previous trial would be a spatial win-stay. For the cued rules, these were win-stay and lose-shift based on the cue position; for example, repeating the choice of the cued arm or lever after being rewarded for it on the previous trial would be a cued win-stay.</p><p>We found strong evidence that rule learning was preceded by a high probability of animals using lose-shift in response to the corresponding feature of the target rule (<xref ref-type="fig" rid="fig5">Figure 5a, d</xref>, top pair of panels). This elevated use of lose-shift to the target-rule’s feature was consistent for both classical and strategy definitions of learning trials and across both the Y-maze (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) and lever-press (<xref ref-type="fig" rid="fig5">Figure 5d</xref>) tasks. The sustained probability of using lose-shift was not due to the lack of opportunity to update that strategy: before the identified learning trial animals had not yet reached ceiling performance with the rule, so were still incurring losses (<xref ref-type="fig" rid="fig5">Figure 5a, d</xref>, bottom panels). This use of lose-shift is consistent with latent learning of the relevant feature of the target rule.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Lose-shift and win-stay independently change around learning and rule switches.</title><p>(<bold>a</bold>) Changes in the probability of win-stay and lose-shift strategies around learning. In the top pair of panels, we plot probabilities for choice-driven (left) and cue-driven (right) forms of win-stay and lose-shift aligned to the identified learning trial (trial 0). Learning trials identified by either the original study’s criterion (top) or our rule-strategy criterion (middle). The bottom panel plots the proportion of correct choices across animals on each trial. Lines and shading show means ± standard error of the mean (SEM; <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> given in <xref ref-type="fig" rid="fig3">Figure 3b</xref>). (<bold>b</bold>) Rule switch aligned changes in probability for choice-based win-stay and lose-shift strategies (<inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> given in <xref ref-type="fig" rid="fig4">Figure 4a</xref>). (<bold>c</bold>) Rule switch aligned changes in probability for cue-based win-stay and lose-shift strategies. (<bold>d–f</bold>) Same as panels a–c, but for the lever-press task. Panel d: <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>28</mml:mn></mml:mstyle></mml:math></inline-formula> rats per curve. Panels e, f: <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mstyle></mml:math></inline-formula> rats spatial → cued; <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mstyle></mml:math></inline-formula> cued → spatial.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig5-v2.tif"/></fig><p>Lose-shift exploration rapidly altered at rule changes too. Immediately after a rule switch, lose-shift in response to the old rule’s target feature declined rapidly as animals now persevered in applying the old rule, so frequently re-chose the same option after losing (<xref ref-type="fig" rid="fig5">Figure 5b, c, e, f</xref>). In the lever-press task, there was a rapid increase of lose-shift in response to the new rule’s feature for both switches from spatial to cued rules (<xref ref-type="fig" rid="fig5">Figure 5e</xref>, right) and vice versa (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, left), again consistent with latent awareness that the original rule had changed.</p><p>For both learning and rule changes, win-stay exploration did not change with lose-shift. During learning, win-stay changed at or around the learning trial in both the Y-maze (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) and lever-press (<xref ref-type="fig" rid="fig5">Figure 5d</xref>) tasks. At a rule change, win-stay for the old rule’s feature changed faster for the cued → spatial (<xref ref-type="fig" rid="fig5">Figure 5c, f</xref>, right panels) than the spatial → cued rule switches (<xref ref-type="fig" rid="fig5">Figure 5b, e</xref>, left panels), underpinning the faster response to the cued → spatial switch (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We ruled out that this independence of win-stay from lose-shift was because one of those strategies could not be updated, as the rate of correct trials was neither at floor nor ceiling before learning and after rule switches (<xref ref-type="fig" rid="fig5">Figure 5</xref>: bottom panels in a, c, d, f). Lose-shift and win-stay were thus independently used and changed during exploration around learning and rule changes.</p></sec><sec id="s2-6"><title>Latent exploratory strategies disguise the absence of learning in humans</title><p>We now turn to demonstrating insights from the inference algorithm on a wider range of tasks. Here, we analyse human performance on a simple stimulus-choice task, with no rule switches, to illustrate how the algorithm performs with only a few trials; in the next section we consider non-human primate performance on a stimulus–action task with stochastic reward.</p><p>Human participants performed a two-choice task, similar to that of <xref ref-type="bibr" rid="bib52">Pessiglione et al., 2006</xref>, in which they had to choose between two stimuli that appeared above and below a central fixation point (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). Which of the pair of stimuli appeared above or below fixation was randomised on each trial, and three pairs of stimuli corresponded to three interleaved trial types. In ‘gain’ trials, one stimulus increased total reward with a probability of 0.8, the other nothing; in ‘loss’ trials, one stimulus reduced total reward with a probability of 0.8, the other nothing; and in ‘look’ trials, participants randomly chose one of the two stimuli, with no associated outcome. Their implicit goal was thus to maximise reward by learning to select the gain stimulus and avoid the loss stimulus. As participants only experienced 30 of each trial type, these data could present strong challenges to classical approaches of identifying if and when participants learnt to correctly choose the gain stimulus or avoid the loss stimulus, and what strategies they adopted during learning. In contrast, our Bayesian approach can track changes in strategies within a few trials (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Human learning and exploration on a gain/loss task.</title><p>(<bold>a</bold>) Gain/loss task. Gain, loss, and look trials each used a different pair of stimuli and were interleaved. A pair of stimuli were randomly assigned to the top or bottom position on each trial. Numbers above Outcome panels refer to the probabilities of those outcomes in the gain and loss trial types. (<bold>b</bold>) For an example participant, the maximum a posteriori (MAP) estimate of <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the target-rule strategy during gain (left panel) and loss (right panel) trials. We plot two estimates, initialised with our default uniform prior (solid) or Jeffrey’s prior (dotted). Horizontal dashed lines indicate chance level. Black arrows indicate the learning trial based on our strategy criterion. (<bold>c</bold>) The MAP estimate of <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the target-rule strategies across all participants. Curves plot mean ± standard error of the mean (SEM) (shading) across 20 participants. The dashed line is chance. (<bold>d</bold>) Proportion of participants that learnt the target strategy for gain or loss trials. Learning was identified by the first trial at which the MAP estimate of <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> was consistently above chance until the end of the task (black arrows in panel b). Vertical bar indicates the 95% Clopper–Pearson confidence intervals for binomial estimates. (<bold>e</bold>) Distributions of learning trials for gain and loss trials, for the participants meeting the learning criterion. Each dot is a subject. Boxplots show the median, and the 25th and 75th percentiles. The whiskers extend to the extreme values. (<bold>f</bold>) Probabilities of exploratory strategies for the learning participants (<italic>N</italic> = 13 for gain, <italic>N</italic> = 5 for loss). Curves plot mean ± SEM (shaded area) MAP probabilities.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Human strategies are robust to priors and do not correlate with bias in preferred target locations.</title><p>(<bold>a</bold>) Difference between maximum a posteriori (MAP) probability of ‘go gain’ and ‘avoid loss’ strategies initialised with uniform prior and Jeffrey’s prior for the example participant in <xref ref-type="fig" rid="fig6">Figure 6b</xref>. (<bold>b</bold>) MAP probability profiles for ‘go gain’ and ‘avoid loss’ strategies along the session during gain and loss trials, respectively. Mean (solid curve) ± standard error of the mean (SEM; shaded area) across participants (<italic>N</italic> = 20). Horizontal dashed line indicates chance level. These strategies were initialised with Jeffrey’s prior. (<bold>c</bold>) Similar to panel a, the difference between the MAP probability of ‘go gain’ and ‘avoid loss’ strategies initialised with uniform and Jeffrey’s prior for every participant (one line per participant). (<bold>d</bold>) The distribution of biased trials during Look trials, defined as the proportion of trials in which the MAP probability for using a direction strategy towards one of the two stimulus locations was consistently above chance. No bias corresponds to 0.5, whereas 1 is when the participant chose the same location on every Look trial. Each dot is a participant. White dot is median and grey bars show the 25th and 75th percentiles. (<bold>e</bold>) The proportion of trials in which ‘go gain’ (left panel) and ‘avoid loss’ (right panel) were the winning strategy for each subject as function of their look bias. Each dot is a participant. <italic>r</italic> is the Pearson correlation coefficient. Solid line plots the linear fit to the data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig6-figsupp1-v2.tif"/></fig></fig-group><p>To identify learning, we separately computed <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the strategies of selecting the gain stimulus and of avoiding the loss stimulus (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). Despite the few trials involved, these estimates were not sensitive to the choice of prior (<xref ref-type="fig" rid="fig6">Figure 6b</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Across all participants, the MAP estimates of <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> showed evident learning of choosing the gain stimulus, but not of avoiding the loss stimulus (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Applying the same learning criterion as above, of the MAP probability being above 0.5 until the end of the session, our inference algorithm suggests most participants learnt to choose the gain stimulus (13/20), but not to avoid the loss stimulus (5/20) (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). Even then, the few who did putatively learn to avoid the loss stimulus did so only towards the end of the 30 trials (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). Our trial-resolution probabilities have thus shown both strong evidence of which participants learnt the gain trials, and that there is weak evidence at best of any participant learning to avoid the loss stimulus.</p><p>To further examine this issue, we asked if the apparent learning to avoid loss could be explained by an exploratory strategy. For the participants who putatively learnt either the gain or loss rule strategies we computed <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for a range of exploratory strategies that captured either responding to the previously chosen stimulus or to the previous location of the chosen stimulus. We found stimulus-driven win-stay and lose-shift were both used for learning gain trials, but not for loss trials (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). In contrast, only location-driven exploration was consistently, albeit weakly, above chance for loss trials (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). Because this suggested participants could be sensitive to the location of the stimuli, we then also examined location-dependent strategies in the ‘look’ trials, and found participants were strongly biased towards choosing one location over the other; however, this bias did not correlate with the participants’ use of the avoid loss strategy (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Together, these results suggest that no participant learnt avoid loss, and those who appeared to were instead executing a location-driven exploratory strategy that happened to coincide with avoid loss for the final few trials.</p></sec><sec id="s2-7"><title>Choice strategy driven by reward probability not magnitude in a stochastic decision task</title><p>More complex decision-making tasks use probabilistic and variable-size rewards for all choices. These present further challenges for the analysis of choice behaviour, not least because the variation of both probability and size of reward increases the number of features upon which subjects may base their choice strategies. To examine these challenges, we used our inference algorithm to analyse a session of data from a rhesus macaque performing a stimulus-primed two-choice task (<xref ref-type="fig" rid="fig7">Figure 7a</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Probabilistic reward.</title><p>(<bold>a</bold>) Schematic of the stimulus-to-action reward task. Left: the monkey initiated a trial by touching a key after the red dot appeared on-screen. A stimulus appeared, predicting which side of the upcoming choice would have the higher probability of obtaining the large reward. Touching the stimulus and then the key brought up the choice options on-screen. The monkey indicated its decision by touching the appropriate circle on-screen. Right: probabilities of obtaining the large reward for each of the six stimuli. (<bold>b</bold>) Maximum a posteriori (MAP) probability for the strategy of choosing the side with the highest probability of reward (go high-probability side). We also plot the MAP probabilities for the choice of side (left or right). Green and blue dots at the top and bottom indicate the side with the higher probability of receiving the large reward; probabilities switched (vertical dashed lines) after the subject chose the high-probability option on 80% of trials in a moving window of 20 trials. (<bold>c</bold>) MAP probabilities in panel b aligned to the transition between blocks (trial 0). Average (bold lines; <italic>n</italic> = 5 blocks) ± standard error of the mean (SEM; shaded areas). (<bold>d</bold>) MAP probabilities along the session for exploratory strategies based on repeating the previous choice after either receiving the large reward or choosing the option with the higher probability of the large reward. (<bold>e</bold>) Proportion of trials in which the two repeating-choice strategies had the largest MAP probability. Vertical bars are 95% Clopper–Pearson confidence intervals. (<bold>f</bold>) As panel d, for exploratory strategies based on switching from the previous choice after either receiving the low reward or choosing the option with the lower probability of high reward. (<bold>g</bold>) As panel e, for the switching strategies in panel f.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86491-fig7-v2.tif"/></fig><p>In this task, the monkey was presented with a stimulus that predicted the probability of obtaining the larger of two rewards if it subsequently chose either the left or right option on a touchscreen (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, left). Six stimuli were associated with a different pair of probabilities for obtaining the larger reward for the left or right choice (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, right); whatever the monkey chose, the small reward was delivered if the probability check for the large reward failed. The stimulus and associated pair probabilities switched once the monkey chose the highest probability option in 80% of trials. With our trial-resolved approach we can immediately ask two questions: did this criterion give sufficient evidence for learning, and was the monkey’s choice based on the probability of obtaining the large reward or something else?</p><p>To test the switching criterion, we computed <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the strategy of choosing the side with the highest probability of reward (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). This confirmed the monkey showed good evidence of learning by the time the criterion was reached, as <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the highest probability option became dominant at least 10 trials before the switch of reward probabilities (orange line, <xref ref-type="fig" rid="fig7">Figure 7c</xref>). Our analysis also suggests that in at least one block (trials 25–160 in <xref ref-type="fig" rid="fig7">Figure 7b</xref>), the monkey had learnt to choose the appropriate option long before the block switch.</p><p>As the choice of either option in this task could result in the large reward, the monkey may have been basing its exploration not on the probability of the large reward, but simply on the value of the obtained reward. To examine this, we computed <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for two sets of exploratory strategies that used different features of the monkey’s choice history: one set looked at whether repeating the same option choice was driven by the value or the probability of reward on the previous trial (<xref ref-type="fig" rid="fig7">Figure 7d</xref>); and the other looked at whether switching the option choice was driven by the value or the probability of reward on the previous trial (<xref ref-type="fig" rid="fig7">Figure 7f</xref>). The monkey’s use of exploratory strategies that repeated or switched choices evolved over the blocks (<xref ref-type="fig" rid="fig7">Figure 7d, f</xref>). Independent of this evolution, both repeating and switching strategies were dominated by the probability of reward rather than the reward obtained (<xref ref-type="fig" rid="fig7">Figure 7e, g</xref>), evidence that the monkey was basing its decisions more on reward probability than immediate value. Because each session had new arbitrary stimuli for the six pairs of probabilities, the monkey was here re-learning the mapping between stimuli and probabilities, and so was not trivially choosing from an already-learnt association. Rather, these results suggest that, through its prior training, the monkey had learnt the structure of the probabilities, and so was able to use that abstract knowledge to guide its exploratory choices.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Subjects may adopt a range of choice strategies during decision-making tasks, which change during training as they learn a target rule and as the rule is changed. Uncovering how subjects learn to make decisions is essential for understanding what information drives behaviour and for linking neural activity to behaviour. Classical approaches to characterising decision making, such as trials-to-criterion measures (<xref ref-type="bibr" rid="bib10">Birrell and Brown, 2000</xref>; <xref ref-type="bibr" rid="bib53">Peyrache et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>), psychometric functions (<xref ref-type="bibr" rid="bib13">Britten et al., 1992</xref>; <xref ref-type="bibr" rid="bib50">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib17">Churchland et al., 2008</xref>), or reward curves (<xref ref-type="bibr" rid="bib24">Gallistel et al., 2004</xref>), cannot track changes over time; algorithms for detecting learning points (<xref ref-type="bibr" rid="bib65">Smith et al., 2004</xref>; <xref ref-type="bibr" rid="bib67">Suzuki and Brown, 2005</xref>) and behavioural change points (<xref ref-type="bibr" rid="bib21">Durstewitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib36">Jang et al., 2015</xref>) are useful, but do not show what strategies were used during learning or around rule changes. To address these issues, we have introduced an approach to decision-making tasks that fully characterises the probability of choice strategies at trial resolution. Our inference algorithm allows tracking both learning, by defining strategies matching target rules, and how subjects learn, by defining exploratory strategies that may or may not depend on features of the task. We have applied our algorithm to a variety of two-choice tasks, across a range of species, including Y-maze and lever-press tasks in rats, a stimulus–outcome mapping task in humans and a stochastic stimulus–action task in non-human primates.</p><p>Across the tasks we analysed, our algorithm showed that lose-shift and win-stay are independent exploratory strategies, because a change in the probability of one being executed is not mirrored by a change in the probability of the other. This challenges views that win-stay and lose-shift are complementary strategies, often yoked together (<xref ref-type="bibr" rid="bib46">Miller et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Izquierdo et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>).</p><p>Emphasising this independence, our analysis shows that changes to lose-shift are evidence of latent learning. Rats adopted a lose-shift strategy matching the target rule before the learning of that rule was identified by overt choice behaviour (<xref ref-type="fig" rid="fig5">Figure 5a, d</xref>). Conversely, after a switch in rewarded rules in the lever-press task, rats rapidly started using a lose-shift strategy that matched the new rule while still using a win-stay strategy based on the old rule (<xref ref-type="fig" rid="fig5">Figure 5b, c, e, f</xref>). These data are consistent with ideas that negative and positive feedback can independently drive changes in decision making. Indeed they support the idea that learning what not to do is easier than learning what to do.</p><p>We derived three potential criteria for learning from the algorithm’s estimate of a subject’s probability of using the correct strategy. The most sensitive criterion showed in both the Y-maze and lever-press tasks that learning occurred earlier and more often than detected in traditional approaches based on counting successful trials in some fixed window. All three criteria showed that switching from a spatial to a cue-based rule is harder than the reverse switch. This is consistent with prior work suggesting rats learn spatial rules more easily than visually cued rules (<xref ref-type="bibr" rid="bib44">Mackintosh, 1969</xref>). Prior work on the same lever-press task had shown some evidence of the opposite, that re-learning is faster after switching from a spatial to a cued rule than switching from a cued to a spatial rule (<xref ref-type="bibr" rid="bib23">Floresco et al., 2008</xref>). Our results suggest that the choice of learning criterion do not underpin this discrepancy.</p><p>There has been considerable recent advocacy for studying the variety of behaviour within and between individuals (<xref ref-type="bibr" rid="bib41">Krakauer et al., 2017</xref>; <xref ref-type="bibr" rid="bib30">Honegger and de Bivort, 2018</xref>; <xref ref-type="bibr" rid="bib51">Pereira et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Ashwood et al., 2022</xref>), both to more richly characterise the diversity of behavioural solutions adopted by humans or other animals facing a task, and as a basis for understanding the link between neural activity and behaviour. As it provides trial-resolution probabilities for each subject, our algorithm quantifies the behavioural diversity in a given task. We illustrated that potential here by defining learning points for each subject in the Y-maze (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), lever-press (<xref ref-type="fig" rid="fig3">Figure 3f</xref>), and human gain/loss tasks (<xref ref-type="fig" rid="fig6">Figure 6d, e</xref>), and by showing the distribution of exploratory behaviours around rule switches (<xref ref-type="fig" rid="fig4">Figure 4c, d</xref>). Further work here could use the trial-resolution probabilities to characterise an individual’s repertoire of strategies and when they deployed each strategy.</p><p>Reflecting this urgent need for fine-grained behavioural insights into how subjects make decisions, recent work by Pillow and colleagues introduced two innovative solutions to the problem of tracking a subject’s behaviour (<xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Ashwood et al., 2022</xref>). One solution (PsyTrack: <xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>) continuously infers the parameters of a psychometric function to track the relative influence of different task features on choice in two-alternative forced-choice tasks; the other (<xref ref-type="bibr" rid="bib4">Ashwood et al., 2022</xref>) infers the hidden states underlying decision-making behaviour and the transitions between them, by modelling psychometric functions in each state. Their elegant work applies to well-learnt behaviour, using computationally intensive models fit to all available data post hoc, but lacks the ability to infer when learning occurs. In contrast, our approach focuses on behavioural strategies adopted during learning and rule switches, accumulates evidence up to trial <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> so can be used on-line, and is able to infer strategies in a few tens of trials (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig6">6</xref>), but lacks a model of the transitions between strategies or the parameters which influence the current strategy. An interesting future avenue for our work would be to use the changing probability of different strategies to infer the transition probabilities between them. Our approach and that of Pillow and colleagues are thus complementary, collectively providing a richer toolbox for understanding a subject’s behaviour on choice tasks.</p><p>The probabilities computed by our algorithm are a description of the observer: what we can tell about the behaviour on trial <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> from the evidence so far. This Bayesian analysis of behaviour does not imply the animal is behaving in any optimal, Bayesian way, nor that its brain is computing or using probabilities (<xref ref-type="bibr" rid="bib40">Knill and Pouget, 2004</xref>; <xref ref-type="bibr" rid="bib22">Fiser et al., 2010</xref>; <xref ref-type="bibr" rid="bib55">Pouget et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Sohn et al., 2019</xref>). Nonetheless, using a Bayesian strategy analysis could let us then formulate those questions of optimality in behaviour and look for neural correlates of probabilistic representations. More generally, such a trial-by-trial analysis of a strategy could then let us look for neural correlates, more accurately tracking the changes in neural activity that precede or align with changes in behavioural strategy (<xref ref-type="bibr" rid="bib59">Rich and Shapiro, 2009</xref>; <xref ref-type="bibr" rid="bib21">Durstewitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib38">Karlsson et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Powell and Redish, 2016</xref>; <xref ref-type="bibr" rid="bib27">Guise and Shapiro, 2017</xref>).</p><p>One issue we faced is that Bayesian inference assumes the parameter being estimated is stationary, but choice strategies are not, so any estimate of their probability must also change. Here, we solve that problem by weighting the evidence entering the Bayesian update by its recency. This allows tracking the probabilities of strategies as they change, and is robust to the exact weighting used (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> to <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p><p>The choice of how much to decay the influence of past evidence can determine how quickly a change in strategy is detected (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> to <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), suggesting other solutions to this non-stationary problem are worth pursuing. For example, having a principled way of varying the weighting <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> of evidence over time could be useful, potentially so that evidence is rapidly decayed around a switch in strategies and less rapidly during stable execution of a strategy. This would require defining <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> as a function of some observable parameter of behaviour. It could also potentially find different evidence decay rates <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> between different strategies.</p><p>Introducing <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> may seem to create a free parameter for the analysis. However, all frequentist approaches to analysing the strategies of subjects also have at least one free parameter, namely the length of the time window over which to compute the probability they are using. This is true both for computing the probability of a particular strategy being used as the proportion of trials it occurs on <xref ref-type="bibr" rid="bib64">Singh et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Harris et al., 2021</xref>; <xref ref-type="bibr" rid="bib69">Trepka et al., 2021</xref> and for classic learning criterion of subjects performing sequential, or a proportion of, successful trials in some time window (e.g. <xref ref-type="bibr" rid="bib10">Birrell and Brown, 2000</xref>; <xref ref-type="bibr" rid="bib11">Boulougouris et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Floresco et al., 2008</xref>; <xref ref-type="bibr" rid="bib43">Leeson et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Peyrache et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>). Sometimes this time window is explicitly chosen (<xref ref-type="bibr" rid="bib64">Singh et al., 2019</xref>); sometimes the choice is implicit, as the probabilities are computed over trial blocks (<xref ref-type="bibr" rid="bib69">Trepka et al., 2021</xref>) or phases of the experiment (<xref ref-type="bibr" rid="bib29">Harris et al., 2021</xref>). Either way, rarely is the influence of that time window considered. Thus, rather than <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> introducing a further free parameter, it instead makes the inevitable free parameter of time explicit and bounded: we have placed strong bounds on the values of <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> by showing the trade-off between the speed of detecting a new strategy and the stability of detecting it.</p><p>Our algorithm provides rich information about choice behaviour by computing a full posterior distribution for each modelled strategy, for every trial. How that information is then used to further our understanding of choice behaviour offers a wealth of potential. Here, we tackled the question of when subjects learnt by computing three criteria from the posterior distribution of the target-rule’s strategy and the question of how they explored by tracking a point estimate of the probability of each exploratory strategy from their posterior distribution. A third question, which we did not address here, is: which strategy is the subject most likely to be using now? In our simulations, the simple decision rule of choosing the strategy with the highest MAP estimate and highest precision worked well; further work could explore solutions that make fuller use of the posterior distribution, for example computing the probability (<xref ref-type="bibr" rid="bib18">Cinotti and Humphries, 2022</xref>) that a strategy is dominant over the others. Whether this is a good question to ask depends upon our model for how a subject is using strategies: seeking the dominant strategy implies a model where subjects switch between strategies discretely; by contrast, a model where subjects sample from a repertoire of strategies on each trial means our estimated <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is tracking the probability of sampling each strategy. Disentangling these models would give deeper insight into how subjects explore while learning.</p><p>The richness of the information depends in part on the choice of strategies examined, which is down to the end-user and what questions they wish to ask of their data. We here distinguished rule strategies and exploratory strategies: while the former should be examined to understand learning, the latter can include a wider range of options than we examined here. We focused predominantly on variants of win-stay and lose-shift strategies as these are widely studied (<xref ref-type="bibr" rid="bib46">Miller et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Constantinople et al., 2019</xref>; <xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; <xref ref-type="bibr" rid="bib60">Roy et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Ashwood et al., 2022</xref>), contrasting the different features of a task – subject’s spatial choice or the task cues – that define the stay or shift response. But the range of strategy models usable with our inference algorithm is broad enough to cover many potential questions about decision making: all the algorithm needs is the success or failure to execute a given strategy on trial <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>. As such, our approach is highly extensible. It can be used with more complex models of choice-history strategies than those considered here: for example, where classic win-stay and lose-shift consider only the previous trial, <italic>n</italic>-back history strategies (<xref ref-type="bibr" rid="bib42">Lau and Glimcher, 2005</xref>; <xref ref-type="bibr" rid="bib2">Akrami et al., 2018</xref>) could capture the effect of choices or outcomes <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> trials ago on the decision at trial <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>. It could also be used with economic games, tracking the probability of each player using specific response strategies to others’ decisions, like Tit-for-Tat (<xref ref-type="bibr" rid="bib5">Axelrod and Hamilton, 1981</xref>; <xref ref-type="bibr" rid="bib6">Axelrod and Dion, 1988</xref>; <xref ref-type="bibr" rid="bib47">Nowak and Sigmund, 1993</xref>; <xref ref-type="bibr" rid="bib48">Ohtsuki et al., 2006</xref>). Pilot or bias tests could be used to estimate the prior distribution’s parameters for each subject. And while all tasks here had two choices, the algorithm can just as easily compute probabilities of choice strategies in tasks with three or more choices. Despite this extensibility, the computational cost per trial is extremely low, as elementary arithmetic alone is needed for computing the update to the posterior distribution. Consequently, the inference algorithm we introduce here can assess the probabilities of an extensive, flexible range of choice strategies, and could do so in real time, allowing detailed interrogation and potentially closed-loop control of animal and human behaviour during decision-making tasks.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Code availability</title><p>Code toolboxes implementing the algorithm and strategy models are available for MATLAB (<ext-link ext-link-type="uri" xlink:href="https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_MATLAB">GitHub</ext-link>) and Python (<ext-link ext-link-type="uri" xlink:href="https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_Python">GitHub</ext-link>). These toolboxes will be updated with future improvements to the algorithm.</p><p>All code used to produce the figures in this paper is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Humphries-Lab/Bayesian_strategy_analysis_Paper">GithHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib33">Humphries-Lab, 2024</xref>). This repository also includes all data in the paper (see Data Availability Statement).</p></sec><sec id="s4-2"><title>Bayesian inference of strategy probability</title><p>Here, we give the full algorithm as used throughout the paper.</p><list list-type="order"><list-item><p>Choose a set of <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> strategy models to test.</p></list-item><list-item><p>Each <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is parameterised by a Beta distribution, with parameters <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, for <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>The prior is defined by the choice of <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We use the uniform prior <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> by default; we use the Jeffrey’s prior <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> where noted.</p></list-item><list-item><p>Initialise the running counts of successful (<inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>) and failed (<inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>) trials to use strategy <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>On every trial <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>:</p><list list-type="alpha-lower"><list-item><p>Label it for strategy <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> as either a:</p><p>Success trial: choice made on trial <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is consistent with strategy <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>;</p><p>Failure trial: choice made on trial <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is inconsistent with strategy <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>,</p><p>Null trial: strategy <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> could not be assessed on trial <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>For success and failure trials, we update the parameters <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>:</p><list list-type="roman-lower"><list-item><p>decay the event totals, and add the new event: <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>; and <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>; with a decay rate <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> .</p></list-item><list-item><p>update the Beta distribution parameters: <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p></list-item></list></list-item><list-item><p>For null trials, we assign the previous trial’s parameter value: <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p></list-item></list></list-item><list-item><p>Output: time-series of <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for each strategy <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, which define a Beta distribution for each trial.</p></list-item></list><p>Null trials occur when a strategy depends on conditions that could not be met in that trial. In our current implementation we interpolate the previous values for <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> for a null trial, to define the probability at which that strategy could have occurred. Future extensions to our work could usefully explore alternative solutions to the handling of null trials.</p></sec><sec id="s4-3"><title>Task descriptions</title><sec id="s4-3-1"><title>Y-maze – rats</title><p>Rat behavioural data on the Y-maze task data came from the study of <xref ref-type="bibr" rid="bib53">Peyrache et al., 2009</xref>. This dataset includes 50 sessions of four male Long-Evans rats performing a cross-modal rule-switch task. Rats were trained to self-initiate the trial by running up the central stem of the maze and choosing one of the two arms. The rats had to learn in sequence one of four rules: ‘go right’, ‘go cued’, ‘go left’, and ‘go uncued’. The cue was a light stimulus randomly switched at the end of one of the two arms. The rule was changed within session after performing 10 consecutive trials or 11 out of 12 correct trials. In the original study, learning was defined as the first of three consecutive trials followed by a performance of ≥80% until the end of the session. For each trial, we used the choice location (right or left), the cue location (right or left), and the reward (yes or no) as input data for the Bayesian model.</p></sec><sec id="s4-3-2"><title>Lever press – rats</title><p>All experimental procedures were conducted in accordance with the requirements of the United Kingdom (UK) Animals (Scientific Procedures) Act 1986, approved by the University of Nottingham’s Animal Welfare and Ethical Review Board (AWERB) and run under the authority of Home Office project license 30/3357.</p><p>Rat behavioural data for the lever-press task were taken from 32 male Lister Hooded rats (aged 11 weeks at the start of testing and food-restricted throughout behavioural training), which served as control groups in two micro-infusion studies (each <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mstyle></mml:math></inline-formula>) targeting prelimbic medial prefrontal cortex. These rats had infusion guide cannulae implanted in the medial prefrontal cortex and received saline infusions in the medial prefrontal cortex (for surgery and infusion procedures, see <xref ref-type="bibr" rid="bib54">Pezze et al., 2014</xref>) at various task stages, including before the sessions during which the data presented in this paper were collected. The task was run in standard two-lever operant boxes with cue lights above the levers and a food well between them (Med Associates, US) based on a previously published training protocol (<xref ref-type="bibr" rid="bib12">Brady and Floresco, 2015</xref>).</p><p>Rats were pre-trained to press the levers for sucrose rewards. The rats then had to learn an initial rule to receive one reward per correct response: either a spatial rule or a visual cue-based rule. The spatial rule required the rat to select the left or right lever, one of which was designated as the correct response and associated with reward. The visual cue-based rule required the rat to select the lever indicated by a cue light, which appeared above one of the two levers and was always associated with reward irrespective of which lever it appeared above. Each trial ended either when a response was made on a lever or when the 10-s time window for responses had elapsed without a response (defined as an omission). After reaching the criterion of 10 consecutive correct trials, the rats had to learn to switch their responses to the second rule: rats which had learnt the spatial rule first were tested on the cue-based rule, and vice versa.</p><p>Half the rats (<inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mstyle></mml:math></inline-formula>) experienced the spatial rule first: all reached criterion on it and on the subsequent cued rule. Half (<inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mstyle></mml:math></inline-formula>) experienced the cued rule first: 12 reached criterion so were switched to the spatial rule, which all 12 reached criterion on as well. Our strategy-based definition of learning (see below) found the same number of learners in each condition, so we have <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>28</mml:mn></mml:mstyle></mml:math></inline-formula> rats throughout for analysis of learning (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and strategy flexibility (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></sec><sec id="s4-3-3"><title>Gain/loss task – humans</title><p>The study was approved by Research Ethics Committee (Stanmore London REC 17/LO/0577). Human participants (<inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula>, 10 male, 10 female) read a participation information leaflet and undertook informed consent. Participants performed a two choice task, similar to that of <xref ref-type="bibr" rid="bib52">Pessiglione et al., 2006</xref>, in which they had to choose between two arbitrary stimuli that appeared above and below a central fixation point (<xref ref-type="fig" rid="fig6">Figure 6a</xref>), across three pairs of stimuli corresponding to three interleaved trial types. The top or bottom position of each pair of stimuli was randomised on each trial. In ‘gain’ trials, one stimulus increased total reward by £1 with a probability of 0.8, the other nothing; in ‘loss’ trials, one stimulus reduced total reward by £1 with a probability of 0.8, the other nothing; and in ‘look’ trials, participants randomly chose one of the two stimuli, with no associated outcome. Thirty trials of each type were presented, randomly interleaved. All participants were informed that the amount they received would be dependent upon their performance, but all received the same amount (£20) at the end of the task.</p></sec><sec id="s4-3-4"><title>Stimulus–action task – non-human primates</title><p>Rhesus macaques were trained to touch a key in front of a touchscreen monitor when a red dot appeared on the screen (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). A visual stimulus then appeared above the dot. After a delay the dot disappeared and the animal had to touch the monitor where the stimulus was present. The dot then reappeared instructing the animal to touch the key again. Two white circles then appeared simultaneously to the left and right of the dot. After a delay the dot disappeared, and the animal chose to select the left or right circle. A large (0.9 ml) or small (0.3 ml) reward of banana smoothie was delivered after every trial. The rewards were probabilistic and asymmetric to either side (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, right panel). There were six reward probability pairings associated with six visual stimuli per session. For example, one visual stimulus predicted the large reward with p = 0.5 following a left action and p = 0.1 following a right action (denoted ‘0.5:0.1’ in <xref ref-type="fig" rid="fig7">Figure 7a</xref>). If the probability check for the large reward failed, the small reward was delivered instead. Each of the six stimuli was presented in a blocked design with the stimulus change and block transition occurring after 80% choices (16/20 trials) of the side with the highest probability of the large reward. We analysed here a single session from one monkey.</p><p>In the full study from which this session was taken, behavioural data were recorded in two young adult male rhesus macaque monkeys (<italic>Macaca mulatta</italic>, one aged 8 years, weight 10–13 kg; the other aged 9 years, weight 11–15 kg). All animals in the Buckley lab are socially housed (or socially housed for as long as possible if later precluded, for example, by repeated fighting with cage-mates despite multiple regrouping attempts) and all are housed in enriched environments (e.g. swings and ropes and objects, all within large pens with multiple wooden ledges at many levels) with a 12-hr light/dark cycle. The NHPs always had ad libitum water access 7 days/week. Most of their daily food ration of wet mash and fruit and nuts and other treats were delivered in the automated testing/lunch-box at the end of each behavioural session (this provided ‘jack-pot’ motivation for quickly completing successful session performance; supplemented by the trial-by-trial rewards for correct choices in the form of drops of smoothie delivered via a sipping tube) and this was supplemented with fruit and foraging mix in the home enclosure. For the study the animal was prepared for parallel neurophysiological investigation, and so was head-fixated during the task using a standard titanium head-post (and also had recording implants); implantation adopted standard aseptic surgical methods used in the non-human primate laboratory that are not directly relevant for this behavioural study but are described in detail elsewhere (<xref ref-type="bibr" rid="bib72">Wu et al., 2021</xref>). All animal training and experimental procedures were performed in accordance with the guidelines of the UK Animals (Scientific Procedures) Act of 1986, licensed by the UK Home Office, and approved by Oxford’s Committee on Animal Care and Ethical Review.</p></sec></sec><sec id="s4-4"><title>Strategy models for each task</title><p>The strategy models we use for the analysis of the Y-maze and lever-press task data are defined in <xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref>. In <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref>, we group the analyses of either ‘go left’ or ‘go right’ as ‘spatial rules’, and the analyses of either ‘go cued’ or ‘go uncued’ as ‘cued’ rules.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Rule strategy models for rat tasks.</title><p>Columns give the conditions to define trial <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> as a success, failure, or null for each strategy corresponding to a target rule. The rule strategies we consider here do not have null conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Success</th><th align="left" valign="bottom">Failure</th><th align="left" valign="bottom">Null</th></tr></thead><tbody><tr><td align="left" valign="bottom">Go left</td><td align="left" valign="bottom">Chose left option</td><td align="left" valign="bottom">Did not choose left option</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Go right</td><td align="left" valign="bottom">Chose the right-hand option</td><td align="left" valign="bottom">Did not choose the right-hand option</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Go cued</td><td align="left" valign="bottom">Chose cued option (e.g. the lit lever)</td><td align="left" valign="bottom">Did not choose the cued option</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Go uncued</td><td align="left" valign="bottom">Chose the uncued option (e.g. the unlit lever)</td><td align="left" valign="bottom">Did not choose the uncued option</td><td align="left" valign="bottom">n/a</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Exploratory strategy models for rat tasks.</title><p>Columns give the conditions to define trial <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> as a success, failure, or null for each exploratory strategy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Success</th><th align="left" valign="bottom">Failure</th><th align="left" valign="bottom">Null</th></tr></thead><tbody><tr><td align="left" valign="bottom">Win-stay-spatial</td><td align="left" valign="bottom">Rewarded on trial <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose the same spatial option (e.g. the left lever) on trial <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Rewarded on trial <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen the same spatial option on trial <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Unrewarded on trial <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Lose-shift-spatial</td><td align="left" valign="bottom">Unrewarded on trial <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose a different spatial option (e.g. the left lever) on trial <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Unrewarded on trial <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen a different spatial option on trial <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Rewarded on trial <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Win-stay-cue</td><td align="left" valign="bottom">Rewarded on trial <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose the same cued option on trial <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> (e.g. chose the lit lever on trials <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>; or the unlit lever on trials <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>)</td><td align="left" valign="bottom">Rewarded on trial <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen the same cued option on trial <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Unrewarded on trial <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Lose-shift-cue</td><td align="left" valign="bottom">Unrewarded on trial <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose a different cued option on trial <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> from the choice on trial <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> (e.g. chose the lit lever on trial <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and the unlit lever on trial <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>; or the unlit lever on trial <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and the lit lever on trial <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>)</td><td align="left" valign="bottom">Unrewarded on trial <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen a different cued option on trial <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Rewarded on trial <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Alternate</td><td align="left" valign="bottom">Chose another spatial option compared to the previous trial</td><td align="left" valign="bottom">Chose the same spatial option as the previous trial</td><td align="left" valign="bottom">n/a</td></tr><tr><td align="left" valign="bottom">Sticky</td><td align="left" valign="bottom">Chose the same spatial option as the previous trial</td><td align="left" valign="bottom">Chose another spatial option compared to the previous trial</td><td align="left" valign="bottom">n/a</td></tr></tbody></table></table-wrap><p>Analyses of the human gain/loss task used the same set of exploratory strategies, with the replacement of ‘spatial’ by ‘location’, and ‘cued’ by ‘stimulus’. The two rule strategies were defined as: ‘go gain’ – success if the subject chose the potentially rewarded stimulus; ‘avoid loss’ – success if the subject did not choose the potentially reward-decreasing stimulus.</p><p>Analyses of the non-human primate stimulus–action task examined three rule strategies, ‘go left’, ‘go right’, and ‘go high-probability’, the latter defined as a success if the subject selected the option with the higher probability of the large reward. <xref ref-type="table" rid="table3">Table 3</xref> defines the exploratory strategy models we used for this task.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Exploratory strategy models for the non-human primate stimulus-to-action task.</title><p>Columns give the conditions to define trial <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> as a success, failure, or null for each exploratory strategy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Success</th><th align="left" valign="bottom">Failure</th><th align="left" valign="bottom">Null</th></tr></thead><tbody><tr><td align="left" valign="bottom">Repeat-Large-Reward</td><td align="left" valign="bottom">Obtained the large reward on trial <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose the same option on trial <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Obtained the large reward on trial <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen the same option on trial <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Obtained the small reward on trial <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Repeat-High-Prob</td><td align="left" valign="bottom">Chose the higher probability of large reward option on trial <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose the same option on trial <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Chose the higher probability of large reward option on trial <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen the same option on trial <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Chose the lower probability of large reward option on trial <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Switch-After-Small-Reward</td><td align="left" valign="bottom">Obtained the small reward on trial <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen the same option on trial <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Obtained the small reward on trial <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chosen the same option on trial <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Obtained the large reward on trial <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Switch-To-High-Prob</td><td align="left" valign="bottom">Chose the lower probability of large reward option on trial <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND NOT chosen the same option on trial <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Chose the lower probability of large reward option on trial <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> AND chose the same option on trial <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Chose the higher probability of large reward option on trial <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap></sec><sec id="s4-5"><title>Interpreting the posterior distributions</title><p>The full posterior distribution <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is a Beta distribution with parameters <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. In this paper, we extract two scalar estimates from that posterior distribution in order to visualise changes in strategies and compare strategies.</p><p>First, the best point estimate of probability of <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is its mode, also called its MAP value. We find the MAP value by numerically evaluating the Beta distribution’s probability density function, and finding the value of <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> that corresponds to the peak of the density function.</p><p>Second, as a summary of evidence for that MAP probability estimate we compute the precision of the posterior distribution, which is the inverse of the variance of the Beta distribution:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Precision</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><sec id="s4-5-1"><title>Learning criteria</title><p>We tested three different criteria for defining learning based on the posterior distribution of <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for the target rule.</p><sec id="s4-5-1-1"><title>Sequence criterion</title><p>The first trial at which the MAP probability estimate for the target-rule’s strategy remained above chance until the end of the session. If the first trial of a session met this criterion, so all trials were above chance, we chose the trial with the minimum MAP probability estimate, to identify the trial at which the strategy probability increased thereafter.</p></sec><sec id="s4-5-1-2"><title>Sequence and precision criteria</title><p>An extension of the sequence criterion to account for the evidence for each strategy: the trial at which both the MAP probability estimate for the target-rule’s strategy remained above chance until the end of the session and the precision of the target-rule’s strategy was greater than for all strategies for the other tested rules.</p></sec><sec id="s4-5-1-3"><title>Expert criteria</title><p>The first trial at which the probability that the posterior distribution <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> contained chance fell below some threshold <italic>θ</italic>, and remained so until the end of the session. We use <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mstyle></mml:math></inline-formula> here.</p></sec></sec></sec><sec id="s4-6"><title>Performance simulations</title><sec id="s4-6-1"><title>Synthetic agent on a 2AFC task</title><p>To illustrate the Bayesian algorithm, we generated synthetic data that simulated an agent working on a two-alternative forced-choice task, described in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The task structure was similar to the tasks described in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig3">3</xref>. The agent could choose between two options (e.g. a right or left lever), and a random cue was alternated between the two options. A reward was delivered only when the agent’s choice matched the cue. We generated 500 trials divided into blocks of 100 trials, in which the agent’s choice followed a specific strategy (‘Go right’, ‘Alternate’, ‘Lose-Shift-Cued’, ‘Go cued’, and ‘Lose-Shift-Choice’). We first defined the vector of cue locations as a random binary vector with p = 0.5; then we defined the vector of choices. For ‘Go right’ and ‘Alternate’ the agent’s choice was consistently right or alternating, respectively. For ‘Lose-Shift’ strategies, we generated a random vector of binary choices; then, for every trial <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> corresponding to a loss (i.e. not matching the cue), we changed the choice in trial <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> in order to match the cue (Lose-Shift-Cued) or choice location (Lose-Shift-Choice). The ‘Go cued’ block consisted of choices matching the cue location.</p><p>To select the most likely strategy being executed at trial <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, we chose strategy <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> with the highest MAP estimate of <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> resolving ties by choosing the strategy with the maximum precision.</p></sec><sec id="s4-6-2"><title>Switching simulations</title><p>To explore the performance of the Bayesian algorithm, we ran extensive simulations of the algorithm detecting the switch between two arbitrary strategies. The general model for the switch between the two strategies generated a time-series of the success (<inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) or failure (<inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>) to execute strategy <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> on each trial. Strategy 1 was executed on trials 1 to <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula>, and strategy 2 was executed on trials 1 to <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula>. From trial <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> the probabilities then reversed linearly:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>j</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>b</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>j</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of trials elapsed since <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> is the rate of change in probability per trial. <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a</xref> illustrates the model for the case of <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>. Note that the probabilities crossover at <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>; we use this in our definitions of detecting the switch below. From this general model we could explore qualitatively different types of strategy switch: whether the switch was abrupt (<inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> is infinity) or gradual (<inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>Given some choice of {<inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>}, we generated the time-series of <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for strategies 1 and 2. On every trial <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> we applied our algorithm to update the two posterior distributions for strategies 1 and 2, and their corresponding MAP estimates <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. We defined the detection of the switch as the first trial for which <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. For abrupt-switching models this was counted from trial <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>; for gradual-switching models this was counted from the ‘crossover’ trial at which <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> first exceeded <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Stability of detection was defined as the proportion of the next <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> trials for which this detection criterion remained true; results are presented for <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mstyle></mml:math></inline-formula>.</p><p>As shown in <xref ref-type="fig" rid="fig2">Figure 2f</xref>, using <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> causes the detection time to plateau with increasing <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula>, because evidence decay means past history has no effect on detection time. For further simulations (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) we thus fixed <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mstyle></mml:math></inline-formula>, to get worst-case performance for detection.</p><p>For the detection of a new conditional strategy, we defined a probability per trial <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> that its condition would be met and simulated a model where its probability of execution <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>new</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> increased linearly from 0 to 1 at a rate of <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula> per trial. We then created time-series of <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>new</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, by first checking if its condition was met, setting <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>new</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> if not; if it was met, we then checked if the strategy was executed (<inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>new</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>) or not (<inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>new</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>). We used our algorithm to find the corresponding MAP estimate <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>new</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> from the time-series, and defined detection as the MAP estimate being greater than 0.5. Results are plotted in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>.</p></sec><sec id="s4-6-3"><title>Effects of missing the true exploratory strategy</title><p>A set of tested exploratory strategies may not contain the true exploratory strategy or strategies used by a subject. To examine how the algorithm performs in this case, we determined the proportion of trials on which a given tested strategy would be expected to make the same choice as a true strategy. Formally, this is the conditional probability <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true strategy</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> that the tested strategy is a success on any trial given that the true strategy is being used, which we call <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>match</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> in the main text. The algorithm’s MAP probability for the tested strategy is an estimate of this conditional probability.</p><p>To compute <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true strategy</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, we decomposed this conditional probability into two main terms, one for trials where the true strategy can be executed, and one for trials where it cannot:<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>true strategy</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>true</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>true</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>true</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>true</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>null</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. To understand what happens if the true strategy is missing, we enumerated the possible values of <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> for four cases: both true and tested strategies were unconditional; one was unconditional and the other conditional (giving two cases); and both were conditional. We explain the enumerations for these cases below, and plot the results in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>.</p><p>We first considered the case where the true and tested exploratory strategies were unconditional. In this case <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>, and so we needed only to consider the possible values of <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. For a task with <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> choices these are <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>: 0, if the tested and true strategy are mutually exclusive (e.g. one repeats the previous choice and one makes a different choice to the previous one); <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, if the true and tested strategy make a single choice; <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> if the true and tested strategy make anything other than a single choice (e.g. the shift to any other choice than the previous one); 1, if the tested and true strategy are equivalent. The latter implies the true strategy is not missing, so we omitted it; consequently, for the case where both true and test strategies are unconditional, <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> gives just three solutions <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true strategy</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>; we plot these three solution as a cumulative distribution in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, panel a. The maximum MAP estimate of <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is thus <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> for this case.</p><p>For the case where the true strategy is unconditional and the tested strategy is conditional, we first decomposed the probability <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> into the probability that the condition for the test strategy is met <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and the probability that the test strategy’s choice, if made, matches the true strategy <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test match true|condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>true</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>test match true | condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>test condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To evaluate <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> using <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>, we tested <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and used <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test match true | condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> as argued for the unconditional case, above. We then evaluated <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> for all combinations of these values in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>, and plot the full set of enumerated probabilities in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, panel d, as a cumulative distribution, omitting the case where the tested and true strategies were entirely equivalent (i.e. <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>). Note that we allowed the possibility of the tested strategy being logically equivalent to the true strategy when the tested strategy’s condition was met, by including <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test match true| condition met</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>; if we do not allow this possibility, we get the set of enumerated probabilities plotted in purple in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, panel b.</p><p>For the case where the true strategy is conditional and the tested strategy is unconditional, we set <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to model the frequency at which the true strategy’s condition was met. We noted that <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> were independent and can each take any value in <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>, except both could not be equal to 1 as this would mean the tested and true strategy were logically equivalent. We then evaluated <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> using all combinations of these values for <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and plot the enumerated probabilities in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, panel b.</p><p>For the case where both the tested and true strategy are conditional, we again used <xref ref-type="disp-formula" rid="equ5">Equation 4</xref> to model the conditional test strategy. We then evaluated <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> using all combinations of values for <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>null</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> specified in the preceding paragraph, and for <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>test</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>true</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> specified under <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>. The resulting enumerated probabilities are plotted in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, panel e.</p><p>In reality we only get to specify the type of test strategy and do not know the type (conditional or unconditional) of true strategy adopted by the subject. Thus, to put bounds on what we can infer from the observed MAP probabilities for a given type of test strategy, we combined the enumerated probabilities for the unconditional and conditional true strategies. The resulting distributions are plotted in the last column of <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, and from these we derive the three bounds discussed in the main text.</p></sec></sec><sec id="s4-7"><title>Derivation of the iterative update and evidence decay</title><p>We first rehearse the standard Bayesian approach to estimating a binomial probability <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> as a random variable when that probability is stationary. We then introduce the decay of evidence to track non-stationary probabilities, and obtain the iterative update rules given in the main text.</p><p>Our data are a sequence of Bernoulli trials <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, each of which is either a success <inline-formula><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> or a failure <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> of an event. A running total of the successful events is <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The posterior distribution <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for estimating the probability of that number of successful events is then given by Bayes theorem<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the denominator is the normalisation constant to obtain a probability distribution function. (Note we write <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> in the main text when applied to our specific problem of estimating the probability of strategy <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>).</p><p>As noted in the main text, the likelihood <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the binomial distribution and we chose the prior <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> to be the Beta distribution with parameters <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>. Using those in <xref ref-type="disp-formula" rid="equ6">Equation 5</xref> gives<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>likelihood</mml:mtext></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>prior</mml:mtext></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Γ</mml:mi></mml:mstyle></mml:math></inline-formula> is the gamma function and <italic>Z</italic> is the denominator in <xref ref-type="disp-formula" rid="equ6">Equation 5</xref>.</p><p>The solution of <xref ref-type="disp-formula" rid="equ7">Equation 6</xref> is another Beta distribution:<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>which is equivalent to simply updating the Beta distribution parameters as <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> given some starting values of <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, which define the prior distribution.</p><p>Rather than updating after <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> trials have elapsed, we can instead update every trial by using the posterior of the previous trial <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> as the prior for the update on the current trial <inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>. To see this, we can rewrite <xref ref-type="disp-formula" rid="equ6">Equation 5</xref> to update on every trial <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> by substituting the Bernoulli distribution <inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> for the binomial distribution, and obtain the iterative equivalent of <xref ref-type="disp-formula" rid="equ8">Equation 7</xref>:<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi></mml:mstyle></mml:math></inline-formula> is the constant <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:math></inline-formula>.</p><p><xref ref-type="disp-formula" rid="equ9">Equation 8</xref> is equivalent to updating the parameters of the Beta distribution on every trial <italic>t</italic> as <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, given some prior values for <inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="right left right left right left right left" rowspacing="3pt" columnspacing="0em 0em 0em 0em 0em 0em 0em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Trial</mml:mtext></mml:mrow><mml:mspace width="mediummathspace"/><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mspace width="1em"/><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Trial</mml:mtext></mml:mrow><mml:mspace width="mediummathspace"/><mml:mn>2</mml:mn><mml:mspace width="1em"/><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mspace width="1em"/><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mspace width="1em"/><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Trial</mml:mtext></mml:mrow><mml:mspace width="mediummathspace"/><mml:mi>t</mml:mi><mml:mspace width="1em"/><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mspace width="1em"/><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>It is this iterative form that allows computationally efficient real-time updates of the probability of each strategy.</p><p>We now turn to our solution to the problem that <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> is not stationary when estimating the use of strategies. Our idea is to decay the evidence entering the posterior update in <xref ref-type="disp-formula" rid="equ6">Equation 5</xref>. Consider the following models for the exponential decay of the Bernoulli trial outcomes up to trial <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, for the successes<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and the failures<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> is the rate of evidence decay.</p><p>We can substitute <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ7">Equation 6</xref> and obtain the posterior distribution<disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:msup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>for the estimate of <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> given the decayed evidence up to that trial. Again, this is equivalent to updating the Beta distribution parameters as <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><p>To obtain the trial-by-trial update equivalent to <xref ref-type="disp-formula" rid="equ13">Equation 12</xref>, we note that the iterative forms of <xref ref-type="disp-formula" rid="equ11 equ12">Equations 10 and 11</xref> are<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mrow><mml:mtable columnalign="right left right left" rowspacing="3pt" columnspacing="0em 0em 0em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>starting from <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.</p><p>With these the update of the Beta distribution parameters on trial <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> becomes<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mtable columnalign="right left right left" rowspacing="3pt" columnspacing="0em 0em 0em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Together, <xref ref-type="disp-formula" rid="equ14 equ15">Equations 13 and 14</xref> form the algorithm we use here.</p></sec><sec id="s4-8"><title>Limits on the posterior distribution</title><p>Any operation altering the evidence entering the likelihood function in <xref ref-type="disp-formula" rid="equ7">Equation 6</xref> could limit the changes to the resulting posterior distribution compared to the standard Bayesian update. Our decay models (<xref ref-type="disp-formula" rid="equ11 equ12">Equations 10 and 11</xref>) have the advantage that we can define these limits exactly.</p><p><xref ref-type="disp-formula" rid="equ11 equ12">Equations 10 and 11</xref> are geometric series with asymptotic limits for the total successes <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and failures <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. They thus also set asymptotic limits for <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ13">Equation 12</xref>). For a long run of successes where <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mstyle></mml:math></inline-formula> these are<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>β</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Similarly, for a long run of failures where <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mstyle></mml:math></inline-formula> these are<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>β</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Consequently, setting the decay parameter <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> creates asymptotic limits for the values of <italic>α</italic> and <italic>β</italic> that could limit how much the posterior <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> changes from the prior.</p><p>We checked the potential limits on the posterior by calculating it at the asymptotic values of <italic>α</italic> and <italic>β</italic> for long-run successes (<xref ref-type="disp-formula" rid="equ16">Equation 15</xref>) across the full range of <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. We then calculated the MAP, precision, and the proportion of the posterior that exceeds chance for two options (i.e. <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). These showed that while the asymptotic limits do not affect the MAP estimate of <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>strategy</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>choices</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, as expected, they do limit how concentrated the posterior becomes around that MAP estimate, which we plot in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Validation, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Supervision, Funding acquisition, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Visualization, Methodology, Writing - original draft, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The human gain/loss task study was approved by Research Ethics Committee (Stanmore London REC 17/LO/0577). All participants were read a participation information leaflet and undertook informed consent.</p></fn><fn fn-type="other"><p>Rat – lever-press task: All experimental procedures were conducted in accordance with the requirements of the United Kingdom (UK) Animals (Scientific Procedures) Act 1986, approved by the University of Nottingham's Animal Welfare and Ethical Review Board (AWERB) and run under the authority of Home Office project license 30/3357. Non-human primate task: All animal training and experimental procedures were performed in accordance with the guidelines of the UK Animals (Scientific Procedures) Act of 1986, licensed by the UK Home Office, and approved by Oxford University's Committee on Animal Care and Ethical Review.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86491-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Source data from the rat Y-maze task data are available from <ext-link ext-link-type="uri" xlink:href="https://crcns.org/">https://crcns.org/</ext-link> at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6080/K0KH0KH5">http://dx.doi.org/10.6080/K0KH0KH5</ext-link>. Source data from the rat lever-press task (32 rats), the human gain/loss task (20 participants), and the primate stimulus-to-action task (one session) are available from the Nottingham Research Data Management Repository at <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.17639/nott.7274">http://doi.org/10.17639/nott.7274</ext-link>. Processed data and analysis code to replicate all figures are available in our GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/Humphries-Lab/Bayesian_strategy_analysis_Paper">https://github.com/Humphries-Lab/Bayesian_strategy_analysis_Paper</ext-link> (copy archived at <xref ref-type="bibr" rid="bib33">Humphries-Lab, 2024</xref>). Our copies of the source data for the Y-maze task, lever-press task, gain/loss task, and stimulus-to-action task are also freely available from the same repository.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Hock</surname><given-names>R</given-names></name><name><surname>Bast</surname><given-names>T</given-names></name><name><surname>Buckley</surname><given-names>M</given-names></name><name><surname>O'Neill</surname><given-names>M</given-names></name><name><surname>Moran</surname><given-names>P</given-names></name><name><surname>Maggi</surname><given-names>S</given-names></name><name><surname>Humphries</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Experimental data for the trial-resolution strategy inference paper</data-title><source>Nottingham Research Data Management Repository</source><pub-id pub-id-type="doi">10.17639/nott.7274</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Benchenane</surname><given-names>K</given-names></name><name><surname>Wiener</surname><given-names>SI</given-names></name><name><surname>Battaglia</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Activity of neurons in rat medial prefrontal cortex during learning and sleep</data-title><source>Collaborative Research in Computational Neuroscience</source><pub-id pub-id-type="doi">10.6080/K0KH0KH5</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Medical Research Council [grant numbers MR/J008648/1, MR/P005659/1, and MR/S025944/1] to MDH and [grant number MR/K005480/1] to MB, and the Biotechnology and Biological Sciences Research Council [grant number BB/T00598X/1] to MB and MDH. SM was supported by an Anne McLaren Fellowship from the University of Nottingham. MS was supported by a Medical Research Council fellowship and a grant from the Dowager Countess Eleanor Peel Trust. Collection of the data on the rat lever-press task was supported by the Biotechnology and Biological Sciences Research Council (BBSRC) Doctoral Training Programme (DTP) at the University of Nottingham [grant number BB/M008770/1, project 1644954], and we are grateful to Stan Floresco for providing MedAssociates programmes to run the lever-press task. We thank Hazem Toutounji for comments on the manuscript, Nathan Lepora for discussions, and Lowri Powell for contributing to the Python port of the toolbox.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguillon-Rodriguez</surname><given-names>V</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bayer</surname><given-names>H</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Cazettes</surname><given-names>F</given-names></name><name><surname>Chapuis</surname><given-names>G</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Dewitt</surname><given-names>E</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Forrest</surname><given-names>H</given-names></name><name><surname>Haetzel</surname><given-names>L</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Krasniak</surname><given-names>C</given-names></name><name><surname>Laranjeira</surname><given-names>I</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Meijer</surname><given-names>G</given-names></name><name><surname>Miska</surname><given-names>NJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Murakami</surname><given-names>M</given-names></name><name><surname>Noel</surname><given-names>J-P</given-names></name><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sanders</surname><given-names>J</given-names></name><name><surname>Socha</surname><given-names>K</given-names></name><name><surname>Terry</surname><given-names>R</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Vergara</surname><given-names>H</given-names></name><name><surname>Wells</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>CJ</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Wool</surname><given-names>LE</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name><collab>The International Brain Laboratory</collab></person-group><year iso-8601-date="2021">2021</year><article-title>Standardized and reproducible measurement of decision-making in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>63711</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63711</pub-id><pub-id pub-id-type="pmid">34011433</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title><source>Nature</source><volume>554</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nature25510</pub-id><pub-id pub-id-type="pmid">29414944</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amarante</surname><given-names>LM</given-names></name><name><surname>Caetano</surname><given-names>MS</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Medial frontal theta is entrained to rewarded actions</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>10757</fpage><lpage>10769</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1965-17.2017</pub-id><pub-id pub-id-type="pmid">28978665</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashwood</surname><given-names>ZC</given-names></name><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Stone</surname><given-names>IR</given-names></name><collab>International Brain Laboratory</collab><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mice alternate between discrete strategies during perceptual decision-making</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>201</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-01007-z</pub-id><pub-id pub-id-type="pmid">35132235</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axelrod</surname><given-names>R</given-names></name><name><surname>Hamilton</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The evolution of cooperation</article-title><source>Science</source><volume>211</volume><fpage>1390</fpage><lpage>1396</lpage><pub-id pub-id-type="doi">10.1126/science.7466396</pub-id><pub-id pub-id-type="pmid">7466396</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axelrod</surname><given-names>R</given-names></name><name><surname>Dion</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The further evolution of cooperation</article-title><source>Science</source><volume>242</volume><fpage>1385</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1126/science.242.4884.1385</pub-id><pub-id pub-id-type="pmid">17802133</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banerjee</surname><given-names>A</given-names></name><name><surname>Parente</surname><given-names>G</given-names></name><name><surname>Teutsch</surname><given-names>J</given-names></name><name><surname>Lewis</surname><given-names>C</given-names></name><name><surname>Voigt</surname><given-names>FF</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Value-guided remapping of sensory cortex by lateral orbitofrontal cortex</article-title><source>Nature</source><volume>585</volume><fpage>245</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2704-z</pub-id><pub-id pub-id-type="pmid">32884146</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolo</surname><given-names>R</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prefrontal cortex predicts state switches during reversal learning</article-title><source>Neuron</source><volume>106</volume><fpage>1044</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.03.024</pub-id><pub-id pub-id-type="pmid">32315603</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birrell</surname><given-names>JM</given-names></name><name><surname>Brown</surname><given-names>VJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Medial frontal cortex mediates perceptual attentional set shifting in the rat</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>4320</fpage><lpage>4324</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-11-04320.2000</pub-id><pub-id pub-id-type="pmid">10818167</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boulougouris</surname><given-names>V</given-names></name><name><surname>Dalley</surname><given-names>JW</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Effects of orbitofrontal, infralimbic and prelimbic cortical lesions on serial spatial reversal learning in the rat</article-title><source>Behavioural Brain Research</source><volume>179</volume><fpage>219</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2007.02.005</pub-id><pub-id pub-id-type="pmid">17337305</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>AM</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Operant procedures for assessing behavioral flexibility in rats</article-title><source>Journal of Visualized Experiments</source><volume>1</volume><elocation-id>e52387</elocation-id><pub-id pub-id-type="doi">10.3791/52387</pub-id><pub-id pub-id-type="pmid">25742506</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-12-04745.1992</pub-id><pub-id pub-id-type="pmid">1464765</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Mansouri</surname><given-names>FA</given-names></name><name><surname>Hoda</surname><given-names>H</given-names></name><name><surname>Mahboubi</surname><given-names>M</given-names></name><name><surname>Browning</surname><given-names>PGF</given-names></name><name><surname>Kwok</surname><given-names>SC</given-names></name><name><surname>Phillips</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dissociable components of rule-guided behavior depend on distinct medial and prefrontal regions</article-title><source>Science</source><volume>325</volume><fpage>52</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1126/science.1172377</pub-id><pub-id pub-id-type="pmid">19574382</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campagner</surname><given-names>D</given-names></name><name><surname>Evans</surname><given-names>MH</given-names></name><name><surname>Chlebikova</surname><given-names>K</given-names></name><name><surname>Colins-Rodriguez</surname><given-names>A</given-names></name><name><surname>Loft</surname><given-names>MSE</given-names></name><name><surname>Fox</surname><given-names>S</given-names></name><name><surname>Pettifer</surname><given-names>D</given-names></name><name><surname>Humphries</surname><given-names>MD</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Petersen</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prediction of choice from competing mechanosensory and choice-memory cues during active tactile decision making</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3921</fpage><lpage>3933</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2217-18.2019</pub-id><pub-id pub-id-type="pmid">30850514</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probing perceptual decisions in rodents</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>824</fpage><lpage>831</lpage><pub-id pub-id-type="doi">10.1038/nn.3410</pub-id><pub-id pub-id-type="pmid">23799475</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision-making with multiple alternatives</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>693</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn.2123</pub-id><pub-id pub-id-type="pmid">18488024</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cinotti</surname><given-names>F</given-names></name><name><surname>Humphries</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Bayesian mapping of the striatal microcircuit reveals robust asymmetries in the probabilities and distances of connections</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>1417</fpage><lpage>1435</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1487-21.2021</pub-id><pub-id pub-id-type="pmid">34893550</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinople</surname><given-names>CM</given-names></name><name><surname>Piet</surname><given-names>AT</given-names></name><name><surname>Bibawi</surname><given-names>P</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>C</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Lateral orbitofrontal cortex promotes trial-by-trial learning of risky, but not spatial, biases</article-title><source>eLife</source><volume>8</volume><elocation-id>e49744</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49744</pub-id><pub-id pub-id-type="pmid">31692447</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname><given-names>M</given-names></name><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human cognition. Foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.1252254</pub-id><pub-id pub-id-type="pmid">24876345</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Vittoz</surname><given-names>NM</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Abrupt transitions between prefrontal neural ensemble states accompany behavioral transitions during rule learning</article-title><source>Neuron</source><volume>66</volume><fpage>438</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.029</pub-id><pub-id pub-id-type="pmid">20471356</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.003</pub-id><pub-id pub-id-type="pmid">20153683</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floresco</surname><given-names>SB</given-names></name><name><surname>Block</surname><given-names>AE</given-names></name><name><surname>Tse</surname><given-names>MTL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inactivation of the medial prefrontal cortex of the rat impairs strategy set-shifting, but not reversal learning, using a novel, automated procedure</article-title><source>Behavioural Brain Research</source><volume>190</volume><fpage>85</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2008.02.008</pub-id><pub-id pub-id-type="pmid">18359099</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallistel</surname><given-names>CR</given-names></name><name><surname>Fairhurst</surname><given-names>S</given-names></name><name><surname>Balsam</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The learning curve: implications of a quantitative analysis</article-title><source>PNAS</source><volume>101</volume><fpage>13124</fpage><lpage>13131</lpage><pub-id pub-id-type="doi">10.1073/pnas.0404965101</pub-id><pub-id pub-id-type="pmid">15331782</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Brasted</surname><given-names>PJ</given-names></name><name><surname>Mitz</surname><given-names>AR</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Prefrontal cortex activity related to abstract response strategies</article-title><source>Neuron</source><volume>47</volume><fpage>307</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.06.006</pub-id><pub-id pub-id-type="pmid">16039571</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Sandoz</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Invertebrate learning and memory: Fifty years of olfactory conditioning of the proboscis extension response in honeybees</article-title><source>Learning &amp; Memory</source><volume>19</volume><fpage>54</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1101/lm.024711.111</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guise</surname><given-names>KG</given-names></name><name><surname>Shapiro</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Medial prefrontal cortex reduces memory interference by modifying hippocampal encoding</article-title><source>Neuron</source><volume>94</volume><fpage>183</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.011</pub-id><pub-id pub-id-type="pmid">28343868</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceptual decision making in rodents, monkeys, and humans</article-title><source>Neuron</source><volume>93</volume><fpage>15</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.003</pub-id><pub-id pub-id-type="pmid">28056343</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>C</given-names></name><name><surname>Aguirre</surname><given-names>C</given-names></name><name><surname>Kolli</surname><given-names>S</given-names></name><name><surname>Das</surname><given-names>K</given-names></name><name><surname>Izquierdo</surname><given-names>A</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unique features of stimulus-based probabilistic reversal learning</article-title><source>Behavioral Neuroscience</source><volume>135</volume><fpage>550</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1037/bne0000474</pub-id><pub-id pub-id-type="pmid">34460275</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honegger</surname><given-names>K</given-names></name><name><surname>de Bivort</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stochasticity, individuality and behavior</article-title><source>Current Biology</source><volume>28</volume><fpage>R8</fpage><lpage>R12</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.058</pub-id><pub-id pub-id-type="pmid">29316423</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2023">2023a</year><data-title>Bayesian_Strategy_Analysis_MATLAB</data-title><version designator="swh:1:rev:7014e7ee7c727cc354dd6bcb470b7773a02c9f0b">swh:1:rev:7014e7ee7c727cc354dd6bcb470b7773a02c9f0b</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:35ea132370bd1f7a96d8f8757e5345f6ad9071fa;origin=https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_MATLAB;visit=swh:1:snp:12792188063ed225b10500a0c7fd4714535b9afc;anchor=swh:1:rev:7014e7ee7c727cc354dd6bcb470b7773a02c9f0b">https://archive.softwareheritage.org/swh:1:dir:35ea132370bd1f7a96d8f8757e5345f6ad9071fa;origin=https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_MATLAB;visit=swh:1:snp:12792188063ed225b10500a0c7fd4714535b9afc;anchor=swh:1:rev:7014e7ee7c727cc354dd6bcb470b7773a02c9f0b</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>MD</given-names></name><name><surname>Powell</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023b</year><data-title>Bayesian_Strategy_Analysis_PythonPublic Watch 3 Fork 0 Star 3</data-title><version designator="swh:1:rev:fa978c04230a22b2c4ae687de328d6e81e1bac4e">swh:1:rev:fa978c04230a22b2c4ae687de328d6e81e1bac4e</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b2ee23fda4ea226045702a723c0aac9d3642ba9d;origin=https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_Python;visit=swh:1:snp:c1ceff2d9985fb6650a51f061575fbbca7ec324c;anchor=swh:1:rev:fa978c04230a22b2c4ae687de328d6e81e1bac4e">https://archive.softwareheritage.org/swh:1:dir:b2ee23fda4ea226045702a723c0aac9d3642ba9d;origin=https://github.com/Humphries-Lab/Bayesian_Strategy_Analysis_Python;visit=swh:1:snp:c1ceff2d9985fb6650a51f061575fbbca7ec324c;anchor=swh:1:rev:fa978c04230a22b2c4ae687de328d6e81e1bac4e</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Humphries-Lab</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Bayesian_Strategy_Analysis_Paper</data-title><version designator="swh:1:rev:8e32969b6ab427cca27f0baf05330357f9266cf1">swh:1:rev:8e32969b6ab427cca27f0baf05330357f9266cf1</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e73262548d7356fc1b64c7e4b2414696200d620f;origin=https://github.com/Humphries-Lab/Bayesian_strategy_analysis_Paper;visit=swh:1:snp:969d42be98d6520fa0687d9cba499112243bb46d;anchor=swh:1:rev:8e32969b6ab427cca27f0baf05330357f9266cf1">https://archive.softwareheritage.org/swh:1:dir:e73262548d7356fc1b64c7e4b2414696200d620f;origin=https://github.com/Humphries-Lab/Bayesian_strategy_analysis_Paper;visit=swh:1:snp:969d42be98d6520fa0687d9cba499112243bb46d;anchor=swh:1:rev:8e32969b6ab427cca27f0baf05330357f9266cf1</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>HT</given-names></name><name><surname>Zhang</surname><given-names>SJ</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A prefrontal-thalamo-hippocampal circuit for goal-directed spatial navigation</article-title><source>Nature</source><volume>522</volume><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1038/nature14396</pub-id><pub-id pub-id-type="pmid">26017312</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izquierdo</surname><given-names>A</given-names></name><name><surname>Brigman</surname><given-names>JL</given-names></name><name><surname>Radke</surname><given-names>AK</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural basis of reversal learning: An updated perspective</article-title><source>Neuroscience</source><volume>345</volume><fpage>12</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2016.03.021</pub-id><pub-id pub-id-type="pmid">26979052</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>AI</given-names></name><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Chudasama</surname><given-names>Y</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The role of frontal cortical and medial-temporal lobe brain areas in learning a bayesian prior belief on reversals</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11751</fpage><lpage>11760</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1594-15.2015</pub-id><pub-id pub-id-type="pmid">26290251</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decision-making behaviors: weighing ethology, complexity, and sensorimotor compatibility</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>42</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.11.001</pub-id><pub-id pub-id-type="pmid">29179005</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname><given-names>MP</given-names></name><name><surname>Tervo</surname><given-names>DGR</given-names></name><name><surname>Karpova</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Network resets in medial prefrontal cortex mark the onset of behavioral uncertainty</article-title><source>Science</source><volume>338</volume><fpage>135</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1126/science.1226518</pub-id><pub-id pub-id-type="pmid">23042898</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>JN</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neural correlates of a decision in the dorsolateral prefrontal cortex of the macaque</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>176</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/5739</pub-id><pub-id pub-id-type="pmid">10195203</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: Correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>B</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamic response-by-response models of matching behavior in rhesus monkeys</article-title><source>Journal of the Experimental Analysis of Behavior</source><volume>84</volume><fpage>555</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1901/jeab.2005.110-04</pub-id><pub-id pub-id-type="pmid">16596980</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leeson</surname><given-names>VC</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Matheson</surname><given-names>E</given-names></name><name><surname>Hutton</surname><given-names>SB</given-names></name><name><surname>Ron</surname><given-names>MA</given-names></name><name><surname>Barnes</surname><given-names>TRE</given-names></name><name><surname>Joyce</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Discrimination learning, reversal, and set-shifting in first-episode schizophrenia: stability over six years and specific associations with medication type and disorganization syndrome</article-title><source>Biological Psychiatry</source><volume>66</volume><fpage>586</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2009.05.016</pub-id><pub-id pub-id-type="pmid">19576575</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackintosh</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Further analysis of the overtraining reversal effect</article-title><source>Journal of Comparative and Physiological Psychology</source><volume>67</volume><elocation-id>Suppl</elocation-id><pub-id pub-id-type="doi">10.1037/h0026784</pub-id><pub-id pub-id-type="pmid">5780871</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname><given-names>FA</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Emergence of abstract rules in the primate brain</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>595</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0364-5</pub-id><pub-id pub-id-type="pmid">32929262</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dorsal hippocampus contributes to model-based planning</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1269</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1038/nn.4613</pub-id><pub-id pub-id-type="pmid">28758995</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowak</surname><given-names>M</given-names></name><name><surname>Sigmund</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner’s Dilemma game</article-title><source>Nature</source><volume>364</volume><fpage>56</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1038/364056a0</pub-id><pub-id pub-id-type="pmid">8316296</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohtsuki</surname><given-names>H</given-names></name><name><surname>Hauert</surname><given-names>C</given-names></name><name><surname>Lieberman</surname><given-names>E</given-names></name><name><surname>Nowak</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A simple rule for the evolution of cooperation on graphs and social networks</article-title><source>Nature</source><volume>441</volume><fpage>502</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1038/nature04605</pub-id><pub-id pub-id-type="pmid">16724065</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packard</surname><given-names>MG</given-names></name><name><surname>McGaugh</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Inactivation of hippocampus or caudate nucleus with lidocaine differentially affects expression of place and response learning</article-title><source>Neurobiology of Learning and Memory</source><volume>65</volume><fpage>65</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1006/nlme.1996.0007</pub-id><pub-id pub-id-type="pmid">8673408</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>J</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The effect of stimulus strength on the speed and accuracy of a perceptual decision</article-title><source>Journal of Vision</source><volume>5</volume><fpage>376</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1167/5.5.1</pub-id><pub-id pub-id-type="pmid">16097871</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname><given-names>M</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</article-title><source>Nature</source><volume>442</volume><fpage>1042</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1038/nature05051</pub-id><pub-id pub-id-type="pmid">16929307</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Benchenane</surname><given-names>K</given-names></name><name><surname>Wiener</surname><given-names>SI</given-names></name><name><surname>Battaglia</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Replay of rule-learning related neural patterns in the prefrontal cortex during sleep</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>919</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1038/nn.2337</pub-id><pub-id pub-id-type="pmid">19483687</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezze</surname><given-names>M</given-names></name><name><surname>McGarrity</surname><given-names>S</given-names></name><name><surname>Mason</surname><given-names>R</given-names></name><name><surname>Fone</surname><given-names>KC</given-names></name><name><surname>Bast</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Too little and too much: hypoactivation and disinhibition of medial prefrontal cortex cause attentional deficits</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>7931</fpage><lpage>7946</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3450-13.2014</pub-id><pub-id pub-id-type="pmid">24899715</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probabilistic brains: knowns and unknowns</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1170</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1038/nn.3495</pub-id><pub-id pub-id-type="pmid">23955561</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Powell</surname><given-names>NJ</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Representational changes of latent strategies in rat medial prefrontal cortex precede changes in behaviour</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12830</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12830</pub-id><pub-id pub-id-type="pmid">27653278</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Sheppard</surname><given-names>JP</given-names></name><name><surname>Schrater</surname><given-names>PR</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory decision-making in rats and humans</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3726</fpage><lpage>3735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4998-11.2012</pub-id><pub-id pub-id-type="pmid">22423093</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>EL</given-names></name><name><surname>Shapiro</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Prelimbic/infralimbic inactivation impairs memory for multiple task switches, but not flexible selection of familiar tasks</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>4747</fpage><lpage>4755</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0369-07.2007</pub-id><pub-id pub-id-type="pmid">17460087</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname><given-names>EL</given-names></name><name><surname>Shapiro</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rat prefrontal cortical neurons selectively code strategy switches</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>7208</fpage><lpage>7219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6068-08.2009</pub-id><pub-id pub-id-type="pmid">19494143</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Bak</surname><given-names>JH</given-names></name><name><surname>Laboratory</surname><given-names>IB</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extracting the dynamics of behavior in sensory decision-making experiments</article-title><source>Neuron</source><volume>109</volume><fpage>597</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.004</pub-id><pub-id pub-id-type="pmid">33412101</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name><name><surname>Baxter</surname><given-names>MG</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Frontal cortex subregions play distinct roles in choices between actions and stimuli</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>13775</fpage><lpage>13785</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3541-08.2008</pub-id><pub-id pub-id-type="pmid">19091968</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>E</given-names></name><name><surname>Ma</surname><given-names>T</given-names></name><name><surname>Spanagel</surname><given-names>R</given-names></name><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Toutounji</surname><given-names>H</given-names></name><name><surname>Köhr</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Coordinated prefrontal state transition leads extinction of reward-seeking behaviors</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>2406</fpage><lpage>2419</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2588-20.2021</pub-id><pub-id pub-id-type="pmid">33531416</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiner</surname><given-names>T</given-names></name><name><surname>Symmonds</surname><given-names>M</given-names></name><name><surname>Guitart-Masip</surname><given-names>M</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dopamine, salience, and response set shifting in prefrontal cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3629</fpage><lpage>3639</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu210</pub-id><pub-id pub-id-type="pmid">25246512</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Humphries</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Medial prefrontal cortex population activity is plastic irrespective of learning</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3470</fpage><lpage>3483</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1370-17.2019</pub-id><pub-id pub-id-type="pmid">30814311</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AC</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Wirth</surname><given-names>S</given-names></name><name><surname>Yanike</surname><given-names>M</given-names></name><name><surname>Hu</surname><given-names>D</given-names></name><name><surname>Kubota</surname><given-names>Y</given-names></name><name><surname>Graybiel</surname><given-names>AM</given-names></name><name><surname>Suzuki</surname><given-names>WA</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dynamic analysis of learning in behavioral experiments</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>447</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2908-03.2004</pub-id><pub-id pub-id-type="pmid">14724243</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian computation through cortical latent dynamics</article-title><source>Neuron</source><volume>103</volume><fpage>934</fpage><lpage>947</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.012</pub-id><pub-id pub-id-type="pmid">31320220</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>WA</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Behavioral and neurophysiological analyses of dynamic learning processes</article-title><source>Behavioral and Cognitive Neuroscience Reviews</source><volume>4</volume><fpage>67</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1177/1534582305280030</pub-id><pub-id pub-id-type="pmid">16251726</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tait</surname><given-names>DS</given-names></name><name><surname>Bowman</surname><given-names>EM</given-names></name><name><surname>Neuwirth</surname><given-names>LS</given-names></name><name><surname>Brown</surname><given-names>VJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Assessment of intradimensional/extradimensional attentional set-shifting in rats</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>89</volume><fpage>72</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.02.013</pub-id><pub-id pub-id-type="pmid">29474818</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trepka</surname><given-names>E</given-names></name><name><surname>Spitmaan</surname><given-names>M</given-names></name><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Entropy-based metrics for predicting choice behavior based on local response to reward</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6567</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26784-w</pub-id><pub-id pub-id-type="pmid">34772943</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wingerden</surname><given-names>M</given-names></name><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Lankelma</surname><given-names>J</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Theta-band phase locking of orbitofrontal neurons during reward expectancy</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>7078</fpage><lpage>7087</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3860-09.2010</pub-id><pub-id pub-id-type="pmid">20484650</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Tait</surname><given-names>DS</given-names></name><name><surname>Brown</surname><given-names>VJ</given-names></name><name><surname>Bowman</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exacerbation of the credit assignment problem in rats with lesions of the medial prefrontal cortex is revealed by Bayesian analysis of behavior in the pre-solution period of learning</article-title><source>Behavioural Brain Research</source><volume>372</volume><elocation-id>112037</elocation-id><pub-id pub-id-type="doi">10.1016/j.bbr.2019.112037</pub-id><pub-id pub-id-type="pmid">31202862</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Kavanova</surname><given-names>M</given-names></name><name><surname>Hickman</surname><given-names>L</given-names></name><name><surname>Boschin</surname><given-names>EA</given-names></name><name><surname>Galeazzi</surname><given-names>JM</given-names></name><name><surname>Verhagen</surname><given-names>L</given-names></name><name><surname>Ainsworth</surname><given-names>M</given-names></name><name><surname>Pedreira</surname><given-names>C</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Low-beta repetitive transcranial magnetic stimulation to human dorsolateral prefrontal cortex during object recognition memory sample presentation, at a task-related frequency observed in local field potentials in homologous macaque cortex, impairs subsequent recollection but not familiarity</article-title><source>The European Journal of Neuroscience</source><volume>54</volume><fpage>7918</fpage><lpage>7945</lpage><pub-id pub-id-type="doi">10.1111/ejn.15535</pub-id><pub-id pub-id-type="pmid">34796568</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>HH</given-names></name><name><surname>Knowlton</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Contributions of striatal subregions to place and response learning</article-title><source>Learning &amp; Memory</source><volume>11</volume><fpage>459</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1101/lm.81004</pub-id><pub-id pub-id-type="pmid">15286184</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86491.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.30.505807" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.30.505807"/></front-stub><body><p>This work describes a valuable method for indexing trial-by-trial learning and decision making strategies in animal and human behavior. The study provides compelling evidence for the validity of this new method.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86491.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.30.505807">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.30.505807v3">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Tracking subjects' strategies in behavioural choice experiments at trial resolution&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1. The method ignores the precision of the posterior in its selection of the best strategy. This is more of a &quot;winner-take-all&quot; approach rather than a method that exploits the Bayesian framework to take the uncertainty of the posterior into account.</p><p>2. The authors do not convincingly demonstrate that their method is robust to the presence or absence of a true strategy. Both reviewers ask for an analysis that shows what happens to the prediction when the true strategy is present or absent.</p><p>3. Figures 3 c through f should be clarified.</p><p>4. The choice of strategies to test the approach is limited. Strategies are static and are unparametrized for choice stochasticity and trial history dependence. Given that more sophisticated strategies are not explored, it is unclear whether this method can be useful in arbitrating between those.</p><p>5. Win-Stay Lose-Shift analysis is confounded and provides limited evidence for what can be learned in real data using the method.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. The approach is presented as Bayesian, however, that is really a stretch, in ways that matter for the interpretation of the tool. First, the evidence at each trial is taken to be all or nothing (l94-95), rather than integrating over any potential uncertainty. This might make sense for deterministic strategies but it is unclear why this approach is taken for non-deterministic policies. I note that for all real data, assuming deterministic policies is very unrealistic. Second, the full posterior computed by the Bayesian method isn't fully used to compare strategies. There should be a better way to incorporate the variance of the posterior into the decision rule (lines 202-204). The variance shouldn't be completely ignored if the MAP probabilities are different. More generally, the full assumptions of the model should be more carefully described when the approach is presented.</p><p>2. The strategy space considered is problematic in a few ways, despite the authors' efforts to include probabilistic strategies.</p><p>a. First the strategies considered are all static, despite the emphasis on a dynamic environment. In that sense, it should be made extremely clear that this method is purely descriptive, rather than explanatory. It is not a modeling approach, rather a data analysis approach that might allow researchers to answer questions of the type &quot;by when did the animal reliably express a [follow the light] strategy&quot;, for example. It cannot offer insights into how the animal arrives at the strategy or learns it.</p><p>b. The possibility that the true strategy might be missing is not sufficiently discussed or analyzed. The authors should show comparisons between the case where the true strategy is known and the case where it is not. For example, what would Figure 2b&amp;c look like if the true strategies are not considered?</p><p>c. The authors make strong statements that are not adequately supported. For example, &quot;a MAP probability approaching 1 is strong evidence that the tested strategy is, or equivalent to, the true strategy (lines 271-273).&quot; The paper does not include any analysis that supports this statement.</p><p>3. The previous two points may limit the usefulness of the new technique. In practice, animals' policies in dynamic environments are unlikely to be stable and unparameterized; most of the literature instead relies on strategies that include noise parameters, and potentially are dependent on trial history in a parameterized way (e.g., multi-choice stickiness, or reward sensitivity). There is no clear way in which such strategies could be considered by the current method; and by contrast, existing model fitting methods would work well for such strategies, which are also more likely to be relevant.</p><p>4. Some analyses lack details in descriptions or analysis in a way that makes them hard to evaluate.</p><p>a. Figure 3 c/f: the plots are very difficult to read due to scaling, making the comparison difficult.</p><p>b. If the strategy is irrelevant in a given trial, the numbers are not updated (i.e., the trial is neither a match nor a non-match). This complicates the win-stay lose shift result interpretation, as it is likely to be a different proportion of win vs. lose trials. This could lead to the apparent slower decay, which would obviate the conclusion that WS-LS is not a single strategy, but two separate strategies.</p><p>c. Figures S5-S6 consider the case of probabilistic strategies; however, the task and strategies considered are insufficiently described. In Figure S5, the authors should show p(match) as a function of MAP probability instead of the other way around, because in practice we are more interested in estimating p(match) with MAP probability.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I find this approach to be extremely compelling and highly useful. I do have one concern about how robust this approach is to the presence or absence of the real strategy in the candidates that are being tested and updated. This is a slightly different question than what was addressed with the enumeration of possible values of p(match) described on pages 8-9. I would like to see a test that compares the MAP estimates of incorrect strategies A and B when the true strategy C is absent versus present in the strategies being tested.</p><p>I congratulate the authors on a well-written and coherent manuscript. I have a few specific suggestions for improving the clarity and readability:</p><p>General: check your manuscript for use of the passive voice.</p><p>Page 9 line 217: consider re-writing the sentence starting with &quot;We deemed […]&quot;.</p><p>Figure 2, panels h and i: the caption reads &quot;number of trials after the crossover trial until detection, as a function of how quickly the probabilities […] change per trial&quot;. While I understand that the word &quot;quickly&quot; is being used as a substitute for the amount of probability change, it is misleading in that the x-axis of those plots has nothing to do with time.</p><p>Figure 3, panels c and f: I found these figures to be especially confusing. It is not clear what each dot represents in each case (in some cases it is trials and in others it is animals). The point the authors are trying to convey is thus not effectively conveyed. For example, the takeaway is that there is a significant difference in the learning trials for each rule between the original and the strategy criterion for the first rule and not the second, aside from the p values, the scatter plots do not serve as effective visualizations of this point.</p><p>Page 12 line 366: The section title should read &quot;lose-shift, not win-stay&quot; instead of the current version (it is inverted).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86491.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. The method ignores the precision of the posterior in its selection of the best strategy. This is more of a &quot;winner-take-all&quot; approach rather than a method that exploits the Bayesian framework to take the uncertainty of the posterior into account.</p></disp-quote><p>As we use selection of the best strategy only once, in our view this refers to a minor aspect of analysing a simulation, and not a critique of our main contributions – the algorithm and its insights on learning. But we agree with the underlying sentiment that more could be done to demonstrate the use of having the full posterior, and expand on this below. We note that our aim was to be careful in the paper to separate the method, the estimation of the trial-resolution posterior of p(strategy), from the use of its output by further processing of the posterior, and this comment refers to the latter, not the former.</p><p>We use selection of the best strategy only once when quantifying the performance of the algorithm on the example simulation of a synthetic agent. As implied by the phrase “winner-takes-all”, here we use a simple approach of choosing the strategy with the maximum MAP, and breaking ties by choosing the strategy with the highest precision, indicating lower uncertainty. While we suggest in the text this could be a useful approach to apply to data, we readily acknowledged that richer criteria could be derived from the posterior distributions obtained by the algorithm (lines 602-612 in the original submission). To make these points more clearly, we have redrafted the Results text describing the synthetic agent simulation (191-194) and the Discussion section suggesting further development in the use of the algorithm’s output (lines 881-890).</p><p>In the paper we outlined two questions we aimed to tackle given the posterior distribution of each strategy: detecting learning and tracking the use of exploratory strategies. Consequently we do not “detect the best strategy” in any data analyses we present (Figure 3-7 and accompanying supplementary figures). We have redrafted the opening paragraphs of the Results (lines 74-79) and the relevant paragraph in the Discussion (lines 875-880) to further clarify the core questions of the paper.</p><p>On the issue of making further use of the uncertainty in the posterior we have done three things:</p><p>We have added a paragraph to the Results after we introduce the algorithm (lines 155 – 160) to draw the reader’s attention to how we use the posterior and what more could be done with it.</p><p>We have given a fuller account of the behaviour of the posterior as a function of the forgetting rate (γ): the Methods now include a clearly marked section and text discussing this around Equations 15 and 16 (lines 947-962); and Figure 2 now includes panels g-i showing the posterior’s behaviour.</p><p>We developed and tested two further criteria for learning that considered the uncertainty in the posterior. These are presented in the Results (lines 312-326); outlined in the Methods (lines 802-808); and the outcomes of using these criteria on the Y-maze and lever-press task are summarised in Figure 3 – Supplemental Figure 1. We find all criteria replicate the result that rats learnt the switch from a cued to a spatial rule considerably faster than switching from a spatial to a cued rule.</p><disp-quote content-type="editor-comment"><p>2. The authors do not convincingly demonstrate that their method is robust to the presence or absence of a true strategy. Both reviewers ask for an analysis that shows what happens to the prediction when the true strategy is present or absent.</p></disp-quote><p>As noted above, the determination of which is the “true” strategy was not our goal, and we do not use our algorithm in this way in the paper. Rather, we focused on tracking the probability of user-specified strategies, so that we can capture the evidence for learning or for the features driving exploratory choice (e.g. are agents responding to losses or wins; are they responding to cues or choice etc).</p><p>One reason for this focus is that to our minds detecting a single “true” strategy is ambiguous. For the observer, multiple strategies may be logically equivalent – for example, a subject that consistently chooses “go right” and gets rewarded is also consistently choosing “win-stay” to their choice. Subjects may be actively using more than one strategy – for example, when they switch strategies when learning a new rule after a previously-established one, then both are often expressed on different trials (e.g. Figure 1d and f). Thus our algorithm seeks to track the probability of the use of each strategy, and interpret these as the observer’s estimate of the likelihood of their expression.</p><p>We have extensively redrafted the section on analysing the algorithm’s performance (Section “Robust tracking of strategies in synthetic data”) to clarify the goals of that analysis and how each set of simulations or analyses addresses them: (1) to provide the rationale for the use of evidence decay; (2) to show how evidence decay affects the algorithm’s output, and thus provide a basis for defining usable values of that parameter; and (3) to provide aid in interpreting the resulting values of P(strategy). In the context of (3), we have considered what values P(strategy) can take when a true exploratory strategy is missing. We redrafted the text on the analysis of the missing “true strategy” (lines 239-246) to clarify its aims and insights.</p><p>In redrafting this section we also addressed individual reviewer requests for more information on the limits of the method (the behaviour of the posterior, noted above, lines 209-217) and concerns about the choice of values for the evidence decay parameter (esp. lines 220-230).</p><disp-quote content-type="editor-comment"><p>3. Figures 3 c through f should be clarified.</p></disp-quote><p>We have done the following to clarify the results:</p><list list-type="bullet"><list-item><p>To panel c: added a histogram of the number of identified learning trials per animal</p></list-item><list-item><p>To panel f: shown all individual animals’ data in light, open symbols; reduced the jitter of the symbols to align them better with the x-axis labels; and overplotted the mean and standard error of the mean as solid symbols to provide a clearer summary of the main results (of earlier detection of learning by strategy-based criterion during the first learnt rule; and of slower learning of a cued rule when it followed a spatial rule).</p></list-item><list-item><p>Added ANOVAs to give statistical support to the result that, in the lever-press task, learning the cued rule after a spatial rule is slower than the reverse.</p></list-item></list><disp-quote content-type="editor-comment"><p>4. The choice of strategies to test the approach is limited. Strategies are static and are unparametrized for choice stochasticity and trial history dependence. Given that more sophisticated strategies are not explored, it is unclear whether this method can be useful in arbitrating between those.</p></disp-quote><p>We politely disagree with this statement. We tested 14 different strategies in the paper (Tables 1-3 of the Methods). These include a range of strategies that use trial history dependence (e.g. Figure 7 and Table 3), and the Discussion already touched on ways to extend to other trial history dependence, such as dependence on outcome N trials in the past. We have redrafted the Results text around Figure 7 (lines 459-463) and the Discussion section (lines 602-606) to make this more explicit.</p><p>We have clarified that our results already showed the algorithm can track the stochastic use of a strategy (Results lines 223-227, and Figure 2 – Supplemental Figure 3).</p><disp-quote content-type="editor-comment"><p>5. Win-Stay Lose-Shift analysis is confounded and provides limited evidence for what can be learned in real data using the method.</p></disp-quote><p>We agree that we did not explain this well. Indeed in the absence of losses Lose-Shift cannot be updated, and in the absence of wins Win-Shift cannot be updated. However, this analysis focussed on the trials preceding the detected learning trial or the trials following a rule switch. In both cases, there is a mixture of wins and losses, so the probabilities of both Lose-Shift and Win-Stay can be updated. We now show this explicitly by plotting the per-trial rate of correct choices around both learning and rule-shifts in Figure 5. The accompanying text results have been redrafted to acknowledge the need for evidence of losses and wins [lines 371-373 and 385-387]; we have also simplified the text in the Results discussing Figure 5 to further clarify the focus of the analysis on the trials preceding learning and following rule-switches [lines 367 – 389].</p><p>Further Revisions</p><p>Reviewer 1 suggested we “Define &quot;strategy&quot; and/or give examples in the Introduction because it can be a vague concept to readers from different backgrounds.” The Introduction now gives explicit examples (lines 40-43).</p><p>Reviewer 1 commented that “it should be made extremely clear that this method is purely descriptive, rather than explanatory. It is not a modeling approach, rather a data analysis approach”. We are in firm agreement: indeed the Discussion included a dedicated paragraph on this point beginning “The probabilities computed by our algorithm are a description of the observer…”, and throughout the algorithm was described from the perspective of the observer, not the agent. To make this even clearer, we have edited the Results text introducing the method to emphasise that the evidence and computation are from the observer’s point of view (lines 71, 87-91).</p><p>We have revised text throughout for clarity.</p></body></sub-article></article>