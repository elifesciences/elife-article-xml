<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">58516</article-id><article-id pub-id-type="doi">10.7554/eLife.58516</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Reconstruction of natural images from responses of primate retinal ganglion cells</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-131602"><name><surname>Brackbill</surname><given-names>Nora</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0308-1382</contrib-id><email>nbrackbill@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131603"><name><surname>Rhoades</surname><given-names>Colleen</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131604"><name><surname>Kling</surname><given-names>Alexandra</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-127402"><name><surname>Shah</surname><given-names>Nishal P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1275-0381</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131606"><name><surname>Sher</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-21116"><name><surname>Litke</surname><given-names>Alan M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3973-3642</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-21117"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5613-0248</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Physics, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Bioengineering, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Neurosurgery, Stanford School of Medicine</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Department of Ophthalmology, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Hansen Experimental Physics Laboratory, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution>Department of Electrical Engineering, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution>Santa Cruz Institute for Particle Physics, University of California, Santa Cruz</institution><addr-line><named-content content-type="city">Santa Cruz</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Meister</surname><given-names>Markus</given-names></name><role>Reviewing Editor</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>11</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e58516</elocation-id><history><date date-type="received" iso-8601-date="2020-05-02"><day>02</day><month>05</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-11-02"><day>02</day><month>11</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Brackbill et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Brackbill et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-58516-v2.pdf"/><abstract><p>The visual message conveyed by a retinal ganglion cell (RGC) is often summarized by its spatial receptive field, but in principle also depends on the responses of other RGCs and natural image statistics. This possibility was explored by linear reconstruction of natural images from responses of the four numerically-dominant macaque RGC types. Reconstructions were highly consistent across retinas. The optimal reconstruction filter for each RGC – its visual message – reflected natural image statistics, and resembled the receptive field only when nearby, same-type cells were included. ON and OFF cells conveyed largely independent, complementary representations, and parasol and midget cells conveyed distinct features. Correlated activity and nonlinearities had statistically significant but minor effects on reconstruction. Simulated reconstructions, using linear-nonlinear cascade models of RGC light responses that incorporated measured spatial properties and nonlinearities, produced similar results. Spatiotemporal reconstructions exhibited similar spatial properties, suggesting that the results are relevant for natural vision.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Vision begins in the retina, the layer of tissue that lines the back of the eye. Light-sensitive cells called rods and cones absorb incoming light and convert it into electrical signals. They pass these signals to neurons called retinal ganglion cells (RGCs), which convert them into electrical signals called spikes. Spikes from RGCs then travel along the optic nerve to the brain. They are the only source of visual information that the brain receives. From this information, the brain constructs our entire visual world.</p><p>The primate retina contains roughly 20 types of RGCs. Each encodes a different visual feature, such as the presence of bright spots of a certain size, or information about texture and movement. But exactly what input each RGC sends to the brain, and how the brain uses this information, is unclear. Brackbill et al. set out to answer these questions by measuring and analyzing the electrical activity in isolated retinas from macaque monkeys. Studying the macaque retina was important because the primate visual system differs from that of other species in several ways. These include the numbers and types of RGCs present in the retina. These primates are also similar to humans in their high-resolution central vision and trichromatic color vision.</p><p>Using electrode arrays to monitor hundreds of RGCs at the same time, Brackbill et al. recorded the responses of macaque retinas to real-life images of landscapes, objects, animals or people. Based on these recordings, plus existing knowledge about RGC responses, Brackbill et al. then attempted to reconstruct the original images using just the electrical activity recorded. The resulting reconstructions were similar across all retinas tested. Moreover, they showed a striking resemblance to the original images. These results made it possible to comprehend how the light-response properties of each cell represent visual information that can be used by the brain.</p><p>Understanding how macaque retinas work in natural conditions is critical to decoding how our own retinas process and convey information. A better knowledge of how the brain uses this input to generate images could ultimately make it possible to design artificial retinas to restore vision in patients with certain forms of blindness.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>macaca fascicularis</kwd><kwd>reconstruction</kwd><kwd>decoding</kwd><kwd>retina</kwd><kwd>natural images</kwd><kwd>retinal ganglion cells</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF IGERT 0801700</award-id><principal-award-recipient><name><surname>Brackbill</surname><given-names>Nora</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>GRFP DGE-114747</award-id><principal-award-recipient><name><surname>Brackbill</surname><given-names>Nora</given-names></name><name><surname>Rhoades</surname><given-names>Colleen</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>F31EY027166</award-id><principal-award-recipient><name><surname>Rhoades</surname><given-names>Colleen</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000875</institution-id><institution>Pew Charitable Trusts</institution></institution-wrap></funding-source><award-id>Fellowship in Biomedical Sciences</award-id><principal-award-recipient><name><surname>Sher</surname><given-names>Alexander</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>John Chen</institution></institution-wrap></funding-source><award-id>Donation</award-id><principal-award-recipient><name><surname>Litke</surname><given-names>Alan M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EY017992</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY029247</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01-EY029247</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>CRCNS Grant IIS-1430348</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>CRCNS Grant IIS-1430348</award-id><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution>Wu Tsai Neurosciences Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The visual message conveyed by retinal neurons to the brain when signaling natural scenes resembles the individual receptive fields only when viewed in context of the neuronal population.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain uses visual information transmitted by retinal neurons to make inferences about the external world. Traditionally, the visual signal transmitted by an individual retinal ganglion cell (RGC) has been summarized by its spatial profile of light sensitivity, or receptive field (RF), measured with stimuli such as spots or bars (<xref ref-type="bibr" rid="bib7">Chichilnisky, 2001</xref>; <xref ref-type="bibr" rid="bib38">Kuffler, 1953</xref>; <xref ref-type="bibr" rid="bib40">Lettvin et al., 1959</xref>). Although intuitively appealing, this description may not reveal how the spikes from a RGC contribute to the visual representation in the brain under natural viewing conditions. In particular, because of the strong spatial correlations in natural images (<xref ref-type="bibr" rid="bib63">Ruderman and Bialek, 1994</xref>), the response of a single RGC contains information about visual space well beyond its RF. Thus, across the RGC population, the responses of many individual cells could contain information about the same region of visual space, and it is not obvious how the brain could exploit this potentially redundant information (<xref ref-type="bibr" rid="bib56">Puchalla et al., 2005</xref>). Complicating this issue is the fact that there are roughly twenty RGC types, each covering all of visual space with their RFs, and each with different spatial, temporal, and chromatic sensitivity (<xref ref-type="bibr" rid="bib10">Dacey et al., 2003</xref>). Furthermore, RGCs show both stimulus-induced and stimulus-independent correlated activity, within and across cell types (<xref ref-type="bibr" rid="bib34">Greschner et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Mastronarde, 1983</xref>), which could substantially influence the encoding of the stimulus (<xref ref-type="bibr" rid="bib47">Meytlis et al., 2012</xref>; <xref ref-type="bibr" rid="bib54">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">Ruda et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Zylberberg et al., 2016</xref>). For these reasons, the visual message transmitted by a RGC to the brain is not fully understood.</p><p>One way to understand how each RGC contributes to vision is to determine how a natural image can be reconstructed from the light-evoked responses of the entire RGC population. This analysis approach mimics the challenge faced by the brain: using sensory inputs to make inferences about the visual environment (<xref ref-type="bibr" rid="bib4">Bialek et al., 1991</xref>; <xref ref-type="bibr" rid="bib59">Rieke et al., 1997</xref>). In the simplest case of linear reconstruction, the visual message of an individual RGC can be summarized by its optimal reconstruction filter, that is its contribution to the reconstructed image. Linear reconstruction has been used to estimate the temporal structure of a spatially uniform stimulus from the responses of salamander RGCs, revealing that reconstruction filters varied widely and depended heavily on the other RGCs included in the reconstruction (<xref ref-type="bibr" rid="bib76">Warland et al., 1997</xref>). However, no spatial information was explored, and only a small number of RGCs of unknown types were examined. A later study linearly reconstructed spatiotemporal natural movies from the activity of neurons in the cat LGN (<xref ref-type="bibr" rid="bib66">Stanley et al., 1999</xref>). However, neurons from many recordings were pooled, without cell type identification or the systematic spatial organization expected from complete populations of multiple cell types. More recently, several studies have used nonlinear and machine learning methods for reconstruction (<xref ref-type="bibr" rid="bib5">Botella-Soler et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Zhang et al., 2020</xref>), although these techniques were not tested in primate, or on large-scale data sets with clear cell type identifications and complete populations of RGCs. Thus, it remains unclear what spatial visual message primate RGCs convey to the brain, in the context of natural scenes and the full neural population.</p><p>We performed linear reconstruction of flashed natural images from the responses of hundreds of RGCs in macaque retina, using large-scale, multi-electrode recordings. These recordings provided simultaneous access to the visual signals of nearly complete populations of ON and OFF parasol cells, as well as locally complete populations of ON and OFF midget cells, the four numerically dominant RGC types that provide high-resolution visual information to the brain (<xref ref-type="bibr" rid="bib10">Dacey et al., 2003</xref>). Data from 15 recordings produced strikingly similar reconstructions. Examination of reconstruction filters revealed that the visual message of a given RGC depended on the responses of other RGCs, due to the statistics of natural scenes. Reconstruction from complete cell type populations revealed that they conveyed different features of the visual scene, consistent with their distinct light response properties. The spatial information carried by one type was mostly unaffected by the contributions of other types, particularly types with the opposite response polarity (ON vs. OFF). Two simple tests of nonlinear reconstruction revealed only minor improvements over linear reconstruction. Similar visual messages and reconstructions were obtained using linear-nonlinear cascade models of RGC light response incorporating measured spatial properties and response nonlinearity. Finally, full spatiotemporal reconstruction with dynamic scenes revealed similar spatial visual messages, suggesting that these findings may generalize to natural vision.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Large-scale multi-electrode recordings from the peripheral macaque retina were used to characterize light responses in complete populations of RGCs (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib23">Field et al., 2010</xref>; <xref ref-type="bibr" rid="bib24">Frechette et al., 2005</xref>; <xref ref-type="bibr" rid="bib41">Litke et al., 2004</xref>). The classical RF of each cell was measured by reverse correlation between its spike train and a spatiotemporal white noise stimulus, resulting in a spike-triggered average (STA) stimulus that summarized the spatial, temporal and chromatic properties of the cell (<xref ref-type="bibr" rid="bib7">Chichilnisky, 2001</xref>). Clustering of these properties revealed multiple identifiable and complete cell type populations (see Materials and methods; <xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib9">Dacey, 1993</xref>; <xref ref-type="bibr" rid="bib17">Devries and Baylor, 1997</xref>; <xref ref-type="bibr" rid="bib21">Field et al., 2007</xref>; <xref ref-type="bibr" rid="bib24">Frechette et al., 2005</xref>), including the four numerically dominant RGC types in macaque: ON parasol, OFF parasol, ON midget, and OFF midget. The RFs of each identified type formed an orderly lattice (mosaic), consistent with the spatial organization of each RGC type known from anatomical studies (<xref ref-type="bibr" rid="bib77">Wässle et al., 1983</xref>).</p><p>Responses to natural images were then characterized by displaying static, grayscale images from the ImageNet database, which contains a wide variety of subjects including landscapes, objects, people, and animals (<xref ref-type="bibr" rid="bib20">Fei-Fei et al., 2009</xref>). Each image was displayed for 100 ms, separated by 400 ms of spatially uniform illumination with intensity equal to the mean intensity across all images (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This stimulus timing produced a strong initial response from both parasol and midget cells, and a return to maintained firing rates prior to the onset of the next image. For each image, the population response was quantified as a vector of RGC spike counts in the 150 ms window after image onset (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; window chosen to optimize reconstruction performance; see Materials and methods). The stimulus (S, dimensions: number of images x number of pixels) was reconstructed from the recorded ON and OFF parasol and midget cell responses (R, dimensions: number of images x number of cells) using a linear model, S = RW. The optimal weights for the linear model (W, dimensions: number of cells x number of pixels) were calculated using least squares regression,<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>R</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>S</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Linear reconstruction from ON and OFF parasol cell responses.</title><p>(<bold>A</bold>) Visual stimulus: static images from the ImageNet database were displayed for 100 ms, with 400 ms of gray between. The thin black rectangle indicates the central image region shown in C and E. (<bold>B</bold>) Example population response: each entry corresponds to the number of spikes from one RGC in a 150 ms window (shown in blue) after the image onset. (<bold>C</bold>) Left: Examples of reconstruction filters for an ON (top) and OFF (bottom) parasol cell. Right: RF locations for the entire population of ON (top) and OFF (bottom) parasol cells used in one recording. (<bold>D</bold>) Reconstruction performance (correlation) across all recordings. (<bold>E</bold>) Example reconstructions for three representative scores (middle row), compared to original images (top row) and smoothed images (bottom row), from the same recording and at the same scale as shown in C. (<bold>F</bold>) Reconstruction performance across 15 recordings. Left: Distributions of scores across images for each recording, ordered by average receptive field (RF) size. Right: Average reconstruction performance vs. average RF radius (ρ=−0.7). Source files for D and F are available in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Linear reconstruction from ON and OFF parasol cell responses.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig1">Figure 1D and F</xref>, which show the distribution of reconstruction scores across recordings, as well as the relationship between reconstruction performance and receptive field (RF) size.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig1-v2.tif"/></fig><p>The weights were then used to reconstruct a held-out set of test images. Reconstruction performance was measured by comparing only the areas of the original and reconstructed images covered by the RF mosaic for each RGC type included in the analysis (see Materials and methods). Pearson’s linear correlation coefficient (ρ) was used as the performance metric; mean squared error (MSE) and the structural similarity (SSIM; <xref ref-type="bibr" rid="bib74">Wang et al., 2004</xref>) showed the same trends. All statistical tests were computed using resampling to generate null models (see Materials and methods). Regularization of reconstruction weights was not necessary, because the number of samples was much larger (&gt;20 x) than the number of parameters in all cases (see Materials and methods). In what follows, reconstruction ‘from RGCs’ is used as a shorthand to indicate reconstruction from their recorded responses, as described above.</p><p>The basic characteristics of spatial linear reconstruction were evaluated by reconstructing images from the responses of populations of ON and OFF parasol cells in 15 recordings from nine monkeys. In each case, both cell types formed complete or nearly complete mosaics with uniform coverage, indicating that nearly every cell of each type over the electrode array was recorded (see <xref ref-type="fig" rid="fig1">Figures 1C</xref> and <xref ref-type="fig" rid="fig2">2</xref>). Thus, the reconstructions revealed the full visual representation in these RGC populations. In each recording, reconstruction performance varied considerably across the set of test images (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, ρ=0.76 +/- 0.12 across n = 2250 images from 15 recordings), but was similar for repeated presentations of the same image (standard deviation across repeats = 0.014). Reconstruction performance was also similar for presentations of the same image in different recordings (standard deviation across recordings = 0.039), despite differences in the population responses and the properties of the RF mosaics (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The reconstructed images themselves were also very similar across recordings (ρ=0.90 +/- 0.06, across 150 images and 66 pairs of recordings; <xref ref-type="fig" rid="fig2">Figure 2</xref>). The minor differences in performance between recordings were correlated with the average RF size in each recording (ρ=−0.7), which in turn is inversely related to RGC density (<xref ref-type="bibr" rid="bib17">Devries and Baylor, 1997</xref>; <xref ref-type="bibr" rid="bib27">Gauthier et al., 2009</xref>). Qualitatively, large scale image structure seemed to be well captured, but fine details were not. These results indicate that the image structure and the spatial resolution of the RGC population, rather than response variability, were primarily responsible for variation in reconstruction performance across images and recordings.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual representation across retinas.</title><p>(<bold>A</bold>) Distribution of correlation between reconstructed images from different recordings, across 150 images and 66 pairs of recordings. (<bold>B</bold>) Example image. (<bold>C</bold>) Across 12 recordings, reconstructed images (top, averaged across trials), ON (middle, blue) and OFF (bottom, orange) parasol responses, shown as the mosaic of Gaussian RF fits shaded by the spike count in response to this image. Source files for A are available in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Visual representation across retinas.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig2">Figure 2A</xref>, which shows the similarity of reconstructed images across separate recordings.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig2-v2.tif"/></fig><p>To further probe the role of the spatial resolution of the RGC population, the reconstructed images were compared to smoothed images, created by convolving the original images with a Gaussian matching the average parasol cell RF size for each recording (see <xref ref-type="fig" rid="fig1">Figure 1E</xref>, bottom row). Broadly, the smoothed images provided a good approximation to the images obtained by reconstruction. On average, the reconstructed image (averaged across trials) was more similar to the smoothed image than to the original image (ρ=0.91 +/- 0.06 vs. ρ=0.78 +/- 0.11 across n = 2250 images from 15 recordings; p&lt;0.001). The residuals from reconstruction and smoothing, obtained by subtracting the original image, were also similar (ρ=0.83 +/- 0.09), suggesting that reconstruction and smoothing captured and discarded similar features of the original images. While smoothed images do not represent a strict upper limit on reconstruction performance, this analysis further indicates that the RGC density is an important factor in image reconstruction.</p><p>Spike latency was also tested as a measure of population response. Spike latency has been shown to convey more stimulus information than spike counts in salamander RGCs in certain conditions (<xref ref-type="bibr" rid="bib32">Gollisch and Meister, 2008</xref>). The RGC response was defined as the time from the image onset to the time of the first spike. This latency response measure led to less accurate reconstruction performance overall (reconstruction from ON and OFF parasol cell responses: Δρ=−0.10 +/- 0.12 across 4500 images from 15 recordings, p&lt;0.001; reconstruction from ON and OFF midget cell responses: −0.16 +/- 0.19 across 3300 images from 11 recordings, p&lt;0.001), although it did improve performance for reconstruction from ON parasol cells alone in two recordings (Δρ=0.04 +/- 0.12 across 600 images from two recordings, p&lt;0.001) and from ON midget cells alone in one recording (Δρ=0.02 +/- 0.1 across 300 images, p&lt;0.001).</p><sec id="s2-1"><title>The visual message conveyed by RGCs</title><p>To understand how the visual message conveyed by a single RGC depends on the signals transmitted by others, reconstruction was performed from a given cell alone or with other cells of the same type. Cells of the same type exhibited similar response properties (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>), with non-overlapping RFs forming a mosaic tiling visual space (<xref ref-type="fig" rid="fig2">Figure 2</xref>). When a single cell was used for reconstruction, its reconstruction filter (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top) was much wider than its spatial RF (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, bottom, measured with white noise; see Materials and methods), or the spatially localized filter obtained in the full population reconstruction described above (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The full width at half maximum of the average single-cell reconstruction filter was roughly four times the average RF width (3.6 +/- 1.4 across 15 recordings). As additional RGCs of the same type were included in reconstruction, the spatial spread of the primary cell’s reconstruction filter was progressively reduced, leveling off to a value slightly higher than the average RF size when the six nearest neighbors were included (1.3 +/- 0.2 across 15 recordings; average filters shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, widths shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Effect of the population on the visual message.</title><p>(<bold>A</bold>) The reconstruction filter of a single cell as more neighboring cells are included in the reconstruction. Left: receptive fields (RFs) of cells in reconstruction, with the primary cell indicated in blue. Right: Filter of the primary cell. (<bold>B</bold>) Autocorrelation structure of the natural images used here. (<bold>C</bold>) Average ON (left) and OFF (right) parasol cell filters for a single recording. From top to bottom: reconstruction from a single cell, reconstruction from that cell plus all nearest neighbors, reconstruction from that cell plus all cells of the same type, and that cell’s RF. (<bold>D</bold>) Filter width, normalized by the RF width. (<bold>E</bold>) Profiles of the same type filters in the horizontal (orange) and diagonal (blue) directions. Average (bold) +/- standard deviation (shaded region) across recordings. Source files for D and E are available in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Effect of the population on the visual message.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig3">Figure 3D and E</xref>, which show how the visual message changes depending on other RGCs. This includes the widths and profiles of the reconstruction filters.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig3-v2.tif"/></fig><p>Both the spatial spread of the single-cell reconstruction filter and its reduction in the context of the neural population can be understood by examining how the optimal filters (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) depend on the statistics of the stimulus (S) and response (R). The matrix R<sup>T</sup>R represents correlations in the activity of different RGCs. The matrix R<sup>T</sup>S represents unnormalized, spike-triggered average (STA) images, one for each RGC. These natural image STAs were broad (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, top), reflecting the strong spatial correlations present in natural scenes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>For reconstruction from a single cell’s responses, R<sup>T</sup>R is a scalar, and therefore the single-cell reconstruction filter is directly proportional to the natural image STA. However, in the case of reconstruction from the population, R<sup>T</sup>R is a matrix that shapes the reconstruction filter based on the activity of other cells. Specifically, each cell’s filter is a linear mixture of its own natural image STA and those of the other cells in the population reconstruction, weighted negatively based on the magnitude of their correlated activity. This mixing resulted in the reduction in the width of the reconstruction filter of a given RGC when nearby cells of the same type were included (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>When the complete population of RGCs of the same type was included in the reconstruction, the resulting spatially localized filters were similar to the RFs obtained with white noise stimuli (ρ=0.78 +/- 0.10, n = 997 ON and 1228 OFF parasol cells from 15 recordings). However, some natural image spatial structure remained and was consistent across recordings, cells, and cell types. Most strikingly, the reconstruction filters exhibited broad vertical and horizontal structure (<xref ref-type="fig" rid="fig3">Figure 3C,E</xref>). This is a known feature of natural scenes (<xref ref-type="bibr" rid="bib28">Girshick et al., 2011</xref>), and is present in the images used here (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>In addition, the visual scene was more uniformly covered by the reconstruction filters than by the RFs (<xref ref-type="fig" rid="fig4">Figure 4A,C</xref>). Coverage was defined as the proportion of pixels that were within the extent of exactly one cell’s filter. The filter extent was defined by a threshold, set separately for the reconstruction filters and for the RFs to maximize the resulting coverage value. Across both the ON and OFF parasol cells in 12 recordings, the average coverage was 0.62 +/- 0.06 for the RFs and 0.78 +/- 0.03 for the reconstruction filters (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; p&lt;0.001). By comparison, expanded RFs, scaled around each RGC’s center location to match the average filter width, led to a small reduction in coverage (0.57 +/- 0.06; p&lt;0.001) due to increased overlap. This indicates that the filters are not simply broader versions of the RF, but rather that they are distorted relative to the RFs to fill gaps in the mosaic.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Effect of visual message on reconstruction.</title><p>(<bold>A</bold>) Receptive field (RF, left) and reconstruction filter (right) contours for two sample recordings. (<bold>B</bold>) Reconstruction of an image (top) using the full, fitted filters (middle) and using scaled RFs (bottom). (<bold>C</bold>) Comparison of RF and filter coverage for ON and OFF parasol cells across 12 recordings. (<bold>D</bold>) Comparison of reconstruction performance using scaled RFs or using full, fitted filters, across n = 4800 images from eight recordings. (<bold>E</bold>) Power in the reconstructed images (as a fraction of power in the original image) using fitted filters (orange) or scaled RFs (blue). Average (bold) +/- standard deviation (shaded region) across eight recordings. The original power structure of the natural images is shown in gray and has arbitrary units. Source files for C, D, and E are available in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Effect of the visual message on reconstruction.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig4">Figure 4C, D and E</xref>, which compare the full and receptive field (RF) reconstructions. This includes the coverage values for the RFs, the filters, and the expanded RFs, as well as the full and RF reconstruction scores, and the power spectra of the full and RF reconstructions.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig4-v2.tif"/></fig><p>To understand how the differences between reconstruction filters and RFs affected the reconstructed images, reconstruction was performed using the spatial RFs in place of the filters (each RF independently scaled to minimize MSE, see Materials and methods; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). This manipulation reduced reconstruction performance by 24% (Δρ=−0.12 +/- 0.09 across 4500 images from ON and OFF parasol cells in 15 recordings; p&lt;0.001; <xref ref-type="fig" rid="fig4">Figure 4D</xref>), primarily in the lower spatial frequencies, which also contain most of the power in the original images (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). The resulting images were noticeably less smooth in appearance than the optimally reconstructed images, and exhibited structure resembling the RGC mosaic (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Thus, although the reconstruction filters generally resembled the RFs, the additional spatial structure related to natural images and the spatial arrangement of RGCs led to smoother reconstructed images. These features may help explain the high consistency in reconstruction performance across many retinas (see above; <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec><sec id="s2-2"><title>Distinct contributions of major cell types</title><p>The visual message transmitted by RGCs of a particular type could additionally be affected by the other cell types encoding the same region of visual space (<xref ref-type="bibr" rid="bib76">Warland et al., 1997</xref>). To test this possibility, reconstructions were performed using the responses of a single RGC alone (the primary cell), or in combination with each of the four major cell type populations. For each combination, the reconstruction filters of the primary cells were averaged across all cells of the same type for each recording (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Inclusion of all cells of any one cell type reduced the magnitude of the primary cell’s reconstruction filter (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, left). This can be understood by noting that the entries in (R<sup>T</sup>R)<sup>-1</sup>, which mix the natural image STAs to produce the reconstruction filters, have the opposite sign of the response correlations. As expected, the correlations were positive for same-polarity cells and negative for opposite-polarity cells (not shown; <xref ref-type="bibr" rid="bib34">Greschner et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Mastronarde, 1983</xref>). Therefore, the cell’s reconstruction filter was reduced in magnitude by positively weighted cells of the opposite polarity, and by negatively weighted cells of the same polarity.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Effect of other cell types on the visual message.</title><p>(<bold>A</bold>) Average reconstruction filters for ON parasol (top row), OFF parasol (second row), ON midget (third row), and OFF midget (bottom row) cells for one recording. Left to right: including all cell types, all cells of the same type, all cells of the same polarity but opposite class, all cells of the opposite polarity but the same class, all cells of opposite polarity and class, and no other cell types. (<bold>B</bold>) Comparison of magnitude (left) and width (right) of average reconstruction filters across conditions, normalized by the features of the single-cell filter. Average +/- standard deviation across recordings is plotted (parasol: n = 11 recordings, midget: n = 5 recordings). Rows correspond to cell types as in A. Source files for B are available in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Effect of other cell types on the visual message.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig5">Figure 5B</xref>, which compares the magnitude and width of the filters when other cell types are included in the reconstruction.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig5-v2.tif"/></fig><p>As discussed previously, for parasol cells, inclusion of the remaining cells of the same type substantially reduced the spatial extent of the primary cell’s filter (<xref ref-type="fig" rid="fig3">Figure 3</xref>). However, this did not occur when cells of other types were included in reconstruction instead (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, right, top two rows). Specifically, the inclusion of the midget cells with the same polarity only slightly reduced the spatial extent of the parasol cell’s filter, and inclusion of opposite polarity cells of either type had little effect. This is likely because the other cell types provide roughly uniform coverage, whereas the remaining cells of the same type have a gap in the location of the primary cell, resulting in significant shaping by the immediately neighboring cells. In summary, the spatial structure of the visual message of a single parasol cell is primarily influenced by neighboring cells of the same type and is largely unaffected by cells of other types.</p><p>The filters for the midget cells were also shaped by the inclusion of the remaining cells of the same type (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, second column) and were largely unaffected by the inclusion of opposite polarity cells of either type. However, unlike parasol cells, midget cell filters were significantly affected by the inclusion of the same-polarity parasol cells (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, third column). This is consistent with known correlations between these cell types (<xref ref-type="bibr" rid="bib34">Greschner et al., 2011</xref>), and the asymmetry may be due to the fact that parasol cells tended to have much stronger responses to the natural images than midget cells. Thus, the interpretation of the visual signal from a midget cell does depend somewhat on the signals sent by the same-polarity parasol cell population.</p><p>The image features represented by each cell type were revealed by analysis of the reconstructed images. In particular, the separate contributions of ON and OFF cells, and of parasol and midget cells, were investigated.</p><p>To estimate the contribution of ON and OFF cells, reconstruction was performed with ON or OFF parasol cells alone and in combination (<xref ref-type="fig" rid="fig6">Figure 6A,B</xref>). Reconstructions using just OFF parasol cells were slightly more accurate than using just ON cells, but both were less accurate than reconstruction using the two types together (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, both: ρ=0.76 +/- 0.12, ON: ρ=0.64 +/- 0.16, OFF: ρ=0.67 +/- 0.14, across n = 2250 images from 15 recordings; all p&lt;0.001). Reconstruction using just ON cells failed to accurately capture intensity variations in dark areas of the image, while reconstruction with just OFF cells failed to capture variations in light areas of the image (for pixel values above the mean value: ρ=0.57 for ON and 0.26 for OFF, for pixel values below the mean value: ρ=0.31 for ON and 0.68 for OFF). Only a narrow middle range of pixel intensities were effectively encoded by both types (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). This is consistent with known output nonlinearities, which suppress responses to stimuli of the non-preferred contrast, and therefore limit linear reconstruction in that range. Thus, both ON and OFF cells were necessary to reconstruct the full range of image contrasts. Reconstruction using the responses of both cell types seemed to encode darker pixels more accurately than lighter pixels (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, bottom panel, black curve), consistent with the reconstruction performance from each type separately. This could reflect the fact that ON cells are less dense (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>), and/or the fact that the natural image distribution is skewed towards darker pixel values (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, bottom panel, gray distribution), potentially placing greater weight on the accurate reconstruction of these values. In addition, ON cells exhibit a more linear contrast-response relationship (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>), so there is less reconstruction performance difference between preferred and non-preferred contrasts.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Contributions of ON and OFF parasol cells.</title><p>(<bold>A,B</bold>) Example images, responses, and reconstructions from ON and OFF parasol cells. Top left: original image. Top right: Parasol cell mosaics shaded by their response value (ON - blue, middle, OFF - orange, right). Bottom left: reconstruction from both cell types. Bottom right: reconstruction from just ON (blue, middle) or just OFF (orange, right) parasol cells. (<bold>C</bold>) Reconstruction performance for ON vs. OFF (top), both vs. ON (middle), and both vs. OFF (bottom), with n = 2250 images from 15 recordings. (<bold>D</bold>) Average estimated pixel intensity (top) and sensitivity (bottom, defined as Δaverage estimated pixel intensity/Δtrue pixel intensity) vs. true pixel intensity for ON (blue), OFF (orange), and both (black). Individual recordings are shown in the top plot, with the average in bold. Source files for C and D are available in <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref>.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Contributions of ON and OFF parasol cells.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig6">Figure 6C and D</xref>, which compare the reconstructions from ON and OFF parasol cell responses. This data includes the performance scores for reconstructions from ON and OFF parasol cell responses, as well as the binned true and estimated pixel values.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig6-v2.tif"/></fig><p>To estimate the contributions of parasol and midget cells, reconstruction was performed using parasol cells or midget cells or both (<xref ref-type="fig" rid="fig7">Figure 7A,B</xref>). As expected, reconstruction using both parasol and midget cells was more accurate than using either alone (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, both: ρ=0.81 +/- 0.10, parasol: ρ=0.77 +/- 0.12, midget: ρ=0.73 +/- 0.13, across n = 1050 images from seven recordings; all p&lt;0.001). Images reconstructed from midget cells contained more high-frequency spatial structure, consistent with their higher density (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). However, the images reconstructed from parasol cells had 50% higher signal-to-noise (defined as standard deviation across images/standard deviation across repeats), resulting in the slightly higher reconstruction performance from parasol cells.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Contributions of the parasol and midget cell classes.</title><p>(<bold>A</bold>) Example image and reconstructions for parasol and midget cells. Top left: original image. Top right: reconstruction with parasol and midget cells (gray). Bottom left: reconstruction with only parasol cells (blue). Bottom right: reconstruction with only midget cells (orange). (<bold>B</bold>) Cell type mosaics shaded by their response values, for ON (top) and OFF (bottom) parasol cells (left, blue) and midget cells (right, orange). (<bold>C</bold>) Reconstruction performance for midget vs. parasol (top), both vs. parasol (middle), and both vs. midget (bottom). (<bold>D</bold>) Power in the reconstructed images as a fraction of power in the original image (left) and receptive fields (right) for parasol cells (blue), midget cells (orange), and both types (gray) for each of three recordings. (<bold>E</bold>) Left: Fraction of peak reconstruction performance with increasing spike integration times for parasol (blue) and midget (orange) cells, with averages across recordings shown in bold. Dotted line indicates 95% performance. Right: Time to 95% performance for parasol and midget reconstructions across seven recordings. Source files for C, D, and E are available in <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref>.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>Contributions of the parasol and midget cell classes.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig7">Figure 7C, D and E</xref>, which compare the reconstructions from parasol and midget cell responses. This data includes the performance scores for reconstructions from parasol and midget cell responses, as well as the power spectra of the resulting images, and the time required to reach 95% reconstruction performance.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig7-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig7-v2.tif"/></fig><p>The above analysis obscures the significantly different temporal responses properties of these two cell classes. In particular, parasol cells have more transient responses (<xref ref-type="bibr" rid="bib13">de Monasterio, 1978</xref>; <xref ref-type="bibr" rid="bib14">De Monasterio and Gouras, 1975</xref>; <xref ref-type="bibr" rid="bib33">Gouras, 1968</xref>), which may allow them to convey information more rapidly than midget cells. To test this possibility, image reconstruction was performed using spikes collected over increasing windows of time after the image onset. The reconstruction performance of parasol cells increased quickly and reached 95% of peak reconstruction performance at 80 +/- 20 ms, while the performance of midget cells increased more slowly, and reached 95% performance at 116 +/- 19 ms (across seven recordings; <xref ref-type="fig" rid="fig7">Figure 7E</xref>). This difference indicates that spatiotemporal reconstruction will be necessary to fully reveal the distinct contributions of these two classes (see Discussion).</p></sec><sec id="s2-3"><title>The effect of correlated firing</title><p>The above results indicate that the visual message of each RGC, and the contributions of each cell type, are shaped by correlated activity. However, these analyses do not distinguish between stimulus-induced (signal) correlations, and stimulus-independent (noise) correlations that arise from neural circuitry within and across cell types in the primate retina (<xref ref-type="bibr" rid="bib34">Greschner et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Mastronarde, 1983</xref>).</p><p>To test the effect of noise correlations, reconstruction performance was evaluated on repeated presentations of test images. This performance was compared to a control condition in which the responses of each cell were independently shuffled across trials to remove noise correlations while preserving single-cell statistics and signal correlations. The reconstruction filters (computed from unshuffled training data) were then used to reconstruct the test images, using either the shuffled or unshuffled responses. In principle, shuffling could result in a net increase or decrease in reconstruction accuracy, due to two opposing factors. Because the reconstruction filters incorporate the correlated activity present in training data (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), any deviation from this correlation structure in the test data could reduce performance. On the other hand, if noise correlations produce spatial structure in the reconstructions that obscures the structure of the natural images, their removal could enhance reconstruction performance. The relative influence of these competing effects could also depend on the overall fidelity of the reconstruction.</p><p>Accordingly, the shuffling manipulation was tested using three response measures. In the first, RGC responses were calculated by counting spikes in the 150 ms window after image onset, as above. In the second, the response was measured at the intrinsic time scale of correlations (~10 ms; <xref ref-type="bibr" rid="bib16">DeVries, 1999</xref>; <xref ref-type="bibr" rid="bib44">Mastronarde, 1983</xref>; <xref ref-type="bibr" rid="bib45">Meister et al., 1995</xref>; <xref ref-type="bibr" rid="bib65">Shlens et al., 2006</xref>), by counting spikes in fifteen 10 ms bins, and reconstructing with this multivariate response vector instead of the scalar spike count. In the third, spikes were counted only in the 10 ms bin that had the highest average firing rate (50–60 ms after image onset). While the third approach did not utilize all the available information in the responses, it was used to mimic low-fidelity or rapid perception scenarios, which would have fewer stimulus-driven spikes available for reconstruction.</p><p>Reconstruction using the first two response measures had similar unshuffled performance (ρ=0.76 +/- 0.12 and 0.75 +/- 0.12 respectively), and low variation across trials (standard deviation across repeats = 0.015). With these measures, shuffling had a very small and detrimental effect on reconstruction (across 3 recordings with 27 repeats of 150 test images: (1) Δρ=−0.0004 +/- 0.0017; |Δρ|=0.0012 +/- 0.0012; p&lt;0.001, (2) Δρ=−0.0008 +/- 0.0019; |Δρ|=0.0014 +/- 0.0015; p&lt;0.001). In each case, the magnitude of the change in correlation represented about 10% of the variation in reconstruction accuracy across trials, which represents roughly how much improvement could be expected (<xref ref-type="fig" rid="fig8">Figure 8</xref>). For comparison, shuffling the responses in each time bin independently across trials (rather than the responses of each cell independently) had a much larger effect (Δρ=−0.02 +/- 0.01), consistent with previous results (<xref ref-type="bibr" rid="bib5">Botella-Soler et al., 2018</xref>), indicating that the autocorrelation structure across time is more important for reconstruction than the noise correlation structure across cells. Thus, in these conditions, noise correlations had a limited impact on reconstruction, regardless of the time scale of analysis.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Effect of noise correlations.</title><p>The change in reconstruction performance (Δρ) when using shuffled data for three scenarios: one 150 ms window, fifteen 10 ms windows, and one 10 ms window. Black bars show median +/- interquartile range for three recordings (each shown separately). Gray bars show the standard deviation in the reconstruction performance across trials. Source files are available in <xref ref-type="supplementary-material" rid="fig8sdata1">Figure 8—source data 1</xref>.</p><p><supplementary-material id="fig8sdata1"><label>Figure 8—source data 1.</label><caption><title>Effect of noise correlations.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig8">Figure 8</xref>, which shows the effects of noise correlations on reconstruction performance.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig8-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig8-v2.tif"/></fig><p>Reconstruction using the third measure had lower unshuffled performance (ρ=0.64 +/- 0.14), and higher variation across trials (standard deviation across repeats = 0.039). In this case, shuffling led to a more consistent, but still small, increase in reconstruction performance (Δρ=0.0071 +/- 0.0076; |Δρ|=0.0075 +/- 0.0072; p&lt;0.001). The increase represented a larger fraction of the variation in reconstruction accuracy across trials (20%; <xref ref-type="fig" rid="fig8">Figure 8</xref>). This suggests that in low-fidelity, high-noise situations, noise correlations in the RGC population can partially obscure the structure of natural images, even if reconstruction is designed to take the correlations into account.</p></sec><sec id="s2-4"><title>Nonlinear reconstruction</title><p>Linear reconstruction provides an easily interpretable estimate of the visual message, but it may limit the quality of reconstruction by not extracting all the information available in the neural responses and may also differ greatly from how the brain processes the retinal input. Therefore, two simple extensions of linear reconstruction were tested: transformation of the responses using a scalar nonlinearity, and inclusion of interaction terms between nearby cells.</p><p>In the first case, the response of each cell was transformed using a scalar nonlinearity, and linear regression (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) was performed to reconstruct images from the transformed response. The stimulus estimate S<sub>NL</sub> is given by S<sub>NL</sub> = f(R)⋅W<sub>NL</sub>, where W<sub>NL</sub> is a matrix of reconstruction weights (refitted using the transformed responses), and f(R) is the scalar nonlinear transform of the population response vector R. This is equivalent to inverting a linear-nonlinear (LN) encoding model of the form R = g(K⋅S), where g is the inverse of f, and K is a different set of weights (note that in general a nonlinear encoder may not require an equivalent nonlinear decoder for optimum performance; see <xref ref-type="bibr" rid="bib59">Rieke et al., 1997</xref> for a full discussion). A common form of the LN encoding model uses an exponential nonlinearity, g = exp(); therefore, the inverse function f = log() was used for reconstruction, and the response for each cell was defined as the spike count plus 1. A square root transformation was also tested, and yielded similar results (not shown).</p><p>The relationship to pixel values was more linear for the transformed RGC responses than for the original responses (Δlinear fit RMSE = −1.9 +/- 1.5 across n = 2225 cells from 15 recordings; <xref ref-type="fig" rid="fig9">Figure 9A,B</xref>), indicating that this inverse function captured at least some of the nonlinearity in retinal signals. The nonlinear transformation slightly increased reconstruction accuracy when using the responses of ON or OFF parasol cells alone (across 15 recordings with 300 images each: ON parasol: Δρ=0.013 +/- 0.051, p&lt;0.001; OFF parasol: Δρ=0.015 +/- 0.035, p&lt;0.001; <xref ref-type="fig" rid="fig9">Figure 9C</xref>). However, it did not help when using the responses of ON and OFF parasol cells together (Δρ=−0.0017 +/- 0.032, p=0.001; <xref ref-type="fig" rid="fig9">Figure 9C</xref>). This likely reflects the fact that the relationship between the true pixel values and the pixel values reconstructed using the original, untransformed responses was already approximately linear when using both cell types, but not when using just one cell type (<xref ref-type="fig" rid="fig6">Figure 6</xref>). In addition, using the raw responses of both cell types was more effective than using the transformed responses of either type alone (ON parasol: Δρ=−0.09 +/- 0.1, p&lt;0.001; OFF parasol: Δρ=−0.06 +/- 0.1, p&lt;0.001), suggesting that intensity information cannot be directly recovered fully from either ON or OFF cells alone.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Nonlinear reconstruction.</title><p>(<bold>A</bold>) Average pixel value in receptive field center vs. original response (blue) and transformed response (orange). (<bold>B</bold>) Distribution (across n = 2225 cells from 15 recordings) of the change in RMSE of a linear model (mapping from response to pixel value) when using the transformed response. (<bold>C</bold>) Change in reconstruction performance (correlation) when using transformed responses (log(R)) for reconstruction with either ON and OFF parasol cells, only ON parasol cells, or only OFF parasol cells. Individual images (n = 300 from each of the 15 recordings) are plotted in gray with jitter in the x-direction. The black bars represent mean +/- standard deviation, and the standard error is smaller than the central dot. (<bold>D</bold>) Change in reconstruction performance (correlation) when including interaction terms. Individual images (n = 300 from each of the three recordings) are plotted in gray with jitter in the x-direction. The black bars represent mean +/- standard deviation, and the standard error is smaller than the central dot. (<bold>E,F</bold>) Average reconstruction filters corresponding to ON-OFF type interactions, centered and aligned along the cell-to-cell axis, for simulation (<bold>E</bold>) and data (<bold>F</bold>). (<bold>G</bold>) 1D Profiles of all ON-OFF interaction filters through the cell-to-cell axis, sorted by distance between the pair. (<bold>H</bold>) Example image (left), reconstructions with and without interaction terms (middle), and difference between the reconstructions, with dotted lines indicating edges (right). Source files for B, C, and D are available in <xref ref-type="supplementary-material" rid="fig9sdata1">Figure 9—source data 1</xref>.</p><p><supplementary-material id="fig9sdata1"><label>Figure 9—source data 1.</label><caption><title>Nonlinear reconstruction.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig9">Figures 9B, C and D</xref>, which show the effects of using a static nonlinear transformation, and of including nonlinear interaction terms.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig9-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig9-v2.tif"/></fig><p>Nonlinear interactions between the signals from different cells could also potentially increase reconstruction performance. To test this idea, the products of spike counts in pairs of neighboring cells were added as predictors in the linear reconstruction. Neighbors were defined as cells with RF centers that were within 1.5 times the median nearest neighbor distance between RF centers of the cells of the same type. For parasol cells, this definition resulted in roughly 6 ON and 6 OFF neighbors per cell, as expected (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Including these interactions produced a small increase in reconstruction accuracy (Δρ=0.0093 +/- 0.023, across three recordings with 300 test images each; p&lt;0.001; regularization did not lead to improved performance). The primary contribution was from ON-OFF pairs (ON-OFF: Δρ = 0.0089 +/- 0.019, not significantly different than all pairs, p=0.2; ON-ON: Δρ=0.0021 +/- 0.010 and OFF-OFF: Δρ=0.0024 +/- 0.013, both significantly different than all pairs, p&lt;0.001; <xref ref-type="fig" rid="fig9">Figure 9D</xref>). The reconstruction filters associated with these interaction terms typically had an oriented structure orthogonal to the line between the RF centers of the two cells (<xref ref-type="fig" rid="fig9">Figure 9F,G</xref>), suggesting that the improvement in reconstruction may come primarily from using the joint activation of partially overlapping ON and OFF cells to capture edges in the visual scene.</p></sec><sec id="s2-5"><title>Comparison to simple models of RGC light response</title><p>The above analyses revealed that noise correlations and interactions between cells and cell types had a limited impact on reconstruction performance, suggesting that more complicated features of retinal encoding may not be important for linear reconstruction. To further explore this idea, simple LN models were used to simulate RGC responses across all 15 recordings, and the primary features of reconstructions from recorded and simulated spike trains were compared. The simulated spike count of each RGC in response to a given image was calculated by filtering the image with the spatial RF, and then passing that value through a fitted sigmoidal nonlinearity to obtain a firing rate (see Materials and methods). The noise in the recorded spike counts was sub-Poisson (not shown; see <xref ref-type="bibr" rid="bib71">Uzzell and Chichilnisky, 2004</xref>); therefore, the simulated firing rate was directly compared to the trial-averaged, recorded firing rate. This model captured RGC responses to static images with reasonable accuracy (correlation between simulated and average recorded spike counts: 0.76 +/- 0.13 across n = 997 ON parasol cells; 0.84 +/- 0.09 across n = 1228 OFF parasol cells; see <xref ref-type="bibr" rid="bib7">Chichilnisky, 2001</xref>). Note that by definition, the model incorporated the measured functional organization of the retina, including retina-specific RF mosaic structure and cell-type specific response properties, both of which are necessary to understand the visual message (see above).</p><p>Reconstructions with recorded and simulated spike trains revealed broadly similar properties in the filters and reconstructed images. The filters fitted to the recorded and simulated spike trains were similar (ρ=0.84 +/- 0.09 across 2225 parasol cells from 15 recordings), and shared key features, such as horizontal and vertical structure (<xref ref-type="fig" rid="fig10">Figure 10A,C</xref>). The reconstructed images themselves were also similar (correlation between images reconstructed from simulated and recorded spike counts: 0.93 +/- 0.04 across n = 2250 images from 15 recordings; <xref ref-type="fig" rid="fig10">Figure 10B,C</xref>), as was the reconstruction performance (simulated: ρ=0.79 +/- 0.11; recorded: ρ=0.78 +/- 0.11; Δρ=−0.003 +/- 0.03; across 2250 images from 15 recordings; <xref ref-type="fig" rid="fig10">Figure 10C</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Comparison to simulated spikes.</title><p>(<bold>A</bold>) Average reconstruction filters calculated from spikes simulated using linear-nonlinear models (left) or recorded (right). (<bold>B</bold>) Images reconstructed from simulated (left) or recorded (middle) spikes, compared to the original images (right). (<bold>C</bold>) Comparison of reconstructions with recorded and simulated spike counts: filters (top; ρ=0.84 +/- 0.09 across 2225 parasol cells from 15 recordings), reconstructed images (middle; ρ=0.93 +/- 0.04 across n = 2250 images from 15 recordings), and performance (bottom; simulated: ρ=0.79 +/- 0.11; recorded: ρ=0.78 +/- 0.11; Δρ=−0.003 +/- 0.03; across 2250 images from 15 recordings). Source files for C are available in <xref ref-type="supplementary-material" rid="fig10sdata1">Figure 10—source data 1</xref>.</p><p><supplementary-material id="fig10sdata1"><label>Figure 10—source data 1.</label><caption><title>Comparison to simulated spikes.</title><p>This zip file contains the code and data for <xref ref-type="fig" rid="fig10">Figure 10C</xref>, which compares reconstruction using recorded and simulated RGC responses.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig10-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig10-v2.tif"/></fig><p>The simulated spike trains also replicated the structure of nonlinear interactions between cells. This was observed by using the simulated responses of ON and OFF cells and the products of the responses of neighboring cells, as above, to reconstruct natural images. The spatial reconstruction filter corresponding to the interaction term between nearby ON and OFF cells was oriented and qualitatively similar to the interaction filters obtained with real data (<xref ref-type="fig" rid="fig9">Figure 9E,F</xref>). However, this was not the case for responses simulated using a linear model without any response rectification (not shown) – in this case, the filter corresponding to the interaction term had no clear structure.</p><p>The model reveals that although the visual messages of RGCs depend on their spatial and cell-type specific organization, as well as the statistics of the stimulus, their essential structure can be understood using simple models of RGC encoding. Furthermore, some degree of nonlinear encoding is necessary to explain the oriented interaction filters observed in the data.</p></sec><sec id="s2-6"><title>Spatial information in a naturalistic movie</title><p>In natural vision, a continuous stream of retinal responses is used to make inferences about the dynamic external world. Therefore, the reconstruction approach above – using the accumulated spikes over a fraction of a second to reconstruct a flashed image – could fail to capture important aspects of normal vision. To test whether the above results extend to spatiotemporal reconstructions, a naturalistic movie, consisting of a continuous stream of natural images with simulated eye movements superimposed, was reconstructed from the spike trains of RGCs. The spike trains were binned at the frame rate of the movie (120 Hz), and linear regression was performed between the frames of the movie and the RGC responses in 15 bins following each frame, resulting in a spatiotemporal reconstruction filter for each RGC.</p><p>A spatial summary of the filter for each cell was obtained by first calculating the average time course of the strongest pixels, and then projecting each pixel of the full filter against this time course (examples shown in <xref ref-type="fig" rid="fig11">Figure 11A</xref>; see Materials and methods). This spatial filter was highly correlated with the spatial reconstruction filters of the same cells obtained in the preceding analysis with flashed images (ρ=0.87 +/- 0.07, n = 351 parasol cells from three recordings; <xref ref-type="fig" rid="fig11">Figure 11B</xref>). The dynamic filters were approximately space-time separable (explained variance from first principal component = 0.85 +/- 0.13). The remaining unexplained variance contained significant apparent structure as well as noise (not shown), which may be important for further understanding spatiotemporal processing in the retina and the underlying mechanisms, but was not explored further (<xref ref-type="bibr" rid="bib1">Benardete and Kaplan, 1997a</xref>; <xref ref-type="bibr" rid="bib2">Benardete and Kaplan, 1997b</xref>; <xref ref-type="bibr" rid="bib12">Dawis et al., 1984</xref>; <xref ref-type="bibr" rid="bib15">Derrington and Lennie, 1982</xref>; <xref ref-type="bibr" rid="bib19">Enroth-Cugell et al., 1983</xref>). The large fraction of variance explained by a space-time separable filter suggests that the essential spatial features of the visual message observed in spatial reconstructions largely extend to spatiotemporal vision. In addition, the reconstructed movie frames were similar to reconstructions of static images (between static reconstruction and average reconstructed frame: ρ=0.72 +/- 0.19 across 120 images from three recordings, <xref ref-type="fig" rid="fig11">Figure 11C</xref>).</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Spatiotemporal reconstruction.</title><p>(<bold>A</bold>) Examples of the spatial components extracted from the spatiotemporal reconstruction filter (top) and the static spatial reconstruction filters (bottom) for an ON (left) and OFF (right) parasol cell. (<bold>B</bold>) Correlation between spatial component and static filter (ρ=0.87 +/- 0.07 across n = 351 cells from three recordings). (<sc><bold>C</bold></sc>) Example reconstructions of movie frames and of static images. Source files for B are available in <xref ref-type="supplementary-material" rid="fig11sdata1">Figure 11—source data 1</xref>.</p><p><supplementary-material id="fig11sdata1"><label>Figure 11—source data 1.</label><caption><title>Spatiotemporal reconstruction.</title><p>This zip file contains code and data for <xref ref-type="fig" rid="fig11">Figure 11B</xref>, which compares static and spatiotemporal reconstruction filters.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58516-fig11-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig11-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Linear reconstruction of natural images was used to investigate the spatial information transmitted to the brain by complete populations of primate RGCs. The quality of the reconstructions was consistent across retinas. The optimal interpretation of the spikes produced by a RGC – that is its visual message – depended not only on its encoding properties, but also on the statistics of natural scenes and the spatial arrangement of other RGCs. These factors enabled smoother natural image reconstructions from the RGC population than would be expected from the RFs alone. In addition, the visual representation conveyed by each cell type reflected its distinct encoding properties, and for ON and OFF parasol cells, was largely independent of the contributions of other cell types. Overall, the results were consistent with a simple, linear-nonlinear model of RGC encoding, incorporating the spatial properties, contrast-response properties, and collective functional organization of the four major RGC types. Finally, a limited test of spatiotemporal reconstruction indicated that these results may generalize to natural vision.</p><p>The results show that the dependence of a given RGC’s visual message on the responses of other RGCs, which was demonstrated previously in the temporal domain using a spatially uniform random flicker stimulus (<xref ref-type="bibr" rid="bib76">Warland et al., 1997</xref>), extends to the spatial domain in natural viewing conditions. For decades, the spatial visual message of a RGC has been estimated using its RF, measured with artificial stimuli. However, due to spatial correlations in natural scenes, the response of a RGC contains information about the stimulus far beyond its RF. In this light, it is at first surprising that the visual message is spatially localized and similar to the classical RF (<xref ref-type="fig" rid="fig3">Figure 3A,C</xref>). However, nearby regions of visual space are already ‘covered’ by the neighboring RGCs of the same type, and the redundant information in adjacent cells apparently contributes little to representing the image structure. Even so, the visual messages retain some explicit horizontal and vertical natural scene structure, and collective spatial organization, not present in the RFs. This structure results in smoother reconstructions and more uniform coverage of visual space than the coverage provided by the RF mosaic (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In this sense, the visual message of each RGC differs from its RF, specifically in a way that reflects its coordination with other nearby cells. The significance of natural scene statistics for interpreting the neural code has also been suggested in the visual cortex (<xref ref-type="bibr" rid="bib48">Naselaris et al., 2009</xref>), and can be used as a prior to improve image estimates in multi-step reconstruction methods (<xref ref-type="bibr" rid="bib52">Parthasarathy et al., 2017</xref>).</p><p>Each of the major RGC types conveyed distinct visual representations, consistent with their encoding properties. For the most part, these were independent of the contributions of the other types, indicating that the major primate RGC types, despite covering the same region of visual space, conveyed different stimulus features. However, this separation was clearer for the ON and OFF types than for the parasol and midget cell classes, because the midget cell filters were influenced by the inclusion of same-polarity parasol cells. Further analysis in the temporal domain (see <xref ref-type="fig" rid="fig7">Figure 7E</xref>) may be necessary to clarify the separation of these two classes. Both ON and OFF cell types were necessary to reconstruct the full contrast range of the images, because responses from a single cell type resulted in less accurate reconstructions even if they were linearized. It is not clear why the retina separates visual information into separate cell type channels. The roughly linear intensity representation by ON and OFF cell types together (but not individually) is consistent with suggestions that encoding by multiple cell types with nonlinear response properties could enable relatively simple linear reconstruction by downstream neurons (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Gjorgjieva et al., 2019</xref>). There also may be more complicated interactions between different cell types that another reconstruction method could reveal. As new cell types are identified and characterized (<xref ref-type="bibr" rid="bib57">Puller et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Rhoades et al., 2019</xref>), their contributions to vision may be more fully revealed by these linear and simple nonlinear reconstruction approaches.</p><p>Overall, the results presented here were consistent with predictions from a simple, independent pseudo-linear model for RGC light responses, despite known nonlinearities and correlations in the retinal circuitry. Specifically, replacing the recorded spike trains with simulated spike trains, generated by LN models fitted to each RGC, resulted in similar reconstruction filters and reconstructed images (<xref ref-type="fig" rid="fig10">Figure 10</xref>). Obviously, the LN model by itself cannot explain the many features of encoding observed here; instead, the specific spatial properties, contrast-response properties, and collective organization of the major RGC types captured in the present measurements are crucial for understanding the structure of the visual message. The similarity of reconstruction from LN models and recorded data is consistent with the limited impact of interaction terms and stimulus-independent (noise) correlations, the importance of which has been debated (<xref ref-type="bibr" rid="bib6">Cafaro and Rieke, 2010</xref>; <xref ref-type="bibr" rid="bib26">Ganmor et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Meytlis et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Nirenberg et al., 2001</xref>; <xref ref-type="bibr" rid="bib54">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib56">Puchalla et al., 2005</xref>; <xref ref-type="bibr" rid="bib62">Ruda et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Zylberberg et al., 2016</xref>). While the impact of noise correlations on reconstruction in the present data was limited by the low total noise in the accumulated spike counts, this may not reflect natural vision, in which perception and action occur too quickly to utilize all the stimulus-driven spikes from each RGC, and sometimes must rely on visual inputs with low light levels or spatial contrast (<xref ref-type="bibr" rid="bib62">Ruda et al., 2020</xref>). A low-fidelity situation was mimicked by reducing the spike integration time window to 10 ms, a manipulation that revealed an increased but still small effect of noise correlations. It is also possible that these results would be affected by removing noise correlations from both the training and testing data, but evaluating this possibility would require longer repeated presentations of training stimuli than were performed here.</p><p>It is uncertain how close the reconstructions presented here are to the best possible reconstructions given the data, and how much additional information could potentially be extracted from the spike trains. Acuity has been shown to track with midget cell RF size (<xref ref-type="bibr" rid="bib9">Dacey, 1993</xref>; <xref ref-type="bibr" rid="bib46">Merigan and Katz, 1990</xref>; <xref ref-type="bibr" rid="bib61">Rossi and Roorda, 2010</xref>; <xref ref-type="bibr" rid="bib68">Thibos et al., 1987</xref>), indicating that the reconstructions shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> may accurately represent the quality of visual information transmitted to the brain. In addition, it has been suggested that simple decoders may be sufficient, even when the encoding is highly nonlinear (<xref ref-type="bibr" rid="bib18">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Gjorgjieva et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Rieke et al., 1997</xref>). However, alternative approaches may be worth exploring, and could extract additional information. For example, different measures of response, such as latency (<xref ref-type="bibr" rid="bib32">Gollisch and Meister, 2008</xref>; <xref ref-type="bibr" rid="bib35">Gütig et al., 2013</xref>) and relative activity (<xref ref-type="bibr" rid="bib55">Portelli et al., 2016</xref>), have been shown to convey more stimulus information than spike counts for non-primates under some conditions. This was not the case in the present data, which may be due to high-maintained firing rates in the mammalian retina (<xref ref-type="bibr" rid="bib69">Troy and Lee, 1994</xref>; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>), which make it difficult to identify the first stimulus-driven spike. In addition, recent studies have indicated that nonlinear and deep learning models could improve reconstruction performance for static images, moving patterns, and naturalistic movies (<xref ref-type="bibr" rid="bib5">Botella-Soler et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Kim et al., 2020</xref>; <xref ref-type="bibr" rid="bib52">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Zhang et al., 2020</xref>). While these approaches make the visual message more difficult to define, they could be used to extract richer information potentially present in RGC responses. Models that are interpretable while allowing for some nonlinearities could also be used to further investigate the visual message (<xref ref-type="bibr" rid="bib54">Pillow et al., 2008</xref>).</p><p>Attempting to extract more sophisticated visual information may also reveal additional information conveyed by RGCs, for example, by expanding to more complex, dynamic natural stimuli. Spatiotemporal stimuli, which were only explored here in a limited way, and/or chromatic stimuli, could further illuminate the impact of spike timing, the encoding of dynamic and space-time inseparable features, and the distinct roles of the multiple cell types (<xref ref-type="bibr" rid="bib1">Benardete and Kaplan, 1997a</xref>; <xref ref-type="bibr" rid="bib2">Benardete and Kaplan, 1997b</xref>; <xref ref-type="bibr" rid="bib3">Berry et al., 1997</xref>; <xref ref-type="bibr" rid="bib10">Dacey et al., 2003</xref>; <xref ref-type="bibr" rid="bib12">Dawis et al., 1984</xref>; <xref ref-type="bibr" rid="bib15">Derrington and Lennie, 1982</xref>; <xref ref-type="bibr" rid="bib19">Enroth-Cugell et al., 1983</xref>; <xref ref-type="bibr" rid="bib43">Masland, 2012</xref>; <xref ref-type="bibr" rid="bib71">Uzzell and Chichilnisky, 2004</xref>). For example, nonlinear spatial summation and motion encoding have been demonstrated in parasol cells but were not utilized here (<xref ref-type="bibr" rid="bib42">Manookin et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Turner and Rieke, 2016</xref>). In addition, pixel-wise mean squared error does not accurately reflect the perceived quality of the visual representation. More sophisticated metrics for optimization and evaluation of reconstruction should be explored (<xref ref-type="bibr" rid="bib75">Wang and Lu, 2002</xref>; <xref ref-type="bibr" rid="bib74">Wang et al., 2004</xref>).</p><p>By projecting neural responses into a common stimulus space, reconstruction enabled direct comparison and evaluation of the visual signals transmitted downstream. The large collection of recordings used here revealed a consistent visual representation across retinas, in spite of differences in RF mosaic structure and firing rates that make comparing the neural response itself difficult. The information contained in the retinal signal limits the information available to downstream visual areas, so the results presented here could inform studies of visual processing in the LGN, V1, and other brain structures. For example, the oriented nature of the interaction term filters supports the hypothesis that orientation selectivity in the cortex results from pairs of nearby ON and OFF RGCs (<xref ref-type="bibr" rid="bib51">Paik and Ringach, 2011</xref>; <xref ref-type="bibr" rid="bib60">Ringach, 2007</xref>). In addition, comparing reconstructions from different visual areas using a standard measurement — the reconstructed image — could help reveal how information about the external world is represented at various stages of the visual system.</p><p>Using reconstruction to understand the signals transmitted by neurons may be increasingly important in future efforts to read and write neural codes using brain-machine interfaces (BMIs). In the retina, certain types of blindness can be treated with implants that use electrical stimulation to activate the remaining retinal neurons (<xref ref-type="bibr" rid="bib30">Goetz and Palanker, 2016</xref>). The visual messages described in the present work could be useful for inferring the perceived visual image evoked by such devices, and thus for selecting optimal electrical stimulation patterns (<xref ref-type="bibr" rid="bib30">Goetz and Palanker, 2016</xref>; <xref ref-type="bibr" rid="bib31">Golden et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Shah et al., 2019</xref>). Reconstruction can also be used to compare the evoked visual representation with the representation produced by natural neural activity. In addition, the observation that reconstructions from different retinas and from recorded and simulated spikes are similar suggests that perfect replication of the neural code of a particular retina may not be necessary. Outside the visual system, many BMIs rely on reconstruction to read out and interpret neural activity, for example controlling prosthetic limbs using activity recorded in the motor cortex (<xref ref-type="bibr" rid="bib39">Lawhern et al., 2010</xref>; <xref ref-type="bibr" rid="bib73">Vargas-Irwin et al., 2010</xref>). While these studies typically focus on performing specific tasks, the present results suggest that examination of the reconstruction filters could reveal contributions of diverse cells and cell types in these modalities.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental methods</title><sec id="s4-1-1"><title>Multi-electrode array recordings</title><p>An ex vivo multi-electrode array preparation was used to obtain recordings from the major types of primate RGCs (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib23">Field et al., 2010</xref>; <xref ref-type="bibr" rid="bib24">Frechette et al., 2005</xref>; <xref ref-type="bibr" rid="bib41">Litke et al., 2004</xref>). Briefly, eyes were enucleated from terminally anesthetized macaques used by other researchers in accordance with institutional guidelines for the care and use of animals. Immediately after enucleation, the anterior portion of the eye and vitreous were removed in room light, and the eye cup was placed in a bicarbonate-buffered Ames’ solution (Sigma, St. Louis, MO). In dim light, pieces of retina roughly 3 mm in diameter and ranging in eccentricity from 7 to 17 mm (6–12 mm temporal equivalent eccentricity; <xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>) or 29-56 degrees (<xref ref-type="bibr" rid="bib11">Dacey and Petersen, 1992</xref>; <xref ref-type="bibr" rid="bib53">Perry and Cowey, 1985</xref>), were placed RGC side down on a planar array consisting of 512 extracellular microelectrodes covering a 1.8 mm × 0.9 mm region (roughly 4 × 8° visual field angle). In all but one preparation, the retinal pigment epithelium (RPE) was left attached to allow for photopigment regeneration and to improve tissue stability, but the choroid (up to Bruch’s membrane) was removed to allow oxygenation and maintain even thickness. For the duration of the recording, the preparation was perfused with Ames’ solution (30–34° C, pH 7.4) bubbled with 95% O<sub>2</sub>, 5% CO<sub>2</sub>. The raw voltage traces recorded on each electrode were bandpass filtered, amplified, and digitized at 20 kHz (<xref ref-type="bibr" rid="bib41">Litke et al., 2004</xref>). Spikes from individual neurons were identified by standard spike sorting techniques, and only spike trains from cells exhibiting a 1 ms refractory period were analyzed further (<xref ref-type="bibr" rid="bib21">Field et al., 2007</xref>; <xref ref-type="bibr" rid="bib41">Litke et al., 2004</xref>).</p></sec><sec id="s4-1-2"><title>Visual stimulation</title><p>The visual stimulus was produced by a 120 Hz, gamma-corrected, CRT monitor (Sony Trinitron Multiscan E100; Sony, Tokyo, Japan), which was optically reduced and projected through the mostly-transparent array onto the retina at low photopic light levels (2000, 1800, and 800 isomerizations per second for the L, M, and S cones respectively at 50% illumination; see <xref ref-type="bibr" rid="bib22">Field et al., 2009</xref>, <xref ref-type="bibr" rid="bib23">Field et al., 2010</xref>). The total visual stimulus area was 3.5 by 1.75 mm, which extended well beyond the recording area.</p><p>A 30-minutespatiotemporal white noise stimulus was used to characterize RGC responses and to periodically assess recording quality (<xref ref-type="bibr" rid="bib7">Chichilnisky, 2001</xref>). The stimulus was updated at either 30 or 60 Hz, and consisted of a grid of pixels (spacing ranged from 44 to 88 μm across recordings). For each update, the intensities for each of the three monitor primaries at each pixel location were chosen randomly from a binary distribution.</p><p>Natural images from the ImageNet database (<xref ref-type="bibr" rid="bib20">Fei-Fei et al., 2009</xref>) were converted to grayscale values. On a scale of 0 to 1, the mean image intensity was 0.45. The natural images were displayed at either 320 × 160 pixels, with each pixel measuring 11 × 11 μm on the retina, or at 160 × 80 pixels, with each pixel measuring 22 × 22 μm on the retina. The images were displayed for 100 ms each (12 frames at 120 Hz), separated by spatially uniform gray at intensity 0.45 for 400 ms, chosen to ensure a return to the average firing rates. The images were displayed in blocks of 1000, interleaved with a repeated set of 150 test images. Stimulation durations ranged from 5 to 40 blocks.</p><p>Dynamic movies consisted of the same set of images, each displayed for 500 ms with eye movements simulated as Brownian motion with a diffusion constant of 10 μm<sup>2</sup>/frame, selected to roughly match recorded eye movements from humans (<xref ref-type="bibr" rid="bib37">Kuang et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Van Der Linde et al., 2009</xref>) and primate (Z.M. Hafed and R.J. Krauzlis, personal communication, June 2008). After 500 ms, a new image appeared, with no gray screen between image presentations, and again was jittered. Each recording consisted of 5000 images, for a total of 300,000 frames of stimulation.</p></sec><sec id="s4-1-3"><title>Cell type classification</title><p>The spike triggered average (STA) stimulus for each neuron was computed from the response to the white noise stimulus (<xref ref-type="bibr" rid="bib7">Chichilnisky, 2001</xref>), to reveal the spatial, temporal, and chromatic properties of the light response. Cell type identification was performed by identifying distinct clusters in the response properties, including features of the time course and the spike train autocorrelation function extracted via principal components analysis, and the spatial extent of the RF (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>; <xref ref-type="bibr" rid="bib9">Dacey, 1993</xref>; <xref ref-type="bibr" rid="bib17">Devries and Baylor, 1997</xref>; <xref ref-type="bibr" rid="bib21">Field et al., 2007</xref>; <xref ref-type="bibr" rid="bib24">Frechette et al., 2005</xref>). This analysis revealed multiple identifiable and complete cell type populations. In particular, the four major types, ON and OFF parasol and midget cells, were readily identifiable by their temporal properties, RF size, density, and mosaic organization (see <xref ref-type="bibr" rid="bib58">Rhoades et al., 2019</xref> for a more detailed discussion). Recorded populations of parasol cells formed nearly complete mosaics over the region of retina recorded; recorded midget cell populations were less complete.</p></sec></sec><sec id="s4-2"><title>Linear reconstruction</title><sec id="s4-2-1"><title>Linear regression</title><p>Reconstruction filters were fitted using linear regression, as described in Results. The responses of every RGC were included in the regression for every pixel; restricting the filters to a local area did not improve reconstructions. Note that the weights for each pixel are independent, and can be fitted together or separately. Prior to regression, the distribution of each cell’s responses and the pixel values at each location were centered around 0 (i.e. the mean over samples was subtracted in each case). The length of time over which spikes were counted after the image onset was chosen to optimize reconstruction performance (tested in 10 ms intervals from 10 ms to 200 ms; see <xref ref-type="fig" rid="fig7">Figure 7E</xref>). For the spike latency comparison, a maximum time of 150 ms was assigned to cells that had not yet spiked.</p></sec><sec id="s4-2-2"><title>Convergence of estimates</title><p>For all recordings, reconstruction performance obtained with half of the data was typically 95-98% of the reconstruction performance obtained with the full data (<xref ref-type="fig" rid="fig12">Figure 12</xref>). Both an L2-penalty on filter coefficients and applying a singular value cutoff when calculating the pseudoinverse of the response matrix (<xref ref-type="bibr" rid="bib31">Golden et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Strang, 1980</xref>) were tested as methods for optimizing performance with limited data. However, neither improved reconstruction performance. Note that despite the large size of the weight matrix, the appropriate comparison for fitting is samples per pixel compared to weights per pixel, which is at least 20 times in every case, even when interaction terms are considered (<xref ref-type="fig" rid="fig9">Figure 9</xref>).</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Verification of data sufficiency.</title><p>(<bold>A</bold>) Performance of reconstructions from parasol cell responses as a function of the amount of training data, for 19 recordings (colors). (<bold>B</bold>) Fraction of performance of reconstructions from parasol cell responses (MSE, correlation, and SSIM) achieved with half of the training data for each recording. (<bold>C,D</bold>) Same as A, B for reconstructions from midget cell responses for 12 recordings (colors).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig12-v2.tif"/></fig></sec><sec id="s4-2-3"><title>Image region selection</title><p>Reconstruction performance was calculated over the image regions covered by the RFs of the recorded RGCs. To define this area, the spatial profile of each RF was fitted with a two-dimensional elliptical Gaussian (<xref ref-type="bibr" rid="bib8">Chichilnisky and Kalmar, 2002</xref>), and any pixel within two standard deviations was considered covered (<xref ref-type="fig" rid="fig13">Figure 13</xref>). For each analysis, pixels were only included if they were covered by at least one of each cell type used in that analysis, so the regions included were limited by the cell type with the least coverage, typically ON or OFF midget cells. Two analyses used a manually selected, rectangular central image region instead of the mosaic coverage logic above: the comparison across recordings (<xref ref-type="fig" rid="fig2">Figure 2</xref>), and the spatial frequency analyses (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Selection of analysis region.</title><p>Reconstruction performance on a sample image (top) is measured by comparing the regions inside the contours shown on the reconstructions in the second row. These contours were obtained using the receptive field mosaics (bottom two rows) of parasol cells, or of both parasol and midget cells, as described in <italic>Image region selection</italic>. Here, OFF midget cells had the least complete mosaic, so the included region was most limited by their coverage. The bounding boxes mark the extent of the visual stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58516-fig13-v2.tif"/></fig></sec><sec id="s4-2-4"><title>Error metrics</title><p>The primary measures of reconstruction performance, mean squared error (MSE) and the correlation coefficient, were calculated between the original and reconstructed image, across all included pixels (as defined above). Note that linear least squares regression, which was used to obtain the filters, by definition minimizes MSE on the training data, but does not necessarily maximize the correlation coefficient. In addition, an alternative measure more closely related to perceptual difference between images, the structural similarity (SSIM; <xref ref-type="bibr" rid="bib74">Wang et al., 2004</xref>), was calculated across the whole image (parameters: radius = 22 μm, exponents = [1 1 1]), and then averaged across the included pixels (see above) for each image. In all cases, similar trends were observed with each metric.</p></sec><sec id="s4-2-5"><title>Statistical analysis</title><p>Statistical significance was determined using resampling. In all cases presented here, two distributions of paired values were being compared, such as reconstruction performance scores for two conditions on the same set of images. To generate values in the null distribution, each pair of values was randomly distributed between the two conditions, and the mean difference was calculated. 1000 random samples were generated this way, and the p-value was the proportion of samples where the magnitude of the mean difference was greater than the recorded value. A report of p&lt;0.001 indicates that no samples had a larger mean difference.</p></sec></sec><sec id="s4-3"><title>Filter analysis</title><sec id="s4-3-1"><title>Spatial receptive field</title><p>The spatial RF (used in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>) was extracted from the full spatial, temporal, and chromatic spike-triggered average (STA; used for cell type classification as described above) as follows. First, the values at each pixel location and time in the STA were summed across the color channels. Significant pixels were identified as those with an absolute maximum value (across time) of more than five times the robust standard deviation of all the pixels in the STA (<xref ref-type="bibr" rid="bib25">Freeman et al., 2015</xref>). Averaging across these significant pixels resulted in a single time course. The inner product of this time course with the time course of each pixel in the STA was then computed, resulting in a spatial RF.</p></sec><sec id="s4-3-2"><title>Average filter calculations</title><p>Average RFs (<xref ref-type="fig" rid="fig3">Figure 3</xref>) were calculated by first upsampling the spatial RFs (with linear interpolation) to match the resolution of the reconstruction filters (across recordings, scaling ranged from 2-8x), then aligning the RF centers (obtained by fitting a 2D Gaussian to the RF as described above) and averaging. Average reconstruction filters (<xref ref-type="fig" rid="fig3">Figure 3</xref>) were not upsampled, but otherwise were calculated the same way. The average RFs and filters shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref> were calculated separately for each recording, cell type, and condition. A one-dimensional profile through the center of each average reconstruction filter was used to calculate full width at half maximum (<xref ref-type="fig" rid="fig3">Figure 3D,E</xref>). This calculation was robust to the angle of the profile. The average filters in <xref ref-type="fig" rid="fig5">Figure 5</xref> only included cells in regions with locally dense populations of all four major cell types (defined by the number of nearby cells of each type).</p></sec><sec id="s4-3-3"><title>Receptive field reconstruction</title><p>Reconstruction from RFs (<xref ref-type="fig" rid="fig4">Figure 4</xref>) was performed as follows. Each image was estimated as a sum of RFs, weighted by the RGC response and a fitted scale factor. These scale factors were calculated by minimizing the MSE between the true and estimated images as follows:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where S is the stimulus, R is the response, F is the RF, and a is the scale factor, calculated using linear least squares regression (as described above). In this case, each pixel in each image was considered a separate sample, and was modeled as a linear combination of the image responses of all RGCs multiplied by the respective values of their RFs at that pixel. Therefore, the outputs were a vector with length equal to N, the number of images times the number of pixels in each image. The input (regressor) matrix had dimensions (N x number of cells), and the weight vector a had dimensions (number of cells x 1). For these analyses, recordings with incomplete mosaics and without high-resolution RF mapping were excluded.</p></sec></sec><sec id="s4-4"><title>Analysis of cell type contributions</title><sec id="s4-4-1"><title>ON and OFF parasol cells</title><p>Images were reconstructed from the responses of either ON or OFF parasol cells and performance was calculated, as described above. The relationship between true and reconstructed pixel value (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) was calculated for each recording by first binning the true pixel values by percentile, resulting in bins with equal numbers of samples. Then, for each bin, the average true pixel value and the average of the corresponding reconstructed pixel values were calculated. The sensitivity was defined as the change in average reconstructed pixel value divided by the change in true pixel value across bins. The observed trends were not dependent on the number of bins.</p></sec><sec id="s4-4-2"><title>Parasol and midget cell classes</title><p>Images were reconstructed from the responses of either parasol or midget cell classes (including both ON and OFF types) and performance was calculated, as described above. The power spectra for the reconstructed images, original images, and average RFs (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) were calculated by discrete Fourier transform. The temporal properties of the parasol and midget classes (<xref ref-type="fig" rid="fig7">Figure 7E</xref>) were compared by gradually increasing the length of the window over which spikes were counted after image onset, from 10 ms to 150 ms (in 10 ms increments). For each window size, the reconstruction filters were refitted, and the performance was calculated as described above.</p><p>For these analyses, only the recordings with the highest midget cell coverage were used, defined by the fraction of pixels included in a parasol cell analysis that would also be included in a midget cell analysis (see Image region selection above). Seven recordings were included for measuring reconstruction performance (<xref ref-type="fig" rid="fig7">Figure 7C</xref>) and comparing temporal properties (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). Only three of those were also included in the spatial frequency analysis (<xref ref-type="fig" rid="fig7">Figure 7D</xref>), which required complete or nearly complete mosaics.</p></sec></sec><sec id="s4-5"><title>Analysis of noise correlations</title><p>Noise correlation analysis (<xref ref-type="fig" rid="fig8">Figure 8</xref>) was limited to the three recordings with the most repeated presentations of the same set of test images (27 repeats each). For each of the three scenarios described in Results, reconstruction filters were fitted on a single repeat of training data, and then tested using either shuffled or unshuffled testing data. The testing data was shuffled by randomly permuting each RGC’s responses independently across repeated presentations of the same image. Reconstruction performance on the test data was measured as described earlier.</p></sec><sec id="s4-6"><title>Interaction terms</title><p>Only the three recordings with the most training data were included (at least 25,000 training images each; the same subset was used for the noise correlation analysis), so that despite the increase in parameter count (from ~200 to ~1000), there were still more than enough samples to calculate the weights, and regularization did not improve cross-validated reconstruction performance.</p></sec><sec id="s4-7"><title>Linear-nonlinear simulation</title><p>Simple linear-nonlinear encoding models (<xref ref-type="bibr" rid="bib7">Chichilnisky, 2001</xref>) were used to simulate spike trains for reconstruction, for each RGC independently. For each image, the inner product was first computed between the image and the spatial RF (see section Spatial receptive field above), restricted to a local region (+/- 440 μm from the RF center, corresponding to either 40 × 40 or 80 × 80 pixels depending on the resolution of the images). The resulting value was then passed through a sigmoidal nonlinearity, given by<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>y</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula>where the parameters {b<sub>i</sub>} were fitted by minimizing the mean-squared error between the predicted and measured RGC responses, on the same data set used to fit the reconstruction filters. This model was then used to simulate responses to the images used to obtain the fitting data and the images used to obtain the held-out, repeated test data. Reconstruction filters, reconstructed images, and performance were then calculated from the simulated responses in the same way as described above for the recorded responses.</p></sec><sec id="s4-8"><title>Spatiotemporal reconstruction</title><p>Each frame of the spatiotemporal movie was reconstructed using the RGC spikes recorded during that frame and the following frames. Therefore, each RGC included in the reconstruction was fitted with a full-rank, spatiotemporal reconstruction filter. The spikes were binned at the frame rate of the movie, and a filter length of 15 frames (125 ms) was selected to optimize performance. A spatial summary of the spatiotemporal filter (<xref ref-type="fig" rid="fig11">Figure 11A,B</xref>) was calculated as described above for spatial RFs. The spacetime separability of the filters was calculated using the explained variance from the first component of a singular value decomposition (limited to a spatially local region to reduce the effects of the many low-magnitude, noisy pixels outside the primary filter peak). Three recordings that contained responses to both static, flashed natural images and dynamic, spatiotemporal natural movies were included. 2400 consecutive movie frames were withheld from fitting for comparison of movie frame and static image reconstructions (<xref ref-type="fig" rid="fig11">Figure 11C</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NSF IGERT 0801700 (NB), NSF GRFP DGE-114747 (NB, CR), NEI F31EY027166 (CR), Pew Charitable Trusts Fellowship in Biomedical Sciences (AS), donation from John Chen (AML), NIH R01EY017992, NIH NEI R01-EY029247, NSF/NIH CRCNS Grant IIS1430348 (EJC), and the Wu Tsai Neurosciences Institute. We thank Fred Rieke for helpful suggestions on the manuscript; Jill Desnoyer and Ryan Samarakoon for technical assistance; Sasi Madugula, Eric Wu and Alex Gogliettino for discussions and feedback; and Corinna Darian Smith and Tirin Moore (Stanford), Jose Carmena and Jack Gallant (UC Berkeley), Jonathan Horton (UCSF), and the UC Davis Primate Center for access to primate retinas.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Funding acquisition, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Software, Funding acquisition</p></fn><fn fn-type="con" id="con6"><p>Resources, Software, Funding acquisition</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: Eyes were removed from terminally anesthetized macaque monkeys (Macaca mulatta, Macaca fascicularis) used by other laboratories in the course of their experiments, in accordance with the Institutional Animal Care and Use Committee guidelines. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (#28860) of the Stanford University. The protocol was approved by the Administrative Panel on Laboratory Animal Care of the Stanford University (Assurance Number: A3213-01).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-58516-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Code and data to generate all of the summary plots are included in the supporting files. We are not able to release the raw voltage recordings, which total &gt;5 TBs and require a complex processing pipeline.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benardete</surname> <given-names>EA</given-names></name><name><surname>Kaplan</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1997">1997a</year><article-title>The receptive field of the primate P retinal ganglion cell, I: linear dynamics</article-title><source>Visual Neuroscience</source><volume>14</volume><fpage>169</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1017/S0952523800008853</pub-id><pub-id pub-id-type="pmid">9057278</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benardete</surname> <given-names>EA</given-names></name><name><surname>Kaplan</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1997">1997b</year><article-title>The receptive field of the primate P retinal ganglion cell, II: nonlinear dynamics</article-title><source>Visual Neuroscience</source><volume>14</volume><fpage>187</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1017/S0952523800008865</pub-id><pub-id pub-id-type="pmid">9057279</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Warland</surname> <given-names>DK</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The structure and precision of retinal spike trains</article-title><source>PNAS</source><volume>94</volume><fpage>5411</fpage><lpage>5416</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.10.5411</pub-id><pub-id pub-id-type="pmid">9144251</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name><name><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name><name><surname>Warland</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Reading a neural code</article-title><source>Science</source><volume>252</volume><fpage>1854</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1126/science.2063199</pub-id><pub-id pub-id-type="pmid">2063199</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botella-Soler</surname> <given-names>V</given-names></name><name><surname>Deny</surname> <given-names>S</given-names></name><name><surname>Martius</surname> <given-names>G</given-names></name><name><surname>Marre</surname> <given-names>O</given-names></name><name><surname>Tkačik</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nonlinear decoding of a complex movie from the mammalian retina</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006057</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006057</pub-id><pub-id pub-id-type="pmid">29746463</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cafaro</surname> <given-names>J</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Noise correlations improve response fidelity and stimulus encoding</article-title><source>Nature</source><volume>468</volume><fpage>964</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1038/nature09570</pub-id><pub-id pub-id-type="pmid">21131948</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Kalmar</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Functional asymmetries in ON and OFF ganglion cells of primate retina</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>2737</fpage><lpage>2747</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-07-02737.2002</pub-id><pub-id pub-id-type="pmid">11923439</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The mosaic of midget ganglion cells in the human retina</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>5334</fpage><lpage>5355</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-12-05334.1993</pub-id><pub-id pub-id-type="pmid">8254378</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname> <given-names>DM</given-names></name><name><surname>Peterson</surname> <given-names>BB</given-names></name><name><surname>Robinson</surname> <given-names>FR</given-names></name><name><surname>Gamlin</surname> <given-names>PD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Fireworks in the primate retina: in vitro photodynamics reveals diverse LGN-projecting ganglion cell types</article-title><source>Neuron</source><volume>37</volume><fpage>15</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01143-1</pub-id><pub-id pub-id-type="pmid">12526769</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname> <given-names>DM</given-names></name><name><surname>Petersen</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Dendritic field size and morphology of midget and parasol ganglion cells of the human retina</article-title><source>PNAS</source><volume>89</volume><fpage>9666</fpage><lpage>9670</lpage><pub-id pub-id-type="doi">10.1073/pnas.89.20.9666</pub-id><pub-id pub-id-type="pmid">1409680</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawis</surname> <given-names>S</given-names></name><name><surname>Shapley</surname> <given-names>R</given-names></name><name><surname>Kaplan</surname> <given-names>E</given-names></name><name><surname>Tranchina</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>The receptive field organization of X-cells in the cat: spatiotemporal coupling and asymmetry</article-title><source>Vision Research</source><volume>24</volume><fpage>549</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(84)90109-3</pub-id><pub-id pub-id-type="pmid">6740975</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Monasterio</surname> <given-names>FM</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Properties of concentrically organized X and Y ganglion cells of macaque retina</article-title><source>Journal of Neurophysiology</source><volume>41</volume><fpage>1394</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1152/jn.1978.41.6.1394</pub-id><pub-id pub-id-type="pmid">104012</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Monasterio</surname> <given-names>FM</given-names></name><name><surname>Gouras</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Functional properties of ganglion cells of the rhesus monkey retina</article-title><source>The Journal of Physiology</source><volume>251</volume><fpage>167</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1975.sp011086</pub-id><pub-id pub-id-type="pmid">810576</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derrington</surname> <given-names>AM</given-names></name><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The influence of temporal frequency and adaptation level on receptive field organization of retinal ganglion cells in cat</article-title><source>The Journal of Physiology</source><volume>333</volume><fpage>343</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1982.sp014457</pub-id><pub-id pub-id-type="pmid">7182469</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeVries</surname> <given-names>SH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Correlated firing in rabbit retinal ganglion cells</article-title><source>Journal of Neurophysiology</source><volume>81</volume><fpage>908</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.81.2.908</pub-id><pub-id pub-id-type="pmid">10036288</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devries</surname> <given-names>SH</given-names></name><name><surname>Baylor</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Mosaic arrangement of ganglion cell receptive fields in rabbit retina</article-title><source>Journal of Neurophysiology</source><volume>78</volume><fpage>2048</fpage><lpage>2060</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.78.4.2048</pub-id><pub-id pub-id-type="pmid">9325372</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enroth-Cugell</surname> <given-names>C</given-names></name><name><surname>Robson</surname> <given-names>JG</given-names></name><name><surname>Schweitzer-Tong</surname> <given-names>DE</given-names></name><name><surname>Watson</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Spatio-temporal interactions in cat retinal ganglion cells showing linear spatial summation</article-title><source>The Journal of Physiology</source><volume>341</volume><fpage>279</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1983.sp014806</pub-id><pub-id pub-id-type="pmid">6620181</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>ImageNet: constructing a large-scale image database</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>1037</elocation-id><pub-id pub-id-type="doi">10.1167/9.8.1037</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatial properties and functional organization of small bistratified ganglion cells in primate retina</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>13261</fpage><lpage>13272</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3437-07.2007</pub-id><pub-id pub-id-type="pmid">18045920</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Rangel</surname> <given-names>C</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Marshak</surname> <given-names>DW</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>High-sensitivity rod photoreceptor input to the blue-yellow color opponent pathway in macaque retina</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1159</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1038/nn.2353</pub-id><pub-id pub-id-type="pmid">19668201</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Machado</surname> <given-names>TA</given-names></name><name><surname>Jepson</surname> <given-names>LH</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Gunning</surname> <given-names>DE</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Dabrowski</surname> <given-names>W</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional connectivity in the retina at the resolution of photoreceptors</article-title><source>Nature</source><volume>467</volume><fpage>673</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1038/nature09424</pub-id><pub-id pub-id-type="pmid">20930838</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frechette</surname> <given-names>ES</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Grivich</surname> <given-names>MI</given-names></name><name><surname>Petrusca</surname> <given-names>D</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fidelity of the ensemble code for visual motion in primate retina</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>119</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1152/jn.01175.2004</pub-id><pub-id pub-id-type="pmid">15625091</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname> <given-names>J</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Li</surname> <given-names>PH</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Gunning</surname> <given-names>DE</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping nonlinear receptive field structure in primate retina at single cone resolution</article-title><source>eLife</source><volume>4</volume><elocation-id>e05241</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05241</pub-id><pub-id pub-id-type="pmid">26517879</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganmor</surname> <given-names>E</given-names></name><name><surname>Segev</surname> <given-names>R</given-names></name><name><surname>Schneidman</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A thesaurus for a neural population code</article-title><source>eLife</source><volume>4</volume><elocation-id>e06134</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06134</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Uniform signal redundancy of parasol and midget ganglion cells in primate retina</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>4675</fpage><lpage>4680</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5294-08.2009</pub-id><pub-id pub-id-type="pmid">19357292</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname> <given-names>AR</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gjorgjieva</surname> <given-names>J</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional diversity among sensory neurons from efficient coding principles</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007476</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007476</pub-id><pub-id pub-id-type="pmid">31725714</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goetz</surname> <given-names>GA</given-names></name><name><surname>Palanker</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Electronic approaches to restoration of sight</article-title><source>Reports on Progress in Physics</source><volume>79</volume><elocation-id>096701</elocation-id><pub-id pub-id-type="doi">10.1088/0034-4885/79/9/096701</pub-id><pub-id pub-id-type="pmid">27502748</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golden</surname> <given-names>JR</given-names></name><name><surname>Erickson-Davis</surname> <given-names>C</given-names></name><name><surname>Cottaris</surname> <given-names>NP</given-names></name><name><surname>Parthasarathy</surname> <given-names>N</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Simulation of visual perception and learning with a retinal prosthesis</article-title><source>Journal of Neural Engineering</source><volume>16</volume><elocation-id>025003</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aaf270</pub-id><pub-id pub-id-type="pmid">30523985</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gollisch</surname> <given-names>T</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Rapid neural coding in the retina with relative spike latencies</article-title><source>Science</source><volume>319</volume><fpage>1108</fpage><lpage>1111</lpage><pub-id pub-id-type="doi">10.1126/science.1149639</pub-id><pub-id pub-id-type="pmid">18292344</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouras</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Identification of cone mechanisms in monkey ganglion cells</article-title><source>The Journal of Physiology</source><volume>199</volume><fpage>533</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008667</pub-id><pub-id pub-id-type="pmid">4974745</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Bakolitsa</surname> <given-names>C</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Jepson</surname> <given-names>LH</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Correlated firing among major ganglion cell types in primate retina</article-title><source>The Journal of Physiology</source><volume>589</volume><fpage>75</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2010.193888</pub-id><pub-id pub-id-type="pmid">20921200</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gütig</surname> <given-names>R</given-names></name><name><surname>Gollisch</surname> <given-names>T</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Computing complex visual features with retinal spike times</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e53063</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0053063</pub-id><pub-id pub-id-type="pmid">23301021</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>YJ</given-names></name><name><surname>Brackbill</surname> <given-names>N</given-names></name><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Lee</surname> <given-names>J</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Tong</surname> <given-names>W</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nonlinear decoding of natural images from large-scale primate retinal ganglion recordings</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.09.07.285742</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuang</surname> <given-names>X</given-names></name><name><surname>Poletti</surname> <given-names>M</given-names></name><name><surname>Victor</surname> <given-names>JD</given-names></name><name><surname>Rucci</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Temporal encoding of spatial information during active visual fixation</article-title><source>Current Biology</source><volume>22</volume><fpage>510</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.01.050</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuffler</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Discharge patterns and functional organization of mammalian retina</article-title><source>Journal of Neurophysiology</source><volume>16</volume><fpage>37</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1152/jn.1953.16.1.37</pub-id><pub-id pub-id-type="pmid">13035466</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawhern</surname> <given-names>V</given-names></name><name><surname>Wu</surname> <given-names>W</given-names></name><name><surname>Hatsopoulos</surname> <given-names>N</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Population decoding of motor cortical activity using a generalized linear model with hidden states</article-title><source>Journal of Neuroscience Methods</source><volume>189</volume><fpage>267</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.03.024</pub-id><pub-id pub-id-type="pmid">20359500</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lettvin</surname> <given-names>J</given-names></name><name><surname>Maturana</surname> <given-names>H</given-names></name><name><surname>McCulloch</surname> <given-names>W</given-names></name><name><surname>Pitts</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>What the frog's Eye Tells the Frog's Brain</article-title><source>Proceedings of the IRE</source><volume>47</volume><fpage>1940</fpage><lpage>1951</lpage><pub-id pub-id-type="doi">10.1109/JRPROC.1959.287207</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Bezayiff</surname> <given-names>N</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Cunningham</surname> <given-names>W</given-names></name><name><surname>Dabrowski</surname> <given-names>W</given-names></name><name><surname>Grillo</surname> <given-names>AA</given-names></name><name><surname>Grivich</surname> <given-names>M</given-names></name><name><surname>Grybos</surname> <given-names>P</given-names></name><name><surname>Hottowy</surname> <given-names>P</given-names></name><name><surname>Kachiguine</surname> <given-names>S</given-names></name><name><surname>Kalmar</surname> <given-names>RS</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Petrusca</surname> <given-names>D</given-names></name><name><surname>Rahman</surname> <given-names>M</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>What does the eye tell the brain?: development of a system for the large-scale recording of retinal output activity</article-title><source>IEEE Transactions on Nuclear Science</source><volume>51</volume><fpage>1434</fpage><lpage>1440</lpage><pub-id pub-id-type="doi">10.1109/TNS.2004.832706</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manookin</surname> <given-names>MB</given-names></name><name><surname>Patterson</surname> <given-names>SS</given-names></name><name><surname>Linehan</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mechanisms mediating motion sensitivity in parasol ganglion cells of the primate retina</article-title><source>Neuron</source><volume>97</volume><fpage>1327</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.02.006</pub-id><pub-id pub-id-type="pmid">29503188</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masland</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The neuronal organization of the retina</article-title><source>Neuron</source><volume>76</volume><fpage>266</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.002</pub-id><pub-id pub-id-type="pmid">23083731</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastronarde</surname> <given-names>DN</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Interactions between ganglion cells in cat retina</article-title><source>Journal of Neurophysiology</source><volume>49</volume><fpage>350</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1152/jn.1983.49.2.350</pub-id><pub-id pub-id-type="pmid">6300342</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname> <given-names>M</given-names></name><name><surname>Lagnado</surname> <given-names>L</given-names></name><name><surname>Baylor</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Concerted signaling by retinal ganglion cells</article-title><source>Science</source><volume>270</volume><fpage>1207</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1126/science.270.5239.1207</pub-id><pub-id pub-id-type="pmid">7502047</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merigan</surname> <given-names>WH</given-names></name><name><surname>Katz</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Spatial resolution across the macaque retina</article-title><source>Vision Research</source><volume>30</volume><fpage>985</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90107-V</pub-id><pub-id pub-id-type="pmid">2392842</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meytlis</surname> <given-names>M</given-names></name><name><surname>Nichols</surname> <given-names>Z</given-names></name><name><surname>Nirenberg</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Determining the role of correlated firing in large populations of neurons using white noise and natural scene stimuli</article-title><source>Vision Research</source><volume>70</volume><fpage>44</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.07.007</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname> <given-names>T</given-names></name><name><surname>Prenger</surname> <given-names>RJ</given-names></name><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Oliver</surname> <given-names>M</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian reconstruction of natural images from human brain activity</article-title><source>Neuron</source><volume>63</volume><fpage>902</fpage><lpage>915</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.006</pub-id><pub-id pub-id-type="pmid">19778517</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname> <given-names>T</given-names></name><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Nishimoto</surname> <given-names>S</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Encoding and decoding in fMRI</article-title><source>NeuroImage</source><volume>56</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id><pub-id pub-id-type="pmid">20691790</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nirenberg</surname> <given-names>S</given-names></name><name><surname>Carcieri</surname> <given-names>SM</given-names></name><name><surname>Jacobs</surname> <given-names>AL</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Retinal ganglion cells act largely as independent encoders</article-title><source>Nature</source><volume>411</volume><fpage>698</fpage><lpage>701</lpage><pub-id pub-id-type="doi">10.1038/35079612</pub-id><pub-id pub-id-type="pmid">11395773</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paik</surname> <given-names>S-B</given-names></name><name><surname>Ringach</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Retinal origin of orientation maps in visual cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>919</fpage><lpage>925</lpage><pub-id pub-id-type="doi">10.1038/nn.2824</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname> <given-names>N</given-names></name><name><surname>Batty</surname> <given-names>E</given-names></name><name><surname>Falcon</surname> <given-names>W</given-names></name><name><surname>Rutten</surname> <given-names>T</given-names></name><name><surname>Rajpal</surname> <given-names>M</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural networks for efficient bayesian decoding of natural images from retinal neurons</article-title><source>Advances in Neural Information Processing Systems</source></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname> <given-names>VH</given-names></name><name><surname>Cowey</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>The ganglion cell and cone distributions in the monkey's retina: implications for central magnification factors</article-title><source>Vision Research</source><volume>25</volume><fpage>1795</fpage><lpage>1810</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(85)90004-5</pub-id><pub-id pub-id-type="pmid">3832605</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portelli</surname> <given-names>G</given-names></name><name><surname>Barrett</surname> <given-names>JM</given-names></name><name><surname>Hilgen</surname> <given-names>G</given-names></name><name><surname>Masquelier</surname> <given-names>T</given-names></name><name><surname>Maccione</surname> <given-names>A</given-names></name><name><surname>Di Marco</surname> <given-names>S</given-names></name><name><surname>Berdondini</surname> <given-names>L</given-names></name><name><surname>Kornprobst</surname> <given-names>P</given-names></name><name><surname>Sernagor</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rank order coding: a retinal information decoding strategy revealed by Large-Scale multielectrode array retinal recordings</article-title><source>Eneuro</source><volume>3</volume><elocation-id>ENEURO.0134-15.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0134-15.2016</pub-id><pub-id pub-id-type="pmid">27275008</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puchalla</surname> <given-names>JL</given-names></name><name><surname>Schneidman</surname> <given-names>E</given-names></name><name><surname>Harris</surname> <given-names>RA</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Redundancy in the population code of the retina</article-title><source>Neuron</source><volume>46</volume><fpage>493</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.03.026</pub-id><pub-id pub-id-type="pmid">15882648</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puller</surname> <given-names>C</given-names></name><name><surname>Manookin</surname> <given-names>MB</given-names></name><name><surname>Neitz</surname> <given-names>J</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name><name><surname>Neitz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Broad thorny ganglion cells: a candidate for visual pursuit error signaling in the primate retina</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>5397</fpage><lpage>5408</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4369-14.2015</pub-id><pub-id pub-id-type="pmid">25834063</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhoades</surname> <given-names>CE</given-names></name><name><surname>Shah</surname> <given-names>NP</given-names></name><name><surname>Manookin</surname> <given-names>MB</given-names></name><name><surname>Brackbill</surname> <given-names>N</given-names></name><name><surname>Kling</surname> <given-names>A</given-names></name><name><surname>Goetz</surname> <given-names>G</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unusual physiological properties of smooth monostratified ganglion cell types in primate retina</article-title><source>Neuron</source><volume>103</volume><fpage>658</fpage><lpage>672</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.036</pub-id><pub-id pub-id-type="pmid">31227309</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rieke</surname> <given-names>F</given-names></name><name><surname>Warland</surname> <given-names>D</given-names></name><name><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Spikes: Exploring the Neural Code</source><publisher-name>Bradford Books</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>On the origin of the functional architecture of the cortex</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e251</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000251</pub-id><pub-id pub-id-type="pmid">17330140</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname> <given-names>EA</given-names></name><name><surname>Roorda</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The relationship between visual resolution and cone spacing in the human fovea</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>156</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/nn.2465</pub-id><pub-id pub-id-type="pmid">20023654</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruda</surname> <given-names>K</given-names></name><name><surname>Zylberberg</surname> <given-names>J</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ignoring correlated activity causes a failure of retinal population codes</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4605</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18436-2</pub-id><pub-id pub-id-type="pmid">32929073</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruderman</surname> <given-names>DL</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistics of natural images: scaling in the woods</article-title><source>Physical Review</source><volume>73</volume><fpage>814</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.73.814</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shah</surname> <given-names>NP</given-names></name><name><surname>Madugula</surname> <given-names>S</given-names></name><name><surname>Grosberg</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>In 2019 9th international IEEE/EMBS conference on neural engineering (NER)</article-title><conf-name>Optimization of Electrical Stimulation for a High-Fidelity Artificial Retina</conf-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Grivich</surname> <given-names>MI</given-names></name><name><surname>Petrusca</surname> <given-names>D</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The structure of multi-neuron firing patterns in primate retina</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>8254</fpage><lpage>8266</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1282-06.2006</pub-id><pub-id pub-id-type="pmid">16899720</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname> <given-names>GB</given-names></name><name><surname>Li</surname> <given-names>FF</given-names></name><name><surname>Dan</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>8036</fpage><lpage>8042</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-18-08036.1999</pub-id><pub-id pub-id-type="pmid">10479703</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strang</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><source>Linear Algebra and Its Applications</source><publisher-name>CENGAGE LEARNING</publisher-name></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thibos</surname> <given-names>LN</given-names></name><name><surname>Cheney</surname> <given-names>FE</given-names></name><name><surname>Walsh</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Retinal limits to the detection and resolution of gratings</article-title><source>Journal of the Optical Society of America A</source><volume>4</volume><fpage>1524</fpage><lpage>1529</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.4.001524</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troy</surname> <given-names>JB</given-names></name><name><surname>Lee</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Steady discharges of macaque retinal ganglion cells</article-title><source>Visual Neuroscience</source><volume>11</volume><fpage>111</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1017/S0952523800011159</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>MH</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Synaptic rectification controls nonlinear spatial integration of natural visual inputs</article-title><source>Neuron</source><volume>90</volume><fpage>1257</fpage><lpage>1271</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.006</pub-id><pub-id pub-id-type="pmid">27263968</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uzzell</surname> <given-names>VJ</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Precision of spike trains in primate retinal ganglion cells</article-title><source>Journal of Neurophysiology</source><volume>92</volume><fpage>780</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1152/jn.01171.2003</pub-id><pub-id pub-id-type="pmid">15277596</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Linde</surname> <given-names>I</given-names></name><name><surname>Rajashekar</surname> <given-names>U</given-names></name><name><surname>Bovik</surname> <given-names>AC</given-names></name><name><surname>Cormack</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>DOVES: a database of visual eye movements</article-title><source>Spatial Vision</source><volume>22</volume><fpage>161</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1163/156856809787465636</pub-id><pub-id pub-id-type="pmid">19228456</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vargas-Irwin</surname> <given-names>CE</given-names></name><name><surname>Shakhnarovich</surname> <given-names>G</given-names></name><name><surname>Yadollahpour</surname> <given-names>P</given-names></name><name><surname>Mislow</surname> <given-names>JM</given-names></name><name><surname>Black</surname> <given-names>MJ</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decoding complete reach and grasp actions from local primary motor cortex populations</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>9659</fpage><lpage>9669</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5443-09.2010</pub-id><pub-id pub-id-type="pmid">20660249</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Z</given-names></name><name><surname>Bovik</surname> <given-names>AC</given-names></name><name><surname>Sheikh</surname> <given-names>HR</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Image quality assessment: from error visibility to structural similarity</article-title><source>IEEE Transactions on Image Processing</source><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Z</given-names></name><name><surname>Lu</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>IEEE international conference on acoustics speech and signal processing</article-title><conf-name>Why Is Image Quality Assessment So Difficult?</conf-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warland</surname> <given-names>DK</given-names></name><name><surname>Reinagel</surname> <given-names>P</given-names></name><name><surname>Meister</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Decoding visual information from a population of retinal ganglion cells</article-title><source>Journal of Neurophysiology</source><volume>78</volume><fpage>2336</fpage><lpage>2350</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.78.5.2336</pub-id><pub-id pub-id-type="pmid">9356386</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wässle</surname> <given-names>H</given-names></name><name><surname>Peichl</surname> <given-names>L</given-names></name><name><surname>Boycott</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Mosaics and territories of cat retinal ganglion cells</article-title><source>Progress in Brain Research</source><volume>58</volume><fpage>183</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(08)60019-9</pub-id><pub-id pub-id-type="pmid">6195688</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Jia</surname> <given-names>S</given-names></name><name><surname>Zheng</surname> <given-names>Y</given-names></name><name><surname>Yu</surname> <given-names>Z</given-names></name><name><surname>Tian</surname> <given-names>Y</given-names></name><name><surname>Ma</surname> <given-names>S</given-names></name><name><surname>Huang</surname> <given-names>T</given-names></name><name><surname>Liu</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reconstruction of natural visual scenes from neural spikes with deep neural networks</article-title><source>Neural Networks</source><volume>125</volume><fpage>19</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2020.01.033</pub-id><pub-id pub-id-type="pmid">32070853</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>J</given-names></name><name><surname>Cafaro</surname> <given-names>J</given-names></name><name><surname>Turner</surname> <given-names>MH</given-names></name><name><surname>Shea-Brown</surname> <given-names>E</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Direction-Selective circuits shape noise to ensure a precise population code</article-title><source>Neuron</source><volume>89</volume><fpage>369</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.019</pub-id><pub-id pub-id-type="pmid">26796691</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.58516.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Meister</surname><given-names>Markus</given-names></name><role>Reviewing Editor</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Asari</surname><given-names>Hiroki</given-names> </name><role>Reviewer</role><aff><institution>EMBL Rome</institution><country>Italy</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This article reveals &quot;what the monkey's eye tells the monkey's brain&quot;. The authors show how one can reconstruct the visual image on the retina based on the spike signals from optic nerve fibers. Taking advantage of recordings from nearly complete populations of retinal neurons, they explore how the different types of retinal ganglion cell interact in shaping the visual message sent to the brain. The resulting rules are pleasingly simple, which may well be a design principle for the retinal code.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Reconstruction of natural images from responses of primate retinal ganglion cells&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Hiroki Asari (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional data or analysis, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Summary:</p><p>This paper regards visual signaling by the macaque retina, specifically as viewed from the perspective of visual centers in the brain. The question is how one should interpret spike trains from retinal ganglion cells in order to reconstruct the visual image shown to the animal. Based on the established method of linear reconstruction, the authors explore how the reconstruction quality and the cells' reconstruction filters depend on the types and numbers of cells used for the reconstruction. They further study how noise correlations, nonlinear response transformations, and interactions between cells may contribute to the reconstruction. The presentation is very clear and pleasantly easy to follow, despite the technical material. The results have implications for an understanding of neural processing in the retina, and perhaps more so for the design of future retinal prostheses. In this regard the study draws particular value from using the macaque retina, which is close in function to our own.</p><p>Essential revisions:</p><p>At the same time this work comes on the background of a well-developed &quot;standard model&quot; of the retina that spells out how retinal ganglion cells encode the visual scene with spikes. Much of what is described here is fully expected based on that standard model. Some of the specific analyses have been done before in other species. Most of the comparison between optimal filters and single-cell receptive fields can be understood based on purely linear processing. A few other items require Linear-Nonlinear processing, which is also part of the standard model. Much of the report reads like elegant and rigorous confirmation of the conventional picture. The authors should take a clear position on the relation of their findings to the background knowledge. The reviewers can envision two possible outcomes:</p><p>A) We tried everything, but actually the optimal decoding filters are just as expected from a simple LN picture of retinal encoding. Even the noise correlations that we reported on before don't make any difference. This is good news for creating retinal prostheses because one doesn't have to engage in any sophisticated encoding.</p><p>B) We tried everything and found some interesting deviations from the conventional model of retinal function. Here they are specifically, along with the magnitude of their contributions. We expect that these deviations will have to be emulated by retinal prostheses.</p><p>In our reading, outcome (A) seems more likely, but either way the authors should choose a position.</p><p>Some specific impressions, organized by the claims in the Abstract:</p><p>1) &quot;Each cell's visual message, defined by the optimal reconstruction filter, reflected natural image statistics, and resembled the receptive field only when nearby, same-type cells were included&quot; This is as observed previously when reconstructing natural signals that include strong correlations. Much of the effect can be explained based on linear processing.</p><p>2) &quot;Each cell type revealed different and largely independent visual representations, consistent with their distinct properties.&quot; This is similar to previous observations on RGCs. Independence of On and Off representations is largely explained by the opposite rectification in On and Off cells. But note midgets and parasols are not &quot;largely independent&quot; by this criterion. In fact the midget filter is <italic>more</italic> affected by including parasols than by including other midgets (Figure 5B). That is a deviation from the conventional idea of independent channels. On the other hand, one would predict these effects on the filters from the previously reported spike correlations among these types (Greschner, 2011).</p><p>3) &quot;Stimulus-independent correlations primarily affected reconstructions from noisy responses.&quot; These noisy responses were created artificially by ignoring most of the spikes. That is not a relevant condition for actual vision. It would have been more interesting to experiment at low light levels, where prior work has shown the importance of noise. The present results seem to say that at high light levels the retinal noise is not limiting for reconstruction.</p><p>4) &quot;Nonlinear response transformation slightly improved reconstructions with either ON or OFF parasol cells, but not both.&quot; Again the small effects seen here for On or Off cells alone are expected from the LN model. But because one has to include both On and Off cells anyway just to get the basic reconstruction correct, one can conclude that overall the nonlinear transformations don't matter for reconstruction.</p><p>5) &quot;Inclusion of ON-OFF interactions enhanced reconstruction by emphasizing oriented edges, consistent with linear-nonlinear encoding models.&quot; These are the tiniest effects in the whole report: Δρ=0.009±0.023. The mean effect is just half of what was called &quot;slightly improved&quot; in point (4). It is also half of the standard deviation. Often the effect is negative, i.e. the reconstruction is worse even though the model has many more parameters; this hints at overfitting. Figure 9G is not impressive.</p><p>6) &quot;Spatiotemporal reconstructions revealed similar spatial visual messages.&quot; A useful but &quot;limited test&quot; of generalization to real vision. One claim is that the spatio-temporal filters had &quot;high space-time separability&quot;. But this leaves on the table 22% of explainable variance in the filter, which is &gt;20 times the effect size in point (5) that got covered at great length. If, in fact, it turned out that space-time separable filters are just fine for reconstruction of videos, that would be an interesting departure from conventional wisdom, where different time course of RF center and surround have figured prominently since the 1960s.</p><p>Detailed suggestions:</p><p>7) To test which properties of retinal encoding contribute to the reconstruction filters, the authors could replace the experimental data with simulated spike trains. They have done this before to great effect using spiking LN or GLM neurons. Is reconstruction performance the same? If not, what are the important differences? Are certain aspects of visual scenes reconstructed better or worse by real cells than model cells?</p><p>8) Parasol cells are very spatially nonlinear in their responses to natural scenes (Turner and Rieke, 2016), and both ON and OFF parasols are highly motion sensitive (Manookin et al., 2018). So their use for a linear reconstruction of a static scene requires justification. Is it possible that their true role will become more apparent when reconstructing movement within the visual scene (Frechette et al., 2005)?</p><p>9) Related to 8: the conclusion that different RGC types &quot;conveyed different and largely independent features of the visual scene&quot; might be inappropriate when comparing midget and parasol cells. Apart from a slight difference at higher spatial frequencies, the reconstructed features of the two cell types actually seem quite similar (Figure 7).</p><p>10) More regarding the interaction between parasol and midget signals: Did adding parasol responses to midget responses aid in reconstruction simply because parasols filled in gaps in the midget mosaics? To test this, perhaps one could artificially fill in the midget mosaics with LN model cells.</p><p>11) Quality of midget vs. parasol reconstructions: RGC density is a dominant factor in image reconstruction (Results), and images reconstructed from midgets cover a wider spatial-frequency range than parasols (Figure 7D; Results). However, the reconstruction from denser midget cells is worse than that from sparser parasol cells (Figure 7C; Results). Why?</p><p>12) For the analysis that included a nonlinear, logarithmic transformation of responses, was the transformation taken into account when re-computing the weight matrix W. Also, what happened with the logarithm for bins with R=0?</p><p>13) Statistical tests: For key claims, authors should perform statistical tests to clarify the significance and better interpret the d(rho): e.g., smoothed vs. non-smoothed images, spike-counts vs. latency coding, etc.</p><p>14) Figure 13: Unclear how the reconstruction boundary relates to the RF mosaics. 'Parasol+midget' looks most like the RFs for OFF midget alone.</p><p>15) All figures: Scale bars, e.g. in degrees of visual angle, would be useful at least in key places. Also were the stimuli sized for a particular viewing distance?</p><p>16) Figure 4C: How many data points?</p><p>17) Figure 7D: What are the 3 different curves for each set? Also maybe show the 'parasol+midget' result in black?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.58516.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>At the same time this work comes on the background of a well-developed &quot;standard model&quot; of the retina that spells out how retinal ganglion cells encode the visual scene with spikes. Much of what is described here is fully expected based on that standard model. Some of the specific analyses have been done before in other species. Most of the comparison between optimal filters and single-cell receptive fields can be understood based on purely linear processing. A few other items require Linear-Nonlinear processing, which is also part of the standard model. Much of the report reads like elegant and rigorous confirmation of the conventional picture. The authors should take a clear position on the relation of their findings to the background knowledge. The reviewers can envision two possible outcomes:</p><p>A) We tried everything, but actually the optimal decoding filters are just as expected from a simple LN picture of retinal encoding. Even the noise correlations that we reported on before don't make any difference. This is good news for creating retinal prostheses because one doesn't have to engage in any sophisticated encoding.</p><p>B) We tried everything and found some interesting deviations from the conventional model of retinal function. Here they are specifically, along with the magnitude of their contributions. We expect that these deviations will have to be emulated by retinal prostheses.</p><p>In our reading, outcome (A) seems more likely, but either way the authors should choose a position.</p></disp-quote><p>We thank the reviewers for the opportunity to refine the main message of the paper, and we agree that the findings primarily support interpretation (A). As suggested, we have compared the results to simulations using a simple linear-nonlinear (LN) model of RGC light response. We found a strong correspondence in both the reconstruction filters and the reconstructed images, which provides additional support for interpretation (A). Details of this comparison are described further under point (7), and these results are described in the manuscript in the newly added subsection “Comparison to simple models of RGC light response”. This section should also help clarify the small impacts of noise correlations and nonlinear reconstruction (mentioned by the reviewers below under points 4 and 5).</p><disp-quote content-type="editor-comment"><p>Some specific impressions, organized by the claims in the Abstract:</p><p>1) &quot;Each cell's visual message, defined by the optimal reconstruction filter, reflected natural image statistics, and resembled the receptive field only when nearby, same-type cells were included&quot; This is as observed previously when reconstructing natural signals that include strong correlations. Much of the effect can be explained based on linear processing.</p></disp-quote><p>Yes, we found that the optimal reconstruction filters were consistent with what would be expected from simple encoding models that have the same spatial structure and organization as the measured RFs, a key aspect of the data presented here. A comparison using simulated spike trains is described below, under point (7).</p><disp-quote content-type="editor-comment"><p>2) &quot;Each cell type revealed different and largely independent visual representations, consistent with their distinct properties.&quot; This is similar to previous observations on RGCs. Independence of On and Off representations is largely explained by the opposite rectification in On and Off cells. But note midgets and parasols are not &quot;largely independent&quot; by this criterion. In fact the midget filter is more affected by including parasols than by including other midgets (Figure 5B). That is a deviation from the conventional idea of independent channels. On the other hand, one would predict these effects on the filters from the previously reported spike correlations among these types (Greschner, 2011).</p></disp-quote><p>Yes, we agree that this analysis revealed that the spatial information carried by the parasol and midget cell classes was not quite independent, and we have updated the manuscript to emphasize this point (Abstract, Introduction, Results, Discussion). The asymmetric nature of the effect (parasols affect midgets, but not vice-versa) is also interesting, and likely reflects differences in response strength (see the response to point (11) below). It also seems likely that future investigations of temporal encoding will clarify the independence of these channels, a point we have now mentioned more prominently in the manuscript (Discussion).</p><disp-quote content-type="editor-comment"><p>3) &quot;Stimulus-independent correlations primarily affected reconstructions from noisy responses.&quot; These noisy responses were created artificially by ignoring most of the spikes. That is not a relevant condition for actual vision. It would have been more interesting to experiment at low light levels, where prior work has shown the importance of noise. The present results seem to say that at high light levels the retinal noise is not limiting for reconstruction.</p></disp-quote><p>We assumed that using a short time window, which captured the peak firing activity but did not include every stimulus driven spike, <italic>does</italic> relate to natural vision: some important aspects of perception happen very rapidly rather than awaiting the arrival of all of the stimulus-driven spikes from the retina. However, the reviewer is right that we assumed that this approach would be a way to emulate a low-fidelity encoding situation such as occurs at low light levels, without performing an empirical test. We have edited the text to clarify these assumptions (subsection “The effect of correlated firing”).</p><disp-quote content-type="editor-comment"><p>4) &quot;Nonlinear response transformation slightly improved reconstructions with either ON or OFF parasol cells, but not both.&quot; Again the small effects seen here for On or Off cells alone are expected from the LN model. But because one has to include both On and Off cells anyway just to get the basic reconstruction correct, one can conclude that overall the nonlinear transformations don't matter for reconstruction.</p></disp-quote><p>We agree that these results support the conclusion that nonlinear transformations are not important for subsequent linear reconstruction when both ON and OFF types are included, as in natural vision. However, a priori, reconstructions could have required only ON or OFF cells, if the appropriate nonlinear transformation were used prior to reconstruction. For example, the responses of the ON parasol cells could potentially contain enough information to reconstruct dark regions of the image, but not in a linear way. This would be the case if the encoding nonlinearity were not strongly rectified, and could be corrected for, and would imply that downstream targets receiving information from just ON cells could fully represent the image. However, we did not find that to be the case. We have attempted to be more explicit about this in the text, and have provided a direct comparison between reconstructions from the transformed responses of a single cell type and the original responses of both cell types (subsection “Nonlinear reconstruction”).</p><disp-quote content-type="editor-comment"><p>5) &quot;Inclusion of ON-OFF interactions enhanced reconstruction by emphasizing oriented edges, consistent with linear-nonlinear encoding models.&quot; These are the tiniest effects in the whole report: Δρ=0.009±0.023. The mean effect is just half of what was called &quot;slightly improved&quot; in point (4). It is also half of the standard deviation. Often the effect is negative, i.e. the reconstruction is worse even though the model has many more parameters; this hints at overfitting. Figure 9G is not impressive.</p></disp-quote><p>We realize that the effect size in this analysis is quite small, which we had intended to convey. However, we still think this point needs to be thoroughly documented, because it was not obvious or known from previous studies. We have updated the text in these sections to emphasize the limited overall impact, and we hope that the new subsection “Comparison to simple models of RGC light response” further clarifies the interpretation of this analysis. In addition, even though there are more parameters, we did not find improvement using regularization, which argues against overfitting. This is likely because even with the interaction terms, each image pixel has roughly 1000 weights, which are estimated from more than 25,000 samples (these weights are not influenced by the weights for other pixels).</p><disp-quote content-type="editor-comment"><p>6) &quot;Spatiotemporal reconstructions revealed similar spatial visual messages.&quot; A useful but &quot;limited test&quot; of generalization to real vision. One claim is that the spatio-temporal filters had &quot;high space-time separability&quot;. But this leaves on the table 22% of explainable variance in the filter, which is &gt;20 times the effect size in point (5) that got covered at great length. If, in fact, it turned out that space-time separable filters are just fine for reconstruction of videos, that would be an interesting departure from conventional wisdom, where different time course of RF center and surround have figured prominently since the 1960s.</p></disp-quote><p>We agree, this is only a limited test of the filters that would be relevant for natural vision, intended to identify whether there were gross differences between the spatial information present in reconstructions of images vs. movies. The remaining, unexplained variance in the spatiotemporal filters (beyond the separable approximation) is a combination of noise and structure, and the structure could in principle have a significant impact on reconstruction. Furthermore, it could have a biological interpretation, for example the center and surround timing mentioned by the reviewer. We have edited the text to point out that some inseparability would be expected and may be important, and to emphasize that generalization to real vision will require more data and analysis (subsection “Spatial information in a naturalistic movie”, Discussion, sixth paragraph).</p><disp-quote content-type="editor-comment"><p>Detailed suggestions:</p><p>7) To test which properties of retinal encoding contribute to the reconstruction filters, the authors could replace the experimental data with simulated spike trains. They have done this before to great effect using spiking LN or GLM neurons. Is reconstruction performance the same? If not, what are the important differences? Are certain aspects of visual scenes reconstructed better or worse by real cells than model cells?</p></disp-quote><p>We thank the reviewers for this suggestion, which we have incorporated into the manuscript (see the new subsection “Comparison to simple models of RGC light response”). The resulting reconstruction filters and reconstructed images found using simulated spike trains, which were fitted for each recorded RGC and therefore incorporated measured spatial and response properties, were very similar to those found using recorded spike trains. While it is still possible that more sophisticated reconstruction models may utilize additional properties of retinal encoding, the visual message described here is therefore consistent with a simple LN encoding model. We have modified the emphasis at several places in the document to be explicit about this, as suggested.</p><disp-quote content-type="editor-comment"><p>8) Parasol cells are very spatially nonlinear in their responses to natural scenes (Turner and Rieke, 2016), and both ON and OFF parasols are highly motion sensitive (Manookin et al., 2018). So their use for a linear reconstruction of a static scene requires justification. Is it possible that their true role will become more apparent when reconstructing movement within the visual scene (Frechette et al., 2005)?</p></disp-quote><p>While the responses of parasol cells contained enough intensity information to enable linear reconstruction of images from their signals, they likely also contain specific information about motion and fine spatial detail, as described in the papers mentioned here. It is not clear how encoding complex features such as these relates to reconstruction, as it has been suggested that nonlinear encoding in the early visual pathways may enable simpler downstream reconstruction (DiCarlo et al., 2012; Rieke, 1997; Gjorgjieva et al., 2019; Naselaris et al., 2011). However, it is certainly possible that reconstruction of more complex, dynamic natural stimuli would reveal additional stimulus features conveyed by parasol cells (and by other RGCs). Extracting those features would probably require a more complex decoding scheme, potentially using a different loss function, an area we are currently exploring. We have elaborated on this point in the Discussion (sixth paragraph).</p><disp-quote content-type="editor-comment"><p>9) Related to 8: the conclusion that different RGC types &quot;conveyed different and largely independent features of the visual scene&quot; might be inappropriate when comparing midget and parasol cells. Apart from a slight difference at higher spatial frequencies, the reconstructed features of the two cell types actually seem quite similar (Figure 7).</p></disp-quote><p>Yes, as discussed above in point (2), we agree that this is not the whole story when comparing parasol and midget cell classes. We have updated the text to more clearly emphasize this (Abstract, Introduction, Results, Discussion). In addition, see the response to point (11) below for some additional details on the differences in reconstructed features.</p><disp-quote content-type="editor-comment"><p>10) More regarding the interaction between parasol and midget signals: Did adding parasol responses to midget responses aid in reconstruction simply because parasols filled in gaps in the midget mosaics? To test this, perhaps one could artificially fill in the midget mosaics with LN model cells.</p></disp-quote><p>The reconstruction performance was only measured in areas of the image that were covered by receptive fields of all cell types (i.e. including midget cells). This is described in the Materials and methods (subsection “Image region selection”) and illustrated in Figure 13 (also see the response to point (14) below). We believe this addresses the question, however, if we have missed something we could revisit this.</p><disp-quote content-type="editor-comment"><p>11) Quality of midget vs. parasol reconstructions: RGC density is a dominant factor in image reconstruction (Results), and images reconstructed from midgets cover a wider spatial-frequency range than parasols (Figure 7D; Results). However, the reconstruction from denser midget cells is worse than that from sparser parasol cells (Figure 7C; Results). Why?</p></disp-quote><p>While images reconstructed from midget cell responses had clearer edges and finer spatial details, they typically did not capture the overall luminance values across the image as well as parasol cells. The images reconstructed from parasol cell responses had 50% higher SNR. Together, these led to lower correlation scores for the reconstructions from midget cell responses. We have updated the text to reflect this (subsection “Distinct contributions of major cell types”).</p><disp-quote content-type="editor-comment"><p>12) For the analysis that included a nonlinear, logarithmic transformation of responses, was the transformation taken into account when re-computing the weight matrix W. Also, what happened with the logarithm for bins with R=0?</p></disp-quote><p>Yes, the reconstruction weights were recalculated based on the transformed responses. In addition, the log transformation included a +1 term, which we neglected to mention. The text has been updated to include these points (subsection “Nonlinear reconstruction”).</p><disp-quote content-type="editor-comment"><p>13) Statistical tests: For key claims, authors should perform statistical tests to clarify the significance and better interpret the d(rho): e.g., smoothed vs. non-smoothed images, spike-counts vs. latency coding, etc.</p></disp-quote><p>We have included statistical tests (computed via resampling, described in the Materials and methods) for the following analyses in the text. Due to large sample sizes, and therefore high statistical power, many of the differences were significant even when the effect sizes were small.</p><p>– Smoothed vs. non-smoothed images</p><p>– Spike-counts vs. latency coding</p><p>– RF vs. filter coverage</p><p>– STA vs. original reconstructions</p><p>– Reconstruction performance across cell types and classes (ON vs. OFF vs. both, parasol vs. midget vs. both)</p><p>– Impact of noise correlations, nonlinear transformations, and interaction terms</p><disp-quote content-type="editor-comment"><p>14) Figure 13: Unclear how the reconstruction boundary relates to the RF mosaics. 'Parasol+midget' looks most like the RFs for OFF midget alone.</p></disp-quote><p>The combined reconstruction boundary is the intersection of areas for each of the four cell types, to ensure that only areas with complete coverage of all four types were considered when measuring performance. In the example shown, this intersection was primarily limited by the less complete OFF midget cell mosaic. The figure legend (Figure 13) has been updated to clarify this.</p><disp-quote content-type="editor-comment"><p>15) All figures: Scale bars, e.g. in degrees of visual angle, would be useful at least in key places. Also were the stimuli sized for a particular viewing distance?</p></disp-quote><p>Scale bars have been included in terms of the size of the projected visual stimulus on the retina. We used millimeters, which we can precisely measure. However, a rough conversion to visual angle is included in the Materials and methods. The stimuli were not sized for a particular viewing distance, as natural scenes have structure and objects at many scales, so there is no inherently appropriate distance.</p><disp-quote content-type="editor-comment"><p>16) Figure 4C: How many data points?</p></disp-quote><p>Figure 4C includes 24 data points, one for each cell type (ON or OFF parasol) from each of 12 experiments (3 of the original 15 were excluded due to incomplete mosaics). This information has been added to the figure legend, and a description of the selection criteria was added to the Materials and methods.</p><disp-quote content-type="editor-comment"><p>17) Figure 7D: What are the 3 different curves for each set? Also maybe show the 'parasol+midget' result in black?</p></disp-quote><p>The curves are the 3 recordings used in this analysis (selection of the recordings is described in the Materials and methods). Since a smaller number of recordings were used here, the individual curves were shown rather than an average. The legend and labels have been updated to reflect this. The combined parasol and midget reconstructions were also added as suggested.</p></body></sub-article></article>