<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82367</article-id><article-id pub-id-type="doi">10.7554/eLife.82367</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Functional cell types in the mouse superior colliculus</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-290679"><name><surname>Li</surname><given-names>Ya-tang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2763-1534</contrib-id><email>yatangli@cibr.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-3549"><name><surname>Meister</surname><given-names>Markus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2136-6506</contrib-id><email>meister4@mac.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Division of Biology and Biological Engineering, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena, CA</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029819q61</institution-id><institution>Chinese Institute for Brain Research</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Tianqiao and Chrissy Chen Institute for Neuroscience, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena, CA</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>19</day><month>04</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82367</elocation-id><history><date date-type="received" iso-8601-date="2022-08-02"><day>02</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-03-16"><day>16</day><month>03</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-04-05"><day>05</day><month>04</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.04.01.486789"/></event></pub-history><permissions><copyright-statement>© 2023, Li and Meister</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Li and Meister</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82367-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82367-figures-v2.pdf"/><abstract><p>The superior colliculus (SC) represents a major visual processing station in the mammalian brain that receives input from many types of retinal ganglion cells (RGCs). How many parallel channels exist in the SC, and what information does each encode? Here, we recorded from mouse superficial SC neurons under a battery of visual stimuli including those used for classification of RGCs. An unsupervised clustering algorithm identified 24 functional types based on their visual responses. They fall into two groups: one that responds similarly to RGCs and another with more diverse and specialized stimulus selectivity. The second group is dominant at greater depths, consistent with a vertical progression of signal processing in the SC. Cells of the same functional type tend to cluster near each other in anatomical space. Compared to the retina, the visual representation in the SC has lower dimensionality, consistent with a sifting process along the visual pathway.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>vision</kwd><kwd>superior colliculus</kwd><kwd>cell types</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01 NS111477</award-id><principal-award-recipient><name><surname>Meister</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>543015SPI</award-id><principal-award-recipient><name><surname>Meister</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>K99EY028640</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Ya-tang</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005237</institution-id><institution>Helen Hay Whitney Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Li</surname><given-names>Ya-tang</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurons in the mouse superior colliculus comprise about 20 types based on their responses to visual stimuli, and neurons of the same type tend to cluster together.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Parallel processing of information by the brain operates at three different levels: single neurons, cell types, and neural pathways. Processing of visual information at the cell-types level starts at the retina, where the signal is split into ∼15 types of bipolar cells (<xref ref-type="bibr" rid="bib49">Shekhar et al., 2016</xref>), ∼63 types of amacrine cells (<xref ref-type="bibr" rid="bib55">Yan et al., 2020</xref>), and ∼40 types of retinal ganglion cells (RGCs; <xref ref-type="bibr" rid="bib44">Roska and Meister, 2014</xref>; <xref ref-type="bibr" rid="bib47">Sanes and Masland, 2015</xref>; <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>). The RGCs send the processed information directly to the superior colliculus (SC), an evolutionarily conserved structure found in all vertebrates (<xref ref-type="bibr" rid="bib28">Isa et al., 2021</xref>; <xref ref-type="bibr" rid="bib3">Basso and May, 2017</xref>). The SC serves as an important visual center and also plays a vital role in coordinating animal behavior (<xref ref-type="bibr" rid="bib54">Wheatcroft et al., 2022</xref>).</p><p>In the rodent, ∼90% of RGCs project to the superficial layer of the SC (<xref ref-type="bibr" rid="bib14">Ellis et al., 2016</xref>), and each SC neuron receives inputs from about six RGCs (<xref ref-type="bibr" rid="bib7">Chandrasekaran et al., 2007</xref>). It remains unclear how the information is transformed at this stage and how many cell types exist in the SC. As in other brain areas, a solid classification of cell types in this circuit would support a systematic study of its function (<xref ref-type="bibr" rid="bib57">Zeng and Sanes, 2017</xref>). By classic criteria of cell morphology and physiology, prior work has distinguished about five cell types in the retino-recipient superficial layer (<xref ref-type="bibr" rid="bib33">Langer and Lund, 1974</xref>; <xref ref-type="bibr" rid="bib39">May, 2006</xref>; <xref ref-type="bibr" rid="bib17">Gale and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib51">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib9">De Franceschi and Solomon, 2018</xref>). Differential expression of molecular markers has been used to describe about ten types in the superficial SC (<xref ref-type="bibr" rid="bib6">Byun et al., 2016</xref>). By contrast, recent work on the primary visual cortex (V1) identified 46 types of neurons based on morphology and electrophysiology (<xref ref-type="bibr" rid="bib22">Gouwens et al., 2019</xref>). Because the major inputs to the superficial SC are from the retina and V1, we hypothesize that the number of functionally distinct cell types in the SC has been underestimated.</p><p>The present work aims to identify functional cell types in the superior colliculus by virtue of their responses to a large set of visual stimuli. These include a panel of stimuli that successfully separated ∼40 types of retinal ganglion cells, confirming many classes previously known from anatomical and molecular criteria (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>). By two-photon calcium imaging, we recorded neuronal responses from the posterior-medial SC of behaving mice while leaving the cortex intact. We classified cell types based on their response to the high-dimensional visual stimulus using unsupervised learning algorithms. We included several transgenic mouse strains that label subsets of SC neurons based on gene expression patterns. The evidence points to ∼24 functional types that come in two major classes: one closely related to retinal responses, the other distinct. We report on the anatomical organization of these functional types, their relation to molecular cell types, and their progression throughout layers of the superficial SC. By comparing the space of visual features encoded in the SC to that in the retina, one finds that the superior colliculus already discards substantial information from the retinal output.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Single-cell imaging reveals diverse neuronal responses to a set of visual stimuli</title><p>To investigate the functional diversity of SC neurons, we imaged neuronal calcium responses to a battery of visual stimuli using two-photon microscopy in head-fixed awake mice (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To maintain the integrity of the overlying cortex, one is limited to the posterior-medial SC that corresponds to the upper lateral visual field (<xref ref-type="bibr" rid="bib15">Feinberg and Meister, 2015</xref>; <xref ref-type="fig" rid="fig1">Figure 1B and C</xref>). We recorded more than 5000 neurons from 41 image planes in 16 animals from different genetic lines, including wild-type, Vglut2-Cre, Vgat-Cre, Tac1-Cre, Rorb-Cre, and Ntsr1-Cre mice. In the Cre lines, the calcium indicator was restricted to the neurons expressing the respective transgene.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Two-photon imaging reveals diverse visual responses in awake mouse superior colliculus.</title><p>(<bold>A</bold>) Schematic of the experimental setup. Mice were head-fixed and free to run on a circular treadmill. Visual stimuli were presented on a screen. Neuronal calcium activity was imaged using two-photon microscopy. PMT, photomultiplier tube. (<bold>B</bold>) Schematic of mouse brain anatomy after insertion of a triangular transparent plug to reveal the posterior-medial part of the superior colliculus underneath the transverse sinus. TS: transverse sinus. (<bold>C</bold>) A standard deviation projection of calcium responses to visual stimuli in one field of view. (<bold>D</bold>) Response profiles of 12 neurons (rows) marked in C to visual stimuli in the bottom row. Columns 1–6 are time-varying calcium responses to a moving bar, expanding and contracting disks, a ‘chirp’ stimulus with modulation of amplitude and frequency, spots of varying size, and blue and green flashes. Gray shading indicates the standard error across identical trials. Each row is scaled to the maximal response. Scale bars: 2 s. Subsequent columns show processed results: (7) response amplitude to an expanding black disc on 10 consecutive trials. (8) polar graph of response amplitude to moving bar in 12 directions. (9 and 10) Receptive field profiles mapped with small squares flashing On or Off.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig1-v2.tif"/></fig><p>We presented a battery of visual stimuli (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) chosen to probe spatio-temporal integration, color-sensitivity, and movement processing (see Visual stimulation in Materials and methods for detail). Included was a ‘chirp’ stimulus that modulates the intensity on the cell’s receptive-field over both frequency and amplitude; the full-screen ‘chirp’ was previously employed in the classification of retinal ganglion cells (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>). Neurons in any given image plane showed robust and diverse responses to these stimuli (<xref ref-type="fig" rid="fig1">Figure 1C–D</xref>). The animals were positioned on a circular treadmill but remained stationary during most of the visual stimulation. Because locomotion barely modulates the visual responses of SC neurons (<xref ref-type="bibr" rid="bib48">Savier et al., 2019</xref>), we did not consider further the effects of movements.</p></sec><sec id="s2-2"><title>Superficial superior colliculus comprises at least 24 functional cell types</title><p>To classify cell types based on their functional properties, we first performed a sparse principal component analysis on the raw response traces (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>; <xref ref-type="bibr" rid="bib37">Mairal et al., 2009</xref>), which led to a 50-dimensional feature vector for each neuron. Then we added 4 designed features that describe different aspects of the response: a habituation index (HI) computed from repeated stimuli, a direction selectivity index (DSI), an orientation selectivity index (OSI), and a motion selectivity index (MSI, see Materials and methods). We focused on 3414 neurons that responded reliably to visual stimuli (signal-to-noise ratio &gt;0.35, see definition in Materials and methods <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), and searched for clusters in the 54-dimensional feature space by fitting the data (3414 cells × 54 features) with a Gaussian mixture model, varying the number of clusters in the mixture (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Twenty-four functional cell types in the mouse SC.</title><p>(<bold>A</bold>) Dendrogram of 24 clusters based on their distance in feature space. For each type, this shows the average time course of the neural response to the stimulus panel. Grey: standard deviation. The vertical scale is identical for all types and stimulus conditions. Blue dashed line separates groups 1 and 2. Numbers in parentheses indicate percentage of each type in the dataset. Stars mark the unstable clusters with JSC&lt; 0.5 (see panel D). Triangles mark the clusters where more than half neurons are contributed by one mouse. Scale bars: 2 s. (<bold>B</bold>) Relative Bayesian information criterion (ΔBIC) for Gaussian mixture models with different numbers of clusters. (<bold>C</bold>) The center of each cluster in the first two principal axes of feature space. Black and red colors label Groups 1 and 2, respectively. (<bold>D</bold>) Jaccard similarity coefficient (JSC) between the full dataset and subsets (Mean ± SD).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Clustering and validating (related to <xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>(<bold>A</bold>) Temporal features were extracted from responses to five visual stimuli, including chirp, moving bar, expanding black disc and receding white disc, flashed blue or green squares, and flashed black discs with different sizes. Horizontal axis: time in s. (<bold>B</bold>) Feature-coefficients for the 24 cell types. Color bar indicates coefficients of features for each cell type. (<bold>C</bold>) Cell-wise co-association matrix (see Materials and methods). Color bar indicates co-clustering fraction. (<bold>D</bold>) Between-cluster rate, which is the cluster-wise average of the co-association matrix. (<bold>E</bold>) Histogram of median correlation coefficients between the original clusters and clusters identified on 100 subsets. (<bold>F</bold>) Black symbols indicate the Euclidean distance between original clusters and clusters identified on the subsets. Gray symbols indicate the shortest Euclidean distance between the original cluster and other clusters. (<bold>G</bold>) Contributions of different mice to each of the functional types. White color indicates no contribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Dendrogram of 24 clusters showing normalized temporal profiles (related to <xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>Display as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. Blue dashed line separates groups 1 and 2. Numbers in parentheses indicate percentage of each type. Stars mark the unstable clusters with JSC&lt; 0.5 (see <xref ref-type="fig" rid="fig2">Figure 2D</xref>). Triangles mark the clusters where more than half of the neurons are contributed by one mouse.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Example responses of single neurons in each type to visual stimuli (related to <xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>Columns 1–6 are time-varying calcium responses to a moving bar, expanding and contracting disks, a &quot;chirp&quot; stimulus with modulation of amplitude and frequency, spots of varying size, and blue and green flashes. Gray shading indicates the standard error across identical trials. Each row is scaled to the maximal response. Scale bars: 2 s. Subsequent columns show processed results: (7) response amplitude to an expanding black disc on 10 consecutive trials. (8) polar graph of response amplitude to moving bar in 12 directions. (9 and 10) Receptive field profiles mapped with small squares flashing On or Off. Red dashed line separates Group 1 and Group 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig2-figsupp3-v2.tif"/></fig></fig-group><p>The quality of each clustering was assessed with the Bayesian information criterion (BIC), which addresses concerns about overfitting by balancing the goodness of fit with generalizability. By this measure, the distribution of cells in feature space was best described with a Gaussian mixture of 24 components (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), suggesting there are 24 functional types of neurons in the surveyed population. Because the optimum in the BIC curve was rather broad (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and one could make a case for both fewer or more clusters, we followed up by testing the stability of each cluster: We fitted various sub-samples of the data set and assessed how well the resulting clusters correspond to those in the full set, using a number of established statistics (see Materials and methods and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). It emerged that 3 of the 24 clusters are somewhat unstable (<xref ref-type="fig" rid="fig2">Figure 2D</xref>); they are marked as such in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. Overall the stability of the cluster definitions matched or exceeded those in related studies of neuronal cell types (<xref ref-type="bibr" rid="bib22">Gouwens et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>). For the purpose of subsequent analysis we will adopt this division into 24 types as suggested by the BIC.</p><p>The hierarchical relationship between these functional types is illustrated by the dendrogram in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, which is based on the distances in feature space between cluster centers. The first branching of the dendrogram splits the types into two groups (<xref ref-type="fig" rid="fig2">Figure 2A, C</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), which we will call Group 1 (types 1–10) and Group 2 (types 11–24). Group 1 further splits into Group 1a (types 1–6) and 1b (types 7–10).</p><p>Group 1 (types 1–10) is distinct from Group 2 (11-24) in that it responds more strongly to the chirp stimuli (flashes and sinusoidal modulations in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, 0.14 ± 0.05 vs. 0.04 ± 0.04, <italic>p</italic>&lt;0.001, two-sample <italic>t</italic>-test). All types in Group 1 prefer the expanding black disc over the receding white disc. Almost all these types are excited by both On and Off stimuli in the chirp in the receptive field. Also they prefer large spots to small spots, type 2 being a notable exception. Within Group 1, types 1–6 (Group 1a) are distinct from types 7–10 (Group 1b) in their response to sinusoid flicker: Group 1a prefers the low frequencies, whereas Group 1b responds over a wider range (<italic>p</italic>=0.02, Wilcoxon rank-sum test). Type 7 in particular rejects the low flicker frequencies.</p><p>In Group 2, the response to chirp stimuli is generally weak compared to the moving stimuli. These types respond well to small spots, unlike the Group 1 types. Other response properties in Group 2 are more diverse, and some of these types have been noted previously. For example, Types 11 and 19 stand out in that moving stimuli suppress their activity (<xref ref-type="bibr" rid="bib29">Ito et al., 2017</xref>). Several types (11, 15, 17) are suppressed by the sinusoid flicker (<xref ref-type="bibr" rid="bib29">Ito et al., 2017</xref>); 11 and 15 also show rebound after cessation of that stimulus. Type 14 responds strongly to moving stimuli but hardly at all to the entire chirp. Type 18 is remarkably insensitive to any moving stimulus.</p><p>What are the distinguishing features in their visual responses? <xref ref-type="fig" rid="fig3">Figure 3B</xref> distills the responses to the stimulus palette into 15 indices (see Materials and methods) that help to characterize each type (only values significantly different from zero are shown, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref> for all values). For some indices, <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows violin or bar plots for each type. As a rule, almost all the types are sensitive to moving stimuli, like the traveling dark bar and the expanding black disc (RtM in <xref ref-type="fig" rid="fig3">Figure 3B</xref>). For many types, these were the stimuli that elicited the strongest response.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Functional diversity among different types.</title><p>(<bold>A</bold>) Violin plots or histograms of various response indices: motion selectivity index (MSI), direction selectivity index (DSI), orientation selectivity index (OSI), habituation index (HI), looming selectivity index (LSI), contrast selectivity index (CSI), best stimulus size (BSS), surround suppression index (SSI), blue green index (BGI), and receptive field size (RFS). Blue dashed line separates Group 1 and Group 2. (<bold>B</bold>) Normalized selectivity index (SI, normalized for each column) of functional properties represented by different cell types (See Materials and methods). RtM: response to motion; PFSI: peak-final selectivity index; FSI: frequency selectivity index; RaFM: response after frequency modulation; RaAM: response after amplitude modulation. Gray: values that are not significantly different from 0 (<italic>p</italic>≥0.05, one-sample <italic>t</italic>-test). Green dashed line separates Group 1 and Group 2. (<bold>C</bold>) Pearson’s correlation coefficients of the representation between pairs of functional properties. Gray: non-significant correlations (<italic>p</italic>≥0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Functional properties of different cell types (related to <xref ref-type="fig" rid="fig3">Figure 3</xref>).</title><p>(<bold>A</bold>) Normalized selectivity index (normalized for each column) of functional properties represented by different cell types (see Materials and methods). (<bold>B</bold>) Pearson’s correlation coefficients of the representation between pairs of functional properties. (<bold>C</bold>) Percentage of neurons that show clear receptive fields to flash stimuli for each cell type.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig3-figsupp1-v2.tif"/></fig></fig-group><p>Some of these features of the visual response were highly correlated with each other (<italic>p</italic>&lt;0.05), in that they co-varied in the same or opposite directions across types (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). For example, direction selectivity (DSI) and orientation selectivity (OSI) tend to be strong in the same cell type (types 9, 14, 20, 24). Another strong correlation exists between the preferred stimulus size (BSS) and the response during recovery from the frequency and amplitude chirps (RaFM and RaAM).</p></sec><sec id="s2-3"><title>Neurons of the same type cluster in anatomical space</title><p>In the retina, ganglion cells come in ∼40 different types (<xref ref-type="bibr" rid="bib47">Sanes and Masland, 2015</xref>), and they tile the surface in a so-called mosaic arrangement. Neurons of the same type are spaced at regular distances from each other. Neurons of different types are distributed more or less independently (<xref ref-type="bibr" rid="bib45">Roy et al., 2021</xref>); therefore a ganglion cell’s nearest neighbor is almost always of a different type. The presumed purpose of this arrangement is to ensure uniform coverage such that every location in the visual field has access to each of the types of retinal ganglion cell. Because the retina projects directly to the SC, we investigated whether neurons there are also organized for uniform coverage.</p><p><xref ref-type="fig" rid="fig4">Figure 4A</xref> illustrates SC neurons in a single image plane, labeled according to functional type (for more examples see also <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). Several features are immediately apparent. First, cells of a given type do not repel each other; in fact, the nearest neighbor is often a neuron of the same type. Second, the types do not cover space uniformly. Some types are segregated from each other (e.g. 7 and 21) whereas others overlap in space (e.g. 14 and 24).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Spatial organization of functional cell types.</title><p>(<bold>A</bold>) Anatomical locations of four types of neurons from one sample recording. (<bold>B</bold>) Averaged density recovery profile (DRP) of five example types for all imaging planes. Error bars denote SEM. (<bold>C</bold>) Density of neurons of a given functional type at various distances from a neuron of the same type. Red stars mark the smallest radius within which the neuron density is larger than half of the peak density. (<bold>D</bold>) Decay of the DRP for each functional type. Gray bars: the ratio between the density at a distance of 0–0.5 RF diameters and the density at 0.5–1 RF diameters. Magenta bars: Same but for cells from all the other types. Cyan line: The value for W3 retinal ganglion cells (<xref ref-type="bibr" rid="bib58">Zhang et al., 2012</xref>). (<bold>E</bold>) Density of neurons from different functional types (columns) within 50 μm of a given neuron whose type is indicated by the row. Note the largest density is for cells of the same type except types 8, 11, and 16. Gray: insufficient data to estimate density. (<bold>F</bold>) Normalized anatomical distance between neurons in any two functional types. Red stars indicate significant separation (<italic>p</italic>&lt;0.05, bootstrap analysis). White: insufficient data. (<bold>G</bold>) Functional properties of neurons in three ranges of depth. Display as in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. (<bold>H</bold>) The percentage of functional types in each depth range. Red dashed line separates Group 1 and Group 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Anatomical organization of functional types (related to <xref ref-type="fig" rid="fig4">Figure 4</xref>).</title><p>(<bold>A</bold>) Anatomical locations of different types of neurons in two examples of imaging fields. (<bold>B</bold>) Number of types that are significantly separated from each reference type (<italic>p</italic>&lt;0.05, bootstrap analysis). (<bold>C</bold>) Temporal response across depth. Display as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. (<bold>D</bold>) The percentage of functional cell types across depth. Error bars denote SEM.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Receptive field center positions for each functional type (related to <xref ref-type="fig" rid="fig4">Figure 4</xref>).</title><p>Cell types are indicated by the red number at the corner. Color indicates different mice. Axes represent azimuth (horizontal) and elevation (vertical) in degrees. Red cross indicates median azimuth and elevation for all types.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig4-figsupp2-v2.tif"/></fig></fig-group><p>To pursue these spatial arrangements in greater detail, we computed for each functional type the spatial autocorrelation function (also called ‘density recovery profile’): This is the average density of neurons plotted as a function of distance from another neuron of the same type (<xref ref-type="bibr" rid="bib58">Zhang et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Rodieck, 1991</xref>). When applied to retinal ganglion cells, this function shows a pronounced hole of zero density at short distances. Here, instead, the density remains high down to a distance of 10 µm, which is the typical diameter of a soma (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In fact for several of these SC types the density is highest just 1–2 cell diameters away.</p><p>Over larger distances, the autocorrelation function decays gradually (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), whereas one would expect a flat curve if the cells appeared at uniform density. The density drops to half the peak value at a radius of 150–250 µm. This suggests that neurons of a given type form patches of 300–500 µm diameter. Note this accords with a similar patchy organization found previously for certain functional parameters, like the preferred orientation (<xref ref-type="bibr" rid="bib15">Feinberg and Meister, 2015</xref>) or preferred direction of motion (<xref ref-type="bibr" rid="bib10">de Malmazet et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Li et al., 2020</xref>).</p><p>We considered a potential source of error that could give the mistaken appearance of a patchy organization: A functional type might fortuitously be limited to just one recording session, owing to some peculiarity of that animal subject, and so would appear only in the visual field covered during that session. This is not the case: <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> shows that recordings from different mice confirm the same functional type. Furthermore, a single recording session reveals separate patches of different types (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>).</p><p>An important scale by which to judge this spatial organization is the size of the receptive field (RF). In the retina, the mosaic arrangement spaces the ganglion cells about one RF diameter apart, so that the RFs of neurons from the same type have little overlap. In the SC that is clearly not the case. From the autocorrelation functions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), we compared the average density of cells within 0.5 RF diameters to the density at 0.5–1.0 RF diameters. In the SC, that ratio is greater than 1 for all functional types and often close to 2 (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). By comparison, for the W3 type of retinal ganglion cell (<xref ref-type="bibr" rid="bib58">Zhang et al., 2012</xref>) that ratio is only 0.4.</p><p>In the retina, the arrangement of one functional type is almost entirely independent of the other types (<xref ref-type="bibr" rid="bib53">Wässle et al., 1981</xref>; but see <xref ref-type="bibr" rid="bib45">Roy et al., 2021</xref>). In the SC we found a strong anticorrelation: Within 50 µm of a given cell, the cells of the same type occurred at greater-than-average density, but cells of the other types at lower density (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). One might say that each functional type tends to displace the others. To illustrate this further, we calculated the normalized distance between all pairs of cell types (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Twenty-one types show significant spatial separation from at least one other cluster (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>, <italic>p</italic>&lt;0.05, bootstrap analysis).</p><p>The functional properties of neurons also varied along the depth axis of the superior colliculus (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). The individual response parameters changed in only subtle ways; for example one can find more neurons with larger receptive fields at greater depth but on average deeper neurons preferred smaller spot stimuli. However, at the level of identified types (which takes many response parameters into account) the differences were more pronounced. In particular, neurons in the upper 100 μm were composed primarily of types 1–10 (Group 1, 56%) while neurons deeper than 100 μm belonged primarily to types 11–24 (Group 2, 68%, <italic>p</italic>&lt;0.001, chi-square test, <xref ref-type="fig" rid="fig4">Figure 4H</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C–D</xref>).</p></sec><sec id="s2-4"><title>Genetically labeled populations comprise multiple functional types</title><p>Several experiments focused on superior colliculus neurons with specific molecular identity, taking advantage of existing mouse Cre-lines (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The Vgat-Cre and Vglut-Cre lines respectively label GABAergic and glutamatergic neurons; Tac1-Cre labels populations of neurons stratified in the superficial SC (<xref ref-type="bibr" rid="bib23">Harris et al., 2014</xref>), Rorb-Cre labels both excitatory and inhibitory neurons in the superficial sublayer (<xref ref-type="bibr" rid="bib6">Byun et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Gale and Murphy, 2018</xref>); finally Ntsr1-Cre labels a subtype of excitatory neurons that have a wide-field morphology at deeper locations (<xref ref-type="bibr" rid="bib17">Gale and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib18">Gale and Murphy, 2016</xref>; <xref ref-type="bibr" rid="bib19">Gale and Murphy, 2018</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Functional properties in genetically labeled populations.</title><p>(<bold>A</bold>) Average response to the chirp stimulus of five genetically labeled cell types. Display as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. (<bold>B</bold>) Functional properties of the genetically labeled types. Display as in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. (<bold>C</bold>) Example receptive fields of Ntsr1+ neurons mapped with squares flashing Off and fitted with an ellipse. (<bold>D</bold>) The percentage of functional types in mice with different genetic backgrounds. Red dashed line separates Group 1 and Group 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig5-v2.tif"/></fig><p>Some systematic differences between these genetically labeled populations are apparent already from the chirp responses (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). For example, excitatory and inhibitory neurons differ at the population level: Vglut + neurons prefer flashed over moving stimuli and spots of small size, whereas Vgat + neurons respond equally to flashed and moving stimuli and prefer large-size spots (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Tac1 + neurons have the highest direction selectivity and small receptive fields, whereas Ntsr1 + neurons show strong orientation selectivity. We refrain from analyzing the fine-grained distribution of the 24 functional types in each genetic population, because the experiments did not cover every combination of genetic label and depth and visual field location, leading to possible confounds.</p><p>Prior reports on the Ntsr1 + neurons suggest that they are of the distinctive wide-field anatomical type, with a broad dendritic fan (<xref ref-type="bibr" rid="bib38">Major et al., 2000</xref>), and that they have the largest receptive fields in the superficial SC (<xref ref-type="bibr" rid="bib17">Gale and Murphy, 2014</xref>). In fact, we did find very large receptive fields among Ntsr1 + neurons, but surprisingly also many small ones (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Notably another recent study also reported a large variation of receptive field sizes in this line (<xref ref-type="bibr" rid="bib26">Hoy et al., 2019</xref>). Perhaps some of the Ntsr1 +neurons have the wide-field morphology but are dominated by just one or a few dendritic inputs.</p><p>At the level of functional classification, two features stand out (<xref ref-type="fig" rid="fig5">Figure 5D</xref>): First the inhibitory neurons are more represented by types in Group 1 (types 1–10, 58%). Second the Rorb + and Tac1 + neurons comprise mostly the functional types of Group 2 (types 11–24, 77% and 82% respectively). Recall that these types respond poorly to the simple chirp stimulus but strongly to moving and expanding objects (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Beyond that, there is no one-to-one relationship between genetic labels and functional properties: Each of these genetic markers labels neurons with various functions, and vice versa.</p></sec><sec id="s2-5"><title>Transformations from retina to superior colliculus</title><p>The superficial layers of the superior colliculus receive direct input from most types of retinal ganglion cells. How is visual information transformed as the superior colliculus processes these signals further? To address this directly, our experimental design included a precise copy of the stimuli used previously to classify and distinguish functional types of retinal ganglion cells (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>). Here, we compare the known retinal ganglion cell response types to those we identified in superior colliculus.</p><p>As a first-order analysis, we fitted the chirp and color responses of each SC type with a linear combination of the responses of RGC types (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A–D</xref>; <xref ref-type="bibr" rid="bib43">Román Rosón et al., 2019</xref>). Under the null hypothesis where each SC type is dominated by one RGC type, this should yield fit weights only along the diagonal. Instead, the optimal fit mixes contributions from many RGC types. While these coefficients cannot be interpreted as representing synaptic connectivity, they nonetheless indicate a broad mixing of retinal inputs at the level of superior colliculus.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparison with functional RGC types.</title><p>(<bold>A</bold>) Weights of different RGC types on each SC type (see Materials and methods). Green dashed line separates Group 1 and Group 2. (<bold>B</bold>) Plot of mean (triangle) and median (star) OSI versus DSI for 32 functional RGC types. <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>. Color codes functional type. (<bold>C</bold>) Plot of mean (triangle) and median (star) OSI versus DSI for 24 functional types in the SC. Color codes functional type. (<bold>D</bold>) PCA of the visual responses of SC and RGC types, plotting the fractional explained variance against the number of components. Inset enlarges the part in the dashed rectangle. The uncertainty in these values was &lt;0.01 (SD) in all cases, as estimated from a bootstrap analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Comparisons between SC neurons and RGCs (related to <xref ref-type="fig" rid="fig6">Figure 6</xref>).</title><p>(<bold>A</bold>) Visual responses of RGC types (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>) and SC types. (<bold>B</bold>) Prediction error from RGC types to SC types (<xref ref-type="disp-formula" rid="equ28">Equation 28</xref>). (<bold>C</bold>) Histogram of the weights of RGC types to SC types in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. (<bold>D</bold>) If one only considers weights <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, this histograms the number of RGC types contributing to one SC type, and vice versa the number of SC types with contributions from one RGC type. (<bold>E</bold>) Plot of OSI vs DSI among RGCs, as reported in <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>. (<bold>F</bold>) Plot of OSI vs DSI for SC neurons, calculated from the present study by the same SVD algorithm used in <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>. (<bold>G</bold>) Plot of OSI vs DSI for SC neurons, using the simpler definition from the present study. (<bold>H</bold>) Plot of DSI for SC neurons, computed by the SVD algorithm (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>) versus the present definition. Note the close correspondence. (<bold>I</bold>) As panel (<bold>H</bold>) for OSI. This analysis shows that the comparison between the retina results in <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref> and SC results in the present study does not suffer from different analysis methods. (<bold>J</bold>) Correlation coefficients between OSI and DSI for each functional type.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82367-fig6-figsupp1-v2.tif"/></fig></fig-group><p>A number of interesting features emerge. First, some RGC types have universally excitatory (types 16 (ON DS trans.) and 23 (ON ‘mini’ alpha)) or universally inhibitory (types 7 (OFF sustained) and 18 (ON transient)) effects on almost all the SC responses. The inhibitory effects could be implemented by recruiting local inhibitory neurons in SC. These RGC types represent a diversity of functional properties. Second, 18 of 32 RGC types have significantly larger contribution to Group 1 (types 1–10) than Group 2 (11–24, <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, two-sample <italic>t</italic>-test). Recall that these groups represent the major division in the dendrogram of SC types (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>Another substantial transformation from retina to colliculus occurs in the representation of orientation and direction for moving stimuli. In the retina, different ganglion cell types are used to encode the direction vs the orientation of bar stimuli (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1E</xref>). By comparison, in the colliculus it appears that the same cells are selective for orientation and for direction (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1F–I</xref>). Similarly, the retina contains many ganglion cell types with purely On-type or Off-type responses (<xref ref-type="bibr" rid="bib47">Sanes and Masland, 2015</xref>; <xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>). By comparison, at the level of the colliculus those two pathways are largely combined, and most functional types are of the On-Off type with a bias towards Off responses (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). These examples suggest a reduction in diversity during transformation from retina to colliculus. To test this more globally, we subjected the entire set of chirp-color responses to a principal component analysis, and compared the results from RGC types to those from SC types (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). In the superior colliculus, the variance in the visual responses can be explained by fewer principal components than in the retina. This suggests a reduction in diversity from retina to colliculus, as might be expected from selective visual filtering of features essential to guide the animal’s behavior.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Parallel pathways are central to the architecture of biological vision. The visual pathway forks already at the photoreceptor synapse – into ON and OFF representations – and by the time the signal emerges from the eye it has been split into about 40 channels. Each of these pathways is carried by a type of retinal ganglion cell, tuned to certain spatio-temporal features of the visual input, and its neural population covers the entire visual field. How these parallel signals are combined and elaborated in subsequent neural stages of the visual system to sustain the behavioral needs of the animal remains a fundamental question for vision science. Here, we sought to follow these parallel pathways into the superior colliculus, the most important retinal projection target in rodents, which receives input form ∼90% of the retinal ganglion cells. The approach was to survey visual response properties across many neurons in the superficial layers of the superior colliculus, using precisely those stimuli that had been instrumental in defining the parallel pathways at the retinal output.</p><sec id="s3-1"><title>Main findings</title><p>Based on their responses to a broad set of visual stimuli, neurons in the upper SC fall into 24 functional types (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). At a coarse level, the types form two groups, depending on whether they respond well to simple chirp stimuli (Group 1) or not (Group 2) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Cells in Group 1 dominate the most superficial regions (&lt;100 µm depth), those in Group 2 the deeper levels (<xref ref-type="fig" rid="fig4">Figure 4H</xref>). Unlike in the retina – where cells of the same type are spaced laterally at a regular distance – in the SC cells of the same type seem to attract each other. In fact many types appear in clusters, about 300–500 µm in size (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Putative excitatory and inhibitory neurons differ in functional properties: On average the inhibitory neurons have a greater preference for large stimuli and for stimulus motion (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Compared to retinal ganglion cells, none of the SC types match an RGC type exactly; instead different retinal sensitivities get combined at the level of SC (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Overall, the visual representation in the superficial SC is somewhat less diverse than in the retina (<xref ref-type="fig" rid="fig6">Figure 6D</xref>).</p></sec><sec id="s3-2"><title>Relation to prior work</title><p>Although the superior colliculus receives many parallel channels of input from the retina and the visual cortex, prior work classified only 4–5 types of neurons in the SC (<xref ref-type="bibr" rid="bib51">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Gale and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib9">De Franceschi and Solomon, 2018</xref>). The greater diversity of response types in the present study results in part from a different conceptual approach: Instead of defining cell types based on a combination of morphological, electrophysiological, and functional criteria (<xref ref-type="bibr" rid="bib51">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Gale and Murphy, 2014</xref>), we focused on visual response properties. This method acknowledges that neurons with the same shape do not necessarily perform the same function in the circuit. Indeed prior work had found that even among Ntsr1 + neurons that have distinctive wide-field shape there is a great range of response properties (<xref ref-type="bibr" rid="bib26">Hoy et al., 2019</xref>).</p><p>Another supporting factor is the much greater number of neurons in the present survey, which allows finer distinctions and statistical assessment of robustness of the functional cell typing. Further, much of the prior work was conducted under anesthesia. This has adverse effects on visual selectivity in the superior colliculus, as pointed out by <xref ref-type="bibr" rid="bib9">De Franceschi and Solomon, 2018</xref>, and thus lowers the number of distinguishable cell types.</p><p>At the same time, some of the 24 functional types described here are also evident in prior studies: Types 11 and 15 that show rebounded responses after the frequency-modulated flash are similar to the suppressed-by-contrast cell type (<xref ref-type="bibr" rid="bib29">Ito et al., 2017</xref>). Types 9 and 14 show strong orientation and direction selectivity, as reported in prior work (<xref ref-type="bibr" rid="bib27">Inayat et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">De Franceschi and Solomon, 2018</xref>).</p><p>The notion of dividing SC types into two groups is supported by a recent report (<xref ref-type="bibr" rid="bib50">Sibille et al., 2022</xref>). From simultaneous recordings of RGC axons and SC neurons, this study showed that one group of SC neurons receives synaptic inputs from functionally homogeneous RGCs while another group combines more functionally diverse inputs. Clearly, this is a powerful way of assessing direct retino-geniculate synapses. By contrast, our analysis includes indirect effects through interneuron circuitry, and suggests that most SC neurons end up combining signals from multiple retinal channels (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p></sec><sec id="s3-3"><title>Implications for visual processing</title><p>Regarding the fate of parallel pathways emerging from the retina, one can contemplate two opposing hypotheses: (1) Further divergence of pathways. In this picture downstream circuits split the visual representation further, yielding more functional types of neurons. Each type responds sparsely only when its favorite feature occurs. (2) Convergence of parallel pathways. In this version downstream circuits begin to narrow the visual representation, computing a few variables from the scene that are useful for controlling behavior, while discarding those bits of information that aren’t needed. The present observations favor the second hypothesis, for the following reasons:</p><p>First, there are fewer recognizable types in this survey of SC responses (24) than there are in the retinal ganglion cell layer (40) under the same tests. Some comments are in order regarding this comparison. In the retina, the number of RGC types is supported by a convergence of functional, molecular, and anatomical criteria (<xref ref-type="bibr" rid="bib47">Sanes and Masland, 2015</xref>), leaving little ambiguity. By comparison, the proposed number of 24 functional types in SC is open to debate. The statistical criterion we used (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) is not the only option, and one can envision a coarser split into fewer types. Furthermore, the cell types reported here, as well as those in previous studies (<xref ref-type="bibr" rid="bib51">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Gale and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib9">De Franceschi and Solomon, 2018</xref>), may well lie at different levels of the synaptic network, whereas the RGCs all lie in the same network layer of parallel representation. All these biases point in the same direction: There are very likely fewer SC types than RGC types. Second, certain features of the visual stimulus that were separated into different retinal cell types appear combined within the superior colliculus. Notably this is the case for the representation of orientation and motion direction (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) and for that of On- and Off-signals (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Third, a global measure of dimensionality in the neural representation is smaller among SC types than RGC types (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Note that ∼10% of RGCs avoid the SC, including SPIG1 + cells in the pan-ventronasal retina (<xref ref-type="bibr" rid="bib56">Yonehara et al., 2008</xref>), which could contribute to the reduction of dimensionality.</p><p>All this suggests that the functional diversity in the visual pathway may be greatest at the retinal output. Subsequent circuits may act primarily as a switchboard to distribute these RGC signals to different brain targets. For example, a recent study traced the projections from SC to two different target areas, and found that the corresponding SC cells combine inputs from different subsets of retinal ganglion cells (<xref ref-type="bibr" rid="bib41">Reinhard et al., 2019</xref>). It will be interesting to explore how these projection-defined types correspond to the functional types reported here and their putative RGC complements (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><p>Another output target from the superficial SC are the deeper layers, where visual information is combined with other sensory pathways as well as motor signals. These are out of reach for effective optical recording but can be accessed by electrodes. There one encounters response properties not seen in the retina, foremost among them a pronounced habituation to repeated stimuli (<xref ref-type="bibr" rid="bib12">Dräger and Hubel, 1975</xref>; <xref ref-type="bibr" rid="bib25">Horn and Hill, 1966</xref>; <xref ref-type="bibr" rid="bib34">Lee et al., 2020</xref>). Also, the range of response properties narrows systematically with depth, reflecting an increased selectivity for behaviorally-relevant visual features (<xref ref-type="bibr" rid="bib34">Lee et al., 2020</xref>). Similarly, it is possible that the functional types varies across regions of the SC. Stimulating the region that represents the upper or low visual field elicits avoidance or orientating behavior respectively (<xref ref-type="bibr" rid="bib46">Sahibzada et al., 1986</xref>), suggesting that other functional types relevant to orienting behaviors likely exist in regions outside the posterior medial sector studied here.</p></sec><sec id="s3-4"><title>The spatial organization of functional cell types</title><p>The patchy organization of functional cell types (<xref ref-type="fig" rid="fig4">Figure 4</xref>) extends the results reported previously regarding specific neuronal response properties, notably the preferred orientation (<xref ref-type="bibr" rid="bib15">Feinberg and Meister, 2015</xref>; <xref ref-type="bibr" rid="bib1">Ahmadlou and Heimel, 2015</xref>; <xref ref-type="bibr" rid="bib50">Sibille et al., 2022</xref>) or movement direction (<xref ref-type="bibr" rid="bib35">Li et al., 2020</xref>; <xref ref-type="bibr" rid="bib10">de Malmazet et al., 2018</xref>). A common theme to all these reports is that the rules of visual processing seem to vary across broad regions of the visual field (for a dissenting report see <xref ref-type="bibr" rid="bib8">Chen et al., 2021</xref>). In the present study we found that different regions of the visual field, measuring ∼20–40 degrees across, are covered by a different complement of functional types. Our recordings were limited to the posterior-medial portion of the SC (upper temporal visual field), and thus the global organization of these functional types remains unclear. Also, it is possible that additional functional types will emerge in other parts of the visual field.</p><p>It is important to contrast this organization with inhomogeneities found elsewhere in the visual system. Among retinal ganglion cells (RGCs), the mosaic arrangement of neurons within a type guarantees that they are properly spaced, such that each point in the visual field is handled by at least one such ganglion cell. Within a given RGC type, the response properties may vary gradually across the visual field, typically in a naso-temporal or dorso-ventral gradient (<xref ref-type="bibr" rid="bib5">Bleckert et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Warwick et al., 2018</xref>). This is very different from the regional specializations encountered in the SC. In the primary visual cortex of large animals, one often finds that functional types are organized in patches or stripes. However, the scale of this functional anatomy is finer than the receptive field size: This guarantees again that each point in the visual field is handled by neurons of every type (<xref ref-type="bibr" rid="bib4">Blasdel and Campbell, 2001</xref>). By contrast, in the SC the observed patches are 10–20 times larger than receptive fields, which implies a regional specialization of certain visual processes. The reasons for this specialization remain unclear. One can certainly invoke ecological arguments for treating the upper and lower visual fields differently, but on the scale of 30 degrees the purpose is less obvious.</p><p>Future work may also inspect the downstream effects of this patchy organization. One hypothesis is that a given functional type gathers visual information for distribution to one of the many downstream visual centers. If so, then a retrograde tracing from one of the SC’s target regions should cover only a patch of the visual field. The rapid progress in connectomic and transcriptomic methods for mapping cell types promises further insights into this unusual functional organization.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal</title><p>Laboratory mice of both sexes were used at age 2–4 months. The strains were C57BL/6J (wild-type), Vglut2-ires-Cre (B6J.129S6(FVB)-<italic>Slc17a6<sup>tm2(cre)Lowl</sup></italic>/MwarJ, JAX: 028863), Vgat-ires-Cre (B6J.129S6(FVB)-<italic>Slc32a1<sup>tm2(cre)Lowl</sup></italic>/MwarJ, JAX: 028862), Tac1-IRES2-Cre-D (B6;129S-<italic>Tac1<sup>tm1.1(cre)Hze</sup></italic>/J, JAX: 021877) (<xref ref-type="bibr" rid="bib23">Harris et al., 2014</xref>), Rorb-IRES2-Cre-D (B6;129S-<italic>Rorb<sup>tm1.1(cre)Hze</sup></italic>/J, JAX: 023526) (<xref ref-type="bibr" rid="bib23">Harris et al., 2014</xref>), and Ntsr1–GN209–Cre (Genset: 030780-UCD) (<xref ref-type="bibr" rid="bib20">Gerfen et al., 2013</xref>). Vglut2-ires-Cre and Vgat-ires-Cre mice express Cre recombinase in <italic>Vglut2</italic>-expressing and <italic>Vgat</italic>-expressing neurons, respectively. Tac1-IRES2-Cre-D and Rorb-IRES2-Cre-D mice express Cre recombinase in <italic>Tac1</italic>-expressing and <italic>Rorb</italic>-expressing neurons, respectively. Cre-D indicates neo/hydro is deleted. Ntsr1–GN209–Cre mice express Cre recombinase in <italic>Ntsr1-GN209</italic>-expressing neurons. GN209 is the founder line. All animal procedures were performed according to relevant guidelines and approved by the Caltech IACUC.</p></sec><sec id="s4-2"><title>Viral injection</title><p>We injected adeno-associated virus (AAV) expressing non-floxed GCaMP6 (AAV2/1.hSyn1.GCaMP6f.WPRE.SV40) into the SC of wild-type mice (C57BL/6 J), and AAV expressing floxed GCaMP6 (AAV1.Syn.Flex.GCaMP6f.WPRE.SV40) into the SC of Vglut2-ires-Cre, Vgat-ires-Cre, Tac1-IRES2-Cre-D, Rorb-IRES2-Cre-D, and Ntsr1–GN209–Cre mice. After 2–3 weeks, we implanted a cranial window coupled to a transparent silicone plug that rested on the surface of the SC and exposed its posterior-medial portion. This portion of the SC corresponds to the upper-temporal part of the visual field. The optics remained clear for several months, which enabled long-term monitoring of the same neurons. Two-photon microscopy was used to image calcium signals in the SC of head-fixed awake mice 3 weeks to 2 months after viral injection.</p></sec><sec id="s4-3"><title>In vivo two-photon calcium imaging</title><p>For imaging experiments, the animal was fitted with a head bar, and head-fixed while resting on a rotating treadmill. The animal was awake and free to move on the treadmill, but not engaged in any conditioned behavior. Two-photon imaging was performed on a custom-built microscope with a 16×, 0.8 NA, 3 mm WD objective (Nikon). A Ti:Sapphire laser with mode-locking technique (Spectra-Physics Mai Tai HP DeepSee) was scanned by galvanometers (Cambridge). GCaMP6f was excited at 920 nm and laser power at the sample plane was typically 20 − 80 mW. A 600 μm × 600 μm field of view was scanned at 4.8 Hz as a series of 250 pixel × 250 pixel images and the imaging depth was up to 350 μm. Emitted light was collected with a T600/200dcrb dichroic (Chroma), passed through a HQ575/250 m-2p bandpass filter (Chroma), and detected by a photomultiplier tube (R3896, Hamamatsu). Artifacts of the strobed stimulus (see below) were eliminated by discarding 8 pixels on either end of each line. The animal’s locomotion on the treadmill and its pupil positions were recorded and synchronized to the image acquisition. The head-fixed animal performs only rare eye movements and locomotion (<xref ref-type="bibr" rid="bib35">Li et al., 2020</xref>).</p></sec><sec id="s4-4"><title>Visual stimulation</title><p>An LCD screen with LED backlight was placed 18 cm away from the mouse’s right eye. The center of the monitor was at 95° azimuth and 25° elevation in the laboratory frame, and the monitor covered a visual field of 106° × 79°. The visual angle that covers the receptive fields of recorded neurons is 60°–140° azimuth and 0°–50° elevation (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The monitor’s LED illuminator was strobed for 12 µs at the end of each laser scan line to minimize interference of the stimulus with fluorescence detection. The monitor was gamma-corrected. For measuring the functional properties, we presented six types of visual stimuli. (1) A full-field moving black bar (5° width at 50°/s) in 12 directions to measure the orientation selectivity and direction selectivity. The sequence of directions was pseudo-randomized. (2) An expanding black disc (diameter 2° to 60° at a speed of 60°/s, stationary at 60° for 0.25 s, followed by grey background for 2 s) and a receding white disc (60° to 2° at a speed of 60°/s, other parameters same as expanding disc) to measure looming-related responses. (3) Sparse (one at a time) 5° × 5° flashing squares (11×11 squares, 1 s black or white +1 s grey) to map the receptive field (RF); (4) A 10° × 10° square modulated by a &quot;chirp&quot; in frequency or amplitude (3 s black +3 s white +3 s black +3 s grey +8 s frequency modulation (2<sup>−1:3</sup> Hz)+3 s grey +8 s amplitude modulation (0 : 1)+3 s grey +3 s black) centered on the RF to measure temporal properties (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>); (5) A 10° × 10° square flashing blue or green (1 s black +3 s blue +4 s black +3 s green +3 s black) centered on the RF to measure the color preference; (6) A flashing disc (2 s black +2 s grey) with different size (2°, 4°, 8°, 16°, 32°) centered on the RF to measure the size tuning. All stimuli were displayed for 10 repetitions. Stimuli of types 4, 5, and 6 were also repeated identically at locations in a 3×3 array shifted by 10°. This effectively covered the visual field recorded during a given imaging session. For each neuron, we based the analysis on the stimulus closest to its receptive field center.</p></sec><sec id="s4-5"><title>Analysis of calcium responses</title><sec id="s4-5-1"><title>Measurement of calcium responses</title><p>Brain motion during imaging was corrected using SIMA (<xref ref-type="bibr" rid="bib30">Kaifosh et al., 2014</xref>) or NoRMCorre (<xref ref-type="bibr" rid="bib40">Pnevmatikakis and Giovannucci, 2017</xref>). Regions of interest (ROIs) were drawn manually using Cell Magic Wand Tool (ImageJ) and fitted with an ellipse in MATLAB. Fluorescence traces of each ROI were extracted after estimating and removing contamination from surrounding neuropil signals as described previously (<xref ref-type="bibr" rid="bib15">Feinberg and Meister, 2015</xref>; <xref ref-type="bibr" rid="bib35">Li et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Göbel and Helmchen, 2007</xref>; <xref ref-type="bibr" rid="bib32">Kerlin et al., 2010</xref>). The true fluorescence signal of a neuron is <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>true</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>raw</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>neuropil</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> , where <inline-formula><mml:math id="inf4"><mml:mi>r</mml:mi></mml:math></inline-formula> is the out-of-focus neuropil contamination factor and the estimated value for our setup is ∼ 0.7. Slow baseline fluctuations were removed by subtracting the eighth percentile value from a 15 s window centered on each frame (<xref ref-type="bibr" rid="bib11">Dombeck et al., 2007</xref>).</p><p>For any given stimulus, the response of a neuron was defined by the fluorescence trace in its ROI during the stimulus period:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf5"><mml:mi>F</mml:mi></mml:math></inline-formula> is the instantaneous fluorescence intensity and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean fluorescence intensity without visual stimulation (grey screen).</p><p>Two criteria were applied to interpret ROIs as neurons: (1) The size of the ROI was limited to 10–20 μm to match the size of a neuron; (2) The response from the ROI had to pass a signal-to-noise ratio (SNR) of 0.35 (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mi>C</mml:mi><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf7"><mml:mi>C</mml:mi></mml:math></inline-formula> is the <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> (time samples) <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (stimulus repetitions) response matrix, <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are the means over repetitions or time respectively, and <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the corresponding variances. All ROIs meeting these criteria were selected for further analysis, yielding a total of 3414 neurons, including 490 neurons from four wild type mice, 337 neurons from one Vglut2-ires-Cre mouse, 1085 neurons from three Vgat-ires-Cre mice, 720 neurons from three Tac1-IRES2-Cre-D mice, 485 neurons from four Rorb-IRES2-Cre-D mice, and 297 neurons from one Ntsr1–GN209–Cre mouse.</p></sec><sec id="s4-5-2"><title>Quantification of functional properties</title><p>The functional properties introduced in <xref ref-type="fig" rid="fig3">Figure 3</xref> are defined as follows:</p><p>The response to motion (RtM) is the response value during moving-bar stimuli with the largest absolute value. For neurons suppressed by motion this will be negative.</p><p>To quantify the tuning of a neuron to motion directions, we calculated the direction selectivity index (DSI) as the normalized amplitude of the response-weighted vector sum of all directions:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf17"><mml:msup><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> direction in radians and <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the peak response at that direction.</p><p>To quantify the orientation tuning, we calculated the orientation selectivity index (OSI) as the normalized amplitude of the response-weighted vector sum of all orientations:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>O</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf20"><mml:msup><mml:mi>k</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> orientation in radians and <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the peak response at that orientation.</p><p>To quantify the habituation to the expanding black disc, we calculated the habituation index (HI):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>H</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the peak response to the first and the tenth looming stimulus respectively.</p><p>To quantify the selectivity to the expanding black disc over the receding white disc, we calculated the looming selectivity index (LSI):<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the black expanding disc and <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the white receding disc.</p><p>To quantify the selectivity to moving stimuli over the flashing stimuli, we calculated the motion selectivity index (MSI):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the moving bar at the preferred direction and <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the flashing chirp stimulus.</p><p>To quantify the selectivity to On/Off contrast, we calculated the contrast selectivity index (CSI):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>R</mml:mi><mml:mi>On</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the flashing white square and <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>R</mml:mi><mml:mi>Off</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the flashing black square.</p><p>To quantify whether neurons show transient or sustained responses to flash stimuli, we calculated the peak-final selectivity index (PFSI):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>R</mml:mi><mml:mi>peak</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the flashing white/black square that elicited larger responses, and <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>R</mml:mi><mml:mi>final</mml:mi></mml:msub></mml:math></inline-formula> is the final response to that stimulus.</p><p>The quantify the selectivity to the flash frequency, we calculated the frequency selectivity index (FSI):<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>R</mml:mi><mml:mi>low</mml:mi></mml:msub></mml:math></inline-formula> is the peak response in the first 3 s to the flashing frequency modulation, and <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>R</mml:mi><mml:mi>high</mml:mi></mml:msub></mml:math></inline-formula> is the peak response in the last 2 s to the frequency modulation.</p><p>We measured the response after frequency modulation (RaFM) as the difference between the response amplitude at 1.6 s after the stop of the frequency modulation and the baseline. Similarly, the response after amplitude modulation (RaAM) was measured as the difference between the response amplitude at 1.6 s after the stop of the amplitude modulation and the baseline.</p><p>The best stimulation size (BSS) was defined the size of flashing black disc that elicited the largest responses.</p><p>To quantify the surround suppression, we calculated the surround suppression index (SSI):<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>R</mml:mi><mml:mi>small</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the flashing black disc with a diameter of 2 degrees, and <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>R</mml:mi><mml:mi>large</mml:mi></mml:msub></mml:math></inline-formula> is the peak response to the flashing black disc with a diameter of 32 degrees.</p><p>To quantify the color preference, we calculated the blue-green index (BGI):<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mi>B</mml:mi><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:math></inline-formula> is the response to the flashing blue stimulus and <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>R</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:msub></mml:math></inline-formula> is the response to the flashing green stimulus.</p><p>To quantify the receptive field size (RFS), the calcium responses at 11×11 locations were fitted with a 2-D Gaussian function (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>):<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The RF size is defined as the area at the tenth of maximum, which equals <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mn>10</mml:mn><mml:mo>⋅</mml:mo><mml:mi>B</mml:mi><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We omitted analysis of the RF if the coefficient of determination for this fit was below 0.5 (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1I</xref>). The RF size of neurons with the coefficient of determination larger than 0.5 is shown in <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref>.</p></sec><sec id="s4-5-3"><title>Construction of the feature matrix</title><p>We reduced the dimensionality of the calcium response traces, by approximating them with a weighted sum of features, while requiring that the weight coefficients be sparse. For this analysis we included the neuronal responses to moving bars (MB), expanding black and receding white disc (EBD and RWD), chirp, color, flashed black discs with different sizes (FDDS). Neuronal responses to MB, EBD, and RWD were aligned to the peak or the trough to remove the effect of RF position. We focused on neurons which responded robustly (SNR &gt;0.35) to at least one stimulus. For each neuron and each stimulus, the response was normalized to [0,1]. The optimal features were extracted with sparse principal components analysis (spca; <xref ref-type="bibr" rid="bib37">Mairal et al., 2009</xref>), as implemented in the scikit-learn package (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>):<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>neuron</mml:mtext></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>time</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>) is the data matrix denoting neuronal responses to a visual stimulus, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>feature</mml:mtext></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>time</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>) is the matrix with the time course corresponding to each feature, and <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>neuron</mml:mtext></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>feature</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>) is the matrix of weight coefficients. <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is regularized so that only sparse values in each row are non-zero, and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We extracted 6 features from the responses to MB at the preferred direction, 6 features from the responses to EBD and RWD, 20 features from the chirp, 8 features from color stimuli, and 10 features from FDDS (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). These extracted features were combined with HI, DSI, OSI, and MSI to make the feature matrix. Each feature was normalized so that the mean is 0 and the standard deviation is 1.</p></sec><sec id="s4-5-4"><title>Clustering of the feature matrix</title><p>We used a Gaussian mixture model (GMM) to fit the distribution of neurons in the space of features:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability density of the feature vector <inline-formula><mml:math id="inf48"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf49"><mml:mi>K</mml:mi></mml:math></inline-formula> is the number of component Gaussian functions, and <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the weight for <inline-formula><mml:math id="inf51"><mml:msup><mml:mi>i</mml:mi><mml:mi>th</mml:mi></mml:msup></mml:math></inline-formula> Gaussian function <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the feature space. We optimized the parameters using the EM algorithm (sklearn.mixture.GaussianMixture in the package scikit-learn). We varied the number of components from 2 to 50 and evaluated the quality with the Bayesian information criterion (BIC) (<xref ref-type="bibr" rid="bib31">Kass and Raftery, 1995</xref>):<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, is the maximized likelihood of model <inline-formula><mml:math id="inf54"><mml:mi>M</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mi>x</mml:mi></mml:math></inline-formula> is the observed data, <inline-formula><mml:math id="inf56"><mml:mi>θ</mml:mi></mml:math></inline-formula> are the parameters that maximize the likelihood, <inline-formula><mml:math id="inf57"><mml:mi>k</mml:mi></mml:math></inline-formula> is the number of parameters in the model, <inline-formula><mml:math id="inf58"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of neurons. For each putative number of components (2–50), we performed the EM fit starting from 1000 random initial states, and chose the fit with the smallest BIC. This minimal BIC is plotted against the number of components in <xref ref-type="fig" rid="fig2">Figure 2B</xref>.</p><p>To evaluate the stability of clusters, we applied sub-sampling analysis (<xref ref-type="bibr" rid="bib24">Hennig, 2007</xref>). We randomly sub-sampled 90% of the dataset 1000 times and fitted the subset with a GMM using the best cluster number determined from the full dataset. For each original cluster, we calculated its Jaccard similarity coefficient (JSC) with the subsets:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of subsets, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the cluster in the full dataset, and <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the th cluster in one subset. Clusters with JSC below 0.5 were considered unstable. The unstable cluster was merged with the cluster that had the highest between-cluster rate if that rate was &gt; 35% (<xref ref-type="bibr" rid="bib22">Gouwens et al., 2019</xref>); otherwise, it was marked as unstable in the figure.</p><p>To assess the robustness of classification by the EM algorithm, we measured the probability that a pair of cells is classified into the same cluster in different subsets, and calculated the co-association matrix (<xref ref-type="bibr" rid="bib16">Fred and Jain, 2005</xref>):<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of times that the pair <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is assigned to the same cluster in <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> attempts. The between-cluster rate is defined as the cluster-wise average of the co-association matrix.</p><p>To plot the dendrogram, we applied a linkage algorithm (scipy.cluster.hierarchy.linkage) to the means of different clusters in the feature space. We measured the Euclidean distance between two points and defined the distance between two clusters with Ward’s minimum variance method.</p></sec><sec id="s4-5-5"><title>Relative selectivity index</title><p>Relative selectivity index (RSI) is defined as the difference of functional property between one type and a reference number:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is functional property <inline-formula><mml:math id="inf66"><mml:mi>i</mml:mi></mml:math></inline-formula> of type <inline-formula><mml:math id="inf67"><mml:mi>j</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the reference of functional property <inline-formula><mml:math id="inf69"><mml:mi>i</mml:mi></mml:math></inline-formula>. The functional properties are response to motion (RtM), motion selectivity index (MSI), direction selectivity index (DSI), orientation selectivity index (OSI), habituation index (HI), looming selectivity index (LSI), contrast selectivity index (CSI), peak-final selectivity index (PFSI), response after frequency modulation (RaFM), response after amplitude modulation (RaAM), best stimulation size (BSS), surround suppression index (SSI), blue-green selectivity index (BGSI), receptive field size (RFS). The reference numbers are RtM: 0, DSI: 0.15, OSI: 0.15, HI: 0, LSI: 0, MSI: 0, CSI: 0, PFSI: 0.5, FSI: 0, RaFM: 0, RaAM: 0, BSS: 2<sup>3</sup>, SSI: 0, BGI: 0, RFS: 10<sup>2.46</sup>.</p></sec><sec id="s4-5-6"><title>Analysis of the anatomical arrangement of functional cell types</title><p>For the results on anatomical arrangement (<xref ref-type="fig" rid="fig4">Figure 4</xref>), only recording sessions with &gt;5 neurons in a field of view were included. The density recovery profile (DRP) plots the probability per unit area of finding a cell as a function of distance from a cell of the same type (<xref ref-type="bibr" rid="bib42">Rodieck, 1991</xref>). We first defined the region of interest (ROI) as the convex hull of all neurons in an image. Within this ROI, we measured the distances from each reference cell to all of the other cells and histogrammed those, which yields<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mtext>average number of cells at radii between </mml:mtext><mml:mi>r</mml:mi><mml:mtext> and </mml:mtext><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then we measured the average area <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> at distance between <inline-formula><mml:math id="inf71"><mml:mi>r</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from any reference point in the window.</p><p>Finally the DRP was calculated as<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The density ratio (DR) for each type is calculated as<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean density of functional type <inline-formula><mml:math id="inf74"><mml:mi>i</mml:mi></mml:math></inline-formula> within <inline-formula><mml:math id="inf75"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> of a cell of that type, <inline-formula><mml:math id="inf76"><mml:mi>R</mml:mi></mml:math></inline-formula> is the average receptive field diameter of that type, and <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean density in the annulus spanning <inline-formula><mml:math id="inf78"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. To connect anatomical distance in the SC with angular distance in the visual field we assumed that 1 mm corresponds to 88 degrees (<xref ref-type="bibr" rid="bib13">Dräger and Hubel, 1976</xref>).</p><p>To quantify how the functional type of one neuron is related to the functional types of its neighbors, we calculated the density of different types:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is number of neurons of functional type <inline-formula><mml:math id="inf80"><mml:mi>j</mml:mi></mml:math></inline-formula> within a certain distance to a neuron of type <inline-formula><mml:math id="inf81"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the number of neurons of functional type <inline-formula><mml:math id="inf83"><mml:mi>j</mml:mi></mml:math></inline-formula> in the same area if neurons were uniformly distributed.</p><p>To quantify the relationship between two types of neurons, we calculated their normalized distance (ND) for each image:<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the Euclidean distance between the center of types <inline-formula><mml:math id="inf85"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf86"><mml:mi>j</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the mean pairwise distance between neurons of type <inline-formula><mml:math id="inf88"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p><p>To quantify the significance of the separation between two types, we shuffled labels for all neurons in these two clusters and calculated the p-value with bootstrap analysis to test whether the two types are significantly separated. If the maximum p-value of all images that have ≥ 10 neurons for both types is ≤ 0.01, these two types are significantly separated.</p></sec><sec id="s4-5-7"><title>Retina-SC transformation</title><p>For each type of SC neuron, we asked whether its responses can be explained by superposition of a small number of retinal ganglion cell types (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Given the known responses of RGC types to these same stimuli (<xref ref-type="bibr" rid="bib2">Baden et al., 2016</xref>) we approximated the response of each SC type as a weighted combination of RGC responses:<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the response vector of the SC type, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the matrix of the response vectors to the same stimuli for all types of RGCs, and <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the desired set of weights. The prediction error is quantified as<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Funding acquisition, Validation, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Validation, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All animal procedures were performed according to relevant guidelines and approved by the Caltech IACUC (protocol 1656).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82367-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code are available in a Caltech DATA Repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.22002/w3n8w-wgx37">https://doi.org/10.22002/w3n8w-wgx37</ext-link>) and in a public Github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/yatangli/Li-CellTypes-2023">https://github.com/yatangli/Li-CellTypes-2023</ext-link> copy archived at <xref ref-type="bibr" rid="bib36">Li and Meister, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Y-T</surname><given-names>Li</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Code and data for Li &amp; Meister (2023) Functional Cell Types in the Mouse Superior Colliculus</data-title><source>Caltech DATA</source><pub-id pub-id-type="doi">10.22002/w3n8w-wgx37</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>MM was supported by grants from NIH (R01 NS111477) and from the Simons Foundation (543015SPI). Y-t L was supported by a grant from the NEI (K99EY028640) and a Helen Hay Whitney Postdoctoral Fellowship.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadlou</surname><given-names>M</given-names></name><name><surname>Heimel</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Preference for concentric orientations in the mouse superior colliculus</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6773</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7773</pub-id><pub-id pub-id-type="pmid">25832803</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The functional diversity of retinal ganglion cells in the mouse</article-title><source>Nature</source><volume>529</volume><fpage>345</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1038/nature16468</pub-id><pub-id pub-id-type="pmid">26735013</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basso</surname><given-names>MA</given-names></name><name><surname>May</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Circuits for action and cognition: A view from the superior colliculus</article-title><source>Annual Review of Vision Science</source><volume>3</volume><fpage>197</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061234</pub-id><pub-id pub-id-type="pmid">28617660</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blasdel</surname><given-names>G</given-names></name><name><surname>Campbell</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Functional retinotopy of monkey visual cortex</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>8286</fpage><lpage>8301</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-20-08286.2001</pub-id><pub-id pub-id-type="pmid">11588200</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bleckert</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>GW</given-names></name><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Wong</surname><given-names>ROL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual space is represented by nonmatching topographies of distinct mouse retinal ganglion cell types</article-title><source>Current Biology</source><volume>24</volume><fpage>310</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.12.020</pub-id><pub-id pub-id-type="pmid">24440397</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byun</surname><given-names>H</given-names></name><name><surname>Kwon</surname><given-names>S</given-names></name><name><surname>Ahn</surname><given-names>HJ</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Forrest</surname><given-names>D</given-names></name><name><surname>Demb</surname><given-names>JB</given-names></name><name><surname>Kim</surname><given-names>IJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Molecular features distinguish ten neuronal types in the mouse superficial superior colliculus</article-title><source>The Journal of Comparative Neurology</source><volume>524</volume><fpage>2300</fpage><lpage>2321</lpage><pub-id pub-id-type="doi">10.1002/cne.23952</pub-id><pub-id pub-id-type="pmid">26713509</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>AR</given-names></name><name><surname>Shah</surname><given-names>RD</given-names></name><name><surname>Crair</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Developmental homeostasis of mouse retinocollicular synapses</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>1746</fpage><lpage>1755</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4383-06.2007</pub-id><pub-id pub-id-type="pmid">17301182</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Savier</surname><given-names>EL</given-names></name><name><surname>DePiero</surname><given-names>VJ</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Lack of evidence for stereotypical direction columns in the mouse superior colliculus</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>461</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1155-20.2020</pub-id><pub-id pub-id-type="pmid">33214319</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual response properties of neurons in the superficial layers of the superior colliculus of awake mouse</article-title><source>The Journal of Physiology</source><volume>596</volume><fpage>6307</fpage><lpage>6332</lpage><pub-id pub-id-type="doi">10.1113/JP276964</pub-id><pub-id pub-id-type="pmid">30281795</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Malmazet</surname><given-names>D</given-names></name><name><surname>Kühn</surname><given-names>NK</given-names></name><name><surname>Farrow</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Retinotopic separation of nasal and temporal motion selectivity in the mouse superior colliculus</article-title><source>Current Biology</source><volume>28</volume><fpage>2961</fpage><lpage>2969</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.001</pub-id><pub-id pub-id-type="pmid">30174186</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombeck</surname><given-names>DA</given-names></name><name><surname>Khabbaz</surname><given-names>AN</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Adelman</surname><given-names>TL</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title><source>Neuron</source><volume>56</volume><fpage>43</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.003</pub-id><pub-id pub-id-type="pmid">17920014</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dräger</surname><given-names>UC</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Responses to visual stimulation and relationship between visual, auditory, and somatosensory inputs in mouse superior colliculus</article-title><source>Journal of Neurophysiology</source><volume>38</volume><fpage>690</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1152/jn.1975.38.3.690</pub-id><pub-id pub-id-type="pmid">1127462</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dräger</surname><given-names>UC</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Topography of visual and somatosensory projections to mouse superior colliculus</article-title><source>Journal of Neurophysiology</source><volume>39</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1152/jn.1976.39.1.91</pub-id><pub-id pub-id-type="pmid">1249606</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>EM</given-names></name><name><surname>Gauvain</surname><given-names>G</given-names></name><name><surname>Sivyer</surname><given-names>B</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shared and distinct retinal input to the mouse superior colliculus and dorsal lateral geniculate nucleus</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>602</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1152/jn.00227.2016</pub-id><pub-id pub-id-type="pmid">27169509</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>EH</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Orientation columns in the mouse superior colliculus</article-title><source>Nature</source><volume>519</volume><fpage>229</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1038/nature14103</pub-id><pub-id pub-id-type="pmid">25517100</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fred</surname><given-names>ALN</given-names></name><name><surname>Jain</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Combining multiple clusterings using evidence accumulation</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>27</volume><fpage>835</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2005.113</pub-id><pub-id pub-id-type="pmid">15943417</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gale</surname><given-names>SD</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distinct representation and distribution of visual information by specific cell types in mouse superficial superior colliculus</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>13458</fpage><lpage>13471</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2768-14.2014</pub-id><pub-id pub-id-type="pmid">25274823</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gale</surname><given-names>SD</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Active dendritic properties and local inhibitory input enable selectivity for object motion in mouse superior colliculus neurons</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>9111</fpage><lpage>9123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0645-16.2016</pub-id><pub-id pub-id-type="pmid">27581453</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gale</surname><given-names>SD</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct cell types in the superficial superior colliculus project to the dorsal lateral geniculate and lateral posterior thalamic nuclei</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>1286</fpage><lpage>1292</lpage><pub-id pub-id-type="doi">10.1152/jn.00248.2018</pub-id><pub-id pub-id-type="pmid">29897837</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Paletzki</surname><given-names>R</given-names></name><name><surname>Heintz</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>GENSAT BAC cre-recombinase driver lines to study the functional organization of cerebral cortical and basal ganglia circuits</article-title><source>Neuron</source><volume>80</volume><fpage>1368</fpage><lpage>1383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.016</pub-id><pub-id pub-id-type="pmid">24360541</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Göbel</surname><given-names>W</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Vivo calcium imaging of neural network function</article-title><source>Physiology</source><volume>22</volume><fpage>358</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1152/physiol.00032.2007</pub-id><pub-id pub-id-type="pmid">18073408</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name><name><surname>Ting</surname><given-names>J</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Barkan</surname><given-names>E</given-names></name><name><surname>Bickley</surname><given-names>K</given-names></name><name><surname>Blesie</surname><given-names>N</given-names></name><name><surname>Braun</surname><given-names>T</given-names></name><name><surname>Brouner</surname><given-names>K</given-names></name><name><surname>Budzillo</surname><given-names>A</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casper</surname><given-names>T</given-names></name><name><surname>Castelli</surname><given-names>D</given-names></name><name><surname>Chong</surname><given-names>P</given-names></name><name><surname>Crichton</surname><given-names>K</given-names></name><name><surname>Cuhaciyan</surname><given-names>C</given-names></name><name><surname>Daigle</surname><given-names>TL</given-names></name><name><surname>Dalley</surname><given-names>R</given-names></name><name><surname>Dee</surname><given-names>N</given-names></name><name><surname>Desta</surname><given-names>T</given-names></name><name><surname>Ding</surname><given-names>S-L</given-names></name><name><surname>Dingman</surname><given-names>S</given-names></name><name><surname>Doperalski</surname><given-names>A</given-names></name><name><surname>Dotson</surname><given-names>N</given-names></name><name><surname>Egdorf</surname><given-names>T</given-names></name><name><surname>Fisher</surname><given-names>M</given-names></name><name><surname>de Frates</surname><given-names>RA</given-names></name><name><surname>Garren</surname><given-names>E</given-names></name><name><surname>Garwood</surname><given-names>M</given-names></name><name><surname>Gary</surname><given-names>A</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Godfrey</surname><given-names>K</given-names></name><name><surname>Gorham</surname><given-names>M</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Habel</surname><given-names>C</given-names></name><name><surname>Hadley</surname><given-names>K</given-names></name><name><surname>Harrington</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Henry</surname><given-names>A</given-names></name><name><surname>Hill</surname><given-names>D</given-names></name><name><surname>Josephsen</surname><given-names>S</given-names></name><name><surname>Kebede</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>L</given-names></name><name><surname>Kroll</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>B</given-names></name><name><surname>Lemon</surname><given-names>T</given-names></name><name><surname>Link</surname><given-names>KE</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Mann</surname><given-names>R</given-names></name><name><surname>McGraw</surname><given-names>M</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Mukora</surname><given-names>A</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Ngo</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>TN</given-names></name><name><surname>Nicovich</surname><given-names>PR</given-names></name><name><surname>Oldre</surname><given-names>A</given-names></name><name><surname>Park</surname><given-names>D</given-names></name><name><surname>Parry</surname><given-names>S</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Potekhina</surname><given-names>L</given-names></name><name><surname>Reid</surname><given-names>D</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Sandman</surname><given-names>D</given-names></name><name><surname>Schroedter</surname><given-names>M</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Soler-Llavina</surname><given-names>G</given-names></name><name><surname>Sulc</surname><given-names>J</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Taskin</surname><given-names>N</given-names></name><name><surname>Teeter</surname><given-names>C</given-names></name><name><surname>Thatra</surname><given-names>N</given-names></name><name><surname>Tung</surname><given-names>H</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Young</surname><given-names>R</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Lein</surname><given-names>E</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Classification of electrophysiological and morphological neuron types in the mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1182</fpage><lpage>1195</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0417-0</pub-id><pub-id pub-id-type="pmid">31209381</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Mills</surname><given-names>M</given-names></name><name><surname>Ng</surname><given-names>LL</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Mortrud</surname><given-names>M</given-names></name><name><surname>Ouellette</surname><given-names>B</given-names></name><name><surname>Kidney</surname><given-names>J</given-names></name><name><surname>Smith</surname><given-names>KA</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Sunkin</surname><given-names>S</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Madisen</surname><given-names>L</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Anatomical characterization of cre driver mice for neural circuit mapping and manipulation</article-title><source>Frontiers in Neural Circuits</source><volume>8</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2014.00076</pub-id><pub-id pub-id-type="pmid">25071457</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cluster-wise assessment of cluster stability</article-title><source>Computational Statistics &amp; Data Analysis</source><volume>52</volume><fpage>258</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2006.11.025</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horn</surname><given-names>G</given-names></name><name><surname>Hill</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Responsiveness to sensory stimulation of units in the superior colliculus and subjacent tectotegmental regions of the rabbit</article-title><source>Experimental Neurology</source><volume>14</volume><fpage>199</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(66)90007-0</pub-id><pub-id pub-id-type="pmid">5943702</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Bishop</surname><given-names>HI</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Defined cell types in superior colliculus make distinct contributions to prey capture behavior in the mouse</article-title><source>Current Biology</source><volume>29</volume><fpage>4130</fpage><lpage>4138</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.017</pub-id><pub-id pub-id-type="pmid">31761701</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inayat</surname><given-names>S</given-names></name><name><surname>Barchini</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Feng</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neurons in the most superficial lamina of the mouse superior colliculus are highly selective for stimulus direction</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7992</fpage><lpage>8003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0173-15.2015</pub-id><pub-id pub-id-type="pmid">25995482</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isa</surname><given-names>T</given-names></name><name><surname>Marquez-Legorreta</surname><given-names>E</given-names></name><name><surname>Grillner</surname><given-names>S</given-names></name><name><surname>Scott</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The tectum/superior colliculus as the vertebrate solution for spatial sensory integration and action</article-title><source>Current Biology</source><volume>31</volume><fpage>R741</fpage><lpage>R762</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.04.001</pub-id><pub-id pub-id-type="pmid">34102128</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>S</given-names></name><name><surname>Feldheim</surname><given-names>DA</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Segregation of visual response properties in the mouse superior colliculus and their modulation during locomotion</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8428</fpage><lpage>8443</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3689-16.2017</pub-id><pub-id pub-id-type="pmid">28760858</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Zaremba</surname><given-names>JD</given-names></name><name><surname>Danielson</surname><given-names>NB</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>SIMA: python software for analysis of dynamic fluorescence imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>80</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00080</pub-id><pub-id pub-id-type="pmid">25295002</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerlin</surname><given-names>AM</given-names></name><name><surname>Andermann</surname><given-names>ML</given-names></name><name><surname>Berezovskii</surname><given-names>VK</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Broadly tuned response properties of diverse inhibitory neuron subtypes in mouse visual cortex</article-title><source>Neuron</source><volume>67</volume><fpage>858</fpage><lpage>871</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.08.002</pub-id><pub-id pub-id-type="pmid">20826316</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langer</surname><given-names>TP</given-names></name><name><surname>Lund</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The upper layers of the superior colliculus of the rat: A Golgi study</article-title><source>The Journal of Comparative Neurology</source><volume>158</volume><fpage>418</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1002/cne.901580404</pub-id><pub-id pub-id-type="pmid">4615112</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>KH</given-names></name><name><surname>Tran</surname><given-names>A</given-names></name><name><surname>Turan</surname><given-names>Z</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The sifting of visual information in the superior colliculus</article-title><source>eLife</source><volume>9</volume><elocation-id>e50678</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50678</pub-id><pub-id pub-id-type="pmid">32286224</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y-T</given-names></name><name><surname>Turan</surname><given-names>Z</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Functional architecture of motion direction in the mouse superior colliculus</article-title><source>Current Biology</source><volume>30</volume><fpage>3304</fpage><lpage>3315</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.06.023</pub-id><pub-id pub-id-type="pmid">32649907</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Li</surname><given-names>YT</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Li-celltypes-2023</data-title><version designator="swh:1:rev:01e9f471c3b258ccc110a25716daa01654fe9636">swh:1:rev:01e9f471c3b258ccc110a25716daa01654fe9636</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:48f3348793ac0c15cd4130a36c89820fed13b002;origin=https://github.com/yatangli/Li-CellTypes-2023;visit=swh:1:snp:001d000fb25c11051d1cfbb08ae483df44706523;anchor=swh:1:rev:01e9f471c3b258ccc110a25716daa01654fe9636">https://archive.softwareheritage.org/swh:1:dir:48f3348793ac0c15cd4130a36c89820fed13b002;origin=https://github.com/yatangli/Li-CellTypes-2023;visit=swh:1:snp:001d000fb25c11051d1cfbb08ae483df44706523;anchor=swh:1:rev:01e9f471c3b258ccc110a25716daa01654fe9636</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mairal</surname><given-names>J</given-names></name><name><surname>Bach</surname><given-names>F</given-names></name><name><surname>Ponce</surname><given-names>J</given-names></name><name><surname>Sapiro</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Online dictionary learning for sparse coding</article-title><conf-name>ICML ’09</conf-name><conf-loc>Montreal Quebec Canada</conf-loc><pub-id pub-id-type="doi">10.1145/1553374.1553463</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Major</surname><given-names>DE</given-names></name><name><surname>Luksch</surname><given-names>H</given-names></name><name><surname>Karten</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Bottlebrush dendritic endings and large dendritic fields: Motion-detecting neurons in the mammalian tectum</article-title><source>The Journal of Comparative Neurology</source><volume>423</volume><fpage>243</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1002/1096-9861(20000724)423:2&lt;243::AID-CNE5&gt;3.0.CO;2-5</pub-id><pub-id pub-id-type="pmid">10867657</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>May</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>The mammalian superior colliculus: laminar structure and connections</chapter-title><person-group person-group-type="editor"><name><surname>Büttner-Ennever</surname><given-names>JA</given-names></name></person-group><source>Progress in Brain Research, Volume 151 of Neuroanatomy of the Oculomotor System</source><publisher-name>Elsevier</publisher-name><fpage>321</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(05)51011-2</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Giovannucci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>NoRMCorre: An online algorithm for piecewise rigid motion correction of calcium imaging data</article-title><source>Journal of Neuroscience Methods</source><volume>291</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.07.031</pub-id><pub-id pub-id-type="pmid">28782629</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhard</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Do</surname><given-names>Q</given-names></name><name><surname>Burke</surname><given-names>EG</given-names></name><name><surname>Heynderickx</surname><given-names>S</given-names></name><name><surname>Farrow</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A projection specific logic to sampling visual inputs in mouse superior colliculus</article-title><source>eLife</source><volume>8</volume><elocation-id>e50697</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50697</pub-id><pub-id pub-id-type="pmid">31750831</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodieck</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The density recovery profile: A method for the analysis of points in the plane applicable to retinal studies</article-title><source>Visual Neuroscience</source><volume>6</volume><fpage>95</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1017/s095252380001049x</pub-id><pub-id pub-id-type="pmid">2049333</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Román Rosón</surname><given-names>M</given-names></name><name><surname>Bauer</surname><given-names>Y</given-names></name><name><surname>Kotkat</surname><given-names>AH</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mouse dlgn receives functional input from a diverse population of retinal ganglion cells with limited convergence</article-title><source>Neuron</source><volume>102</volume><fpage>462</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.040</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roska</surname><given-names>B</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>The retina dissects the visual scene into distinct features</chapter-title><person-group person-group-type="editor"><name><surname>Werner</surname><given-names>JS</given-names></name><name><surname>Chalupa</surname><given-names>LM</given-names></name></person-group><source>The New Visual Neurosciences</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>163</fpage><lpage>182</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Jun</surname><given-names>NY</given-names></name><name><surname>Davis</surname><given-names>EL</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inter-mosaic coordination of retinal receptive fields</article-title><source>Nature</source><volume>592</volume><fpage>409</fpage><lpage>413</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03317-5</pub-id><pub-id pub-id-type="pmid">33692544</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahibzada</surname><given-names>N</given-names></name><name><surname>Dean</surname><given-names>P</given-names></name><name><surname>Redgrave</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Movements resembling orientation or avoidance elicited by electrical stimulation of the superior colliculus in rats</article-title><source>The Journal of Neuroscience</source><volume>6</volume><fpage>723</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.06-03-00723.1986</pub-id><pub-id pub-id-type="pmid">3958791</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Masland</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The types of retinal ganglion cells: Current status and implications for neuronal classification</article-title><source>Annual Review of Neuroscience</source><volume>38</volume><fpage>221</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071714-034120</pub-id><pub-id pub-id-type="pmid">25897874</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savier</surname><given-names>EL</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effects of locomotion on visual responses in the mouse superior colliculus</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9360</fpage><lpage>9368</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1854-19.2019</pub-id><pub-id pub-id-type="pmid">31570535</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shekhar</surname><given-names>K</given-names></name><name><surname>Lapan</surname><given-names>SW</given-names></name><name><surname>Whitney</surname><given-names>IE</given-names></name><name><surname>Tran</surname><given-names>NM</given-names></name><name><surname>Macosko</surname><given-names>EZ</given-names></name><name><surname>Kowalczyk</surname><given-names>M</given-names></name><name><surname>Adiconis</surname><given-names>X</given-names></name><name><surname>Levin</surname><given-names>JZ</given-names></name><name><surname>Nemesh</surname><given-names>J</given-names></name><name><surname>Goldman</surname><given-names>M</given-names></name><name><surname>McCarroll</surname><given-names>SA</given-names></name><name><surname>Cepko</surname><given-names>CL</given-names></name><name><surname>Regev</surname><given-names>A</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comprehensive classification of retinal bipolar neurons by single-cell transcriptomics</article-title><source>Cell</source><volume>166</volume><fpage>1308</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.07.054</pub-id><pub-id pub-id-type="pmid">27565351</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sibille</surname><given-names>J</given-names></name><name><surname>Gehr</surname><given-names>C</given-names></name><name><surname>Benichov</surname><given-names>JI</given-names></name><name><surname>Balasubramanian</surname><given-names>H</given-names></name><name><surname>Teh</surname><given-names>KL</given-names></name><name><surname>Lupashina</surname><given-names>T</given-names></name><name><surname>Vallentin</surname><given-names>D</given-names></name><name><surname>Kremkow</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>High-Density electrode recordings reveal strong and specific connections between retinal ganglion cells and midbrain neurons</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>5218</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-32775-2</pub-id><pub-id pub-id-type="pmid">36064789</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Sarnaik</surname><given-names>R</given-names></name><name><surname>Rangarajan</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual receptive field properties of neurons in the superficial superior colliculus of the mouse</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>16573</fpage><lpage>16584</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3305-10.2010</pub-id><pub-id pub-id-type="pmid">21147997</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warwick</surname><given-names>RA</given-names></name><name><surname>Kaushansky</surname><given-names>N</given-names></name><name><surname>Sarid</surname><given-names>N</given-names></name><name><surname>Golan</surname><given-names>A</given-names></name><name><surname>Rivlin-Etzion</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inhomogeneous encoding of the visual field in the mouse retina</article-title><source>Current Biology</source><volume>28</volume><fpage>655</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.016</pub-id><pub-id pub-id-type="pmid">29456141</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wässle</surname><given-names>H</given-names></name><name><surname>Peichl</surname><given-names>L</given-names></name><name><surname>Boycott</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Morphology and topography of on- and off-alpha cells in the cat retina</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>212</volume><fpage>157</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1098/rspb.1981.0032</pub-id><pub-id pub-id-type="pmid">6166012</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheatcroft</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Functional organisation of the mouse superior colliculus</article-title><source>Frontiers in Neural Circuits</source><volume>16</volume><elocation-id>792959</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2022.792959</pub-id><pub-id pub-id-type="pmid">35601532</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>W</given-names></name><name><surname>Laboulaye</surname><given-names>MA</given-names></name><name><surname>Tran</surname><given-names>NM</given-names></name><name><surname>Whitney</surname><given-names>IE</given-names></name><name><surname>Benhar</surname><given-names>I</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mouse retinal cell atlas: Molecular identification of over sixty amacrine cell types</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5177</fpage><lpage>5195</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0471-20.2020</pub-id><pub-id pub-id-type="pmid">32457074</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yonehara</surname><given-names>K</given-names></name><name><surname>Shintani</surname><given-names>T</given-names></name><name><surname>Suzuki</surname><given-names>R</given-names></name><name><surname>Sakuta</surname><given-names>H</given-names></name><name><surname>Takeuchi</surname><given-names>Y</given-names></name><name><surname>Nakamura-Yonehara</surname><given-names>K</given-names></name><name><surname>Noda</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Expression of SPIG1 reveals development of a retinal ganglion cell subtype projecting to the medial terminal nucleus in the mouse</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e1533</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0001533</pub-id><pub-id pub-id-type="pmid">18253481</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuronal cell-type classification: Challenges, opportunities and the path forward</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>530</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.85</pub-id><pub-id pub-id-type="pmid">28775344</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>IJ</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title><source>PNAS</source><volume>109</volume><fpage>E2391</fpage><lpage>E2398</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211547109</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82367.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.04.01.486789" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.01.486789"/></front-stub><body><p>This important paper will be of interest to neuroscientists interested in how the representation of sensory information is refined between the sensory periphery and more central areas. The work provides compelling evidence for a much greater diversity of functional cell types in the superior colliculus than previously suggested, and that the functional organization of cell types in the superior colliculus is distinct from that of the retina.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82367.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.01.486789">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.01.486789v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Functional Cell Types in the Mouse Superior Colliculus&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Fred Rieke as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by a Reviewing Editor and Claude Desplan as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers all agreed that the data presented in the paper are quite valuable, but that several issues need to be improved before the paper can be fully evaluated. Chief among these are:</p><p>1. Results from the genetic lines are not integrated well with the rest of the paper, and this misses an opportunity to provide more information about the properties of the labeled cells and how they fit into the remainder of the recorded population.</p><p>2. Some key statements lack confirming analyses – e.g. the separation of cells into those that respond to the chirp stimulus and those that do not.</p><p>3. Related to the point above, the paper contains several general statements about how SC is organized (e.g. the number of cell types and spatial organization of these types). The paper would benefit from focusing on those that are directly supported by the results and appropriate analyses. It is not entirely clear which those are without seeing how additional analyses turn out.</p><p>These and other important points are detailed in the individual reviews below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In some parts of the manuscript, it is unclear how the conclusions were reached. An example is l. 93-99, where the authors say that &quot;Group 1 … is distinct from Group 2 … in that it responds more strongly to the chirp stimuli&quot;, but this statement is not accompanied by any quantification. &quot;Almost all these types are excited by both On and Off stimuli&quot; – do the authors refer to the responses to the steps in the chirp or any other part of the data? No quantification / statistics are given for the statements regarding the differential sensitivity to temporal frequency. Similar in l. 107 ff, where statements about strong correlations are made, but not quantified. Or in l. 203, where it is stated that the sign of contribution changes between groups 1 and 2. Please add such information here and throughout.</p><p>Several parts of the manuscript are directly related to the recently published findings from the Kremkow lab (10.1038/s41467-022-32775-2). While I assume that the questions have evolved independently in the two labs and that the data have been collected in parallel, I still find it somewhat disappointing that the important work from a junior π is not cited in the manuscript. Beyond acknowledging the achievement, it would be worth discussing the similarities and differences between the two studies, which I perceive as clearly complementary. Similarly, I think the Baden dataset deserves a citation also in text around l. 196 (not only in the figure legend), and the idea of fitting higher visual responses for a weighted sum of RGC inputs was previously performed for dLGN in Roman-Roson et al., 2019.</p><p>The authors use an elegant approach of imaging SC while leaving cortex intact, but this comes with the caveat that only a small portion of SC is accessible. The authors touch upon this point, but should discuss in more depth to which degree some of the features they observe might or might not be related to this caveat.</p><p>I think that an asset of the paper is the imaging of cre mouse lines, but I also have a number of questions. While the authors stated that in the overall population, they have mostly responses to on and off, they find off-type responses in some of the cre lines (e.g., l. 173). Why is this the case? Can the authors rule out that neurons with on or off responses are mixed in the clusters presented in Figure 2, and thus the cluster response is on-off? Also, instead of saying that the number of neurons is too low for clustering the data from the cre-lines, could the authors try to find the best-matching cluster of the 24 clusters obtained in the large population of neurons?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Li and Meister recorded the visual responses of more than 5000 neurons from the visual layers of the superior colliculus of 16 awake head-fixed mice allowed to run on a disc. Clustering of the visual responses suggests a greater diversity of functional cell types (24) in the superior colliculus than previously suggested (5 – 10). Neurons fell into two groups, those that respond similarly to retinal ganglion cells and a second group that showed more specialized stimulus selectivity. By comparing this data set with comparable recordings from the mouse retina (Baden et al., 2016) the authors suggest two differences: (i) The visual representation within the superior colliculus has a lower dimensionality than in the retina, (ii) In contrast to the retina, neurons of the same functional type form local clusters. This dataset has the potential to add important details to our understanding of how visual response properties are organized within the superior colliculus and highlight some potentially important key differences with the retina.</p><p>While this paper has collected impressive data, there are two significant weaknesses that limit the impact of this work:</p><p>1. Clear information about the relationship between the functional classes and genetically labeled populations.</p><p>2. Clear description of how the different functional cell types are distributed across visual space.</p><p>Providing a link to genetically targetable neuronal components of the superior colliculus would dramatically increase the usability of the data while describing how the set of functional cell types is distributed across the visual field would provide biological insight into the functional organization of the mouse superior colliculus.</p><p>1. The integration of the data from the different genetically targeted cell types into the complete data set is not explicit or transparent. As currently presented it is unclear if this is exclusively an issue of analysis or a limitation of the data. An understanding of how functional classes are represented within and among different genetic cell types would make the dataset much more useful to the community and help to better assess the utility of these mouse lines. We have the following questions and requests.</p><p>1.1. Please provide information about the distribution of the 24 functional classes for each genetic cell type.</p><p>1.2. How separate are the different genetic cell classes?</p><p>1.3. Is the functional segregation/overlap consistent with the anatomical overlap of these cell types (e.g Ntsr, tac1, and VGlut2, Zhou et al., 2017; Vgat and <italic>Rorb</italic>, Gale and Murphy, 2018)?</p><p>1.4. What is the degree of anatomic overlap between genetic lines?</p><p>1.5. Does this predict the amount of functional overlap?</p><p>1.6. Please provide the anatomic distribution of the different genetic lines within the superficial layers of the colliculus.</p><p>1.7. Please report the number of cells and animals recorded for each cell type in Figure 4.</p><p>2. A second issue in the paper is that the distribution of the different functional cell types in visual space is not analyzed. While it is reported in Figure 10 this representation does not report a systematic or unified analysis. In Figure 10 it is suggested that the recording technique measures from ~ 50 – 140 degrees of visual space horizontally and -10 to +60 degrees of visual space vertically. This appears to offer the opportunity to report useful information.</p><p>2.1. What portion(s) of the visual world does each functional cell type look at?</p><p>2.2. How consistent is this between animals?</p><p>2.3. What is the relationship in the portion(s) of the visual world of the different cell types sample?</p><p>2.4. To compare with retinal cell types, the receptive field overlap within and between different cell types should be presented.</p><p>3. The cluster analysis suggesting there are 24 functional cell types requires further support.</p><p>3.1. Please demonstrate that functional cell types are not highly over-represented in any single animal and that variability between animals is not a major contributor to the number of clusters.</p><p>3.2. It is not clear how separate the different clusters are. Could you please provide a visual representation of this? For example, scatter plots of the different clusters in feature space (like a pairs plot, e.g. seaborn pairplot or ggpairs in R), or demonstrate the separation using nonlinear embedding visualizations like tSNE.</p><p>4. There is currently very little evidence supporting the claim in lines 8-9 of the abstract &quot;The second group is dominant at greater depths, consistent with a vertical progression of signal processing in the SC.&quot;</p><p>4.1. Further analysis should be provided to support this statement, or remove the claim.</p><p>5. The evidence supporting the reduction in dimensionality from the retina to the superior colliculus requires further support.</p><p>5.1. In panel 5D it is shown that there is a small decrease in the number of principal components needed to explain a certain level of variance. This is an interesting finding. Please demonstrate that this is not explained by the fact that the superior colliculus receives only a subset of visual information (~85% of the ganglion cells). Please report/estimate the dimensionality of the retinal ganglion cell types that project to the superior colliculus, rather than all retinal ganglion cells.</p><p>6. In panels 5B and 5C there is a presentation of the apparent opposite relationship between orientation and direction selectivity of neurons in the retina with a negative correlation and the superior colliculus with a positive correlation. This is a very interesting result. Could you please add the following to better support this assertion?</p><p>6.1. In Figure 5B please add an indication of which cell type is which, so it is easier to cross-reference with Baden et al. (2016). Do this also for Figure 5C so one can cross-reference with other figures.</p><p>6.2. Please also report this relationship for median OSI vs DSI in Figure 5B and 5C. In Baden et al. 2016 and figure 1 many of the distributions for OSI and DSI are highly skewed. It is quite amazing that no cell type has a low OSI and low DSI.</p><p>6.3. Why only report the relationship for 32 of the RGCs? They report 39 clusters of &quot;certain&quot; RGCs (Figure 2), where different clusters of the same group have different OSi vs DSi relationships.</p><p>6.4. In figure 11 please report E – G as 2D histograms/density plots. There are too many points to visualize the relationship.</p><p>6.5. Please re-plot the data in Figure 11 to demonstrate the relationship of DSI vs OSI of individual neurons by cluster. Is the relationship strongly positive for each cluster?</p><p>6.6. In the discussion, please provide references to other sources that the relationship between OSI and DSI in the retinal ganglion cells does indeed have a negative correlation.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Clustering approach: The steps between the measured calcium responses and the functional clusters in Figure 2 need to be described in more detail. Figure 6 is critical in this regard, but at present, it is quite hard to understand. One suggestion is to start with a measured calcium trace, show it in relation to several of the temporal features identified, and then show the decomposition of the response into features as in Figure 6. The leap currently straight to the feature representation in Figure 6A is too much. The figure caption could correspondingly be improved. Understanding this process is critical to evaluating the paper.</p><p>Number of clusters: Figure 6 shows that considering responses to individual stimuli results in more clusters than considering responses to the collection of stimuli together. This is hard to understand. Why doesn't the clustering using the full collection of responses project those responses down to the features identified, for example, for the chirp alone? From Figure 6 that would appear to increase the separable clusters. Related, why doesn't the ability to identify ~30 clusters for some stimuli indicate that there are ~30 cell types in SC? This result is not discussed sufficiently, and it would seem to bring into question one of the main results in the paper. Generally, the dependence of the number of clusters on the number and type of stimuli needs to be considered.</p><p>Figures: some of the figures are too small to appreciate. Some examples follow. Figure 1: the text above the traces in D is too small, and the directional selectivity and receptive field panels are too small. Figure 2: panel A is too packed and individual panels are too small. Figure 7: summary indices could be removed since they are already in Figure 2; doing so would create more space for the other panels. Axis labels are too small to easily read in several figures. It would also help to provide more information in several of the figures – e.g. in Figure 2F repeating the definition of the functional properties on the x-axis would be helpful.</p><p>Introduction: Suggest adding a sentence or two about the role of the SC in behavior.</p><p>Line 54: Suggest defining the stimuli in more detail.</p><p>Figure 2E: Can you flip the y-axis so that it corresponds (top to bottom) to Figure 2A?</p><p>Lines 108-109: Can you point out cells that have both orientation and directional tuning?</p><p>Section 2.3: This section discusses both visual space and space in the circuit (retina or SC). It is at times unclear which is being described. Related to this point, the Discussion makes a point about clustering in visual space, which should be made more clearly in this section of Results.</p><p>Section 2.4: Were the functional properties of cells with a specific genetic label similar (aside from RF sizes)?</p><p>Section 2.4 ends on a rather detailed point. Suggest coming back to differences between excitatory and inhibitory cells (or some other higher level point).</p><p>Lines 201-202: Can you add the identity of the RGC types corresponding to the clusters mentioned here?</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Functional Cell Types in the Mouse Superior Colliculus&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Claude Desplan (Senior Editor) and a Reviewing Editor.</p><p>Most of the issues raised in the first round of review have been resolved. A few issues remain:</p><p>Line 30: Clarify whether the 10 types described here are in the superficial layer or entire SC.</p><p>Line 88: How are OSI and DSi incorporated into the distance measure? Are they weighted the same as the other parameters, or reweighted so that they have a larger impact on the distance measure?</p><p>Line 107: Can you state the difference between Figure 3B and 10A briefly in the main text so it is clearer why both are shown?</p><p>Line 137-138: Suggest stating the expected shape of the autocorrelation if the cells are randomly distributed. This would help interpret the decay in autocorrelation described here.</p><p>Line 160: What is &quot;significant spatial separation&quot; – is this a statistical statement?</p><p>Figure 6A: suggest flipping y-axis so that it corresponds to type 1 and type 2 groups in earlier figure. Text labels specifying SC type 1 and type 2 would also help.</p><p>The legends of the supplementary figures could get expanded considerably. At present they are quite brief and some of those figures are difficult to appreciate.</p><p>The format of the revised paper was inconvenient. The figure legends were embedded in the text, but the figures came at the end. Most importantly, the figures were not in the same order as the figure legends and were not themselves labeled (this had to do with where the supplementary figures ended up). Please provide an easier to read version when you resubmit!</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82367.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers all agreed that the data presented in the paper are quite valuable, but that several issues need to be improved before the paper can be fully evaluated. Chief among these are:</p><p>1. Results from the genetic lines are not integrated well with the rest of the paper, and this misses an opportunity to provide more information about the properties of the labeled cells and how they fit into the remainder of the recorded population.</p></disp-quote><p>We have addressed this issue in responding to specific concerns raised by the reviewers.</p><disp-quote content-type="editor-comment"><p>2. Some key statements lack confirming analyses – e.g. the separation of cells into those that respond to the chirp stimulus and those that do not.</p></disp-quote><p>We have added more statistical analysis as requested by the reviewers.</p><disp-quote content-type="editor-comment"><p>3. Related to the point above, the paper contains several general statements about how SC is organized (e.g. the number of cell types and spatial organization of these types). The paper would benefit from focusing on those that are directly supported by the results and appropriate analyses. It is not entirely clear which those are without seeing how additional analyses turn out.</p></disp-quote><p>We have performed more analyses and focused the claims accordingly.</p><disp-quote content-type="editor-comment"><p>These and other important points are detailed in the individual reviews below.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>An example is l. 93-99, where the authors say that &quot;Group 1 … is distinct from Group 2 … in that it responds more strongly to the chirp stimuli&quot;, but this statement is not accompanied by any quantification.</p></disp-quote><p>We have compared the mean responses to the chirp stimuli between the two groups and found Group 1 responds significantly stronger than Group 2 (0.14 ± 0.05 vs 0.04 ± 0.04, p&lt;0.001, t test).</p><disp-quote content-type="editor-comment"><p>&quot;Almost all these types are excited by both On and Off stimuli” – do the authors refer to the responses to the steps in the chirp or any other part of the data?</p></disp-quote><p>Yes, we refer to the responses to the steps in the chirp; this is now clarified in the text.</p><disp-quote content-type="editor-comment"><p>No quantification/statistics are given for the statements regarding the differential sensitivity to temporal frequency.</p></disp-quote><p>The preference for temporal frequency was quantified by the frequency selectivity index. Group 1a is more selective to low frequency than Group 1b (p=0.02, Wilcoxon rank-sum test).</p><disp-quote content-type="editor-comment"><p>Similar in l. 107 ff, where statements about strong correlations are made, but not quantified.</p></disp-quote><p>The correlation was quantified by the correlation coefficients, and only significant values were color-coded in Figure 2F (new Figure 3C), with clarifications in the text and figure captions.</p><disp-quote content-type="editor-comment"><p>Or in l. 203, where it is stated that the sign of contribution changes between groups 1 and 2. Please add such information here and throughout.</p></disp-quote><p>We have revised this text and removed the claims that were not supported by the quantifications.</p><disp-quote content-type="editor-comment"><p>Several parts of the manuscript are directly related to the recently published findings from the Kremkow lab (10.1038/s41467-022-32775-2). While I assume that the questions have evolved independently in the two labs and that the data have been collected in parallel, I still find it somewhat disappointing that the important work from a junior PI is not cited in the manuscript. Beyond acknowledging the achievement, it would be worth discussing the similarities and differences between the two studies, which I perceive as clearly complementary. Similarly, I think the Baden dataset deserves a citation also in text around l. 196 (not only in the figure legend), and the idea of fitting higher visual responses for a weighted sum of RGC inputs was previously performed for dLGN in Roman-Roson et al., 2019.</p></disp-quote><p>We note that the Kremkow paper appeared only after we submitted our manuscript. These are all useful references, and they are cited in the revised text, including a comparison between our study and work from the Kremkow lab.</p><disp-quote content-type="editor-comment"><p>The authors use an elegant approach to imaging SC while leaving the cortex intact, but this comes with the caveat that only a small portion of SC is accessible. The authors touch upon this point but should discuss in more depth to which degree some of the features they observe might or might not be related to this caveat.</p></disp-quote><p>We have provided more discussion on this issue.</p><disp-quote content-type="editor-comment"><p>I think that an asset of the paper is the imaging of cre mouse lines, but I also have a number of questions. While the authors stated that in the overall population, they have mostly responses to on and off, they find off-type responses in some of the cre lines (e.g., l. 173). Why is this the case?</p></disp-quote><p>We have removed that statement, following further analysis of contrast selectivity for individual neurons.</p><disp-quote content-type="editor-comment"><p>Can the authors rule out that neurons with on or off responses are mixed in the clusters presented in Figure 2, and thus the cluster response is on-off?</p></disp-quote><p>We calculated the contrast selectivity index for each individual neuron, and conclude that the on-off character of a cluster’s average response is not the result of mixing On and Off cells. Please see the single-mode histograms in the CSI column of Figure 3A.</p><disp-quote content-type="editor-comment"><p>Also, instead of saying that the number of neurons is too low for clustering the data from the cre lines, could the authors try to find the best-matching cluster of the 24 clusters obtained in the large population of neurons?</p></disp-quote><p>We have added a new figure on this point (new Figure 5D).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Li and Meister recorded the visual responses of more than 5000 neurons from the visual layers of the superior colliculus of 16 awake head-fixed mice allowed to run on a disc. Clustering of the visual responses suggests a greater diversity of functional cell types (24) in the superior colliculus than previously suggested (5 – 10). Neurons fell into two groups, those that respond similarly to retinal ganglion cells and a second group that showed more specialized stimulus selectivity. By comparing this data set with comparable recordings from the mouse retina (Baden et al., 2016) the authors suggest two differences: (i) The visual representation within the superior colliculus has a lower dimensionality than in the retina, (ii) In contrast to the retina, neurons of the same functional type form local clusters. This dataset has the potential to add important details to our understanding of how visual response properties are organized within the superior colliculus and highlight some potentially important key differences with the retina.</p><p>While this paper has collected impressive data, there are two significant weaknesses that limit the impact of this work:</p><p>1. Clear information about the relationship between the functional classes and genetically labeled populations.</p></disp-quote><p>We have provided such information in the new Figure 5D.</p><disp-quote content-type="editor-comment"><p>2. Clear description of how the different functional cell types are distributed across visual space.</p></disp-quote><p>Please see our responses below.</p><disp-quote content-type="editor-comment"><p>Providing a link to genetically targetable neuronal components of the superior colliculus would dramatically increase the usability of the data while describing how the set of functional cell types is distributed across the visual field would provide biological insight into the functional organization of the mouse superior colliculus.</p><p>1. The integration of the data from the different genetically targeted cell types into the complete data set is not explicit or transparent. As currently presented it is unclear if this is exclusively an issue of analysis or a limitation of the data. An understanding of how functional classes are represented within and among different genetic cell types would make the dataset much more useful to the community and help to better assess the utility of these mouse lines. We have the following questions and requests.</p><p>1.1. Please provide information about the distribution of the 24 functional classes for each genetic cell type.</p></disp-quote><p>The new Figure 5D shows such information for Cre-lines in which we collected data from at least three animals.</p><disp-quote content-type="editor-comment"><p>1.2. How separate are the different genetic cell classes?</p><p>1.3. Is the functional segregation/overlap consistent with the anatomical overlap of these cell types (e.g Ntsr, tac1, and VGlut2, Zhou et al., 2017; Vgat and Rorb, Gale and Murphy, 2018)?</p><p>1.4. What is the degree of anatomic overlap between genetic lines?</p><p>1.5. Does this predict the amount of functional overlap?</p><p>1.6. Please provide the anatomic distribution of the different genetic lines within the superficial layers of the colliculus.</p></disp-quote><p>VGluT2<sup>+</sup> neurons are excitatory and Vgat+ neurons are inhibitory, so they are exclusive to each other. <italic>Rorb</italic>+ labels both excitatory and inhibitory neurons (Gale and Murphy, 2018). Ntsr1+ neurons are excitatory and show wide-field morphology (Gale and Murphy, 2018). Ntsr1+ neurons locate at the deeper part of superficial layer, and Vgat+ neurons are found across all the depth (Gale and Murphy, 2014) with the number decreasing slightly (Mize, 1992).</p><p>The new Figure 5D shows that more Vgat+ neurons belong to Group 1 that dominate the upper superficial layer. We provide this background information in the text (Sec 2.4). As regards the relation between anatomical and functional overlap, we don’t really have a hypothesis for such a relationship.</p><disp-quote content-type="editor-comment"><p>1.7. Please report the number of cells and animals recorded for each cell type in Figure 4.</p></disp-quote><p>We have added these numbers to the Methods.</p><disp-quote content-type="editor-comment"><p>2. A second issue in the paper is that the distribution of the different functional cell types in visual space is not analyzed. While it is reported in Figure 10 this representation does not report a systematic or unified analysis. In Figure 10 it is suggested that the recording technique measures from ~ 50 – 140 degrees of visual space horizontally and -10 to +60 degrees of visual space vertically. This appears to offer the opportunity to report useful information.</p><p>2.1. What portion(s) of the visual world does each functional cell type look at?</p><p>2.2. How consistent is this between animals?</p><p>2.3. What is the relationship in the portion(s) of the visual world of the different cell types sample?</p></disp-quote><p>We offer data related to these points in the new Supplementary Figure 12. Also, we re-assessed the data set with these questions in mind and concluded that it is insufficient to sustain any strong claims about the visual fields of different functional types. The issue is that in each mouse the imaging window is in a slightly different place, and thus a different section of the visual field is covered. The regional variation in expression of the indicator contributes further variability across animals. This makes it difficult to piece together a global layout of functional types in the visual field from many local windows from different animals. We agree that the distribution of different processing functions across the visual field is an interesting subject, but it would benefit from a more global recording method. By contrast, we can be confident about the anatomical relationship of functional types within the same imaging window, and our report focuses on those.</p><disp-quote content-type="editor-comment"><p>2.4. To compare with retinal cell types, the receptive field overlap within and between different cell types should be presented.</p></disp-quote><p>We have added the RF overlap between different cell types to Figure 4D.</p><disp-quote content-type="editor-comment"><p>3. The cluster analysis suggesting there are 24 functional cell types requires further support.</p><p>3.1. Please demonstrate that functional cell types are not highly over-represented in any single animal and that variability between animals is not a major contributor to the number of clusters.</p></disp-quote><p>We now report how each functional type derives from the various animals (Figure 12). Indeed, a few clusters are dominated by one or two animals. We have flagged those clusters in Figure 2, along with others that fail the Jacard criterion of robustness. One animal contributed about 25% of the recordings, owing to efficient expression of the indicator over a wide area of the SC. If we eliminate that animal from consideration, the number of functional types drops from 24 to 19, using the same BIC criterion. We mention in the corresponding Results section that the number of functional types is not constrained to high precision, but the data indicate a number near 20.</p><disp-quote content-type="editor-comment"><p>3.2. It is not clear how separate the different clusters are. Could you please provide a visual representation of this? For example, scatter plots of the different clusters in feature space (like a pairs plot, e.g. seaborn pairplot or ggpairs in R), or demonstrate the separation using nonlinear embedding visualizations like tSNE.</p></disp-quote><p>We have added a figure showing the relative positions of each cluster in the first 2 principal axes of feature space (new Figure 2C). This is similar to the cluster visualization for RGCs in Baden et al. 2016.</p><disp-quote content-type="editor-comment"><p>4. There is currently very little evidence supporting the claim in lines 8-9 of the abstract &quot;The second group is dominant at greater depths, consistent with a vertical progression of signal processing in the SC.&quot;</p><p>4.1. Further analysis should be provided to support this statement, or remove the claim.</p></disp-quote><p>We subjected this to further analysis. 68% of deeper neurons are from Group 2 (p&lt;0.001, chisquare test).</p><disp-quote content-type="editor-comment"><p>5. The evidence supporting the reduction in dimensionality from the retina to the superior colliculus requires further support.</p><p>5.1. In panel 5D it is shown that there is a small decrease in the number of principal components needed to explain a certain level of variance. This is an interesting finding. Please demonstrate that this is not explained by the fact that the superior colliculus receives only a subset of visual information (~85% of the ganglion cells). Please report/estimate the dimensionality of the retinal ganglion cell types that project to the superior colliculus, rather than all retinal ganglion cells.</p></disp-quote><p>Yes, it is thought that ~10% of RGCs avoid the superior colliculus. It is not entirely clear which RGC types those are. In at least one case (SPIG1+ cells) there appears to be a regional difference across the retina within the same RGC type. So certainly part of the reduction in dimensionality could result from selection of the afferents, but not enough is known about the cell types involved for a precise calculation. We have added this caveat to the discussion.</p><disp-quote content-type="editor-comment"><p>6. In panels 5B and 5C there is a presentation of the apparent opposite relationship between orientation and direction selectivity of neurons in the retina with a negative correlation and the superior colliculus with a positive correlation. This is a very interesting result. Could you please add the following to better support this assertion?</p><p>6.1. In Figure 5B please add an indication of which cell type is which, so it is easier to cross-reference with Baden et al. (2016). Do this also for Figure 5C so one can cross-reference with other figures.</p></disp-quote><p>We have added a color coding for this to the new Figure 6B and 6C.</p><disp-quote content-type="editor-comment"><p>6.2. Please also report this relationship for median OSI vs DSI in Figure 5B and 5C. In Baden et al. 2016 and figure 1 many of the distributions for OSI and DSI are highly skewed. It is quite amazing that no cell type has a low OSI and low DSI.</p></disp-quote><p>We have added that to the new Figure 6B and 6C.</p><disp-quote content-type="editor-comment"><p>6.3. Why only report the relationship for 32 of the RGCs? They report 39 clusters of &quot;certain&quot; RGCs (Figure 2), where different clusters of the same group have different OSi vs DSi relationships.</p></disp-quote><p>Baden 2016 reported 49 clusters, which were then combined into 32 types based on staining and morphology for better interpretation. Therefore we adopted 32 types rather than 49 clusters.</p><disp-quote content-type="editor-comment"><p>6.4. In figure 11 please report E – G as 2D histograms/density plots. There are too many points to visualize the relationship.</p></disp-quote><p>We have replotted the panels as suggested in the new Supplementary Figure 13.</p><disp-quote content-type="editor-comment"><p>6.5. Please re-plot the data in Figure 11 to demonstrate the relationship of DSI vs OSI of individual neurons by cluster. Is the relationship strongly positive for each cluster?</p></disp-quote><p>In the new Figure 13J, we show the correlation coefficient for each cluster.</p><disp-quote content-type="editor-comment"><p>6.6. In the discussion, please provide references to other sources that the relationship between OSI and DSI in the retinal ganglion cells does indeed have a negative correlation.</p></disp-quote><p>This result is based on our own analysis of Baden’s dataset. We are not aware of another report.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Clustering approach: The steps between the measured calcium responses and the functional clusters in Figure 2 need to be described in more detail. Figure 6 is critical in this regard, but at present, it is quite hard to understand. One suggestion is to start with a measured calcium trace, show it in relation to several of the temporal features identified, and then show the decomposition of the response into features as in Figure 6. The leap currently straight to the feature representation in Figure 6A is too much. The figure caption could correspondingly be improved. Understanding this process is critical to evaluating the paper.</p></disp-quote><p>The calcium traces of individual neurons were and still are shown in Figure 1 and the new Supplementary Figure 9. We have expanded the text surrounding the feature matrix, in both methods and the caption of Figure 7. The dimensionality-reduction method we used is fairly standard.</p><disp-quote content-type="editor-comment"><p>Number of clusters: Figure 6 shows that considering responses to individual stimuli results in more clusters than considering responses to the collection of stimuli together. This is hard to understand. Why doesn't the clustering using the full collection of responses project those responses down to the features identified, for example, for the chirp alone? From Figure 6 that would appear to increase the separable clusters. Related, why doesn't the ability to identify ~30 clusters for some stimuli indicate that there are ~30 cell types in SC? This result is not discussed sufficiently, and it would seem to bring into question one of the main results in the paper. Generally, the dependence of the number of clusters on the number and type of stimuli needs to be considered.</p></disp-quote><p>These are good questions. The choice of stimuli will certainly influence the identification of functional types. Rather than devising a new stimulus set arbitrarily, we used the stimuli that had previously served to separate retinal ganglion cell types. This is relevant for two reasons: (1) In case of the retina there is ground truth about the identity of cell types, based on gene expression data and the anatomical tiling criterion. (2) RGCs are immediately presynaptic to the superior colliculus, and the transformation that occurs from retina to SC is of special interest here. Ultimately the number of clusters is also influenced by the Bayesian Information Criterion, that balances the goodness of the fit with the complexity of the model. A section in the Discussion brings up these caveats regarding the precise number of types from the cluster analysis.</p><disp-quote content-type="editor-comment"><p>Figures: some of the figures are too small to appreciate. Some examples follow. Figure 1: the text above the traces in D is too small, and the directional selectivity and receptive field panels are too small. Figure 2: panel A is too packed and individual panels are too small. Figure 7: summary indices could be removed since they are already in Figure 2; doing so would create more space for the other panels. Axis labels are too small to easily read in several figures. It would also help to provide more information in several of the figures – e.g. in Figure 2F repeating the definition of the functional properties on the x-axis would be helpful.</p></disp-quote><p>For Figure 1, we have deleted the text. For Figure 2, we have divided it into two figures. For Figure 7, we have removed the summary indices. We have also increased the size of labels as suggested.</p><disp-quote content-type="editor-comment"><p>Introduction: Suggest adding a sentence or two about the role of the SC in behavior.</p></disp-quote><p>We have added a sentence at the end of the first paragraph.</p><disp-quote content-type="editor-comment"><p>Line 54: Suggest defining the stimuli in more detail.</p></disp-quote><p>We added a brief description of stimuli to the figure caption. The detailed definition is in the Methods. We have indicated this in the text.</p><disp-quote content-type="editor-comment"><p>Figure 2E: Can you flip the y-axis so that it corresponds (top to bottom) to Figure 2A?</p></disp-quote><p>We have flipped the y-axis.</p><disp-quote content-type="editor-comment"><p>Lines 108-109: Can you point out cells that have both orientation and directional tuning?</p></disp-quote><p>Cells in types 9, 14, 20, 24 show both orientation and direction tuning. We have added that to the text.</p><disp-quote content-type="editor-comment"><p>Section 2.3: This section discusses both visual space and space in the circuit (retina or SC). It is at times unclear which is being described. Related to this point, the Discussion makes a point about clustering in visual space, which should be made more clearly in this section of Results.</p></disp-quote><p>This Results section is only about anatomical space in the SC. We revised the section title to make this clear.</p><disp-quote content-type="editor-comment"><p>Section 2.4: Were the functional properties of cells with a specific genetic label similar (aside from RF sizes)?</p></disp-quote><p>This information was provided in Figure 4B (new Figure 5B) with histograms for each functional property and genetic label. Some of these distributions are rather sharp, for example most <italic>Rorb</italic>+ neurons show very low orientation and direction selectivity. Others vary more broadly, for example the best stimulus size (BSS) covers quite a range within all the genetic types.</p><disp-quote content-type="editor-comment"><p>Section 2.4 ends on a rather detailed point. Suggest coming back to differences between excitatory and inhibitory cells (or some other higher level point).</p></disp-quote><p>We changed the ending as suggested.</p><disp-quote content-type="editor-comment"><p>Lines 201-202: Can you add the identity of the RGC types corresponding to the clusters mentioned here?</p></disp-quote><p>We have added their identifications.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Most of the issues raised in the first round of review have been resolved. A few issues remain:</p><p>Line 30: Clarify whether the 10 types described here are in the superficial layer or entire SC.</p></disp-quote><p>We have clarified that they are in the superficial SC.</p><disp-quote content-type="editor-comment"><p>Line 88: How are OSI and DSi incorporated into the distance measure? Are they weighted the same as the other parameters, or reweighted so that they have a larger impact on the distance measure?</p></disp-quote><p>They are weighted the same as the other parameters. All features were normalized in the same way, which is described in line 503 of Methods section 4.5.3.</p><disp-quote content-type="editor-comment"><p>Line 107: Can you state the difference between Figure 3B and 10A briefly in the main text so it is clearer why both are shown?</p></disp-quote><p>Figure 3B only shows values that are significant from zero while Figure 10A shows all values. We have revised the text accordingly.</p><disp-quote content-type="editor-comment"><p>Line 137-138: Suggest stating the expected shape of the autocorrelation if the cells are randomly distributed. This would help interpret the decay in autocorrelation described here.</p></disp-quote><p>We have revised the text accordingly.</p><disp-quote content-type="editor-comment"><p>Line 160: What is &quot;significant spatial separation&quot; – is this a statistical statement?</p></disp-quote><p>Yes. We have added the statistical information to the text and figure legend.</p><disp-quote content-type="editor-comment"><p>Figure 6A: suggest flipping y-axis so that it corresponds to type 1 and type 2 groups in earlier figure. Text labels specifying SC type 1 and type 2 would also help.</p><p>The legends of the supplementary figures could get expanded considerably. At present they are quite brief and some of those figures are difficult to appreciate.</p></disp-quote><p>We have expanded some of these legends. Note also that each supplement figure is clearly associated with a main text figure.</p><disp-quote content-type="editor-comment"><p>The format of the revised paper was inconvenient. The figure legends were embedded in the text, but the figures came at the end. Most importantly, the figures were not in the same order as the figure legends and were not themselves labeled (this had to do with where the supplementary figures ended up). Please provide an easier to read version when you resubmit!</p></disp-quote><p>This format must have been constructed by the submission system by piecing together figures and text. We had also provided a properly formatted PDF file. We did the same on this revision.</p></body></sub-article></article>