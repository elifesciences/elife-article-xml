<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86126</article-id><article-id pub-id-type="doi">10.7554/eLife.86126</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Rodent ultrasonic vocal interaction resolved with millimeter precision using hybrid beamforming</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-262802"><name><surname>Sterling</surname><given-names>Max L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2114-2265</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-327569"><name><surname>Teunisse</surname><given-names>Ruben</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-62308"><name><surname>Englitz</surname><given-names>Bernhard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9106-0356</contrib-id><email>englitz@science.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Computational Neuroscience Lab, Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Visual Neuroscience Lab, Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Department of Human Genetics, Radboudumc, Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>07</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e86126</elocation-id><history><date date-type="received" iso-8601-date="2023-01-11"><day>11</day><month>01</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2023-07-25"><day>25</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2023-01-20"><day>20</day><month>01</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.18.524540"/></event></pub-history><permissions><copyright-statement>© 2023, Sterling et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Sterling et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86126-v4.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86126-figures-v4.pdf"/><abstract><p>Ultrasonic vocalizations (USVs) fulfill an important role in communication and navigation in many species. Because of their social and affective significance, rodent USVs are increasingly used as a behavioral measure in neurodevelopmental and neurolinguistic research. Reliably attributing USVs to their emitter during close interactions has emerged as a difficult, key challenge. If addressed, all subsequent analyses gain substantial confidence. We present a hybrid ultrasonic tracking system, Hybrid Vocalization Localizer (HyVL), that synergistically integrates a high-resolution acoustic camera with high-quality ultrasonic microphones. HyVL is the first to achieve millimeter precision (~3.4–4.8 mm, 91% assigned) in localizing USVs, ~3× better than other systems, approaching the physical limits (mouse snout ~10 mm). We analyze mouse courtship interactions and demonstrate that males and females vocalize in starkly different relative spatial positions, and that the fraction of female vocalizations has likely been overestimated previously due to imprecise localization. Further, we find that when two male mice interact with one female, one of the males takes a dominant role in the interaction both in terms of the vocalization rate and the location relative to the female. HyVL substantially improves the precision with which social communication between rodents can be studied. It is also affordable, open-source, easy to set up, can be integrated with existing setups, and reduces the required number of experiments and animals.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Most animals – from insects to mammals – use vocal sounds to communicate with each other. But not all of these sounds are audible to humans. Frogs, mice and even some primates can produce noises that are ultrasonic, meaning their frequency is so high they cannot be detected by the human ear. These ‘ultrasonic vocalizations’ are used to relay a variety of signals, including distress, courtship and defense.</p><p>To understand the role ultrasonic vocalizations play in social interactions, it is important to work out which animal is responsible for emitting the sound. Current methods have a high error rate and often assign vocalizations to the wrong participant, especially if the animals are in close contact with each other. To solve this issue, Sterling et al. developed the hybrid vocalization localizer (HyVL), a system which detects ultrasonic sounds using two different types of microphones. The tool is then able to accurately locate where an ultrasonic vocalization is emitted from within a precision of millimeters.</p><p>Sterling et al. used their new system to study courtship interactions between two to three mice. The experiments revealed that female courtship vocalizations were substantially rarer than previously reported when two mice were interacting. When three mice were together (one female, two males), Sterling et al. found that one of the male mice typically dominated the conversation. This result was also reflected by the male mouse locating themselves anogenitally to the female, as males tend to vocalize more when in this position.</p><p>In neuroscience, researchers often measure ultrasonic vocalizations to monitor social interactions between rats and mice. HyVL could provide neuroscientists with a more affordable and easier to use platform for conducting these kinds of experiments, which are important for studying behavior and how the brain develops.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ultrasonic vocalizations</kwd><kwd>social interaction</kwd><kwd>vocal communication</kwd><kwd>sound localization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Noldus IT</institution></institution-wrap></funding-source><award-id>DCN Internal Grant</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>NWO VIDI grant</institution></institution-wrap></funding-source><award-id>016.VIDI.189.052</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001826</institution-id><institution>ZonMw</institution></institution-wrap></funding-source><award-id>Technology Hotel Grant 40-43500-98-4141</award-id><principal-award-recipient><name><surname>Englitz</surname><given-names>Bernhard</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The accuracy of the Hybrid Vocalization Localizer (HyVL) brings a revolution to the study of social vocalizations of rodents and other animals, where vocalizations often occur in close proximity, and will empower downstream analysis of sequence and semantic analyses.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Ultrasonic vocalizations (USVs) fulfill an important role in animal ecology as means of communication or navigation in many rodents (<xref ref-type="bibr" rid="bib44">Mahrt et al., 2013</xref>; <xref ref-type="bibr" rid="bib6">Brudzynski, 2021</xref>; <xref ref-type="bibr" rid="bib100">Zaytseva et al., 2019</xref>; <xref ref-type="bibr" rid="bib90">Volodin et al., 2022</xref>; <xref ref-type="bibr" rid="bib53">Murrant et al., 2013</xref>), bats (<xref ref-type="bibr" rid="bib76">Schnitzler et al., 2003</xref>), frogs (<xref ref-type="bibr" rid="bib21">Feng et al., 2006</xref>), cetaceans (<xref ref-type="bibr" rid="bib51">Mourlam and Orliac, 2017</xref>), and even some primates (<xref ref-type="bibr" rid="bib3">Bakker and Langermans, 2018</xref>; <xref ref-type="bibr" rid="bib70">Ramsier et al., 2012</xref>). In many of these species, USVs have been shown to be present innately and to have significance at multiple stages of life, from neonates (<xref ref-type="bibr" rid="bib37">Kikusui et al., 2011</xref>) to adults (<xref ref-type="bibr" rid="bib44">Mahrt et al., 2013</xref>), often with diverse functions as distress/alarm calls (<xref ref-type="bibr" rid="bib37">Kikusui et al., 2011</xref>; <xref ref-type="bibr" rid="bib42">Litvin et al., 2007</xref>), courtship signals (<xref ref-type="bibr" rid="bib46">Marconi et al., 2020</xref>), territorial defense signals (<xref ref-type="bibr" rid="bib71">Rieger and Marler, 2018</xref>), private communication (<xref ref-type="bibr" rid="bib70">Ramsier et al., 2012</xref>), and echolocation (<xref ref-type="bibr" rid="bib76">Schnitzler et al., 2003</xref>). USVs have been extensively studied in mice, where their communicative significance has been widely demonstrated by their influence on conspecific behavior (<xref ref-type="bibr" rid="bib27">Hammerschmidt et al., 2009</xref>; <xref ref-type="bibr" rid="bib68">Pultorak et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Musolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib80">Sugimoto et al., 2011</xref>; <xref ref-type="bibr" rid="bib84">Tschida et al., 2019</xref>; also in line with observational studies; <xref ref-type="bibr" rid="bib95">Warren et al., 2020</xref>; <xref ref-type="bibr" rid="bib57">Nicolakis et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Rieger et al., 2021</xref>; <xref ref-type="bibr" rid="bib63">Petric and Kalcounis-Rueppell, 2013</xref>). USVs can be grouped into different types that are highly context-dependent (<xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Musolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Nicolakis et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Chen et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">de Chaumont et al., 2021</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>; <xref ref-type="bibr" rid="bib69">Pultorak et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Burke et al., 2018</xref>; <xref ref-type="bibr" rid="bib98">Zala et al., 2017a</xref>; <xref ref-type="bibr" rid="bib52">Mun et al., 2015</xref>; <xref ref-type="bibr" rid="bib91">von Merten et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Scattoni et al., 2009</xref>; <xref ref-type="bibr" rid="bib96">Warren et al., 2021</xref>; <xref ref-type="bibr" rid="bib17">Dou et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Hoier et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Chabout et al., 2012</xref>), and USV syntax itself is predictive of USV sequence (<xref ref-type="bibr" rid="bib30">Hertz et al., 2020</xref>). Taken together, the current literature suggests USVs convey affective and social information in different behavioral contexts. This is further supported by the modulatory effect that testosterone and oxytocin have on USV production (<xref ref-type="bibr" rid="bib39">Kikusui et al., 2021b</xref>; <xref ref-type="bibr" rid="bib38">Kikusui et al., 2021a</xref>; <xref ref-type="bibr" rid="bib82">Timonin et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Pultorak et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Guoynes and Marler, 2021</xref>; <xref ref-type="bibr" rid="bib86">Tsuji et al., 2021</xref>; <xref ref-type="bibr" rid="bib85">Tsuji et al., 2020</xref>). Importantly, the neuronal circuitry underlying USVs has recently been identified and is being studied extensively (<xref ref-type="bibr" rid="bib84">Tschida et al., 2019</xref>; <xref ref-type="bibr" rid="bib12">Chen et al., 2021</xref>; <xref ref-type="bibr" rid="bib49">Michael et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Gao et al., 2019</xref>; <xref ref-type="bibr" rid="bib81">Tasaka et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Fröhlich et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Shepard et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Arriaga and Jarvis, 2013</xref>; <xref ref-type="bibr" rid="bib24">Fujita et al., 2012</xref>; <xref ref-type="bibr" rid="bib92">Wang et al., 2008</xref>).</p><p>Because of their social and affective significance and our growing mechanistic understanding, mouse USVs are increasingly being used as a behavioral measure in neurodevelopmental and neurolinguistic translational research (<xref ref-type="bibr" rid="bib16">de Chaumont et al., 2021</xref>; <xref ref-type="bibr" rid="bib91">von Merten et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Fröhlich et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Yang et al., 2021</xref>; <xref ref-type="bibr" rid="bib5">Binder et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Hepbasli et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Agarwalla et al., 2020</xref>; <xref ref-type="bibr" rid="bib83">Tsai et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">Hodges et al., 2017</xref>). Their manipulation and precise measurement not only provide the basis for tackling many fundamental questions but also pave the way, via advanced animal models, for the discovery of essential, novel drug targets for many debilitating conditions such as autism-spectrum disorder (<xref ref-type="bibr" rid="bib83">Tsai et al., 2012</xref>; <xref ref-type="bibr" rid="bib79">Silverman et al., 2010</xref>), Parkinson’s disease (<xref ref-type="bibr" rid="bib14">Ciucci et al., 2009</xref>), stroke-induced aphasia (<xref ref-type="bibr" rid="bib61">Palmateer et al., 2016</xref>), epilepsy aphasia syndromes (<xref ref-type="bibr" rid="bib19">Erata et al., 2021</xref>), progressive language disorders (<xref ref-type="bibr" rid="bib48">Menuet et al., 2011</xref>), chronic pain (<xref ref-type="bibr" rid="bib60">Palazzo et al., 2008</xref>), and depression/anxiety disorders (<xref ref-type="bibr" rid="bib50">Moskal and Burgdorf, 2018</xref>), where ultrasonic vocalizations serve as a biomarker for animal well-being and normal development. Consequently, we expect the scientific importance of mouse USVs to continue to increase in the coming years, highlighting the necessity to advance the methods required for their study. In recent years, substantial advances have been made in USV detection (<xref ref-type="bibr" rid="bib15">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Fonseca et al., 2021</xref>; <xref ref-type="bibr" rid="bib99">Zala et al., 2017b</xref>; <xref ref-type="bibr" rid="bib88">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Chabout et al., 2017</xref>), classification (<xref ref-type="bibr" rid="bib15">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Fonseca et al., 2021</xref>; <xref ref-type="bibr" rid="bib88">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Ivanenko et al., 2020</xref>), and localization (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>; <xref ref-type="bibr" rid="bib28">Heckman et al., 2017</xref>; <xref ref-type="bibr" rid="bib93">Warren et al., 2018a</xref>; <xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>).</p><p>Localization is of particular importance during social interactions, when most USVs are emitted and any meaningful analysis of USV properties rests on a reliable assignment of each USV to its emitter. This task is complex for multiple reasons: (i) most USVs are emitted at close range, (ii) social behavior often requires free movement of the animals, and (iii) USV production is invisible (<xref ref-type="bibr" rid="bib9">Chabout et al., 2012</xref>; <xref ref-type="bibr" rid="bib45">Mahrt et al., 2016</xref>). With reliable assignment, all subsequent analyses can be conducted with substantial confidence concerning each USV’s emitter. Although USVs could in theory be classified and assigned based on their shape (<xref ref-type="bibr" rid="bib46">Marconi et al., 2020</xref>; <xref ref-type="bibr" rid="bib43">Liu et al., 2003</xref>; <xref ref-type="bibr" rid="bib33">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib4">Barnes et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Musolf et al., 2010</xref>), this approach will depend strongly on different behavioral contexts and strains. Recent advances in acoustic localization (<xref ref-type="bibr" rid="bib28">Heckman et al., 2017</xref>; <xref ref-type="bibr" rid="bib93">Warren et al., 2018a</xref>; <xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>) have improved the localization accuracy to 11–14 mm; however, close-up snout–snout interactions – which is when a large fraction of USVs are emitted – require an even higher precision.</p><p>We have developed an advanced localization system for USVs in which is a high-resolution 'acoustic camera' consisting of 64 ultrasound microphones with an array of four high-quality ultrasound microphones. Both systems can individually localize USVs but exhibit rather complementary patterns of localization errors. We fuse them into a hybrid system that exploits their respective advantages in sensitivity, detection, and localization accuracy. We achieve a median absolute localization error of 3.4–4.8 mm, translating to an assignment rate of ~91%. Compared to the previous state of the art (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>; <xref ref-type="bibr" rid="bib93">Warren et al., 2018a</xref>), the accuracy represents a threefold improvement that halves the proportion of previously unassigned USVs. Given the physical dimensions of the mouse snout (ø ~10 mm), this likely approaches the physical limit of localizability for USVs. We successfully apply it to and analyze dyadic and triadic courtship interactions between male and female mice. The comparison of dyadic and triadic interactions is chosen here as courtship interactions in nature are naturally competitive and this comparison is therefore both scientifically relevant and can benefit from high-reliability assignment of USVs. We demonstrate that the fraction of female vocalizations has likely been overestimated in previous analyses due to a lack of precision in sound localization. Further, in the triadic recordings we find that in competitive male–male–female courtship, one male takes a dominant role, which shows in emitting most USVs and also positioning itself more closely to the female abdomen.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We analyzed courtship interactions of mice in dyadic and triadic pairings. The mice interacted on an elevated platform inside an anechoic booth (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>, for details see <italic>‘</italic>Recording setup’). Each trial consisted of 8 min of free interaction while movements were tracked with a high-speed camera (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>), and USVs were recorded with a hybrid acoustic system composed of four high-quality microphones (i.e., USM4) as well as a 64-channel microphone array (Cam64, often referred to as an acoustic camera; see <xref ref-type="fig" rid="fig1">Figure 1C</xref> for raw data samples, green and red dots mark the start and stop times of USVs).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Mice emit ultrasonic vocalizations (USVs) in close proximity during courtship behavior.</title><p>(<bold>A</bold>) Two or three mice of different sexes were allowed to interact freely on an elevated platform. Vocalizations were recorded with four high-quality ultrasonic microphones in a rectangular arrangement around the platform and a 64-channel microphone array ('Cam64') mounted above the platform. The spatial location of the pair was recorded visually with a high-speed camera. The platform was located in an ultrasonically sound-proof and anechoic box and illuminated uniformly using an array of LEDs. (<bold>B</bold>) Sample image from the camera that shows the high contrast between the mice and the interaction platform. The two-letter abbreviations indicate the locations of the four high-quality microphones (F = front, B = back, L = left, R = right). (<bold>C</bold>) Sample spectrograms from the four ultrasonic microphones and the average of all Cam64 microphones for a bout of vocalizations (start/end times marked by green/red dots). The Cam64 microphones are of lower quality than the USM4 microphones, evidenced by the rising noise floor for higher frequencies, affecting very-high-frequency USVs. (<bold>D</bold>) Most USVs in the present paradigm were emitted in close proximity to the interaction partners, with the vast majority within 10 cm snout–snout distance (i.e., ~93 and 72% for dyadic and triadic, respectively).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig1-v4.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Comparison of noise spectra of the two microphone arrays.</title><p>Spectra are computed from the output voltage of the microphones during the same silent stretch of a recording (duration: 1 s, M82,R41, T = [150,151] s), verified to not contain any discernible vocalizations, footsteps, or other discrete sounds. (<bold>A</bold>) The high-quality ultrasonic microphones (gray: 4× individual, black: average) had a rather flat baseline noise level across frequency, with the only exception of an ~10 dB deviation for low frequencies, far below the relevant frequency range contained in USVs (red overlay). For reference, the spectrum was shifted to the average of the power in the 30–50 kHz range (blue overlay), which is listed in the manual as being the ‘input-referred self-noise level,’ corresponding to 18 dB SPL (<ext-link ext-link-type="uri" xlink:href="http://www.avisoft.com/ultrasound-microphones/cm16-cmpa/">http://www.avisoft.com/ultrasound-microphones/cm16-cmpa/</ext-link>). (<bold>B</bold>) The Cam64 micro-electromechanical systems (MEMS) microphones (gray: 64× individual, black: average, <ext-link ext-link-type="uri" xlink:href="https://nl.mouser.com/datasheet/2/720/PB24-1.0%20-%20AKU242%20Product%20Brief-770082.pdf">Akustica AKU242</ext-link>) have a highly frequency-dependent baseline noise across frequency. The peaks at frequencies &lt;50 kHz are less relevant for USV detection, while the substantial rise of the noise level for higher frequencies poses an issue for detecting very high frequencies as their contribution to the recorded signal gets progressively buried in the baseline noise (see <xref ref-type="fig" rid="fig1">Figure 1C</xref> for the effect on the detectability of high-frequency USVs). Since no information was available on the input-referred self-noise level in the technical documentation, we shifted the curve to its minimum. In reality it should be shifted higher to be quantitatively compared with the Avisoft microphone as the latter’s large membrane is expected to outperform the AKU242 at all frequencies. For clarity, the above spectra are not equivalent to the sensitivity of the microphone at different frequencies; however, the baseline noise limits the sensitivity at these frequencies. While in principle a frequency-dependent increase in sensitivity could overcome the baseline noise, this does not seem to be the case (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>, top).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig1-figsupp1-v4.tif"/></fig></fig-group><p>Most USVs were emitted in close proximity in dyadic and triadic pairings (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). Reliably assigning most USVs to their emitter therefore requires a highly precise acousto-optical localization system. The presently developed Hybrid Vocalization Localizer (HyVL) system is the first to achieve sub-centimeter precision, that is, ~3.4–4.8 mm (see <xref ref-type="fig" rid="fig2">Figure 2</xref> for an overview). This accuracy on the acoustic side is achieved by combining the complementary strengths of the USM4 and Cam64 data. The Cam64 data is processed using acoustic beamforming (<xref ref-type="bibr" rid="bib89">Van Veen and Buckley, 1988</xref>), which delivers highly precise estimates (median absolute errors [MAE] = ~4–5 mm), but is not sensitive enough for very-high-frequency USVs (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The USM4 data is analyzed using the previously published SLIM algorithm (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>), which delivers accurate (MAE = ~11–14 mm) and less frequency-limited estimates. The accuracy of SLIM, the previously most accurate ultrasonic localization technique (see ‘Discussion’ for a comparison), is generally lower than that of HyVL, but it makes essential contributions to the overall accuracy of HyVL through the integration of the complementary strength of the two methods/microphone arrays (see <xref ref-type="fig" rid="fig3">Figure 3A and L</xref>, shape of errors). The methods exhibit a complementary pattern of localization errors, which predestines them for high synergy when combined (see below).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Overview of the combined acoustic and visual tracking pipeline.</title><p>(Top) Acoustic tracking of animal vocalizations was enabled by a hybrid acoustic system, which recorded the sounds in the booth using a 64-channel ultrasonic microphone array ('Cam64') and four high-quality ultrasonic microphones ('USM4'). Vocalizations were automatically detected using USM4 data (start/end times marked by green/red dots) and then localized on the platform using both the SLIM algorithm on USM4 data and delay-and-sum beamforming on the corresponding Cam64 data. The Cam64 localization proceeded in two steps: first coarse (10 mm resolution), then fine centered around the coarse peak at 1 mm resolution (30 × 30 mm local window). The local, weighted average (green circle) was then used as the ultrasonic vocalization (USV) origin localized by Cam64. For each USV, the Cam64 localization was chosen if its SNR &gt;5, otherwise the USM4/SLIM estimate was used (for details, see ‘Localization of ultrasonic vocalizations’). (Bottom) Animals were tracked visually on the basis of concurrently acquired videos. Two tracking strategies were employed: (i) manual tracking in the video frames corresponding to the midpoint of USVs in all recordings and (ii) automatic tracking for all frames in dyadic recordings. (i) <italic>Manual visual tracking</italic>: the observer was presented with a combined display of the vocalization spectrogram and the concurrent video image at the temporal midpoint of each USV and annotated the snout and head center (i.e., midpoint between the ears). (ii) <italic>Automatic visual tracking</italic>: started with finding the optimal locations of each marker based on marker estimate clouds produced by <italic>DeepLabCut</italic> (<xref ref-type="bibr" rid="bib47">Mathis et al., 2018</xref>) (DLC) for all frames. Next, these marker positions were assembled into spatiotemporal threads with the same, unknown identity based on a combination of spatial and temporal analysis. Finally, the thread ends still loose were connected based on quadratic spatial trajectory estimates for each marker, yielding the complete track for both mice (see ‘Automatic visual animal tracking’ and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig2-v4.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Spatial accuracy of localizing ultrasonic vocalizations (USVs) during mouse social interaction improves approximately threefold over the state of the art (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>).</title><p>(<bold>A</bold>) The vast majority of USVs is localized with very small errors for both methods, concentrated close to the axes and thus hardly visible, evidenced by the median absolute errors (MAE) for Cam64 (light red line) and SLIM (light green line). The fewer larger errors form an L-shape, emphasizing the synergy of a hybrid approach that compensates for the weaknesses of each method. Location estimates were excluded (gray) if they were &gt;50 mm from either mouse, or the hybrid Mouse Probability Index (MPI) &lt;0.95. (<bold>B</bold>) The hybrid localization system Hybrid Vocalization Localizer (HyVL) (orange) combines the virtues of SLIM and Cam64 enabling the localization of 91.1% of all USVs (light orange), achieving an MAE = 4.8 mm. Cam64 localization (red) alone only includes 74.4% of all USVs, but at an MAE = 4.55 mm (light red). SLIM-based localization (green) only includes 79.8% of all USVs, at an MAE = 14.8 mm (light green, see ‘USV assignment’ for details on the relation between accuracy and selection criteria). (<bold>C</bold>) USVs emitted when all animals were &gt;100 mm apart and a single mouse condition was used to assess the ideal accuracy of HyVL. For the far condition, virtually all USVs (332/339, 97.9%) were assigned at an MAE = 3.79 mm, similarly to the single animal condition (MAE = 3.45 mm, 251/255, 98.4%). (<bold>D, E</bold>) Comparison of actual with estimated snout locations along the X (horizontal; <bold>D</bold>) and Y (vertical; <bold>E</bold>) dimensions indicating strong agreement. Colors indicate peak-normalized occurrence rates. (<bold>F</bold>) Centered overlay of USV localizations relative to emitter snout. Precision is depicted as a circle with a radius equivalent to the median absolute error (green: SLIM; orange: HyVL, all USVs; light orange: HyVL, selected USVs, dark gray: HyVL, when mice &gt;100 mm apart).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig3-v4.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Schematic depiction of the progression of marker localization and identity attribution.</title><p>(<bold>A</bold>) First, unattributed estimate clouds (i.e., not assigned to an animal) for each of the six markers are collected using DeepLabCut (DLC). Within each marker class, all estimates are treated as identical at this point (colors indicate marker class). (<bold>B</bold>) Optimal marker locations are then generated by within-frame, k-means clustering of the estimate clouds, or if that results in an incorrect number of clusters, by probability-weighted averaging of estimate clouds heuristically separated by animal. (<bold>C, D</bold>) Next, all markers undergo a temporal and spatial analysis in tandem, both with the goal of constructing pieces of unattributed tracks. (<bold>C</bold>) In the temporal analysis, small spatiotemporal threads of marker locations are assembled and assessed in terms of speed and acceleration to extend them as much as possible across neighboring frames. (<bold>D</bold>) In the spatial analysis, all marker positions are analyzed spatially on a frame-by-frame basis, grouping markers with the same identity based on a logical combination of anatomically permitted inter-marker distances. (<bold>E</bold>) Finally, complete, attributed tracks are constructed by combining both analyses (see ‘Automatic visual animal tracking’). For actual data, the analysis is less ideal than in the figure: not all markers necessarily have an estimate cloud representing them all the time, nor are the estimates always accurate. The finished tracks were visually checked and corrected if necessary (~10 corrections per trial on average, a major reduction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig3-figsupp1-v4.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Ground truth localization of band-limited noise emitted from a small speaker.</title><p>(<bold>A</bold>) Noise target sounds were presented from 25 locations arranged in a 5 × 5 grid spanning nearly the size of the interaction platform. A speaker was manually placed at each location and 100 repetitions of a band-limited noise sound (50–75 kHz) for three intensities ([70,80,90] dB SPL, measured at the speaker) were presented from a miniature high-quality in-ear driver (Sennheiser IE800, calibrated up to 80 kHz). Due to the physical size of the speaker, its membrane was located ~4 cm above the platform, which was taken into account in the source localization. The video shown was slightly corrected for lens distortions to exhibit orthogonal/parallel lines on the placement grid (see ‘Materials and methods’). (<bold>B</bold>) The accuracy of the Hybrid Vocalization Localizer (HyVL) estimates (orange) at each location (gray dots) was quite similar, after minor, linear rescaling (~2% in both directions) and residual shifting (4.5 mm in x, error bars show x and y [17,83] percentiles around the median). The remaining shifts in, for example, the lower left corner could partly be due to slight misplacements of the speaker. (<bold>C</bold>) Density of estimates centered on known speaker locations. The errors group around the individual locations, while the variance inside the groups is below a single millimeter. This further suggests that the main source of shifts was imperfect placements/orientation of the speaker. (<bold>D</bold>) The HyVL-based source locations estimates were largely dominated by the Cam64 estimates (96.2%) leading to an accuracy of 1.87 mm, compared to 8.57 mm for SLIM and 1.92 for Cam64 alone. (<bold>E</bold>) As expected, the accuracy of localization depended on the sound intensity (p&lt;&lt;0.001 for all methods, Kruskal–Wallis ANOVA). As mice are known to vocalize with ~60–100 dB SPL (<xref ref-type="bibr" rid="bib66">Portfors and Perkel, 2014</xref>), this is in line with our ability to localize most vocalizations during social interactions. We estimate that at the distance to the Cam64 (42.5 cm above the speaker), the sound intensity would have been reduced by ~26 dB.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig3-figsupp2-v4.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Localization results on the basis of automatic tracking from dyadic and triadic recordings.</title><p>(<bold>A–F</bold>) Analogous to <xref ref-type="fig" rid="fig3">Figure 3</xref>. All plotting matched to the manual results for dyadic and triadic tracking in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig3-figsupp3-v4.tif"/></fig></fig-group><p>For each USV, a choice is made between the USM4/SLIM and Cam64/Beamforming estimates based on a comparison of each method’s USV-specific certainty and the relative position of the mice to the estimates, using an extended, hybrid Mouse Probability Index (MPI; <xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>). HyVL is the first system of its kind that exploits a hybrid microphone array to overcome the limitations of each subarray. The positions of the mice are obtained via manual and automatic video tracking using <italic>DeepLabCut</italic> (<xref ref-type="bibr" rid="bib47">Mathis et al., 2018</xref>), each of which achieve millimeter precision for localizing the snout.</p><p>Overall, 228 recordings were collected from 14 male and 4 female mice (153 dyadic, 67 triadic, and 8 with a single mouse). In 90 recordings, USVs were produced and recorded with Cam64 and USM4 simultaneously (55 dyadic, 28 triadic, and 7 single). The single mouse recordings were also used in a previous publication (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>) where only the SLIM accuracy was evaluated. A total of 112 recordings were recorded in a balanced design (four dyadic and four triadic per male mouse paired with all females) and the remaining recordings conducted with good vocalizers to maximize the number of USVs for downstream analysis. In all trials combined, 13714 USVs were detected.</p><sec id="s2-1"><title>Precision of USV localization</title><p>Assigning USVs to individual mice required combining high-speed video imaging with the HyVL location estimates at the times of vocalization. We manually tracked the animal snouts at the temporal midpoint of each USV to obtain near-optimal position estimates (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). We first assessed the relative structure of the localization errors between both methods, USM4/SLIM (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, green) and Cam64/Beamforming (<italic>red</italic>, each dot is a USV). While most errors were small, and clustered close to the origin of the graph (evidenced by the small MAE, shown as horizontal and vertical lines, respectively), the less frequent, larger errors exhibited an L-shape. This error pattern is an optimal situation for combining estimates from the two methods, to compensate for each other’s limitations. While the Cam64 data can compensate for single microphone noise through the large number of microphones, the nature of its micro-electromechanical systems (MEMS) microphones deteriorates for very high frequencies (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Conversely, the USM4 microphones show an excellent noise level across frequencies (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>) but can produce erroneous estimates if there is noise in a single microphone and have an intrinsic limitation in spatial accuracy due to the physical size of their receptive membrane (ø ~20 mm).</p><p>We therefore designed an analytical strategy to combine the estimates of both systems to optimize the number of reliably assignable USVs, while evaluating the resulting spatial accuracy alongside. Briefly, the location estimates of both methods each come with an estimate of localization uncertainty. First, we assess for each method’s estimate how reliably it can be assigned to one of the mice, taking into account the positions of the other mice. This is quantified using the MPI (<xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>), which compares the probability of assignment to a particular mouse to the sum of probabilities for all mice, weighted by the estimate’s uncertainty. If the largest MPI exceeds 0.95, it is considered a reliable assignment to the corresponding mouse. If both methods allowed reliable assignments, the one with smaller residual distance was chosen. If only one method was reliable for a particular USV, its estimate was used. If neither method allowed for reliable assignment, the USV was not used for further analysis. This typically happens if the snouts are extremely close or the USV is very quiet. This approach outperformed many other combination approaches in accuracy and assignment percentage, for example, maximum likelihood (see ‘‘Assigning USVs’ and ‘Discussion<italic>’</italic> for details).</p><p>Analyzing all courtship vocalizations, HyVL performed significantly better than either method alone (see <xref ref-type="fig" rid="fig3">Figure 3</xref>), allowing a total of 91.1% of USVs to be assigned at a spatial accuracy of 4.8 mm (MAE). This constitutes a substantial 2.9-fold improvement in accuracy over the previous state of the art, the SLIM algorithm (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>). On the full set of USVs where both microphone arrays were recording (N = 7982), HyVL outperformed both USM4/SLIM and Cam64/Beamforming significantly, both in residual error (SLIM: 14.8 mm; Cam64: 5.33 mm; HyVL: 5.08 mm; p&lt;10<sup>–10</sup> for all comparisons, Wilcoxon rank-sum test) and percentage of reliably assigned USVs (SLIM: 74.4%; Cam64: 79.8%; HyVL: 91.1%). Cam64/Beamforming performed even more precisely on its reliably assignable subset (4.55 mm), which was, however, smaller than the HyVL set. This difference emphasizes the complementarity of the two methods and thus the synergy through their combination. There was no significant difference between tracking on dyadic and triadic recordings (HyVL: 5.0 mm vs. 5.1 mm, p=0.71, Wilcoxon rank-sum test) with correspondingly similar selection percentages (92 vs. 90%, respectively).</p><p>The accuracies above are an average over localization performance at any distance. In particular during close interaction, USVs will often be reflected or obstructed, complicating localization. While this constitutes the realistic challenge during mouse social interactions, we also investigated the 'ideal', unobstructed performance of HyVL by comparing the performance on USVs emitted when all animals were 'far' (&gt;100 mm) apart, that is, &gt;~20 times the average accuracy of HyVL, as well as for a single male mouse on the platform. For the <italic>far</italic> USVs, the reliably assignable fraction increased to 97.9%, and the accuracy significantly improved to 3.79 mm (<xref ref-type="fig" rid="fig3">Figure 3C</xref> gray<bold><italic>,</italic></bold> p=8.6 × 10<sup>–7</sup>, Wilcoxon rank-sum test). For the <italic>single animal</italic> USVs, the accuracy was even better at 3.45 mm with 98.4% reliably assigned (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, blue). In addition, we evaluated HyVL’s performance on sounds emitted from a miniature speaker placed in a regular grid of locations (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). In this condition, the accuracy was even higher (1.87 mm, or even ~0.5 mm when correcting for experimental factors, see figure caption); however, given the differences in the emitter characteristics, emitted sounds and lack of adsorption, this should be treated as a lower bound that will be hard to achieve with mice.</p><p>Next, we inspected separate localization along the X and Y axis to check for anisotropies of localization (<xref ref-type="fig" rid="fig3">Figure 3D/E</xref>, histograms normalized to maximum). The position of the closest animal aligned precisely with the estimated position in both dimensions, indicated by the high density along the diagonal (<italic>Pearson r</italic> &gt; 0.99 for both dimensions) and the MAE’s along the X and Y direction separately (X = 3.1 mm, Y = 2.8 mm). These one-dimensional accuracies might be of relevance for interactions where movement is restricted.</p><p>Lastly, we visualized the localization density relative to the mouse that the vocalization was assigned to (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). Combining both dimensions and appropriately rotating them, the estimated position of the USVs is shown relative to the mouth. The density is narrowly centered on the snout of the mouse (circle radius = MAE: green: SLIM method; orange: HyVL; light orange: HyVL assigned USVs; gray: far assigned USVs).</p><p>In summary, the HyVL system provides a substantial improvement in the localization precision. In comparison to other methods, its precision also allows a larger fraction of vocalizations to be reliably assigned and retained for later analysis, which enables a near complete analysis of vocal communication between mice or other vocal animals (see ‘Discussion<italic>’</italic> for details).</p></sec><sec id="s2-2"><title>Sex distribution of vocalizations during social interaction</title><p>Courtship interactions between mice lead to high rates of vocal production, but are challenging due to the relative proximity, including facial contact. Previous studies using a single microphone have often assumed that only the male mouse vocalized (<xref ref-type="bibr" rid="bib73">Rotschafer et al., 2012</xref>; <xref ref-type="bibr" rid="bib13">Choi et al., 2011</xref>; <xref ref-type="bibr" rid="bib64">Pomerantz and Clemens, 1981</xref>; <xref ref-type="bibr" rid="bib58">Nunez et al., 1978</xref>), while more recent research has concluded that female mice vocalize as well (<xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>; <xref ref-type="bibr" rid="bib74">Sangiamo et al., 2020</xref>). Female vocalizations were typically less frequent, but constituted a substantial fraction of the vocalizations (11–18%) (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>; <xref ref-type="bibr" rid="bib28">Heckman et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>; <xref ref-type="bibr" rid="bib94">Warren et al., 2018b</xref>). Below, we demonstrate that the accuracy of the localization system can be an important factor for conclusions about the contribution of different sexes to the vocal interaction.</p><p>Over all dyadic and triadic trials combined, females produced the minority of vocalizations. Naive estimation without MPI selection using SLIM estimates ~14%, while HyVL tallies it at just 7% (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Applying MPI selection, SLIM estimates only 5.5%, while HyVL arrives at significantly less, just 4.4% (p=0.002, paired Wilcoxon signed-rank test, <xref ref-type="fig" rid="fig4">Figure 4A/B</xref>), while reliably classifying 91.1% of all vocalizations.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Analysis of sex-dependent vocalizations can depend on localization accuracy.</title><p>(<bold>A</bold>) Female vocalizations constitute a small fraction of the total set of vocalizations. The female fraction further reduces with increased precision and when selecting vocalizations based on the Mouse Probability Index (MPI). Vocalization fractions are separated by sex, not by individual mouse. Fractions include all dyadic and triadic recordings with ultrasonic vocalizations (USVs) (N = 83), same for all other panels. (<bold>B</bold>) Using the hybrid method instead of SLIM significantly reduces the fraction of female vocalizations, suggesting that less accurate algorithms overestimate the female fraction (only results for MPI-selected USVs shown). (<bold>C</bold>) The fraction of female vocalizations further reduces if only USVs are considered that are emitted while all animal snouts were &gt;50 mm apart from each other. This indicates a preference of female mice to vocalize in close snout–snout contact; however, this entails that female vocalizations are more prone to confusion with male vocalizations due to their relative spatial occurrence. (<bold>D</bold>) There was no difference in the female fraction of USVs between dyadic and triadic pairings (two male and two female conditions combined here; N<sub>Dyadic</sub> = 55, N<sub>Triadic</sub> = 28). (<bold>E</bold>) High-accuracy localization of USVs allows one to analyze the relative spatial vocalization preferences of the mice, that is, their occurrence density in relation to the relative position of other mice to the emitter. We quantified this by collecting the position of the nonvocalizing mice at the times of vocalization, in relation to the vocalizing mouse. Symbol α corresponds to the angle between the emitter’s snout and the snout of other mice. (<bold>F</bold>) Female mice appear to emit vocalizations in very close snout–snout contact, with a small fraction of vocalizations also occurring when the male mouse around the hind-paws/ano-genital region. (<bold>G</bold>) Male mice emit vocalizations both in snout–snout contact, but also at greater distances, which dominantly correspond to a close approach of the male’s snout to the female ano-genital region. This was verified separately with a corresponding analysis, where the recipient’s tail-onset was used instead (not shown). (<bold>H</bold>) Radial distance density of receiver animals, marginalized over directions, shows a significant difference, with females vocalizing mostly when males (blue) are in close proximity of the snout, while males vocalize when the female mouse’s snout is very close (corresponding to snout-snout contact), but also when the female’s snout is about 1 body length away (snout–ano-genital interaction). Plots show means and SEM confidence bounds. (<bold>I</bold>) Direction density of receiver animals, marginalized over distances, shows that female mice vocalize primarily when the male mouse’s snout is very close and in front of them. Note that the overall angle of approach of the male mouse is not from directly ahead (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig4-v4.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Relative spatial vocalization preferences relative to receiver’s ano-genital region for dyadic recordings.</title><p>(<bold>A</bold>) The male abdomen is found in a range of different relative locations to the female's snout. (<bold>B</bold>) A large fraction of the male vocalizations are emitted when the female mouse's abdomen is very close to the male’s snout. (<bold>C</bold>) Radial distance density of receiver animals, marginalized over directions, shows a significant difference, with males vocalizing mostly (red) when the female abdominal region is in close proximity of their snout (corresponding to snout-ano-genital contact), while females vocalize when the male’s abdomen is relatively far from their snout. Plots show means and SEM confidence bounds. (<bold>D</bold>) Direction density of receiver animals, marginalized over distances, shows that male mice vocalize (red) primarily when the female’s abdomen is directly in front of them.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig4-figsupp1-v4.tif"/></fig></fig-group><p>Using HyVL instead of SLIM significantly reduces the fraction of female vocalizations, suggesting that less accurate algorithms overestimate the female fraction (only results for MPI-selected USVs shown, <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Considering only vocalizations that are emitted when the snouts are &gt;50 mm apart further significantly reduces the fraction to female USVs to 1.1% after MPI selection (p=5.2 × 10<sup>–8</sup>, Wilcoxon rank-sum test). Comparing the percentage of female vocalizations between dyadic and triadic trials, no significant differences were found (p=0.22, Wilcoxon rank-sum test, <xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>Beyond the absolute distance between the mouths of the mice, high-accuracy localization of USVs allows one to position the bodies of the animals relative to one another at the times of vocalization by combining acoustic data with multiple concurrently tracked visual markers. This provides an occurrence density of other mice relative to the emitter (<xref ref-type="fig" rid="fig4">Figure 4E</xref>).</p><p>Female mice appear to emit vocalizations in very close snout–snout contact, with a small fraction of vocalizations occurring when the male snout is around the hind-paws/ano-genital region (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Male mice emit vocalizations both in snout–snout contact, but also at greater distances, which dominantly correspond to a close approach of the male’s snout to the female ano-genital region (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). This was verified separately with a corresponding analysis, where the recipient’s tail-onset was used instead (not shown).</p><p>In summary, the combination of high-precision localization and selection using the MPI indicates that female vocalizations may be even less frequent than previously thought. When they vocalize, the mice appear to almost exclusively be in close snout–snout contact. As this is incidentally also the condition that has the highest chance of mis-assignments, even the remaining female vocalizations need to be treated with caution.</p></sec><sec id="s2-3"><title>Vocalization rate analysis</title><p>In dyadic trials, one female and one male mouse interacted, whereas in triadic trials either two males and one female or two females and one male mouse interacted. We first address in dyadic trials, whether there were significant differences in individual vocalization rates between the mice. For the balanced dataset of 14 × 4 dyadic interactions (pairing of all males with all females), we did not find a significant effect of individual on vocalization rates for either male and female mice (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, p=0.46 and p=0.16, respectively, one-way ANOVA analysis with factor individual, for n = 4 recordings in males and n = 14 recordings in females). For triadic trials, we could not perform the corresponding analysis since the two male/female recordings could not be distinguished reliably in post hoc tracking.</p><p>In the balanced dyadic and triadic datasets, only 23/112 recordings contained vocalizations. We collected additional dyadic and triadic recordings for the purpose of maximizing the number of USVs, both for assessing HyVL performance and comparing dyadic and triadic interactions. In this enlarged dataset, a total of 83 recordings (55 dyadic, 28 triadic) were available, which contained USVs. This dataset was still balanced for female mice, but, unbalanced for male mice, that is, although the same mice participated in both dyadic and triadic recordings, however, not with exactly the same number of recordings. While the analysis on the balanced dataset above did not suggest significant differences between individuals, we thus cannot fully exclude that the reported differences below are partially due to individual differences between some male mice.</p><p>In the analysis of triadic interactions, we separate competitive and alternative contexts depending on whether a mouse had to compete with another same sex mouse or could interact with two opposite sex mice, respectively. For triadic trials we further separate the same-sex mice into dominant and subordinate, based on who vocalized more.</p><p>However, in competitive interactions between males, one male mouse significantly and strongly dominated the 'conversation,' with on average ninefold more vocalizations than the other male mouse (T<sub>D</sub> vs. T<sub>s</sub>, <xref ref-type="fig" rid="fig5">Figure 5A and B</xref>, both comparisons: p&lt;0.005 [Wilcoxon sum of ranks test]) after Bonferroni correction. Specifically, Bonferroni correction was conducted per panel/measured variable on the basis of the number of hypotheses actually tested for, that is, six tests per panel, three for each sex: dyadic vs. triadic; triadic: dominant vs. subordinate; triadic: competitive vs. alternatives. While the present division into dominant and subordinate mouse based on a higher vocalization rate within a recording will always lead to a significant difference, the quantitative difference between them is the striking aspect in this comparison. Overall male vocalization rates were similar in competitive and alternative triadic trials. Female vocalization rates were similar across all compared conditions.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>In triadic interaction, one male vocalizes dominantly and males vocalize even closer to females.</title><p>(<bold>A</bold>) Overall, vocalization rates were comparable between dyadic (D) and triadic (T) conditions. Male mice (blue) vocalized at higher rates than female mice (red). However, this was restricted to the dominant male mouse (T<sub>D</sub>: dominant = emitted more ultrasonic vocalizations [USVs] within same-sex) in triadic, competitive (2 m/1 f) conditions (see text for all p-values). Male vocalization rates were similar in competitive (T<sub>C</sub>: with same-sex competitors) and alternative (T<sub>A</sub>: no same-sex competitor, i.e., for male vocs: 2 f/1 m) pairings. Female vocalization rates remained low and similar across all conditions. T<sub>S</sub>: submissive mouse = emitted fewer USVs within same sex during competitive trial; white dot: median; horizontal bar: mean (N = 83 recordings in all panels, in the groupings D/T vocalizations are grouped by sex, whereas in T<sub>D,S,C,A</sub> USVs are per individual, same in panels <bold>B–D</bold>). (<bold>B</bold>) While the fraction of USVs emitted by males was overall comparable between D and T pairings, the dominant male (T<sub>D</sub>) emitted a substantially larger fraction than their submissive counterpart (T<sub>S</sub>), roughly a factor of 9. In competitive pairings, male mice tended to emit an overall larger fraction of all USVs than in alternative pairings (T<sub>C</sub> vs. T<sub>A</sub>), but this is unsurprising as both males vocalize. In female mice, the overall fraction of USVs in D and T pairings was also similar (see details in ‘Results’ for potential caveats of the dominant/subordinate classification). (<bold>C</bold>) In triadic pairings, dominant male mice tended to vocalize more intensely than in dyadic pairings; however, this difference was not significant at the current sample size. No significant differences were found for female mice. (<bold>D</bold>) Male mice emitted USVs in closer proximity to the closest female mouse in triadic compared to dyadic interactions. Female mice generally emitted USVs at closer distances (see also <xref ref-type="fig" rid="fig4">Figure 4F/H</xref>), in particular for alternative vs. competitive pairings.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig5-v4.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Vocalization rates in dyadic recordings based on a balanced set of four recordings per male mouse and condition (n = 14 male mice, 112 recordings).</title><p>Vocalization rate did not depend significantly on individual for males (<bold>A</bold>, p=0.46, one-way ANOVA with factor individual, n = 4 recordings per animal) or females (<bold>B</bold>, p=0.16, one-way ANOVA with factor individual, n = 14 recordings per animal).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig5-figsupp1-v4.tif"/></fig></fig-group><p>The mean vocalization energy of dominant males in triadic pairings tends to be higher than those of submissive males in triadic pairings; however, this result did not reach significance in the present dataset (see <xref ref-type="fig" rid="fig5">Figure 5C</xref>). No effects of vocalization energy were found in females.</p><p>The distance to the closest animal of the <italic>opposite sex</italic> was found to be even closer during triadic trials (see <xref ref-type="fig" rid="fig5">Figure 5D</xref>), driven purely by male vocalizers (p=0.00046, after Bonferroni correction as above, Wilcoxon sum of ranks test): the distance to the closest animal does not change between conditions for vocalizing females (p=0.975, Wilcoxon sum of ranks test). Interestingly, the distance to the closest animal was larger for females at the time of vocalization when they had a same-sex competitor on the interaction platform with them than when they were the only female (T<sub>c</sub> vs. T<sub>a</sub>, p=0.0068, Wilcoxon sum of ranks test).</p><p>Lastly, we investigated whether the division into a dominant and subordinate male based on the vocalization rate was also reflected in the spatial behavior of the male mice relative to the female mouse. For this purpose, we again constructed relative spatial interactions histograms (see <xref ref-type="fig" rid="fig6">Figure 6</xref>, analogous to <xref ref-type="fig" rid="fig4">Figure 4</xref>), separately for USV-rate-dominant and subordinate males. The results are displayed as the relative location between the male snout and the female abdomen. Dominant males spent more time close to the female abdomen, thus engaging in ano-genital contact (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, center), in comparison with subordinate males (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). This is highlighted in the difference between the spatial interaction histograms (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), where the most salient dominant peak occurs in the center, while the subordinate male spent more time in snout–snout contact, indicated by the blue arc at about one mouse body length from the center (shown in blue here). These differences were significant, in addition to a number of other locations in the spatial interaction histogram. Significance analysis was performed using 100× bootstrapping on the relative spatial positions to estimate p=0.99 confidence bounds around the histograms of the dominant and subordinate, respectively. Significance at a level of p&lt;0.01 highlights multiple relative spatial positions.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Dominant male animals spend more time close to the female’s abdomen.</title><p>(<bold>A</bold>) The abdomen of the female was typically close to the <italic>dominant</italic> male’s snout (center of plot), with a ring of approximately one mouse length also visible deriving from snout–snout interactions. The histogram was created based on all-frame tracking of the 14 triadic interactions with two male mice using skeleton tracking in SLEAP over a total of N = 276,358 frames. Dominant and subordinate males were defined based on their vocalization rate per recording. Each histogram was peak normalized. (<bold>B</bold>) For the <italic>subordinate</italic> male, the histogram was less peaked around the proximal snout–abdomen interactions, but showed a more visible arc between 90 and 180°, pointing to snout–snout interactions. (<bold>C</bold>) The difference between the two histograms (each density-normalized to a sum of 1) shows the focused snout–abdominal interactions for the dominant male, and the arc pointing to snout–snout interactions for the subordinate male, in addition to smaller absolute differences in other relative locations. (<bold>D</bold>) Spatial regions of significant difference between the dominant and subordinate male were found both in the regions highlighted in (<bold>C</bold>), as well as more distant regions. Significance was assessed by bootstrapping confidence bounds on the histograms of the dominant and subordinate males (based on relative locations, rebuilding the histogram, 100×). The distance to the most extreme values were taken as the limits for significant deviation at p&lt;0.01, and the difference in (<bold>C</bold>) was then compared in both the positive/negative direction against these bounds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86126-fig6-v4.tif"/></fig><p>In summary, in competitive triadic interactions, one of the male mice took a strongly dominant role, evidenced both in the vocalization rate and the more abundant ano-genital interactions with the female throughout the recordings. In triadic interactions, the female mouse was generally approached more closely by a male mouse, in particular in the alternative condition. The latter could, however, be a consequence of the larger number of male animals on the platform compared to dyadic and triadic competitive (from the perspective of the female).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have developed and evaluated a novel, hybrid sound localization system (HyVL) for USVs emitted by mice and other rodents. USVs are innately used by rodents to communicate social and affective information and are increasingly being used in neuroscience as a behavioral measure in neurodevelopmental and neurolinguistic research. In the context of dyadic and triadic social interactions between mice, we demonstrate that HyVL achieves a groundbreaking increase in localization accuracy down to ~3.4–4.8 mm, enabling the reliable assignment of &gt;90% of all USVs to their emitter. Further, we demonstrate that this can be combined with automatic tracking, enabling a near-complete and automated analysis of vocal interaction between rodents. The showcased analyses demonstrate the advantages obtained through more precise localization, further discussed below. HyVL is based on an array of high-quality microphones in combination with a commercially available, affordable acoustic camera. With our freely available code, this system can be readily reproduced by other researchers and has the potential to revolutionize the study of natural interactions of mice.</p><sec id="s3-1"><title>Comparison with previous approaches for localizing vocalizations</title><p>Localization accuracy was first systematically reported by <xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref> using a four-microphone setup and a maximum likelihood approach (<xref ref-type="bibr" rid="bib101">Zhang et al., 2008</xref>), who attained an MAE of ~38 mm that conferred an assignment rate of 14.6–18.1% (their Table 1, <italic>assigned</italic> relative to <italic>detected</italic> or <italic>localized</italic>). Originating from the same research group, <xref ref-type="bibr" rid="bib93">Warren et al., 2018a</xref> employed both a four- and eight-microphone setup in a follow-up study, achieving an MAE of ~30 mm for four microphones (~52% assignment rate) and ~20 mm with eight microphones (~62% assignment rate), both using a jackknife approach to increase robustness of localization. <xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref> introduced the SLIM algorithm, reaching an MAE of ~11–14 mm (~80–85% assignment rate depending on the dataset) using four microphones. Presently, we advance the state of the art in multiple ways: we use 68 microphones, combining a 64-channel 'acoustic camera' with four high-quality ultrasonic microphones. While the acoustic camera has relatively basic MEMS microphones, it is inexpensive and features a high degree of integration and correspondingly easy operation. Combining the complementary strengths of the two arrays is the key advantage of the present approach over previous approaches as it allows for a quantum leap in accuracy (3.4–4.8 mm, 91% assignment rate), while keeping the complexity of the system manageable. A comparable alternative might be a 16-channel array from high-quality microphones, which would, however, be substantially more expensive (~€40,000) as well as cumbersome to build and refine. A future generation of MEMS microphones might make the use of the high-quality microphones unnecessary and thus further simplify the system setup, allowing for inexpensive, small-form factor deployment (see below).</p></sec><sec id="s3-2"><title>Expected impact for future research</title><p>Mice and rats are social animals (<xref ref-type="bibr" rid="bib77">Shemesh et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Lee and Beery, 2019</xref>), and isolated housing (<xref ref-type="bibr" rid="bib35">Kappel et al., 2017</xref>) or testing (<xref ref-type="bibr" rid="bib40">Kondrakiewicz et al., 2019</xref>) can affect subsequent research outcomes. Social isolation also has direct effects on the number and characteristics of USVs, at least in males (<xref ref-type="bibr" rid="bib36">Keesom et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Portfors, 2007</xref>). <xref ref-type="bibr" rid="bib74">Sangiamo et al., 2020</xref> demonstrated that distinct USV patterns can be linked to specific social actions and the latter that locomotion and USVs influence each other in a context-dependent way. Using HyVL, such analyses could be extended to more close-range behaviors, when a substantial fraction of the vocalizations are emitted (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). The development of more unrestricted behavioral paradigms, made viable by increased localization precision, will thus also likely prove valuable to the fields of human language impairment and animal behavior. As an added benefit, better USV localization will also likely increase lab animal well-being via (i) more social contact in specific cases where they spend much time with their conspecifics in the testing environment, or when the home environment is the testing environment (e.g., PhenoTyper; Noldus Information Technologies), and (ii) a reduced need for (non-)invasive markers.</p><p>Here, we conducted a limited set of showcase analyses on the spatial characteristics of vocalization behavior. As expected, the system was accurate enough to assign vocalizations during many snout–snout interactions as well as other, slightly more distant interactions, for example, snout contact with the ano-genital region of the dyadic partner. We found the male mice to vocalize most while making snout contact with the abdomen and ano-genital region of the female wild-type. Females vocalized predominantly during snout–snout contact, with the male’s snout in front of the female mouse’s snout.</p><p>This highlights an example of how localization accuracy can shape our understanding of roles in social interaction between mice: a recent, pivotal study (<xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>) demonstrated that female mice vocalize during courtship interactions. Research from our group (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>) concluded further that mice primarily vocalize in snout–snout interactions, incidentally the condition that makes assignment the most difficult. While the present results maintain that female mice vocalize, the fraction appears to be lower than previously thought. We, however, emphasize that this conclusion still requires further study under different social contexts, for example, interaction of more mice as in some of the previous studies (<xref ref-type="bibr" rid="bib96">Warren et al., 2021</xref>; <xref ref-type="bibr" rid="bib74">Sangiamo et al., 2020</xref>).</p><p>The compact form factor of the HyVL microphone arrays, in particular the Cam64, enables studies of social interaction in home cages. There, rodents are less stressed and likely to exhibit more natural behavior, in particular if the home cage includes enrichments. The relatively low hardware costs for HyVL allows deployment of multiple systems to cover larger and more natural environments. Research in animal communication with other species could also benefit from use of HyVL, for example, with different insects or other vocal animals, as there is little reason to suspect that the performance of HyVL would not extend to lower frequencies. Flying animals, such as bats or birds, could also be studied; however, the subsequent data analysis would have to be extended by one dimension.</p></sec><sec id="s3-3"><title>Current limitations and future improvements of the presented system</title><p>The millimeter accuracy by HyVL enables the assignment of USVs even during close interaction, certainly including all snout–ano-genital interactions, and many snout–snout interactions. However, certain snout–snout interactions are still too close to reliably assign co-occurring USVs. While the MPI criterion maintains reliability even then, subsequent analysis will be partially biased due to the exclusion of these USVs during the closest interactions. While a further improvement of accuracy may be possible, close inspection of the sound density maps available via beamforming from the Cam64 recordings suggests that the mouse’s snout acts as a distributed source: the sound density is rather evenly distributed on it, without a clear internal peak. During free interaction, we noticed that the sound density was co-elongated with the head direction of the mouse and could thus be used as an additional feature to identify the vocalizer. However, this proved unreliable during close interaction, likely due to absorption and reflection of sounds based on the mice’s bodies. More advanced modeling of the local acoustics or deep learning might be able to resolve these issues by analyzing interactions where one mouse is known to be silent, for example, by cutting the laryngeal nerves.</p><p>The present strategy for combining the estimates from Cam64/Beamforming and USM4/SLIM was chosen as it optimized the reliably assigned percentage of USVs, while minimizing the residual distance. We also tested alternative approaches, for example, using direct beamforming on the combined data from Cam64 and USM4 (unreliable estimates due to mismatch of number of microphones, not further pursued), maximum likelihood combination of estimates (MAE = 7.1 mm) (<xref ref-type="bibr" rid="bib20">Ernst and Banks, 2002</xref>) and making the selection solely depend on the MPI (MAE = 5.2 mm). While each of these approaches has certain, theoretically attractive features, the results were worse in each case, likely due to particular idiosyncrasies of the MPI computation, the different microphone characteristics, and the estimation of single-estimate uncertainty.</p><p>A small set of vocalizations was not assigned solely due to the overall proximity threshold of 50 mm (see ‘Materials and methods,’ 2.9%). We have previously shown that very quiet or very short USVs are, unsurprisingly, harder to detect and localize (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>). In addition, spectrally narrow and acoustically occluded USVs are likely hard to localize: USVs that are spectrally very narrow – that is, close to a pure tone – will have phase ambiguity, which will make it hard to assign a single location. USVs that are acoustically occluded – for example, an animal vocalizing away from a microphone, or a mouse body in the path of the sound – will have a reduced signal-to-noise ratio (SNR) on one or more microphones. In our experience, the latter two affect the Mic4 data more than Cam64 due to their different placement relative to the platform.</p><p>A very small percentage of vocalizations (&lt;0.1%) contained multiple, differently shaped vocalization traces that, when reanalyzed in shortened time-frequency bins with beamforming, could be assigned to two different males. Such overlapping vocalizations did not form a harmonic stack. Overall, overlaps were surprisingly rare and only occurred when our USV detection algorithm produced a longer interval, affecting the cumulative heatmap because beamforming is separately performed from the onset to the end of each vocalization. Although the identity of the assigned vocalizer could shift in these very rare cases depending on which time bin was reanalyzed, the system’s localization performance remained in principle unaffected: as mentioned above, shorter time bins on nonoverlapping parts correctly show the origin of the vocalizations in this case, and we think that improved USV detection/separation based on the harmonic structure will partially address this issue. During the beamforming, each vocalization can then be separately localized by restricting the beamforming to the corresponding time <italic>and</italic> frequency range. Further, the beamforming analysis could be refined so that multiple salient peaks can be detected in the soundfield estimate, for example, a sequence of soundfield estimates would be computed on shorter segments of data and later fused again. As this uses less data per single estimate, it also increases the possibility of false positives, which in the current situation with very few overlaps in time would likely reduce the overall accuracy of the system. Lastly, for the present data, if a time window was analyzed such that the intensity map of the sound field contains multiple hotspots of an approximately equal magnitude, the USV would likely remain unassigned because the within-soundfield uncertainty would be higher than for a single peak, and this would reduce the MPI. However, given the rarity of these cases in our dataset, we do not think that their exclusion would change the results appreciably.</p><p>Lastly, for the purpose of online feedback during experiments and to reduce data warehousing, it would be advantageous to perform the localization of USVs in real time. This would be enabled by streaming the data to a GPU, performing localization immediately and keeping only a single channel, beamformed estimate of each USV. Ideally, the same device could run visual tracking simultaneously, which would remove all temporal limitations on the recordings in terms of data size and enable continuous audiovisual tracking.</p></sec><sec id="s3-4"><title>Conclusion and outlook</title><p>HyVL delivers breakthrough accuracy and assignment rates, likely approaching the physical limits of assignment. The low system costs (&lt;€10k) in relation to its performance make HyVL an excellent choice for labs studying rodent social interaction. Many recent questions regarding the sequencing of vocalizations during social interactions become addressable with HyVL without intrusive interventions. Its use can both refine the precision and reliability of the analysis, while reducing the number of animals required to complete the research due to a larger fraction of assigned USVs per animal.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Transfected construct (<italic>Mus musculus</italic>)</td><td align="left" valign="bottom">Foxp2<italic><sup>flox/flox</sup></italic>;Pcp2<italic><sup>Cre</sup></italic></td><td align="left" valign="bottom">Bred locally at animal facility</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><p>All experimental procedures were approved by the animal welfare body of the Radboud University under the protocol DEC-2017-0041-002 and conducted according to the Guidelines of the National Institutes of Health.</p><sec id="s4-1"><title>Animals</title><p>In our experiment, four female C57Bl/6J-WT, six male C57Bl/6J-WT, and eight male C57Bl/6J-Foxp2<italic><sup>flox/flox</sup></italic>;Pcp2<italic><sup>Cre</sup></italic> mice (bred locally at the animal facility) were studied. For subsequent analyses, WT and KO mice were combined (see beginning of ‘Results’ for reasoning). The mice were 8 weeks old at the start of the experiments. After 1 wk of acclimation in the animal facility, the experiments were started. Mice of the same sex were housed socially (2–5 mice per cage) on a 12 hr light/dark cycle with ad libitum access to food and water in individually ventilated, conventional EU type II mouse cages at 20°C with paper strip bedding and a plastic shelter for basic enrichment. Upon completion of the experiments, the animals were anesthetized using isoflurane and sacrificed using CO<sub>2</sub>.</p><p>The current experiment was performed as an add-on to an existing set of experiments, whose focus included a region-specific knockout of <italic>Foxp2</italic> in the cerebellar Purkinje cells of the male mice, denoted as Foxp2<italic><sup>flox/flox</sup></italic>;Pcp2<italic><sup>Cre</sup></italic>. Neither previous work nor our own work has detected any differences in USV production between WT and KO animals (<xref ref-type="bibr" rid="bib87">Urbanus et al., 2020</xref>), so – given the mostly methodological focus of the present work – we considered it acceptable to pool them in the current analysis, reducing the number of animals needed, thus treating all males as WT C57Bl/6J, the genotype of the female mice.</p></sec><sec id="s4-2"><title>Recording setup</title><p>The behavioral setup consisted of an elevated interaction platform in the middle of an anechoic booth together with four circumjacent ultrasonic microphones as well as an overhanging 64-channel microphone array and high-speed video camera (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><p>The booth had internal dimensions of 70 × 130 × 120 cm (L × W × H). The walls and floor were covered with acoustic foam on the inside (thickness: 5 cm, black surface Basotect Plan50, BASF). The acoustic foam shields against external noises above ~1 kHz with a sound absorption coefficient &gt;0.95 (N.B., defined as the ratio between absorbed and incident sound intensity), which corresponds to &gt;26 dB of shielding apart from the shielding provided by the booth itself. In addition, the foam strongly attenuates internal reflections of high-frequency sounds like USVs. Illumination was provided via three dimmable LED strips mounted to the ceiling, providing light from multiple angles to minimize shadows.</p><p>The support structure for the interaction platform and all recording devices was a common frame constructed from slotted aluminum (30 × 30 mm) mounted to the floor of the anechoic booth, guaranteeing precise relative positioning throughout the entire experiment. The interaction platform itself was a 40 × 30 cm rectangle of laminated, white acoustic foam (thickness 5 cm; Basotect Plan50) chosen to maximize the visual contrast with the mice and simplify the cleaning of excreta. The interaction platform had no walls to avoid acoustic reflections and was located centrally in the booth. Its surface was elevated 25 cm above the floor (i.e., 20 cm above the foam on the booth floor), which was generally sufficient in preventing animals from leaving the platform. If a mouse left the platform, data was excluded from further analysis (&lt;5% of frames).</p><p>Sounds inside the booth were recorded with two sets of microphones: (i) four high-quality microphones (USM4) and (ii) a 64-channel microphone array (Cam64), both recording at a sampling rate of 250 kHz at 16 bits. (i) The four high-quality microphones (CM16/CMPA48AAF-5V, AviSoft, Berlin) were placed in a rectangle that contained the platform (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>) at a height exceeding the platform by 12.1 cm to minimize the amount of sound blocked by the mice during interaction. The position of a microphone was defined as the center of the recording membrane. Considering the directional receptivity of the microphones (~25 dB attenuation at 45°), the microphones were placed a short distance away from the corners of the platform to maximize sound capture (5 cm in the long direction and 6 cm in the short direction of the platform). The rotation of each microphone was chosen to be such that it aimed at the platform center. The microphones produce a flat (±5 dB) frequency response within 7–150 kHz that was low-pass filtered at 120 kHz to prevent aliasing (using the analog, 16th-order filter, which is part of the microphone amplifier). Recorded data was digitized using a data acquisition card (PCIe-6351, National Instruments). (ii) In addition, a 64-channel microphone array (Cam64 custom ultrasonic version, Sorama B.V.) was mounted above the platform with a relative height of 46.5 cm measured to the bottom of the Cam64 and a relative lateral shift of 6.52 cm to the right of the platform midpoint. The Cam64 utilizes 64 MEMS microphones (Knowles, Digital Zero-Height SiSonic, SPH0641LU4H-1) for acoustic data collection that are positioned in a Fermat’s spiral over a circle with an ~16 cm diameter. Raw microphone data was streamed to an m.2 SSD for later analysis. Synchronization between the samples acquired by the Cam64 and the ultrasonic microphones was performed by presenting two brief acoustic clicks (realized by stepping a digital output from 0 to 5 V) close to one of the microphones on the Cam64 at the start and end of each trial using a headphone driver (IE 800, Sennheiser). The recorded pulses were automatically retrieved and used to temporally align the recording sources.</p><p>A high-speed camera (PointGrey Flea3 FL3-U3-13Y3M-C, Monochrome, USB3.0) was mounted above the platform with a relative height of 46.5 cm measured to the bottom of the front end of the lens (6 mm, Thorlabs, part number: MVL6WA) and a relative lateral shift of 4.48 cm to the left of the platform midpoint. Video was recorded with a field of view of 52.2 × 41.7 cm at ~55.6 fps (18 ms inter-frame interval) and digitized at 640 × 512 pixels (producing an effective resolution of ~0.815 mm/pixel). The shutter time was set to 10 ms to guarantee good exposure while keeping the illumination rather dim. The frame triggers from the camera were recorded on an analog channel in the PCIe-6531 card for subsequent temporal alignment with the acoustic data.</p></sec><sec id="s4-3"><title>Experimental procedures</title><p>The experiment had three conditions: dyadic (with two mice), triadic (with three mice), and monadic (single male mouse, one type of ground truth data). For each of the male animals (n = 14), we conducted one trial with each female (n = 4) in dyadic and triadic conditions, that is, 112 trials in total, in pseudo-random order. The third animal in triadic conditions was chosen pseudo-randomly. Afterwards, to maximize the number of USVs for evaluation of the localization system, another 108 trials were run with the best male vocalizers in both dyadic and triadic conditions, leading to a total of 220 trials. In 85/220 trials, USVs were emitted by the mice (57 dyadic, 28 triadic), prompting the experimenter to initiate a Cam64 recording (see below). Two dyadic trials were excluded from further analysis due to repeated but required experimenter interference during the recordings leaving 55 dyadic trials. The USVs from the remaining 83 trials formed the basis for the evaluation of the tracking accuracy of HyVL, while we used the 112 balanced-design dyadic and triadic recordings (with and without USVs) in the analysis of differences in dyadic/triadic interactions (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Lastly, eight trials were recorded with just a single male mouse on the platform.</p><p>Each trial consisted of 8 min of free interaction between at least one female and at least one male mouse on the platform. Females were always placed on the platform first, and males were added shortly thereafter. In the monadic case, fresh female urine was placed on the platform instead of a female mouse to prompt the male mouse to vocalize. The high-speed camera and four high-quality microphones started recording after all mice had been placed on the platform and continued for 8 min. Data points where one mouse had left the platform or the hand of the experimenter were visible 10 s before or after (e.g., to pick up a mouse) were discarded (&lt;5% of frames). Due to the rate of data generation of the Cam64 recordings (32 MB/s), their duration and timing were optimized manually. The experimenter had access to the live spectrogram from the USM4 microphones, and upon the start of USVs, triggered a new Cam64 recording (of fixed 2 min duration). If additional USVs occurred after that point, the experimenter could trigger additional recordings.</p></sec><sec id="s4-4"><title>Data analysis</title><p>The analysis of the raw data involved multiple stages (see <xref ref-type="fig" rid="fig2">Figure 2</xref>): from the audio data, the presence and origin of USVs were estimated automatically. From the video data, mice were carefully tracked by hand at the temporal midpoint of each USV as near-optimal estimates for their acoustically localized origin. To estimate what proportion of our precision would be lost when using a faster and more scalable visual tracking method, we also tracked the mice automatically during dyadic trials. The estimated locations of the mice and USVs were then used to attribute the USVs to their emitter. All these steps are described in detail below.</p><sec id="s4-4-1"><title>Audio preprocessing</title><p>Prior to further analysis, acoustic recordings were filtered at different frequencies. USM4 data was band-pass filtered between 30 and 110 kHz before further analysis using an inverse impulse response filter or order 20 in MATLAB (function: designfilt, type: bandpassiir). Cam64 data was band-pass filtered with a frequency range adapted to the frequency content of each USV. Specifically, first the frequency range of the USV was estimated as the 10th–90th percentile of the set of most intense frequencies at each time point. Next, this range was broadened by 5 kHz at both ends, and then limited at the top end to 95 kHz. If this range exceeded 50 kHz, the lower end was set to 45 kHz. This ensured that beamforming was conducted over the relevant frequencies for each USV and avoided the high-frequency regions where the Cam64 microphones are dominated by noise (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s4-4-2"><title>Video preprocessing</title><p>The high-speed camera lens failed to produce perfect rectilinear mapping and was placed off-center with respect to the interaction platform, thereby producing a nonlinear radial-tangential visual distortion. We corrected for the radial distortion with<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represent the radially distorted image coordinates, <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the coordinates of the image center, <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the Euclidean distance to the radial distortion center, <inline-formula><mml:math id="inf4"><mml:mi>λ</mml:mi></mml:math></inline-formula> the distortion strength, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the radially undistorted coordinates, and <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> axis-specific zoom factors. The tangential distortion, on the other hand, we corrected with<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mfrac><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mfrac><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represent the tangentially distorted image coordinates, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the tangentially undistorted coordinates, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the coordinates of the tangential distortion center, <inline-formula><mml:math id="inf10"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> the size of the image, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the tangential distortion strengths, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the size of the interaction platform in the undistorted image, and <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the offset of the platform with respect to the top-left corner of the undistorted image.</p></sec><sec id="s4-4-3"><title>Detection of ultrasonic vocalizations</title><p>USVs were detected automatically using a set of custom algorithms described elsewhere (<xref ref-type="bibr" rid="bib34">Ivanenko et al., 2020</xref>). Detection was only performed on the USM4 data as their sensitivity and frequency range were generally better than for the Cam64 (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). A vocalization only had to be detected on one of the four high-quality microphones to be included into the set. In total, we collected 13,406 USVs, out of which 8424 occurred when the Cam64 recordings were active.</p></sec><sec id="s4-4-4"><title>Automatic visual animal tracking</title><p>To assess whether we could reliably assign USVs to their emitter in a fast and scalable way, we automatically tracked multiple body parts of interacting mice in all frames — most importantly the snout and head center — for all dyadic trials (using <italic>DeepLabCut</italic> [<xref ref-type="bibr" rid="bib6">Brudzynski, 2021</xref>]; see <xref ref-type="fig" rid="fig2">Figure 2</xref>) and a subset of triadic trials (using SLEAP [<xref ref-type="bibr" rid="bib62">Pereira et al., 2022</xref>]; see <xref ref-type="fig" rid="fig6">Figure 6</xref>). With this approach, tracking is not temporally restricted to the midpoint of USV production, but can be performed for every frame of the entire recording. This data can be used to establish spatial densities of interaction against which, for example, the spatial density of vocalizations can be compared (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>).</p><p>For the dyadic recordings, mice were tracked offline using a combination of <italic>DeepLabCut (DLC</italic>) (<xref ref-type="bibr" rid="bib47">Mathis et al., 2018</xref>) and extensive pos-processing to maintain animal identity over the entire recording. While the tracking results from DLC were generally quite accurate, we refrained from using them directly because of inaccuracies and identity switches that occurred on many hundreds of occasions in every recording. Instead we adopted a strategy where DLC generated an overcomplete set of candidate locations followed by custom synthesis and tracing of these alternatives in space and time (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). In short, improved marker locations were generated from marker estimate clouds produced by DLC. Next, these marker positions were assembled into short spatiotemporal threads with the same, unknown identity based on a combination of spatial and temporal analysis. Finally, the thread ends were connected based on quadratic spatial trajectory estimates for each marker, yielding the complete track for both mice. This strategy resulted in reliable, high-quality tracking for all recordings, with a greatly reduced number of manual corrections needed overall (~10 per trial on average). All resulting tracks were visually verified (for a representative example, see <xref ref-type="video" rid="video1">Video 1</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86126-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Example of Hybrid Vocalization Localizer (HyVL) tracking and sound localization.</title><p>Marker color represents animal sex (light blue: male; light red: female). Marker shape represents body part (circle: body center; cross: snout or tail; downward triangle: left ear; upward triangle: head center; diamond: right ear). Cam64 ultrasonic vocalization (USV) localizations (yellow) are overlaid on the beamforming densities (red) that are often very narrow and therefore hard to see underneath the localization marker (yellow dot). SLIM USV localizations are shown as well (orange '+'), typically further away from the snout in comparison to Cam64-based localization markers.</p></caption></media><p>For tracking the triadic interactions with two males, we used the SLEAP (<xref ref-type="bibr" rid="bib62">Pereira et al., 2022</xref>) tracking system (version 1.3.0). To obtain the frame-by-frame pose estimations, we utilized the SLEAP graphical user interface to train a bottom-up U-net model, which is used to identify the body parts first and then attribute them to separate instances. Initially, we trained the model on the manually annotated frames from the dyadic tracking process. Subsequently, we annotated ~200 additional frames, all in triadic conditions in which the model exhibited poor performance. The extended dataset was then used to retrain the model. To establish the basis for triadic tracking, we employed SLEAP’s tracker to group the predicted instances across frames. The tracker compared instances across the full six-node skeleton and aimed to maximize the overall similarity across the three track assignments using the Hungarian algorithm. To identify candidate instances for comparison, it employed optical flow based on the previous five frames and selected instances based on the 0.95 quantile of similarity scores. We also applied SLEAP’s post-tracking data cleaning techniques to connect any breaks in single tracks. Subsequently, we examined all 14 recordings frame by frame to rectify any identity switches and eliminate inaccurate predictions. For instance, we addressed cases where two instances were detected on a single mouse or when one instance appeared to cover two mice. To further refine the results, we interpolated outlying instances based on velocity jumps.</p><p>We compared the accuracy of localization on the basis of manual tracking with that of automatic tracking (N = 5046 USVs, see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Directly comparing the snout positions between the methods shows a median difference of 3.76 mm. The resulting error for localizing USVs was still superior to other systems, but significantly increased by ~0.9 mm (MAE = 5.71 mm) relative to manual tracking. Both manual and automatic tracking appear to have particular patterns of residual errors, indicated by the fact that the error between the tracking methods is much larger than their difference in USV localization error. The percentage of reliably assignable USVs interestingly increased to 93.6% (HyVL) compared to 92% with manual tracking for the dyadic recordings only. We optimized the mouth location on the snout-to-head-center line, finding an optimal distance of 15% of the snout to head center distance <italic>to the front</italic> of the animal. This indicated that the automatic tracking tended to place the snout tracking point a bit further into the snout than manual tracking, which might also explain the increase in assignment, due to a slight – but erroneous – increase in the separation between the snouts. While these results suggest that manual tracking is still advantageous, it highlights that completely automatic analysis of dyadic and possibly n-adic social interaction experiments is feasible at slightly reduced accuracy.</p></sec><sec id="s4-4-5"><title>Manual visual animal tracking</title><p>To test the maximum precision of HyVL, we manually tracked the spatial locations of all mice during all USVs from the video data to assess the precision of the automatic visual and acoustic tracking. During manual tracking, the observer was presented with a combined display of the vocalization spectrogram and the concurrent video image at the temporal midpoint of each USV (<italic>MultiViewer</italic>, custom-written, MATLAB-based visualization tool). The display included a zoom function for optimal accuracy as tracking was click-based. Users could also freely scroll in time to ensure consistent animal identities. Only the snout and head center (i.e., midpoint between the ears) needed to be annotated because these points define a vector representing the head location and direction, which was all that was required in subsequent behavioral analyses.</p></sec><sec id="s4-4-6"><title>Localization of ultrasonic vocalizations</title><p>USVs were spatially localized using a hybrid approach that integrates SLIM (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>) (based on four high-quality microphones) and beamforming (based on the 64-channel microphone array), drawing on the complementary strengths of the two microphone arrays (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). For example, the Cam64 array provided excellent localization for USVs with energy below ~90 kHz due to the increasing noise floor of the MEMS microphones with sound frequency. Conversely, the four high-quality ultrasonic microphones (USM4) have a rather flat noise level as a function of frequency. On the other hand, USM4 will occasionally have glitches in one of the microphones, which can be compensated for in Cam64-based estimates through the number of microphones. As a consequence, the errors of the two methods show an L-shape (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>), which highlights the synergy of a hybrid approach.</p><p>Acoustic localization using the Cam64 recordings was performed on the basis of delay-and-sum beamforming (<xref ref-type="bibr" rid="bib89">Van Veen and Buckley, 1988</xref>). In beamforming, signals from all microphones are combined to estimate a spatial density that correlates with the probability of a given location being the origin of the sound. Specifically, we computed beamforming estimates for a surface situated 1 cm above and co-centered with the interaction platform, extending to 5 cm beyond all edges of the platform (i.e., 50 × 40 cm in total) at a final resolution of 1 mm in both dimensions. We refer to this density of sound origin as <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> where <inline-formula><mml:math id="inf15"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mi>y</mml:mi></mml:math></inline-formula> denote spatial coordinates. To prevent noises unrelated to a specific USV from contaminating the location estimate, we limited beamforming to a particular frequency range estimated from the simultaneous data of the USM4 array that enveloped the USV. Spatial density was defined as<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>64</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf17"><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> denotes the difference in arrival time at each microphone <inline-formula><mml:math id="inf18"><mml:mi>m</mml:mi></mml:math></inline-formula> for sounds emitted from a location with coordinates <inline-formula><mml:math id="inf19"><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> , where <inline-formula><mml:math id="inf20"><mml:mi>z</mml:mi></mml:math></inline-formula> is omitted in <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> as it is a fixed distance to the plane of the microphone array. Beamforming was performed in the computational cloud backend provided by the Cam64 manufacturer, the so-called Sorama Portal (<ext-link ext-link-type="uri" xlink:href="https://www.sorama.eu/sorama-portal">https://www.sorama.eu/sorama-portal</ext-link>).</p><p>The final beamforming estimate was calculated sequentially in two steps: first, a coarse estimate with 1 cm resolution was generated over the entire beamforming surface. Second, a fine-grained estimate with 1 mm resolution was generated over a 30 × 30 mm window centered on the peak location of the coarse estimate (see <xref ref-type="fig" rid="fig2">Figure 2</xref> for an example). This two-step approach was chosen to optimize performance, as an estimate with 1 mm resolution over the entire beamforming surface would be computationally expensive while failing to produce a better result. For USVs of sufficient quality (i.e., containing frequency content below ~90 kHz while being sufficiently intense and long), both the coarse and fine estimates of <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> contained a peak whose height was typically very large compared to the surrounding values at distances greater than a few centimeters. The peak location of the fine-grained estimate was used as the final estimate of the USV’s origin. To assess the quality of this location estimate, we computed a SNR per USV as follows:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mn>64</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is assumed to be calculated for the USV <inline-formula><mml:math id="inf24"><mml:mi>v</mml:mi></mml:math></inline-formula>. The inverse, <inline-formula><mml:math id="inf25"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mn>64</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was used as a proxy for the uncertainty of localization for a given USV.</p><p>Localization from the USM4 recordings was performed using the SLIM method (<xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>). Briefly, SLIM analytically estimates submanifolds (in 2D: surfaces) of a sound’s spatial origin for each pair of microphones and combines these into a single estimate by intersecting the manifolds (in 2D: lines). The intersection has an associated uncertainty that scales with the uncertainty of the localization estimate for a given USV, specifically the uncertainty was defined as the standard deviation of all locations that were &gt;90% times the maximum of the intersection density of all origin curves.</p><p>Lastly, for each USV where both Cam64 and SLIM location estimates <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mn>64</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> were available, a single estimate <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>y</mml:mi><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was computed based on the two estimates, spatial uncertainties and their spatial relation to the mice at the current time (see below).</p></sec><sec id="s4-4-7"><title>USV assignment</title><p>The final hybrid location estimate and assignment to a mouse was performed while taking into account the probability of making a false assignment as proposed before (<xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>), through the calculation of the mouse probability index <inline-formula><mml:math id="inf29"><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula>. While the <inline-formula><mml:math id="inf30"><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula> was previously only used to exclude uncertain assignments (e.g., if two mice are nearly equidistant to the estimated sound location), we also adapted it here to select and combine the location estimates. The <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each mouse <italic>k</italic> was computed as,<disp-formula id="equ7"><mml:math id="m7"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability that the USV in question originated from mouse <inline-formula><mml:math id="inf33"><mml:mi>k</mml:mi></mml:math></inline-formula> computed as</p><p><inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , where <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is an estimate of the acoustic origin, <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the position of the mouth of mouse <inline-formula><mml:math id="inf37"><mml:mi>k</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf38"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> the uncertainty of the estimate, with <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>Ẋ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> specific to the Method used. <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was assumed to lie on a line connecting the snout and head-center. For manually tracked recordings, the optimal location on this line was close to the snout (~2% toward the head, where % is relative to the snout-to-head-center tracked distance), while in the automatic tracking it was ahead of the snout tracking point (~15% away from the head). <inline-formula><mml:math id="inf42"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> was computed for each USV as the method’s intrinsic per-USV uncertainty estimate. As these uncertainty estimates only correlate with the absolute uncertainty (i.e., in millimeters), we scaled them such that their average across all USVs matched the residual error of each method in the Far-condition (all animals &gt;100 mm apart, see <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="bibr" rid="bib59">Oliveira-Stahl et al., 2023</xref>). In this way, the <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for individual USVs took into account the uncertainty of each method: if the uncertainty of one method was higher, probabilities across mice would become more similar and the <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> would reduce.</p><p>For a given USV, we computed the <inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for all mice for both methods. The mouse with the largest <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> per method, which coincides with the mouse at the smallest distance to the estimate, was denoted as <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mn>64</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , respectively. If only one of the two exceeded 0.95, this method’s estimate was selected. If both exceeded 0.95, then the estimate with the smaller distance to the mouse with the highest <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was chosen. This combination ensured that only reliable assignments were performed, while minimizing the residual error. Similar to <xref ref-type="bibr" rid="bib56">Neunuebel et al., 2015</xref>, we also excluded estimates that were too far away from any mouse (50 mm). This distance threshold mainly serves to compensate for a deficiency of the <inline-formula><mml:math id="inf50"><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula>: if all mice are far from the estimate, all <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are extremely small; however, the <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> will often exceed 0.95. The distance threshold corresponds to setting the individual <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> in the <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , thus excluding candidate mice that are highly unlikely to be the source of the USV. USVs that had no <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> &gt; 0.95 for either method were excluded from further analysis. The fraction of included USVs is referred to as <italic>selected</italic> in the plots. Maximizing this fraction is essential to perform a complete analysis of vocal communication.</p><p>We compared the above-described combination strategy to a large number of alternative strategies, including maximum likelihood combination of estimators (<xref ref-type="bibr" rid="bib20">Ernst and Banks, 2002</xref>), or selecting directly based on the largest <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or largest <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . While all these approaches led to broadly similar results, the described approach achieved the most robust and reliable results (see ‘Discussion’ for additional details).</p></sec><sec id="s4-4-8"><title>Audiovisual alignment</title><p>For both microphone sets, precise measurements of their location in relation to the camera’s location were used to position acoustic estimates in the coordinate system of the images provided by the camera. In the final analysis, we noticed for each microphone set small, systematic (0.5–2 mm) shifts in both X and Y. We interpreted these as very small measurement errors in the relative positions of the camera or microphone arrays and corrected these post hoc in the setup definition, followed by rerunning all subsequent analysis steps. This reduced all systematic shifts to near 0.</p></sec><sec id="s4-4-9"><title>Spatial vocalization analysis</title><p>To gain insight into the spatial positioning of the interacting mice, we represented the relative animal positions in a polar reference frame centered on the snout of the emitter. In this format, the radial distance corresponded to the snout–snout distance and the radial angle described the relative angle between the gaze direction of the emitter and the snout position of the recipient (i.e., with the line from the head center to the snout of the emitter pointing towards 0°; see also <xref ref-type="fig" rid="fig4">Figure 4E</xref>).</p><p>The position density of the recipient mouse was collected in cumulative fashion, with the polar coordinate system translated appropriately for each USV based on its temporal midpoint. We assumed that the mice had no preference for relative vocalizations to either side of their snout, so all relative spatial positions were agglomerated in the right hemispace for further analysis. All data points were then binned using a polar, raw-count histogram with bins of 10° and 1 cm.</p></sec></sec><sec id="s4-5"><title>Statistical analysis</title><p>To avoid distributional assumptions, all statistical tests were nonparametric, that is, Wilcoxon rank-sum test for two-group comparisons and Kruskal–Wallis for single-factor ANOVA. Correlations were computed as Spearman’s rank-based correlation coefficients. Error bars represent standard errors of the mean (SEM) unless stated otherwise. All statistical analyses were performed in MATLAB v.2018b (The MathWorks, Natick) using functions from the Statistics Toolbox.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All of the animals and experimental procedures were conducted according to the guidelines of the Animal Welfare Body of the Central Animal Facility at the Radboud University. The protocol was approved by the Dutch National Committee CCD (Permit Number: 2017-0041).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86126-mdarchecklist1-v4.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code necessary to implement the HyVL system has been deposited at <ext-link ext-link-type="uri" xlink:href="https://github.com/benglitz/HyVL">https://github.com/benglitz/HyVL</ext-link> (copy archived at <xref ref-type="bibr" rid="bib18">Englitz, 2023</xref>) and <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/7kgc-ta72">https://doi.org/10.34973/7kgc-ta72</ext-link>. All data has been made available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.34973/7kgc-ta72">https://doi.org/10.34973/7kgc-ta72</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Sterling</surname><given-names>M</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Teunisse</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Ultrasonic vocal interaction resolved with millimeter precision using hybrid beamforming</data-title><source>Donders Repository</source><pub-id pub-id-type="doi">10.34973/7kgc-ta72</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Lucas Noldus for suggesting the use of the Sorama Cam64 and Maurice Camp and Toros Senan for technical support relating to the operation and data handling of the Cam64 and the Sorama Portal. We would like to thank Amber van der Stam, Dionne Lenferink, and Soha Farboud for assisting with the animal handling.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agarwalla</surname><given-names>S</given-names></name><name><surname>Arroyo</surname><given-names>NS</given-names></name><name><surname>Long</surname><given-names>NE</given-names></name><name><surname>O’Brien</surname><given-names>WT</given-names></name><name><surname>Abel</surname><given-names>T</given-names></name><name><surname>Bandyopadhyay</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Male-specific alterations in structure of isolation call sequences of mouse pups with 16p11.2 deletion</article-title><source>Genes, Brain, and Behavior</source><volume>19</volume><elocation-id>e12681</elocation-id><pub-id pub-id-type="doi">10.1111/gbb.12681</pub-id><pub-id pub-id-type="pmid">32558237</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arriaga</surname><given-names>G</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mouse vocal communication system: are ultrasounds learned or innate?</article-title><source>Brain and Language</source><volume>124</volume><fpage>96</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.10.002</pub-id><pub-id pub-id-type="pmid">23295209</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bakker</surname><given-names>J</given-names></name><name><surname>Langermans</surname><given-names>JAM</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Ultrasonic components of vocalizations in Marmosets</chapter-title><source>Handbook of Ultrasonic Vocalization - A Window into the Emotional Brain</source><publisher-name>Elsevier</publisher-name><fpage>535</fpage><lpage>544</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>TD</given-names></name><name><surname>Rieger</surname><given-names>MA</given-names></name><name><surname>Dougherty</surname><given-names>JD</given-names></name><name><surname>Holy</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Group and individual variability in mouse pup isolation calls recorded on the same day show stability</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>11</volume><elocation-id>243</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2017.00243</pub-id><pub-id pub-id-type="pmid">29326565</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>MS</given-names></name><name><surname>Shi</surname><given-names>HD</given-names></name><name><surname>Bordey</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>CD-1 Outbred Mice Produce Less Variable Ultrasonic Vocalizations Than FVB Inbred Mice, While Displaying a Similar Developmental Trajectory</article-title><source>Frontiers in Psychiatry</source><volume>12</volume><fpage>687060</fpage><pub-id pub-id-type="doi">10.3389/fpsyt.2021.687060</pub-id><pub-id pub-id-type="pmid">34475829</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brudzynski</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological functions of rat ultrasonic vocalizations, arousal mechanisms, and call initiation</article-title><source>Brain Sciences</source><volume>11</volume><elocation-id>605</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci11050605</pub-id><pub-id pub-id-type="pmid">34065107</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>K</given-names></name><name><surname>Screven</surname><given-names>LA</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CBA/CaJ mouse ultrasonic vocalizations depend on prior social experience</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0197774</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0197774</pub-id><pub-id pub-id-type="pmid">29874248</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname><given-names>GA</given-names></name><name><surname>Calbick</surname><given-names>D</given-names></name><name><surname>McCormick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The temporal organization of mouse ultrasonic vocalizations</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0199929</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0199929</pub-id><pub-id pub-id-type="pmid">30376572</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Serreau</surname><given-names>P</given-names></name><name><surname>Ey</surname><given-names>E</given-names></name><name><surname>Bellier</surname><given-names>L</given-names></name><name><surname>Aubin</surname><given-names>T</given-names></name><name><surname>Bourgeron</surname><given-names>T</given-names></name><name><surname>Granon</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Adult male mice emit context-specific ultrasonic vocalizations that are modulated by prior isolation or group rearing environment</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e29401</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0029401</pub-id><pub-id pub-id-type="pmid">22238608</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Sarkar</surname><given-names>A</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Male mice song syntax depends on social contexts and influences female preferences</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id><pub-id pub-id-type="pmid">25883559</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Jones-Macopson</surname><given-names>J</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Eliciting and analyzing male mouse ultrasonic vocalization (usv) songs</article-title><source>Journal of Visualized Experiments</source><volume>01</volume><elocation-id>54137</elocation-id><pub-id pub-id-type="doi">10.3791/54137</pub-id><pub-id pub-id-type="pmid">28518074</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Lilascharoen</surname><given-names>V</given-names></name><name><surname>Taylor</surname><given-names>S</given-names></name><name><surname>Sheurpukdi</surname><given-names>P</given-names></name><name><surname>Keller</surname><given-names>JA</given-names></name><name><surname>Jensen</surname><given-names>JR</given-names></name><name><surname>Lim</surname><given-names>BK</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name><name><surname>Stowers</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Flexible scaling and persistence of social vocal communication</article-title><source>Nature</source><volume>593</volume><fpage>108</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03403-8</pub-id><pub-id pub-id-type="pmid">33790464</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Park</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Two genetic loci control syllable sequences of ultrasonic courtship vocalizations in inbred mice</article-title><source>BMC Neuroscience</source><volume>12</volume><elocation-id>104</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-12-104</pub-id><pub-id pub-id-type="pmid">22018021</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciucci</surname><given-names>MR</given-names></name><name><surname>Ahrens</surname><given-names>AM</given-names></name><name><surname>Ma</surname><given-names>ST</given-names></name><name><surname>Kane</surname><given-names>JR</given-names></name><name><surname>Windham</surname><given-names>EB</given-names></name><name><surname>Woodlee</surname><given-names>MT</given-names></name><name><surname>Schallert</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reduction of dopamine synaptic activity: degradation of 50-kHz ultrasonic vocalization in rats</article-title><source>Behavioral Neuroscience</source><volume>123</volume><fpage>328</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1037/a0014593</pub-id><pub-id pub-id-type="pmid">19331456</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffey</surname><given-names>KR</given-names></name><name><surname>Marx</surname><given-names>RE</given-names></name><name><surname>Neumaier</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title><source>Neuropsychopharmacology</source><volume>44</volume><fpage>859</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Chaumont</surname><given-names>F</given-names></name><name><surname>Lemière</surname><given-names>N</given-names></name><name><surname>Coqueran</surname><given-names>S</given-names></name><name><surname>Bourgeron</surname><given-names>T</given-names></name><name><surname>Ey</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>LMT USV Toolbox, A Novel Methodological Approach to Place Mouse Ultrasonic Vocalizations in Their Behavioral Contexts-A Study in Female and Male C57BL/6J Mice and in <italic>Shank3</italic> Mutant Females</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>15</volume><elocation-id>735920</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2021.735920</pub-id><pub-id pub-id-type="pmid">34720899</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dou</surname><given-names>X</given-names></name><name><surname>Shirahata</surname><given-names>S</given-names></name><name><surname>Sugimoto</surname><given-names>H</given-names></name><name><surname>Abreu-Villaça</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional clustering of mouse ultrasonic vocalization data</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0196834</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0196834</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Hyvl - hybrid vocalization Localizer</data-title><version designator="swh:1:rev:7ab9f02a38d8ae5b668e4801463bb22367da4637">swh:1:rev:7ab9f02a38d8ae5b668e4801463bb22367da4637</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9d9e1598ff96a36cdc3d45cd54c5de47746fe3cd;origin=https://github.com/benglitz/HyVL;visit=swh:1:snp:b6db3bffd9022374a42a8fdb8464bcf19d8bae1e;anchor=swh:1:rev:7ab9f02a38d8ae5b668e4801463bb22367da4637">https://archive.softwareheritage.org/swh:1:dir:9d9e1598ff96a36cdc3d45cd54c5de47746fe3cd;origin=https://github.com/benglitz/HyVL;visit=swh:1:snp:b6db3bffd9022374a42a8fdb8464bcf19d8bae1e;anchor=swh:1:rev:7ab9f02a38d8ae5b668e4801463bb22367da4637</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erata</surname><given-names>E</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Purkey</surname><given-names>AM</given-names></name><name><surname>Soderblom</surname><given-names>EJ</given-names></name><name><surname>McNamara</surname><given-names>JO</given-names></name><name><surname>Soderling</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cnksr2 Loss in Mice Leads to Increased Neural Activity and Behavioral Phenotypes of Epilepsy-Aphasia Syndrome</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>9633</fpage><lpage>9649</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0650-21.2021</pub-id><pub-id pub-id-type="pmid">34580165</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>AS</given-names></name><name><surname>Narins</surname><given-names>PM</given-names></name><name><surname>Xu</surname><given-names>C-H</given-names></name><name><surname>Lin</surname><given-names>W-Y</given-names></name><name><surname>Yu</surname><given-names>Z-L</given-names></name><name><surname>Qiu</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>Z-M</given-names></name><name><surname>Shen</surname><given-names>J-X</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Ultrasonic communication in frogs</article-title><source>Nature</source><volume>440</volume><fpage>333</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1038/nature04416</pub-id><pub-id pub-id-type="pmid">16541072</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonseca</surname><given-names>AH</given-names></name><name><surname>Santana</surname><given-names>GM</given-names></name><name><surname>Bosque Ortiz</surname><given-names>GM</given-names></name><name><surname>Bampi</surname><given-names>S</given-names></name><name><surname>Dietrich</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Analysis of ultrasonic vocalizations from mice using computer vision and machine learning</article-title><source>eLife</source><volume>10</volume><elocation-id>e59161</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59161</pub-id><pub-id pub-id-type="pmid">33787490</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fröhlich</surname><given-names>H</given-names></name><name><surname>Rafiullah</surname><given-names>R</given-names></name><name><surname>Schmitt</surname><given-names>N</given-names></name><name><surname>Abele</surname><given-names>S</given-names></name><name><surname>Rappold</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Foxp1 expression is essential for sex-specific murine neonatal ultrasonic vocalization</article-title><source>Human Molecular Genetics</source><volume>26</volume><fpage>1511</fpage><lpage>1521</lpage><pub-id pub-id-type="doi">10.1093/hmg/ddx055</pub-id><pub-id pub-id-type="pmid">28204507</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujita</surname><given-names>E</given-names></name><name><surname>Tanabe</surname><given-names>Y</given-names></name><name><surname>Imhof</surname><given-names>BA</given-names></name><name><surname>Momoi</surname><given-names>MY</given-names></name><name><surname>Momoi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cadm1-expressing synapses on Purkinje cell dendrites are involved in mouse ultrasonic vocalization activity</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e30151</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0030151</pub-id><pub-id pub-id-type="pmid">22272290</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>SC</given-names></name><name><surname>Wei</surname><given-names>YC</given-names></name><name><surname>Wang</surname><given-names>SR</given-names></name><name><surname>Xu</surname><given-names>XH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Medial preoptic area modulates courtship ultrasonic vocalization in adult male mice</article-title><source>Neuroscience Bulletin</source><volume>35</volume><fpage>697</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1007/s12264-019-00365-w</pub-id><pub-id pub-id-type="pmid">30900143</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guoynes</surname><given-names>CD</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An acute dose of intranasal oxytocin rapidly increases maternal communication and maintains maternal care in primiparous postpartum California mice</article-title><source>PLOS ONE</source><volume>16</volume><elocation-id>e0244033</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0244033</pub-id><pub-id pub-id-type="pmid">33886559</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Radyushkin</surname><given-names>K</given-names></name><name><surname>Ehrenreich</surname><given-names>H</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Female mice respond to male ultrasonic “songs” with approach behaviour</article-title><source>Biology Letters</source><volume>5</volume><fpage>589</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1098/rsbl.2009.0317</pub-id><pub-id pub-id-type="pmid">19515648</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heckman</surname><given-names>JJ</given-names></name><name><surname>Proville</surname><given-names>R</given-names></name><name><surname>Heckman</surname><given-names>GJ</given-names></name><name><surname>Azarfar</surname><given-names>A</given-names></name><name><surname>Celikel</surname><given-names>T</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-precision spatial localization of mouse vocalizations during social interaction</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>3017</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-02954-z</pub-id><pub-id pub-id-type="pmid">28592832</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hepbasli</surname><given-names>D</given-names></name><name><surname>Gredy</surname><given-names>S</given-names></name><name><surname>Ullrich</surname><given-names>M</given-names></name><name><surname>Reigl</surname><given-names>A</given-names></name><name><surname>Abeßer</surname><given-names>M</given-names></name><name><surname>Raabe</surname><given-names>T</given-names></name><name><surname>Schuh</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Genotype- and Age-Dependent Differences in Ultrasound Vocalizations of SPRED2 Mutant Mice Revealed by Machine Deep Learning</article-title><source>Brain Sciences</source><volume>11</volume><elocation-id>1365</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci11101365</pub-id><pub-id pub-id-type="pmid">34679429</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>S</given-names></name><name><surname>Weiner</surname><given-names>B</given-names></name><name><surname>Perets</surname><given-names>N</given-names></name><name><surname>London</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal structure of mouse courtship vocalizations facilitates syllable labeling</article-title><source>Communications Biology</source><volume>3</volume><elocation-id>333</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-020-1053-7</pub-id><pub-id pub-id-type="pmid">32591576</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodges</surname><given-names>SL</given-names></name><name><surname>Nolan</surname><given-names>SO</given-names></name><name><surname>Reynolds</surname><given-names>CD</given-names></name><name><surname>Lugo</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spectral and temporal properties of calls reveal deficits in ultrasonic vocalizations of adult Fmr1 knockout mice</article-title><source>Behavioural Brain Research</source><volume>332</volume><fpage>50</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2017.05.052</pub-id><pub-id pub-id-type="pmid">28552599</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoier</surname><given-names>S</given-names></name><name><surname>Pfeifle</surname><given-names>C</given-names></name><name><surname>von Merten</surname><given-names>S</given-names></name><name><surname>Linnenbrink</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Communication at the garden fence--context dependent vocalization in female house mice</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0152255</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0152255</pub-id><pub-id pub-id-type="pmid">27022749</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holy</surname><given-names>TE</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ultrasonic songs of male mice</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id><pub-id pub-id-type="pmid">16248680</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivanenko</surname><given-names>A</given-names></name><name><surname>Watkins</surname><given-names>P</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007918</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007918</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappel</surname><given-names>S</given-names></name><name><surname>Hawkins</surname><given-names>P</given-names></name><name><surname>Mendl</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>To group or not to group? Good practice for housing male laboratory mice</article-title><source>Animals</source><volume>7</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3390/ani7120088</pub-id><pub-id pub-id-type="pmid">29186765</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keesom</surname><given-names>SM</given-names></name><name><surname>Finton</surname><given-names>CJ</given-names></name><name><surname>Sell</surname><given-names>GL</given-names></name><name><surname>Hurley</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Early-life social isolation influences mouse ultrasonic vocalizations during male-male social encounters</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0169705</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0169705</pub-id><pub-id pub-id-type="pmid">28056078</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikusui</surname><given-names>T</given-names></name><name><surname>Nakanishi</surname><given-names>K</given-names></name><name><surname>Nakagawa</surname><given-names>R</given-names></name><name><surname>Nagasawa</surname><given-names>M</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cross fostering experiments suggest that mice songs are innate</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e17721</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0017721</pub-id><pub-id pub-id-type="pmid">21408017</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikusui</surname><given-names>T</given-names></name><name><surname>Shima</surname><given-names>Y</given-names></name><name><surname>Sonobe</surname><given-names>M</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name><name><surname>Nagasawa</surname><given-names>M</given-names></name><name><surname>Nomoto</surname><given-names>K</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Testosterone regulates the emission of ultrasonic vocalizations and mounting behavior during different developmental periods in mice</article-title><source>Developmental Psychobiology</source><volume>63</volume><fpage>725</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1002/dev.22045</pub-id><pub-id pub-id-type="pmid">33070342</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikusui</surname><given-names>T</given-names></name><name><surname>Sonobe</surname><given-names>M</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name><name><surname>Nagasawa</surname><given-names>M</given-names></name><name><surname>Ey</surname><given-names>E</given-names></name><name><surname>de Chaumont</surname><given-names>F</given-names></name><name><surname>Bourgeron</surname><given-names>T</given-names></name><name><surname>Nomoto</surname><given-names>K</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Testosterone increases the emission of ultrasonic vocalizations with different acoustic characteristics in mice</article-title><source>Frontiers in Psychology</source><volume>12</volume><elocation-id>680176</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2021.680176</pub-id><pub-id pub-id-type="pmid">34248780</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kondrakiewicz</surname><given-names>K</given-names></name><name><surname>Kostecki</surname><given-names>M</given-names></name><name><surname>Szadzińska</surname><given-names>W</given-names></name><name><surname>Knapska</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ecological validity of social interaction tests in rats and mice</article-title><source>Genes, Brain, and Behavior</source><volume>18</volume><elocation-id>e12525</elocation-id><pub-id pub-id-type="doi">10.1111/gbb.12525</pub-id><pub-id pub-id-type="pmid">30311398</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>NS</given-names></name><name><surname>Beery</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural circuits underlying rodent sociality: A comparative approach</article-title><source>Current Topics in Behavioral Neurosciences</source><volume>43</volume><fpage>211</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1007/7854_2018_77</pub-id><pub-id pub-id-type="pmid">30710222</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvin</surname><given-names>Y</given-names></name><name><surname>Blanchard</surname><given-names>DC</given-names></name><name><surname>Blanchard</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rat 22kHz ultrasonic vocalizations as alarm cries</article-title><source>Behavioural Brain Research</source><volume>182</volume><fpage>166</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2006.11.038</pub-id><pub-id pub-id-type="pmid">17173984</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>RC</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Acoustic variability and distinguishability among mouse ultrasound vocalizations</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>3412</fpage><lpage>3422</lpage><pub-id pub-id-type="doi">10.1121/1.1623787</pub-id><pub-id pub-id-type="pmid">14714820</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahrt</surname><given-names>EJ</given-names></name><name><surname>Perkel</surname><given-names>DJ</given-names></name><name><surname>Tong</surname><given-names>L</given-names></name><name><surname>Rubel</surname><given-names>EW</given-names></name><name><surname>Portfors</surname><given-names>CV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Engineered deafness reveals that mouse courtship vocalizations do not require auditory experience</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>5573</fpage><lpage>5583</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5054-12.2013</pub-id><pub-id pub-id-type="pmid">23536072</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahrt</surname><given-names>E</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Perkel</surname><given-names>D</given-names></name><name><surname>Portfors</surname><given-names>C</given-names></name><name><surname>Elemans</surname><given-names>CPH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mice produce ultrasonic vocalizations by intra-laryngeal planar impinging jets</article-title><source>Current Biology</source><volume>26</volume><fpage>R880</fpage><lpage>R881</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.032</pub-id><pub-id pub-id-type="pmid">27728788</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marconi</surname><given-names>MA</given-names></name><name><surname>Nicolakis</surname><given-names>D</given-names></name><name><surname>Abbasi</surname><given-names>R</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name><name><surname>Zala</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultrasonic courtship vocalizations of male house mice contain distinct individual signatures</article-title><source>Animal Behaviour</source><volume>169</volume><fpage>169</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2020.09.006</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menuet</surname><given-names>C</given-names></name><name><surname>Cazals</surname><given-names>Y</given-names></name><name><surname>Gestreau</surname><given-names>C</given-names></name><name><surname>Borghgraef</surname><given-names>P</given-names></name><name><surname>Gielis</surname><given-names>L</given-names></name><name><surname>Dutschmann</surname><given-names>M</given-names></name><name><surname>Van Leuven</surname><given-names>F</given-names></name><name><surname>Hilaire</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Age-related impairment of ultrasonic vocalization in Tau.P301L mice: possible implication for progressive language disorders</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e25770</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0025770</pub-id><pub-id pub-id-type="pmid">22022446</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michael</surname><given-names>V</given-names></name><name><surname>Goffinet</surname><given-names>J</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Tschida</surname><given-names>K</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Circuit and synaptic organization of forebrain-to-midbrain pathways that promote and suppress vocalization</article-title><source>eLife</source><volume>9</volume><elocation-id>e63493</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63493</pub-id><pub-id pub-id-type="pmid">33372655</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moskal</surname><given-names>JR</given-names></name><name><surname>Burgdorf</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Ultrasonic vocalizations in rats as a measure of emotional responses to stress: models of anxiety and depression</chapter-title><source>Handbook of Ultrasonic Vocalization - A Window into the Emotional Brain</source><publisher-name>Elsevier</publisher-name><fpage>413</fpage><lpage>421</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mourlam</surname><given-names>MJ</given-names></name><name><surname>Orliac</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Infrasonic and ultrasonic hearing evolved after the emergence of modern whales</article-title><source>Current Biology</source><volume>27</volume><fpage>1776</fpage><lpage>1781</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.04.061</pub-id><pub-id pub-id-type="pmid">28602653</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mun</surname><given-names>HS</given-names></name><name><surname>Lipina</surname><given-names>TV</given-names></name><name><surname>Roder</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ultrasonic vocalizations in mice during exploratory behavior are context-dependent</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>316</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00316</pub-id><pub-id pub-id-type="pmid">26696847</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murrant</surname><given-names>MN</given-names></name><name><surname>Bowman</surname><given-names>J</given-names></name><name><surname>Garroway</surname><given-names>CJ</given-names></name><name><surname>Prinzen</surname><given-names>B</given-names></name><name><surname>Mayberry</surname><given-names>H</given-names></name><name><surname>Faure</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasonic vocalizations emitted by flying squirrels</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e73045</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0073045</pub-id><pub-id pub-id-type="pmid">24009728</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musolf</surname><given-names>K</given-names></name><name><surname>Hoffmann</surname><given-names>F</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ultrasonic courtship vocalizations in wild house mice, <italic>Mus musculus</italic></article-title><source>Animal Behaviour</source><volume>79</volume><fpage>757</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2009.12.034</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musolf</surname><given-names>K</given-names></name><name><surname>Meindl</surname><given-names>S</given-names></name><name><surname>Larsen</surname><given-names>AL</given-names></name><name><surname>Kalcounis-Rueppell</surname><given-names>MC</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ultrasonic vocalizations of male mice differ among species and females show assortative preferences for male calls</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0134123</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0134123</pub-id><pub-id pub-id-type="pmid">26309246</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neunuebel</surname><given-names>JP</given-names></name><name><surname>Taylor</surname><given-names>AL</given-names></name><name><surname>Arthur</surname><given-names>BJ</given-names></name><name><surname>Egnor</surname><given-names>SER</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Female mice ultrasonically interact with males during courtship displays</article-title><source>eLife</source><volume>4</volume><elocation-id>e06203</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06203</pub-id><pub-id pub-id-type="pmid">26020291</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolakis</surname><given-names>D</given-names></name><name><surname>Marconi</surname><given-names>MA</given-names></name><name><surname>Zala</surname><given-names>SM</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultrasonic vocalizations in house mice depend upon genetic relatedness of mating partners and correlate with subsequent reproductive success</article-title><source>Frontiers in Zoology</source><volume>17</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1186/s12983-020-00353-1</pub-id><pub-id pub-id-type="pmid">32265997</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunez</surname><given-names>AA</given-names></name><name><surname>Nyby</surname><given-names>J</given-names></name><name><surname>Whitney</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The effects of testosterone, estradiol, and dihydrotestosterone on male mouse (<italic>Mus musculus</italic>) ultrasonic vocalizations</article-title><source>Hormones and Behavior</source><volume>11</volume><fpage>264</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1016/0018-506x(78)90030-2</pub-id><pub-id pub-id-type="pmid">753695</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliveira-Stahl</surname><given-names>G</given-names></name><name><surname>Farboud</surname><given-names>S</given-names></name><name><surname>Sterling</surname><given-names>ML</given-names></name><name><surname>Heckman</surname><given-names>JJ</given-names></name><name><surname>van Raalte</surname><given-names>B</given-names></name><name><surname>Lenferink</surname><given-names>D</given-names></name><name><surname>van der Stam</surname><given-names>A</given-names></name><name><surname>Smeets</surname><given-names>CJLM</given-names></name><name><surname>Fisher</surname><given-names>SE</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>High-precision spatial analysis of mouse courtship vocalization behavior reveals sex and strain differences</article-title><source>Scientific Reports</source><volume>13</volume><elocation-id>5219</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-31554-3</pub-id><pub-id pub-id-type="pmid">36997591</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palazzo</surname><given-names>E</given-names></name><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Ji</surname><given-names>G</given-names></name><name><surname>Maione</surname><given-names>S</given-names></name><name><surname>Neugebauer</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Group III mGluR7 and mGluR8 in the amygdala differentially modulate nocifensive and affective pain behaviors</article-title><source>Neuropharmacology</source><volume>55</volume><fpage>537</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/j.neuropharm.2008.05.007</pub-id><pub-id pub-id-type="pmid">18533199</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmateer</surname><given-names>J</given-names></name><name><surname>Pan</surname><given-names>J</given-names></name><name><surname>Pandya</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>L</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Ofomata</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>TA</given-names></name><name><surname>Gore</surname><given-names>AC</given-names></name><name><surname>Schallert</surname><given-names>T</given-names></name><name><surname>Hurn</surname><given-names>PD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Ultrasonic vocalization in murine experimental stroke: A mechanistic model of aphasia</article-title><source>Restorative Neurology and Neuroscience</source><volume>34</volume><fpage>287</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.3233/RNN-150583</pub-id><pub-id pub-id-type="pmid">26889967</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Tabris</surname><given-names>N</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>DM</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Ravindranath</surname><given-names>S</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Normand</surname><given-names>E</given-names></name><name><surname>Deutsch</surname><given-names>DS</given-names></name><name><surname>Wang</surname><given-names>ZY</given-names></name><name><surname>McKenzie-Smith</surname><given-names>GC</given-names></name><name><surname>Mitelut</surname><given-names>CC</given-names></name><name><surname>Castro</surname><given-names>MD</given-names></name><name><surname>D’Uva</surname><given-names>J</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name><name><surname>Kocher</surname><given-names>SD</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Publisher Correction: SLEAP: A deep learning system for multi-animal pose tracking</article-title><source>Nature Methods</source><volume>19</volume><fpage>486</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01495-2</pub-id><pub-id pub-id-type="pmid">35468969</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petric</surname><given-names>R</given-names></name><name><surname>Kalcounis-Rueppell</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Female and male adult brush mice (Peromyscus boylii) use ultrasonic vocalizations in the wild</article-title><source>Behaviour</source><volume>150</volume><fpage>1747</fpage><lpage>1766</lpage><pub-id pub-id-type="doi">10.1163/1568539X-00003118</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pomerantz</surname><given-names>SM</given-names></name><name><surname>Clemens</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Ultrasonic vocalizations in male deer mice (Peromyscus maniculatus bairdi): their role in male sexual behavior</article-title><source>Physiology &amp; Behavior</source><volume>27</volume><fpage>869</fpage><lpage>872</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(81)90055-x</pub-id><pub-id pub-id-type="pmid">7323194</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portfors</surname><given-names>CV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Types and functions of ultrasonic vocalizations in laboratory rats and mice</article-title><source>Journal of the American Association for Laboratory Animal Science</source><volume>46</volume><fpage>28</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">17203913</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portfors</surname><given-names>CV</given-names></name><name><surname>Perkel</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The role of ultrasonic vocalizations in mouse communication</article-title><source>Current Opinion in Neurobiology</source><volume>28</volume><fpage>115</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.07.002</pub-id><pub-id pub-id-type="pmid">25062471</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pultorak</surname><given-names>JD</given-names></name><name><surname>Fuxjager</surname><given-names>MJ</given-names></name><name><surname>Kalcounis-Rueppell</surname><given-names>MC</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Male fidelity expressed through rapid testosterone suppression of ultrasonic vocalizations to novel females in the monogamous California mouse</article-title><source>Hormones and Behavior</source><volume>70</volume><fpage>47</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.yhbeh.2015.02.003</pub-id><pub-id pub-id-type="pmid">25725427</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pultorak</surname><given-names>JD</given-names></name><name><surname>Matusinec</surname><given-names>KR</given-names></name><name><surname>Miller</surname><given-names>ZK</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ultrasonic vocalization production and playback predicts intrapair and extrapair social behaviour in a monogamous mouse</article-title><source>Animal Behaviour</source><volume>125</volume><fpage>13</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2016.12.023</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pultorak</surname><given-names>JD</given-names></name><name><surname>Alger</surname><given-names>SJ</given-names></name><name><surname>Loria</surname><given-names>SO</given-names></name><name><surname>Johnson</surname><given-names>AM</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Changes in behavior and ultrasonic vocalizations during pair bonding and in response to an infidelity challenge in monogamous california mice</article-title><source>Frontiers in Ecology and Evolution</source><volume>6</volume><elocation-id>125</elocation-id><pub-id pub-id-type="doi">10.3389/fevo.2018.00125</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramsier</surname><given-names>MA</given-names></name><name><surname>Cunningham</surname><given-names>AJ</given-names></name><name><surname>Moritz</surname><given-names>GL</given-names></name><name><surname>Finneran</surname><given-names>JJ</given-names></name><name><surname>Williams</surname><given-names>CV</given-names></name><name><surname>Ong</surname><given-names>PS</given-names></name><name><surname>Gursky-Doyen</surname><given-names>SL</given-names></name><name><surname>Dominy</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Primate communication in the pure ultrasound</article-title><source>Biology Letters</source><volume>8</volume><fpage>508</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1098/rsbl.2011.1149</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rieger</surname><given-names>NS</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The function of ultrasonic vocalizations during territorial defence by pair-bonded male and female California mice</article-title><source>Animal Behaviour</source><volume>135</volume><fpage>97</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2017.11.008</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rieger</surname><given-names>NS</given-names></name><name><surname>Monari</surname><given-names>PK</given-names></name><name><surname>Hartfield</surname><given-names>K</given-names></name><name><surname>Schefelker</surname><given-names>J</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pair-bonding leads to convergence in approach behavior to conspecific vocalizations in California mice (Peromyscus californicus)</article-title><source>PLOS ONE</source><volume>16</volume><elocation-id>e0255295</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0255295</pub-id><pub-id pub-id-type="pmid">34383820</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rotschafer</surname><given-names>SE</given-names></name><name><surname>Trujillo</surname><given-names>MS</given-names></name><name><surname>Dansie</surname><given-names>LE</given-names></name><name><surname>Ethell</surname><given-names>IM</given-names></name><name><surname>Razak</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Minocycline treatment reverses ultrasonic vocalization production deficit in a mouse model of Fragile X Syndrome</article-title><source>Brain Research</source><volume>1439</volume><fpage>7</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2011.12.041</pub-id><pub-id pub-id-type="pmid">22265702</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sangiamo</surname><given-names>DT</given-names></name><name><surname>Warren</surname><given-names>MR</given-names></name><name><surname>Neunuebel</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultrasonic signals associated with different types of social behavior of mice</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>411</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0584-z</pub-id><pub-id pub-id-type="pmid">32066980</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scattoni</surname><given-names>ML</given-names></name><name><surname>Crawley</surname><given-names>J</given-names></name><name><surname>Ricceri</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Ultrasonic vocalizations: a tool for behavioural phenotyping of mouse models of neurodevelopmental disorders</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>33</volume><fpage>508</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2008.08.003</pub-id><pub-id pub-id-type="pmid">18771687</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnitzler</surname><given-names>HU</given-names></name><name><surname>Moss</surname><given-names>CF</given-names></name><name><surname>Denzinger</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>From spatial orientation to food acquisition in echolocating bats</article-title><source>Trends in Ecology &amp; Evolution</source><volume>18</volume><fpage>386</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1016/S0169-5347(03)00185-X</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shemesh</surname><given-names>Y</given-names></name><name><surname>Sztainberg</surname><given-names>Y</given-names></name><name><surname>Forkosh</surname><given-names>O</given-names></name><name><surname>Shlapobersky</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-order social interactions in groups of mice</article-title><source>eLife</source><volume>2</volume><elocation-id>e00759</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00759</pub-id><pub-id pub-id-type="pmid">24015357</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>KN</given-names></name><name><surname>Chong</surname><given-names>KK</given-names></name><name><surname>Liu</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Contrast enhancement without transient map expansion for species-specific vocalizations in core auditory cortex during learning</article-title><source>eNeuro</source><volume>3</volume><elocation-id>ENEURO.0318-16.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0318-16.2016</pub-id><pub-id pub-id-type="pmid">27957529</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silverman</surname><given-names>JL</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Lord</surname><given-names>C</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Behavioural phenotyping assays for mouse models of autism</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>490</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1038/nrn2851</pub-id><pub-id pub-id-type="pmid">20559336</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>H</given-names></name><name><surname>Okabe</surname><given-names>S</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Koshida</surname><given-names>N</given-names></name><name><surname>Shiroishi</surname><given-names>T</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Kikusui</surname><given-names>T</given-names></name><name><surname>Koide</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A role for strain differences in waveforms of ultrasonic vocalizations during male-female interaction</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e22093</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0022093</pub-id><pub-id pub-id-type="pmid">21818297</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tasaka</surname><given-names>G-I</given-names></name><name><surname>Guenthner</surname><given-names>CJ</given-names></name><name><surname>Shalev</surname><given-names>A</given-names></name><name><surname>Gilday</surname><given-names>O</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name><name><surname>Mizrahi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Genetic tagging of active neurons in auditory cortex reveals maternal plasticity of coding ultrasonic vocalizations</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>871</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-03183-2</pub-id><pub-id pub-id-type="pmid">29491360</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timonin</surname><given-names>ME</given-names></name><name><surname>Kalcounis‐Rueppell</surname><given-names>MC</given-names></name><name><surname>Marler</surname><given-names>CA</given-names></name><name><surname>Ebensperger</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Testosterone pulses at the nest site modify ultrasonic vocalization types in a monogamous and territorial mouse</article-title><source>Ethology</source><volume>124</volume><fpage>804</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1111/eth.12812</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>PT</given-names></name><name><surname>Hull</surname><given-names>C</given-names></name><name><surname>Chu</surname><given-names>Y</given-names></name><name><surname>Greene-Colozzi</surname><given-names>E</given-names></name><name><surname>Sadowski</surname><given-names>AR</given-names></name><name><surname>Leech</surname><given-names>JM</given-names></name><name><surname>Steinberg</surname><given-names>J</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name><name><surname>Regehr</surname><given-names>WG</given-names></name><name><surname>Sahin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Autistic-like behaviour and cerebellar dysfunction in Purkinje cell Tsc1 mutant mice</article-title><source>Nature</source><volume>488</volume><fpage>647</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1038/nature11310</pub-id><pub-id pub-id-type="pmid">22763451</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschida</surname><given-names>K</given-names></name><name><surname>Michael</surname><given-names>V</given-names></name><name><surname>Takatoh</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>B-X</given-names></name><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Sakurai</surname><given-names>K</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A specialized neural circuit gates social vocalizations in the mouse</article-title><source>Neuron</source><volume>103</volume><fpage>459</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.025</pub-id><pub-id pub-id-type="pmid">31204083</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuji</surname><given-names>C</given-names></name><name><surname>Fujisaku</surname><given-names>T</given-names></name><name><surname>Tsuji</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Oxytocin ameliorates maternal separation-induced ultrasonic vocalisation calls in mouse pups prenatally exposed to valproic acid</article-title><source>Journal of Neuroendocrinology</source><volume>32</volume><elocation-id>e12850</elocation-id><pub-id pub-id-type="doi">10.1111/jne.12850</pub-id><pub-id pub-id-type="pmid">32321197</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuji</surname><given-names>T</given-names></name><name><surname>Mizutani</surname><given-names>R</given-names></name><name><surname>Minami</surname><given-names>K</given-names></name><name><surname>Furuhara</surname><given-names>K</given-names></name><name><surname>Fujisaku</surname><given-names>T</given-names></name><name><surname>Pinyue</surname><given-names>F</given-names></name><name><surname>Jing</surname><given-names>Z</given-names></name><name><surname>Tsuji</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Oxytocin administration modulates the complex type of ultrasonic vocalisation of mice pups prenatally exposed to valproic acid</article-title><source>Neuroscience Letters</source><volume>758</volume><elocation-id>135985</elocation-id><pub-id pub-id-type="doi">10.1016/j.neulet.2021.135985</pub-id><pub-id pub-id-type="pmid">34048819</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanus</surname><given-names>BHA</given-names></name><name><surname>Peter</surname><given-names>S</given-names></name><name><surname>Fisher</surname><given-names>SE</given-names></name><name><surname>De Zeeuw</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Region-specific Foxp2 deletions in cortex, striatum or cerebellum cannot explain vocalization deficits observed in spontaneous global knockouts</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>21631</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-78531-8</pub-id><pub-id pub-id-type="pmid">33303861</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Segbroeck</surname><given-names>M</given-names></name><name><surname>Knoll</surname><given-names>AT</given-names></name><name><surname>Levitt</surname><given-names>P</given-names></name><name><surname>Narayanan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MUPET-Mouse ultrasonic profile extraction: A signal processing tool for rapid and unsupervised analysis of ultrasonic vocalizations</article-title><source>Neuron</source><volume>94</volume><fpage>465</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id><pub-id pub-id-type="pmid">28472651</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname><given-names>BD</given-names></name><name><surname>Buckley</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Beamforming: a versatile approach to spatial filtering</article-title><source>IEEE ASSP Magazine</source><volume>5</volume><fpage>4</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1109/53.665</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volodin</surname><given-names>IA</given-names></name><name><surname>Dymskaya</surname><given-names>MM</given-names></name><name><surname>Smorkatcheva</surname><given-names>AV</given-names></name><name><surname>Volodina</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Ultrasound from underground: cryptic communication in subterranean wild-living and captive northern mole voles (<italic>Ellobius talpinus</italic>)</article-title><source>Bioacoustics</source><volume>31</volume><fpage>414</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1080/09524622.2021.1960191</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Merten</surname><given-names>S</given-names></name><name><surname>Hoier</surname><given-names>S</given-names></name><name><surname>Pfeifle</surname><given-names>C</given-names></name><name><surname>Tautz</surname><given-names>D</given-names></name><name><surname>Rosenfeld</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A role for ultrasonic vocalisation in social communication and divergence of natural populations of the house mouse (<italic>Mus musculus</italic> domesticus)</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e97244</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0097244</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Liang</surname><given-names>S</given-names></name><name><surname>Burgdorf</surname><given-names>J</given-names></name><name><surname>Wess</surname><given-names>J</given-names></name><name><surname>Yeomans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Ultrasonic vocalizations induced by sex and amphetamine in M2, M4, M5 muscarinic and D2 dopamine receptor knockout mice</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e1893</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0001893</pub-id><pub-id pub-id-type="pmid">18382674</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>MR</given-names></name><name><surname>Sangiamo</surname><given-names>DT</given-names></name><name><surname>Neunuebel</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>High channel count microphone array accurately and precisely localizes ultrasonic signals from freely-moving mice</article-title><source>Journal of Neuroscience Methods</source><volume>297</volume><fpage>44</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.12.013</pub-id><pub-id pub-id-type="pmid">29309793</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>MR</given-names></name><name><surname>Spurrier</surname><given-names>MS</given-names></name><name><surname>Roth</surname><given-names>ED</given-names></name><name><surname>Neunuebel</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Sex differences in vocal communication of freely interacting adult mice depend upon behavioral context</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0204527</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0204527</pub-id><pub-id pub-id-type="pmid">30240434</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>MR</given-names></name><name><surname>Clein</surname><given-names>RS</given-names></name><name><surname>Spurrier</surname><given-names>MS</given-names></name><name><surname>Roth</surname><given-names>ED</given-names></name><name><surname>Neunuebel</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultrashort-range, high-frequency communication by female mice shapes social interactions</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>2637</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-59418-0</pub-id><pub-id pub-id-type="pmid">32060312</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>MR</given-names></name><name><surname>Spurrier</surname><given-names>MS</given-names></name><name><surname>Sangiamo</surname><given-names>DT</given-names></name><name><surname>Clein</surname><given-names>RS</given-names></name><name><surname>Neunuebel</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Correction: Mouse vocal emission and acoustic complexity do not scale linearly with the size of a social group</article-title><source>The Journal of Experimental Biology</source><volume>224</volume><elocation-id>jeb243045</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.243045</pub-id><pub-id pub-id-type="pmid">34279028</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Shi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Altered postnatal developmental patterns of ultrasonic vocalizations in Dock4 knockout mice</article-title><source>Behavioural Brain Research</source><volume>406</volume><elocation-id>113232</elocation-id><pub-id pub-id-type="doi">10.1016/j.bbr.2021.113232</pub-id><pub-id pub-id-type="pmid">33705839</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zala</surname><given-names>SM</given-names></name><name><surname>Reitschmidt</surname><given-names>D</given-names></name><name><surname>Noll</surname><given-names>A</given-names></name><name><surname>Balazs</surname><given-names>P</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Sex-dependent modulation of ultrasonic vocalizations in house mice (<italic>Mus musculus</italic>)</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0188647</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0188647</pub-id><pub-id pub-id-type="pmid">29236704</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zala</surname><given-names>SM</given-names></name><name><surname>Reitschmidt</surname><given-names>D</given-names></name><name><surname>Noll</surname><given-names>A</given-names></name><name><surname>Balazs</surname><given-names>P</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name><name><surname>Cooper</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Automatic mouse ultrasound detector (A-MUD): A new tool for processing rodent vocalizations</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0181200</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0181200</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaytseva</surname><given-names>AS</given-names></name><name><surname>Volodin</surname><given-names>IA</given-names></name><name><surname>Ilchenko</surname><given-names>OG</given-names></name><name><surname>Volodina</surname><given-names>EV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ultrasonic vocalization of pup and adult fat-tailed gerbils (Pachyuromys duprasi)</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0219749</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0219749</pub-id><pub-id pub-id-type="pmid">31356642</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Florencio</surname><given-names>D</given-names></name><name><surname>Ba</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Maximum likelihood sound source localization and beamforming for directional microphone arrays in distributed meetings</article-title><source>IEEE Transactions on Multimedia</source><volume>10</volume><fpage>538</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1109/TMM.2008.917406</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86126.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2023.01.18.524540" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.18.524540"/></front-stub><body><p>This study introduces a novel and important hybrid strategy for recording ultrasonic vocalizations by combining data from several high quality microphones with data from a dense array of less sensitive microphones. This method enables recordings to be made from pairs and trios of freely interacting mice and accurate localization of their point of origin to convincingly determine the identity of the caller for each vocalization. This technology opens the door to new experiments incorporating analysis of vocal communication into behavioral paradigms.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86126.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Shea</surname><given-names>Stephen D</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qz8b764</institution-id><institution>Cold Spring Harbor Laboratory</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Ey</surname><given-names>Elodie</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.18.524540">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.01.18.524540v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Rodent ultrasonic vocal interaction resolved with millimeter precision using hybrid beamforming&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Brice Bathellier as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Andrew King as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Stephen D Shea (Reviewer #2); Elodie Ey (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions (for the authors):</p><p>1) Clarify the relationship of the present manuscript to https://www.biorxiv.org/content/10.1101/2021.10.22.464496v1</p><p>2) Provide ground truth accuracy measurements based on known sources (loudspeakers).</p><p>3) Provide more explanations about the impact of obstacles (e.g. other animals).</p><p>4) Clarify the impact of pre-experiment social isolation on vocal behavior.</p><p>5) Release the code online.</p><p>6) Detail male-female interactions as requested by reviewer 3 (vocalization during anogenital sniffing?). Clarify tryadic interactions.</p><p>7) Improve statistics as suggested by reviewer 3 and address all small clarification comments.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>– The calibration of the mouse probability index against ground truth data would be a useful addition to the study, to evaluate if the estimator has biases in particular situations in which the method e.g. fails more often.</p><p>– The authors find that females vocalize at a level close to the emitter identification accuracy. The authors discuss that but could be more clear whether females vocalize or not: comparing male-female USVs, showing female USVs in isolation or far from the male even if this is extremely rare.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The manuscript is well written. Here are a few comments that could help to clarify some points:</p><p>l. 92-93 + l. 439-442: The overestimation of female vocalisations during male-female encounters in previous studies might also be related to the fact that the animals used were socially isolated for at least two weeks before the experiments (for instance in Neunuebel et al. 2015, <italic>eLife</italic>; Sangiamo et al. 2020 Nat Neurosci). This type of isolation is drastic for females and leads to an increase in social motivation, and therefore ultrasonic vocalisations emission even when interacting with females (see Ey et al. 2018 Frontiers Mol Neurosci).</p><p>The proportions cited in the main text (l. 195-207) and in the legend of Figure 3 are not corresponding to the ones depicted in the graphs of Figure 3.</p><p>l. 195-207: Is this analysis only for dyadic interactions? If yes, this should be specified in the figure legend. Maybe the same analysis conducted with triadic interactions could be conducted in a supplementary figure?</p><p>l. 216: Could the authors explain why a small proportion of USVs cannot be assigned? Were the USVs too soft? Did they include specific acoustic characteristics that render them difficult to localise?</p><p>l. 231: Could the system be applied to other species? Which ones? With what types of adaptations?</p><p>l. 259: References should be added after the citation of the proportions.</p><p>l. 262: There are two times the word &quot;overall&quot; in the sentence.</p><p>l. 262-266 and Figure 4A and B: Are the analysis conducted for dyadic and triadic interactions together?</p><p>l. 314-315: As the tracking also depicts the tail basis, would it be possible to add a plot of the distance between the emitter's snout and the receiver's tail basis, to confirm the result that males vocalise mostly during anogenital sniffing of the female? This could be conducted at least on a subset of data.</p><p>l. 331: The authors mention a Bonferroni correction for multiple testing. Could they precise how many tests they conducted, over which data…</p><p>Figures 4 and 5: The legend should provide more information about the sample sizes of each condition and what represents each point (an individual?). As stated before, the individual should be taken as a factor in the statistical analyses.</p><p>Figure 5: How were females separated into dominant and subordinate? Just based on the call rate like males?</p><p>l. 331-334: To better justify the separation between dominant and subordinate that is done here based on the call rate, could the authors confirm this status with behavioural markers such as approaches/escapes, types of approaches, proximity to other individuals, and other measures?</p><p>l. 370-371: In Figure 5D, the distance to the closest female is shorter for the subordinate compared to the dominant in triadic encounters. This does not seem to fit with the statement that the dominant gets closer to the receiver when vocalising.</p><p>Figure 5C and 5D: As the females are emitting few vocalisations, are the analyses of mean energy and distance to the closest individual meaningful for females? Could the authors provide the sample size (number of vocalisations, number of individuals)?</p><p>l. 433-434: Does that mean that females turn around on themselves before vocalising? How have the authors estimated the approach of the male? Is it based on the direction of the animals? If yes, maybe the authors could reformulate the sentence by saying that females vocalise during snout-snout contact while being oriented in the same direction as a receiver or something equivalent.</p><p>l. 463: Given USVs production mechanisms that are discussed (i.e., Boulanger-Bertolus and Mouly (2021) Brain Sci), cutting vocal folds will likely not be sufficient, but maybe cutting laryngeal nerves (Pomerantz et al. 1983 Physiology and Behavior).</p><p>l. 563: What is the light intensity?</p><p>l. 580: &quot;fixed 2 min duration&quot;: This is also an issue to be solved for continuous monitoring.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86126.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions (for the authors):</p><p>1) Clarify the relationship of the present manuscript to https://www.biorxiv.org/content/10.1101/2021.10.22.464496v1</p></disp-quote><p>See response to point 1 by Reviewer 2 in Public Review.</p><disp-quote content-type="editor-comment"><p>2) Provide ground truth accuracy measurements based on known sources (loudspeakers).</p></disp-quote><p>See point 2 by Reviewer 2 in Public Review.</p><disp-quote content-type="editor-comment"><p>3) Provide more explanations about the impact of obstacles (e.g. other animals).</p></disp-quote><p>See response to point 4 by Reviewer 2 in Public Review.</p><disp-quote content-type="editor-comment"><p>4) Clarify the impact of pre-experiment social isolation on vocal behavior.</p></disp-quote><p>See Response to first major point in recommendations to Reviewers by Reviewer 3.</p><disp-quote content-type="editor-comment"><p>5) Release the code online.</p></disp-quote><p>See response to point 6 by Reviewer 2 in Public Review.</p><disp-quote content-type="editor-comment"><p>6) Detail male-female interactions as requested by reviewer 3 (vocalization during anogenital sniffing?). Clarify tryadic interactions.</p></disp-quote><p>See responses to comment relating to lines 314-315 by Reviewer 3.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>– The calibration of the mouse probability index against ground truth data would be a useful addition to the study, to evaluate if the estimator has biases in particular situations in which the method e.g. fails more often.</p></disp-quote><p>This is an interesting idea, and we agree that it would be useful, however, we have come to conclude that only an experiment with two mice in which one mouse is devocalized can provide the corresponding data to address this question (for which we do not have an ethical permit). In our opinion, basing such an MPI investigation on a speaker that produces vocalizations would not likely lead to a trustable outcome, due to the manifold differences between speaker generated sounds and mouse generated ones (e.g. , among others). Likely, results from a speaker would likely lead to an unreasonably low variability and thus unreasonably low rate of failure/misattribution, which does not translate to actual mouse vocalizations (see the new Supplementary Figure 4, where we demonstrate that the accuracy for a speaker is likely much higher than for a mouse).</p><disp-quote content-type="editor-comment"><p>– The authors find that females vocalize at a level close to the emitter identification accuracy. The authors discuss that but could be more clear whether females vocalize or not: comparing male-female USVs, showing female USVs in isolation or far from the male even if this is extremely rare.</p></disp-quote><p>This is an excellent suggestion, and we had partially demonstrated this in Figure 4C, showing that the fraction of female vocalizations further decreases substantially and significantly for instances where the snouts are far apart (relative to the localization accuracy of HvVL, i.e. &gt;50mm).</p><p>However, to give more insight into the instances where female vocalization appears most accurate at far distances, we filtered all female vocalizations and ranked them on the basis of highest relative accuracy (N.B., while maintaining a minimum separation of 5 cm between the female snout and the other snout(s)), such that the ratio between the distance of the estimated sound origin to the female snout and the male snout(s) was minimal. The reasoning was that this method could provide us with clear examples where the female did in fact vocalize. However, it is not clear at all from looking at these individual examples that the females did in fact vocalize, and rather seem to reflect either (i) extremely rare (&lt;0.1%) cases where the assigned identity differed between the Cam64 and SLIM methods while simultaneously providing usable MPI criteria for both methods that was higher for the method where the female was closest. To be clear, in these cases, the Cam64 is likely more accurate and located between two mice that have their snouts in close proximity, leading to a relatively low MPI, while the SLIM method coincidentally is located precisely on a third, relatively distant, female. The other examples that rank high for female accuracy represent (ii) less rare (&lt;1%) instances where the Cam64 does not produce a clear focal point and instead has many competing hotspots. We produced a short video (see Rebuttal Video 1 and the legend at the bottom of this document) where (i) represents the first instance shown, and (ii) represent the second and third instance shown, respectively.</p><p>To summarize, although we have a high confidence in the accuracy of our system, borderline false-positive cases for female vocalization are unavoidable. We thus cannot completely exclude the possibility of female vocalization, which is why we were tentative in our initial discussion of the subject in our manuscript, but we suspect that the number of female vocalization is likely still overestimated in our data, but likely even more in previously published studies<ext-link ext-link-type="uri" xlink:href="https://sciwheel.com/work/citation?ids=1353574,12090470&amp;pre=&amp;pre=&amp;suf=&amp;suf=&amp;sa=0,0">1,2.</ext-link></p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The manuscript is well written. Here are a few comments that could help to clarify some points:</p><p>l. 92-93 + l. 439-442: The overestimation of female vocalisations during male-female encounters in previous studies might also be related to the fact that the animals used were socially isolated for at least two weeks before the experiments (for instance in Neunuebel et al. 2015, eLife; Sangiamo et al. 2020 Nat Neurosci). This type of isolation is drastic for females and leads to an increase in social motivation, and therefore ultrasonic vocalisations emission even when interacting with females (see Ey et al. 2018 Frontiers Mol Neurosci).</p></disp-quote><p>Thank you for mentioning this important, potential confound. First, we would like to emphasize that we do not generally doubt that female vocalizations can be more abundant in other settings, e.g. multiple animals, after social isolation, etc, as also emphasized in the manuscript. To check the influence of social isolation in our case, we conducted a limited set of experiments with two female mice that were socially isolated for &gt;7 days, and then dyadically interacted with two males subsequently. The isolation time is longer than in Ey et al. 2018, where you already found a significant difference in call rate. While this is of course not a representative sample and requires further study, we would like to share the preliminary results from these experiments with the reviewers: in the 4 experiments only a handful of USV were potentially from the female mouse (&lt;10), despite a total of ~2000 USVs (mirroring results of at least one other study<ext-link ext-link-type="uri" xlink:href="https://sciwheel.com/work/citation?ids=13174375&amp;pre=&amp;suf=&amp;sa=0">6)</ext-link>. These USVs were checked manually, to be able to integrate the behavioral context into the assignment, i.e. the HyVL sub-estimates were shown on the video, and compared to the snout locations of the two mice, in addition to showing the preceding and following spectrograms. As assessed by the MPI criterion there are some USVs where the snouts are too close to draw any safe conclusions. One interesting issue that we noticed during the analysis is that occasionally the sound appears to pass underneath an animal, if the emitter is really close, but with the snout underneath the body of the other mouse, and is then most strongly visible on the other side, maybe reflecting off the platform (despite it being made from sound-absorbing foam).</p><p>In the experiments with the first female, it is noteworthy that one of the males was very active and emitted &gt;1200 USVs in 8 minutes, while the other male (cagemate, interacting with the same female on the same day) emitted no vocalizations at all.</p><p>For the experiments with the second female, we observed the same pattern: one male was very active, emitting more than 800 USVs in the recording period, while the second male did not emit a single USV (again cagemate, interacting with the same female on the same day).</p><p>While we do not doubt the results from your work, particularly if they are in female(juvenile female) interactions as in your work, or in resident intruder interactions with an anesthetized intruder (e.g Hammerschmidt et al. PLOS One, 2012)<ext-link ext-link-type="uri" xlink:href="https://sciwheel.com/work/citation?ids=2070326&amp;pre=&amp;suf=&amp;sa=0">7,</ext-link> which are beyond doubt evidence for female vocalizations. However, in our opinion this highlights the relevance of highly accurate localization systems for fully addressing this question in the future for all contexts of interest.</p><disp-quote content-type="editor-comment"><p>l. 231: Could the system be applied to other species? Which ones? With what types of adaptations?</p></disp-quote><p>There is a substantial number of species that the localization system could be applied to, essentially all animals that vocalize. The precise accuracy of localization will likely depend on the frequency range of vocalizations, with the highest accuracy possible for higher frequencies, however, still very high accuracy for lower frequencies, e.g. we can typically localize steps or scratches of a mouse, which have most energy &lt;10 kHz. A non-exhaustive list of animals would be rats, cats, different species of insects (e.g. grasshoppers or crickets) and most bird species. For studies in the plane, i.e. on a flat surface the present acoustic localization system could be used 'as is', with the only required adaptation to retrain the spatial tracking. For studies in space, the analysis of the origin of the sound would have to be extended by a depth dimension, which would mostly increase computation time, but not introduce any fundamental changes to the localization analysis otherwise. Visual tracking in 3D should probably be done with depth cameras instead, and visual occlusion could become a bigger issue. We have removed the mentioning from this location and instead added a paragraph to the discussion containing the above information.</p><disp-quote content-type="editor-comment"><p>l. 314-315: As the tracking also depicts the tail basis, would it be possible to add a plot of the distance between the emitter's snout and the receiver's tail basis, to confirm the result that males vocalise mostly during anogenital sniffing of the female? This could be conducted at least on a subset of data.</p></disp-quote><p>No problem, we have added the corresponding plot as a new extended data figure to Figure 4. As expected, a large fraction of the male vocalizations are emitted, when the female mouse's abdomen is very close to the male snout (B). Conversely, the male abdomen was in a range of different relative locations to the female's snout (A). Note, however, that only dyadic interactions are shown here, because the tail marker was only tracked using the automatic tracking, which in turn was only available for dyadic interactions Automatic tracking was less accurate than manual tracking, which might help explain why a larger fraction of USVs were assigned to the female. See figure legend in the manuscript for additional interpretation of this result.</p></body></sub-article></article>