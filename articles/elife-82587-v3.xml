<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82587</article-id><article-id pub-id-type="doi">10.7554/eLife.82587</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Visual and motor signatures of locomotion dynamically shape a population code for feature detection in <italic>Drosophila</italic></article-title></title-group><contrib-group><contrib contrib-type="author" id="author-114305"><name><surname>Turner</surname><given-names>Maxwell H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4164-9995</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290521"><name><surname>Krieger</surname><given-names>Avery</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290522"><name><surname>Pang</surname><given-names>Michelle M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-4720"><name><surname>Clandinin</surname><given-names>Thomas R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6277-6849</contrib-id><email>trc@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Neurobiology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e82587</elocation-id><history><date date-type="received" iso-8601-date="2022-08-10"><day>10</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-25"><day>25</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-07-14"><day>14</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.14.500082"/></event></pub-history><permissions><copyright-statement>Â© 2022, Turner et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Turner et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82587-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-82587-figures-v3.pdf"/><abstract><p>Natural vision is dynamic: as an animal moves, its visual input changes dramatically. How can the visual system reliably extract local features from an input dominated by self-generated signals? In <italic>Drosophila</italic>, diverse local visual features are represented by a group of projection neurons with distinct tuning properties. Here, we describe a connectome-based volumetric imaging strategy to measure visually evoked neural activity across this population. We show that local visual features are jointly represented across the population, and a shared gain factor improves trial-to-trial coding fidelity. A subset of these neurons, tuned to small objects, is modulated by two independent signals associated with self-movement, a motor-related signal, and a visual motion signal associated with rotation of the animal. These two inputs adjust the sensitivity of these feature detectors across the locomotor cycle, selectively reducing their gain during saccades and restoring it during intersaccadic intervals. This work reveals a strategy for reliable feature detection during locomotion.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>central visual pathways</kwd><kwd>suppression</kwd><kwd>spatial vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>F32-MH118707</award-id><principal-award-recipient><name><surname>Turner</surname><given-names>Maxwell H</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K99-EY032549</award-id><principal-award-recipient><name><surname>Turner</surname><given-names>Maxwell H</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EY022638</award-id><principal-award-recipient><name><surname>Clandinin</surname><given-names>Thomas R</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS110060</award-id><principal-award-recipient><name><surname>Clandinin</surname><given-names>Thomas R</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>GRFP</award-id><principal-award-recipient><name><surname>Krieger</surname><given-names>Avery</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014037</institution-id><institution>National Defense Science and Engineering Graduate</institution></institution-wrap></funding-source><award-id>Fellowship</award-id><principal-award-recipient><name><surname>Pang</surname><given-names>Michelle M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurons responsible for detecting local visual features are modulated by visual and motor-related forms of gain control that increase the threshold for detection when self-generated movement signals would dominate visual input.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Sighted animals frequently move their bodies, heads, and eyes to achieve their behavioral goals and to actively sample the environment. As a result, the image on the retina is frequently subject to self-generated motion. This presents a challenge for the visual system, as visual circuitry must extract and represent specific features of the external visual scene in a rapidly changing context where the dominant sources of visual changes on the retina may be self-generated. While this problem has been well studied in the context of motion estimation (<xref ref-type="bibr" rid="bib15">Borst et al., 2010</xref>; <xref ref-type="bibr" rid="bib19">Britten, 2008</xref>), the broader question of how visual neurons might extract local features of the scene under naturalistic viewing conditions is relatively poorly understood. How do visual neurons selectively encode local features of interest under these dynamic conditions?</p><p>Local feature detection during self-motion presents unique challenges. For detecting widefield motion, or large static features of the scene like oriented edges and landmarks, the visual scene is intrinsically redundant, as many neurons distributed across the visual field can encode information that is relevant to the feature of interest even as the scene moves. Conversely, local features like prey, conspecifics, or approaching predators engage only a small part of the visual field, dramatically reducing the redundancy of the visual input. In addition, neurons that selectively respond to small features could also be activated by high spatial frequency content in the broader scene, potentially corrupting their responses under naturalistic viewing conditions. Neurons that respond selectively to local visual features have been described in many species, including flies, amphibians, rodents, and primates (<xref ref-type="bibr" rid="bib39">KeleÅ and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib41">Kerschensteiner, 2022</xref>; <xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Lettvin et al., 1959</xref>; <xref ref-type="bibr" rid="bib70">Pasupathy and Connor, 2001</xref>; <xref ref-type="bibr" rid="bib72">Piscopo et al., 2013</xref>). However, these studies have typically been conducted either in non-behaving animals, or under conditions of visual fixation. Here, we explore the neural mechanisms by which local feature detection is made robust to the visual inputs and behavioral signals associated with natural vision.</p><p>Strategies for reliable visual feature detection during self-motion fall into one of at least three categories. First, behavioral strategies can help mitigate the impact of self-motion on visual feature encoding by changing the nature of the neural encoding task at hand. For example, compensatory movements of the eyes, head, or body can stabilize the image on the retina during self-motion (<xref ref-type="bibr" rid="bib4">Angelaki and Hess, 2005</xref>; <xref ref-type="bibr" rid="bib36">Hardcastle and Krapp, 2016</xref>; <xref ref-type="bibr" rid="bib48">Land, 1999</xref>; <xref ref-type="bibr" rid="bib96">Walls, 1962</xref>), and saccadic movement dynamics compress the fraction of time during which large self-generated motion signals corrupt retinal input (<xref ref-type="bibr" rid="bib56">Martinez-Conde et al., 2013</xref>; <xref ref-type="bibr" rid="bib94">Van Der Linde et al., 2009</xref>; <xref ref-type="bibr" rid="bib101">Wurtz, 2018</xref>; <xref ref-type="bibr" rid="bib26">Cruz et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Geurten et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Collett and Land, 1975b</xref>). In other cases, behavior is shaped by the demands of a specific visual task. For example, dragonflies and other predatory insects often approach prey from below, increasing the likelihood that a target will be seen against a background of the low contrast sky (<xref ref-type="bibr" rid="bib64">NordstrÃ¶m and OâCarroll, 2009</xref>), and male hoverflies hover in place while monitoring for conspecific territorial trespassers (<xref ref-type="bibr" rid="bib23">Collett and Land, 1975a</xref>), ensuring that self-generated motion signals are low during a demanding visual discrimination task.</p><p>Second, neural mechanisms can exploit the fact that self-generated motion produces characteristic sensory inputs. For example, visual surrounds can be tuned to the global motion signals characteristic of self-motion, allowing for self-motion signals to be subtracted from excitatory center signals that code for a feature of interest (<xref ref-type="bibr" rid="bib6">Aptekar et al., 2015</xref>; <xref ref-type="bibr" rid="bib10">Baccus et al., 2008</xref>; <xref ref-type="bibr" rid="bib66">Olveczky et al., 2003</xref>; <xref ref-type="bibr" rid="bib28">Egelhaaf, 1985</xref>; <xref ref-type="bibr" rid="bib22">Collett, 1971</xref>). However, in some flying insects, target-detecting neurons are tightly tuned for very small visual targets (<xref ref-type="bibr" rid="bib63">NordstrÃ¶m and OâCarroll, 2006</xref>), even in the context of moving, cluttered backgrounds (<xref ref-type="bibr" rid="bib62">NordstrÃ¶m et al., 2006</xref>; <xref ref-type="bibr" rid="bib99">Wiederman and OâCarroll, 2011</xref>), suggesting that multiple levels of spatial inhibition can work together to shape feature selectivity (<xref ref-type="bibr" rid="bib14">Bolzon et al., 2009</xref>), and that robust feature detection need not rely on relative motion cues (<xref ref-type="bibr" rid="bib65">NordstrÃ¶m, 2012</xref>; <xref ref-type="bibr" rid="bib64">NordstrÃ¶m and OâCarroll, 2009</xref>; <xref ref-type="bibr" rid="bib98">Wiederman et al., 2008</xref>).</p><p>The third strategy for reliable vision during self-motion uses signals related to the animalsâ motor commands or behavioral states to modulate neural response gain. For example, the motor commands that initiate primate saccades produce efference copy signals that are associated with neural gain changes and a perceptual decrease in sensitivity called saccadic suppression (<xref ref-type="bibr" rid="bib12">Binda and Morrone, 2018</xref>; <xref ref-type="bibr" rid="bib16">Bremmer et al., 2009</xref>; <xref ref-type="bibr" rid="bib101">Wurtz, 2018</xref>). In flies, efference copy signals can cancel expected motion in widefield motion-sensitive neurons during flight (<xref ref-type="bibr" rid="bib29">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Kim et al., 2017</xref>), but can also provide independent information about intended movements (<xref ref-type="bibr" rid="bib33">Fujiwara et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Fujiwara et al., 2022</xref>; <xref ref-type="bibr" rid="bib26">Cruz et al., 2021</xref>). In this way, neural response gain is modulated so that motion-sensitive neurons encode unexpected deviations in motion signals after accounting for behavior.</p><p>Previous studies have each examined these respective strategies in the context of single-cell types. However, how do these varied strategies work together across a population of disparately tuned visual neurons? We explore this issue using populations of visual projection neurons (VPNs) in <italic>Drosophila</italic>. VPNs are situated at a critical computational and anatomical bottleneck through which highly processed visual information moves from the optic lobes to the central brain. A subset of VPNs, the Lobula Columnar (LC) and Lobula Plate Lobula Columnar (LPLC) cells (<xref ref-type="bibr" rid="bib30">Fischbach and Dittrich, 1989</xref>; <xref ref-type="bibr" rid="bib67">Otsuna and Ito, 2006</xref>; <xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>) make up a large fraction of all VPN types, thus accounting for a substantial portion of the visual information available to guide behavior. These cell types encode distinct local visual features with behavioral relevance, including looming objects (<xref ref-type="bibr" rid="bib1">Ache et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Klapoetke et al., 2017</xref>) and small moving objects (<xref ref-type="bibr" rid="bib39">KeleÅ and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib76">Ribeiro et al., 2018</xref>) (for a recent survey of VPN visual tuning, see <xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>), and project to small, distinct regions in the central brain called optic glomeruli (<xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Panser et al., 2016</xref>). Previous work has also implicated some types of LCs in figure-ground discrimination, that is, the ability to detect an object moving independently of a global background motion signal (<xref ref-type="bibr" rid="bib6">Aptekar et al., 2015</xref>). Each optic glomerulus receives input from all of the individual cells belonging to a single-cell type, resulting in a functional map in the central brain (<xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>). Moreover, both stimulation and silencing experiments argue that at least some VPN classes strongly modulate specific visually guided behaviors (<xref ref-type="bibr" rid="bib37">Hindmarsh Sten et al., 2021</xref>; <xref ref-type="bibr" rid="bib87">Tanaka and Clark, 2020</xref>; <xref ref-type="bibr" rid="bib88">Tanaka and Clark, 2022</xref>). Finally, the visual tuning of VPN types is heterogeneous across the population, allowing us to explore how strategies for reliable visual encoding during self-motion vary across differently tuned populations.</p><p>To explore how local visual features are represented across populations of VPNs, we developed a new method to register functional imaging data to the fruit fly connectome, allowing us to measure neural responses across many optic glomeruli simultaneously. We show that this method allowed for reliable and repeatable measurement of VPN responses. This population imaging method allowed us to measure the covariance of optic glomerulus population responses to visual stimuli. This analysis revealed strongly correlated trial-to-trial variability across glomeruli, which improves stimulus encoding fidelity. Importantly, this could not have been inferred from non-simultaneous measurements. We next demonstrate that walking behavior selectively suppressed responses of small object detecting glomeruli, leaving responses to looming objects unchanged. We then focus on body rotations as an example of self-motion that introduces large, uniform displacements in visual input during behavior to show that visual stimuli characteristic of rotational self-motion, including those produced by locomotor saccades, also suppressed VPN responses to small objects. Finally, we show that these two forms of gain controlâvisual and motor-associatedâcan be independently recruited and reinforce one another when both are active. Taken together, these results reveal that both visual and motor cues associated with self-motion can tune local feature detecting VPNs, adjusting their sensitivity to match the dynamics of natural walking behavior. This suggests a strategy for resolving the ambiguities associated with detecting external object motion in a scene dominated by self-generated visual motion.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Visual rotation complicates local feature detection</title><p>To build intuition about how self-generated motion might impact local feature selectivity, we designed a task inspired by VPN selectivity to small, moving objects (<xref ref-type="bibr" rid="bib39">KeleÅ and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>), and by target discrimination tasks performed by other flying insects (<xref ref-type="bibr" rid="bib28">Egelhaaf, 1985</xref>; <xref ref-type="bibr" rid="bib63">NordstrÃ¶m and OâCarroll, 2006</xref>). For this analysis, we focused on the impact of rotational self-motion, because it is a prominent component of self-generated optic flow during movement that causes large movement signals that are uniform across the visual field. In this detection task, a 15Â° dark patch moved on top of a grayscale natural image background, through a receptive field whose size was typical of small object detecting LCs (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). When the natural image background was static, as would be the case if a stationary fly were observing an external moving object in a rich visual environment, detecting the moving patch is trivial given the change in local luminance and/or spatial contrast as the patch traverses the receptive field (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). How is this detection task impacted by rotational self-motion? We simulated self-generated rotational motion by moving the background image at a single, constant velocity (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This background motion caused large fluctuations in local luminance and spatial contrast, reflecting the heterogeneous spatial structure of the scene (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, red traces). These fluctuations were often larger than the changes induced by the moving patch alone (e.g., compare <xref ref-type="fig" rid="fig1">Figure 1B</xref> to <xref ref-type="fig" rid="fig1">Figure 1D</xref>). Moreover, with an independently moving patch added to the foreground, the change in local luminance or contrast was negligible for this example image (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, blue traces), making discrimination between these two conditions very difficult.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Small spot detection is unreliable during self-generated rotation.</title><p>(<bold>A</bold>) Schematic illustrating the stimulus and detection task. A small dark spot on top of a grayscale natural image moved through a visual receptive field (white dashed circle). (<bold>B</bold>) When the background image was stationary, small spot detection was trivial using either local luminance (top) or spatial contrast (bottom) cues. Vertical dashed lines indicate the window of time that the spot passes through the receptive field. (<bold>C</bold>) To mimic an object detection task during self-generated rotation, we moved the background image independently of the spot with a variable speed. (<bold>D</bold>) Movement of the background image alone (red trace) caused dramatic fluctuations in luminance and contrast within the receptive field. The addition of the small moving spot (blue trace) caused relatively small changes in the luminance or contrast signal, which depends on the spatial structure of the image. (<bold>E</bold>) Discriminability of the spot based on luminance (left) or contrast (right) during the time period when the spot passed through the receptive field, as a function of background image speed. Points indicate mean Â± S.E.M. across a collection of 20 grayscale natural images. Even for slow background speeds, detection was corrupted, and the discriminability of the spot decreased further as the background speed increased.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig1-v3.tif"/></fig><p>We quantified discriminability, dâ², between traces where only the background image moved and traces where the small patch moved on top of the moving background, using either local luminance signals (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, left) or local contrast signals (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, right). This metric captures the difference between the mean responses to âspot presentâ versus âspot absentâ normalized by the standard deviation of the response traces (see Materials and methods). dâ² reflects the z-scored difference between the responses to these two conditions, meaning that a dâ² of 0 corresponds to chance under an ideal observer model. With a static or absent background, the discriminability of the patch is perfect. Across a collection of 20 natural images (<xref ref-type="bibr" rid="bib95">van Hateren and van der Schaaf, 1998</xref>), moving at velocities between 20Â°/s and 320Â°/s, small object detection was corrupted even for small amounts of background motion, and discriminability decreased further as background motion increased (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). These observations suggest that as self-motion signals increase, neurons that respond selectively to local features like small moving objects might increase their response thresholds in order to avoid relaying false positive signals.</p></sec><sec id="s2-2"><title>A connectome-based alignment method to measure population activity across optic glomeruli</title><p>To efficiently characterize the responses of individual VPNs to many visual stimuli, and to relate the gain of multiple VPNs with one another and to animal behavior, we needed to measure responses across different VPN types simultaneously. Presently, specific driver lines exist to target single VPN types in a single experiment (<xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>), but no approach exists to measure across many VPN types simultaneously. To develop such a population recording approach, we exploited the fact that optic glomeruli are physically non-overlapping (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Each optic glomerulus receives dominant input from one type of LC or LPLC cell (with one known exception being LPLC4/LC22 <xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>, not included in this study). At the same time, the fly brain is highly stereotyped, meaning that by aligning functional imaging data to the <italic>Drosophila</italic> connectome (<xref ref-type="bibr" rid="bib81">Scheffer et al., 2020</xref>), we could use the positions of VPN presynaptic active zones (T-bars) to identify voxels that correspond to specific glomeruli.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A method to extract optic glomerulus population responses from bulk-labeled neuropil.</title><p>(<bold>A</bold>) Schematic of the left half of the brain showing optic lobe and the optic glomeruli of the central brain, which receive inputs from distinct visual projection neurons. Me: Medulla, LP: Lobula Plate, Lo: Lobula, PVLP: Posterior Ventrolateral Protocerebrum, PLP: Posterior Lateral Protocerebrum. (<bold>B</bold>) Schematic of imaging and stimulation setup. (<bold>C</bold>) LC4 neurons expressing plasma membrane-bound myr::tdTomato (magenta) and presynaptically localized syt1GCaMP6f (green), which is enriched in axons in the optic glomerulus. (<bold>D</bold>) For pan-glomerulus imaging, cholinergic neurons express myr::tdTomato (purple) and syt1GCaMP6f (green). The optic glomeruli in the PVLP/PLP can be seen. (<bold>E</bold>) Pipeline for generating the mean brain from in vivo, high-resolution anatomical scans and aligning this mean brain to the JRC2018 template brain. Using this bridging registration, neuron and presynaptic site locations from the hemibrain connectome can be transformed into the mean brain space, allowing in vivo voxels to be assigned to distinct, non-overlapping optic glomeruli. (<bold>F</bold>) Montage showing z planes (rows) of the registered brain space for the mean brain myr::tdTomato (purple) and syt1GCaMP6f (green) channels (first and second columns, respectively), JRC2018 template brain (third column) and optic glomeruli map (fourth column). (<bold>G</bold>) Mean brain images at indicated z levels showing distinct glomerulus locations of interest. (<bold>H</bold>) For the locations of interest in (<bold>G</bold>), the optic glomerulus map is overlaid on the mean brain (first column) and alignment is shown for each of the 10 individual flies (remaining columns). For all images, scale bar is 25 Î¼m.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Number of voxels in each LC/LPLC glomerulus.</title><p>For each LC/LPLC glomerulus in or near the imaging volume, the number of voxels in each transformed glomerulus mask that lies within the imaging volume is shown for 10 flies. Glomeruli highlighted in red were excluded from this analysis because they were consistently small, or often not included in the volume. The LPLC4 glomerulus was excluded because this glomerulus is co-extensive with the LC22 glomerulus. The horizontal line indicates a threshold we set for further excluding glomeruli from individual flies for being small in that flyâs imaging volume. For example, the LC17 glomerulus was sometimes excluded from individual flies because it lies at the bottom of our imaging volume, and brain motion or slight changes in the imaging depth caused the imaged portion of the LC17 glomerulus to be very small.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig2-figsupp1-v3.tif"/></fig></fig-group><p>We selected the optic glomeruli in the Posterior Ventrolateral Protocerebrum (PVLP) and Posterior Lateral Protocerebrum (PLP) for imaging (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), because this region of the brain contains the majority of known optic glomeruli in a confined volume. We imaged the left PVLP/PLP using a two-photon resonant scanning microscope, which allowed for sampling of the volume of interest at â¼7 Hz (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, see Materials and methods). As previous work had demonstrated that individual VPN cells respond to visual stimuli with monophasic calcium responses that span several hundred milliseconds (as measured using GCaMP6f <xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>), this volume rate provides dense temporal sampling of each VPN type.</p><p>Optic glomeruli contain neurites from many neuron types, including the presynaptic terminals of their dominant VPN input, but also postsynaptic targets of those cells as well as other local interneurons. We used a two-pronged approach to bias measured calcium signals toward those selective to presynaptic terminals of VPNs. First, we developed a GCaMP6f variant that preferentially localizes to presynaptic terminals (syt1GCaMP6f). This construct showed much brighter GCaMP6f fluorescence in axon terminals in the optic glomerulus compared to dendrites in the lobula (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Second, as almost every LC and LPLC neuron is cholinergic, we specifically targeted cholinergic neurons using a ChAT-T2A knock-in Gal4 driver line (<xref ref-type="bibr" rid="bib27">Deng et al., 2019</xref>). Using this driver line, we expressed both syt1GCaMP6f as well as myr::tdTomato, a plasma-membrane bound red structural indicator that was used for motion correction and alignment (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>To extract glomerulus responses from our in vivo imaging volumes, we used techniques similar to other recent imaging alignment studies in the <italic>Drosophila</italic> brain (<xref ref-type="bibr" rid="bib17">Brezovec et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">Mann et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Pacheco et al., 2021</xref>; <xref ref-type="bibr" rid="bib90">Turner et al., 2021</xref>). First, we generated a âmean brainâ volume by iteratively aligning and averaging a collection of high-resolution, in vivo anatomical scans of the volume of interest (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, n=11 flies). Next, we used the syt1GCaMP6f channel of the mean brain to align to the JRC2018 template brain (<xref ref-type="fig" rid="fig2">Figure 2F</xref>; <xref ref-type="bibr" rid="bib13">Bogovic et al., 2020</xref>). Finally, we generated a glomerulus map using locations of the presynaptic T-bars belonging to LC and LPLC neurons, which we extracted from the hemibrain connectome (<xref ref-type="bibr" rid="bib81">Scheffer et al., 2020</xref>), and aligned it to the JRC2018 template brain. Using the mean brain and mean brain-template alignment, we could consistently align individual volumes to the mean brain and to the glomerulus map (<xref ref-type="fig" rid="fig2">Figure 2G and H</xref>). This method, which we refer to as pan-glomerulus imaging, allowed us to assign voxels in a single flyâs in vivo volume to a specific optic glomerulus. In this paper, we focus on 13 glomeruli (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>).</p><p>To test whether pan-glomerulus imaging reliably captured visually driven calcium responses across glomeruli, we presented a suite of synthetic stimuli meant to explore VPN feature detection (<xref ref-type="bibr" rid="bib39">KeleÅ and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib45">Klapoetke et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>; <xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>). Our stimulus suite therefore consisted of small, moving spots, static flicker, looming spots, moving bars, and other stimuli (see Materials and methods). <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows mean glomerulus responses across animals to these stimuli. As expected, the visual tuning measured in one glomerulus in one fly was very similar to tuning seen in corresponding glomeruli measured in other animals (<xref ref-type="fig" rid="fig3">Figure 3B and C</xref>). To determine whether our pan-glomerulus imaging method accurately captured the visual tuning of the VPN that provides the major input to that glomerulus, we used cell-type-specific split-Gal4 driver lines for select VPN types (LC18, LC9, and LC4), chosen because together they span the anatomical volume of interest, and presented the same stimulus suite (<xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>). We then compared these targeted recordings to those previously measured in the corresponding glomeruli, using our population imaging approach. For each of these VPN/glomerulus pairs, the responses and visual tuning looked qualitatively similar (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) and were highly correlated (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Taken together, these results show that pan-glomerulus imaging reliably measures visually driven responses across a population of optic glomeruli, and that these visual responses are dominated by VPN signals.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Pan-glomerulus imaging reliably measures optic glomerulus responses dominated by visual projection neurons.</title><p>(<bold>A</bold>) Responses of thirteen optic glomeruli to a panel of synthetic visual stimuli (top, see Materials and methods). Responses are shown according to the indicated stimulus order, but stimuli were presented in randomly interleaved trial order. Shown are mean glomerulus response traces across 10 flies, shading indicates S.E.M. We hierarchically clustered mean glomerulus responses to yield four functional groups of glomeruli. (<bold>B</bold>) Visual tuning can be reliably estimated in single flies using pan-glomerulus imaging. For each glomerulus, we computed the correlation between each individual fly tuning and the mean tuning (excluding that fly). Large dots and bars indicate mean Â± S.E.M., and small dots correspond to individual flies. (<bold>C</bold>) Example responses of three glomeruli in individual flies to a 15Â° bright moving spot. For each panel, the colored trace is the across-fly average response and the gray trace is the individual fly response. (<bold>D</bold>) Comparison of glomerulus tuning to the LC neurons that dominate their input, for three glomerulus/LC pairs. Left: glomerulus map and example image of the LC axons. Right: syt1GCaMP6f responses to the stimulus panel above. Colored traces show tuning measured using pan-glomerulus imaging procedure and black traces show split-Gal4 LC responses. (<bold>E</bold>) For the three LC types in (<bold>D</bold>), the tuning of LC axons is highly correlated with the tuning of corresponding optic glomeruli (pan-glomerulus imaging: n=10 flies; Split-Gal4 imaging: n=5, 6, and 4 flies for LC18, LC9, and LC4, respectively). For all calcium traces, Scale bar is 2 s and 25% dF/F. r is the Pearson correlation coefficient.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig3-v3.tif"/></fig><p>At a high level, this initial suite of stimuli revealed that optic glomeruli show broad, overlapping tuning (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) in line with previous observations using cell-type-specific driver lines (<xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>). To conveniently organize the results presented in subsequent analyses, we applied a hierarchical clustering approach to identify functional groupings of VPN types based on their responses to our synthetic stimulus suite. Group 1 was characterized by LCs that responded to moving spots 5Â° in diameter (the smallest stimuli presented here), and showed relatively weak responses to loom and vertical bars. Group 2 contained glomeruli that were not sensitive to very small objects and showed strong loom responses. Group 3 contained glomeruli that were typically only weakly driven by any of these stimuli but responded to looming stimuli. Finally, group 4 glomeruli had large responses to vertical bars and medium and large moving spots as well as some loom sensitivity.</p></sec><sec id="s2-3"><title>Population activity is modulated by a dominant gain factor which impacts stimulus coding fidelity</title><p>Previous characterization of VPNs relied on targeting each individual cell class using cell-type-specific driver lines (<xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>; <xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>). This allows for the measurement of neural response mean and variance, but not the covariance among different VPNs, which requires simultaneous measurement. Trial-by-trial covariance can have a dramatic impact on stimulus encoding (<xref ref-type="bibr" rid="bib9">Averbeck and Lee, 2006</xref>; <xref ref-type="bibr" rid="bib8">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib77">Romo et al., 2003</xref>; <xref ref-type="bibr" rid="bib105">Zylberberg et al., 2016</xref>), and can shed light on the circuit mechanisms that govern sensory computation (<xref ref-type="bibr" rid="bib3">Ala-Laurila et al., 2011</xref>; <xref ref-type="bibr" rid="bib74">Rabinowitz et al., 2015</xref>). To examine the covariance structure of optic glomerulus responses, we presented a subset of the synthetic stimuli (<xref ref-type="fig" rid="fig3">Figure 3</xref>), and collected 30 trials for each stimulus. We observed significant trial-to-trial variability. Indeed, on some presentations of a stimulus which, on average, drives a strong response, many glomeruli failed to respond at all. Moreover, this large modulation in response gain was shared across many glomeruli on a trial-by-trial basis (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>). When we averaged the trial-to-trial correlations across flies, we observed strong, positive pairwise correlations across the glomerulus population (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), and across stimuli (<xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). Because we also collected myr::tdTomato fluorescence through the red channel, we could use this structural signal to assess whether the trial covariance we observed in syt1GCaMP6f responses was due to other factors not associated with neural responses, like brain motion that was not removed during motion correction, or other imaging factors. As expected, myr::tdTomato signals showed very little modulation across trials (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, gray traces), and as a result the trial to trial covariance was weaker and showed a qualitatively different structure than the covariance in syt1GCaMP6f signals (<xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>). This indicates that trial covariance in syt1GCaMP6f signals was dominated by visually driven responses.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Glomerulus responses are modulated by a shared gain factor that impacts stimulus encoding fidelity.</title><p>(<bold>A</bold>) For a reduced stimulus set (images above), we presented many trials in a randomly interleaved order. For display, we have grouped responses by stimulus identity. Red marks indicate stimulus presentation times. Example single-trial responses for representative glomeruli show large trial-to-trial variability. Scale bar is 4 s and 25% dF/F. Gray traces below show simultaneous glomerulus signals from the red (myr::tdTomato) channel showing that fluorescence changes are due to visually driven responses, not motion artifacts or other imaging factors. (<bold>B</bold>) For the example glomeruli in (<bold>A</bold>), plotting one glomerulus response amplitude against another for a given stimulus (here, a 15Â° moving spot), reveals high correlated variability. Each point is a trial. Ellipses show 2D Gaussian fit derived from the trial covariance matrix. r is the Pearson correlation coefficient. (<bold>C</bold>) Trial-to-trial variability is strongly correlated across different glomeruli. Heatmap shows the average correlation matrix across all stimuli and flies (n=17 flies). (<bold>D</bold>) Single-trial responses of 13 optic glomeruli, concatenated together for each trial. Stimulus identity is indicated to the left. Black dots indicate the peak responses of each glomerulus on each trial, which will be used for stimulus decoding. The response amplitudes for all 13 glomeruli were used to train a multinomial logistic regression model to predict stimulus identity (see Materials and methods). (<bold>E</bold>) Confusion matrix, with rows and columns corresponding to stimulus identity above. (<bold>F</bold>) We used held-out data to test the ability of the decoding model to predict the stimulus identity given the single-trial response amplitudes of different groupings of glomeruli. In each barchart, gray bars correspond to a model with access to all 13 glomerulus responses for each trial. Colored bars show mean Â± S.E.M. performance of a model with access to only the indicated functional group of glomeruli. (<bold>G</bold>) Trial shuffling population responses remove pairwise correlations among glomeruli (insets to the left show correlation matrices before and after trial shuffling). Right: Decoding model performance, averaged across all stimuli, for each fly (n=11 flies). Large marker shows mean Â± S.E.M. (<bold>H</bold>) Performance of the decoding model suffers across all stimulus classes when trial-to-trial correlations are removed.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Trial correlation matrix for each stimulus class.</title><p>From the trial covariance analysis in <xref ref-type="fig" rid="fig4">Figure 4</xref>, each panel shows the trial correlation matrix for each of the six stimuli shown in the reduced stimulus set for these experiments. Positive correlations are seen for all stimuli, but tend to be higher for stimuli that drive glomerulus responses strongly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig4-figsupp1-v3.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>Trial covariance matrix for both myr::tdTomato and sytGCaMP6f fluorescence channels.</title><p>Trial covariance matrix for both the red (myr::tdTomato) and green (syt1GCaMP6f) fluorescence signals, averaged across flies (n=18 flies), showing that trial covariance in the green channel is dominated by neural responses, not by brain movement or other factors associated with imaging. Residual covariance structure in the red channel is present primarily for glomeruli with weak visually driven responses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig4-figsupp2-v3.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 3.</label><caption><title>Decoding stimulus identity for a reduced subset of behaviorally discriminable stimulus classes.</title><p>We performed our decoding analysis on a reduced subset of visual stimulus classes that are known to drive distinct behaviors in flies: a drifting grating, a dark spot, a looming spot, and a vertically oriented bar. Gray bars show model performance with access to all optic glomeruli, colored bars show model performance using only responses from the indicated glomerulus group. Horizontal line shows 25% chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig4-figsupp3-v3.tif"/></fig></fig-group><p>This large response variance suggests a challenge for downstream circuits integrating information across optic glomeruli: how can a visual feature be reliably decoded when response strength shows such large variability from trial to trial? To explore this issue, we implemented a multinomial logistic regression decoder to predict the identity of a stimulus given single-trial population responses. Since the animal does not have a priori information about when or where a local visual feature might appear, we did not want the model to be able to use different stimulus dynamics to trivially learn the decoding task based on response timing. Therefore, we trained the model using only the peak response amplitude from each glomerulus on each trial (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), and tested the ability of the model to predict stimulus identity on held-out trials. This decoding model performed with an overall accuracy rate of around 40%, on average (compared to a chance performance of 7%), and performance for some stimulus classes was considerably higher (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). For example, for dark moving spots with diameter 5Â°, 15Â°, and 50Â°, performance was 55%, 67%, and 78%, respectively. For a slowly looming spot, performance was 72%. This high performance was surprising given that the model only had access to scalar response amplitudes on each trial, which themselves displayed high trial-to-trial variability.</p><p>We next asked how a model provided with different subsets of optic glomeruli performed on the decoding task by training the model using only responses from a single functional group (identified in <xref ref-type="fig" rid="fig3">Figure 3</xref>). As expected, decoding models with access to responses from only a subset of the population performed more poorly than those with access to the full glomerulus population. Strikingly, however, subpopulations of glomeruli were unable to perform as well as the full population even for correctly classifying the stimuli to which they were most strongly tuned (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). For example, group 1 contains the glomeruli that showed strong responses to small, 5Â° spots. Yet a model trained using the responses from that group alone was unable to encode information about this stimulus nearly as well as the full population model. To test whether similar distributed representation exists for a reduced subset of stimuli that are known to be discriminable by flies, we repeated this analysis focusing only on four stimuli that drive distinct visual behaviors: a drifting grating, a 15Â° spot, a looming spot, and a vertically oriented bar (<xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3</xref>). As expected because the task is easier, overall decoding performance across the population was higher. As with the larger stimulus set, many stimulus classes could be decoded at above chance level by multiple glomeruli groups, and for some stimuli, like the drifting grating and vertically oriented bar, decoding ability across the population was higher than for any individual group.</p><p>In other sensory systems, positive correlations in neural responses can mitigate the effects of trial-to-trial variability in cases of heterogeneous population tuning (<xref ref-type="bibr" rid="bib9">Averbeck and Lee, 2006</xref>; <xref ref-type="bibr" rid="bib31">Franke et al., 2016</xref>; <xref ref-type="bibr" rid="bib77">Romo et al., 2003</xref>; <xref ref-type="bibr" rid="bib105">Zylberberg et al., 2016</xref>). This is because, relative to uncorrelated variability, correlated variability can be oriented in a direction in population response space where it does not interfere with stimulus decoding (see Discussion and <xref ref-type="bibr" rid="bib73">Pruszynski and Zylberberg, 2019</xref>). We therefore hypothesized that the strong trial-to-trial gain correlations (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) were partly responsible for the high decoding performance for some stimuli in spite of the high response variance. To test this, we trained and tested the decoding model using trial-shuffled responses, such that for each glomerulus the mean and variance of each stimulus response were the same, but the trial-to-trial correlations were removed (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, left). With trial-to-trial correlations removed, the decoding model performed about 35% worse than the model trained on correlated single-trial responses (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, right). The decrease in performance upon trial shuffling was present across stimuli, indicating that this is a general feature of stimulus encoding for this population, and not specific for selected visual features (<xref ref-type="fig" rid="fig4">Figure 4H</xref>). This result highlights the importance of performing simultaneous measurements to characterize population responses: using independent measurements and assuming uncorrelated response variability in this case would suggest a significantly worse single-trial decoding ability than is present in the full population. Taken together, these results show that, rather than a single visual feature being encoded by one or a few VPNs, all visual features are likely represented jointly across the population. Moreover, positive correlations in response variance enhance stimulus decoding relative to uncorrelated variability.</p></sec><sec id="s2-4"><title>Walking behavior selectively suppresses responses of small-object detecting glomeruli</title><p>Because sensory neural activity has been shown to be modulated by behavior in flies (<xref ref-type="bibr" rid="bib20">Chiappe et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib85">Strother et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Kim et al., 2017</xref>) and other animals (<xref ref-type="bibr" rid="bib53">Maimon, 2011</xref>; <xref ref-type="bibr" rid="bib61">Niell and Stryker, 2010</xref>), we wondered whether the trial-to-trial gain changes shown above were related to the behavioral state of the animal. To test this, we measured glomerulus population responses while the animal walked on an air-suspended ball (<xref ref-type="fig" rid="fig5">Figure 5AâB</xref>, see Materials and methods). Under this fictive walking paradigm, forward and rotational velocity components of movement were highly correlated. Because of this, we cannot disambiguate between contributions from forward and rotational velocity components in isolation. In sum, the fictive walking data show intermittent bouts of walking activity, and these movement bouts consisted of both forward and rotational velocity components. Because of this, we used total ball rotation as a measure of locomotor activity. To simplify the gain characterization, we showed a repeated probe stimulus on every trial, for 100 trials. First, we showed a 15Â° dark moving spot, since this stimulus drives strong responses in many glomeruli, including LC11, LC21, LC18, LC6, LC26, LC17, LC12, and LC15. We will refer to these glomeruli as âsmall object detecting glomeruliâ, recognizing that they also respond to other stimuli (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Examining the single-trial responses to the probe alongside fictive walking behavior revealed a striking relationship: probe stimuli that appeared when the fly was walking drove much weaker responses in some glomeruli than stimuli that appeared while the fly was stationary (<xref ref-type="fig" rid="fig5">Figure 5C and D</xref>). On average, responses of the LC11, LC21, L18, LC12, and LC15 glomeruli showed significant negative correlation with behavior. Conversely, responses of the LC6, LC26, and LC17 glomeruli did not show significant negative correlation with behavior. We next examined a measure of the population response gain of these five modulated glomeruli as a function of walking amplitude, across all trials and all flies (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). This analysis revealed that the weakest walking amplitudes were not associated with gain changes, while walking amplitudes that exceeded â¼10Â°/s showed lower-than-average response gain.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Walking behavior suppresses glomerulus responses to small object stimuli.</title><p>(<bold>A</bold>) Schematic showing fly on air-suspended ball for tracking behavior. (<bold>B</bold>) We used the change in ball rotation to measure overall walking behavior. Red line indicates threshold for binary classification of behaving versus nonbehaving, and gray shading indicates trials that were classified as behaving. (<bold>C</bold>) We presented a 15Â° moving spot repeatedly to probe glomerulus gain throughout the experiment. Example responses and quantification are shown only for glomeruli which respond reliably to this probe stimulus. Red triangles show stimulus presentation times. Black traces (top) show walking amplitude, and gray shading indicates trials classified as behaving. Scale bar is 4 sec and 25% dF/F. (<bold>D</bold>) Correlation (Spearmanâs rho) between behavior and response amplitude. Each small point is a fly, large point is the across-fly mean (n=8 flies), and asterisks indicate glomeruli with a significant negative correlation between response gain and behavior (One sample t-test, p&lt;0.05 after correction for multiple comparisons). (<bold>EâF</bold>) Same as C-D for a looming probe stimulus, with responses from the subset of glomeruli that respond strongly to looming stimuli. On average, there is no correlation between behavior and loom response strength (n=8 flies).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Population response gain as a function of walking amplitude.</title><p>Population response gain was defined as the z-scored trial response amplitude, averaged across all glomeruli that showed a significant negative correlation with behavior in <xref ref-type="fig" rid="fig5">Figure 5</xref>. We binned this population gain measure by peak walking amplitude on that trial (into equally-populated bins), to examine the relationship between peak walking amplitude and population response gain. Horizontal line at zero indicates the average response gain across all trials (n=8 flies).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig5-figsupp1-v3.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Genetically targeted VPN imaging shows behavioral modulation of response gain.</title><p>(<bold>A</bold>) Image of LC11 dendrites in Lobula, with three dendritic stalk ROIs highlighted. (<bold>B</bold>) Example walking amplitude (top, black) and LC11 response (bottom, blue) to repeated presentations of a small moving spot, as in <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>C</bold>) Spearman correlation between response gain and walking amplitude (n=4 flies). Asterisk indicates p&lt;0.05 (one sample t-test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig5-figsupp2-v3.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 3.</label><caption><title>The shared glomerulus gain factor is negatively correlated with behavior.</title><p>(<bold>A</bold>) Eigenvector decomposition of the covariance matrix reveals a dominant principal component which accounts for a large plurality of the trial-to-trial variance across the population. Points show mean +/- S.E.M. across flies (n=6 flies). (<bold>B</bold>) The first principal component of the response gain, averaged across flies. (<bold>C</bold>) For the variance analysis experiments in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we tracked behavior for a subset of the animals tested. For each fly, we z-scored the behavior amplitude across trials as well as the shared gain factor for each trial (equal to the projection of the first PC against the population response matrix). Each plot shows the relationship between the trial-by-trial shared gain factor and the average behavior amplitude for each trial. For every fly, there was a negative correlation between these two measures (mean  <inline-formula><mml:math id="inf1"><mml:mi>Ï</mml:mi></mml:math></inline-formula> = â0.23, n=6 flies).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig5-figsupp3-v3.tif"/></fig></fig-group><p>Because glomerulus responses in these experiments are dominated by the VPN that provides most of their input (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we expected that this gain modulation was due to changes in VPN responses. To test this more directly, we repeated this experiment using a specific split-Gal4 driver line for LC11 VPNs. We observed a similar negative correlation between response gain and walking amplitude using this genetically targeted approach (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>).</p><p>We next tested whether a similar behavioral modulation exists for those glomeruli which respond more strongly to loom, namely LC6, LC26, LC16, LPLC2, LC4, LPLC1, LC9, LC17, and LC12, using a dark looming spot as a probe (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). Across animals, we saw no significant modulation of loom responses by walking (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). Thus, walking behavior selectively suppressed the visually evoked responses of specific optic glomeruli, with the strongest effects on a subset of small object detecting glomeruli, while having no significant effect on glomeruli that respond most strongly to loom.</p><p>The gain changes associated with walking strongly resemble the correlated gain changes we saw in earlier experiments with the broader stimulus suite (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This suggests that the trial-to-trial shared gain was associated with the behavioral state of the animal. To test this idea, we examined the subset of flies from the experiments in <xref ref-type="fig" rid="fig4">Figure 4</xref> where we also collected walking behavior. We found that for each fly, the first principal component of the population response, corresponding to the large shared gain factor, was negatively correlated with walking (<xref ref-type="fig" rid="fig5s3">Figure 5âfigure supplement 3</xref>), with an average rank correlation coefficient of  <inline-formula><mml:math id="inf2"><mml:mi>Ï</mml:mi></mml:math></inline-formula>=â0.23. Thus, the shared gain modulation is associated with walking, but importantly, this relationship is incomplete. This means that one could not infer the population correlation structure seen in <xref ref-type="fig" rid="fig4">Figure 4</xref> by leveraging information about walking behavior.</p></sec><sec id="s2-5"><title>Visual inputs associated with self-generated rotation modulate glomerulus sensitivity</title><p>Self-generated motion is associated with characteristic visual cues, including wide-field, coherent visual motion on the retina. In the next series of experiments, we set out to test the hypothesis that optic glomerulus gain might be modulated by these visual signatures of self-generated motion. To test whether glomeruli respond to visual cues characteristic of walking, we first created a complex visual stimulus designed to include several features thought to be components of natural visual inputs to walking flies, including objects at different depths (vertically oriented, dark bars), as well as images dominated by low spatial frequencies (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). To move this scene, we measured fly walking trajectories using a 1 m<sup>2</sup> arena with automated tracking, as described previously (<xref ref-type="bibr" rid="bib102">York et al., 2022</xref>), and applied short segments of these walking trajectories to the camera location and heading in our visual environment, creating an open loop âplay-backâ stimulus (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). These VR stimuli drove very weak responses across all glomeruli, including the small object detecting glomeruli (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), despite these glomeruli in the same flies responding very robustly to isolated vertical bars similar to those in the scene (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). The relatively weak responses of most glomeruli to these play-back stimuli suggested that some features characteristic of visual inputs during walking suppress glomerulus responses via the visual surround of each VPN. To test this idea, we focused on one prominent component of visual inputs during self-motion, namely the coherent visual rotation associated with body turns. We note that although we did not address it here, other components of self-motion, including forward translation, may also play an important role in shaping visual responses. To explore the spatial and temporal frequency tuning of this visual surround, we presented a 15Â° dark spot, a probe stimulus that many glomeruli respond to (<xref ref-type="fig" rid="fig3">Figure 3</xref>), while drifting a sine wave grating in the background with variable spatial period and speed (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). The LC11 glomerulus, which responds strongly to small moving objects on uniform backgrounds, showed strongly suppressed probe responses to gratings with low spatial frequencies, and across speeds chosen to span the typical range of angular speeds experienced during fly locomotor turning (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). This suppression by low spatial frequency gratings across a range of rotational speeds was seen for all small object detecting glomeruli (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). Thus, these glomerulus responses are subject to a suppressive surround that is sensitive to low spatial frequencies and to a broad range of retinal speeds.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Visual input associated with rotational motion suppresses optic glomerulus responses.</title><p>(<bold>A</bold>) Still image from a VR stimulus. (<bold>B</bold>) Twenty second snippet of a fly walking trajectory. Green and red points indicate start and end of the trajectory, respectively. Arrows show the flyâs heading. (<bold>C</bold>) For an example fly, responses of small object detecting glomeruli to an example virtual reality trajectory. These glomeruli respond very weakly to the virtual reality stimulus, despite showing strong, reliable responses to solitary vertical bars (right) similar to those present in the virtual reality scene. Scale bar is 5 s and 25% dF/F. (<bold>D</bold>) Histograms showing, for each glomerulus, the distribution of peak responses to each VR trajectory. Vertical line indicates mean response to a single dark, vertical bar stimulus (five VR trajectories were presented to each fly, n=10 flies). (<bold>E</bold>) Schematic showing the surround suppression tuning stimulus. A small dark probe stimulus moves through the center of the screen while a grating with varying spatial frequency and speed moves in the background. (<bold>F</bold>) For the LC11 glomerulus, probe responses are suppressed by low spatial frequency gratings across a range of speeds consistent with locomotor turns. Small, color-coded numbers indicate the temporal frequency associated with each grating speed and spatial period. Scale bar is 2 s and 25% dF/F. (<bold>G</bold>) Heatmaps showing probe responses as a joint function of background spatial period and speed for each of these 8 glomeruli (n=10 flies). (<bold>H</bold>) Schematic of random dot coherence stimulus. At zero coherence (top image), each dot moves independently of all the other dots. At a coherence of 1.0 (bottom image), all dots move in the same direction. (<bold>I</bold>) For an example fly, responses of the small object detecting glomeruli are shown to coherence values of 0 (top row) and 1.0 (bottom row). Scale bar is 2 s and 25% dF/F. (<bold>J</bold>) Summary data showing response amplitude (normalized to the 0 coherence condition within each fly) of each glomerulus to varying degrees of motion coherence. Asterisk at the top indicates a significant difference between the response to 0 and 1 coherence (n=11 flies, paired t-test, p&lt;0.05 after correction for multiple comparisons).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig6-v3.tif"/></fig><p>While the previous experiments show that the surround is responsive to low spatial frequency drifting gratings, they do not test whether the surround is selective for rotational motion. We next designed a stimulus to test this idea. A prominent feature of self-generated visual motion, especially rotational turns, is widefield motion coherence. That is, when an animal turns, all local motion signals across the visual field are aligned along an axis defined by the axis of rotation. From a visual circuit perspective, coherent rotational motion concentrates activity of elementary motion detecting neurons T4/T5 within a single layer of the lobula plate, where as incoherent local motion would spread T4/T5 activity across all layers. To test whether motion coherence impacted surround suppression of optic glomeruli, we designed a stimulus inspired by random dot kinematograms (<xref ref-type="bibr" rid="bib18">Britten et al., 1992</xref>). This stimulus was composed of a field of small dots, roughly 15Â° in size, that moved around the fly at constant speed. Individual spots of this size drive robust responses in most small object detecting glomeruli, and should also recruit elementary motion detectors T4/T5.</p><p>This moving dot field had a tunable degree of coherence, such that at a coherence level of 0, each dot moved at the defined speed, but in a random direction. At a coherence level of 1, every dot moved in the same direction (<xref ref-type="fig" rid="fig6">Figure 6H</xref>). Intermediate coherence values correspond to the fraction of dots moving along the pre-defined âsignalâ direction. Importantly, this stimulus has the same overall mean intensity, contrast and motion energy for every coherence level. As expected, at 0 coherence, small object detecting glomeruli responded strongly. However, as the motion coherence was increased, responses of many small object detecting glomeruli decreased (<xref ref-type="fig" rid="fig6">Figure 6H, I</xref>). Taken together, these results are strong evidence that the suppressive surround of these glomeruli is sensitive to widefield motion cues that are characteristic of self-motion.</p></sec><sec id="s2-6"><title>Natural images recruit surround suppression</title><p>To test whether rotational self-motion cues derived from natural scenes can drive surround suppression in small object detecting glomeruli, we used a moving 15Â°spot to probe response gain while presenting natural images in the background (<xref ref-type="bibr" rid="bib95">van Hateren and van der Schaaf, 1998</xref>; <xref ref-type="fig" rid="fig7">Figure 7A</xref>). When presented on top of a stationary image, the probe stimulus elicited a large response in LC11. However, when the probe was presented on top of a rotating natural image, LC11 glomerulus responses were strongly suppressed for rotational speeds spanning the range of locomotor turns (<xref ref-type="fig" rid="fig7">Figure 7B</xref>), compared to a stationary image background. In agreement with <xref ref-type="fig" rid="fig6">Figure 6HâJ</xref>, this suggests that rotational motion recruits surround suppression. We next explored surround speed tuning across all eight small object detecting glomeruli (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). The LC11, LC21, and LC18 glomeruli showed strong suppression at all non-zero image speeds tested (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, left). The LC6 and LC26 glomeruli showed a shallower dependence of surround suppression on image speed (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, center), while the LC17, LC12, and LC15 glomeruli showed intermediate speed dependence (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, right). In summary, natural images suppress small object responses in these glomeruli and surround suppression generally increases with increasing background speed.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Visual suppression is tuned to natural image statistics.</title><p>(<bold>A</bold>) Stimulus schematic: a small spot probe is swept across the visual field while a grayscale natural image moves in the background at a variable speed. (<bold>B</bold>) For the LC11 glomerulus, natural image movement strongly suppresses the probe response across a range of speeds (average across three images, n=8 flies). (<bold>C</bold>) Mean probe response as a function of background image speed, normalized by probe response with static background. Surround speed tuning curves are grouped by functional glomerulus groupings for the small object detecting glomeruli. (<bold>D</bold>) Average power spectra (left) and example image (right) for the original natural images (top), whitened images (second row), high-pass filtered images (third row), and low-pass filtered images (bottom row). Gray line shows p <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>E</bold>) For LC11, image suppression of probe responses is attenuated by whitening the image or high-pass filtering it, but not by low-pass filtering the image. (<bold>F</bold>) Dependence of probe suppression on image speed and filtering for each of the eight small object detecting glomeruli (n=9 flies). Scale bar is 2 s and 25% dF/F.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig7-v3.tif"/></fig><p>We hypothesized that the low spatial frequency content of natural images was critical for these effects, since the grating results (<xref ref-type="fig" rid="fig6">Figure 6</xref>) showed the strongest suppression for low spatial frequency gratings, and because natural images are characterized by long-range intensity correlations and low spatial frequencies (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). To test the effect of spatial frequency content of images on surround suppression, we repeated this experiment with filtered versions of the natural images. For each of three natural images, we presented the original (unfiltered) image, a whitened natural image, which has a roughly flat power spectrum at low spatial frequencies, a high-pass filtered image, and a low-pass filtered image (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). For LC11, and all other small object detecting glomeruli, the natural image and its low-pass filtered version strongly suppressed responses to the probe, whereas the whitened and high-pass filtered images recruited much weaker suppression (<xref ref-type="fig" rid="fig7">Figure 7E and F</xref>). We note that this spatial and temporal frequency tuning of these suppressive surrounds is broadly consistent with the tuning properties of elementary motion detecting neurons T4 and T5 (<xref ref-type="bibr" rid="bib49">Leong et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">Maisak et al., 2013</xref>). These observations raise the possibility that local motion detectors provide critical input to the visual surrounds of small object detecting glomeruli. Moreover, the observation that surround speed tuning was similar for glomeruli that clustered together using the synthetic stimulus suite (<xref ref-type="fig" rid="fig3">Figure 3</xref>) suggests that this functional clustering may reflect properties of the surround. More broadly, the differential speed sensitivities of these surrounds may further diversify feature selectivity across these groups of glomeruli in the context of natural visual inputs.</p></sec><sec id="s2-7"><title>Behaviorally and visually driven suppression independently modulate small object detectors</title><p>The results presented thus far show that the gain of small object detecting glomeruli was tuned by both locomotor behavior and widefield visual motion. Both of these cues are associated with self-generated movements of the animal. How can the fly reliably track external objects during self-motion if small-object detecting glomeruli are suppressed by visual and behavioral cues? We hypothesized that the answer might lie in the temporal dynamics of locomotor behavior. Natural fly walking behavior is saccadic, interspersing fast turns with periods of relatively straight walking bouts (<xref ref-type="bibr" rid="bib26">Cruz et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Geurten et al., 2014</xref>; <xref ref-type="bibr" rid="bib38">Juusola et al., 2017</xref>; <xref ref-type="bibr" rid="bib75">Reynolds and Frye, 2007</xref>). We hypothesized that the saccadic structure of walking ensures that glomerulus gain is suppressed only transiently during a saccade, and once the saccade is over, visual response gain is restored to sample external objects.</p><p>To test this idea, we first examined the temporal dynamics of locomotor turns under conditions where animal movement is unconstrained. To do this, we examined walking trajectories from our open behavioral arena (see <xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="bibr" rid="bib102">York et al., 2022</xref>; <xref ref-type="fig" rid="fig8">Figure 8</xref>). Walking trajectories in an open arena showed a wide range of angular speeds and forward velocities (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>). Examining the angular velocity of a single walking trajectory revealed saccadic temporal dynamics (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). Across all flies, the time between saccades (the âinter-turn intervalâ) showed a wide range, but there were few inter-turn intervals less than â¼0.5 s, a peak near 1 s, and a long tail (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). We chose a threshold angular speed to classify saccadic turns, here 160Â°/s, but inter-turn interval distributions were similar across a range of threshold values that include the vast majority of turns. For all saccade thresholds, there was a low probability of a saccade within â¼0.5 s of the previous saccade. We next split up snippets of walking velocity trajectories based on whether they occurred within a 400 ms window around a saccade or during an intersaccade interval. As expected, angular speeds experienced during a saccade were large, and during intersaccade intervals, most angular speeds were very low (<xref ref-type="fig" rid="fig8">Figure 8E</xref>, top). Interestingly, distributions of forward velocities were not different during a saccade compared to the intersaccade interval (<xref ref-type="fig" rid="fig8">Figure 8E</xref>, bottom), meaning that saccades tend to occur while the fly is moving forward as well. Taken together, this means that a typical locomotor saccade is followed by at least a 500 ms, and often a â¼1-s period of relative heading stability.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Natural fly walking is punctuated by saccadic turns.</title><p>(<bold>A</bold>) Example unconstrained fly walking trajectory measured in an open behavioral arena. Arrows show the flyâs heading. Green point indicates start of walking trajectory. Scale bar=1 cm. (<bold>B</bold>) Heatmap of instantaneous forward velocity and rotational speed across all time points from 81 fly trajectories. Note logarithmic color scale. (<bold>C</bold>) Example trace of instantaneous angular velocity (top) and instantaneous forward velocity (bottom). Red arrowheads on top indicate saccades. (<bold>D</bold>) Histogram of inter-turn intervals shows a peak around 1 s and a long tail. (<bold>E</bold>) Histogram of angular speed (top) and forward velocity (bottom) within a 400 ms window around a saccade (red) or during the intersaccade interval (gray).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig8-v3.tif"/></fig><p>We next asked whether saccadic visual inputs recruit surround suppression, and whether the timescale of this suppression could support such a visual sampling strategy. We designed a stimulus meant to mimic the retinal input during a locomotor saccade. As before, we presented a probe stimulus on every trial to measure the response gain of small object detecting glomeruli. In the background was a grayscale natural image (<xref ref-type="fig" rid="fig9">Figure 9A</xref>), which underwent a lateral rotation of 70Â° in 200 ms at a variable time relative to the glomerular response to the probe (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). As a result, the saccade signal could precede, co-occur with, or lag the glomerulus response to the probe. When the saccade occurred within â¼500 ms of the probe response, the probe response was attenuated, suggesting that this saccade stimulus recruits the motion-sensitive suppressive surround. Across many small object detecting glomeruli, including LC11, LC21, LC17, LC12, and LC15, we saw strong gain suppression when the saccade occurred around the time of the probe response (<xref ref-type="fig" rid="fig9">Figure 9C and D</xref>). Interestingly, the other glomeruli, LC18, LC6, and LC26 showed much weaker and more variable saccade suppression, maintaining their response gain regardless of saccade timing. Note that because we presented only one saccade on each probe trial, and because we are quantifying gain using the response amplitude, the timing dependence measured here is independent of calcium indicator dynamics and therefore reflects dynamics associated with the glomerulus response. The timescale of this surround suppression, combined with the temporal dynamics of fly turning (<xref ref-type="fig" rid="fig8">Figure 8</xref>) suggests that visually driven saccade suppression transiently reduces glomerulus response gain around the time of a locomotor saccade, but gain recovers while the flyâs heading is stable and before the next saccade occurs. We infer that this dynamic gain adjustment allows the fly to sample the scene during the inter-saccadic periods of heading stability.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Visually driven saccade suppression works in concert with behavioral suppression.</title><p>(<bold>A</bold>) Stimulus schematic: A small moving probe is used to measure glomerulus response gain while a full-field natural image in the background undergoes a fast, ballistic lateral displacement meant to mimic fly walking saccades. (<bold>B</bold>) The time of the saccade is varied throughout the trial, such that it occurs at different phases of the probe response. Each saccade lasted 200 ms and translated the image by 70Â°. (<bold>C</bold>) Trial-average probe responses in an example LC11 glomerulus (top) and an example LC6 glomerulus (bottom). Saccade times are indicated by the yellow vertical line in each panel. As the saccade approaches the probe in time, the probe response is suppressed in LC11 but not LC6. (<bold>D</bold>) Summary data showing, for each small object detecting glomerulus, the response gain as a function of saccade time relative to the response onset. t=0 corresponds to coincident probe response and saccade onset. Suppression is strongest when the saccade occurs near probe response onset (n=5 flies). (<bold>E</bold>) We monitored fly walking behavior while presenting the saccadic visual stimuli above. (<bold>F</bold>) For an example LC11 glomerulus, both the saccadic stimulus and walking behavior reduced probe responses, and these gain control mechanisms could be recruited separately. (<bold>G</bold>) Population data showing average response gain for visual saccades versus response gain during walking behavior, for each small object detecting glomerulus. Most glomeruli lie below unity (dashed line), indicating that visual saccade suppression is typically stronger than behavior-linked suppression. These two sources of suppression are also correlated across glomeruli (r=0.80, Pearson correlation coefficient). (<bold>H</bold>) For each small object detecting glomerulus, we compared the response gain when both visual-related and motor-related gain modulation were recruited (vertical axis) to the product of both response gains in isolation (horizontal axis). Dashed line is unity, and points falling along that line indicate a linear interaction between those forms of gain control.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-fig9-v3.tif"/></fig><p>Because the brief saccade stimulus did not completely suppress probe responses, we could examine the relationship between visual-related and motor-related gain control mechanisms. One way to interpret these data is that motor signals suppress small object detecting glomeruli, and that widefield, coherent visual motion induces a turning response, recruiting the same motor-command derived suppression. Is the apparent visual suppression a result of the motor feedback, or are the visual-based and motor-based suppression mechanisms independent? To test this idea, we monitored walking behavior while presenting saccadic visual stimuli (<xref ref-type="fig" rid="fig9">Figure 9E</xref>). We first examined the LC11 glomerulus response to the probe under both behavioral conditions (walking versus stationary), and under both visual conditions (saccade coincident with the probe response, vs. no saccade coincident with the probe response). Strikingly, when the fly was stationary, visual saccades still reduced the gain of the response (<xref ref-type="fig" rid="fig9">Figure 9F</xref>). Interestingly, the glomeruli that were subject to stronger gain reductions by the visual saccade also showed stronger gain reductions by walking (<xref ref-type="fig" rid="fig9">Figure 9G</xref>, r=0.80). Finally, to test whether these two gain modulation mechanisms were independent, we compared the measured probe responses when the fly was receiving both saccadic visual input and was walking to the product of each gain change measured independently (i.e., when the fly was receiving either saccadic visual input or was walking, but not both) (<xref ref-type="fig" rid="fig9">Figure 9H</xref>). Across the small object detecting glomeruli, the prediction that these two gain mechanisms were independent accurately captured the jointly measured gain modulation. Taken together, these data indicate that visual suppression is not the indirect effect of an induced turning response and that saccadic visual modulation and behavior-related modulation are both balanced in magnitude and independent.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we show that local feature detection is challenged by rotational self-motion signals in rich visual environments (<xref ref-type="fig" rid="fig1">Figure 1</xref>). To determine how feature detecting neurons might maintain selectivity under natural viewing conditions, we first developed a new connectome-based method to segment functional imaging signals that allowed us to measure neural responses across a heterogeneous population of VPNs (<xref ref-type="fig" rid="fig2">Figures 2</xref>â<xref ref-type="fig" rid="fig3">3</xref>). Using this method, we found that information about different visual features is distributed across multiple VPN types, meaning that stimulus identity cannot be decoded from a single glomerulus alone (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Further, we found that strong trial-to-trial response correlations improve stimulus encoding fidelity (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Strikingly, the locomotor behavior of the fly selectively modulated responses of small object detecting, but not loom detecting, glomeruli (<xref ref-type="fig" rid="fig5">Figure 5</xref>). We then showed that visual motion signals characteristic of walking also modulated the responses of glomeruli tuned to small objects (<xref ref-type="fig" rid="fig6">Figures 6</xref>â<xref ref-type="fig" rid="fig7">7</xref>). Finally, we demonstrated that visual suppression occurs during naturalistic body saccades made by walking flies, and that behavioral and visual gain modulation are both balanced in magnitude and independent, such that these two cues combine linearly (<xref ref-type="fig" rid="fig8">Figures 8</xref>â<xref ref-type="fig" rid="fig9">9</xref>). Taken together, these two forms of gain control reduce the sensitivity of small object detectors to inputs that can diminish the discriminability of local features, thereby allowing for reliable feature detection during saccadic vision.</p><sec id="s3-1"><title>Population coding of local visual features</title><p>Our characterization of the optic glomeruli using solitary visual features (<xref ref-type="fig" rid="fig3">Figure 3</xref>) largely agrees with what has been described previously, in that many glomeruli respond strongly to small moving objects, others respond to visual loom, and responses to stationary flicker or widefield motion are weak or nonexistent (<xref ref-type="bibr" rid="bib37">Hindmarsh Sten et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">KeleÅ and Frye, 2017</xref>; <xref ref-type="bibr" rid="bib40">KeleÅ et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>; <xref ref-type="bibr" rid="bib82">StÃ¤dele et al., 2020</xref>). These data have been used as evidence that particular VPNs are linked to specific visual features and corresponding visually guided behaviors (<xref ref-type="bibr" rid="bib37">Hindmarsh Sten et al., 2021</xref>; <xref ref-type="bibr" rid="bib76">Ribeiro et al., 2018</xref>). At the same time, the responses of individual VPN classes overlap, in the sense that an individual visual stimulus will evoke responses from many VPN classes, suggesting a dense population code. We note, however, that how downstream circuits make use of the information available across VPNs to guide behavior is not well understood, and further work incorporating connectomics, targeted perturbations of VPN channels, and behavioral analyses might shed light on this question. For example, a recent study used genetic silencing, coupled with a goal-oriented neural network model, to show that VPNs jointly encode behaviorally relevant visual features during <italic>Drosophila</italic> courtship (<xref ref-type="bibr" rid="bib25">Cowley et al., 2022</xref>).</p><p>To what extent is it possible to decode stimulus identity based on the activity of a single VPN class? Our results show that population measurements are important to describe feature encoding by VPNs for two reasons. First, evaluation of stimulus decoding revealed that most visual features are encoded jointly across the population, not by single VPN types. This is because information about stimulus identity is contained not only in the responses of glomeruli that are strongly tuned to a particular feature but also in the weaker responses of glomeruli that have different tuning properties. Second, while responses in each VPN type showed high trial-to-trial variability, simultaneous measurements revealed that this variability was strongly correlated across the population, and that this improved coding fidelity across the population relative to uncorrelated variability. This is consistent with past experimental and theoretical work showing that positive correlations can help offset the deleterious effect of response variability by shaping the noise in directions in population response space that do not interfere with stimulus decoding (<xref ref-type="bibr" rid="bib31">Franke et al., 2016</xref>; <xref ref-type="bibr" rid="bib105">Zylberberg et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Pruszynski and Zylberberg, 2019</xref>; <xref ref-type="bibr" rid="bib9">Averbeck and Lee, 2006</xref>; <xref ref-type="bibr" rid="bib60">Moreno-Bote et al., 2014</xref>). A similar structure of neural variability relative to population tuning permits accurate stimulus decoding in the face of large movement-related signals in mouse cortex (<xref ref-type="bibr" rid="bib79">Rumyantsev et al., 2020</xref>; <xref ref-type="bibr" rid="bib84">Stringer et al., 2021</xref>). This effect relies on heterogeneous tuning across the population of neurons, and downstream decoders can extract information about the stimulus by comparing activation across differently tuned neurons. Indeed, in the case of a population of identically tuned neurons, positive noise correlations degrade rather than improve coding fidelity (<xref ref-type="bibr" rid="bib103">Zohary et al., 1994</xref>; <xref ref-type="bibr" rid="bib8">Averbeck et al., 2006</xref>).</p><p>The trial-to-trial variability we observed among VPNs was dominated by a single, shared population response gain that was associated with walking behavior, but only weakly (<xref ref-type="fig" rid="fig5s3">Figure 5âfigure supplement 3</xref>). Thus, this shared gain is likely modulated by other factors, for example, shared upstream noise (<xref ref-type="bibr" rid="bib3">Ala-Laurila et al., 2011</xref>; <xref ref-type="bibr" rid="bib105">Zylberberg et al., 2016</xref>) or other behavioral or physiological states that we did not measure. How downstream circuits combine signals across glomeruli may provide insight into how the brain decodes VPN population responses to encode local features, and available connectomic data sets can accelerate progress on this question (<xref ref-type="bibr" rid="bib46">Klapoetke et al., 2022</xref>; <xref ref-type="bibr" rid="bib81">Scheffer et al., 2020</xref>).</p></sec><sec id="s3-2"><title>Natural locomotor behavior modulates the sensitivity of small object detectors</title><p>Behavior-associated gain changes are widespread in visual systems across phyla (<xref ref-type="bibr" rid="bib53">Maimon, 2011</xref>; <xref ref-type="bibr" rid="bib52">Maimon et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib58">McBride et al., 2019</xref>; <xref ref-type="bibr" rid="bib61">Niell and Stryker, 2010</xref>). Recent work demonstrates that locomotor signals are prevalent throughout the <italic>Drosophila</italic> brain, including in the visual system (<xref ref-type="bibr" rid="bib2">Aimon et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Brezovec et al., 2022</xref>; <xref ref-type="bibr" rid="bib80">Schaffer et al., 2021</xref>), but has been examined most extensively in circuits involved in elementary motion detection and widefield motion encoding. Behavioral activity has been shown to modulate response gain in widefield motion detecting lobula plate tangential cells (LPTCs) and some of their upstream circuitry (<xref ref-type="bibr" rid="bib20">Chiappe et al., 2010</xref>; <xref ref-type="bibr" rid="bib47">Kohn et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Maimon et al., 2010</xref>; <xref ref-type="bibr" rid="bib85">Strother et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">Suver et al., 2012</xref>), and LPTC membrane potential tightly tracks walking behavior, even in the absence of visual stimulation (<xref ref-type="bibr" rid="bib33">Fujiwara et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Fujiwara et al., 2022</xref>). During flight, efference-copy based modulation of LPTC membrane potential has been proposed to cancel expected visual motion due to self-generated turns (<xref ref-type="bibr" rid="bib29">Fenk et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Kim et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Kim et al., 2017</xref>). In each of these cases, behavioral signals adjust response gain according to expected visual inputs, for example faster rotational speeds during flight. <xref ref-type="bibr" rid="bib42">Kim et al., 2015</xref> showed that flying saccades were associated with hyperpolarization of optic glomeruli interneurons, which the authors speculated could be useful for canceling spurious small object detector responses during self-motion, much like what we see in VPNs during walking behavior. The behavioral gain modulation we describe here selectively adjusts visual sensitivities to reflect the fact that specific visual inputs are particularly corrupted by self-motion. Small object detection is an especially challenging task during self-motion (<xref ref-type="fig" rid="fig1">Figure 1</xref>), and consequently, gain modulation most strongly affects glomeruli involved in this task. Glomeruli that are tuned more strongly to looming visual objects were not modulated by walking behavior, suggesting that these larger visual features can be reliably extracted under walking conditions. During a high-velocity locomotor saccade, sensitivity to small objects is transiently decreased, and in the subsequent inter-saccade interval, small object detector gain is restored, allowing for selective encoding of visual features at different points in the locomotor cycle.</p><p>Interestingly, our estimates of visual gain suppression (<xref ref-type="fig" rid="fig7">Figure 7</xref>) combined with the statistics of free walking (<xref ref-type="fig" rid="fig8">Figure 8</xref>), suggest that some small object detecting glomeruli may experience visual suppression due to the smaller rotational motion present during intersaccadic periods, as well. For example, LC11, LC21, and LC18 showed strong visual suppression even for image rotations of only 40Â°/s, which can be achieved in the time between saccades. This suggests that these cells operate best as small feature detectors during periods of high heading stability, which may be achieved during some periods of straight, forward walking or while the fly is stationary. We have chosen to focus on visual inputs during rotation because these movements cause rapid, uniform shifts in visual inputs, but we note that the more complex widefield motion inputs that are associated with forward translation, which produces nonuniform flow fields across the retina, likely also impact local visual feature encoding.</p><p>Importantly, our experiments measuring fictive walking on an air-suspended ball (<xref ref-type="fig" rid="fig5">Figure 5</xref>) are not able to decouple forward from rotational components of velocity, because these velocity components are highly correlated. During free walking, saccades nearly always co-occur with forward movement, making disambiguation of these walking components difficult even under natural conditions. However, free-walking flies do perform forward walking bouts without much rotational velocity component (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Further work to recapitulate natural walking statistics under conditions of head fixation would help elucidate the relative contributions of specific locomotor components to motor-related visual gain control.</p><p>Is the behavioral modulation of small object detecting glomeruli related to the well studied modulation of widefield motion detecting circuits? A parsimonious explanation of both of these observations is that neurons in the elementary and widefield motion pathways feed into the suppressive surround of small object detecting glomeruli, as is the case for figure detecting neurons in blowfly (<xref ref-type="bibr" rid="bib28">Egelhaaf, 1985</xref>; <xref ref-type="bibr" rid="bib97">Warzecha et al., 1993</xref>). This would endow optic glomerulus surrounds with both the widefield, coherent motion sensitivity as well as the behavioral modulation that we see. In support of this proposed mechanism, the glomeruli that show strong visual suppression are also subject to strong behavioral suppression (<xref ref-type="fig" rid="fig9">Figure 9</xref>). This hypothesis further predicts that glomeruli which derive their excitatory center inputs from elementary motion detectors (e.g., the loom-selective LPLC2; <xref ref-type="bibr" rid="bib45">Klapoetke et al., 2017</xref>) might be positively gain modulated under other behavioral conditions, such as flight. Taken together, these results demonstrate that understanding local feature detection during natural vision requires accounting for the structure of locomotion. More broadly, we have shown that walking behavior modulates a subset of glomeruli, raising the possibility that different behavioral states might selectively alter other glomeruli subsets, reshaping population coding of visual features to subserve different goals.</p></sec><sec id="s3-3"><title>Motor signals and visual cues provide independent inputs to feature detectors</title><p>In addition to the motor-related gain modulation, small object detecting glomeruli are modulated by a visual surround that is tuned to widefield, coherent visual motion that would normally be associated with locomotion. This is similar to motion-tuned surrounds in object motion-sensitive cells in the vertebrate retina (<xref ref-type="bibr" rid="bib10">Baccus et al., 2008</xref>; <xref ref-type="bibr" rid="bib66">Olveczky et al., 2003</xref>), and in figure detecting neurons of the blowfly, which are suppressed by optic flow produced by self-motion (<xref ref-type="bibr" rid="bib28">Egelhaaf, 1985</xref>; <xref ref-type="bibr" rid="bib44">Kimmerle and Egelhaaf, 2000</xref>). Why would the fly visual system rely on these two seemingly redundant cues to estimate self-motion? One possibility is that either cue alone could be unreliable or ambiguous under some conditions. For example, a striking characteristic of natural scenes is their immense variability from scene to scene. As a result, detecting small moving objects could occur against a background of a dense, contrast-rich visual environment like a forest or a uniform, low-contrast background like a cloudy sky. These two scenes would be expected to be associated with very different wide-field motion signals, even given the same self-motion. Because of this, relying on visual cues alone for evidence of self-motion will be unreliable under the diversity of natural scenes. Thus, motor signals and visual cues characteristic of self-motion work together to provide a robust estimate of self-motion to feature detectors.</p><p>Our observation that small object detecting glomeruli are modulated by a visual surround tuned to widefield motion agrees with previous observations that flies use global motion as well as local figure information to support object tracking behavior during flight (<xref ref-type="bibr" rid="bib5">Aptekar et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Aptekar et al., 2015</xref>), where rotational velocities are much greater in magnitude than those associated with locomotor turns (<xref ref-type="bibr" rid="bib32">Fry et al., 2003</xref>). How strategies for reliable object tracking during walking relate to flying conditions is not clear, and more work is needed to understand how small object detectors can support object tracking under these drastically different visual conditions.</p></sec><sec id="s3-4"><title>Saccade suppression as a general visual strategy</title><p>Visual motion is a prominent feature of realistic retinal inputs for both flies and vertebrates. Primates make frequent eye movements at different spatial scales during free viewing which can rapidly translate the image on the retina (<xref ref-type="bibr" rid="bib78">Rucci and Victor, 2015</xref>; <xref ref-type="bibr" rid="bib94">Van Der Linde et al., 2009</xref>; <xref ref-type="bibr" rid="bib104">Zuber et al., 1965</xref>). Eye movements in primates are dominated by saccades, large movements that can shift the image on the retina by up to tens of degrees of visual angle. Walking flies perform locomotor saccades, which similarly rapidly shift the image impinging on the retina in a short time period (<xref ref-type="fig" rid="fig8">Figure 8</xref>; <xref ref-type="bibr" rid="bib26">Cruz et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Geurten et al., 2014</xref>). We found that the responses of some small object detecting glomeruli were suppressed around the time of a simulated visual saccade, while others (LC18, LC6, and LC26) showed no visual saccade suppression. Similarly, saccades in primates induce variable changes in response gain across different brain regions, a physiological effect thought to underlie the perceptual phenomenon of saccadic suppression (<xref ref-type="bibr" rid="bib12">Binda and Morrone, 2018</xref>; <xref ref-type="bibr" rid="bib16">Bremmer et al., 2009</xref>; <xref ref-type="bibr" rid="bib101">Wurtz, 2018</xref>; <xref ref-type="bibr" rid="bib89">Thiele et al., 2002</xref>). Our data show that in flies, a similar form of saccade-related suppression can be recruited selectively to circuit elements whose feature selectivity is most sensitive to the corrupting effect of self-motion on the visual input. More broadly, this work suggests that a saccade-and-sample visual strategy is shared between flies and primates.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data and code availability</title><p>Data collected for this study can be found on Dryad at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.h44j0zpp8">https://doi.org/10.5061/dryad.h44j0zpp8</ext-link>. All software and analysis code used for this study can be found on GitHub. Of particular note, the analysis code used to analyze these data and generate the figures presented here, can be found on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/mhturner/glom_pop">https://github.com/mhturner/glom_pop</ext-link>; <xref ref-type="bibr" rid="bib91">Turner, 2022</xref>.</p></sec><sec id="s4-2"><title>Fly lines and genetic constructs</title><p>We generated the 20xUAS-syt1GCaMP6f construct (Addgene plasmid #190896) by cloning the cDNA sequence of <italic>Drosophila</italic> synaptotagmin 1, a 3Ã GS linker, and the GCaMP6f sequence into the pJFRC7-20XUAS vector (<xref ref-type="bibr" rid="bib71">Pfeiffer et al., 2010</xref>) (Genscript Biotech). The GS linker connects the C-terminus of syt1 to the N-terminus of GCaMP6f (after <xref ref-type="bibr" rid="bib21">Cohn et al., 2015</xref>). Transgenic flies were generated by PhiC31-mediated integration of the construct to produce two landing site insertions (BestGene): P{20xUAS-syt1GCaMP6f}attP40, and PBac{20xUAS-syt1GCaMP6f}VK00005. Both insertions express well. P{20xUAS-syt1GCaMP6f}attP40 was used in the paper.</p><p>The genotype of flies used for pan-glomerulus imaging was the following:<disp-formula id="equ1"> <mml:math id="m1"><mml:mrow><mml:mfrac><mml:msup><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mfrac><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mn>6</mml:mn><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mfrac><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>::</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">A</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mn>4</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">A</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mn>4</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For Split-Gal4 imaging (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we used the following genotype:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mfrac><mml:msup><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mfrac><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mn>6</mml:mn><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mn>65.</mml:mn><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mfrac><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>::</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mn>4.</mml:mn><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where LCxx corresponds to a pair of LC subtype-specific hemidrivers from <xref ref-type="bibr" rid="bib100">Wu et al., 2016</xref>.</p></sec><sec id="s4-3"><title>Animal preparation and imaging</title><p>Female flies, 2â7 days post eclosion, were selected for imaging. Flies were cold anesthetized and mounted in a custom-cut hole in an aluminum shim at the bottom of an imaging chamber before being immobilized with UV curing glue. The front left leg was removed to prevent occluding the left eye, and the proboscis was immobilized using a small drop of UV curing glue. The cuticle covering the left half of the posterior head capsule was removed using a fine dissection needle, and fat bodies and trachea covering the brain were removed. The prep was continuously perfused with room temperature, carbogen-bubbled fly saline throughout the experiment. We imaged the left optic glomeruli in each fly.</p><p>For in vivo imaging, we used a two-photon resonant scanning microscope (Bruker) with a 20Ã 1.0 NA objective (Leica) and a fast piezo-driven Z drive to control the focal plane during volumetric imaging. Two-photon laser wavelength was 920 nm and post-objective power was â¼15 mW. We collected red and green channel fluorescence to image myr::tdTomato and syt1GCaMP6f, respectively. For functional scans, to record GCaMP responses, we collected volumes with voxel resolution 1Ã1Ã4 Âµm<sup>3</sup> (x, y, z) at a sampling frequency of 7.22 Hz. For high-resolution anatomical scans, voxels were 0.5Ã0.5Ã1 Âµm<sup>3</sup>. The imaging volume for glomerulus imaging was 177Ã101Ã45 Âµm<sup>3</sup>. Each fly was typically imaged for approximately 30â45 min. For Split-Gal4 imaging, we used the same imaging parameters that we did for the pan-glomerulus imaging experiments. Only animals with visible GCaMP6f responses in the lobula or in the optic glomeruli were included.</p></sec><sec id="s4-4"><title>Visual stimulation</title><p>We back-projected visual stimuli from two LightCrafter 4500 projectors onto a fabric screen covering the front visual field of the animal. The screens subtended approximately 60Â° in elevation and 140Â° in azimuth. We used the blue LED of the projectors and a 482/18 nm bandpass spectral filter to limit bleedthrough into our green PMT channel. Visual stimuli were generated using a python and OpenGL-based, open-source software package we have developed in the lab, called flystim <ext-link ext-link-type="uri" xlink:href="https://github.com/ClandininLab/flystim">https://github.com/ClandininLab/flystim</ext-link>; <xref ref-type="bibr" rid="bib83">Steven et al., 2022</xref>. Flystim renders three-dimensional objects in real time and computes the required perspective correction based on the geometry of the screen and animal position in the experimental setup to generate perspective-appropriate virtual reality stimuli. Rotating stimuli (e.g., gratings and images) were rendered as textures on the inside of virtual cylinders. Small spot stimuli were rendered as patches moving on cylindrical or spherical trajectories. Another custom, open-source software package, visprotocol <ext-link ext-link-type="uri" xlink:href="https://github.com/ClandininLab/visprotocol">https://github.com/ClandininLab/visprotocol</ext-link>, <xref ref-type="bibr" rid="bib92">Turner and Choi, 2022</xref> was used to control visual stimulation protocols and handle experimental metadata.</p><p>Stimulus code for every stimulus used here can be found in the GitHub repositories for flystim and visprotocol. Below we describe some of the key visual stimulus parameters. For the synthetic visual stimulus suite, we presented 32 distinct stimulus parameterizations. All stimuli were presented from a mean gray background that remained on, between trials, throughout the entire experiment. Each stimulus presentation period was 3 s long, and was preceded and followed by 1.5 s of pre- and tail time with a mean gray background. Note that we also presented uniform flashes of Â±100% contrast, but these stimuli did not drive responses in any glomerulus so we have excluded these stimuli from this paper. Visual stimuli were randomly interleaved within each imaging series.</p><p>For natural image experiments (<xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig7">7</xref> and <xref ref-type="fig" rid="fig8">8</xref>), we used grayscale natural images from the van Hateren database (<xref ref-type="bibr" rid="bib95">van Hateren and van der Schaaf, 1998</xref>). When presenting filtered versions of natural images, we rescaled the filtered images such that they had the same mean and standard deviation pixel values as the original images. We scaled the whitened images to have the same peak pixel intensity as the original image.</p><p>For the saccade stimulus (<xref ref-type="fig" rid="fig9">Figure 9</xref>), we used a van Hateren natural image as the background while a small, dark probe stimulus (15Â° in diameter) moved across the screen at 100Â°/s. The background image was translated by 70Â° over 200 ms to mimic fly walking saccades (<xref ref-type="bibr" rid="bib26">Cruz et al., 2021</xref>).</p><p>Virtual reality stimuli (<xref ref-type="fig" rid="fig6">Figure 6</xref>) consisted of a 3D environment with a Gaussian-smoothed random noise texture on the âfloorâ and a collection of randomly located vertical, dark, cylinders. To simulate the visual input that would be generated from <italic>Drosophila</italic> walking through such an environment, we moved the camera through the scene according to measured fly walking trajectories. Trajectories of female flies walking in the dark were measured in a 1 m<sup>2</sup> arena with automatic locomotion tracking, as described previously (<xref ref-type="bibr" rid="bib102">York et al., 2022</xref>). About 20 s snippets from measured trajectories were selected to include periods of locomotor movement, and to exclude long stationary periods. Each fly was presented with five walking trajectories, each with its own randomly-generated pattern of cylinder locations, and five trials of each trajectory were shown.</p></sec><sec id="s4-5"><title>Behavior tracking</title><p>For experiments with behavior tracking, we raised a patterned, air-suspended ball underneath the fly to monitor its fictive walking behavior, as in <xref ref-type="bibr" rid="bib17">Brezovec et al., 2022</xref>. We monitored the fly and ball movement using IR illumination and a camera triggered by our imaging acquisition software at 50 Hz frame rate.</p></sec><sec id="s4-6"><title>Alignment between in vivo functional imaging data and glomerulus map</title><p>To assign voxels in a single flyâs functional in vivo image to an optic glomerulus of interest, we generated a chain of image registrations using ANTsPy (<xref ref-type="bibr" rid="bib7">Avants et al., 2014</xref>; <xref ref-type="bibr" rid="bib93">Tustison et al., 2021</xref>). First, each volumetric image series, including both functional and anatomical scans, was motion corrected using the myr::tdTomato signal. We then created a âmean brainâ using high-resolution anatomical scans from 11 different animals, which we aligned to one another using the myr::tdTomato channel, and averaged iteratively until a clean, crisp mean brain of the PVLP/PLP was produced. The syt1GCaMP6f channel of the mean brain was then used to register the mean brain to a hand-cropped subregion of the JRC2018 template brain (<xref ref-type="bibr" rid="bib13">Bogovic et al., 2020</xref>). To generate glomerulus masks, we first extracted the presynaptic T-bar locations in the PVLP/PLP for all LC and LPLC neurons using the <italic>Drosophila</italic> hemibrain connectome (<xref ref-type="bibr" rid="bib81">Scheffer et al., 2020</xref>) and custom-written R code relying on the natverse suite of registration tools (<xref ref-type="bibr" rid="bib11">Bates et al., 2020</xref>). We used a published transformation between JRC2018 space and the <italic>Drosophila</italic> hemibrain connectome space (<xref ref-type="bibr" rid="bib81">Scheffer et al., 2020</xref>), as a start to map hemibrain synapse locations to JRC2018 space, but we also computed a small additional transformation between VPN T-Bar density and JRC2018 to improve alignment at the glomerulus level. This yielded masks for each glomerulus in our in vivo mean brain space. Finally, each flyâs functional image was registered to that flyâs own high-resolution anatomical scan, and this anatomical scan was aligned to the mean brain. We could then bring each glomerulus mask into the functional image space of each individual fly. These masks were used to collect voxels corresponding to each distinct glomerulus, and the included voxel signals were averaged over space to yield the glomerulus response. For Split-Gal4 imaging data, we hand-drew ROIs in the glomerulus.</p></sec><sec id="s4-7"><title>Analysis of visually evoked calcium signals</title><p>Glomerulus responses from the imaging series were aligned to visual stimulus onset times using a photodiode tracking the projector timing. We used a window of time before stimulus onset (typically 1â2 s) to measure a baseline fluorescence for each trial. Using this baseline, we converted trial responses to reported dF/F values. For the functional clustering presented in <xref ref-type="fig" rid="fig3">Figure 3</xref>, we used a complete linkage criterion. Statistical significance was determined using step-down Bonferroni corrected p values from t test, and a significance criterion of 0.05.</p></sec><sec id="s4-8"><title>Small object discriminability analysis</title><p>For the small object discrimination task in <xref ref-type="fig" rid="fig1">Figure 1</xref>, we moved a 15Â° dark patch across a grayscale natural image and through a âreceptive fieldâ similar in size to small object detecting VPNs. For each time point, we defined the local luminance as the average pixel intensity within the receptive field and the local spatial contrast as the variance of pixel intensities normalized by the mean pixel intensity within the receptive field. We quantified discriminability between the âspot presentâ and âspot absentâ conditions using dâ², defined below:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msup><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">â²</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">n</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">n</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where mean and var represent the mean and variance of luminance or contrast within the time window when the patch passed through the receptive field. For luminance-based discrimination, we inverted the sign of dâ² because the presence of the patch was indicated by a decrease in local luminance.</p></sec><sec id="s4-9"><title>Single-trial stimulus decoding model</title><p>For the single-trial decoding model presented in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we used a multinomial logistic regression model to predict stimulus identity using a vector of glomerulus response amplitudes for each trial. For the decoding model, responses for each glomerulus were z-scored to standardize the mean and variance across glomeruli. To train the model, we used 90% of trials, and the remaining 10% of trials were used to test performance. We iterated training/testing 100 times and we present averages across all iterations. For the trial shuffling analysis in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we shuffled response amplitudes across trials of the same stimulus identity independently for each glomerulus, such that the stimulus-dependent means and variances of responses were the same, but the covariance structure was removed.</p></sec><sec id="s4-10"><title>Analysis of behavior data</title><p>To measure fictive walking behavior from video recordings of flies on an air-suspended ball, we used FicTrac (<xref ref-type="bibr" rid="bib59">Moore et al., 2014</xref>) to process videos post hoc. To measure walking amplitude, at each point in time, we calculated the magnitude of the total rotation vector, using the ball rotation over all three axes of rotation, that is, walking amplitude=<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msubsup><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msubsup><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msubsup><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. To classify trials as walking versus not walking, a threshold was automatically determined for each walking amplitude trajectory, using the Li minimum cross entropy method (<xref ref-type="bibr" rid="bib51">Li and Lee, 1993</xref>). A trial was classified as walking if the walking amplitude exceeded this threshold for at least 25% of the time points in that trial.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82587-mdarchecklist1-v3.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All software and code is available on GitHub. Main analysis, modeling and figure generation code can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/mhturner/glom_pop">https://github.com/mhturner/glom_pop</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d412b092f500d397c4993c5c2636629a438cf8e4;origin=https://github.com/mhturner/glom_pop;visit=swh:1:snp:dfd606fd55235e4053a5db4549a9213341c49a56;anchor=swh:1:rev:4a8de1aba83bf1a7f2baadd86e23234d5cddd9fa">swh:1:rev:4a8de1aba83bf1a7f2baadd86e23234d5cddd9fa</ext-link>); Visual stimulus code can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/ClandininLab/visanalysis">https://github.com/ClandininLab/visanalysis</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:ad440c2f79ec195bed544a7669f7d8960b1a9171;origin=https://github.com/ClandininLab/visanalysis;visit=swh:1:snp:6049a08b6389a5f8e61e41806cd01189c01283ec;anchor=swh:1:rev:9e50cf2f38ea0e78dcab6818ff7ad0d1b7a1585a">swh:1:rev:9e50cf2f38ea0e78dcab6818ff7ad0d1b7a1585a</ext-link>) and here: <ext-link ext-link-type="uri" xlink:href="https://github.com/ClandininLab/flystim">https://github.com/ClandininLab/flystim</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:797ec5183e689fa1836adecae3c3f8def1804b2b;origin=https://github.com/ClandininLab/flystim;visit=swh:1:snp:6ba88c11fce4712585869446754e819dc6f5735c;anchor=swh:1:rev:bcc8f3e106544444e3442396b14b817df98937fd">swh:1:rev:bcc8f3e106544444e3442396b14b817df98937fd</ext-link>). Extracted ROI responses and associated stimulus metadata, along with raw imaging data, can be found in a Dryad repository here: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.h44j0zpp8">https://doi.org/10.5061/dryad.h44j0zpp8</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data from: Visual and motor signatures of locomotion dynamically shape a population code for feature detection in <italic>Drosophila</italic></data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.h44j0zpp8</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank Estela Stephenson for excellent technical support. Steven Herbst designed the original version of flystim, of which an updated version was used for visual stimulation for this work. The authors thank Fred Rieke, Karin NordstrÃ¶m, the reviewers, and members of the Clandinin lab for helpful feedback on earlier versions of this manuscript. This project was supported by NIH grants F32-MH118707 (MHT), K99-EY032549 (MHT), R01 EY022638 (TRC), R01NS110060 (TRC), the NSF GRFP (AK), and an NDSEG fellowship (MMP).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ache</surname><given-names>JM</given-names></name><name><surname>Polsky</surname><given-names>J</given-names></name><name><surname>Alghailani</surname><given-names>S</given-names></name><name><surname>Parekh</surname><given-names>R</given-names></name><name><surname>Breads</surname><given-names>P</given-names></name><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>von Reyn</surname><given-names>CR</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural basis for looming size and velocity encoding in the <italic>Drosophila</italic> giant fiber escape pathway</article-title><source>Current Biology</source><volume>29</volume><fpage>1073</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.01.079</pub-id><pub-id pub-id-type="pmid">30827912</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aimon</surname><given-names>S</given-names></name><name><surname>Katsuki</surname><given-names>T</given-names></name><name><surname>Jia</surname><given-names>T</given-names></name><name><surname>Grosenick</surname><given-names>L</given-names></name><name><surname>Broxton</surname><given-names>M</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Greenspan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast near-whole-brain imaging in adult <italic>Drosophila</italic> during responses to stimuli and behavior</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e2006732</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2006732</pub-id><pub-id pub-id-type="pmid">30768592</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ala-Laurila</surname><given-names>P</given-names></name><name><surname>Greschner</surname><given-names>M</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cone photoreceptor contributions to noise and correlations in the retinal output</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1309</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1038/nn.2927</pub-id><pub-id pub-id-type="pmid">21926983</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Hess</surname><given-names>BJM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Self-motion-induced eye movements: effects on visual acuity and navigation</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>966</fpage><lpage>976</lpage><pub-id pub-id-type="doi">10.1038/nrn1804</pub-id><pub-id pub-id-type="pmid">16340956</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aptekar</surname><given-names>JW</given-names></name><name><surname>Shoemaker</surname><given-names>PA</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Figure tracking by flies is supported by parallel visual streams</article-title><source>Current Biology</source><volume>22</volume><fpage>482</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.01.044</pub-id><pub-id pub-id-type="pmid">22386313</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aptekar</surname><given-names>JW</given-names></name><name><surname>KeleÅ</surname><given-names>MF</given-names></name><name><surname>Lu</surname><given-names>PM</given-names></name><name><surname>Zolotova</surname><given-names>NM</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neurons forming optic glomeruli compute figure-ground discriminations in <italic>Drosophila</italic></article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7587</fpage><lpage>7599</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0652-15.2015</pub-id><pub-id pub-id-type="pmid">25972183</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Tustison</surname><given-names>N</given-names></name><name><surname>Johnson</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>Advanced normalization tools (ANTS)</data-title><version designator="0.1">0.1</version><source>Brianavants</source><ext-link ext-link-type="uri" xlink:href="https://issuu.com/brianavants/docs/ants2">https://issuu.com/brianavants/docs/ants2</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Effects of noise correlations on information encoding and decoding</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>3633</fpage><lpage>3644</lpage><pub-id pub-id-type="doi">10.1152/jn.00919.2005</pub-id><pub-id pub-id-type="pmid">16554512</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Manu</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A retinal circuit that computes object motion</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6807</fpage><lpage>6817</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4206-07.2008</pub-id><pub-id pub-id-type="pmid">18596156</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Manton</surname><given-names>JD</given-names></name><name><surname>Jagannathan</surname><given-names>SR</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Rohlfing</surname><given-names>T</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The natverse, a versatile toolbox for combining and analysing neuroanatomical data</article-title><source>eLife</source><volume>9</volume><elocation-id>e53350</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53350</pub-id><pub-id pub-id-type="pmid">32286229</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binda</surname><given-names>P</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vision during saccadic eye movements</article-title><source>Annual Review of Vision Science</source><volume>4</volume><fpage>193</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091517-034317</pub-id><pub-id pub-id-type="pmid">30222534</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogovic</surname><given-names>JA</given-names></name><name><surname>Otsuna</surname><given-names>H</given-names></name><name><surname>Heinrich</surname><given-names>L</given-names></name><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Jeter</surname><given-names>J</given-names></name><name><surname>Meissner</surname><given-names>G</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Malkesman</surname><given-names>O</given-names></name><name><surname>Ito</surname><given-names>K</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An unbiased template of the <italic>Drosophila</italic> brain and ventral nerve cord</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0236495</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0236495</pub-id><pub-id pub-id-type="pmid">33382698</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolzon</surname><given-names>DM</given-names></name><name><surname>NordstrÃ¶m</surname><given-names>K</given-names></name><name><surname>OâCarroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Local and large-range inhibition in feature detection</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>14143</fpage><lpage>14150</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2857-09.2009</pub-id><pub-id pub-id-type="pmid">19906963</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Reiff</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Fly motion vision</article-title><source>Annual Review of Neuroscience</source><volume>33</volume><fpage>49</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-060909-153155</pub-id><pub-id pub-id-type="pmid">20225934</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Kubischik</surname><given-names>M</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name><name><surname>Krekelberg</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural dynamics of saccadic suppression</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>12374</fpage><lpage>12383</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2908-09.2009</pub-id><pub-id pub-id-type="pmid">19812313</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brezovec</surname><given-names>LE</given-names></name><name><surname>Berger</surname><given-names>AB</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mapping the Neural Dynamics of Locomotion across the <italic>Drosophila</italic> Brain</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.03.20.485047</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage><pub-id pub-id-type="pmid">1464765</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of self-motion perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>389</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112953</pub-id><pub-id pub-id-type="pmid">18558861</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiappe</surname><given-names>ME</given-names></name><name><surname>Seelig</surname><given-names>JD</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Walking modulates speed sensitivity in <italic>Drosophila</italic> motion vision</article-title><source>Current Biology</source><volume>20</volume><fpage>1470</fpage><lpage>1475</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.06.072</pub-id><pub-id pub-id-type="pmid">20655222</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohn</surname><given-names>R</given-names></name><name><surname>Morantte</surname><given-names>I</given-names></name><name><surname>Ruta</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Coordinated and compartmentalized neuromodulation shapes sensory processing in <italic>Drosophila</italic></article-title><source>Cell</source><volume>163</volume><fpage>1742</fpage><lpage>1755</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.11.019</pub-id><pub-id pub-id-type="pmid">26687359</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Visual neurones for tracking moving targets</article-title><source>Nature</source><volume>232</volume><fpage>127</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1038/232127a0</pub-id><pub-id pub-id-type="pmid">4933247</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>TS</given-names></name><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1975">1975a</year><article-title>Visual control of flight behaviour in the hoverflysyritta pipiens L</article-title><source>Journal of Comparative Physiology? A</source><volume>99</volume><fpage>1</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1007/BF01464710</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>TS</given-names></name><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1975">1975b</year><article-title>Visual spatial memory in a hoverfly</article-title><source>Journal of Comparative Physiology? A</source><volume>100</volume><fpage>59</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1007/BF00623930</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Calhoun</surname><given-names>AJ</given-names></name><name><surname>Rangarajan</surname><given-names>N</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>One-to-One Mapping between Deep Network Units and Real Neurons Uncovers a Visual Population Code for Social Behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.07.18.500505</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cruz</surname><given-names>TL</given-names></name><name><surname>PÃ©rez</surname><given-names>SM</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast tuning of posture control by visual feedback underlies gaze stabilization in walking <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>31</volume><fpage>4596</fpage><lpage>4607</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.08.041</pub-id><pub-id pub-id-type="pmid">34499851</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>R</given-names></name><name><surname>Mao</surname><given-names>R</given-names></name><name><surname>Zhou</surname><given-names>E</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Chemoconnectomics: mapping chemical transmission in <italic>Drosophila</italic></article-title><source>Neuron</source><volume>101</volume><fpage>876</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.045</pub-id><pub-id pub-id-type="pmid">30799021</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>On the neuronal basis of figure-ground discrimination by relative motion in the visual system of the fly. 2: figure-dectection cells, a new class of visual interneurones</article-title><source>Biol Cybern</source><volume>52</volume><fpage>195</fpage><lpage>209</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenk</surname><given-names>LM</given-names></name><name><surname>Kim</surname><given-names>AJ</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Suppression of motion vision during course-changing, but not course-stabilizing, navigational turns</article-title><source>Current Biology</source><volume>31</volume><fpage>4608</fpage><lpage>4619</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.09.068</pub-id><pub-id pub-id-type="pmid">34644548</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischbach</surname><given-names>KF</given-names></name><name><surname>Dittrich</surname><given-names>APM</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The optic lobe of <italic>Drosophila melanogaster</italic>. I. A golgi analysis of wild-type structure</article-title><source>Cell and Tissue Research</source><volume>258</volume><elocation-id>BF00218858</elocation-id><pub-id pub-id-type="doi">10.1007/BF00218858</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>F</given-names></name><name><surname>Fiscella</surname><given-names>M</given-names></name><name><surname>Sevelev</surname><given-names>M</given-names></name><name><surname>Roska</surname><given-names>B</given-names></name><name><surname>Hierlemann</surname><given-names>A</given-names></name><name><surname>da Silveira</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Structures of neural correlation and how they favor coding</article-title><source>Neuron</source><volume>89</volume><fpage>409</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.037</pub-id><pub-id pub-id-type="pmid">26796692</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fry</surname><given-names>SN</given-names></name><name><surname>Sayaman</surname><given-names>R</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The aerodynamics of free-flight maneuvers in <italic>Drosophila</italic></article-title><source>Science</source><volume>300</volume><fpage>495</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1126/science.1081944</pub-id><pub-id pub-id-type="pmid">12702878</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujiwara</surname><given-names>T</given-names></name><name><surname>Cruz</surname><given-names>TL</given-names></name><name><surname>Bohnslav</surname><given-names>JP</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A faithful internal representation of walking movements in the <italic>Drosophila</italic> visual system</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>72</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1038/nn.4435</pub-id><pub-id pub-id-type="pmid">27798632</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujiwara</surname><given-names>T</given-names></name><name><surname>Brotas</surname><given-names>M</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Walking strides direct rapid and flexible recruitment of visual circuits for course control in <italic>Drosophila</italic></article-title><source>Neuron</source><volume>110</volume><fpage>2124</fpage><lpage>2138</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.04.008</pub-id><pub-id pub-id-type="pmid">35525243</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geurten</surname><given-names>BRH</given-names></name><name><surname>JÃ¤hde</surname><given-names>P</given-names></name><name><surname>Corthals</surname><given-names>K</given-names></name><name><surname>GÃ¶pfert</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Saccadic body turns in walking <italic>Drosophila</italic></article-title><source>Frontiers in Behavioral Neuroscience</source><volume>8</volume><elocation-id>365</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00365</pub-id><pub-id pub-id-type="pmid">25386124</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardcastle</surname><given-names>BJ</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evolution of biological image stabilization</article-title><source>Current Biology</source><volume>26</volume><fpage>R1010</fpage><lpage>R1021</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.059</pub-id><pub-id pub-id-type="pmid">27780044</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindmarsh Sten</surname><given-names>T</given-names></name><name><surname>Li</surname><given-names>R</given-names></name><name><surname>Otopalik</surname><given-names>A</given-names></name><name><surname>Ruta</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sexual arousal gates visual processing during <italic>Drosophila</italic> courtship</article-title><source>Nature</source><volume>595</volume><fpage>549</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03714-w</pub-id><pub-id pub-id-type="pmid">34234348</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Dau</surname><given-names>A</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Solanki</surname><given-names>N</given-names></name><name><surname>Rien</surname><given-names>D</given-names></name><name><surname>Jaciuch</surname><given-names>D</given-names></name><name><surname>Dongre</surname><given-names>SA</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Microsaccadic sampling of moving image information provides <italic>Drosophila</italic> hyperacute vision</article-title><source>eLife</source><volume>6</volume><elocation-id>e26117</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26117</pub-id><pub-id pub-id-type="pmid">28870284</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>KeleÅ</surname><given-names>MF</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Object-detecting neurons in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>27</volume><fpage>680</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.01.012</pub-id><pub-id pub-id-type="pmid">28190726</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>KeleÅ</surname><given-names>MF</given-names></name><name><surname>Hardcastle</surname><given-names>BJ</given-names></name><name><surname>StÃ¤dele</surname><given-names>C</given-names></name><name><surname>Xiao</surname><given-names>Q</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inhibitory interactions and columnar inputs to an object motion detector in <italic>Drosophila</italic></article-title><source>Cell Reports</source><volume>30</volume><fpage>2115</fpage><lpage>2124</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2020.01.061</pub-id><pub-id pub-id-type="pmid">32075756</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Feature detection by retinal ganglion cells</article-title><source>Annual Review of Vision Science</source><volume>8</volume><fpage>135</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-100419-112009</pub-id><pub-id pub-id-type="pmid">35385673</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>AJ</given-names></name><name><surname>Fitzgerald</surname><given-names>JK</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cellular evidence for efference copy in <italic>Drosophila</italic> visuomotor processing</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1247</fpage><lpage>1255</lpage><pub-id pub-id-type="doi">10.1038/nn.4083</pub-id><pub-id pub-id-type="pmid">26237362</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>AJ</given-names></name><name><surname>Fenk</surname><given-names>LM</given-names></name><name><surname>Lyu</surname><given-names>C</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Quantitative predictions orchestrate visual signaling in <italic>Drosophila</italic></article-title><source>Cell</source><volume>168</volume><fpage>280</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.12.005</pub-id><pub-id pub-id-type="pmid">28065412</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmerle</surname><given-names>B</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Performance of fly visual interneurons during object fixation</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>6256</fpage><lpage>6266</lpage><pub-id pub-id-type="pmid">10934276</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klapoetke</surname><given-names>NC</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Rogers</surname><given-names>EM</given-names></name><name><surname>Breads</surname><given-names>P</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ultra-selective looming detection from radial motion opponency</article-title><source>Nature</source><volume>551</volume><fpage>237</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1038/nature24626</pub-id><pub-id pub-id-type="pmid">29120418</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klapoetke</surname><given-names>NC</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Rogers</surname><given-names>EM</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A functionally ordered visual feature map in the <italic>Drosophila</italic> brain</article-title><source>Neuron</source><volume>110</volume><fpage>1700</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.02.013</pub-id><pub-id pub-id-type="pmid">35290791</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>JR</given-names></name><name><surname>Portes</surname><given-names>JP</given-names></name><name><surname>Christenson</surname><given-names>MP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Behnia</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Flexible filtering by neural inputs supports motion computation across states and stimuli</article-title><source>Current Biology</source><volume>31</volume><fpage>5249</fpage><lpage>5260</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.09.061</pub-id><pub-id pub-id-type="pmid">34670114</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Motion and vision: why animals move their eyes</article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><volume>185</volume><fpage>341</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s003590050393</pub-id><pub-id pub-id-type="pmid">10555268</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>JCS</given-names></name><name><surname>Esch</surname><given-names>JJ</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Direction selectivity in <italic>Drosophila</italic> emerges from preferred-direction enhancement and null-direction suppression</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>8078</fpage><lpage>8092</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1272-16.2016</pub-id><pub-id pub-id-type="pmid">27488629</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lettvin</surname><given-names>JY</given-names></name><name><surname>Maturana</surname><given-names>HR</given-names></name><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>What the frogâs eye tells the frogâs brain</article-title><source>Proceedings of the IRE</source><volume>47</volume><fpage>1940</fpage><lpage>1951</lpage><pub-id pub-id-type="doi">10.1109/JRPROC.1959.287207</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>CH</given-names></name><name><surname>Lee</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Minimum cross entropy thresholding</article-title><source>Pattern Recognition</source><volume>26</volume><fpage>617</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(93)90115-D</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maimon</surname><given-names>G</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Active flight increases the gain of visual motion processing in <italic>Drosophila</italic></article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>393</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1038/nn.2492</pub-id><pub-id pub-id-type="pmid">20154683</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modulation of visual physiology by behavioral state in monkeys, mice, and flies</article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>559</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.05.001</pub-id><pub-id pub-id-type="pmid">21628097</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maisak</surname><given-names>MS</given-names></name><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Ammer</surname><given-names>G</given-names></name><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Schilling</surname><given-names>T</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name><name><surname>Reiff</surname><given-names>DF</given-names></name><name><surname>Hopp</surname><given-names>E</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A directional tuning map of <italic>Drosophila</italic> elementary motion detectors</article-title><source>Nature</source><volume>500</volume><fpage>212</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1038/nature12320</pub-id><pub-id pub-id-type="pmid">23925246</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>K</given-names></name><name><surname>Gallen</surname><given-names>CL</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Whole-brain calcium imaging reveals an intrinsic functional network in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>27</volume><fpage>2389</fpage><lpage>2396</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.076</pub-id><pub-id pub-id-type="pmid">28756955</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Conde</surname><given-names>S</given-names></name><name><surname>Otero-Millan</surname><given-names>J</given-names></name><name><surname>Macknik</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The impact of microsaccades on vision: towards a unified theory of saccadic function</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1038/nrn3405</pub-id><pub-id pub-id-type="pmid">23329159</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00431.1999</pub-id><pub-id pub-id-type="pmid">9870971</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McBride</surname><given-names>EG</given-names></name><name><surname>Lee</surname><given-names>SYJ</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local and global influences of visual spatial selection and locomotion in mouse primary visual cortex</article-title><source>Current Biology</source><volume>29</volume><fpage>1592</fpage><lpage>1605</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.03.065</pub-id><pub-id pub-id-type="pmid">31056388</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>RJD</given-names></name><name><surname>Taylor</surname><given-names>GJ</given-names></name><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Pearson</surname><given-names>T</given-names></name><name><surname>van Swinderen</surname><given-names>B</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>FicTrac: a visual method for tracking spherical motion and generating fictive animal paths</article-title><source>Journal of Neuroscience Methods</source><volume>225</volume><fpage>106</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.01.010</pub-id><pub-id pub-id-type="pmid">24491637</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NordstrÃ¶m</surname><given-names>K</given-names></name><name><surname>Barnett</surname><given-names>PD</given-names></name><name><surname>OâCarroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Insect detection of small targets moving in visual clutter</article-title><source>PLOS Biology</source><volume>4</volume><elocation-id>e54</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0040054</pub-id><pub-id pub-id-type="pmid">16448249</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NordstrÃ¶m</surname><given-names>K</given-names></name><name><surname>OâCarroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Small object detection neurons in female hoverflies</article-title><source>Proceedings. Biological Sciences</source><volume>273</volume><fpage>1211</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1098/rspb.2005.3424</pub-id><pub-id pub-id-type="pmid">16720393</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NordstrÃ¶m</surname><given-names>K</given-names></name><name><surname>OâCarroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Feature detection and the hypercomplex property in insects</article-title><source>Trends in Neurosciences</source><volume>32</volume><fpage>383</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2009.03.004</pub-id><pub-id pub-id-type="pmid">19541374</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NordstrÃ¶m</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural specializations for small target detection in insects</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>272</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.12.013</pub-id><pub-id pub-id-type="pmid">22244741</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Segregation of object and background motion in the retina</article-title><source>Nature</source><volume>423</volume><fpage>401</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1038/nature01652</pub-id><pub-id pub-id-type="pmid">12754524</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otsuna</surname><given-names>H</given-names></name><name><surname>Ito</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Systematic analysis of the visual projection neurons of <italic>Drosophila melanogaster</italic>. I. lobula-specific pathways</article-title><source>The Journal of Comparative Neurology</source><volume>497</volume><fpage>928</fpage><lpage>958</lpage><pub-id pub-id-type="doi">10.1002/cne.21015</pub-id><pub-id pub-id-type="pmid">16802334</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pacheco</surname><given-names>DA</given-names></name><name><surname>Thiberge</surname><given-names>SY</given-names></name><name><surname>Pnevmatikakis</surname><given-names>E</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Auditory activity is diverse and widespread throughout the central brain of <italic>Drosophila</italic></article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>93</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00743-y</pub-id><pub-id pub-id-type="pmid">33230320</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panser</surname><given-names>K</given-names></name><name><surname>Tirian</surname><given-names>L</given-names></name><name><surname>Schulze</surname><given-names>F</given-names></name><name><surname>Villalba</surname><given-names>S</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name><name><surname>BÃ¼hler</surname><given-names>K</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Automatic segmentation of <italic>Drosophila</italic> neural compartments using GAL4</article-title><source>Expression Data Reveals Novel Visual Pathways. Current Biology</source><volume>26</volume><fpage>1943</fpage><lpage>1954</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.05.052</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Shape representation in area V4: position-specific tuning for boundary conformation</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>2505</fpage><lpage>2519</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.5.2505</pub-id><pub-id pub-id-type="pmid">11698538</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BD</given-names></name><name><surname>Ngo</surname><given-names>TTB</given-names></name><name><surname>Hibbard</surname><given-names>KL</given-names></name><name><surname>Murphy</surname><given-names>C</given-names></name><name><surname>Jenett</surname><given-names>A</given-names></name><name><surname>Truman</surname><given-names>JW</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Refinement of tools for targeted gene expression in <italic>Drosophila</italic></article-title><source>Genetics</source><volume>186</volume><fpage>735</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1534/genetics.110.119917</pub-id><pub-id pub-id-type="pmid">20697123</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piscopo</surname><given-names>DM</given-names></name><name><surname>El-Danaf</surname><given-names>RN</given-names></name><name><surname>Huberman</surname><given-names>AD</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Diverse visual features encoded in mouse lateral geniculate nucleus</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>4642</fpage><lpage>4656</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5187-12.2013</pub-id><pub-id pub-id-type="pmid">23486939</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pruszynski</surname><given-names>JA</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The language of the brain: real-world neural population codes</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>30</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.06.005</pub-id><pub-id pub-id-type="pmid">31326721</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Goris</surname><given-names>RL</given-names></name><name><surname>Cohen</surname><given-names>M</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention stabilizes the shared gain of V4 populations</article-title><source>eLife</source><volume>4</volume><elocation-id>e08998</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08998</pub-id><pub-id pub-id-type="pmid">26523390</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>AM</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Free-flight odor tracking in <italic>Drosophila</italic> is consistent with an optimal intermittent scale-free search</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e354</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000354</pub-id><pub-id pub-id-type="pmid">17406678</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>IMA</given-names></name><name><surname>Drews</surname><given-names>M</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Machacek</surname><given-names>C</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual projection neurons mediating directed courtship in <italic>Drosophila</italic></article-title><source>Cell</source><volume>174</volume><fpage>607</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.020</pub-id><pub-id pub-id-type="pmid">30033367</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>HernÃ¡ndez</surname><given-names>A</given-names></name><name><surname>Zainos</surname><given-names>A</given-names></name><name><surname>Salinas</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Correlated neuronal discharges that increase coding efficiency during perceptual discrimination</article-title><source>Neuron</source><volume>38</volume><fpage>649</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00287-3</pub-id><pub-id pub-id-type="pmid">12765615</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The unsteady eye: an information-processing stage, not a bug</article-title><source>Trends in Neurosciences</source><volume>38</volume><fpage>195</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2015.01.005</pub-id><pub-id pub-id-type="pmid">25698649</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumyantsev</surname><given-names>OI</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Hernandez</surname><given-names>O</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Savall</surname><given-names>J</given-names></name><name><surname>Chrapkiewicz</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fundamental bounds on the fidelity of sensory cortical coding</article-title><source>Nature</source><volume>580</volume><fpage>100</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2130-2</pub-id><pub-id pub-id-type="pmid">32238928</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schaffer</surname><given-names>ES</given-names></name><name><surname>Mishra</surname><given-names>N</given-names></name><name><surname>Whiteway</surname><given-names>MR</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Vancura</surname><given-names>MB</given-names></name><name><surname>Freedman</surname><given-names>J</given-names></name><name><surname>Patel</surname><given-names>KB</given-names></name><name><surname>Voleti</surname><given-names>V</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Hillman</surname><given-names>EMC</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Flygenvectors: The Spatial and Temporal Structure of Neural Activity across the Fly Brain Pages: 2021.09.25.461804 Section: New Results</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.09.25.461804</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Xu</surname><given-names>CS</given-names></name><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Takemura</surname><given-names>S-Y</given-names></name><name><surname>Hayworth</surname><given-names>KJ</given-names></name><name><surname>Huang</surname><given-names>GB</given-names></name><name><surname>Shinomiya</surname><given-names>K</given-names></name><name><surname>Maitlin-Shepard</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Clements</surname><given-names>J</given-names></name><name><surname>Hubbard</surname><given-names>PM</given-names></name><name><surname>Katz</surname><given-names>WT</given-names></name><name><surname>Umayam</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Ackerman</surname><given-names>D</given-names></name><name><surname>Blakely</surname><given-names>T</given-names></name><name><surname>Bogovic</surname><given-names>J</given-names></name><name><surname>Dolafi</surname><given-names>T</given-names></name><name><surname>Kainmueller</surname><given-names>D</given-names></name><name><surname>Kawase</surname><given-names>T</given-names></name><name><surname>Khairy</surname><given-names>KA</given-names></name><name><surname>Leavitt</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>PH</given-names></name><name><surname>Lindsey</surname><given-names>L</given-names></name><name><surname>Neubarth</surname><given-names>N</given-names></name><name><surname>Olbris</surname><given-names>DJ</given-names></name><name><surname>Otsuna</surname><given-names>H</given-names></name><name><surname>Trautman</surname><given-names>ET</given-names></name><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Goldammer</surname><given-names>J</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Svirskas</surname><given-names>R</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Neace</surname><given-names>E</given-names></name><name><surname>Knecht</surname><given-names>CJ</given-names></name><name><surname>Alvarado</surname><given-names>CX</given-names></name><name><surname>Bailey</surname><given-names>DA</given-names></name><name><surname>Ballinger</surname><given-names>S</given-names></name><name><surname>Borycz</surname><given-names>JA</given-names></name><name><surname>Canino</surname><given-names>BS</given-names></name><name><surname>Cheatham</surname><given-names>N</given-names></name><name><surname>Cook</surname><given-names>M</given-names></name><name><surname>Dreher</surname><given-names>M</given-names></name><name><surname>Duclos</surname><given-names>O</given-names></name><name><surname>Eubanks</surname><given-names>B</given-names></name><name><surname>Fairbanks</surname><given-names>K</given-names></name><name><surname>Finley</surname><given-names>S</given-names></name><name><surname>Forknall</surname><given-names>N</given-names></name><name><surname>Francis</surname><given-names>A</given-names></name><name><surname>Hopkins</surname><given-names>GP</given-names></name><name><surname>Joyce</surname><given-names>EM</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Kirk</surname><given-names>NA</given-names></name><name><surname>Kovalyak</surname><given-names>J</given-names></name><name><surname>Lauchie</surname><given-names>SA</given-names></name><name><surname>Lohff</surname><given-names>A</given-names></name><name><surname>Maldonado</surname><given-names>C</given-names></name><name><surname>Manley</surname><given-names>EA</given-names></name><name><surname>McLin</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>C</given-names></name><name><surname>Ndama</surname><given-names>M</given-names></name><name><surname>Ogundeyi</surname><given-names>O</given-names></name><name><surname>Okeoma</surname><given-names>N</given-names></name><name><surname>Ordish</surname><given-names>C</given-names></name><name><surname>Padilla</surname><given-names>N</given-names></name><name><surname>Patrick</surname><given-names>CM</given-names></name><name><surname>Paterson</surname><given-names>T</given-names></name><name><surname>Phillips</surname><given-names>EE</given-names></name><name><surname>Phillips</surname><given-names>EM</given-names></name><name><surname>Rampally</surname><given-names>N</given-names></name><name><surname>Ribeiro</surname><given-names>C</given-names></name><name><surname>Robertson</surname><given-names>MK</given-names></name><name><surname>Rymer</surname><given-names>JT</given-names></name><name><surname>Ryan</surname><given-names>SM</given-names></name><name><surname>Sammons</surname><given-names>M</given-names></name><name><surname>Scott</surname><given-names>AK</given-names></name><name><surname>Scott</surname><given-names>AL</given-names></name><name><surname>Shinomiya</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>NL</given-names></name><name><surname>Sobeski</surname><given-names>MA</given-names></name><name><surname>Suleiman</surname><given-names>A</given-names></name><name><surname>Swift</surname><given-names>J</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Talebi</surname><given-names>I</given-names></name><name><surname>Tarnogorska</surname><given-names>D</given-names></name><name><surname>Tenshaw</surname><given-names>E</given-names></name><name><surname>Tokhi</surname><given-names>T</given-names></name><name><surname>Walsh</surname><given-names>JJ</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Horne</surname><given-names>JA</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Parekh</surname><given-names>R</given-names></name><name><surname>Rivlin</surname><given-names>PK</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name><name><surname>Ito</surname><given-names>K</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Hess</surname><given-names>HF</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A connectome and analysis of the adult <italic>Drosophila</italic> central brain</article-title><source>eLife</source><volume>9</volume><elocation-id>e57443</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57443</pub-id><pub-id pub-id-type="pmid">32880371</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>StÃ¤dele</surname><given-names>C</given-names></name><name><surname>KeleÅ</surname><given-names>MF</given-names></name><name><surname>Mongeau</surname><given-names>JM</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Non-canonical receptive field properties and neuromodulation of feature-detecting neurons in flies</article-title><source>Current Biology</source><volume>30</volume><fpage>2508</fpage><lpage>2519</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.069</pub-id><pub-id pub-id-type="pmid">32442460</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Steven</surname><given-names>H</given-names></name><name><surname>Maxwell</surname><given-names>TH</given-names></name><name><surname>Choi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Flystim</data-title><version designator="swh:1:rev:bcc8f3e106544444e3442396b14b817df98937fd">swh:1:rev:bcc8f3e106544444e3442396b14b817df98937fd</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:797ec5183e689fa1836adecae3c3f8def1804b2b;origin=https://github.com/ClandininLab/flystim;visit=swh:1:snp:6ba88c11fce4712585869446754e819dc6f5735c;anchor=swh:1:rev:bcc8f3e106544444e3442396b14b817df98937fd">https://archive.softwareheritage.org/swh:1:dir:797ec5183e689fa1836adecae3c3f8def1804b2b;origin=https://github.com/ClandininLab/flystim;visit=swh:1:snp:6ba88c11fce4712585869446754e819dc6f5735c;anchor=swh:1:rev:bcc8f3e106544444e3442396b14b817df98937fd</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Tsyboulski</surname><given-names>D</given-names></name><name><surname>Lindo</surname><given-names>SE</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>High-Precision coding in visual cortex</article-title><source>Cell</source><volume>184</volume><fpage>2767</fpage><lpage>2778</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2021.03.042</pub-id><pub-id pub-id-type="pmid">33857423</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strother</surname><given-names>JA</given-names></name><name><surname>Wu</surname><given-names>ST</given-names></name><name><surname>Rogers</surname><given-names>EM</given-names></name><name><surname>Eliason</surname><given-names>JLM</given-names></name><name><surname>Wong</surname><given-names>AM</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Behavioral state modulates the ON visual motion pathway of <italic>Drosophila</italic></article-title><source>PNAS</source><volume>115</volume><fpage>E102</fpage><lpage>E111</lpage><pub-id pub-id-type="doi">10.1073/pnas.1703090115</pub-id><pub-id pub-id-type="pmid">29255026</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suver</surname><given-names>MP</given-names></name><name><surname>Mamiya</surname><given-names>A</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Octopamine neurons mediate flight-induced modulation of visual processing in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>22</volume><fpage>2294</fpage><lpage>2302</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.10.034</pub-id><pub-id pub-id-type="pmid">23142045</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>R</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Object-displacement-sensitive visual neurons drive freezing in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>30</volume><fpage>2532</fpage><lpage>2550</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.068</pub-id><pub-id pub-id-type="pmid">32442466</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>R</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural mechanisms to exploit positional geometry for collision avoidance</article-title><source>Current Biology</source><volume>32</volume><fpage>2357</fpage><lpage>2374</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.04.023</pub-id><pub-id pub-id-type="pmid">35508172</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Henning</surname><given-names>P</given-names></name><name><surname>Kubischik</surname><given-names>M</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural mechanisms of saccadic suppression</article-title><source>Science</source><volume>295</volume><fpage>2460</fpage><lpage>2462</lpage><pub-id pub-id-type="doi">10.1126/science.1068788</pub-id><pub-id pub-id-type="pmid">11923539</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Mann</surname><given-names>K</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The connectome predicts resting-state functional connectivity across the <italic>Drosophila</italic> brain</article-title><source>Current Biology</source><volume>31</volume><fpage>2386</fpage><lpage>2394</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.03.004</pub-id><pub-id pub-id-type="pmid">33770490</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Glom_pop</data-title><version designator="swh:1:rev:4a8de1aba83bf1a7f2baadd86e23234d5cddd9fa">swh:1:rev:4a8de1aba83bf1a7f2baadd86e23234d5cddd9fa</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d412b092f500d397c4993c5c2636629a438cf8e4;origin=https://github.com/mhturner/glom_pop;visit=swh:1:snp:dfd606fd55235e4053a5db4549a9213341c49a56;anchor=swh:1:rev:4a8de1aba83bf1a7f2baadd86e23234d5cddd9fa">https://archive.softwareheritage.org/swh:1:dir:d412b092f500d397c4993c5c2636629a438cf8e4;origin=https://github.com/mhturner/glom_pop;visit=swh:1:snp:dfd606fd55235e4053a5db4549a9213341c49a56;anchor=swh:1:rev:4a8de1aba83bf1a7f2baadd86e23234d5cddd9fa</ext-link></element-citation></ref><ref id="bib92"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Choi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Visanalysis</data-title><version designator="2.0.1">2.0.1</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/ClandininLab/visprotocol">https://github.com/ClandininLab/visprotocol</ext-link></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Holbrook</surname><given-names>AJ</given-names></name><name><surname>Johnson</surname><given-names>HJ</given-names></name><name><surname>Muschelli</surname><given-names>J</given-names></name><name><surname>Devenyi</surname><given-names>GA</given-names></name><name><surname>Duda</surname><given-names>JT</given-names></name><name><surname>Das</surname><given-names>SR</given-names></name><name><surname>Cullen</surname><given-names>NC</given-names></name><name><surname>Gillen</surname><given-names>DL</given-names></name><name><surname>Yassa</surname><given-names>MA</given-names></name><name><surname>Stone</surname><given-names>JR</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The antsx ecosystem for quantitative biological and medical imaging</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>9068</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-87564-6</pub-id><pub-id pub-id-type="pmid">33907199</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Linde</surname><given-names>I</given-names></name><name><surname>Rajashekar</surname><given-names>U</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Cormack</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Doves: a database of visual eye movements</article-title><source>Spatial Vision</source><volume>22</volume><fpage>161</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1163/156856809787465636</pub-id><pub-id pub-id-type="pmid">19228456</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>van der Schaaf</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B</source><volume>265</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0303</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walls</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>The evolutionary history of eye movements</article-title><source>Vision Research</source><volume>2</volume><fpage>69</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(62)90064-0</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warzecha</surname><given-names>AK</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Neural circuit tuning fly visual interneurons to motion of small objects. I. dissection of the circuit by pharmacological and photoinactivation techniques</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>329</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.2.329</pub-id><pub-id pub-id-type="pmid">8459270</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiederman</surname><given-names>SD</given-names></name><name><surname>Shoemaker</surname><given-names>PA</given-names></name><name><surname>OâCarroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A model for the detection of moving targets in visual clutter inspired by insect physiology</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e2784</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0002784</pub-id><pub-id pub-id-type="pmid">18665213</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiederman</surname><given-names>SD</given-names></name><name><surname>OâCarroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Discrimination of features in natural scenes by a dragonfly neuron</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>7141</fpage><lpage>7144</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0970-11.2011</pub-id><pub-id pub-id-type="pmid">21562276</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>M</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Williamson</surname><given-names>WR</given-names></name><name><surname>Morimoto</surname><given-names>MM</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual projection neurons in the <italic>Drosophila</italic> lobula link feature detection to distinct behavioral programs</article-title><source>eLife</source><volume>5</volume><elocation-id>e21022</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21022</pub-id><pub-id pub-id-type="pmid">28029094</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Corollary discharge contributions to perceptual continuity across saccades</article-title><source>Annual Review of Vision Science</source><volume>4</volume><fpage>215</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061207</pub-id><pub-id pub-id-type="pmid">30222532</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>York</surname><given-names>RA</given-names></name><name><surname>Brezovec</surname><given-names>LE</given-names></name><name><surname>Coughlan</surname><given-names>J</given-names></name><name><surname>Herbst</surname><given-names>S</given-names></name><name><surname>Krieger</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>SY</given-names></name><name><surname>Pratt</surname><given-names>B</given-names></name><name><surname>Smart</surname><given-names>AD</given-names></name><name><surname>Song</surname><given-names>E</given-names></name><name><surname>Suvorov</surname><given-names>A</given-names></name><name><surname>Matute</surname><given-names>DR</given-names></name><name><surname>Tuthill</surname><given-names>JC</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The evolutionary trajectory of drosophilid walking</article-title><source>Current Biology</source><volume>32</volume><fpage>3005</fpage><lpage>3015</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.05.039</pub-id><pub-id pub-id-type="pmid">35671756</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohary</surname><given-names>E</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title><source>Nature</source><volume>370</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1038/370140a0</pub-id><pub-id pub-id-type="pmid">8022482</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuber</surname><given-names>BL</given-names></name><name><surname>Stark</surname><given-names>L</given-names></name><name><surname>Cook</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>Microsaccades and the velocity-amplitude relationship for saccadic eye movements</article-title><source>Science</source><volume>150</volume><fpage>1459</fpage><lpage>1460</lpage><pub-id pub-id-type="doi">10.1126/science.150.3702.1459</pub-id><pub-id pub-id-type="pmid">5855207</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Cafaro</surname><given-names>J</given-names></name><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Direction-selective circuits shape noise to ensure a precise population code</article-title><source>Neuron</source><volume>89</volume><fpage>369</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.019</pub-id><pub-id pub-id-type="pmid">26796691</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82587.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.07.14.500082" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.14.500082"/></front-stub><body><p>This manuscript investigates how the fly visual system can encode specific features in the presence of self-generated motion. Using volumetric imaging, it explores the encoding of visual features in population activity in the <italic>Drosophila</italic> visual glomeruli â a set of visual &quot;feature detectors&quot;. Through an elegant combination of neural imaging, visual stimulus manipulations, and behavioral analysis, it demonstrates that two different mechanisms, one based on motor signals and one based on visual input, serve to suppress local features during movements that would corrupt these features. The results of this study open up new directions to determine how motor and visual signals are integrated into visual processing at the level of neural circuits.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82587.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Fujiwara</surname><given-names>Terufumi</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03g001n57</institution-id><institution>Champalimaud Foundation</institution></institution-wrap><country>Portugal</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Niell</surname><given-names>Cristopher M</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0293rh119</institution-id><institution>University of Oregon</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.14.500082">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.07.14.500082v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Visual and motor signatures of locomotion dynamically shape a population code for feature detection in <italic>Drosophila</italic>&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Claude Desplan as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Terufumi Fujiwara (Reviewer #1); Cristopher M Niell (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers appreciate the quality of the work presented in this interesting manuscript. The combination of neural imaging, visual stimulus manipulations, and behavioral analysis elegantly demonstrates that two different mechanisms, one based on motor signals and the other based on visual input, serve to suppress local features during movements that would corrupt these features. In spite of the high quality of the work, the reviewers raised several technical concerns that should be addressed prior to the publication of the manuscript. It is very likely that most of these points can be resolved through the analysis of existing data and/or appropriate editing of the main text. The reviewers agree that the addition of new experimental data can be minimized.</p><p>1) You should rule out that the correlated gain modulation observed in Figure 4 (and subsequent) is not due to motion artifacts or other factors that might vary during imaging. This control could be achieved by showing/analyzing red channel traces that you might already have. Alternatively, you could add a few caveats in the discussion about whether other factors might influence correlations across the population of VPNs.</p><p>2) In Figure 5, could the walking behavior be decomposed into forward and angular velocity components? This would strengthen the association between visual signals and specific behaviors. Through this analysis, it would be important to clarify the scope of &quot;self-motion&quot; by defining whether the visual inputs associated with rotations and forward movements are processed in the same way. Could the angular velocity range be computed during inter-saccade intervals of free-moving behavior to estimate the corresponding visual responses?</p><p>3) We encourage you to split the data in Figure 3D based on walking versus stationary states to demonstrate that the VPNs projecting to LC18 show the modulation seen in Figure 5C. This result would mitigate the possibility that the modulation by self-motion results from other inputs into the glomeruli that weren't completely eliminated by the genetic manipulations.</p><p>4) If possible, please complement the data presented in Figure 6 with a comparison of the activity observed upon rotational motion and stationary gratings.</p><p>5) Please motivate the idea that stimulus identity is encoded at the level of population activity and that positive correlations enhance stimulus decoding. The enhancement in stimulus decoding appears counter-intuitive. Related to this point, it would be helpful to improve the representation of the trial-to-trial correlations in a stimulus-dependent manner.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Figure 4</p><p>Even though the analysis looks quite reasonable, I have difficulty understanding how exactly the trial-to-trial activity correlation among glomeruli improves decoding visual feature identity. If the trial-to-trial correlation is totally random across identical visual stimulations, it will not provide extra information on visual feature identity. Therefore, the trial-to-trial correlation needs to be organized such that the shared activity (amplitude) is somehow specific to each visual feature stimulus. For example, the shared activity amplitude is always around 0.8 for looming and always 0.3 for single stripe, etc. Then, isn't such consistent activity already reflected in the total activity at each glomerulus? Alternatively, one possibility I could imagine is like following:</p><p>The activity of glomerulus A to looming: total activity = 1 (glomerulus-specific activity=0.2 + shared activity=0.8).</p><p>The activity of glomerulus A to single stripe: total activity = 1 (glomerulus-specific activity=0.7 + shared activity=0.3).</p><p>In this case, we cannot decode if the visual stimulus was looming or a single stripe from the total activity of glomerulus A, but we can decode if the total activity is divided into glomerulus-specific + shared activities. Is this the correct direction to interpret the result? Anyway, I wonder if the authors could provide a bit more intuitive explanation of how the shared activity contributes to the decoding.</p><p>Figure 8</p><p>Authors elegantly demonstrated that responses to local visual features are largely suppressed during visual and body saccades. On the other hand, it is not clear yet if the responses are not disturbed by suppression during inter saccade intervals or when the fly wants to process it. I wonder if the authors could estimate the angular velocity range during inter saccade interval from free moving behavior and estimate how many visual responses can be maintained in that range.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Overall, the study was well-designed and the data was presented clearly.</p><p>1. The authors make a compelling argument that they have restricted the glomerular signals to the PN terminals, and Fig 3 verifies that they match up in terms of mean response. However, it seems possible that some of the modulations by self-motion could represent either pre-synaptic modulation, or other inputs into the glomeruli that weren't completely eliminated with their genetic approach. Splitting the data in Fig 3D based on walking vs stationary and demonstrating that the VPNs projecting to LC18 also show the modulation seen in 5C would be a good way to confirm this.</p><p>2. The data in Fig 6A-G compellingly demonstrates that low SF stimuli suppress the response, but it's not clear that it is the rotational motion that is important since there is no comparison to stationary gratings. If that data is available (as it is for Fig 7) it would be very helpful, otherwise, it might be best to clarify that this data supports low SF stimuli suppressing, and the rotational effect is only shown later. On a related point, it is a little surprising that the coherent dots suppress the response since I would expect these to be more like the high SF / whitened stimuli of Fig 7.</p><p>3. The fact that visual stimuli can be decoded from the population in the presence of modulation by movement signals is quite similar to the findings of Stringer et al 2021 and Rumyantsev et al 2020, so it might be worth noting these.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1) My first concern is about whether any of the correlated gain modulations the authors observe could be due to motion artifacts or other factors that might vary during imaging but might not reflect actual neural signal intensity variations. This is a particular concern in Figure 4 where such shared variability is first introduced. Ideally, the authors would show imaging of the red channel from the same trials showing that this is not modulated by the shared gain factor. At the very least the authors should mention possible confounds that could give rise to this variability and discuss measures taken to rule these out.</p><p>2) Although the manuscript is framed in terms of &quot;self-motion,&quot; most of the analysis and experiments focus on fast rotational motion evoked by body saccades. For example, the analysis in Figure 1 deals only with rotational motion, as do the visual suppression experiments in Figures 6 and 7. However, Figure 5 shows suppression driven by walking (not necessarily turning) and it is not clear from the figure if these represent rotations or forward movements. Therefore, it is not clear if the two inputs discussed are &quot;working together&quot; as suggested in the Discussion, or cover different types of input (during forward motion versus turns).</p><p>Although this does not detract from the interest of the work, it is confusing, as self-motion also includes large translational components which are not discussed (much) here and as saccadic suppression of visual signals has been discussed elsewhere. The authors should clarify in the Abstract and Introduction that their focus will be on rotational motion related to body saccades, and should address the differences between these types of motion in the Discussion.</p><p>3) The authors perform a decoding analysis of stimulus identity to argue that stimulus identity is encoded at the level of population activity and that positive correlations enhance stimulus decoding. This seems strange to me because classical studies of correlations in visual encoding (e.g. Shadlen Newsome) emphasized the way that correlated variability <italic>reduces</italic> the encoding capacity of a network. The emphasis on encoding stimulus identity within the particular stimulus set presented also seemed strange to me because it is not clear that the fly needs to discriminate between each of these stimuli in order to make appropriate behavioral responses. For example, flies are known to respond differently to vertical stripes versus short spots, however, it is not clear if they care about the difference between spots of slightly different sizes, or between spots moving on a gray versus grating background. Presumably, psychophysics experiments combined with connectomics can help determine which combinations of glomerular responses are actually used by the fly to shape its behavior. At any rate, I find that the conclusions about how VPNs encode visual features (e.g. Discussion line 439) rest on an assumption about what the fly is trying to do with these stimuli that may not be accurate.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82587.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) You should rule out that the correlated gain modulation observed in Figure 4 (and subsequent) is not due to motion artifacts or other factors that might vary during imaging. This control could be achieved by showing/analyzing red channel traces that you might already have. Alternatively, you could add a few caveats in the discussion about whether other factors might influence correlations across the population of VPNs.</p></disp-quote><p>Thank you for this suggestion. We have added example single trial red channel (myr::tdTomato) traces to Figure 4A, underneath the corresponding example syt1GCaMP6f traces. These structural signals show very little modulation from trial to trial relative to visually driven responses in GCaMP. We have also added a new Figure 4 âfigure supplement 2 showing the trial covariance matrix for both red and green channel signals, showing much lower covariances in red channel signals compared to green channel signals, and a qualitatively different covariance structure. We address this concern and the results of these control analyses in the main text, beginning at line 231. To specifically look for an impact of animal movement on motion artifacts, we examined the correlation between red channel traces and animal walking behavior, and found no significant correlation for any glomerulus (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82587-sa2-fig1-v3.tif"/></fig><disp-quote content-type="editor-comment"><p>2) In Figure 5, could the walking behavior be decomposed into forward and angular velocity components? This would strengthen the association between visual signals and specific behaviors. Through this analysis, it would be important to clarify the scope of &quot;self-motion&quot; by defining whether the visual inputs associated with rotations and forward movements are processed in the same way.</p></disp-quote><p>This is an excellent question, and it has prompted us to substantially expand our analysis of free walking behavior. We have moved these data to a main figure (new Figure 8) to give them the focus they deserve. During free walking behavior, saccadic turns nearly always occur while the animal is also running forward, and forward velocities during a saccade are often no different than during an intersaccadic interval (Figure 8E). Because of this, periods of high rotational velocity cannot be isolated from forward velocity in freely moving animals. On the other hand, free walking flies do run forward without much rotational velocity, creating straight runs, but such runs are generally not seen in our fictive walking on the ball data, where movement bouts are interspersed with stationary periods. In our experience, these periods of ârestâ are relatively common when flies are tethered. Because of this, we cannot decompose fictive rotational from translational velocity components in any way that allows us to associate response gain changes with one or the other component in isolation. What the ball locomotion measurements do provide is a measure of when the fly is standing still and when it is engaged in a movement bout, which consists of both rotational and forward translational components. We have updated the text to reflect the fact that in Figure 5 we cannot disambiguate between these two specific types of movement (line 297), and we later use free walking statistics in the new Figure 8 to better motivate the focus on the rotational components of saccades. We also modified the discussion (line 595) to note the inability to disambiguate forward from rotational velocity using fictive ball walking data, as well as the idea that these locomotor components are not fully separable even under natural walking conditions.</p><disp-quote content-type="editor-comment"><p>Could the angular velocity range be computed during inter-saccade intervals of free-moving behavior to estimate the corresponding visual responses?</p></disp-quote><p>This is a great question, and one that we have explored extensively in response. We will address it first as it relates to the visual gain suppression, and then as it relates to the motor-related suppression. As noted above, because of differences in walking behavior between free walking and fictive ball walking it is difficult to make a direct comparison, but two pieces of evidence shed light on how much suppression would be expected to be associated with intersaccadic walking. First, our new Figure 8 now shows forward and rotational velocities conditioned on saccadic vs. intersaccadic interval. During intersaccadic periods, rotational velocities are typically small (&lt;40 deg/sec), but they do occasionally extend into quite fast rotations (long tail of gray distribution in Figure 8E). Comparing this to the image-driven suppression we measured in Figure 7, it indicates that some glomeruli (LC11, 21, and 18) could experience visual suppression during intersaccadic intervals, whereas other glomeruli (LC6 and 26) would be relatively unaffected by the typical intersaccadic run. We now note this interesting difference in the discussion (line 584).</p><p>To address the motor-related gain control component of this question, we added a new Figure 5 âfigure supplement 1 showing the relationship between population response gain and walking amplitude, which shows minimal gain suppression up until ~10 deg/sec walking amplitude, and strong gain suppression being reached by ~40 deg/sec. Again, because of the differences between free walking and fictive ball walking itâs difficult to make absolute comparisons of these numbers to rotational velocities seen during free walking, but it does</p><disp-quote content-type="editor-comment"><p>3) We encourage you to split the data in Figure 3D based on walking versus stationary states to demonstrate that the VPNs projecting to LC18 show the modulation seen in Figure 5C. This result would mitigate the possibility that the modulation by self-motion results from other inputs into the glomeruli that weren't completely eliminated by the genetic manipulations.</p></disp-quote><p>While we donât have behavioral tracking data for an LC18-specific driver line, we do have such data using a split-Gal4 driver line for LC11, which also showed behavioral modulation in our pan-glomerulus imaging experiments. We have added these data to the new Figure 5 âfigure supplement 2, and address this important control in the main text (line 316). This genetically targeted approach recapitulated what we saw using the population imaging approach, giving us confidence that the behavioral modulation we saw came from VPNs, not some other, unidentified, ChAT-positive cell types that have neurites in the glomerulus.</p><disp-quote content-type="editor-comment"><p>4) If possible, please complement the data presented in Figure 6 with a comparison of the activity observed upon rotational motion and stationary gratings.</p></disp-quote><p>We do not have data for probe responses on top of stationary gratings, but as reviewer 2 notes, the distinction between rotating and stationary backgrounds is made in Figure 7 using natural images. We have updated the text to reflect the fact that the gratings experiments alone are not evidence that the surround is selective for rotational motion (lines 368, 358). We also clarify how the experiments using natural images in Figure 7 and the coherent dots stimulus was designed to test for selectivity for coherent, rotational motion (lines 372, 396).</p><disp-quote content-type="editor-comment"><p>5) Please motivate the idea that stimulus identity is encoded at the level of population activity and that positive correlations enhance stimulus decoding. The enhancement in stimulus decoding appears counter-intuitive. Related to this point, it would be helpful to improve the representation of the trial-to-trial correlations in a stimulus-dependent manner.</p></disp-quote><p>We apologize for the confusion about the decoding effect of positive correlations. The original manuscript did not do a good job explaining what is known about the effect of positive correlations in heterogeneously tuned populations and how this is related to what we see in VPNs. We have modified the text to highlight two key aspects of how this works: First, we have better highlighted that correlations improve decoding ability given a set amount of total variance. The overall variability itself is not helpful, only its correlation structure relative to a null model with no correlations (see paragraph starting at line 273). Second, previous work (e.g. that of Zohary, Shadlen and Newsome) that shows a deleterious effect of positive noise correlations is based on a homogeneously tuned population of neurons that use averaging across many cells to estimate the stimulus. In the case of heterogeneously tuned populations, like VPNs, correlations can âshapeâ noise along directions in population response space such that it does not interfere with estimates of the stimulus. This relies on a decoding strategy that is different from arithmetic averaging, for example a decoder that can compare differences or relative activations of differently tuned populations (see updated text at line 539). We have also added some more references to theoretical results in the literature on this topic, as our decoding results are in line with this past work and we think what we are seeing is the result of these previously-described impacts of noise correlations.</p><p>Related to this issue, and in response to feedback from reviewer 3, we have also added a new Figure 4 âfigure supplement 3 showing decoding model performance on a discrimination task using only four visual stimulus classes that are known from previous behavioral work to elicit distinct visual behaviors (grating, spot, loom and vertical bar). That is, these are stimulus classes that the fly certainly can distinguish. Similar to the main figure, this new analysis also shows that most glomerulus groups encode information about most stimuli, and that for at least some stimuli, the population contains significantly more information than a single group alone. This is in line with our conclusion that VPNs encode most visual features in a distributed fashion, rather than single glomeruli being responsible for encoding single visual features.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Figure 4</p><p>Even though the analysis looks quite reasonable, I have difficulty understanding how exactly the trial-to-trial activity correlation among glomeruli improves decoding visual feature identity. If the trial-to-trial correlation is totally random across identical visual stimulations, it will not provide extra information on visual feature identity. Therefore, the trial-to-trial correlation needs to be organized such that the shared activity (amplitude) is somehow specific to each visual feature stimulus. For example, the shared activity amplitude is always around 0.8 for looming and always 0.3 for single stripe, etc. Then, isn't such consistent activity already reflected in the total activity at each glomerulus? Alternatively, one possibility I could imagine is like following:</p><p>The activity of glomerulus A to looming: total activity = 1 (glomerulus-specific activity=0.2 + shared activity=0.8).</p><p>The activity of glomerulus A to single stripe: total activity = 1 (glomerulus-specific activity=0.7 + shared activity=0.3).</p><p>In this case, we cannot decode if the visual stimulus was looming or a single stripe from the total activity of glomerulus A, but we can decode if the total activity is divided into glomerulus-specific + shared activities. Is this the correct direction to interpret the result? Anyway, I wonder if the authors could provide a bit more intuitive explanation of how the shared activity contributes to the decoding.</p></disp-quote><p>Thanks for raising this point of confusion, which is the result of our poor explanation in the original manuscript. We have updated the text to better explain how positively correlated response variability can improve stimulus decoding relative to uncorrelated variability. See response to essential revision (5) above and updated text around lines 273 and 539.</p><disp-quote content-type="editor-comment"><p>Figure 8</p><p>Authors elegantly demonstrated that responses to local visual features are largely suppressed during visual and body saccades. On the other hand, it is not clear yet if the responses are not disturbed by suppression during inter saccade intervals or when the fly wants to process it. I wonder if the authors could estimate the angular velocity range during inter saccade interval from free moving behavior and estimate how many visual responses can be maintained in that range.</p></disp-quote><p>Thanks for this suggestion. See response to major revision (2) above. We now address this point in the updated discussion (line 584) and with a new Figure 5 âfigure supplement 1.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Overall, the study was well-designed and the data was presented clearly.</p><p>1. The authors make a compelling argument that they have restricted the glomerular signals to the PN terminals, and Fig 3 verifies that they match up in terms of mean response. However, it seems possible that some of the modulations by self-motion could represent either pre-synaptic modulation, or other inputs into the glomeruli that weren't completely eliminated with their genetic approach. Splitting the data in Fig 3D based on walking vs stationary and demonstrating that the VPNs projecting to LC18 also show the modulation seen in 5C would be a good way to confirm this.</p></disp-quote><p>This is a great suggestion and an important control. We did not collect behavioral tracking data during the split-Gal4 experiments in Fig. 3, but we do have behavioral tracking data using an LC11 split-Gal4 driver line, which shows a very similar behavioral modulation as we saw in the pan-glomerulus imaging approach. As noted above, we have added these new data to the new Figure 5 â supplement 2. As LC11 and LC18 both show strong behavioral modulation, we hope that these data address the main thrust of this concern.</p><disp-quote content-type="editor-comment"><p>2. The data in Fig 6A-G compellingly demonstrates that low SF stimuli suppress the response, but it's not clear that it is the rotational motion that is important since there is no comparison to stationary gratings. If that data is available (as it is for Fig 7) it would be very helpful, otherwise, it might be best to clarify that this data supports low SF stimuli suppressing, and the rotational effect is only shown later. On a related point, it is a little surprising that the coherent dots suppress the response since I would expect these to be more like the high SF / whitened stimuli of Fig 7.</p></disp-quote><p>Thank you - we were not clear on this in the original text. We have updated the text to clarify that the gratings experiments do not demonstrate a selectivity for rotational motion (line 368), they only characterize the spatial frequency and speed tuning of the surround (line 358). As you say, this point is made using coherent dots stimuli in Fig. 6 and natural images in Fig. 7 (updated text around line 390).</p><p>With regards to the suppression by coherent dots, itâs hard to make a direct comparison to gratings or filtered natural images, but it is important to note that the high spatial frequency gratings that fail to recruit strong surround suppression are composed of bars that are 2.5-5 degrees, whereas the individual dots that make up the coherent dots stimulus are close to 15 degrees. This is consistent with what we would expect to drive elementary motion detecting neurons T4/T5 given the spatial frequency tuning of those cells: 2.5-5 degree bars elicit weak T4/T5 responses, whereas a 15 degree spot will nearly fill the center of a T4/T5 receptive field. We have updated the text introducing this experiment (lines 372 &amp; 378) to clarify the logic behind the experiment and more explicitly lay out that this stimulus was designed to recruit different subpopulations of T4/T5, and that we have some idea of what stimuli should drive T4/T5 from previous work.</p><disp-quote content-type="editor-comment"><p>3. The fact that visual stimuli can be decoded from the population in the presence of modulation by movement signals is quite similar to the findings of Stringer et al 2021 and Rumyantsev et al 2020, so it might be worth noting these.</p></disp-quote><p>Thanks - we have added this to the discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1) My first concern is about whether any of the correlated gain modulations the authors observe could be due to motion artifacts or other factors that might vary during imaging but might not reflect actual neural signal intensity variations. This is a particular concern in Figure 4 where such shared variability is first introduced. Ideally, the authors would show imaging of the red channel from the same trials showing that this is not modulated by the shared gain factor. At the very least the authors should mention possible confounds that could give rise to this variability and discuss measures taken to rule these out.</p></disp-quote><p>Thank you for raising this important concern. We have added traces from the red channel for the same trials shown in Figure 4A and added Figure 4 âfigure supplement 2 which shows the covariance matrices for the green and red channels. The covariance in the red signal is across the board weaker than what is seen in the green channel, and its structure across the population looks different than the shared gain revealed by covariance analysis of GCaMP signals, indicating that the apparent shared gain is the result of visually driven responses, not motion correction errors or other imaging factors.</p><disp-quote content-type="editor-comment"><p>2) Although the manuscript is framed in terms of &quot;self-motion,&quot; most of the analysis and experiments focus on fast rotational motion evoked by body saccades. For example, the analysis in Figure 1 deals only with rotational motion, as do the visual suppression experiments in Figures 6 and 7. However, Figure 5 shows suppression driven by walking (not necessarily turning) and it is not clear from the figure if these represent rotations or forward movements. Therefore, it is not clear if the two inputs discussed are &quot;working together&quot; as suggested in the Discussion, or cover different types of input (during forward motion versus turns).</p><p>Although this does not detract from the interest of the work, it is confusing, as self-motion also includes large translational components which are not discussed (much) here and as saccadic suppression of visual signals has been discussed elsewhere. The authors should clarify in the Abstract and Introduction that their focus will be on rotational motion related to body saccades, and should address the differences between these types of motion in the Discussion.</p></disp-quote><p>Thank you for this feedback. We have clarified in the abstract, introduction (line 100), results (lines 115, 355, 372, 390), figure legend titles and subsection headings that our focus is on rotational visual motion specifically. We also discuss this choice and the differences between rotational and translational motion in the discussion (line 590). Finally, we have also modified the text to reflect the fact that forward and rotational components of self motion cannot be completely dissociated in natural walking or in our fictive walking data (see response to major revision (2) above).</p><disp-quote content-type="editor-comment"><p>3) The authors perform a decoding analysis of stimulus identity to argue that stimulus identity is encoded at the level of population activity and that positive correlations enhance stimulus decoding. This seems strange to me because classical studies of correlations in visual encoding (e.g. Shadlen Newsome) emphasized the way that correlated variability reduces the encoding capacity of a network. The emphasis on encoding stimulus identity within the particular stimulus set presented also seemed strange to me because it is not clear that the fly needs to discriminate between each of these stimuli in order to make appropriate behavioral responses. For example, flies are known to respond differently to vertical stripes versus short spots, however, it is not clear if they care about the difference between spots of slightly different sizes, or between spots moving on a gray versus grating background. Presumably, psychophysics experiments combined with connectomics can help determine which combinations of glomerular responses are actually used by the fly to shape its behavior. At any rate, I find that the conclusions about how VPNs encode visual features (e.g. Discussion line 439) rest on an assumption about what the fly is trying to do with these stimuli that may not be accurate.</p></disp-quote><p>We apologize for poorly explaining the effect of positive noise correlations in heterogeneous populations. See response to essential revision (5) above. Of particular note, we highlight that the information limiting correlations explored by Zohary, Shadlen and Newsom are in the case of a homogeneously tuned population, and as you say positive correlations in such a population limit the accuracy with which the population average response can be estimated, thereby interfering with stimulus decoding. Our decoding model, however, can compare activations across differently tuned populations, thereby taking advantage of the fact that in a heterogeneously tuned population, correlations can shape noise in response space so as to not limit the information available about the stimulus. We have improved the explanation for this result and its relation to currently existing theoretical work on this topic (see lines 273 and 539).</p><p>Regarding the second point, we have added a new Figure 4 âfigure supplement 3 showing decoding for a reduced subset of stimuli that are known to be discriminable from past behavioral work. We agree that we should have acknowledged how assessing the information available within a neural population is only part of the story, and that understanding how this information is used to guide behavior is critical. We have updated the text to reflect this fact (line 524, discussion).</p></body></sub-article></article>