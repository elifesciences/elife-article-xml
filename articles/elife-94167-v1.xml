<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">94167</article-id><article-id pub-id-type="doi">10.7554/eLife.94167</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94167.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Pan-cortical 2-photon mesoscopic imaging and neurobehavioral alignment in awake, behaving mice</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-342752"><name><surname>Vickers</surname><given-names>Evan D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7053-4740</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-138965"><name><surname>McCormick</surname><given-names>David A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9803-8335</contrib-id><email>davidmc@uoregon.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0293rh119</institution-id><institution>Institute of Neuroscience, University of Oregon</institution></institution-wrap><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0293rh119</institution-id><institution>Department of Biology, University of Oregon</institution></institution-wrap><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Grunwald Kadow</surname><given-names>Ilona C</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01xnwqx93</institution-id><institution>University of Bonn</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Huguenard</surname><given-names>John R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University School of Medicine</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>29</day><month>05</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP94167</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-06"><day>06</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-10-24"><day>24</day><month>10</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.19.563159"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-23"><day>23</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94167.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-28"><day>28</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94167.2"/></event></pub-history><permissions><copyright-statement>© 2024, Vickers and McCormick</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Vickers and McCormick</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-94167-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-94167-figures-v1.pdf"/><abstract><p>The flow of neural activity across the neocortex during active sensory discrimination is constrained by task-specific cognitive demands, movements, and internal states. During behavior, the brain appears to sample from a broad repertoire of activation motifs. Understanding how these patterns of local and global activity are selected in relation to both spontaneous and task-dependent behavior requires in-depth study of densely sampled activity at single neuron resolution across large regions of cortex. In a significant advance toward this goal, we developed procedures to record mesoscale 2-photon Ca<sup>2+</sup> imaging data from two novel <italic>in vivo</italic> preparations that, between them, allow for simultaneous access to nearly all 0f the mouse dorsal and lateral neocortex. As a proof of principle, we aligned neural activity with both behavioral primitives and high-level motifs to reveal the existence of large populations of neurons that coordinated their activity across cortical areas with spontaneous changes in movement and/or arousal. The methods we detail here facilitate the identification and exploration of widespread, spatially heterogeneous neural ensembles whose activity is related to diverse aspects of behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>2-photon</kwd><kwd>mesoscope</kwd><kwd>cortex</kwd><kwd><italic>in vivo</italic></kwd><kwd>behavior</kwd><kwd>calcium imaging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R35NS097287</award-id><principal-award-recipient><name><surname>McCormick</surname><given-names>David A</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS118461</award-id><principal-award-recipient><name><surname>McCormick</surname><given-names>David A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Procedures were developed to perform mesoscale 2-photon Ca<sup>2+</sup> imaging simultaneously from all of mouse dorsolateral neocortex, facilitating identification of widespread neural ensembles with activity related to diverse aspects of behavior.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Recent advances in large-scale neural recording technology, such as widefield imaging (<xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Gallero-Salas et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Esmaeili et al., 2021</xref>), large field-of-view (FOV) 2-photon (2p) imaging (<xref ref-type="bibr" rid="bib47">Sofroniew et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Yu et al., 2022</xref>), and Neuropixels high-density extracellular electrophysiology recordings (<xref ref-type="bibr" rid="bib28">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Steinmetz et al., 2019</xref>) have allowed for rapid advancement in our understanding of the relationships between brain-wide neural activity and both spontaneous and task-engaged behavior in mice. For example, these techniques can now be deployed in recently developed behavioral paradigms that allow for the temporal separation of periods during which cortical activity is dominated by activity related to stimulus representation, choice/decision, maintenance of choice, and response or implementation of choice during different intra-trial epochs of 2-alternative forced choice (2-AFC) discrimination tasks (<xref ref-type="bibr" rid="bib20">Guo et al., 2014</xref>), the standardization of training and performance analysis across laboratories (<xref ref-type="bibr" rid="bib1">Aguillon-Rodriguez et al., 2021</xref>; eLife), and the separation of context-dependent rule representation and choice in a working memory task (<xref ref-type="bibr" rid="bib60">Wu et al., 2020</xref>).</p><p>One of the striking features of neocortical neuronal activity is how strongly changes in behavioral state, such as task engagement, movement, or arousal, affect the spontaneous and evoked activity of neurons within visual, auditory, somatosensory, motor and other cortical regions (<xref ref-type="bibr" rid="bib39">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib34">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>). However, this is not to say that these effects are uniform across the cerebral cortex (<xref ref-type="bibr" rid="bib37">Morandell et al., 2023</xref>; <xref ref-type="bibr" rid="bib57">Wang et al., 2023</xref>). Neurons exhibit diversity in the dependence of their neural activity on arousal and behavioral state both between and within cortical areas (<xref ref-type="bibr" rid="bib39">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib34">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Shimaoka et al., 2018</xref>), and these areas are active at different times during a rewarded task (<xref ref-type="bibr" rid="bib45">Salkoff et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Esmaeili et al., 2021</xref>; <xref ref-type="bibr" rid="bib15">Gallero-Salas et al., 2021</xref>). Furthermore, the arousal dependence of membrane potential across cortical areas has been shown to be diverse and predictable by a temporally filtered readout of pupil diameter and walking speed (<xref ref-type="bibr" rid="bib46">Shimaoka et al., 2018</xref>).</p><p>For this reason, directly combining and/or comparing the correlations between behavior and neural activity across regions imaged in separate sessions may not reveal the true differences in the relationship between behavior and neural activity across cortical areas, due to a ‘temporal filtering effect’. In other words, the correlations between behavior and neural activity in each region appear to depend on the exact time since the behavior began (<xref ref-type="bibr" rid="bib46">Shimaoka et al., 2018</xref>). In our view, this makes the simultaneous recording of multiple cortical areas essential for proper comparison of the dependencies of their neural activities on arousal/movement, because only then are the distributions of behavioral state dwell times the same across cortical areas.</p><p>Areas involved in sensory decision making are often far from each other (<xref ref-type="bibr" rid="bib15">Gallero-Salas et al., 2021</xref>) and can exhibit coordinated state-dependent changes in functional coupling (<xref ref-type="bibr" rid="bib8">Clancy et al., 2019</xref>). Also, multimodal sensory information is multiplexed and combined as it ascends across the cortical hierarchy (<xref ref-type="bibr" rid="bib9">Coen et al., 2023</xref>). For these reasons, understanding the brain activity underlying optimal performance during multimodal, task-engaged behavior will require dense sampling of many brain areas at single neuron resolution across lateral, dorsal, and frontal cortices simultaneously at a temporal resolution high enough to describe both spontaneous behavioral state transitions and the neural dynamics relevant for a given task.</p><p>Dense intra-cortical sampling at a fixed depth/cortical layer across many areas is not possible with current Neuropixels probes (<xref ref-type="bibr" rid="bib49">Steinmetz et al., 2021</xref>), 1-photon (1p) widefield imaging can be contaminated by neuropil and hemodynamic signal (<xref ref-type="bibr" rid="bib58">Waters, 2020</xref>; <xref ref-type="bibr" rid="bib53">Valley et al., 2020</xref>) and typically does not achieve single cell resolution (but see <xref ref-type="bibr" rid="bib63">Yoshida et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Kauvar et al., 2020</xref>), and standard 2p imaging is limited by scanning speed (see <xref ref-type="bibr" rid="bib19">Gong et al., 2015</xref>) and field of view spatial extent (FOV; i.e. simultaneously imageable, either contiguous or non-contiguous, regions; <xref ref-type="bibr" rid="bib2">Allen et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Hattori et al., 2019</xref>; but see <xref ref-type="bibr" rid="bib64">Yu et al., 2021</xref>). Note that although some recent advances in 1p widefield imaging have allowed for the imaging of individual cells, both in head-fixed and freely moving mice, they do not achieve true single cell resolution in practice (they get close to ~10 μm xy or ‘lateral’ resolution, with undefined z or ‘depth’ resolution) and rely in part on the sparse labeling of neurons with Ca<sup>2+</sup> indicators either within or across cortical layers (<xref ref-type="bibr" rid="bib6">Cai et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Hope et al., 2023</xref>, bioRxiv; <xref ref-type="bibr" rid="bib61">Xie et al., 2023</xref>).</p><p>Recent advancements in cranial window preparations have enabled imaging over large portions of the dorsal cortex (<xref ref-type="bibr" rid="bib30">Kim et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). However, simultaneous 2p imaging over a large cortical area including both dorsal and lateral regions, particularly in a preparation that allows for simultaneous imaging of the major primary sensory cortices (auditory, visual, and somatosensory) and frontal motor/choice areas (M1, M2) in awake, behaving mice has not been previously shown.</p><p>Here, we developed a set of new techniques and integrated them with existing technologies in order to overcome the limitations of current state-of-the-art methods and provide the first pan-cortical 2p assays at single-cell resolution in awake, behaving mice. To achieve this, we designed custom 3D-printed titanium headposts, mounting devices, adapters, and cranial windows, and modified (‘Crystal Skull’; <xref ref-type="bibr" rid="bib30">Kim et al., 2016</xref>; <xref ref-type="fig" rid="fig1">Figure 1a, d</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c, upper, e</xref>) or developed (‘A1/V1/M2’ or ‘temporo-parietal’; <xref ref-type="fig" rid="fig1">Figure 1b, e</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a, f</xref>) two <italic>in vivo</italic> surgical preparations, which we will henceforth refer to as the ‘dorsal mount’ and ‘side mount’, respectively.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Technical adaptations for dual-mount <italic>in vivo</italic> pan-cortical imaging with the Thorlabs 2p-RAM mesoscope.</title><p>(<bold>a</bold>) Mesoscope behavioral apparatus for the dorsal mount preparation. Mouse is mounted upright on a running wheel with headpost fixed to dual adjustable support arms mounted on vertical posts (1” diameter). Behavior cameras are fixed at front-left, front-right, and center-posterior, with ultraviolet and infrared light-emitting diodes aligned with goose-neck supports parallel to right and left cameras. (<bold>b</bold>) Mesoscope behavioral apparatus for side mount preparation. Same as in (<bold>a</bold>), except that the mouse is rotated 22.5 degrees to its left so that the objective lens (at angle 0) is orthogonal to the center-of-mass of the preparation across the right cortical hemisphere. The objective can rotate +/-20 degrees medio-laterally, if needed, to optimize imaging of any portion of the cortex under the cranial window. The right behavior camera is positioned more posterior and lower than in (<bold>a</bold>), to allow for imaging of the eye under the acute angle formed with the horizontal light shield, shown in (<bold>c</bold>). (<bold>c</bold>) Mouse running on wheel with side mount preparation receiving visual stimulation from an LED screen positioned on the left side, with linear motor-positioned dual lick-spouts in place and 3D printed vertical light shield (wok 2) attached to rim of flat shield (wok 1) to block extraneous light from entering the objective. (<bold>d</bold>) Overhead view of dorsal mount preparation with 3D printed titanium headpost, custom cranial window, and Allen CCF aligned to paraformaldehyde-fixed mouse brain. Motor region = light green, somatosensory = dark green, visual = dark blue, retrosplenial = light blue. Olfactory bulbs (anterior) at top, cerebellum (posterior) at bottom of image. Note ridge along perimeter of headpost for fitted horizontal light shield (wok 1) attachment. (<bold>e</bold>) Rotated dorsal view (22.5 degrees right) of side mount preparation with 3D printed titanium headpost, custom cranial window, and Allen CCF aligned to paraformaldehyde-fixed mouse brain. Auditory region shown in light blue, ventral and anterior to visual areas, and ventral and posterior to somatosensory areas (right side of image is lateral/ventral, and left side of headpost perimeter is medial/dorsal). Other regions shown with the same color scheme as in (<bold>d</bold>). UV = ultraviolet, IR = infrared, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex, A1 = primary auditory cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Detailed technical adaptations for dual-mount <italic>in vivo</italic> pan-cortical imaging with 2p-RAM mesoscope.</title><p>(<bold>a</bold>) Photographs of widefield through-skull (top) and 2-photon cranial window (bottom) states of side mount preparation. Bregma (β) and lambda (λ) skull landmarks used in multimodal mapping alignment to common coordinate framework (CCF) map are indicated (top), along with their corresponding positions relative to cortical blood vessels visible through cranial window (bottom). Note that all visible tissue inside the perimeter of the headpost in the lower image is seen through curved, 210-µm-thick borosilicate Schott D263 glass. (<bold>b</bold>) Custom aluminum support arms for use with both dorsal mount and side mount preparations. Fixed arm (top) has flat proximal angle at bend near dual-screw headpost mounting slot (right red arrow), and can be mounted on a Thorlabs 1-inch diameter post mounted to a rotating ball-joint base (SL20, see Supplementary methods and materials) to achieve leftward 22.5 degree rotation for side mount (not shown). Flexible positioning on wheel is possible due to ¼”–20 bolt accepting slot that allows support posts to be positioned closer to or farther from headpost mounting slot (left red arrow). Custom adjustable arm (bottom) has rotating brass shaft attached to ball-and-socket joint with hexagonal tightening ring (top, long black arrow), stabilized by dual set screws on both ends (bottom, two short black arrows), at a bend proximal to the headpost mounting slot. This allows for rapid, fixable on-rig fine adjustment of the mounting angle in all directions, and eliminates the need for a Thorlabs rotating ball-joint base so that 1” diameter vertical posts can be used (instead of ½” diameter, which we used at first), thus enhancing overall stability for imaging. (<bold>c</bold>) Examples of headpost mounting for dorsal mount (top, with cranial window) and side mount (bottom, headpost only). Both headposts are attached with two 82 degree countersunk 2–56 thread 1 ⁄ 4 '' stainless steel screws (McMaster-Carr, see Supplementary methods and materials) and positioned in a custom machined slot within the support arm for enhanced stability. The headpost arm can easily be held within the support arm slot by thumb prior to screw affixation during mounting. (<bold>d</bold>) Mounting of 3D printed light shields and application of objective to water meniscus. Flat shields (top left, bottom left) are attached with 1:1 rapid curing Sylgard (top right; 170 Fast Cure encapsulant, see Supplementary methods and materials) to the perimeter of the headpost. Stable positioning during curing is enabled by attachment of dual 3-pronged lab clamps attached to left and right support arms with quick ties (not shown). Then, a tip-clipped transfer pipette is used to add deionized or Millipure water to the window, with the Sylgard acting as a dam, until the objective can be lowered far enough to raise a meniscus. Additional water can then be added from either side of the preparation under the objective. (<bold>e</bold>) Side view 3D rendering (left, from AutoDesk Inventor; L = left, R = right, A = anterior, P = posterior, center-line = dashed line) and top view photograph with cranial window placed on paraformaldehyde-fixed brain, for dorsal mount preparation. (<bold>f</bold>) Same as in (<bold>e</bold>), but for side mount preparation. (<bold>g</bold>) Cumulative success rate ‘survivorship’ diagram for dorsal and side mount preparations (note: survival rate is higher, but some preparations are not fully imageable, some mice fail to acquire the task, etc), loosely based on empirical data from mice both used to develop the preparations, those used in this study, and those used in pilot experiments for 2-alternative forced choice (2-AFC) behavior. Note that the standard full timeline involves around 100 days from headpost implantation to experiment completion. In our experience, successful cranial windows last between 100 and 150 days, or in some rare cases up to 300 days.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Widefield-imaging based dual-mount multimodal CCF alignment, ROI detection, and areal assignment of GCaMP6s<sup>+</sup> excitatory neurons.</title><p>(<bold>a</bold>) Example MMM to CCF alignment for dorsal mount, shown projected onto a through-the-skull vasculature image and including skull landmarks (top). The series of user-selected points used for alignment of the widefield image onto the CCF map are shown as blue points, with corresponding red points showing the actual mapping locations used by the affine transformation. 2p-RAM pseudo-widefield vasculature montage from the same mouse, with boundaries of an example 2-photon imaging session FOV shown as a red dashed line (bottom). View angle relative to orthogonal indicated at top of dashed black line indicating mid-line. (<bold>b</bold>) Same as in (<bold>a</bold>), but for example side mount preparation. (<bold>c</bold>) Density of Suite2p-detected ROIs (density of 1 equal to one neuron per 25 x 25 µm square; total large FOV area generally 5000 x 4620 µm) for large field of view (FOV) 2-photon imaging sessions as a function of session duration for dorsal mount (blue) and side mount (red) preparations. (<bold>d</bold>) Neural density as a function of imaging resolution for 2-photon imaging sessions where multiple smaller FOVs were imaged pseudo-simultaneously. Same density calculation used as in (<bold>c</bold>), for multi-FOV sessions where the dimensions of each small FOV are generally between 660 x 660 and 1320 x 1320 µm. Here, a resolution of 0.2 pixels / µm would correspond to 5 µm per pixel resolution in both x and y dimensions (i.e. ⅕=0.2). (<bold>e</bold>) Summary statistics for all 2p imaging sessions in the current study that passed quality control for all data modalities (i.e. MMM, 2p, behavior video, etc). The number of neurons per 2p imaging session is shown for both large- (two columns at left) and small- (two columns at right) FOV sessions, split by mount type (dorsal mount in blue, side mount in red). Numbers in parentheses indicate total session counts (“S”), “N” indicates number of mice with each mount type. FOV = field of view, V1 = primary visual cortex, A1 = primary auditory cortex, S1_b = primary somatosensory barrel cortex, ß = bregma, λ = lambda.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig1-figsupp2-v1.tif"/></fig></fig-group><p>The dorsal mount, which was based on the earlier ‘Crystal Skull’ preparation (<xref ref-type="bibr" rid="bib30">Kim et al., 2016</xref>; see also <xref ref-type="bibr" rid="bib18">Ghanbari et al., 2019</xref> for a similar preparation), included modifications such as a novel headpost and support arms, along with other hardware and novel surgical, data acquisition, and analysis methods, and enabled simultaneous imaging across nearly all of bilateral dorsal cortex. Our novel side mount preparation, on the other hand, allowed for simultaneous imaging of much of the dorsal and lateral cortex across the right hemisphere (although our design can easily be ‘flipped’ across a mirror-image plane to allow for imaging of the left hemisphere, if desired).</p><p>These two novel preparations enabled mesoscale 2p imaging (here, we used the 2p-RAM mesoscope, Thorlabs; <xref ref-type="bibr" rid="bib47">Sofroniew et al., 2016</xref>) of up to ~7,500 individual neurons simultaneously at ~3 Hz across a 5x5 mm FOV (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>; <xref ref-type="video" rid="video1 video2">Videos 1, 2</xref>, 8 and 9; Protocol 1), or up to 800 neurons combined across four 660x660 µm FOVs at ~10 Hz (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2d, e</xref>; Protocol 1; see Supplementary methods and materials). Although these recording speeds are not fast enough to capture the full dynamics of rapid neural processing involved in, for example, initial cortical sensory encoding and decision making, they are faster than earlier similar mesoscale recordings in visual cortex (<xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>), and they are likely fast enough to capture important components of spontaneous arousal and movement related fluctuations in neural dynamics across dorsal and lateral cortex. The total number of imaged neurons can be significantly increased in our preparations by using other mouse lines or different imaging or analysis parameters.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Widefield-imaging-based multimodal mapping (MMM) and CCF alignment, ROI detection, and areal assignment of GCaMP6s<sup>+</sup> excitatory neurons for the dorsal mount preparation.</title><p>(<bold>a</bold>) Mean projection of 30 s, 470 nm excitation epifluorescence widefield movie of vasculature performed through a dorsal mount cranial window. Midline indicated by a dashed white line. (<bold>b</bold>) Mean 1 s baseline subtracted dF/F images of 1 s responses to ~20–30 repetitions of left-side presentations of a 5–40 kHz auditory tone cloud (auditory TC; left), visual full field (FF) isoluminant Gaussian noise stimulation (center), and 100 ms x 5 Hz burst whisker deflection (right) in an awake dorsal mount preparation mouse. (<bold>c</bold>) Example two-step CCF alignment to dorsal mount preparation, performed in Python on MMM masked blood-vessel image (upper left), rotated outline CCF (upper right), and Suite2p region of interest (ROI) output image containing exact position of all 2p imaged neurons (lower right). Yellow outlines show the area of masks created from the thresholded mean dF/F image for, in this example, repeated full-field visual stimulation. In step 1 (top row), the CCF is transformed and aligned to the MMM image using a series of user-selected points (blue points with red numbered labels, set to matching locations on both left and right images) defining a bounding-box and known anatomical and functional locations. In step 2 (bottom row), the same process is applied to transformation and alignment of Suite2p ROIs onto the MMM with user-selected points defining a bounding box and corresponding to unique, identifiable blood-vessel positions and/or intersections. Finally, a unique CCF area name and number are assigned to each Suite2p ROI (i.e. neuron) by applying the double reverse-transformation from Suite2p cell-center location coordinates, to MMM, to CCF. (<bold>d</bold>) CCF-aligned Suite2p ROIs from an example dorsal mount preparation with 7328 neurons identified in a single 30-min session from a spontaneously behaving mouse. TC = tone cloud, FF = full-field, defl = deflection, dF/F = change in fluorescence over baseline fluorescence, CCF = Allen common coordinate framework version 3.0, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex, A = anterior, P = posterior, R = right, L = left.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig2-v1.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Widefield-imaging-based multimodal mapping (MMM) and CCF alignment, ROI detection, and areal assignment of GCaMP6s<sup>+</sup> excitatory neurons for the side mount preparation.</title><p>(<bold>a</bold>) Mean projection of 30 s, 470 nm excitation fluorescence widefield movie of vasculature performed through a side mount cranial window. Midline indicated by a dashed white line. (<bold>b</bold>) Mean 1 s baseline subtracted dF/F images of 1 s responses to ~20–30 repetitions of left-side presentations of a 5–40 kHz auditory tone cloud (auditory TC; left), visual full field (FF) isoluminant Gaussian noise stimulation (center), and 100 ms x 5 Hz burst whisker deflection (right) in an example side mount preparation mouse under 1.5% isoflurane anesthesia. (<bold>c</bold>) Example two-step CCF alignment to side mount preparation, performed in Python on MMM masked blood-vessel image (upper left), rotated outline CCF (upper right), and Suite2p region of interest (ROI) output image containing exact position of all 2p imaged neurons (bottom right). Yellow outlines show the area of masks from thresholded mean dF/F image for repeated auditory, full-field visual, and/or whisker stimulation. In step 1 (top row), the CCF is transformed and aligned to the MMM image using a series of user-selected points (blue points with red numbered labels, set to matching locations on both left and right images) defining a bounding-box and known anatomical and functional locations. In step 2 (bottom row), the same process is applied to transformation and alignment of Suite2p ROIs onto the MMM with user-selected points defining a bounding box and corresponding to unique, identifiable blood-vessel positions and/or intersections. Finally, a unique CCF area name and number are assigned to each Suite2p ROI (i.e. neuron) by applying the double reverse-transformation from Suite2p cell-center location coordinates, to MMM, to CCF. (<bold>d</bold>) CCF-aligned Suite2p ROIs from an example side mount preparation with 4782 neurons identified in a single 70 min session from a mouse performing our 2-alternative forced choice (2-AFC) auditory discrimination task. TC = tone cloud, FF = full-field, defl = deflection, dF/F = change in fluorescence over baseline fluorescence, CCF = Allen common coordinate framework version 3.0, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, A1 = primary auditory cortex, V1 = primary visual cortex, A = anterior, P = posterior, R = right, L = left.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig3-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Dorsal mount.</title><p>Upper left, 3D printed titanium headpost for dorsal mount (left; i.materialise.com and sculpteo.com) and accompanying cranial window for dorsal mount (labmaker.org, or TLCInternational.com and glaswerk.com), shown in top and side orthogonal projection, followed by isometric projection (AutoDesk Inventor, Adobe Acrobat Pro 3D viewer). Upper right, 3D printed plastic (PLA) light-shields or ‘woks’, same views as upper left. Horizontal light shield (left) fits onto perimeter of dorsal mount headpost and is attached with Sylgard 170 Fast Cure silicone elastomer, and vertical light shield fits onto vertical perimeter ridge of horizontal light shield and is held by gravity. Bottom, simultaneous and temporally aligned high-resolution videography from three points-of-view of a mouse under spontaneously behaving conditions (shown at 3x speed or 90 Hz; left and right camera are GigE Teledyne Dalsa M2050 cameras, and posterior camera is FLIR grasshopper USB3 a camera). Example pose tracking labeling by DeepLabCut (<xref ref-type="bibr" rid="bib33">Mathis et al., 2018</xref>) is shown (right camera).</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Side mount: same as in dorsal mount (<xref ref-type="video" rid="video1">Video 1</xref>), but for side mount hardware.</title><p>Note that the mouse’s headpost is retained by a single fixed support arm, rotated 22.5 degrees to the left, whereas the mouse in the dorsal mount example is held by dual orthogonally positioned fixed support arms.</p></caption></media><p>Finally, we designed a custom, LabView-controlled behavioral setup with up to 3 high-speed body and face cameras (<xref ref-type="fig" rid="fig1">Figure 1a, b</xref>; <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>). This behavioral monitoring allowed us to compare widespread, densely sampled, high-resolution neural activity to movement and behavioral arousal state variation in head-fixed, awake, ambulatory (i.e. running atop a cylindrical wheel) mice under conditions of spontaneous behavior (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref> and <xref ref-type="fig" rid="fig4s2">2</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), passive sensory stimulation (<xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), or 2-alternative forced choice (2-AFC) task engagement (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, Protocol 1).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The relationship between spontaneous behavioral measures and neural activity across the dorsal cortex is globally heterogeneous.</title><p>(<bold>a</bold>) Rastermap (top), first principal component (PC1; middle), and second principal component (PC2, bottom) sorting of normalized, rasterized, neuropil subtracted dF/F neural activity for 3096 cells from a single 20-min duration example dorsal mount 2p imaging session. Each row in the display corresponds to a ‘superneuron’, or average activity of 50 adjacent neurons in the Rastermap sort (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). Activity for each superneuron is separately z-scored, then displayed as low activity in blue, intermediate activity in green, and maximum activity level in yellow (standard viridis color look-up table). Red and blue arrowheads show alignment of walking and whisking bouts, respectively, to neural activity. White dashed boxes indicate dual selections highlighted in panels (<bold>c</bold>) and (<bold>d</bold>). Scale bar shows color look-up map for each separately z-scored, individually displayed superneuron in the raster displays. (<bold>b</bold>) Behavioral arousal primitives of walk speed, whisker motion energy, and pupil diameter shown temporally aligned to the rasterized neural activity traces in (<bold>a</bold>), directly above. Horizontal black bar indicates time of expanded inset in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. (<bold>c</bold>) Expanded insets of top and bottom fifths of rasterized PC1 sorting from the middle segment of panel (<bold>a</bold>), with mean dF/F activity traces shown above each. Red and blue arrowheads indicate the same walking and whisking bouts, respectively, as in (<bold>a</bold>) and (<bold>b</bold>). (<bold>d</bold>) Normalized density of neurons in each CCF area belonging to two example Rastermap sorted groups (d1, left, red: MIN = 0%, MAX = 25%; d2, right, blue: MIN = 0%, MAX = 13%), with rasterized activities shown in corresponding labeled white dashed boxes in the top segment of panel (<bold>a</bold>). The neurons present in selected Rastermap groups are shown as Suite2p footprint outlines. The type of behavioral arousal primitive activity typically concurrently active with high neural activity for each Rastermap group is indicated below each Rastermap group’s CCF density map (i.e. walk and whisk for group d1, left (red), and whisk for group d2, right (blue)). (<bold>e</bold>) Normalized mean correlations of neural activity and walk speed (left, red; mean: MIN = 0.01, MAX = 0.03) and whisker motion energy (right, blue; mean: MIN = 0.02, MAX = 0.08) per CCF area (i.e. average correlation of all neurons in each area) are shown for this example session. Mean walk speed correlations with neural activity (dF/F) were significantly greater than zero (p&lt;0.001, median t(3095)=3.7, single-sample t-test; python: scipy.stats.ttest_1samp) for 15 of the 19 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were right VISam, right SSp_ll, left VIS_rl, and left AUDd. Mean whisker motion energy correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(3095)=4.4, single-sample t-test) for 16 of the 19 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were right VISam, left VISam, and left VISrl. PC = principal component, ME = motion energy, au = arbitrary units, M2=secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comparison of neurobehavioral alignments with and without online z-motion correction.</title><p>Comparison of neurobehavioral alignments in back-to-back mesoscope imaging sessions either without (<bold>a</bold>, ‘OFF’) and with (<bold>b</bold>, ‘ON’) GPU-enabled online z-motion correction. (<bold>a</bold>) Neural and behavioral data are shown temporally aligned for the first session. Rastermap sorted neural activity is shown at top, with a blue/green/yellow (min/mid/max) heatmap applied to activity from each individual neuron z-scored separately. Walk speed (red), whisker motion energy (blue), and pupil diameter (gray) are shown below. Red dashed vertical lines indicate times of two major walking bouts for the purpose of alignment visualization. Note that roughly the top ~30% of neurons show strong activity time-locked to the identified walking bouts (red vertical line at right, labeled ‘walk’). (<bold>b</bold>) Same as in (<bold>a</bold>), except for a 2nd consecutive session in the same mouse. The only change was that, for this session, online GPU-enabled z-motion correction was used (“ON”) following acquisition of a 60 x 1 µm step anatomical z-stack for frame-by-frame comparison and adjustment (ScanImage/Vidrio). Note the similar percentage of cells aligned with each walking bout in the top ~30% of superneurons in Rastermap sorts in both (<bold>a</bold>) and (<bold>b</bold>) (indicated by red vertical line at right labeled ‘walk’), and the overall qualitative similarity of Rastermap clustering and activity motifs compared to that of the immediately prior session without z-motion correction shown in (<bold>a</bold>), despite the fact that these back-to-back sessions contained different detailed patterns of spontaneous activity. ME = motion energy, au = arbitrary units, GPU = graphics processing unit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Alignment of neural activity sorted Rastermap groups and spontaneous behavior in an example dorsal mount session.</title><p>Expanded insets of <xref ref-type="fig" rid="fig4">Figure 4</xref> (d1), corresponding to the walk and whisk Rastermap group (<bold>a</bold>, white dashed box) and <xref ref-type="fig" rid="fig4">Figure 4</xref> (d2), corresponding to the whisk Rastermap group (<bold>c</bold>, white dashed box), shown aligned to walk speed and whisker motion energy (<bold>b</bold>, top, red, and bottom, blue, respectively). A blue/green/yellow (low/mid/high) heat map was applied to individually z-scored neural activity (<bold>a</bold>), as indicated by the scale bar (bottom right). Neural data is shown vertically temporally aligned to behavioral arousal and movement data (<bold>b</bold>). Walk speed (red) and whisker motion energy (blue) are shown for this example dorsal mount session. Red and blue arrowheads indicate temporal alignment of walk and whisk bouts, respectively, between neural and behavioral data. ME = motion energy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig4-figsupp2-v1.tif"/></fig></fig-group><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The relationship between spontaneous behavioral measures and neural activity across the lateral cortex is regionally patterned.</title><p>(<bold>a</bold>) Rastermap (top), first principal component (PC1; middle), and second principal component (PC2, bottom) sortings of normalized, rasterized, neuropil subtracted dF/F neural activity for 5678 cells from a single 90 min duration example side mount 2p imaging session. Each row in the display corresponds to a ‘superneuron’, or average of 50 adjacent neurons in the Rastermap sort (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). Low activity (z-scored) in blue, intermediate activity in green, maximum activity level in yellow. Red and blue arrowheads show alignment of walking and whisking bouts, respectively, to neural activity. White dashed boxes indicate selections highlighted in panels (<bold>c</bold>) and (<bold>d</bold>). Scale bar shows color look-up map for each separately z-scored, individually displayed superneuron in the raster displays. (<bold>b</bold>) Behavioral arousal primitives of walk speed, whisker motion energy, and pupil diameter shown temporally aligned to the rasterized neural activity traces in (<bold>a</bold>), directly above. (<bold>c</bold>) Expanded insets of top and bottom fifths of rasterized PC1 sorting from the middle segment of panel (<bold>a</bold>), with mean activity traces shown above each. Red and blue arrowheads indicate the same walking and whisking bouts, respectively, as in (<bold>a</bold>) and (<bold>b</bold>). Horizontal black bar indicates time of expanded inset shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. (<bold>d</bold>) Normalized density of neurons in each CCF area belonging to two example Rastermap sorted groups (d1, left, red: MIN = 0%, MAX = 34%; d2, right, blue: MIN = 0%, MAX = 32%), with rasterized activities shown in corresponding labeled white dashed boxes in the top segment of panel (<bold>a</bold>). Only cells in selected Rastermap groups are shown. The type of behavioral arousal primitive (i.e. walk and whisk, left, in red; whisk, right, in blue) that was typically concurrently active with high neural activity is indicated below each Rastermap group’s CCF density map. (<bold>e</bold>) Normalized mean correlations of neural activity and walk speed (left, red; mean: MIN = 0.01, MAX = 0.06; standard deviation: MIN = 0.000, MAX = 0.029) and whisker motion energy (right, blue; mean: MIN = 0.01, MAX = 0.08; standard deviation: MIN = 0.000, MAX = 0.043) per CCF area for this example session. Mean walk speed correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(5677)=4.4, single-sample t-test; python: scipy.stats.ttest_1samp) for 20 of the 24 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were left VISp, right SSpn, right AUDpo, and right TEa. Mean whisker motion energy correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.001, median t(5677)=7.0, single-sample t-test) for 21 of the 24 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were right SSpn, right AUDpo, and right TEa. PC = principal component, ME = motion energy, au = arbitrary units, M2 = secondary motor cortex, S1b = primary somatosensory barrel cortex, V1 = primary visual cortex, A1 = primary auditory cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Alignment of neural activity sorted Rastermap groups and spontaneous behavior in example side mount session.</title><p>Expanded insets of <xref ref-type="fig" rid="fig5">Figure 5</xref> (d1; white dashed box), corresponding to the walk and whisk Rastermap group (<bold>a</bold>), and <xref ref-type="fig" rid="fig5">Figure 5</xref> (d2; white dashed box), corresponding to the whisk Rastermap group (<bold>c</bold>), shown aligned to walk speed and whisker motion energy (<bold>b</bold>, top, red, and bottom, blue, respectively). A blue/green/yellow (low/mid/high) heat map was applied to individually z-scored neural activity (<bold>a</bold>), as indicated by the scale bar (bottom right). Neural data is shown temporally aligned to behavioral arousal and movement data (<bold>b</bold>). Walk speed (red) and whisker motion energy (blue) are shown for this example side mount session. Red and blue arrowheads indicate temporal alignment of walk and whisk bouts between neural and behavioral data. ME = motion energy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Heterogeneity of arousal and movement correlations with neural activity in the dorsal and side mount preparations.</title><p>(<bold>a</bold>) Color-coded correlations of dF/F with whisker motion energy in individual neurons of the dorsal preparation in a different example session from the same mouse as in <xref ref-type="fig" rid="fig4">Figure 4</xref>, with large positive correlations shown in blue, large negative correlations shown in red, and small correlations shown in white/light-blue/light-red. (<bold>b</bold>) Histograms of whisk ME, pupil diameter, and walk speed (cm/s) for the same example session as in (<bold>a</bold>). (<bold>c</bold>) Normalized mean of mean correlations of neural activity with behavioral arousal primitives of whisker motion energy (blue; mean: MIN = 0.00, MAX = 0.05; standard deviation (STD): MIN = 0.002, MAX = 0.045), pupil diameter (gray; mean: MIN = 0.00, MAX = 0.06; STD: MIN = 0.004, MAX = 0.038), and walk speed (red; mean: MIN = 0.00, MAX = 0.02; STD: MIN = 0.004, MAX = 0.018) for each CCF area across eight sessions from the same dorsal mount mouse shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Mean whisker motion energy correlations with neural activity (dF/F) across these eight sessions from the same mouse were significantly more than zero (p&lt;0.05, median t(7)=3.0, single-sample t-test; python: scipy.stats.ttest_1samp) for 24 of the 27 CCF areas with at least 20 neurons present. The areas with mean correlations not significantly larger than zero were left SSp-m, SSp-n, and SSp-un. Mean pupil diameter correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.05, median t(7)=1.3, single-sample t-test) for 8 of the 27 CCF areas, including left VISp, VISam, VISpm, MOp, and SSp-ll, SSp-ul, and right VISp and SSp-ul. Mean walk speed correlations with neural activity (dF/F) were not significantly more than zero (p&lt;0.05, median t(7)=1.0, single-sample t-test) in any of the 27 CCF areas. (<bold>d</bold>) Color-coded correlations (same lookup table as in a) of dF/F with whisker motion energy in individual neurons of the side mount preparation in an example session from the same mouse as in <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>. (<bold>e</bold>) Histograms of whisk ME, pupil diameter, and walk speed (cm/s) for the same example session as in (<bold>d</bold>). (<bold>f</bold>) Normalized mean of mean correlations of neural activity with behavioral arousal primitives of whisker motion energy (blue; mean: MIN = 0.00, MAX = 0.08; standard deviation (STD): MIN = 0.000, MAX = 0.043), pupil diameter (gray; mean: MIN = 0.00, MAX = 0.02; STD: MIN = 0.000, MAX = 0.034), and walk speed (red; mean: MIN = 0.00, MAX = 0.01; STD: MIN = 0.000, MAX = 0.029) for each CCF area across eight sessions from the same side mount mouse shown in <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>. Mean whisker motion energy correlations with neural activity (dF/F) across these eight sessions from the same mouse were significantly more than zero (p&lt;0.05, median t(7)=5.3, single-sample t-test; python: scipy.stats.ttest_1samp) for 23 of the 23 CCF areas with at least 20 neurons present. Mean pupil diameter correlations with neural activity (dF/F) were significantly more than zero (p&lt;0.05, median t(7)=1.2, single-sample t-test) for the right VISl and right TEa areas only. Mean walk speed correlations with neural activity (dF/F) were not significantly more than zero (p&lt;0.05, median t(7)=1.1, single-sample t-test) in any of the 23 CCF areas.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig5-figsupp2-v1.tif"/></fig></fig-group><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Alignment of activity sorted neural ensembles and spontaneous behavioral motifs reveals sparse, distributed encoding of arousal measures across dorsolateral cortex in the side mount preparation.</title><p>(<bold>a</bold>) Manual identification of high-level, qualitative behaviors (twitch, green dashed box; whisk, blue dashed box; walk, red dashed box; pupil oscillate, gray dashed box), aligned to sets of raw, unfiltered behavioral motifs (B-SOiD) extracted from pose-estimates (DeepLabCut) for a single example session. Numbered B-SOiD motifs (trained on a set of four sessions from the same mouse, motif # indicated by the position of the thick blue line relative to the y-axis at each time point) are color-coded (horizontal bars) across the entire 90-min session. (<bold>b</bold>) High-level behaviors (dashed colored boxes, (i–iv) and BSOiD motifs from a) are vertically temporally aligned to the behavioral primitive movement and arousal measures of walk speed, whisker motion energy, and pupil diameter. Expanded alignments of high-level behaviors ii (whisk) and iii (walk) are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a, b</xref>, Red arrowheads indicate temporal alignment positions for multiple walk bouts across BSOiD motifs (<bold>a</bold>), behavioral arousal primitives (<bold>b</bold>), and Rastermap sorted neural data (<bold>c</bold>). (<bold>c</bold>) Rastermap sorted, z-scored, rasterized, neuropil subtracted dF/F neural activity (10th percentile baseline, rolling 30 s window) from the same side mount session, temporally aligned to the behavioral data directly above. Numbers at left (R0–R7) indicate Rastermap motif numbers, selected manually ‘by-eye’ for this session, and the horizontal dashed white lines on the Rastermap-sorted neural data indicate the separation between neighboring Rastermap motifs. Each row in the display corresponds to a ‘superneuron’, or average of 50 adjacent neurons in the Rastermap sort (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). Colored, dashed boxes indicate alignment of high-level behaviors with neural activity epochs from the same example session. White dashed ovals indicate areas of Rastermap group-aligned active neural ensembles during periods of defined high-level behaviors. The 2p sampling rate for this session was 3.38 Hz. (<bold>d</bold>) Normalized mean activity traces for all neurons in each neural ensemble indicated in (<bold>c</bold>) (white dashed ovals). Each trace also corresponds and is aligned to the indicated Rastermap groups. (<bold>e</bold>) Color-coded neuron densities per CCF area corresponding to Rastermap groups shown in (<bold>c</bold>), shown grouped by qualitative high-level behaviors, as indicated in (<bold>a, b</bold>). Cross-indexing with neurobehavioral alignments (white dashed ovals) in (<bold>c</bold>) allows for visualization of spatial distribution of neurons active during identified high-level behaviors (<bold>a</bold>) consisting of defined patterns of behavioral primitive movement and arousal measures (<bold>b</bold>). Corresponding maximum cell density percentages in each CCF area (white to red scale bar, top right) for each normalized Rastermap (R0–7, excluding R2) heatmap color lookup table are 11.4, 31.8, 22.9, 24.1, 38.4, 33.2, and 24.0%, respectively (left to right: R3, R4, R5, R6, R7, R0, and R1). This list of cell density percentages, therefore, corresponds to that of the CCF area filled with the darkest shade of red in each Rastermap group. White CCF areas in each Rastermap group are areas where no cells of that group were found in this example session, or where the total number of cells was less than 20 and therefore the density estimate was deemed unreliable and not reported. ME = motion energy, au = arbitrary units, CCF = common coordinate framework.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Example expanded alignment of high-level behaviors and primary corresponding BSOiD motifs, arousal measures, and Rastermap sorted neural data from side mount preparation, same example session as in <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>.</title><p>(<bold>a</bold>) Inset (ii) from <xref ref-type="fig" rid="fig6">Figure 6a</xref>, an example of active whisking (“whisk”) behavior. Whisker motion energy (solid blue line, top) is shown aligned to BSOiD motif #2 (one-hot encoded, solid blue square-wave line; up = 1 (ON), down = 0 (OFF), and Rastermap sorted neural activity (z-scored for each neuron; blue = min, green = intermediate, yellow = high) for this example session. Number of neurons = 5,678. (<bold>b</bold>) Same session and display as in a), but for inset (iii) “walk” and BSOiD motif #13 (yellow) from <xref ref-type="fig" rid="fig6">Figure 6a</xref>. White vertical dashed lines are aligned with major walking bouts. ME = motion energy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94167-fig6-figsupp1-v1.tif"/></fig></fig-group><p>We present here detailed methods, along with example recording sessions during spontaneous behavior from both our dorsal and side mount preparations, to demonstrate the feasibility of widespread 2p cortical neuronal imaging simultaneously with behavioral monitoring. For a graphical schematic overview of our methods, please see the following supplementary material: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Recent studies employing large-scale imaging (e.g. widefield 1p and mesoscale 2p) and/or electrophysiology (e.g. Neuropixels) recording technologies have suggested that a significant percentage of the variance of neural activity across neocortex can be accounted for by rapid spontaneous fluctuations in arousal and self-directed movement during both spontaneous behavior and task performance (<xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Jacobs et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Salkoff et al., 2020</xref>; <xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). Given that arousal state- and movement-related activity appears to be a ubiquitous feature of cortical activity across all regions, it would be advantageous to develop methodologies that allow for simultaneous, single neuron resolution, contiguous monitoring of neuronal activity during second-to-second movements and changes in arousal, during both spontaneous and trained behaviors. Here, we harness new imaging and analysis technologies in order to both address this methodological gap and provide a proof of principle test of these methods by examining the relationship of behavioral arousal and movement to detailed spatial patterns of neural activity across the dorsal and lateral neocortex.</p><p>The overall workflow for our mouse preparations, data acquisition, and data analysis can be found in the following supplementary document: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>. The remainder of our supplementary materials are also hosted on the main /mesoscope_spontaneous folder of our GitHub repository, including documented analysis code, design and 3D-component printable files, grayscale versions of all figures, and supplementary figures and movies. Related data for example recording sessions shown in main and supplementary figures are publicly available on FigShare+ at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link>.</p><sec id="s2-1"><title>Large field of view 2-photon imaging in behaving mice</title><p>In order to simultaneously monitor the activity of cortical neurons across a large area (~25 mm<sup>2</sup>, or up to 36 mm<sup>2</sup> in some cases) of either the bilateral dorsal cortex (dorsal mount), or of both dorsal and lateral cortex across the right hemisphere (side mount) in awake, behaving mice, we chose to utilize 2p imaging of GCaMP6s fluorescence using an existing and commercially available 2p random access mesoscope (Thorlabs; 2p-RAM; <xref ref-type="bibr" rid="bib47">Sofroniew et al., 2016</xref>). We chose this form of data collection for use in head-fixed mice because it allows for: (1) rapid scanning (i.e. it uses a resonant scanner coupled to a virtually conjugated galvo pair) over a large (5x5 mm fully corrected, or up to 6x6 mm imageable), field-curvature corrected field of view (FOV); (2) subcellular resolution in the z-axis to avoid region-of-interest (ROI) contamination by neuropil and neighboring cells (0.61 x 0.61 x 4.25 µm xyz point-spread function at 970 nm laser excitation wavelength); (3) correction for aberrations at all wavelengths between 900–1070 nm, to allow for combined imaging of multiple fluorophores. We achieved these three objectives in head-fixed mice that were able to walk freely on a running wheel (See <xref ref-type="fig" rid="fig1">Figure 1a, b</xref>, <xref ref-type="video" rid="video1">Video 1</xref>, and Video 6), to clearly view visual monitors on both sides, and to lick either of two water spouts (left, right) for 2-alternative forced choice responses, while we performed left, right, and rear videography to monitor each mouse’s face, pupil, and body.</p><p>The neuronal imaging methodology we chose (see below) is not the only one available. For example, another promising and recently developed alternative implementation, Diesel 2p, allows for similarly flexible and large FOV imaging at single-cell resolution, but with a significantly higher z-axis point-spread function (~8–10 µm; <xref ref-type="bibr" rid="bib64">Yu et al., 2021</xref>). Despite this drawback, its use of dual scan engines allows for true simultaneous imaging of different cortical areas, while the 2p RAM mesoscope (Thorlabs) accomplishes this by rapid jumping (~1 ms) between FOVs. Both systems offer excellent corrected field curvatures on the order of +/-25 µm over ~5 mm.</p><p>Several primary design constraints needed to be met to achieve our goal of monitoring neuronal activity over a large portion of the mouse lateral and/or dorsal cortices. These ranged from aspects of the surgical preparation, mounting, and imaging, to neural recording (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>, Protocols II-III). To increase the extent of the neocortex over which we could monitor neural activity, we developed two distinct headpost/cranial window preparations. In the first, the dorsal mount, the mouse is head-fixed upright on a cylindrical running wheel and the objective is vertical (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Here, both wings of the headpost are attached to support arms, and each support arm is mounted on two vertical 1” diameter posts secured with flexing feet for maximum stability. This preparation allowed for monitoring of neural activity over large regions of both cerebral hemispheres, from the posterior aspects of visual cortex to the anterior aspects of motor cortex, and laterally to the dorsal-most aspects of auditory cortex, depending on the placement of the 2p mesoscope objective (<xref ref-type="fig" rid="fig1">Figure 1d</xref>).</p><p>The second preparation, the side mount, required the head of the mouse to be rotated 22.5<sup>o</sup> to the left (or right, not shown), with the microscope objective vertical or tilted 1–5 degrees to the right, so as to extend the 5x5 mm imaging FOV to include the auditory cortex and other neighboring ventral/lateral cortical areas (<xref ref-type="fig" rid="fig1">Figure 1b and e</xref>). For the side mount, the headpost had only a single, left wing so that the right side of the mouse’s face would not be occluded. The deep lateral/ventral extent of the side mount headpost prevented easy addition of a right headpost wing. Attaching the left wing to a single support arm supported by two 1” diameter vertical posts mounted with flexing feet, as in the dorsal mount preparation, was sufficient to minimize movement artifacts. A key additional difference to note here is that two small screws were used to attach the left wing of the side mount headpost to the support arm, thus giving it additional stability. The dorsal mount preparation used only one screw (the outermost hole) on each of the left and right side headpost wings. Thus, both preparations were secured by a total of two mounting screws, each countersunk into headpost arms. An important design feature is that the headpost wings fit into recessed rectangular slots machined into the support arms, facilitating stability.</p><p>We observed that mice adapt well to head tilt in the side mount preparation, and will readily whisk, walk or run, and learn to perform lick response tasks in this configuration. Keeping the microscope objective in a vertical or near-vertical orientation, and rotating the mouse’s head (instead of rotating the objective), significantly improved the manageability of the water meniscus between the objective and the cranial window. However, if needed, the objective of the Thorlabs mesoscope may be rotated laterally up to +/- 20<sup>o</sup> for direct access to more ventral cortical areas, for example if one wants to use a smaller, flat cortical window that requires the objective to be positioned orthogonally to the target region. In general, setups such as Sutter’s MOM system and Thorlabs’ Bergamo microscope, which offer even greater degrees of microscope objective rotation, would allow enhanced versatility with our preparations.</p><p>Preliminary comparisons across mice indicated that side and dorsal mount mice showed a similar degree of behavioral variability both within and across sessions. An example of the relative ease with which mice adapted to the ~22.5 degree neck rotation can be seen in <xref ref-type="video" rid="video2">Video 2</xref> and Video 10. It was in general important to make sure that the distance between the wheel and all four limbs was similar when using either preparation. In particular, careful attention must be paid to the positioning of the front limbs in the side mount mice so that they are not too high off the wheel. This can be accomplished by a slight forward angling of the left support arm. With dorsal mount mice, it was important to not place them too close to the wheel, in which case they would exert pressure on the headpost and support arms, leading to increased movement artifacts, or too far above the wheel, which can significantly reduce walking bout frequency.</p><p>Although it was in principle possible to image the side mount preparation in the same optical configuration without rotating the mouse (by rotating the objective to 20 degrees to the right), we found that the last 2–3 degrees of unavailable, yet necessary, rotation (our preparation is rotated 22.5 degrees left, which is more than the full available 20 degrees rotation of the objective), along with several other factors, made this undesirable. First, it was difficult or impractical to attach the horizontal light shield and to establish a water meniscus with the objective fully rotated. One could use gel instead of water (although, in our hands, gel was optically inferior), but without the horizontal light shield, light from the UV and IR LEDs can reach the photomultiplier tubes (PMTs) via the objective and contaminate the image or cause tripping of the PMT. Second, imaging the right pupil and face of the mouse was difficult under these conditions because the camera would need the same optical access angle as the objective, or would need to be moved down toward the air table and rotated up 20 degrees, in which case its view would be blocked by the running wheel and other objects mounted on the air table. A system of mirrors for imaging the animal could be used to overcome this problem, but would further complicate the current setup.</p><p>Cortical imaging in the novel side mount preparation was restricted to one hemisphere and a thin medial strip (~1 mm wide) of the contralateral hemisphere. This allowed for imaging from roughly the anterio-medial areas of V1 to medial and medio-lateral secondary regions of motor cortex (or slightly farther, if the anterior-posterior axis of the brain is oriented along the diagonal of the 5x5 mm FOV, or if one images to the full allowable but partially uncorrected 6x6 mm extent with ScanImage). This anterior-posterior reach was similar to that observed in the dorsal mount preparation. However, the side mount windowing and the accompanying mounting procedure significantly increased our ability to image neural activity in lateral (ventral) cortical aspects, including auditory, somatosensory, and association cortical regions (<xref ref-type="fig" rid="fig1">Figure 1e</xref>), notably without the need for substantial rotation of the objective.</p><p>To achieve 2p neuronal imaging from broad cortical regions, we created large and stable, custom (designed in AutoDesk Inventor) 3D-printed titanium (laser sintered powder with active cooling and shot-peened post-processing) headposts (<xref ref-type="fig" rid="fig1">Figure 1d, e</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1e, f</xref>; Suppl Design Files; i.materialise.com and sculpteo.com) and glass (0.21 mm thick, Schott D263T) cranial windows (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a, c, d, e, and f</xref> ; <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>; Suppl Design files: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link> (copy archived at <xref ref-type="bibr" rid="bib54">Vickers, 2024a</xref>); Labmaker.org for dorsal mount; TLC International for cutting, and GlasWerk for bending, for the side mount and early prototypes of the dorsal mount). We incorporated these custom made headposts and cranial windows into the dorsal mount (<xref ref-type="fig" rid="fig1">Figure 1a and d</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref> upper, d upper left, e), and side mount (<xref ref-type="fig" rid="fig1">Figure 1b, e</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a, c, lower, d, lower left and right, and f, upper right</xref>) preparations. The dorsal mount preparation allowed for imaging over roughly the same extent of bilateral dorsal cortex as in <xref ref-type="bibr" rid="bib30">Kim et al., 2016</xref>, although we were able to image up to ~25 mm<sup>2</sup> at a time, compared to the more typical maximum 2p FOV of ~1 mm<sup>2</sup>. In addition, the area of bilateral cortex imageable with this preparation is comparable to that of many recent studies employing widefield 1-photon imaging, such as <xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>. The side mount preparation allowed for imaging of an extent of right hemisphere comparable to that of <xref ref-type="bibr" rid="bib13">Esmaeili et al., 2021</xref>, although they used through-the-skull widefield 1-photon imaging, while our cranial window preparation allows for either widefield 1-photon imaging, or single neuron resolution 2p imaging, in the same mouse.</p><p>To mimic the curvature of the brain and reduce tissue compression, which is a problem with flat coverslips with a diameter larger than ~3 mm, cranial windows were curved by heating pre-cut (TLC International) glass pieces over 9 or 10 mm bend-radius molds (GlasWerk). The bend radius (i.e. radius of half cylinder mold over which the melted glass is bent to achieve its curved shape) was fixed at 10 mm for Labmaker.org dorsal mount windows (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1e</xref>, right; <xref ref-type="video" rid="video1">Video 1</xref>; early attempts with 11 or 12 mm bend radii failed for all but the largest adult male mice). For windows custom designed in collaboration with GlasWerk (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1f</xref>, right; <xref ref-type="video" rid="video2">Video 2</xref>), the bend radius was set at either 9 mm, for a tight fit to ventral auditory areas, or at 10 mm, to enable simultaneous imaging of the entire preparation at a single focal depth. In our experience, successful cranial windows last between 100 and 150 days, or in some rare cases up to 300 days: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>. The keys to this stability were even pressure across the surface of the window, complete enclosure of the glass-skull interface with flow-it, causing minimal damage to the dura during the craniotomy, and constant attention to the minimization of infection either across the surface of the skull, between the headpost and window surgeries, or around the external perimeter of the headpost, after the window surgery (See Protocol III and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>The success rate for window implantation, for both dorsal mount and side mount preparations, was around ~65% (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1g</xref>; see also Protocol III). However, with extensive practice this can be raised to be closer to 75–80%, especially if care is taken to both maximize the area of the craniotomy and to leave the dental cement attaching the headpost to the skull undamaged during drilling, so that the perimeter of the window fits fully inside the craniotomy, the window ‘floats’ or sits freely directly on the surface of the brain, and the headpost remains firmly attached to the skull. This was accomplished in later iterations of our designs and methods by making the side mount window slightly smaller at the anterior edge of M2, and by using an adjustable support arm (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>, lower) mounted on a small breadboard attached underneath the base of the stereotax in order to fix the mouse in its 22.5 degree left-rotated position during the window surgery so that it would not move or vibrate during drilling of the craniotomy. This also acted to minimize ‘break-throughs’ of the drill bit tip through the skull and into the brain.</p><p>The main causes of death during cranial window surgery were exsanguination during the surgery following a ruptured major blood vessel on the surface of the brain (either sagittal or lateral, usually), damage to the dura caused by the skull during the removal step of the craniotomy or by the sharp tip of a tool used to remove the skull, failure to properly pressurize the window across the entire cortical surface, or failure to completely seal the window to the skull fragment around its entire perimeter with flow-it (here, rapid UV light application during flow-it application can help). In some cases where the mouse survived the window surgery it was still considered a failure if a bleed re-erupted and occluded at least 30% of the window, or if glue occluded at least 30% of the window either above or below the surface of the window. In general, most bleeds cleared, on their own, by cerebrospinal fluid within a week if there was no direct clotting on the surface of the brain.</p><p>Changes in vasculature following cranial window surgery were usually minimal but could involve the following: (i) sometimes a vessel is displaced or moved during the window surgery, (ii) sometimes a vessel, in particular the sagittal sinus, will enlarge or increase its apparent diameter over time if it is not properly pressured by the cranial window, and (iii) sometimes an area experiencing window pressure that is too low will, over time, show outgrowth of fine vascular endings. The most common of these was (i), and (iii) is perhaps the least common. In general the vasculature is quite stable, and window preparations that we observed to be of high quality at ~1 week post-surgery showed minimal changes over the first ~150 days (see supplementary materials: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>).</p><p>Within a field of view of 5x5 mm, curvature of the brain, especially in the mediolateral direction, is a significant problem. To partially compensate for brain curvature, online field-curvature correction (FCC) was applied in ScanImage (Vidrio Technologies, MBF Biosciences) during mesoscope image acquisition. In addition, complementary fast-z ‘sawtooth’ or ‘step’ corrections were applied when all ROIs were acquired at the same or different z-planes, respectively (see <xref ref-type="bibr" rid="bib47">Sofroniew et al., 2016</xref>). Despite this, small uncorrected discrepancies in the depth of the imaging plane below the pial surface may have remained, especially along the long-axis of acquisition ROIs in the side mount preparation (i.e. mediolateral). It is likely that optical effects of the curvature of the glass partially compensated for these differences in targeted imaging depth (i.e. by refraction to normalize approach of the laser to the pial surface), although we did not quantify this effect. In cases where we imaged multiple small ROIs, nominal imaging depth was adjusted in an attempt to maintain a constant relative cortical layer depth (i.e. depth below the pial surface).</p><p>We estimate that we experienced ~200 μm of depth offset across 2.5 mm. If the objective is orthogonal to our 10 mm bend window and centered at the apex of its convexity, a small ROI located at the lateral edge of the side mount preparation would need to be positioned around 200 μm below that of an equivalent ROI placed near the apex, and would be at close to the same depth as an ROI placed at or near the midline, at the medial edge of the window. We determined this by examining the geometry of our cranial windows in CAD drawings (available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/cranial_windows">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/cranial_windows</ext-link>), and by comparing z-depth information from adjacent sessions in the same mouse, the first of which used a large FOV and the second of which used multiple small FOVs optimized so that they sampled from the same cortical layers across areas.</p><p>The mouse was restrained by fixation of the headpost with two countersunk screws to either a fixed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>, top) or adjustable aluminum support arm (custom; <xref ref-type="fig" rid="fig1">Figure 1a, b</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>, bottom) and mounting apparatus (<xref ref-type="fig" rid="fig1">Figure 1a and b</xref>). For the dorsal mount preparation, two headpost support arms, one on each side of the head, were used (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>, top; outer screw of both headpost wings used) while for the side mount preparation, a single, left side support arm was sufficient (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, bottom; inner and outer screws in the left headpost wing were both used). It was important to fix the headpost mounting arms to the top of the airtable with 1-inch diameter vertical mounting posts (Thorlabs RS6P8, RSH4, PF175), making sure all joints were clean of debris and tightened securely, to reduce movement artifacts.</p><p>To further minimize movement artifacts, it was also important to mount the mouse at a distance from the surface of the running wheel that was not too low, which would allow the mouse to push up with its legs, or too high, so that part of the mouse’s body weight would be supported by the headpost. A final consideration to minimize movement artifacts was to ensure that proper, even pressure was maintained across the entire interface between the brain and the cranial window during window implantation, both by making the craniotomy large enough so that the entire window could be set freely on the brain surface, and by using the 3D-printed window stabilizer properly during the application of flow-it glue to attach the edge of the window to the skull (see Protocols II, III).</p><p>Both preparations were found to be relatively free of vibration and movement artifacts and allowed for micro-adjustments (i.e. pitch, yaw, and roll) of the mouse relative to both the objective and the running wheel (<xref ref-type="fig" rid="fig1">Figure 1a, b and c</xref>). Increased positioning flexibility of the support-arm assembly was achieved through either of two ways: (1) by positioning it on a distal base-attached ball-joint mount (Thorlabs, SL20 articulating base; not shown); (2) through the use of a custom adjustable headpost support arm that was proximally adjusted around a ball-and-joint assembly (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>, bottom). This adjustable support arm operated through the use of a custom wrench and reverse-threaded (left-handed) nut for initial coarse tightening, and a micro-clamp for rotational stabilization during tightening. Four micro-hex wrench driven set-screws, two on either side of the ball-and-joint fitting, were used to achieve the final, fully locked state for imaging. The adjustable support arm was found to be superior to the base-attached ball joint mount in allowing for iterative micro-positioning of the animal in relation to the running wheel, objective, and lick spouts after initial fixation (mounting) of the headstage to the support bar.</p><p>In order to perform 2p neuronal imaging, the large 2p-RAM water-immersion objective (~10x net magnification, 0.6 NA, ~1.3 kg, 12 mm diameter tip, 25.6 mm shaft) must be positioned between 2.2 and 2.8 mm (i.e. ~working distance minus window thickness) from the curved glass surface of the cranial window while maintaining a stable water meniscus. Providing a base for the water meniscus while also blocking incident light entry was achieved by creating a custom 3D printed plastic light shield (AutoDesk Inventor, MakerGear M3 printer, PLA) that was attached to the protruding, fitted rim of the headpost with 170 FAST CURE Sylgard (‘wok one’; <xref ref-type="fig" rid="fig1">Figure 1c</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c, d</xref>). A second light shield (‘wok two’) served to further block extraneous light entry into the objective and was attached to the base of the lower light shield (‘wok one’) by fitting of a U-shaped profile over a vertically protruding single edge profile along the perimeter of the first, lower light shield (‘wok one’; <xref ref-type="fig" rid="fig1">Figure 1c</xref>).</p><p>Together, these two custom-printed light shields prevented incident light (from the video stimulus monitor, the ultraviolet LEDs used for controlling the baseline and dynamic range of pupil diameter, and the infrared LEDs used to illuminate the mouse) from entering the imaging objective and thereby either contaminating the image or tripping the photomultiplier tubes (PMTs). Line-of-sight for the animal to the video stimulus monitor was retained, and this design allowed for free vertical and rotational (over a limited range) movement of the objective lens, which was contacted directly only by the water meniscus.</p><p>Direct left, right, and posterior camera angles, or ‘lines of sight’, were preserved to enable reliable recording and proper illumination of pupil diameter, whisker pad movement, and the movement of other body parts such as the nose, mouth, ear, paws, and tail (<xref ref-type="fig" rid="fig1">Figure 1a and b</xref>; see <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>). Placement of the rotated mouse in the side mount preparation near the left side of the running wheel allowed for the animal’s left and right fields of view to not be significantly obstructed. Adjustable positioning of two vertically and laterally offset conductance-based lick spouts for 2-alternative forced choice (2-AFC) task performance (lick left, lick right) and reward delivery was achieved with a rapidly translatable motorized linear stage (<xref ref-type="fig" rid="fig1">Figure 1c</xref>; 63.7ms total travel time over 7 mm; Zaber Technologies). This system allowed for rapid withdrawal of lick spouts between trials, and presentation of the lick spouts during the response period of each trial.</p></sec><sec id="s2-2"><title>Pseudo-widefield imaging and optogenetic stimulation hardware modifications of the Thorlabs 2-photon mesoscope</title><p>Examining neuronal activity at the single cell level and relating it to stimulus-evoked activity observed at the widefield level (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>), as well as manipulating regions of the cortex through localized light delivery and optogenetics (see Video 11), required precise optical alignment across all three of these methods. First, a standardized cortical map needed to be fitted onto images of skull landmarks and the cortical vasculature by widefield multimodal sensory mapping (MMM; <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3a, b, and c, top</xref>). Then, a method was needed for transferring this cortical map onto the field of view of the mesoscope to both allow for online FOV targeting and for post-hoc assignment of each imaged neuron to a designated cortical area on the Allen common-coordinate framework (CCF v3.0) map for subsequent analyses (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3c, bottom, d</xref>). Finally, the spatial coordinate system of the 1p opto-stimulation laser routed in through an auxiliary light-path of the mesoscope needed to be aligned to that of the 2p laser so that specific cortical subregions could be targeted for optogenetic inhibition by light activated, ChR2-mediated excitation of parvalbumin interneurons in Thy1-RGECO x PV-Cre x Ai32 mice (Video 11).</p><p>In order to align 2p maps of single neuron activity with functional 1p cortical maps determined through pseudo-widefield imaging (i.e. combined reflected and fluorescence light imaging with a standard ‘non-widefield’ format CCD camera), we designed a method for reliably aligning a standardized cortical map (i.e. the Allen CCF v3.0; we used either a 0 degree, <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/CCF_map_rotation/0deg/CCF_MMM.png">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/CCF_map_rotation/0deg/CCF_MMM.png</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>, or 22.5 degree, <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/CCF_map_rotation/22deg/CCF_MMM.png">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/CCF_map_rotation/22deg/CCF_MMM.png</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>, rotated CCF map outline created with the following code: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/CCF_map_rotation/Rotate_CCF.py">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/CCF_map_rotation/Rotate_CCF.py</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>), to an image of the cortical vasculature pattern, which can be directly visualized with both widefield and 2p imaging techniques for each brain (see Protocol V). The epifluorescence light source (Excelitas) for the pseudo-widefield imaging, which was parfocal with the 2p imaging coordinate system, was controllable by a dial, shutter, and remote foot-pedal for optimal ease of positioning the objective in the water meniscus at a distance of ~2.2 mm from the headpost and cranial window without crashing into the preparation (<xref ref-type="fig" rid="fig1">Figure 1c</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>, top and bottom right). This allowed us to ‘drive’ the position of the 2p mesoscope FOV to the desired Allen CCF cortical area by moving the objective to a location where the observed vasculature pattern matched that of a processed image we created ahead of time on our 1-photon widefield imaging rig that combined vasculature, sensory responses, and skull landmarks from each mouse.</p><p>This technique required several modifications of the auxiliary light-paths of the Thorlabs mesoscope, and would also likely involve similar modifications in other comparable microscope setups, such as Diesel 2p (<xref ref-type="bibr" rid="bib64">Yu et al., 2021</xref>). For switchable blue/green widefield imaging and 2p imaging in our original configuration (see (a) in <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_optical_path_opto_switching.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_optical_path_opto_switching.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>), we used a 469/35 Semrock excitation filter, 466/40 Semrock dichroic (1st, top cube), and mirror (2nd, bottom cube). For the combination of pseudo-widefield imaging and rapid, targetable optogenetic 1-photon (1p) stimulation with concurrent 2p imaging (see (b) in <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_optical_path_opto_switching.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_optical_path_opto_switching.pdf</ext-link>), which we used with PV-Cre x Ai32 x Thy1-RGECO mice (Video 11), we established dual coupling of the broadband fluorescence light-source (Excelitas) used in the original configuration and a 473 nm laser driver (SF4C 473, Thorlabs), coupled via liquid light guide, to converging, dual input auxiliary light paths of the Thorlabs mesoscope via two in-series, magnetically secured, switchable filter cubes (DFM1T1 cube, Thorlabs; see <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_filterCube_schematic_dual_opto_Jan0320.jpg">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_filterCube_schematic_dual_opto_Jan0320.jpg</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>; the top cube can be switched to allow pseudo-widefield imaging of either GCaMP6s or Thy1-RGECO, and the bottom cube can be switched to allow either pseudo-widefield imaging or optogenetic stimulation).</p><p>The 473 nm (blue) laser driver was connected to a single open loop, high speed buffer (50 LD, Thorlabs), and targeted to the coordinate system of the 2p laser with a grid-calibrated, auxiliary galvo-galvo scanner (GVSM002, Thorlabs). This allowed for 2p imaging and blue 1p laser optogenetic stimulation to occur pseudo-simultaneously, because rapid, electronic control of the PMT1 and PMT2 shutters was able to limit interruption of image acquisition to a brief period during presentation of the optogenetic stimulus (~100 ms; see Video 11; see <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_optical_path_opto_switching.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/mesoscope_optical_path_opto_switching.pdf</ext-link>). Here, replacement of the switchable mirror behind the objective, which blocked 2p imaging during pseudo-widefield imaging in our original configuration, with a dichroic that, when positioned in the light path, allowed both 920 nm (2p excitation) and 473 nm (opto-excitation) light to reach the mouse brain without requiring any additional slow switching of optical components.</p></sec><sec id="s2-3"><title>Reduction of resonant scanner noise</title><p>Resonant scanners in 2p microscopes emit intense sound at the resonant mirror frequency, which is well within the hearing range of mice (~12.5 kHz emitted in the Thorlabs mesoscope). Because scanning precision requires the scanner to remain at a stable, elevated temperature, the scanner must remain on during the entire experimental session (i.e. up to ~2 hr per mouse). The unattenuated or native high-frequency background noise generated by the resonant scanner causes stress to both mice (<xref ref-type="bibr" rid="bib44">Sadananda et al., 2008</xref>) and experimenters (<xref ref-type="bibr" rid="bib14">Fletcher et al., 2018</xref>), and likely acts to prevent mice from achieving maximum performance in auditory mapping, spontaneous activity sessions, auditory stimulus detection, and auditory discrimination sessions/tasks.</p><p>To reduce this acoustic noise, we encased the resonant scanner and attached light path tubes with a custom 3-dimensional (3D)-printed assembly containing dense interior insulating foam. See Supplementary methods and materials for diagrams: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_for_noise_reduction_on_resonant_scanner_devices.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_for_noise_reduction_on_resonant_scanner_devices.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>, and for a text description: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_methodology_summary.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_methodology_summary.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>; ‘3D scanning and honeycomb patterned nylon print’ (University of Oregon Innovation Disclosure #DIS-23–001, US provisional patent application UOR-145-PROV). It was critical to use 3D scanning of encased components in the design of the noise reduction shield so that the sound-reduction assembly would closely follow all of the surface contours and fit accurately in spaces with tight tolerances.</p><p>The result of this encasement was a large reduction in resonance scanner sound, from ~60 dB to ~5 dB measured at the head of the mouse (Bruel and Kjaer ¼” pressure filled microphone 4938 A-011; i.e. below the mouse hearing threshold at 12.5 kHz of roughly 15 dB; <xref ref-type="bibr" rid="bib66">Zheng et al., 1999</xref>). By comparison, encasements designed using standard 3D-design and printing techniques (i.e. not based on a 3D scan) were, in our hands, only able to achieve a noise reduction of ~30 dB, to a level still audible to both mice and humans. This difference was due to the enhanced precision of encasement fit enabled by using the 3D-scanned map of the microscope’s surface contours.</p></sec><sec id="s2-4"><title>Cortical alignment to the common coordinate framework (CCF) v3.0 map</title><p>Interpretation of the diversity of activity of thousands of neurons simultaneously identifiable with the mesoscope requires proper cortical areal localization, including that of areas anatomically or functionally distant from primary sensory cortices. Such mapping is not routinely possible directly on the 2p mesoscope when performing neuronal-level imaging, due to both the heterogeneity of responses within primary sensory cortices at the single neuron level, and to animal-to-animal variations in the spatial extent of cortical regions (<xref ref-type="bibr" rid="bib12">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Bimbard et al., 2023</xref>). To facilitate the assignment of neurons to cortical areas, we sought to align the Allen Institute Common Coordinate Framework (CCF v3.0; <xref ref-type="bibr" rid="bib56">Wang et al., 2020</xref>) to our 2p neuronal imaging results, using blood vessels, skull landmarks, and widefield imaging responses as intermediaries. We used an overlaid image of these features that we refer to as the ‘multimodal map’ (MMM) as on-the-fly guidance for FOV placement during 2p mesoscope imaging sessions, and then performed a precise post-hoc CCF alignment based on the MMM and vasculature patterns observed during 2p imaging to assign a unique CCF area identifier to each Suite2p-identified ROI/neuron during preprocessing stages of our data analysis (see <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/matlab_code">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/matlab_code</ext-link> for creation of the MMM, and <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/python_code">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/python_code</ext-link> for generation of rotated CCF map outlines and precise neuron assignment to CCF areas, which takes place in the following jupyter notebook: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_pre_proc/meso_pre_proc_1.ipynb">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_pre_proc/meso_pre_proc_1.ipynb</ext-link>).</p><p>Alignment of the CCF to the dorsal surface of cortex and subsequent image-stack registration based on widefield imaging Ca<sup>2+</sup> fluorescence data has been performed elsewhere with a variety of techniques. These include the use of skull landmarks (e.g. bregma and lambda; <xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>), responses to unimodal sensory stimuli (<xref ref-type="bibr" rid="bib15">Gallero-Salas et al., 2021</xref>), visual field mapping (<xref ref-type="bibr" rid="bib68">Zhuang et al., 2017</xref>), and/or autocorrelation maps of spontaneous activity (<xref ref-type="bibr" rid="bib42">Peters et al., 2021</xref>). Few studies, however, have performed such alignments with rotated cortex (i.e. imaging lateral portions of the cortex at an angle; but, see <xref ref-type="bibr" rid="bib13">Esmaeili et al., 2021</xref>).</p><p>To summarize our approach, we used a technique where, for both the dorsal mount (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>) and side mount (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b</xref>) preparations, we first imaged 1p widefield GCaMP6s responses to unimodal passive sensory stimulation (e.g. visual, auditory, and somatosensory) to create a MMM consisting of multiple sensory area masks on top of the cortical vasculature. We then overlaid skull landmarks (e.g. bregma and lambda), imaged with reflected green light, onto the MMM. This intermediate overlay image, created using custom MatLab code to extract mean widefield dF/F sensory responses (<ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/matlab_code/SensoryMapping_Vickers_Jun2520.m">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/matlab_code/SensoryMapping_Vickers_Jun2520.m</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>), followed by z-projection and selection masking techniques in Fiji/ImageJ, was used to guide selection of the FOV during 2p mesoscope imaging sessions. The Allen CCF was then warped onto the vasculature/skull/MMM overlay, and the mean 2p image was warped onto the MMM by vasculature alignment using custom code (see:<ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_pre_proc/meso_pre_proc_1.ipynb">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_pre_proc/meso_pre_proc_1.ipynb</ext-link> for full initial processing steps of 2p data, or <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_preprocess_MMM_creation.ipynb">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_preprocess_MMM_creation.ipynb</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref> for a short notebook containing only the relevant CCF alignment steps - as with all of our jupyter notebooks, these should be run in the ‘uobrainflex’ python environment: <ext-link ext-link-type="uri" xlink:href="https://github.com/sjara/uobrainflex">https://github.com/sjara/uobrainflex</ext-link>; <xref ref-type="bibr" rid="bib27">Jaramillo, 2020</xref>). Finally, each neural ROI in the 2p image was assigned, post-hoc, to a position in the MMM x-y coordinate system and a corresponding CCF area based on the final overall alignment of the MMM, CCF, and 2p image. The following sections describe these steps in more detail.</p></sec><sec id="s2-5"><title>Widefield multimodal mapping</title><p>To create the multimodal map, each mouse was put under light isoflurane anesthesia (~1.5%) after head-post surgery, but before implantation of the cranial window, and exposed to 5 min each of full-field visual (vertical and horizontal stationary grating patches; 0.16 cpd, 30 deg; <xref ref-type="bibr" rid="bib35">Michaiel et al., 2019</xref>), tone-cloud auditory (a series of overlapping 30 ms duration tones randomly selected from a frequency range of 5–40 kHz and presented at 100 Hz; <xref ref-type="bibr" rid="bib62">Xiong et al., 2015</xref>), and piezo-driven whisker deflection (or, in some cases, forelimb and trunk stimulation; see <xref ref-type="bibr" rid="bib15">Gallero-Salas et al., 2021</xref>). For whisker deflection, a 1 s, 5 Hz burst of five 100 ms duration forward sweeps was used (i.e. each sweep consists of 100ms of forward movement and 100 ms of backward movement), consisting of posterior to anterior sweeps. The whisker deflector was a custom 3D-printed triangular polylactic acid (PLA) piece mounted on a 21-gauge needle and attached to a PL140.11 piezo-actuator (PI Ceramic) with epoxy glue. It was actuated with a Physik-Instrumente controller driven by a custom pulse sequence generated in Spike2 and delivered from an analog output of a CED Power 1401, with an inter-stimulus interval of ~10 s (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, right, and <xref ref-type="fig" rid="fig3">Figure 3b</xref>, right; see also <xref ref-type="video" rid="video3">Videos 3</xref>–<xref ref-type="video" rid="video7">7</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Dorsal mount multimodal mapping: example widefield (1p) imaging of GCaMP6s fluorescence responses during a visual multimodal mapping session.</title><p>Top right, full field, left-side isoluminant Gaussian noise stationary grating patches (vertical and horizontal stationary grating patches; 0.16 cpd, 30 deg; <xref ref-type="bibr" rid="bib35">Michaiel et al., 2019</xref>) presented to elicit a visual response in right cortex, with small upper left-corner alternating white/black box positioned under photodiode to record precise stimulus presentation times. Top left, pixelwise dF/F response of the entire image for a single trial, recorded at 50 Hz and shown at 0.5 x speed. The baseline was calculated as the median of a 1 s period leading up to the stimulus onset. The dashed white circle indicates the putative primary visual cortex (right V1). A = anterior, L = left, R = right, P = posterior, dF/F = change in fluorescence divided by baseline fluorescence; midline extends vertically near the center of frame from bottom to top edge roughly between the ‘A’ and ‘P’: labels. Bottom, trace of mean dF/F for all pixels inside dashed white circle (mask), expressed as percent change. Vertical black line indicates stimulus onset. The visual stimulus is present through the end of the epoch shown. Upper left, overlay: mean of 33 dF/F responses (mean of 1 s after stimulus onset minus mean of 1 s leading up to stimulus onset) in a single dorsal mount session under 2–3% isoflurane anesthesia. ITI = inter-trial interval, measured from beginning of one stimulus to beginning of the next stimulus.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Whisker stimulation: same as in dorsal mount multimodal mapping visual stimulation example video (<xref ref-type="video" rid="video3">Video 3</xref>), with same mouse on same day, except with 5 Hz, 100 ms duration forward swipes with custom 3D printed plastic (PLA) whisker-deflector, as indicated by vertical deflections in stimulus trace, mid-right.</title><p>Example video of mouse shown from a different session than dF/F data, because mouse face video was typically not recorded during multimodal alignment sessions. Red S1b (and dashed line) = right primary whisker barrel cortex. A = anterior, P = posterior, L = left, R = right, V1 = primary visual cortex.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video5.mp4" id="video5"><label>Video 5.</label><caption><title>Side mount multimodal mapping: Same as in <xref ref-type="video" rid="video3">Video 3</xref> but for side mount preparation. 1.5–3% isoflurane anesthesia was used in all 3 sessions.</title><p>Auditory: 1 s tone cloud with tones between 2 and 40 kHz presented for 0.5 s starting at black vertical dashed line (bottom). Sonogram display from Spike2 (CED) shows individual tones as horizontal green lines, where y-axis is sound frequency (~0–25 kHz) and x-axis is time (0–1 s). Movies shown at 0.25x speed. As in other example videos, the mouse shown is from a different session, but is exposed to the same stimulus at the indicated time. The mouse shown is from a different session type when videography was enabled, to show the normal response of the mouse to the stimulus. ml = midline, A1 = primary auditory cortex, A = anterior, L = left, R = right, P = posterior, dF/F = change in fluorescence divided by baseline fluorescence.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video6.mp4" id="video6"><label>Video 6.</label><caption><title>Same as in <xref ref-type="video" rid="video5">Video 5</xref> but for visual stimulation, with different side mount example mouse.</title><p>ml = midline, A1 = primary auditory cortex, A = anterior, L = left, R = right, P = posterior, dF/F = change in fluorescence divided by baseline fluorescence.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video7.mp4" id="video7"><label>Video 7.</label><caption><title>Same as in <xref ref-type="video" rid="video5">Videos 5</xref> and <xref ref-type="video" rid="video6">6</xref> but for whisker stimulation, with same side mount example mouse as in <xref ref-type="video" rid="video5">Video 5</xref>.</title><p>ml = midline, A1 = primary auditory cortex, A = anterior, L = left, R = right, P = posterior, dF/F = change in fluorescence divided by baseline fluorescence.</p></caption></media><p>Light anesthesia was used to minimize unwanted cortical activity due to spontaneous movements and arousal fluctuations, and to prevent the spread of cortical sensory responses to areas downstream of primary sensory areas. Imaging through the skull between the headpost and cranial window implantation surgeries was done to allow for coregistration of skull landmarks with vasculature and sensory responses, and in general yielded more contiguous, easily interpretable multimodal maps than the same widefield mapping done through the cranial window. Although the resulting overlay of vasculature and the multimodal sensory map was useful for cranial window placement, it was not strictly necessary. Additional MMM sessions performed through the cranial window were performed every 30–60 days or as necessary due to slight changes in vasculature. In general, vasculature was stable throughout the entire ~150–200 day lifespan of a successful cranial window preparation (see <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>), although in some cases slight increases in the diameter of major blood vessels (e.g. sagittal sinus), or outgrowth of fine arteriole endings, were observed.</p><p>Averaged, baseline-subtracted dF/F responses (<xref ref-type="fig" rid="fig2">Figures 2b</xref> and <xref ref-type="fig" rid="fig3">3b</xref>) were thresholded, masked, outlined, and layered onto the blood vessel image (<xref ref-type="fig" rid="fig2">Figure 2a, c</xref>, <xref ref-type="fig" rid="fig3">Figure 3a, c</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a, b</xref>) along with bregma and lambda skull landmarks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>) identified with 530 nm (green) reflected-light skull-imaging using a custom protocol/macro in Fiji (ImageJ; <xref ref-type="fig" rid="fig2">Figure 2c</xref>, <xref ref-type="fig" rid="fig3">Figure 3c</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a, b</xref>; <xref ref-type="video" rid="video3">Videos 3</xref>–<xref ref-type="video" rid="video7">7</xref>). This multimodal mapping procedure and alignment to the CCF and skull landmarks was repeated if significant changes in vasculature pattern occurred over the days/weeks of the experiment. More precise maps of visual cortical areas can be achieved through visual field mapping using a topographic stimulus consisting of a bar sweeping in azimuth or elevation (<xref ref-type="bibr" rid="bib16">Garrett et al., 2014</xref>). This technique was not applied here, because our goal was global alignment of the Allen Institute cortex-wide CCF to our dorsal or side mount views, and not precise alignment to visual subareas per se.</p></sec><sec id="s2-6"><title>Co-alignment of 2-photon image to vasculature and CCF</title><p>The MMM was aligned to an overlay of the rotated CCF edges and region of interest (ROI) masks using custom Python code and the built-in function ‘PiecewiseAffineTransform’, given a user-supplied series of bounding-box and alignment points common to both images (<xref ref-type="fig" rid="fig2">Figure 2c</xref> (upper, part 1), 3 c (upper, part 1)). A second, similar alignment was then performed between the multimodal map and all neural ROI locations (i.e. Suite2p-identified neurons) relative to the 2p image plane; here, vasculature is inferred from the pattern of gaps (e.g. vascular ‘shadows’) in the spatial distribution of neurons (<xref ref-type="fig" rid="fig2">Figure 2c</xref> (lower, part 2), 3 c (lower, part 2)), and can be confirmed by parfocal pseudo-widefield (i.e. combined reflected and epifluorescence light) imaging directly on the 2p Thorlabs mesoscope. Note that in 2p imaging below the pial surface, the effective/apparent width of the vasculature, or its “shadow”, is significantly larger than that of the actual blood vessel, and its width increases with depth. The transforms resulting from each of these two alignments were applied in serial (i.e. multimodal map to CCF, then multimodal map to 2p neural image; <xref ref-type="fig" rid="fig2">Figures 2c</xref> and <xref ref-type="fig" rid="fig3">3c</xref>) to overlay the neural image directly onto the cortical map, using outlines, and to assign a unique CCF area identifier (i.e. name and ID number) to each Suite2p-extracted, 2p-imaged neuron using CCF masks (<xref ref-type="fig" rid="fig2">Figures 2d</xref> and <xref ref-type="fig" rid="fig3">3d</xref>). Note that, while the final assignment of CCF area identifiers to each Suite2p-identified neuron (<xref ref-type="fig" rid="fig2">Figure 2c</xref> (2), right) was performed post-hoc, the intermediate blood-vessel/MMM overlay image (<xref ref-type="fig" rid="fig2">Figure 2</xref> (1) and (2), left) was used for online guidance of 2p mesoscope FOV selection during imaging sessions.</p></sec><sec id="s2-7"><title>2-photon imaging across broad regions of the dorsal and lateral cortex in behaving mice</title><p>Previous 2p imaging studies have demonstrated that arousal and/or orofacial/body movement can explain a significant proportion of the variance in spontaneous and sensory evoked neural responses in visual and other restricted dorsal cortical regions (<xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>). Here, as a proof of principle, we examined the generality of this finding by simultaneously monitoring neuronal activity across broad regions of bilateral dorsal cortex, with our dorsal mount preparation, and both dorsal and lateral cortex across the right hemisphere, in our side mount preparation. Previous studies suggest that there may be both commonalities as well as heterogeneity in the effects of changes in arousal/movement on spontaneous and sensory-evoked responses across different cortical areas (<xref ref-type="bibr" rid="bib34">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>). For example, running typically enhances the gain of visually evoked responses in the mouse primary visual cortex (<xref ref-type="bibr" rid="bib39">Niell and Stryker, 2010</xref>), while it significantly decreases that of evoked auditory responses in primary auditory cortex (<xref ref-type="bibr" rid="bib67">Zhou et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">McGinley et al., 2015</xref>).</p><p>Assessing such potential differences between functionally distinct, and potentially distant, cortical areas required simultaneous imaging across multiple cortical regions while maintaining adequate imaging speed, quality, and resolution. To achieve this level of 2p imaging across several millimeters of cerebral cortex (e.g. from visual to motor or auditory cortical areas), we first sought to optimize the parameters of our mesoscope imaging methods and protocols.</p><p>Conventional 2p imaging using a preparation similar to our dorsal mount (“Crystal Ckull”; <xref ref-type="bibr" rid="bib30">Kim et al., 2016</xref>) previously employed serial acquisition of ~1 x 1 mm FOVs (<xref ref-type="bibr" rid="bib30">Kim et al., 2016</xref>). Other 2p Thorlabs mesoscope imaging studies have used acquisition protocols targeting z-stacks of 600x600 µm FOVs in barrel cortex (3 planes at 7 Hz, 1.17 µm/pixel; <xref ref-type="bibr" rid="bib41">Peron et al., 2015</xref>), 900x935 µm FOVs in visual cortex (11 planes at ~1 Hz; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>), and three adjacent 600x1800 µm FOVs in CA1 hippocampus (<xref ref-type="bibr" rid="bib52">Sun et al., 2023</xref>; bioRxiv). Here, with our dorsal and side mount preparations, we were able to routinely image between 2000 and 7600 neurons per session over up to ~25 mm<sup>2</sup> of dorsal cortex simultaneously at ~3 Hz with a resolution of 5 µm/pixel in GCaMP6s mice (combined total in both preparations for all large-FOV sessions: N=17 mice, n=91 sessions, ~350,000 neurons; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2e</xref>, <xref ref-type="video" rid="video8">Video 8</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video8.mp4" id="video8"><label>Video 8.</label><caption><title>Dorsal mount.</title><p>Titanium 3D printed headpost shown from above, with paraformaldehyde-fixed mouse brain shown beneath custom cranial window, colored Allen CCF outlines, and an example 5 x 4.62 mm mesoscope FOV corresponding to data in this video shown inside an inset box indicated by a white dashed line. Next segment, left, full 5 x 4.62 mm ScanImage (Vidrio) rendered mesoscope 2-photon field of view, rotated and flipped so that the top corresponds to the front of the mouse and the left corresponds to the left hemisphere of the cerebral cortex. Black horizontal joining lines indicate the ‘seams’ between adjacent ROIs where they are joined by rendering. The resonant scanner moves along the short axis of each rectangle, perpendicular to the ‘seams’, and mechanical galvanometers move along the long axis, parallel to the ‘seams’. Midline vertically oriented (i.e. from posterior, at bottom, to anterior, at top) blood vessel (sinus) prominent at center of frame. A movie (100 s duration shown) acquired with unidirectional scanning at 1.62 Hz, smoothed with a running average of three frames, is shown at 3x playback rate. Center top, expanded inset from white dashed box (left), shown synchronized with larger video. Bottom center, high-resolution left face videography of mouse from this session, synchronized with 2p video, rasterized neural data (upper right), and Spike2-recorded (CED) behavioral data (whisking motion energy in blue, pupil diameter in gray, and walk speed in red). Neural data was sorted in Suite2p (<xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>) with Rastermap (top), or by its first (middle) or second (bottom) principal component of activity over the entire session, and is displayed as z-scored (normalized) dF/F neuropil subtracted activity indexed to a single color-map lookup table with yellow as maximum, green as intermediate, and blue/purple as minimum activity level (i.e. GCaMP6s fluorescence; same lookup table scale as in <xref ref-type="fig" rid="fig4">Figure 4a</xref>). Red arrowheads indicate co-alignment of transient arousal increases accompanied by walking, whisking, and pupil dilation, with diverse changes in neural activity across rastermap, PC1-, and PC2-sorted ensembles.</p></caption></media><p>In order to achieve spatially broad and dense sampling, while monitoring changing activity in each neuron as frequently as possible, we typically tiled seven mediolaterally aligned 5000x660 µm FOVs (i.e. mechanical scanning along a long-axis oriented mediolaterally, combined with fast resonant-scanning across multiple short-axes oriented anterior-posterior) over either posterior-medial, posterior-lateral, or anterior dorsal, cortex at a pial depth of ~200–300 µm (cortical layers 2/3). Bidirectional scanning (with a scan-phase correction of between -0.9 and -1.0) and field-curvature correction were typically enabled, with a 1-5 ms ‘flyback’ and ‘frame’ time, which are defined in ScanImage as the time delay for retargeting of the laser from the end of one line or frame acquisition to the beginning of the next, respectively. Here, there is a trade-off between longer flyback times, which reduce imaging speed but minimize positioning errors at the beginning of each scan line or ROI, and shorter values, which accelerate overall imaging acquisition speed but increase these positioning errors.</p><p>We typically imaged GCaMP6s fluorescence in awake, behaving mice at an excitation wavelength of 920 nm with 60–95 mW power calibrated at the pia over 20–90 min sessions, with little to no bleaching or change in baseline fluorescence. Suite2p reliably extracted neurons with &gt;0.9 classifier probabilities, and nearly completely (i.e. to less than 0.1 pixels) removed x-y movement artifacts with uncorrected, raw principal components of up to ~1.3 pixels (i.e. up to ~15% of a 20–40 µm, or 4–8 pixels, diameter neuron at an imaging resolution of 0.2 µm/pixel, reduced to less than 0.5 µm or 1–2% of the diameter of a neuron after combined rigid and non-rigid motion correction; see Video 11).</p><p>To further control for z-movement artifacts (whose correction is intractable post-acquisition under normal scanning conditions) we also occasionally used GPU-enabled online fast-z motion correction (42/52 large FOV side mount sessions, and 9/39 large FOV dorsal mount sessions, used online z-correction) with ScanImage (Vidrio Technologies, LLC). This was only possible, in our hands, during multi-frame imaging when all frames were acquired at the same z-level. Quality of post-hoc motion-corrected dF/F traces and correlations with behavioral variables did not appear, by eye, to be different under these conditions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1a vs b</xref>; see <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>), as both types of sessions contained similar mixtures of neurons with both walk-bout dependent and independent activity, for example. Because the z-motion correction did not lead to large qualitative differences in neural activity and its relationship to behavioral primitives, both types of session are used interchangeably here.</p><p>In order to confirm neural region-of-interest (ROI) selection (i.e. each neuron as identified by a spatiotemporal footprint in Suite2p), increase scan speed, and correct our cortical layer 2/3 targeting for the effects of brain curvature, we also routinely imaged in random access mode with 4–6 small FOVs (660–1320 x 660 µm) positioned over visual (V1), somatosensory (S1), retrosplenial (RSpl), posterior parietal (PPC), and/or motor (MOs_pm, MOs_am, and/or MOs_al - ALM) cortices at independently controlled z-levels, with an imaging speed of 4–10 Hz and a resolution of 1.0–2.0 µm/pixel (see Protocol I; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2d, e</xref>). These multi-FOV sessions yielded hundreds of high quality neurons per FOV, at a density roughly twice that of the lower resolution sessions (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2c vs d</xref>).</p><p>Uncontrolled image motion due to the movement of behaving animals is a significant impediment to image quality (<xref ref-type="bibr" rid="bib40">Pachitariu et al., 2016</xref>). We have taken significant care in our headpost design and fixation (see <xref ref-type="fig" rid="fig1">Figure 1a, b</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a–f</xref>, and Supplementary methods and materials) so that our preparations are highly stable, such that motion-corrected or ‘registered’ movies showed very little to no residual movement in either large FOV, low-resolution, or multiple small FOV, high-resolution sessions (<xref ref-type="video" rid="video9">Video 9</xref>; Video 11). Furthermore, movement-related neural activities across neighboring detected ROIs in large FOV, low-resolution sessions were correlated but non-identical upon detailed examination, and differed from dF/F Ca<sup>2+</sup> fluorescence fluctuations detected in nearby non-ROI and blood-vessel regions of similar cross-sectional area (see <xref ref-type="video" rid="video9">Video 9</xref>; Video 11), thus strongly suggesting that it was non-artifactual. In theory, movement artifact signals should be relatively time-locked or nearly synchronous across cortical regions during vigorous walking or movement. However, we did not observe this in our preparations.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video9.mp4" id="video9"><label>Video 9.</label><caption><title>Example two-dimensional field of view (FOV) rendered movie and insert from an example session where factorial hidden Markov modeling showed near global synchrony of PC1 loading during walking bouts in a side mount mouse.</title><p>A 5 x 5 mm full FOV movie (~10x real-time) is shown at top left, with a 1 x 1 mm inset (white dashed box) expanded and shown at bottom left. Three example Suite2p-extracted neuron ROIs (1, 2, and 3) and a blood vessel ROI (bv) are indicated with white dashed circles. Gray traces at right indicate mean pixel fluorescence intensity of all pixels within each of these ROIs over ~450 2p movie frames. Rasterized rendering at top right indicates PC1 loading (blue/green/yellow = low/medium/high) across all CCF areas contained in the 2p movie (one per row). The red horizontal lines at the end of the shown epoch are a graphical rendering artifact that was not removed. White/gray/black rows indicate the current state of each of four hidden factors (i.e. state 0 = white, state 1 = gray, and state 2 = black) in a factorial hidden Markov model (fHMM) fit to the first 15 PCs of global neural data, the blue row indicates walk speed, the next, split blue row indicates left and right whisker pad motion energy, and the red, bottom row indicates pupil diameter. Actual behavioral arousal and movement primitives are shown as traces below (walk = red, whisk = blue, pupil = gray). Note that walk-related activity changes are correlated with independent fluctuations across the three example neurons, and furthermore that blood vessel fluctuations are small and not movement-locked, consistent with the idea that neurobehavioral activity alignment in this case, even with detected transient global synchrony across the first PC of activity, is not driven by brain movement artifact.</p></caption></media><p>These findings, therefore, gave us confidence that neural activity changes detected during spontaneous movements of the mouse were due to real changes in Ca<sup>2+</sup> fluorescence within each neuron, and not to movements of the brain causing the neuron to shift into or out of each Suite2p-defined ROI and leading to artifactual fluorescence transients (see <xref ref-type="video" rid="video9">Video 9</xref>; Video 11, which use blood vessels and inactive fluorophore/neuropil, respectively, as examples of the lack of significant movement-driven artifactual transients in GCaMP6s(-) structures in our preparations). In other studies, performed using the same setup and methods as those applied here, we have demonstrated that non-activity dependent fluorescent markers, such as the expression of a non-activity dependent fluorescent marker mCherry in cholinergic or noradrenergic axons, or autofluorescent blebs, exhibit no significant movement-related changes in fluorescence, indicating that movement artifacts are minimal in our preparations (<xref ref-type="bibr" rid="bib10">Collins et al., 2023</xref>). It should be noted that axonal diameters are on the order of ~1 µm or less - this is significantly smaller than the diameter of the neuronal cell bodies (~15–30 µm) imaged here. Axons, therefore, are a more stringent test of the stability of our imaging preparations.</p></sec><sec id="s2-8"><title>Modulation of neural activity by spontaneous movements and arousal as observed with the dorsal mount preparation</title><p>Recent work has shown that sorting fluorescence based 2p GCaMP cortical neuronal activity with an algorithm called Rastermap is a powerful method to reveal clusters of cells that exhibit similar patterns of activity (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). An example of such sorting of our data for a dorsal mount session of 3096 neurons is shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref> (top). This method for one-dimensional nonlinear embedding works by sorting and clustering of ROIs (i.e. neurons) by similarity of neural activity patterns across multiple timescales, using k-means clustering, sorting by asymmetric similarity as defined by peak cross-correlations at non-negative timelags, and upsampling of individual cluster activities in principal components space to allow sorting within clusters (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv). Here, sorting with Rastermap revealed clustering of neurons with clearly distinguishable activity patterns that exhibited heterogeneous relationships to the behavioral primitive arousal/movement variables of walk speed, whisker motion energy, and pupil diameter (<xref ref-type="fig" rid="fig4">Figure 4a, top, and b</xref>; <xref ref-type="video" rid="video8">Video 8</xref>).</p><p>The activity generated by these thousands of neurons was clearly not random, with sub-groups of cells exhibiting similar activity patterns. Many neurons exhibited activity that appeared to be coupled to walking/whisking (<xref ref-type="fig" rid="fig4">Figure 4a</xref> top, b). It has previously been shown that the first principal component of cortical neuronal activity is highly correlated with movement (<xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>). Indeed, sorting our neurons based on degree of activity loading onto their first two principal components (i.e. neurons at the top of the sort have the largest amount of activity accounted for by the first principal component of activity across the entire population) revealed several distinct patterns of activity. The first principal component, PC1 (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, middle) demonstrated a clear relationship to walking/whisking, such that some neurons were strongly activated in association with movement (e.g. <xref ref-type="fig" rid="fig4">Figure 4a and c1 in PC1, middle, and c, top</xref>), while other neurons appeared to be deactivated during movement bouts (e.g. <xref ref-type="fig" rid="fig4">Figure 4a and c2 in PC1, middle, and c, bottom</xref>).</p><p>Closer examination revealed that some manually selected Rastermap and PC1 clusters were active primarily during walking (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, top, d1, <xref ref-type="fig" rid="fig4">Figure 4a</xref>, middle, c1, and <xref ref-type="fig" rid="fig4">Figure 4</xref>, d1) and/or whisking, with either transient or sustained activity, while others were either active during low arousal periods (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, middle, c2) or appeared to display a combination of ON/OFF transient whisking-related activity (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, top, d2, and <xref ref-type="fig" rid="fig4">Figure 4d</xref>, right; see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). In the illustrated example, alignment of marked increases and decreases in bulk population neural activity to eight brief walking bouts and whisking bouts in the absence of walking (red and blue arrowheads, respectively; <xref ref-type="fig" rid="fig4">Figure 4a–c</xref>), both of which were accompanied by pupil dilations, was more clearly seen following ROI sorting by first and second principal components (<xref ref-type="fig" rid="fig4">Figure 4a</xref> middle and bottom; PC1 and PC2, respectively, <xref ref-type="fig" rid="fig4">Figure 4c</xref>) than in Rastermap sorting (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, top). In particular, PC1 seemed to sort neurons by transient walk and whisk activity, while PC2 appeared to sort neurons according to a more sustained component of whisking activity.</p><p>Interestingly, separation of the top and bottom fifth of neurons in the example data set sorted by PC1 (<xref ref-type="fig" rid="fig4">Figure 4a</xref> middle, c1 and c2, respectively; <xref ref-type="fig" rid="fig4">Figure 4c</xref> top and bottom, respectively) showed large groups of neurons with clear positive (<xref ref-type="fig" rid="fig4">Figure 4c1</xref>, top) and negative (<xref ref-type="fig" rid="fig4">Figure 4c2</xref>, bottom) correlations with arousal/movement, as demonstrated by alignment of rasterized neural activity with walk and whisker transients, and local maxima in pupil diameter (see <xref ref-type="fig" rid="fig4">Figure 4b</xref>). These separate groups of ‘ON’ and ‘OFF’ neurons were not as well identified when sorting by PC2. Specifically, neurons with high PC1 values (<xref ref-type="fig" rid="fig4">Figure 4c1</xref>) showed elevated activity during periods of increased arousal/movement, while neurons with low PC1 values (<xref ref-type="fig" rid="fig4">Figure 4c2</xref>) showed depressed activity levels during these periods. Subsets of these neurons showed either elevated basal activity with a suppression of activity during periods of enhanced arousal and/or movement (<xref ref-type="fig" rid="fig4">Figure 4c2</xref>, top of Rastermap), or transient increased activity leading up to movement/arousal (<xref ref-type="fig" rid="fig4">Figure 4c2</xref>, bottom of Rastermap), followed by transient suppression (see <xref ref-type="video" rid="video8">Video 8</xref>).</p><p>Examination of individual ‘raw’ dF/F traces confirmed that these differences were not due to artifacts of z-scoring or superneuron averaging (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv) in the PC and Rastermap sorted displays (not shown). The term ‘superneuron’ here refers to the fact that when Rastermap displays more than ~1000 neurons it averages the activity of each group of adjacent 50 neurons in the sort to create a single display row, to avoid exceeding pixel limitations of the display. Each single row representing the average activity of 50 neurons is referred to as a ‘superneuron’ (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>; bioRxiv).</p></sec><sec id="s2-9"><title>Spatial distribution of neurons related to movement/arousal</title><p>Performing simple correlations between dF/F in each neuron with either whisker movements, walking, or pupil diameter revealed a broad range of values (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2b</xref>; whisk –0.4 to +0.6, pupil –0.6 to +0.6, and walk –0.2 to +0.4; same mouse as <xref ref-type="fig" rid="fig4">Figure 4</xref>, but a different session). The population distributions of these correlations were highly skewed (positive skew for whisk and walk, negative skew for pupil diameter) with modes at small positive values (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a, b</xref>). Plotting the spatial location of neurons by correlation value within the cortical map area revealed a broad distribution across the dorsal cortex of neurons whose activity was either strongly negatively or positively (or in between) correlated with walking, whisking, and/or pupil diameter, inclusively across every CCF subregion imaged (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a–c</xref>).</p><p>At the local spatial scale (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a</xref>, colored dots indicate individual neurons), neurons whose GCaMP6s activity was strongly positively correlated with movement or pupil diameter could often be found near neurons whose activity exhibited a wide variety of correlations, from strongly positive to strongly negative. Plotting the average correlation value for neurons within a CCF region revealed modest yet statistically significant (see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2c</xref>, legend) spatial heterogeneities (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) that were evident both in each of two different single sessions (<xref ref-type="fig" rid="fig4">Figure 4e</xref>; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a, b</xref>) and across sessions (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2c</xref>, mean of 8 sessions from the same mouse).</p><p>To examine the spatial distribution of different Rastermap groups, we created heatmaps of the density of cells in each CCF area that belong to a particular Rastermap group (i.e. percent of cells in each region belonging to that Rastermap group; <xref ref-type="fig" rid="fig4">Figure 4d</xref>; only areas with at least 20 cells were considered). A Rastermap group with increased activity in relationship to walking and whisking bouts (Rastermap cell group d1 in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, top, and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2a</xref>) had a CCF density distribution ranging from 0 to 25% and was concentrated in select motor, somatosensory and visual subareas (MOs, SSn, VISrl, and VISa; CCF neural density map; <xref ref-type="fig" rid="fig4">Figure 4d</xref>, left). Here, CCF density distribution refers to the full set of neural density percentages (i.e. percentage of neurons in each CCF area that belong to a particular Rastermap group) across all CCF areas present in a given preparation. A second example Rastermap group, d2 (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, top, and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2c</xref>) exhibited increases in fluorescence in relationship to whisking, but not as strikingly during bouts of walking. This cell group had a CCF density distribution ranging from 0 to 13% and was concentrated in select somatosensory and auditory subareas (SSun, SSb, and AUDd; <xref ref-type="fig" rid="fig4">Figure 4d</xref>, right).</p><p>Next, we compared, across all CCF areas, the proportion of neurons within each CCF area that exhibited large positive correlations with walking speed and whisker motion energy. In this example dorsal mount session, walk and whisk related neurons formed the largest relative proportion of cells in select CCF areas such as motor and somatosensory subareas (MOs, SSll, and SSn; <xref ref-type="fig" rid="fig4">Figure 4e</xref> left and right, respectively). Mean correlations for each CCF area were generally statistically significantly greater than zero (see <xref ref-type="fig" rid="fig4">Figure 4e</xref>, legend).</p></sec><sec id="s2-10"><title>Modulation of neural activity by spontaneous movements and arousal as observed with the side mount preparation</title><p>To extend the potential diversity of neural activity patterns during spontaneous behavior, and to examine the generality of the observation of movement/arousal related neuronal activity to the lateral cortex, we monitored neuronal activity in spontaneously behaving mice with our lateral window preparation. We imaged ~85 sessions in the side mount preparation (20–90 minutes each, including both large- and multi-FOV sessions) across 11 GCaMP6s mice, for a total of ~320,000 neurons (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2e</xref>; <xref ref-type="video" rid="video10">Video 10</xref>). In an example imaging session (<xref ref-type="fig" rid="fig5">Figure 5</xref>) of a mouse implanted with our custom 3D-printed titanium side mount headpost (<xref ref-type="fig" rid="fig1">Figure 1e</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1f</xref>) and 10 mm radius bend cranial window (<xref ref-type="fig" rid="fig1">Figure 1e</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a, d, f</xref>; <xref ref-type="video" rid="video1">Video 1</xref>), we were able to image 5678 neurons at 5x5 µm/pixel resolution over a combined 5.0x4.62 mm FOV (<xref ref-type="fig" rid="fig5">Figure 5</xref>). For the ScanImage online fast-z motion correction documentation corresponding to this example session, see the .csv files for 3056_200924_E235_1: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>. Note here that, although there were no neurons present in the primary auditory cortex in the example session in <xref ref-type="fig" rid="fig5">Figure 5</xref>, such neurons were present in other sessions from the same mouse (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d–f</xref>), and in other side mount preparation mice in general (e.g. <xref ref-type="fig" rid="fig3">Figure 3d</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video10.mp4" id="video10"><label>Video 10.</label><caption><title>Side mount.</title><p>Same as in <xref ref-type="video" rid="video8">Video 8</xref>, but for an example session from a side mount mouse instead of a dorsal mount mouse. Data from a100 s contiguous segment of single session, played at 3x original speed (3.06 Hz 2-photon acquisition, bidirectional scanning with 3-frame running average applied). Right cortex is shown (5 x 4.62 mm total at 5 µm resolution per pixel in x and y), with anterior at front (top), auditory cortex at bottom right, and midline at left edge of frame. Behavioral videography shown is taken from the right side and shows the entire front end of the mouse including torso, paws, ears and face. Time scaling is the same as in <xref ref-type="video" rid="video8">Video 8</xref>.</p></caption></media><p>As in the dorsal mount (<xref ref-type="fig" rid="fig4">Figure 4</xref>), Rastermap sorting of neurons across this single side mount example session (<xref ref-type="fig" rid="fig5">Figure 5</xref>) was performed in order to further explore the broad range of relationships between movement/arousal measures and neural activity. As in the dorsal mount preparation, Rastermap sorting showed non-random patterns of neuronal activity that were often related to movement/arousal measures. Here, we identify and further characterize both a cluster with increased activity in relationship to bouts of walking/whisking (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, top, cell group d1; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a, b</xref>), and another manually selected neural ensemble whose activity appeared, by eye, to be more related to whisking than walking, per se (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, top, cell group d2; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c, b</xref>). Interestingly, these two groups consisted of ~300 neurons each spread across the right lateral cortex, with some neurons nearby in the same CCF areas, and others in distant, functionally distinct regions (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, left, right).</p><p>Sorting by the first principal component (PC1) of activity, as in our dorsal mount preparation (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, middle), revealed that a majority of imaged neurons exhibited activity related to bouts of walking/whisking (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, middle; walk bouts indicated by red arrowheads, whisk bouts indicated by blue arrowheads; see <xref ref-type="video" rid="video10">Video 10</xref> and <xref ref-type="video" rid="video11">Video 11</xref> ). The top fifth of these movement-activated neurons showed activity nearly completely dominated by walk-related signals (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, middle, c1, and 5 c, top, (1)), while the bottom fifth showed activity negatively correlated with walking (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, middle, c2, and 5 c, bottom, (2)). Sorting by the second principal component (PC2), on the other hand, appeared to show increases in activity more closely aligned to the onset of whisking movements (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, bottom). Interestingly, some neurons in PC2 exhibited sustained inter-walk bout activation (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, bottom).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-94167-video11.mp4" id="video11"><label>Video 11.</label><caption><title>Example simultaneous dual color mesoscope acquisition shows independence of activity-driven Ca<sup>2+</sup> transients from movement artifacts.</title><p>(a), (c) Simultaneous dual color acquisition of activity-independent EYFP (top) and activity-dependent RCaMP (bottom; Thy1-RGECO). Dashed white circles indicate ROIs whose mean pixel intensities are shown in (b), (d). (b) Mean raw pixel intensity for the two ROIs shown in (a). The blue arrow indicates an example of blanking during 473 nm optogenetic stimulation (i.e. PMT is shuttered during 473 nm laser stimulation, so that section of trace is removed because it does not reflect actual fluorescence activity from the mouse cortex), and the red arrow shows an example of walking-induced movement of the imaged FOV. (d) Same as in (b), but for 3 ROIs shown in (c). Note that the magnitude of the movement artifact related transient in the Ca<sup>2+</sup> indicator fluorescence (i.e. the red channel, PMT2) is relatively unchanged relative to adjacent peaks in ROI1, an identified neuron, and is very small or nonexistent in other neuropil and blood vessel ROIs, and in the yellow anatomical channel (PMT1). PMT = photomultiplier tube.</p></caption></media><p>As in our dorsal mount example session (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a–c</xref>), we examined the overall spatial pattern of correlations in a side mount example session, which in this case had 3897 recorded neurons (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d–f</xref>). Here, correlations of individual neuron activity with whisker motion energy, pupil diameter, and walk speed ranged from –0.6 to +0.6 (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d, e</xref>; whisk –0.4 to +0.6, pupil –0.4 to +0.4, and walk –0.2 to +0.5) with positively skewed distributions (except for pupil diameter) and modes at small positive values (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d, e</xref>). Strong heterogeneity of these correlations, as in the dorsal preparation example (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a–c</xref>), was evident at both local (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d, e</xref>, single session) and regional (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2f</xref>, mean of 8 sessions from the same mouse) spatial scales.</p><p>Examining the regional variations in mean correlations between neural activity and walk speed (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2f</xref>, bottom, in red), pupil diameter (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2f</xref>, middle, in gray) and whisker motion energy (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1f</xref>, top, in blue) showed that neurons with high correlations to arousal/movement tended to concentrate in M1 and somatosensory upper and lower limb (as one might expect for movement related activity) and dorsal auditory areas in this example session (dark red, gray, and dark blue, respectively), while neurons with lower correlations tended to concentrate in somatosensory barrel, nose, and mouth regions (light red, gray, and blue, respectively). As in the dorsal preparation (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2c</xref>), the mean correlations across 8 sessions from this example side mount mouse (i.e. the grand mean for each CCF area, equal to the mean of all means for that area across all 8 individual sessions) were general statistically significantly greater than zero (see <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2f</xref>, legend).</p></sec><sec id="s2-11"><title>Behavioral video analysis and neurobehavioral alignment</title><p>From the above results, we can see that there are clear, but complex, relationships between behavior and neuronal activity across the dorsal and lateral cortex of the mouse. Principal component analysis of this activity suggests that one major factor is variations in behavioral state, as indicated by movement/arousal (see also <xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>). Indeed, previous studies have demonstrated that the use of dozens or more of the top principal components of the motion energy of facial movement videos is beneficial in explaining variance in visual cortical activity (<xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>). However, beyond the first few principal components of facial movement, the precise behavioral meaning of the subsequent components are difficult to discern (<xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>).</p><p>Here, we took a different approach by using a multi-step semi-automated analysis pipeline to identify higher order behavioral motifs and to examine the relationship of these motifs to the diversity of patterns of neural activity observed in the cortex.</p><p>To test the feasibility of this approach, we first created pose estimates consisting of (x,y) positions of a set of user-labeled body-parts and joints from our left, right, and posterior mouse videos using DeepLabCut (DLC; <xref ref-type="bibr" rid="bib33">Mathis et al., 2018</xref>; see <xref ref-type="video" rid="video1">Video 1</xref>). These pose estimate outputs were used for unsupervised machine learning extraction of behavioral motifs by an algorithm called ‘Behavioral segmentation of open field in DeepLabCut’ (‘B-SOiD’; <xref ref-type="bibr" rid="bib23">Hsu and Yttri, 2021</xref>; see <xref ref-type="video" rid="video10">Video 10</xref>). To identify high-level patterns of spontaneous head-fixed mouse behavior, we next performed manifold embedding (‘UMAP’) and cluster identification (‘HDBSCAN’) of DLC pose tracking outputs in BSOiD. A random-forests machine classifier was trained with these clusters to be able to predict, or identify, behavioral motifs in any test session with the same set or subset of labeled body part pose estimates (i.e. x-y positions for a certain number of body parts with the same names), either within or between mice.</p><p>In an example side mount session (same session as <xref ref-type="fig" rid="fig5">Figure 5</xref>), we identified 16 BSOiD behavioral motifs that exhibited good coarse and fine alignment to the dynamics of behavioral arousal primitives (<xref ref-type="fig" rid="fig6">Figure 6a</xref>), including walking, and manually selected discrete Rastermap groups from the same session. Here, BSOiD was trained on a randomly selected 80% subset of four sessions from the same mouse across three days, including the target session itself, with minimum cumulative motif times set to 1% of total video time or ~100 s, and then used to label individual video frames from the entirety of the target session. We found that, in some cases, training on individual sessions resulted in poor identification of rare and or brief behaviors, such as walking (which occurs rarely in some sessions; not shown). Initial attempts to train BSOiD on sessions across mice failed, likely due to differences in their behavioral repertoires. However, we were able to identify a subset of repeating behavioral motifs across mice. Also, the number of motifs per session and per mouse were comparable and tended to exhibit state transitions on similar timescales.</p><p>Examination of four types of qualitatively identified behavioral epochs (‘twitch’, ‘whisk’, ‘walk’, and ‘pupil oscillate’) at a finer temporal scale showed good alignment of BSOiD motifs with fine-scale features of spontaneous head-fixed mouse behavior at the level of movement and arousal primitives (<xref ref-type="fig" rid="fig6">Figure 6a and b</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). For example, balancing wheel movements during ‘twitch’ seem to align well to flickering into BSOiD motifs 8 and 11, while flickering into motif 13 during ‘walk’ appears to correspond well to individual periods of high walk speed during the extended walking bout (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>). The qualitative labels that best describe these behaviors (‘twitch’, <xref ref-type="fig" rid="fig6">Figure 6a–c</xref>; ‘whisk’, <xref ref-type="fig" rid="fig6">Figure 6a–c</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>; ‘walk’, <xref ref-type="fig" rid="fig6">Figure 6a–c</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>; ‘pupil oscillate’, <xref ref-type="fig" rid="fig6">Figure 6a–c</xref>) did not appear to align with or be clearly explained by simple ‘first-order’ interaction effects of movement and arousal primitives (i.e. whisk/walk/pupil x ON/OFF), despite the fact that the above-mentioned, closer, albeit preliminary, analysis of fine-scale behavioral features revealed precise and reliable alignments. This suggests that these methods will yield novel and important insights into the organization of neurobehavioral alignment across cortex, because other factors that are currently poorly understood appear to be contributing to the alignments.</p><p>Alignment of the bulk activity of all neurons in each Rastermap group with BSOiD motif transitions for this example session (<xref ref-type="fig" rid="fig6">Figure 6a–c</xref>, colored, vertically oriented dashed boxes) showed differential alignment with the above-mentioned four high-level, manually identified qualitative behaviors (‘twitch’, ‘whisk’, ‘walk’, and ‘pupil oscillate’). Note that, although individual BSOiD motifs could occur across multiple qualitatively identified higher level behaviors, each behavior consisted of a repeating pattern of a restricted set of motifs. For example, motif 13 in this session seems to correspond to a syllable that might best be termed ‘symmetric and ready stance’. This tended to occur just before and after walking, but also during rhythmic wheel balancing movements that appear during the ‘oscillate’ behavior.</p><p>The occurrences of these behaviors, then, were clearly aligned with both behavioral movement/arousal primitives (<xref ref-type="fig" rid="fig6">Figure 6b</xref>) and the activity of putative, qualitatively segmented neural ensembles (<xref ref-type="fig" rid="fig6">Figure 6c–d</xref>) that displayed varied spatial distributions across defined CCF areas of the side mount preparation (<xref ref-type="fig" rid="fig6">Figure 6e</xref>).</p><p>For example, the qualitatively identified high-level behavior‘whisk’, which in this case corresponds to whisking in the absence of walking (<xref ref-type="fig" rid="fig6">Figure 6a and b</xref>), with relatively elevated pupil diameter, aligns with strong activity in Rastermap group 5 (<xref ref-type="fig" rid="fig6">Figure 6c–d</xref>, white dashed oval corresponding to Rastermap group #5, mean trace in blue), which has a higher density of neurons in retrosplenial, secondary visual, somatosensory barrel, and dorsal auditory CCF areas (<xref ref-type="fig" rid="fig6">Figure 6e</xref>, R5). On the other hand, the qualitatively identified high-level behavior ‘walk’, which in this case refers to neurons with strong, transient activity aligned to seven individual bursts of walking during an extended walking bout, aligns with strong activity in Rastermap groups 6 and 7 (<xref ref-type="fig" rid="fig6">Figure 6c–d</xref>, white dashed oval corresponding to Rastermap groups #6 and 7, mean trace in red), which has a higher density of neurons in primary and secondary motor cortices, upper and lower limb somatosensory cortices, and primary visual cortex.</p><p>Note that the qualitative behaviors identified in <xref ref-type="fig" rid="fig6">Figure 6</xref> do not correspond directly to the Rastermap group labels used in earlier figures to point out possible alignment between neural activity and behavioral primitives. For example, the ‘whisk’ group in <xref ref-type="fig" rid="fig5">Figure 5d</xref>, right, corresponds to a subset of the neurons with strong neural activity aligned to the ‘twitch’ behavior in <xref ref-type="fig" rid="fig6">Figure 6c</xref> (R3 and R4), and the walk and whisk group in <xref ref-type="fig" rid="fig5">Figure 5d</xref>, left, corresponds to a subset of the neurons with strong neural activity aligned to the ‘walk’ behavior in <xref ref-type="fig" rid="fig6">Figure 6c</xref> (R6 and R7). Furthermore, these single session results are preliminary and are not intended, at this point, as a result that necessarily generalizes across sessions or across mice. Rastermap maximum neuron densities can be found in the corresponding figure legend, but we did not apply tests of statistical significance here for these reasons. However, with ~30 CCF areas, a uniform distribution of neurons in a given Rastermap group would be ~3.3% per area. One would expect, therefore, that some of the neural density values we observed, which ranged up to ~40% for area M2 in Rastermap group 7 corresponding to the ‘walk’ qualitative behavior, would be highly significant if they were to be observed consistently across sessions.</p><p>We expect that, in general and based both on our preliminary results and emerging trends in existing literature, that the predictive contribution of high level behavioral motifs relative to movement and arousal primitives will be greater both in lateral cortical areas, such as A1, in non-primary CCF areas further up the cortical hierarchy, such as secondary sensory cortices and M2 (<xref ref-type="bibr" rid="bib57">Wang et al., 2023</xref>; bioRxiv), and over intermediate and longer timescales of activity aligned with purposeful and/or goal oriented movements organized at higher levels of the behavioral hierarchy (<xref ref-type="bibr" rid="bib36">Mimica et al., 2023</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Overview and main findings</title><p>We developed novel headpost designs, surgical procedures, 3D-printed accessories, experimental protocols, and analysis pipelines that allowed us to routinely perform 2p imaging of up to ~7500 neurons simultaneously across ~25 mm<sup>2</sup> of either bilateral parieto-occipital (‘dorsal mount’) or unilateral temporo-parietal (‘side mount’) cortex in separate populations of awake, behaving mice, while simultaneously monitoring an array of global arousal state variables. Correlations between neural activity and arousal/movement were broadly, but heterogeneously, represented across both local and regional spatial scales. Preliminary analysis showed that this rich dataset could be used to segregate specific mappings between high-level, qualitatively identified behaviors (e.g. ‘twitch’, ‘whisk’, ‘walk’, and ‘pupil oscillate’; <xref ref-type="fig" rid="fig6">Figure 6a</xref>), each consisting of a persistent pattern of robustly identifiable, high-level behavioral motifs, low-level behavioral arousal, and movement primitives (i.e. walk, whisker, and pupil; <xref ref-type="fig" rid="fig6">Figure 6b</xref>), and spatially localized neural activity clusters (i.e. ‘Rastermap’ groups; <xref ref-type="fig" rid="fig6">Figure 6c–e</xref>). Further analysis of such mappings will both allow for the fine dissection of patterned behaviors and neural activity, and for the development of a powerful framework for the sophisticated prediction of performance in mice engaged in a variety of multimodal sensory discrimination tasks (see <xref ref-type="bibr" rid="bib25">Hulsey et al., 2023</xref>; bioRxiv, and <xref ref-type="bibr" rid="bib57">Wang et al., 2023</xref>; bioRxiv).</p><p>Taken together, these findings show the strong potential of our novel methods for further elucidation of the principles of pan-cortical neurobehavioral alignment at the level of densely sampled individual neurons. For example, the spatial distributions of densities of neurons in each Rastermap (<xref ref-type="bibr" rid="bib51">Stringer et al., 2023</xref>, bioRxiv) group across CCF areas were partially overlapping, yet strongly dissociable (see <xref ref-type="fig" rid="fig4">Figures 4d</xref>—<xref ref-type="fig" rid="fig6">6e</xref>). This suggested that dynamic, recurring patterns of arousal-dependent reorganization might act to enable activity mode switching between various distributed, cortical functional ‘communities’. Each of these communities, then, could be specialized for different forms of activity related to various aspects of spontaneous behavior or task performance. This topic should be further explored with principled statistical techniques for recursive, top-down hierarchical community detection (<xref ref-type="bibr" rid="bib31">Li et al., 2020</xref>).</p><p>In summary, we have shown here that it is feasible to monitor individual neuronal activities across broad expanses of the cerebral cortex and to perform neurobehavioral alignment of high resolution behavioral arousal state motifs and pan-cortical, activity-clustered neural ensembles in awake, behaving mice. Furthermore, our preliminary findings suggest a detailed alignment between both arousal/movement primitives and high-level behavioral motifs that appears to vary across broad regions of cortex (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). These findings extend those of earlier studies that showed widespread encoding of behavioral state during spontaneous behavior in visual cortex (<xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref> ; imaging and Neuropixels recordings) and encoding of uninstructed movements during task performance both across dorsal cortex (<xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref> ; widefield imaging across dorsal cortex, 2p imaging in restricted FOVs in primary visual and secondary motor cortex) and brain-wide during task performance (<xref ref-type="bibr" rid="bib48">Steinmetz et al., 2019</xref>; Neuropixels recordings at low volumetric sampling density).</p><p>In particular, our findings are consistent with those of <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref> in that we observe widespread, strong correlations of cortical neural activity with the behavioral state variables of movement (facial and locomotor) and pupil diameter. However, we imaged larger areas of cortex simultaneously, including medial, anterior, and lateral cortical areas, at a higher overall imaging frequency (~3 Hz vs. ~1 Hz). Although admittedly this imaging speed is still not fast enough to record fast cortical encoding of sensory information or decision making, these factors, taken together, may have contributed to our ability to observe enhanced heterogeneity across these non-primary cortical areas, including in neurons whose activities were strongly suppressed during periods of increased arousal/movement (see <xref ref-type="fig" rid="fig4">Figure 4c</xref>, 2). Higher concentrations of such neurons were evident over lateral regions of cortex such as primary auditory and somatosensory nose and mouth regions, and in anterior regions including secondary motor cortex.</p><p>We also showed that high-level behaviors, manually identified and aligned to clusters of repeating motifs by UMAP embedding (BSOiD; <xref ref-type="bibr" rid="bib23">Hsu and Yttri, 2021</xref>), aligned well with patterns of neural activity identified by Rastermap corresponding to neural ensembles with non-uniform spatial distributions (<xref ref-type="fig" rid="fig6">Figure 6</xref>). This suggests that encoding of behavioral state across cortex may occur at multiple levels of the behavioral hierarchy, in addition to that of ‘low-level’ behavioral arousal/movement primitives such as pupil diameter, whisker motion energy, and walking speed (see <xref ref-type="bibr" rid="bib36">Mimica et al., 2023</xref>).</p><p>Our finding that neurons with no correlation to arousal/movement, or with a large negative correlation, are also widespread across cortex, suggests that multiple mechanisms or pathways may exist in parallel to distribute information about behavioral state across the brain. Interestingly, as evidenced by our discovery with Rastermap of neural ensembles exhibiting diverse response kinetics and polarities (<xref ref-type="fig" rid="fig4">Figure 4a, top, d</xref>, <xref ref-type="fig" rid="fig5">Figure 5a, top, d</xref>, and <xref ref-type="fig" rid="fig6">Figure 6c–e</xref>), these pathways may be activated with different temporal dynamics simultaneously across various brain areas, thus making our approach for simultaneous recording of many areas one of the keys to potentially understanding the nature of their neuronal activity. The reason for this is that correlations between behavior and neural activity across different cortical regions appear to depend on the exact time since the behavior began (<xref ref-type="bibr" rid="bib46">Shimaoka et al., 2018</xref>). Thus, the distribution of behavioral state dwell times must be the same across the recording epochs corresponding to all cortical areas in order to detect distributed patterns of correlations between behavior and neural activity. In practice this would seem to require the simultaneous recording of all the individual neurons across the dorsolateral cortex that are part of the neural ensembles identified by Rastermap, as we have done here - accurately piecing such ensembles together across multiple separate recordings might not be possible.</p><p>For this reason, directly combining and/or comparing the correlations between behavior and neural activity across regions imaged in separate sessions may not reveal the true differences in the relationship between behavior and neural activity across cortical areas, due to a ‘temporal filtering effect’, because the correlations between behavior and neural activity in each region appear to depend on the exact time since the behavior began (<xref ref-type="bibr" rid="bib46">Shimaoka et al., 2018</xref>). In our view, this makes the simultaneous recording of multiple cortical areas essential for proper comparison of the dependence of their neural activity on arousal/movement, because only then are the distributions of behavioral state dwell times the same across the recording epochs corresponding to all cortical areas.</p><p>Additional experiments are warranted to uncover the mechanistic basis of such differences across large populations of cortical neurons associated with changes in activity level during periods of fluctuating arousal/engagement. For example, correlations between spiking-related Ca<sup>2+</sup> activity and movement/arousal under 2p imaging, both as shown here and in previous studies (<xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>), and Neuropixels recordings (<xref ref-type="bibr" rid="bib48">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Stringer et al., 2019</xref>), appear not to mirror those observed with widefield voltage imaging of membrane potential (<xref ref-type="bibr" rid="bib46">Shimaoka et al., 2018</xref>; especially in VIS_lm and SS_b). This suggests the existence of a complex interaction between movement/arousal related changes in membrane potential and spiking that may vary across cortical areas.</p><p>In general, our preparations allow for the observation of diverse pan-cortical neural populations that likely communicate with each other in a transient and/or sustained manner during alternating periods of low and high movement/arousal and drive different patterns of global functional connectivity. Furthermore, the apparent variability of these neurobehavioral alignments suggests that interactions between cognitive and arousal/movement encodings may also occur during spontaneous behavior, in addition to during task performance (<xref ref-type="bibr" rid="bib38">Musall et al., 2019</xref>), and that the degree of this interaction may depend on the cortical area.</p></sec><sec id="s3-2"><title>Advantages and disadvantages of our approach (with potential solutions)</title><p>A main advantage of our approach is the flexibility afforded by the combination of our two surgical preparations (see Protocols II, III), in terms of the sheer number of cortical areas spanning distant regions that can be recorded simultaneously at single cell resolution under 2p imaging. The interoperability of our preparations across widefield 1p and Thorlabs mesoscope 2p imaging rigs (see Protocol IV) allows for the straight-forward development of an experimental pipeline that first surveys widespread activity at fast timescales (widefield,~10–50 Hz) and then investigates the activity of large subregions in a serial manner at intermediate timescales (Thorlabs mesoscope,~1–10 Hz).</p><p>A drawback of our current preparations is that they require use of a large water-immersion objective with a relatively short working distance (~2.4–3.0 mm). This limits access for simultaneous electrophysiological recordings during mesoscale imaging, even though the same preparations could be used for simultaneous imaging and electrophysiological recordings on other acquisition rigs (i.e. widefield and/or standard 2p), used in parallel to the mesoscope with the same mice.</p><p>In tandem with our headpost, surgical, behavioral, and analytic protocols, this could enable simultaneous Neuropixels, whole-cell electrophysiology, and mesoscale single-cell resolution imaging experiments. Such an approach would allow integration of neural data from multiple brain depths and areas, as well as across multiple spatiotemporal scales. Thus, these techniques would allow modification of our current methods to allow examination of faster timescale cortical dynamics than we can currently capture with 2p imaging alone, such as those related to initial encoding of sensory information and decision making.</p><p>Another limitation of our current methodologies for single large FOV imaging, as presented here, is that we imaged neurons at a single cortical depth, and at an imaging rate that did not take full advantage of the speed of the Ca<sup>2+</sup> indicator that we used (i.e. GCaMP6s; we estimate that imaging at ~10 Hz would sample all available information and avoid aliasing with a rise time of ~200ms and decay time of ~1.2 s; <xref ref-type="bibr" rid="bib7">Chen et al., 2013</xref>). To address these limitations, we are expanding our analyses to sessions, which we have already recorded, acquired at ~5–10 Hz over 4–6 FOVs at multiple z-depths (e.g. V1, A1, M2, RSpl, and SS_m/_n cortical areas; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2e</xref>), with resolutions ranging from 0.2 to 1 µm per pixel (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2d</xref>).</p></sec><sec id="s3-3"><title>Future directions</title><p>Reliability of cortical area identification and alignment in our dorsal and side mount preparations is enhanced due to standardization of headposts and cranial windows, and the combined use of skull landmark and cortical vasculature alignment in a rigorous, semi-automated widefield multimodal sensory mapping protocol. This has allowed us to acquire an extensive, standardized neurobehavioral data set (~200 hours of spontaneous behavior with 30 Hz high-resolution face and body video from three cameras,~1,000,000 total neurons at ~3–10 Hz 2p acquisition rates imaged at ~1000 x 1,000 pixels across ~250 sessions, total combined across both preparations and both FOV imaging configurations in 17 mice; see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2e</xref>).</p><p>We are currently working on modifying these preparations to allow combined 2p imaging and whole-cell electrophysiological recordings (i.e. using a Thorlabs Bergamo II microscope) and/or widefield imaging and high-density electrophysiological recordings (i.e. Neuropixels; <xref ref-type="bibr" rid="bib28">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Peters et al., 2021</xref>). This will allow us to use a single preparation in which each mouse can be used to acquire data across multiple spatiotemporal scales during spontaneous and task-engaged behavioral epochs. Such combined recordings, with either Neuropixels or whole cell recording, may soon be possible with mesoscale 2p imaging, either on the Diesel 2p or the Thorlabs 2p mesoscope (<xref ref-type="bibr" rid="bib47">Sofroniew et al., 2016</xref>), by use of a recently developed air-immersion objective (<xref ref-type="bibr" rid="bib65">Yu et al., 2022</xref>, bioRxiv). A similar mesoscale air-immersion 2p objective is also under development at Thorlabs.</p><p>In the future, we hope to expand our analyses of this extensive dataset to include identification of joint Rastermap/BSOiD (or keypoint-MOSEQ motif “syllable”; <xref ref-type="bibr" rid="bib59">Weinreb et al., 2023</xref>; bioRxiv) transitions, and to perform further precision neurobehavioral alignment, using other methodologies such as hierarchical state-space models (Lindermann, S; <ext-link ext-link-type="uri" xlink:href="https://github.com/lindermanlab/ssm">https://github.com/lindermanlab/ssm</ext-link>; <xref ref-type="bibr" rid="bib32">LindermanLab, 2018</xref>), and factorial HMMs (<xref ref-type="bibr" rid="bib17">Ghahramani and Jordan, 1997</xref>). These methods will allow increased accuracy over multiple behavioral timescales and the ability to predict transition or change points between different behavioral and/or neural activity states.</p><p>We will also examine changes in pan-cortical functional connectivity between communities of neurons with similar activity relationships associated with changes in behavioral state with statistical techniques such as Vector Autoregressive Union of Intersections (VA-UoI; <xref ref-type="bibr" rid="bib3">Balasubramanian et al., 2020</xref>; <xref ref-type="bibr" rid="bib43">Ruiz et al., 2020</xref>; <xref ref-type="bibr" rid="bib5">Bouchard and Pabhat Snijders, 2017</xref>) that allow us to rigorously test for and remove putative but ‘false positive’ connections based on coincidental activity correlations. Together, these analysis tools, when combined with the experimental techniques demonstrated here, will enable both the identification of patterns of neurobehavioral alignment between arousal/movement and neural activity that are strongly predictive of high levels of behavioral performance, and the direct probing control of these patterns with targeted optogenetic manipulations.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Mice</title><p>All experiments were approved by the University of Oregon Institutional Animal Care and Use Committee, under protocols 20–12 and 20–14 (IACUC IDs TR202300000012 and TR202300000014). To image cortical activity, we used both male and female, 8–50 week old, CaMKII-tTA x tetO-GCaMP6s (provided by Cris Niell, University of Oregon; JAX # 007004x024742), CaMKII-Cre x Ai148 (GCaMP6f; JAX #005359x030328), and CaMKII-Cre x Ai162 (GCaMP6s; JAX# 005359x031562) mice (Jackson Labs, Allen Institute). Neural activity was typically imageable for up to between 30 and 180 days post-windowing. For experiments with combined Ca<sup>2+</sup> imaging and optogenetic inhibition we used PV-Cre x Ai32 x Thy1-RGECO mice (JAX # 017320x024109 x 030528 or 030527). Habituation of each mouse to the experimental setup was performed for 2–3 days prior to 2p data acquisition during standard protocol for spontaneous behavior.</p></sec><sec id="s4-2"><title>Surgeries</title><p>See Protocols II and III, and Supplementary methods and materials.</p></sec><sec id="s4-3"><title>Behavioral videography</title><p>See Supplementary methods and materials, Overall workflow, Protocol I. See <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref>.</p></sec><sec id="s4-4"><title>Widefield imaging and multimodal mapping</title><p>See Supplementary methods and materials, Overall workflow, Protocol I, and Multimodal mapping. See <xref ref-type="video" rid="video3">Videos 3</xref>–<xref ref-type="video" rid="video7">7</xref>.</p></sec><sec id="s4-5"><title>2-photon imaging and ROI extraction</title><p>See Supplementary methods and materials, ScanImage 2p acquisition. 2p imaging was performed with a Spectra Physics Mai Tai HP-244 tunable femtosecond laser and the Thorlabs Multiphoton Mesoscope. We controlled imaging with ScanImage (2018–2021; Vidrio) running in Matlab, with GPU-enabled online z-axis motion correction. Behavioral synchronization was achieved by triggering 2p acquisition with the first right body camera frame clock signal. Left and posterior cameras were triggered by the same signal, but were subject to variable LabView runtime-related delays on the order of up to ~10 ms and, on rare occaision, dropped frames. Exposure times for both left and right body cameras, as well as frame-clock times for all three body cameras, were recorded in Spike2 (CED Power1401) as continuous waveforms and events, respectively, to allow for temporal alignment of recorded videos.</p><p>We used the ScanImage function SI render to combine image strips from large field-of-view (FOV) sessions, and in some cases we used custom Suite2p Matlab scripts (courtesy of Carsen Stringer) to align small FOVs from the same or different z-planes in a single session. Rigid and non-rigid motion correction were performed in Suite2p, along with region of interest (ROI) detection and classifier screening of ROIs likely to be neurons. Rigid motion correction computes the shift between a reference image and each frame using phase-correlation, and non-rigid correction divides the image into subsections and calculates the shift of each block, or subsection, separately (<xref ref-type="bibr" rid="bib40">Pachitariu et al., 2016</xref>; also: <ext-link ext-link-type="uri" xlink:href="https://suite2p.readthedocs.io/en/latest/registration.html">https://suite2p.readthedocs.io/en/latest/registration.html</ext-link>).</p><p>Fluorescence intensities were calculated as changes in fluorescence divided by baseline fluorescence (dF/F), where F was calculated, for each neuron, using an ~30–60 s (100–200 frame) rolling 10th or 15th percentile baseline of the neuropil subtracted mean ROI pixel intensity (F-0.7*Fneu). Traces of dF/F were then truncated to match the length of acquired behavioral data and resampled (python: scipy.signal.resample for upsampling, or numpy: slicing for downsampling), along with behavioral data, to 10 Hz for further analysis. 2p acquisition rates for the large FOV sessions ranged from roughly 1.6–4.5 Hz, with most sessions acquired at ~3 Hz. ROIs with a Suite2p classifier-assigned cell probability less than 0.5 were excluded from further analysis.</p><p>Behavioral data was acquired with a Power1401 in Spike2 (CED) at 5000 Hz and downsampled to 10 Hz without prior filtering, during preprocessing in Python, from real measurement rates of 100 Hz at the encoder for walk speed, and 30 Hz at the face/body cameras for pupil diameter and whisker motion energy.</p></sec><sec id="s4-6"><title>Pose-tracking analysis (DeepLabCut)</title><p>An example DeepLabCut (DLC) (<ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">https://github.com/DeepLabCut/DeepLabCut</ext-link>; <xref ref-type="bibr" rid="bib11">DeepLabCut, 2018</xref>; <xref ref-type="bibr" rid="bib33">Mathis et al., 2018</xref>) annotated video with 66 labeled points is shown in <xref ref-type="video" rid="video1">Video 1</xref> (bottom right). Video snippets were concatenated, cropped, truncated, and aligned to 2p frames with FFMPEG under the Linux subsystem for Windows (Ubuntu 20.04; Windows 10 Enterprise 2004) or Ubuntu 18.04 running on local network GPU clusters. In the example session shown in <xref ref-type="fig" rid="fig5">Figures 5</xref>, <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, we analyzed the right Teledyne Dalsa camera video (30 Hz, 1020x760 pixel with 2x2 binning, 1” x 1.8” sensor, 90 min video, ffmpeg crop compression to 525.12 MB), which was natively aligned to the 2p acquisition by a common trigger with frameclock and exposure times recorded by a CED 1401 data acquisition device in Spike2 software (Cambridge Electronic Design), running 650 K iterations (i.e. below the default of 1.03 million, to avoid potential overfitting) with the resnet_50 model in DLC (see labeled video, <xref ref-type="video" rid="video2">Video 2</xref>). Some movies had noticeable sporadic motion blur (in particular of whiskers during whisking), small areas of overexposure, and brief periods of paw occlusion or mislabeled points during walking, grooming, and other brief periods of high motor activity. These difficulties could be remedied in the future by increasing sampling frame rate (although this generates larger file sizes), as well as further optimizing IR illumination. For the preliminary analysis contained here, we performed multiple reruns of DLC to achieve an acceptable quality standard, but we did not rigorously employ tracking, exclusion, or smoothing of DLC outputs.</p></sec><sec id="s4-7"><title>Behavioral motif analysis (BSOiD)</title><p>Pose-tracking data outputs from DLC in the form of comma-separated value files were loaded into B-SOiD (<ext-link ext-link-type="uri" xlink:href="https://github.com/YttriLab/B-SOID">https://github.com/YttriLab/B-SOID</ext-link>; <xref ref-type="bibr" rid="bib24">Hsu et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Hsu and Yttri, 2021</xref>) along with concatenated, cropped, and spatially downsampled (FFMPEG) right-camera videos (~20–90 min,~100 MB to 1 GB per video). B-SOiD uniform manifold approximation and projection (UMAP) embedding training was performed on between 1 and 4 sessions from the same mouse. Between 20 and 60 labeling space dimensions were typically assigned per session, with between 75 and 98% of features confidently assigned to between ~2 and 15 motifs per session. Minimum bout time was set at 900 ms, and minimum cumulative motif time per session was set at ~1–5 min.</p><p>Pixel-wise analysis of cumulative motion energy showed that the motifs identified by B-SOiD differed in terms of body-region localized basic movement patterns in a manner that was consistent with human-annotated categorization labels. Thresholding of cumulative motion energy by z-score further confirmed this and made the following kinematic patterns evident: (i) In asymmetric pose motifs, a clear region of low movement (dark pixels) is visible in the chest region between the left and right shoulders that is either absent or not as prominent in high-arousal and symmetric pose motifs; (ii) Enhanced nose and mouth movement was associated with high arousal walking but could also occur independently of walking; and (iii) Increased whisker movement was associated with high arousal walking but could also occur independently of walking.</p></sec><sec id="s4-8"><title>Protocol I</title><sec id="s4-8-1"><title>Overall workflow</title><list list-type="order"><list-item><p>Headpost implantation in mice between 8 and 12 weeks postnatal (male preferred due to larger skull size). 60 minutes under isoflurane. (90% survival)</p></list-item><list-item><p>Recovery for 3–7 days. (90% survival)</p></list-item><list-item><p>Multimodal mapping with widefield 1p Ca<sup>2+</sup> imaging on day 2 or 3 post-surgery. (100% survival)</p></list-item><list-item><p>Window implantation surgery. 60-90 minutes under isoflurane. (75% survival)</p></list-item><list-item><p>Recovery for 7–10 days. (90% survival)</p></list-item><list-item><p>Habituation to head-fixation and wheel (2–3 days). (100% survival)</p></list-item><list-item><p>Spontaneous behavior mesoscope imaging sessions (90 min each). At least three with [5000x660 µm] x 7 FOVs at ~3 Hz, and at least three with [660x660 µm] x 4 FOVs at ~10 Hz. Interleaved with passive auditory and passive visual stimulation sessions. Up to 3 sessions per day, for a total of 6–9 days of imaging (2–3 weeks). (100% survival)</p></list-item><list-item><p>Mice trained in the behavioral tasks (~3–6 weeks). (67% success)</p></list-item><list-item><p>Mice imaged on mesoscope while performing multimodal behavior (3–5 sessions over 1–2 weeks). (90% success)</p></list-item><list-item><p>Mice imaged on mesoscope, with targeted optogenetic inhibition of cortical subregions, while performing multimodal behavior (3–5 sessions over 1–2 weeks). (90% success)</p></list-item></list><list list-type="simple"><list-item><p>Total time (t0=surgery 1): 15 weeks (~3.5 months)</p></list-item><list-item><p>Age range of mouse: 8–27 weeks postnatal</p></list-item><list-item><p>Overall survival rate to stage 10: 0.9 * 0.9 * 0.75 * 0.9=55%</p></list-item><list-item><p>Overall success rate to stage 10: 0.9 * 0.9 * 0.75 * 0.9 * 0.67 * 0.9 * 0.9=30%</p></list-item><list-item><p>Estimated initial sample size of mice needed to get N=6 for spontaneous and passive: 10</p></list-item><list-item><p>Estimated initial sample size of mice needed to get N=6 for behavior: 20</p></list-item></list><p>*Note: Perform additional widefield multimodal mapping sessions and vasculature to Allen Common Coordinate Framework (CCF) coalignment and registration approximately once every 4 weeks or as needed.</p></sec></sec><sec id="s4-9"><title>Protocol II</title><sec id="s4-9-1"><title>Headpost surgery: ‘Side mount’</title><list list-type="order"><list-item><p>Weigh the mouse and prepare 1 mL ringers, 6.0 mg/kg Meloxicam SR, and 0.5 mg/kg Buprenorphine SR.</p></list-item><list-item><p>Anesthetize the mouse with 2–4% isoflurane in the induction chamber.</p></list-item><list-item><p>Mount mouse in stereotax and rotate 22.5° to the left. Reduce isoflurane to 1.4% at vaporizer, set oxygen flow rate to ~1.5 L per minute.</p></list-item><list-item><p>Apply ophthalmic ointment, adjust mouse position on heating blanket, place temperature probe in rectum, and stabilize temperature at 35.5°C. Re-adjust head and neck position until breathing is regular and not jerky.</p></list-item><list-item><p>Inject Meloxicam SR and Buprenorphine SR subcutaneously.</p></list-item><list-item><p>Shave dorsal and right temporoparietal surface of head and remove residual hair with Nair and surgical spears. Sterilize area three times with alternating cottonswab-applied betadine solution and 70% ethanol wipes. Be careful to clean residual Nair away from eyes with Ringers.</p></list-item><list-item><p>Use sharp-tipped surgical scissors to cut skin along the sagittal line from just behind lambda to the middle of the olfactory bulb. Next, cut along the parasagittal arc exposing 1–2 mm of skull over the left cortex. Make a final parasagittal cut deep over the right side of the skull to just below the auditory cortex, then follow closely around the right eye to join the initial cut over the right anterior snout.</p></list-item><list-item><p>Clip and retract skin around the opening with 4–6 thin ‘bulldog’ clamps. Use #3 forceps tips, cotton swabs, non-woven sponges, and angled broad edge of #23 or #11 scalpel blade to remove periosteum, clean surface of skull, and score expected headpost contact surface.</p></list-item><list-item><p>Make a parasagittal cut separating temporal muscle from right parietal ridge, then use blunt back edge of 90 degree angled hook and broad edge of scalpel to tease muscle away from ridge along its entire length. While holding muscle with #3 forceps, use fine straight spring scissors to cut away the entire muscle from posterior to anterior along the dorsal surface of the zygomatic process, passing closely behind the eye while taking care to avoid rupturing ophthalmic artery. Move 2 bulldog clamps from ventral (right) skin flap to residual edge of muscle so that entire flat lower section of temporal bone is exposed down to corner interface with zygomatic process. If a bleed erupts, place ringers-soaked Vetspon onto soft-tissue pocket and leave for at least 30 s, then remove carefully with #3 forceps. Inject subcutaneous Ringers as necessary to supplement for lost blood.</p></list-item><list-item><p>Use broad-side of 1.4 mm diameter drill bit to round-off entire length of parietal suture, pick off stray bone shards with #3 forceps, and clean the surface of the skull. If the mouse will be used primarily as widefield preparation, smooth surface additionally with rotary polishing tips.</p></list-item><list-item><p>Clean and score contact surface of headpost, then place over surface of skull in intended attachment position. Medial edge of headpost perimeter should be 1–2 mm left of and parallel to midline. Back edge of headpost should be posterior to lambdoid sutures, front edge of headpost should be over olfactory bulbs, the right eye should be close-in and centered in the eye-loop, and ventral (right) edge should be tight and low over and PAST zygomatic process. Carefully note areas where additional skull surface needs to be exposed and cleaned in order to attach headpost. Also note where the zygomatic process crosses the opening of the headpost (important for the upcoming headpost modification step).</p></list-item><list-item><p>Use the broad edge of scalpel blade to retract soft tissue and periosteum where needed to make room for headpost attachment, reposition bulldog clamps, and clean and dry skull surface with cotton swabs and non-woven sponges.</p></list-item><list-item><p>Place the headpost ventral (right) edge upwards (contact surface facing you) in a small metal crinkle dish. Add UV curable dental cement to the ventral edge, allow gravity to pull it downwards to form a ‘curtain’, UV cure, and repeat until the edge of dental cement runs along the position of the zygomatic process observed in step 9 (above).</p></list-item><list-item><p>Re-check headpost positioning on skull as in step 10 (above), then re-dry skull and headpost contact surfaces. Apply a thin coating of UV curable dental cement (Applicap, 3 M) with the spatula along the entire contact surface perimeter of the headpost and place firmly onto the skull. While applying pressure with one hand, use the other to irradiate dental cement with pulsing ultraviolet (UV) light for ~30 s. Forceps can also be used to secure headpost in position after initial contact is formed. Be careful to keep cement away from eyes.</p></list-item><list-item><p>Apply remaining dental cement (or deploy 2nd capsule as needed) around the interior perimeter of the headpost, filling all gaps with the skull. Use the spatula to remove excess cement from the skull, then UV cure as in step 13 (above).</p></list-item><list-item><p>Use a 1.4 mm diameter drill-bit to remove dental cement along the ventral edge of preparation until a smooth continuous bridge is formed between the zygomatic process, the dental cement, and the headpost.</p></list-item><list-item><p>Apply a thin layer of Zap cyanoacrylate (‘super-glue’) to the entire surface of the skull and UV cure/let-dry for 15–30 min. If the mouse is to be used primarily for wide-field 1p imaging, apply Norland UV curable glue after 1–3 days. In either case, cover Zap with Kwik-SIL (WPI), making sure to include an extended ‘tab’ over the arm of the headpost to enable easy removal.</p></list-item><list-item><p>Remove bull-dog clamps, dry skin and push upward around the exterior perimeter of the headpost. Attach skin to the headpost with Vetbond super-glue. Inject 1 mL ringers subcutaneously, un-rotate and remove mouse from stereotax. Return the mouse to their home cage in a heated incubator unit for at least 24 hr. Inject 1 mL ringers subcutaneously every 24 hr until postoperative weight stabilizes. Total time: 50-70 min.</p></list-item></list></sec></sec><sec id="s4-10"><title>Protocol III</title><sec id="s4-10-1"><title>Cranial window surgery: ‘Side mount’</title><list list-type="order"><list-item><p>Surgery to be performed between 3 and 7 days after headpost implantation, after the initial round of widefield multimodal mapping.</p></list-item><list-item><p>Inject 10 mg/kg Dexamethasone and 10 mg/kg Baytril subcutaneously on the day before surgery and also on the day of surgery, ~2 hr prior to placing the mouse on the stereotax.</p></list-item><list-item><p>Re-weigh the mouse and prepare 0.5 mg/kg Buprenorphine, 17.5 µL of 25% mannitol heated to 30 ° C, and 1 mL ringers.</p></list-item><list-item><p>Place mouse in induction box under 2–4% isoflurane until fully anesthetized and then transfer to stereotax with head rotated 22.5 degrees to the left using adjustable support arm (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>, bottom) mounted on breadboard clamped to the base of the stereotax. Decrease isoflurane to 1.5%. Use one or two 82 degree countersunk 2/56 x 1 ⁄ 4 '' screws (McMaster-Carr, MS51959-3D) to affix headpost to support the arm so that the skull does not move during drilling. Make sure that tightening screws does not move headpost relative to skull by iterative micropositioning of head and support arm.</p></list-item><list-item><p>Apply ophthalmic ointment and insert a rectal temperature probe. Inject Buprenorphine SR and mannitol subcutaneously. Do NOT inject Meloxicam SR, as N-SAIDs are contraindicated for coadministration with Dexamethasone. Place non-woven sponge under rear paws and remove Kwik-SIL from skull/headpost. Sterilize the skull with betadine and 70% ethanol wipes.</p></list-item><list-item><p>Clean 3 custom cranial windows (9 mm radius bend for targeted imaging of ventral-temporal areas and/or imaging of multiple small fields-of-view (FOVs), or 10 mm radius bend for large FOV imaging) by placing them first in a plastic weigh-dish full of 70% ethanol, then rinsing them in ringers and submerging them in ringers in a second plastic weigh-dish.</p></list-item><list-item><p>Dry one window by dabbing on sterile surgical drape, then place on skull preparation, centered inside the headpost perimeter. Use a fine point permanent pen to mark the perimeter of the window on the skull. Remove and re-clean the window.</p></list-item><list-item><p>Prepare 20–30 mL of ice-cold Ringer’s solution. Increase oxygen flow-rate to 1.7 L/min and lower isoflurane to 0.3% to maintain regular breathing and decrease intracranial pressure.</p></list-item><list-item><p>Use a 0.7 mm diameter drill bit to mark corner positions of the window based on pen markings, then to lightly trace the window perimeter. Gently expand the traced perimeter as much as necessary to fit the entire window without removing too much dental cement, as this can destabilize the headpost.</p></list-item><list-item><p>Switch to a 1.4 mm diameter drill bit and remove the next ~80% of skull thickness by tracing slowly and continuously around the perimeter of the craniotomy. Alternate with a 0.9 mm diameter bit as necessary. Continue until vasculature is clearly visible through the skull.</p></list-item><list-item><p>Switch to 0.5 mm diameter drill bit and thin skull around perimeter until skull cap is sparsely connected, then switch to manual tools to complete craniotomy. Use 90 and 45 degree micropoints (Fine Science Tools) in a coordinated manner to separate the skull cap around the entire perimeter. Test for separation by pushing down on the skull cap at each location. When bleeds occur, apply large volumes of ice cold ringers across the surface and wick out with non-woven sponge – only use Vetspon when absolutely necessary to stop large bleeds, as formation of clots below the skull cap will significantly impair local window clarity.</p></list-item><list-item><p>Prepare one window for placement by carefully drying with the tip of a KimWipe or Wek-Cel / Sugi-spear and placing in the correct orientation on the surface of the surgical drape. Attach a left-side stereotaxic microinjection arm (Kopf) and position then rotate-out a 3D printed window stabilizer attached to the end of an injection needle so that it can be quickly repositioned as necessary once the window is in-place. Increase O<sub>2</sub> flow rate from 1.25 to 2.0 L/min, and decrease isoflurane concentration from 1.5 to 1.0% for the remainder of the surgery. This may help to decrease intracranial pressure and to elevate respiration during the final, critical steps of the window implantation.</p></list-item><list-item><p>Remove the skull cap by placing the 90 degree micropoint at the rostral limit and the 45 degree micropoint at the caudal limit just right of the sagittal sinus, then lifting the skull up from the caudal limit until the sagittal sinus is pulled away from the dura. Re-lower the skull to remove tension on the vasculature, then re-lift skull slightly higher than the first lift to tease the skull vasculature off the skull so that it remains on the dura. Repeat these steps until the skull is free of the dura and vasculature. Use the rostral micropoint to prevent the skull from digging into the brain as the caudal edge is raised, then lift together until the skull is cleared.</p></list-item><list-item><p>Perfuse surface of brain with ice-cold ringers for ~30 s or until all bleeds have stably ceased, using fresh non-woven sponges to flow and wick liquid off opposite side of preparation. Use Vetspon sparingly and only as necessary while continuously perfusing. If a significant blood clot occludes a large area, attempt to remove it with #5 forceps but be extremely careful not to damage dura.</p></list-item><list-item><p>Place window on surface of brain with two pairs of #3 forceps. Position 3D printed window stabilizer near center of window at ~22.5 degree angle to the right and adjust until even pressure is achieved across the entire window-brain interface. Iteratively lower and raise stabilizer by small amounts with stereotax until visual clarity and contrast of blood vessels is maximized and breathing stabilizes. Rinse with ice-cold ringers and then thoroughly dry with fine-rolled KimWipe corners and/or sugi spears.</p></list-item><list-item><p>Apply Flow-It ALC around the perimeter of the window then UV cure. Make sure that there are no bubbles or gaps. Preemptively apply UV light in areas where there might be a risk of Flow-It invading the preparation under the window.</p></list-item><list-item><p>(optional) Apply Loctite 4305 around the outer edge of Flow-It, and also at the interface between dental cement and skull, being very careful not to contaminate the surface of the window. UV cure once more.</p></list-item><list-item><p>Remove window stabilizer slowly to minimize window rebound. Clean the window with ringers, dry, then apply KwikCast to cover the window, leaving a tab for easy removal over the headpost arm.</p></list-item><list-item><p>Subcutaneously inject 1 mL ringers, remove mouse from stereotax, and place in heated recovery unit for at least 24 h. Subcutaneously inject Meloxicam SR on Day 1 post-surgery, along with additional ringers as needed for post-operative weight stabilization. Total time: 60-90 min.</p></list-item></list></sec></sec><sec id="s4-11"><title>Protocol IV</title><sec id="s4-11-1"><title>Mounting a headpost and window implanted mouse onto Mesoscope or Widefield rigs</title><list list-type="order"><list-item><p>If mounting a dorsal mount preparation mouse, attach two horizontally aligned or ‘orthogonal’ support arms to the breadtable on either side of the running wheel (see <xref ref-type="fig" rid="fig1">Figure 1a</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>, top).</p></list-item><list-item><p>If mounting a side mount preparation mouse, attach one 22.5 degree-angled headpost attachment on the left side of the running wheel (see <xref ref-type="fig" rid="fig1">Figure 1b and c</xref>, bottom).</p></list-item><list-item><p>Insert and fully tighten, and then remove countersunk test screws into the support arm to test the threading. If mounting a dorsal mount preparation mouse, affix a ‘test’ headpost to both support arms to ensure that their 3D alignment is correct, before attempting with the mouse.</p></list-item><list-item><p>Place the mouse centered on the running wheel and hold the middle of its tail with your right hand.</p></list-item><list-item><p>If mounting a dorsal mount preparation mouse, use your left hand to place the left wing of the headpost into the support arm screw slot (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b, c, top, and e</xref>). Hold the wing in the slot with your thumb, and use your right hand to affix the right wing to its support arm with a single screw.</p></list-item><list-item><p>Use a second screw to affix the left wing, then tighten both sides fully in step.</p></list-item><list-item><p>If mounting a side mount preparation mouse, use your left hand to place the left wing of the headpost in the support arm slot. Then, use your left thumb to hold the tip of the wing in the slot while using your right hand to insert and tighten a screw in the hole proximal to the mouse’s head. Then, use your right hand to hold the middle of the mouse’s tail and your left hand to insert and tighten a screw in the hole distal to the mouse’s head.</p></list-item><list-item><p>Fully tighten both screws in step.</p></list-item><list-item><p>If mounting on the mesoscope, place a flat 3D-printed light-shield (wok) fitted to the headpost that you are using onto the preparation, and hold in place bilaterally with 3-pronged lab clamps attached to support arms with quick-ties. With UV and IR illumination on, use LabView NI MAX software to align cameras and ensure that the mouse’s face and eyes/pupils are fully visible on both the left and right sides.</p></list-item><list-item><p>Mix 1:1 black and white 170 FAST CURE Sylgard in a plastic weigh boat. While holding the flood light fiber in your left hand, use the wooden back-end of a cotton swab to carefully drip Sylgard into the interface between the 3D printed plastic (black PLA) light-shield (wok) and the titanium headpost. Make sure that there are no gaps, and that you do not contaminate the cranial window. Wait 5–6 min for curing to complete, then add water to full meniscus height to test for leaks before proceeding with imaging.</p></list-item><list-item><p>If mounting on the widefield microscope for 1p imaging, attach the 3D-printed light blocking cone (left half for multi-modal, full circumference for behavioral experiment) by pressing it onto the edge of the headpost and using an inverted 3-pronged lab clamp attached to the imaging lens with velcro to grip and secure the top edge of the cone before proceeding with imaging.</p></list-item></list></sec></sec><sec id="s4-12"><title>Protocol V</title><sec id="s4-12-1"><title>Allen CCF, widefield, vasculature, and 2-photon co-alignment and registration (see <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, <xref ref-type="video" rid="video3">Videos 3</xref>–<xref ref-type="video" rid="video7">7</xref>)</title><list list-type="order"><list-item><p>Acquire co-aligned blood vessel, skull, and multimodal widefield reference images.</p></list-item><list-item><p>Create blood vessel and skull images by loading 30 s (1500 frames at 50 Hz) widefield movies into Fiji, auto-adjusting brightness and contrast, and creating standard deviation z-stack projections.</p></list-item><list-item><p>Create stimulus triggered mean dF/F (here F<sub>0</sub> is equal to the global 10th percentile of fluorescence intensity) images for each 5 min (15000 frames at 50 Hz) sensory stimulation movie by averaging the 1 s baseline-subtracted Ca<sup>2+</sup> fluorescence response in a 1 s window following each stimulus onset using custom Matlab code (SensoryMapping_Vickers_Jun2520.m). Adjust stimulus timing signal detection threshold for each modality as needed.</p></list-item><list-item><p>For each multimodal mapping session, load skull, blood vessel, and multimodal dF/F images into Fiji and make sure that image size and resolution are the same for all images before proceeding.</p></list-item><list-item><p>Create a master overlay image by starting with the blood vessel image. Then proceed through each overlay image with the following substeps: (i) Threshold image so that the saturated area is contiguous in the target area and has an outline that matches its shape in the Allen CCF. (ii) Create a mask. (iii) Create selection. (iv) Use the magic wand tool to select the region of interest. (v) Select master blood vessel image. (vi) Press shift +e to place selection outline as overlay. (vii) Select Image/overlay/flatten and save new image.</p></list-item><list-item><p>It may be necessary to perform step 5 (above) for the skull image by manually selecting bregma and lambda with Fiji circle drawing tool.</p></list-item><list-item><p>Open custom Matlab code for CCF alignment and check to confirm that it’s ‘on path’ (align_recording_to_allen_Vickers_affine_Jan0120.m, or align_recording_to_allen_Vickers_pwl.m; adapted code from Shreya Saxena and Matt Kaufman, personal communication, 2018).</p></list-item><list-item><p>Navigate to the folder containing outputs of steps 5 and 6.</p></list-item><list-item><p>If aligning a ‘Crystal Skull’ preparation or an ‘A1/V1/M2’ preparation with fewer than 6 alignment points, use code for ‘affine’ transformation. If aligning an ‘A1/V1/M2’ preparation with more than or equal to 6 alignment points, use code for ‘pwl’ (piecewise linear) transformation.</p></list-item><list-item><p>Run ‘computeAllenDorsalMap.m’ to create ‘allenDorsalMap.mat’, and create a string array called ‘alignareas’ containing a list of CCF areas whose centers you will designate based on your master overlay image. Each entry will be in the form ‘Cortical_hemisphere Area_name’; for example: ‘R VISp1’, ‘R AUDp1’, or ‘R SSp-bfd1’. Copy these files into your working directory.</p></list-item><list-item><p>Load your image by double-clicking its name in the ‘Current folder’ window.</p></list-item><list-item><p>Convert your image to grayscale and auto-adjust brightness and contrast by typing ‘im0=mat2gray(imageName)’, followed by im0=imadjust(im0).</p></list-item><list-item><p>Load ‘alignareas’, ‘areanames’, and ‘dorsalMapScaled’ by double-clicking on ‘preprocessed_allenDorsalMap.mat’ and ‘alignareas’ in the ‘Current Folder’ window.</p></list-item><list-item><p>Run the ‘align_recording…..m’ code by typing “tform = align_recording_to_allen_Vickers_affine(im0,alignareas,true)</p></list-item><list-item><p>Select points on the master image as requested.</p></list-item><list-item><p>Program will generate output images with CCF overlaid on the master image with reference points shown as red ‘x’, user-selected points as blue ‘o’. For affine transformation, use ‘….inverse.png’ output so see CCF overlay on original coordinate system.</p></list-item><list-item><p>Run custom Python code (<ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_preprocess_MMM_creation.ipynb">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_preprocess_MMM_creation.ipynb</ext-link>; <xref ref-type="bibr" rid="bib55">Vickers, 2024b</xref>) to warp and align mesoscope meanImage with cellMap onto master overlay image with CCF based on user-identified common vasculature intersections.</p></list-item><list-item><p>For each Suite2p-identified cell in the cellMap, assign a coded CCF area name (i.e. a dictionary with a unique number and color identifier for each area).</p></list-item></list></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the University of Oregon Institutional Animal Care and Use Committee under protocols 20-12 and 20-14 (IACUC IDs TR202300000012 and TR202300000014) and performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All surgery was performed under isoflurane anesthesia, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-94167-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data related to all main and supplementary figures have been deposited on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">FigShare</ext-link>. All related code, supplementary figures and movies, and design files have been deposited on <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib54">Vickers, 2024a</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>ED</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Datasets supporting &quot;Pan-cortical 2-photon mesoscopic imaging and neurobehavioral alignment in awake, behaving mice&quot;</data-title><source>figshare</source><pub-id pub-id-type="doi">10.25452/figshare.plus.c.7052513</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Luca Mazzucato for helpful comments on the manuscript and Paul Steffan, Lawrence Scatena, Julian McAdams, and Daniel Hulsey for technical assistance. We thank Jack Waters for contributing CCF outlines and masks for the side mount rotated view cortical map. We thank Kazi Rafizullah, John Boosinger, and Eowyn Boosinger for helping conceive of, design, and build the mesoscope resonant scanner noise reduction shield. We also thank Elliott Abe, David Wyrick, Shreya Saxena, Matthew Kaufman, Alexander Hsu, Caleb Weinreb, Jens Tillmann, and Carsen Stringer for assistance with coding and setting up preliminary data analysis for both neural and behavioral data, and the teams at Vidrio ScanImage and Thorlabs for technical assistance with software and hardware related to imaging acquisition, respectively. This work was supported by NIH grants R35NS097287 (DAM) and R01NS118461 (DAM).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguillon-Rodriguez</surname><given-names>V</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bayer</surname><given-names>H</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Cazettes</surname><given-names>F</given-names></name><name><surname>Chapuis</surname><given-names>G</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Dewitt</surname><given-names>E</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Forrest</surname><given-names>H</given-names></name><name><surname>Haetzel</surname><given-names>L</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Khanal</surname><given-names>A</given-names></name><name><surname>Krasniak</surname><given-names>C</given-names></name><name><surname>Laranjeira</surname><given-names>I</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Meijer</surname><given-names>G</given-names></name><name><surname>Miska</surname><given-names>NJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Murakami</surname><given-names>M</given-names></name><name><surname>Noel</surname><given-names>J-P</given-names></name><name><surname>Pan-Vazquez</surname><given-names>A</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sanders</surname><given-names>J</given-names></name><name><surname>Socha</surname><given-names>K</given-names></name><name><surname>Terry</surname><given-names>R</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Vergara</surname><given-names>H</given-names></name><name><surname>Wells</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>CJ</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Wool</surname><given-names>LE</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2021">2021</year><article-title>Standardized and reproducible measurement of decision-making in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e63711</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63711</pub-id><pub-id pub-id-type="pmid">34011433</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>WE</given-names></name><name><surname>Kauvar</surname><given-names>IV</given-names></name><name><surname>Chen</surname><given-names>MZ</given-names></name><name><surname>Richman</surname><given-names>EB</given-names></name><name><surname>Yang</surname><given-names>SJ</given-names></name><name><surname>Chan</surname><given-names>K</given-names></name><name><surname>Gradinaru</surname><given-names>V</given-names></name><name><surname>Deverman</surname><given-names>BE</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Global representations of goal-directed behavior in distinct cell types of mouse neocortex</article-title><source>Neuron</source><volume>94</volume><fpage>891</fpage><lpage>907</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.017</pub-id><pub-id pub-id-type="pmid">28521139</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Balasubramanian</surname><given-names>M</given-names></name><name><surname>Ruiz</surname><given-names>TD</given-names></name><name><surname>Cook</surname><given-names>B</given-names></name><name><surname>Prabhat</surname><given-names>M</given-names></name><name><surname>Bhattacharyya</surname><given-names>S</given-names></name><name><surname>Shrivastava</surname><given-names>A</given-names></name><name><surname>Bouchard</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Scaling of union of intersections for inference of granger causal networks from observational data</article-title><conf-name>2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS</conf-name><conf-loc>New Orleans, LA, USA</conf-loc><pub-id pub-id-type="doi">10.1109/IPDPS47924.2020.00036</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Sit</surname><given-names>TPH</given-names></name><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Behavioral origin of sound-evoked activity in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>251</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01227-x</pub-id><pub-id pub-id-type="pmid">36624279</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bouchard</surname><given-names>KE</given-names></name><name><surname>Pabhat Snijders</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Union of Intersections (Uoi) for Interpretable Data Driven Discovery and Prediction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.07585">https://arxiv.org/abs/1705.07585</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>DJ</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Shuman</surname><given-names>T</given-names></name><name><surname>Shobe</surname><given-names>J</given-names></name><name><surname>Biane</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wei</surname><given-names>B</given-names></name><name><surname>Veshkini</surname><given-names>M</given-names></name><name><surname>La-Vu</surname><given-names>M</given-names></name><name><surname>Lou</surname><given-names>J</given-names></name><name><surname>Flores</surname><given-names>SE</given-names></name><name><surname>Kim</surname><given-names>I</given-names></name><name><surname>Sano</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Baumgaertel</surname><given-names>K</given-names></name><name><surname>Lavi</surname><given-names>A</given-names></name><name><surname>Kamata</surname><given-names>M</given-names></name><name><surname>Tuszynski</surname><given-names>M</given-names></name><name><surname>Mayford</surname><given-names>M</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A shared neural ensemble links distinct contextual memories encoded close in time</article-title><source>Nature</source><volume>534</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1038/nature17955</pub-id><pub-id pub-id-type="pmid">27251287</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T-W</given-names></name><name><surname>Wardill</surname><given-names>TJ</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Pulver</surname><given-names>SR</given-names></name><name><surname>Renninger</surname><given-names>SL</given-names></name><name><surname>Baohan</surname><given-names>A</given-names></name><name><surname>Schreiter</surname><given-names>ER</given-names></name><name><surname>Kerr</surname><given-names>RA</given-names></name><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Kim</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><volume>499</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/nature12354</pub-id><pub-id pub-id-type="pmid">23868258</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clancy</surname><given-names>KB</given-names></name><name><surname>Orsolic</surname><given-names>I</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Locomotion-dependent remapping of distributed cortical networks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>778</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0357-8</pub-id><pub-id pub-id-type="pmid">30858604</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Sit</surname><given-names>TPH</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mouse frontal cortex mediates additive multisensory decisions</article-title><source>Neuron</source><volume>111</volume><fpage>2432</fpage><lpage>2447</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.05.008</pub-id><pub-id pub-id-type="pmid">37295419</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>L</given-names></name><name><surname>Francis</surname><given-names>J</given-names></name><name><surname>Emanuel</surname><given-names>B</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Cholinergic and noradrenergic axonal activity contains a behavioral-state signal that is coordinated across the dorsal cortex</article-title><source>eLife</source><volume>12</volume><elocation-id>e81826</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.81826</pub-id><pub-id pub-id-type="pmid">37102362</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><collab>DeepLabCut</collab></person-group><year iso-8601-date="2018">2018</year><data-title>Deeplabcut</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLabCut/DeepLabCut">https://github.com/DeepLabCut/DeepLabCut</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Keenan</surname><given-names>T</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>S</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Barber</surname><given-names>C</given-names></name><name><surname>Berbesque</surname><given-names>N</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Bowles</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>SD</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Edwards</surname><given-names>M</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gilbert</surname><given-names>TL</given-names></name><name><surname>Griffin</surname><given-names>F</given-names></name><name><surname>Hargrave</surname><given-names>P</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Jewell</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>N</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>F</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Sjoquist</surname><given-names>N</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Valenza</surname><given-names>R</given-names></name><name><surname>White</surname><given-names>C</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id><pub-id pub-id-type="pmid">31844315</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esmaeili</surname><given-names>V</given-names></name><name><surname>Tamura</surname><given-names>K</given-names></name><name><surname>Muscinelli</surname><given-names>SP</given-names></name><name><surname>Modirshanechi</surname><given-names>A</given-names></name><name><surname>Boscaglia</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>AB</given-names></name><name><surname>Oryshchuk</surname><given-names>A</given-names></name><name><surname>Foustoukos</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Crochet</surname><given-names>S</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rapid suppression and sustained activation of distinct cortical regions for a delayed sensory-triggered motor response</article-title><source>Neuron</source><volume>109</volume><fpage>2183</fpage><lpage>2201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.05.005</pub-id><pub-id pub-id-type="pmid">34077741</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>MD</given-names></name><name><surname>Lloyd Jones</surname><given-names>S</given-names></name><name><surname>White</surname><given-names>PR</given-names></name><name><surname>Dolder</surname><given-names>CN</given-names></name><name><surname>Leighton</surname><given-names>TG</given-names></name><name><surname>Lineton</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effects of very high-frequency sound and ultrasound on humans. Part I: Adverse symptoms after exposure to audible very-high frequency sound</article-title><source>The Journal of the Acoustical Society of America</source><volume>144</volume><fpage>2511</fpage><lpage>2520</lpage><pub-id pub-id-type="doi">10.1121/1.5063819</pub-id><pub-id pub-id-type="pmid">30404512</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallero-Salas</surname><given-names>Y</given-names></name><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Sych</surname><given-names>Y</given-names></name><name><surname>Voigt</surname><given-names>FF</given-names></name><name><surname>Laurenczy</surname><given-names>B</given-names></name><name><surname>Gilad</surname><given-names>A</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensory and behavioral components of neocortical signal flow in discrimination tasks with short-term memory</article-title><source>Neuron</source><volume>109</volume><fpage>135</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.10.017</pub-id><pub-id pub-id-type="pmid">33159842</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Topography and areal organization of mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>12587</fpage><lpage>12600</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1124-14.2014</pub-id><pub-id pub-id-type="pmid">25209296</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Factorial hidden markov models</article-title><source>Machine Learning</source><fpage>1</fpage><lpage>31</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghanbari</surname><given-names>L</given-names></name><name><surname>Carter</surname><given-names>RE</given-names></name><name><surname>Rynes</surname><given-names>ML</given-names></name><name><surname>Dominguez</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Naik</surname><given-names>A</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Sagar</surname><given-names>MAK</given-names></name><name><surname>Haltom</surname><given-names>L</given-names></name><name><surname>Mossazghi</surname><given-names>N</given-names></name><name><surname>Gray</surname><given-names>MM</given-names></name><name><surname>West</surname><given-names>SL</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name><name><surname>Ebner</surname><given-names>TJ</given-names></name><name><surname>Kodandaramaiah</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortex-wide neural interfacing via transparent polymer skulls</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1500</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09488-0</pub-id><pub-id pub-id-type="pmid">30940809</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>JZ</given-names></name><name><surname>Grewe</surname><given-names>BF</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Eismann</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>High-speed recording of neural spikes in awake mice and flies with a fluorescent voltage sensor</article-title><source>Science</source><volume>350</volume><fpage>1361</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1126/science.aab0810</pub-id><pub-id pub-id-type="pmid">26586188</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><volume>81</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id><pub-id pub-id-type="pmid">24361077</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattori</surname><given-names>R</given-names></name><name><surname>Danskin</surname><given-names>B</given-names></name><name><surname>Babic</surname><given-names>Z</given-names></name><name><surname>Mlynaryk</surname><given-names>N</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Area-specificity and plasticity of history-dependent value coding during learning</article-title><source>Cell</source><volume>177</volume><fpage>1858</fpage><lpage>1872</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.04.027</pub-id><pub-id pub-id-type="pmid">31080067</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hope</surname><given-names>J</given-names></name><name><surname>Beckerle</surname><given-names>T</given-names></name><name><surname>Cheng</surname><given-names>PH</given-names></name><name><surname>Viavattine</surname><given-names>Z</given-names></name><name><surname>Feldkamp</surname><given-names>M</given-names></name><name><surname>Fausner</surname><given-names>S</given-names></name><name><surname>Saxena</surname><given-names>K</given-names></name><name><surname>Ko</surname><given-names>E</given-names></name><name><surname>Hryb</surname><given-names>I</given-names></name><name><surname>Carter</surname><given-names>R</given-names></name><name><surname>Ebner</surname><given-names>T</given-names></name><name><surname>Kodandaramaiah</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Brain-Wide Neural Recordings in Mice Navigating Physical Spaces Enabled by a Cranial Exoskeleton</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.06.04.543578</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5188</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id><pub-id pub-id-type="pmid">34465784</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name><name><surname>Shin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>B-Soid</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/YttriLab/B-SOID">https://github.com/YttriLab/B-SOID</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hulsey</surname><given-names>D</given-names></name><name><surname>Zumwalt</surname><given-names>K</given-names></name><name><surname>Mazzucato</surname><given-names>L</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name><name><surname>Jaramillo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Decision-Making Dynamics Are Predicted by Arousal and Uninstructed Movements</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.02.530651</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>EAK</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Peters</surname><given-names>AJ</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical state fluctuations during sensory decision making</article-title><source>Current Biology</source><volume>30</volume><fpage>4944</fpage><lpage>4955</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.09.067</pub-id><pub-id pub-id-type="pmid">33096037</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jaramillo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Uobrainflex</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/sjara/uobrainflex">https://github.com/sjara/uobrainflex</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kauvar</surname><given-names>IV</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Yuen</surname><given-names>E</given-names></name><name><surname>Kochalka</surname><given-names>J</given-names></name><name><surname>Choi</surname><given-names>M</given-names></name><name><surname>Allen</surname><given-names>WE</given-names></name><name><surname>Wetzstein</surname><given-names>G</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical observation by synchronous multifocal optical sampling reveals widespread population encoding of actions</article-title><source>Neuron</source><volume>107</volume><fpage>351</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.04.023</pub-id><pub-id pub-id-type="pmid">32433908</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>TH</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Jung</surname><given-names>JC</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Long-term optical access to an estimated one million neurons in the live mouse cortex</article-title><source>Cell Reports</source><volume>17</volume><fpage>3385</fpage><lpage>3394</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.12.004</pub-id><pub-id pub-id-type="pmid">28009304</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Lei</surname><given-names>L</given-names></name><name><surname>Bhattacharyya</surname><given-names>S</given-names></name><name><surname>Van den Berge</surname><given-names>K</given-names></name><name><surname>Sarkar</surname><given-names>P</given-names></name><name><surname>Bickel</surname><given-names>PJ</given-names></name><name><surname>Levina</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hierarchical community detection by recursive partitioning</article-title><source>Journal of the American Statistical Association</source><volume>117</volume><fpage>951</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1080/01621459.2020.1833888</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><collab>LindermanLab</collab></person-group><year iso-8601-date="2018">2018</year><data-title>Ssm</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/lindermanlab/ssm">https://github.com/lindermanlab/ssm</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical membrane potential signature of optimal states for sensory signal detection</article-title><source>Neuron</source><volume>87</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.038</pub-id><pub-id pub-id-type="pmid">26074005</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaiel</surname><given-names>AM</given-names></name><name><surname>Parker</surname><given-names>PRL</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A hallucinogenic serotonin-2a receptor agonist reduces visual response gain and alters temporal dynamics in mouse V1</article-title><source>Cell Reports</source><volume>26</volume><fpage>3475</fpage><lpage>3483</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.02.104</pub-id><pub-id pub-id-type="pmid">30917304</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mimica</surname><given-names>B</given-names></name><name><surname>Tombaz</surname><given-names>T</given-names></name><name><surname>Battistin</surname><given-names>C</given-names></name><name><surname>Fuglstad</surname><given-names>JG</given-names></name><name><surname>Dunn</surname><given-names>BA</given-names></name><name><surname>Whitlock</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Behavioral decomposition reveals rich encoding structure employed across neocortex in rats</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>3947</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-39520-3</pub-id><pub-id pub-id-type="pmid">37402724</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Morandell</surname><given-names>K</given-names></name><name><surname>Yin</surname><given-names>A</given-names></name><name><surname>Del Rio</surname><given-names>RT</given-names></name><name><surname>Schneider</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Movement-Related Modulation in Mouse Auditory Cortex Is Widespread yet Locally Diverse</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.03.547560</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Schröder</surname><given-names>S</given-names></name><name><surname>Rossi</surname><given-names>LF</given-names></name><name><surname>Dalgleish</surname><given-names>H</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Suite2p: Beyond 10,000 Neurons with Standard Two-Photon Microscopy</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peron</surname><given-names>SP</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Iyer</surname><given-names>V</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A cellular resolution map of barrel cortex activity during tactile behavior</article-title><source>Neuron</source><volume>86</volume><fpage>783</fpage><lpage>799</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.027</pub-id><pub-id pub-id-type="pmid">25913859</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>AJ</given-names></name><name><surname>Fabre</surname><given-names>JMJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Striatal activity topographically reflects cortical activity</article-title><source>Nature</source><volume>591</volume><fpage>420</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03166-8</pub-id><pub-id pub-id-type="pmid">33473213</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruiz</surname><given-names>T</given-names></name><name><surname>Bhattacharyya</surname><given-names>S</given-names></name><name><surname>Balasubramanian</surname><given-names>M</given-names></name><name><surname>Bouchard</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sparse and low-bias estimation of high dimensional vector autoregressive models</article-title><source>Proceedings of Machine Learning Research</source><volume>120</volume><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadananda</surname><given-names>M</given-names></name><name><surname>Wöhr</surname><given-names>M</given-names></name><name><surname>Schwarting</surname><given-names>RKW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Playback of 22-kHz and 50-kHz ultrasonic vocalizations induces differential c-fos expression in rat brain</article-title><source>Neuroscience Letters</source><volume>435</volume><fpage>17</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2008.02.002</pub-id><pub-id pub-id-type="pmid">18328625</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salkoff</surname><given-names>DB</given-names></name><name><surname>Zagha</surname><given-names>E</given-names></name><name><surname>McCarthy</surname><given-names>E</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Movement and performance explain widespread cortical activity in a visual detection task</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>421</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz206</pub-id><pub-id pub-id-type="pmid">31711133</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimaoka</surname><given-names>D</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effects of arousal on mouse sensory cortex depend on modality</article-title><source>Cell Reports</source><volume>22</volume><fpage>3160</fpage><lpage>3167</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.02.092</pub-id><pub-id pub-id-type="pmid">29562173</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Flickinger</surname><given-names>D</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A large field of view two-photon mesoscope with subcellular resolution for <italic>in vivo</italic> imaging</article-title><source>eLife</source><volume>5</volume><elocation-id>e14472</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14472</pub-id><pub-id pub-id-type="pmid">27300105</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title><source>Nature</source><volume>576</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1787-x</pub-id><pub-id pub-id-type="pmid">31776518</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Aydin</surname><given-names>C</given-names></name><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Beau</surname><given-names>M</given-names></name><name><surname>Bhagat</surname><given-names>J</given-names></name><name><surname>Böhm</surname><given-names>C</given-names></name><name><surname>Broux</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Gardner</surname><given-names>RJ</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Kostadinov</surname><given-names>D</given-names></name><name><surname>Mora-Lopez</surname><given-names>C</given-names></name><name><surname>O’Callaghan</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Sauerbrei</surname><given-names>B</given-names></name><name><surname>van Daal</surname><given-names>RJJ</given-names></name><name><surname>Vollan</surname><given-names>AZ</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Welkenhuysen</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings</article-title><source>Science</source><volume>372</volume><elocation-id>eabf4588</elocation-id><pub-id pub-id-type="doi">10.1126/science.abf4588</pub-id><pub-id pub-id-type="pmid">33859006</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Zhong</surname><given-names>L</given-names></name><name><surname>Syeda</surname><given-names>A</given-names></name><name><surname>Du</surname><given-names>F</given-names></name><name><surname>Kesa</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Rastermap: a discovery method for neural population recordings</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.25.550571</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Winnubst</surname><given-names>J</given-names></name><name><surname>Natrajan</surname><given-names>M</given-names></name><name><surname>Lai</surname><given-names>C</given-names></name><name><surname>Kajikawa</surname><given-names>K</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Gattoni</surname><given-names>R</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Flickinger</surname><given-names>D</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning Produces a Hippocampal Cognitive Map in the Form of an Orthogonalized State Machine</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.08.03.551900</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valley</surname><given-names>MT</given-names></name><name><surname>Moore</surname><given-names>MG</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Mesa</surname><given-names>N</given-names></name><name><surname>Castelli</surname><given-names>D</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Reimers</surname><given-names>M</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Separation of hemodynamic signals from GCaMP fluorescence measured with wide-field imaging</article-title><source>Journal of Neurophysiology</source><volume>123</volume><fpage>356</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1152/jn.00304.2019</pub-id><pub-id pub-id-type="pmid">31747332</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2024">2024a</year><data-title>Mesoscope spontaneous</data-title><version designator="swh:1:rev:1c3e4f845e09b8c11ff430ad1c15cb57c11d1a88">swh:1:rev:1c3e4f845e09b8c11ff430ad1c15cb57c11d1a88</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b0847ccd30f80b949edce860d9598ddc12156b99;origin=https://github.com/vickerse1/mesoscope_spontaneous;visit=swh:1:snp:f4c013cd864dd494e3d02798e7635a80d4931d8e;anchor=swh:1:rev:1c3e4f845e09b8c11ff430ad1c15cb57c11d1a88">https://archive.softwareheritage.org/swh:1:dir:b0847ccd30f80b949edce860d9598ddc12156b99;origin=https://github.com/vickerse1/mesoscope_spontaneous;visit=swh:1:snp:f4c013cd864dd494e3d02798e7635a80d4931d8e;anchor=swh:1:rev:1c3e4f845e09b8c11ff430ad1c15cb57c11d1a88</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2024">2024b</year><data-title>vickerse1/mesoscope_spontaneous: &quot;Pan-cortical 2-photon mesoscopic imaging and neurobehavioral alignment in awake, behaving mice&quot;</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10966793</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Ding</surname><given-names>S-L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Royall</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Lesnar</surname><given-names>P</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Naeemi</surname><given-names>M</given-names></name><name><surname>Facer</surname><given-names>B</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Dee</surname><given-names>N</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Hawrylycz</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The allen mouse brain common coordinate framework: A 3D reference atlas</article-title><source>Cell</source><volume>181</volume><fpage>936</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.04.007</pub-id><pub-id pub-id-type="pmid">32386544</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>ZA</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Not everything, not everywhere, not all at once: a study of brain-wide encoding of movement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.06.08.544257</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waters</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sources of widefield fluorescence from the brain</article-title><source>eLife</source><volume>9</volume><elocation-id>e59841</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59841</pub-id><pub-id pub-id-type="pmid">33155981</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Weinreb</surname><given-names>C</given-names></name><name><surname>Pearl</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>S</given-names></name><name><surname>Osman</surname><given-names>MAM</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Annapragada</surname><given-names>S</given-names></name><name><surname>Conlin</surname><given-names>E</given-names></name><name><surname>Hoffman</surname><given-names>R</given-names></name><name><surname>Makowska</surname><given-names>S</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Jay</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>S</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Pereira</surname><given-names>T</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Keypoint-Moseq: Parsing Behavior by Linking Point Tracking to Pose Dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.16.532307</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Shamash</surname><given-names>P</given-names></name><name><surname>Taylor</surname><given-names>A</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Context-dependent decision making in a premotor circuit</article-title><source>Neuron</source><volume>106</volume><fpage>316</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.034</pub-id><pub-id pub-id-type="pmid">32105611</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>H</given-names></name><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Xiao</surname><given-names>G</given-names></name><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>D</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Dai</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Multifocal fluorescence video-rate imaging of centimetre-wide arbitrarily shaped brain surfaces at micrometric resolution</article-title><source>Nature Biomedical Engineering</source><pub-id pub-id-type="doi">10.1038/s41551-023-01155-6</pub-id><pub-id pub-id-type="pmid">38057428</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>Q</given-names></name><name><surname>Znamenskiy</surname><given-names>P</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective corticostriatal plasticity during acquisition of an auditory discrimination task</article-title><source>Nature</source><volume>521</volume><fpage>348</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1038/nature14225</pub-id><pub-id pub-id-type="pmid">25731173</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>E</given-names></name><name><surname>Terada</surname><given-names>S-I</given-names></name><name><surname>Tanaka</surname><given-names>YH</given-names></name><name><surname>Kobayashi</surname><given-names>K</given-names></name><name><surname>Ohkura</surname><given-names>M</given-names></name><name><surname>Nakai</surname><given-names>J</given-names></name><name><surname>Matsuzaki</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title><italic>In vivo</italic> wide-field calcium imaging of mouse thalamocortical synapses with an 8 K ultra-high-definition camera</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>8324</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-26566-3</pub-id><pub-id pub-id-type="pmid">29844612</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>CH</given-names></name><name><surname>Stirman</surname><given-names>JN</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Hira</surname><given-names>R</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Diesel2p mesoscope with dual independent scan engines for flexible capture of dynamics in distributed neural circuitry</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6639</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26736-4</pub-id><pub-id pub-id-type="pmid">34789723</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>CH</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Adsit</surname><given-names>LM</given-names></name><name><surname>Chang</surname><given-names>JT</given-names></name><name><surname>Barchini</surname><given-names>J</given-names></name><name><surname>Moberly</surname><given-names>AH</given-names></name><name><surname>Benisty</surname><given-names>H</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Young</surname><given-names>BK</given-names></name><name><surname>Heng</surname><given-names>K</given-names></name><name><surname>Farinella</surname><given-names>DM</given-names></name><name><surname>Leikvoll</surname><given-names>A</given-names></name><name><surname>Pavan</surname><given-names>R</given-names></name><name><surname>Vistein</surname><given-names>R</given-names></name><name><surname>Nanfito</surname><given-names>BR</given-names></name><name><surname>Hildebrand</surname><given-names>DGC</given-names></name><name><surname>Otero-Coronel</surname><given-names>S</given-names></name><name><surname>Vaziri</surname><given-names>A</given-names></name><name><surname>Goldberg</surname><given-names>JL</given-names></name><name><surname>Ricci</surname><given-names>AJ</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name><name><surname>Higley</surname><given-names>MJ</given-names></name><name><surname>Smith</surname><given-names>GB</given-names></name><name><surname>Kara</surname><given-names>P</given-names></name><name><surname>Nielsen</surname><given-names>KJ</given-names></name><name><surname>Smith</surname><given-names>IT</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The Cousa Objective: A Long Working Distance Air Objective for Multiphoton Imaging <italic>in vivo</italic></article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.11.06.515343</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>QY</given-names></name><name><surname>Johnson</surname><given-names>KR</given-names></name><name><surname>Erway</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Assessment of hearing in 80 inbred strains of mice by ABR threshold analyses</article-title><source>Hearing Research</source><volume>130</volume><fpage>94</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/s0378-5955(99)00003-9</pub-id><pub-id pub-id-type="pmid">10320101</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Liang</surname><given-names>F</given-names></name><name><surname>Xiong</surname><given-names>XR</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Xiao</surname><given-names>Z</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>841</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1038/nn.3701</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Valley</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An extended retinotopic map of mouse cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e18372</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18372</pub-id><pub-id pub-id-type="pmid">28059700</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Supplementary methods and materials</title><sec sec-type="appendix" id="s8-1"><title>Overall workflow, Protocol I</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Approximate costs–single behavior rig.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Product description</th><th align="left" valign="bottom">Manufacturer</th><th align="left" valign="bottom">Quantity</th><th align="left" valign="bottom">Unit price</th><th align="left" valign="bottom">Extended price ($USD)</th></tr></thead><tbody><tr><td align="left" valign="bottom">25 Watt power supply</td><td align="left" valign="bottom">Tucker Davis</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">325</td><td align="left" valign="bottom">$325.00</td></tr><tr><td align="left" valign="bottom">Electrostatic loudspeakers</td><td align="left" valign="bottom">Tucker Davis</td><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">195</td><td align="left" valign="bottom">780.00</td></tr><tr><td align="left" valign="bottom">Electrostatic loudspeaker drivers</td><td align="left" valign="bottom">Tucker Davis</td><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">650</td><td align="left" valign="bottom">1,300.00</td></tr><tr><td align="left" valign="bottom">PC14461 Sound card</td><td align="left" valign="bottom">National Instruments</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">4,805.00</td><td align="left" valign="bottom">4,805.00</td></tr><tr><td align="left" valign="bottom">SCB 68 with Connector Cable</td><td align="left" valign="bottom">National Instruments</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">478.00</td><td align="left" valign="bottom">478.00</td></tr><tr><td align="left" valign="bottom">PCle 6321 Multifunction I/O</td><td align="left" valign="bottom">National instruments</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">690.00</td><td align="left" valign="bottom">690.00</td></tr><tr><td align="left" valign="bottom">810 nm narrow bandpass filter</td><td align="left" valign="bottom">Midwestern Optical Systems</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">141</td><td align="left" valign="bottom">141</td></tr><tr><td align="left" valign="bottom">Genie Nano M2050 Mono</td><td align="left" valign="bottom">Teledyne Dalsa</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">878.43</td><td align="left" valign="bottom">878.43</td></tr><tr><td align="left" valign="bottom">NE-500 Programmable OEM Syringe Pump</td><td align="left" valign="bottom">New Era Pumps</td><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">495</td><td align="left" valign="bottom">3,465.00</td></tr><tr><td align="left" valign="bottom">US OEM Starter kit</td><td align="left" valign="bottom">New Era Pumps</td><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">175</td></tr><tr><td align="left" valign="bottom">C-Mount 55 mm Telecentric Fixed Focus Lens COTEC55</td><td align="left" valign="bottom">computar</td><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">327.52</td><td align="left" valign="bottom">1,310.08</td></tr><tr><td align="left" valign="bottom">Assorted Optomechanical Posts and breadboard (approximate)</td><td align="left" valign="bottom">Thorlabs</td><td align="left" valign="bottom">varied</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">488.88</td></tr><tr><td align="left" valign="bottom">LCD Monitor with Blanking Apparatus (Includes Arduino Micro-Controller)<break/>(approx.)</td><td align="left" valign="bottom">Amazon/Arduino</td><td align="left" valign="bottom">varied</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">120.00</td></tr><tr><td align="left" valign="bottom">Rotary Encoder</td><td align="left" valign="bottom">KAMAN AUTOMATION INC</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">217</td><td align="left" valign="bottom">217.00</td></tr><tr><td align="left" valign="bottom">Custom Cylindrical Treadmill</td><td align="left" valign="bottom">Public Missiles LTD</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">84</td><td align="left" valign="bottom">84.00</td></tr><tr><td align="left" valign="bottom">Lick Detection Unit</td><td align="left" valign="bottom">Custom Build</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">1000</td><td align="left" valign="bottom">1000.00</td></tr><tr><td align="left" valign="bottom">Computer for Behavior (Intel i5 or better, 32 GB RAM or better) (approx.)</td><td align="left" valign="bottom">User Preference</td><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom">1400.00</td></tr><tr><td align="left" valign="bottom">Assorted Electrical Components, Misc. Hardware, Adapters, etc.</td><td align="left" valign="bottom">Varied</td><td align="left" valign="bottom">varied</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">700.00</td></tr><tr><td align="left" valign="bottom">Power 1401</td><td align="left" valign="bottom">CED</td><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">5697.60</td><td align="left" valign="bottom">5697.60</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"><bold>Total</bold></td><td align="left" valign="bottom"><bold>24,054.99</bold></td></tr></tbody></table></table-wrap><sec sec-type="appendix" id="s8-1-1"><title>Design software</title><list list-type="simple"><list-item><p>autodesk.com: Inventor and Fusion</p></list-item><list-item><p>emachineshop.com</p></list-item></list></sec><sec sec-type="appendix" id="s8-1-2"><title>3D printed titanium headposts, shot-peening finish</title><list list-type="simple"><list-item><p>i.materialise.com/</p></list-item><list-item><p>sculpteo.com/</p></list-item><list-item><p>~$35-60per headpost</p></list-item></list></sec><sec sec-type="appendix" id="s8-1-3"><title>3D printed light deflectors and accessories</title><list list-type="simple"><list-item><p>makergear.com</p></list-item><list-item><p>black PLA, 0.35 mm nozzle</p></list-item></list></sec><sec sec-type="appendix" id="s8-1-4"><title>Kopf stereotax</title><list list-type="simple"><list-item><p>926-B mouse nose/tooth bar assembly</p></list-item><list-item><p>907 mouse anesthesia mask</p></list-item></list></sec><sec sec-type="appendix" id="s8-1-5"><title>Widefield imaging</title><list list-type="simple"><list-item><p>Redshirt Imaging, DaVinci SciMeasure &amp; Turbo-SM64</p></list-item><list-item><p>PCO.edge 5.5 M-AIR-CLHS-PCO, &amp; CamWare 4.0</p></list-item><list-item><p>Tamron SP 90 mm f/2.8 Di Macro 1:1 VC USD Lens for Nikon F</p></list-item></list></sec></sec><sec sec-type="appendix" id="s8-2"><title>Headpost surgery, Protocol II</title><list list-type="simple"><list-item><p>Mouse electric trimmer combo kit with detailer (CL9990-1201, Kent Scientific)</p></list-item><list-item><p>Nair sensitive hair removal cream (<ext-link ext-link-type="uri" xlink:href="https://www.amazon.com">https://www.amazon.com</ext-link>)</p></list-item><list-item><p>PurSwab 3” small pointed ESD foam swabs (<ext-link ext-link-type="uri" xlink:href="https://www.amazon.com">https://www.amazon.com</ext-link>)</p></list-item><list-item><p>Vetoquinol Nutri Cal (4.25 oz Paste; <ext-link ext-link-type="uri" xlink:href="https://www.amazon.com">https://www.amazon.com</ext-link>)</p></list-item><list-item><p>Puralube Vet Ophthalmic Ointment (<ext-link ext-link-type="uri" xlink:href="https://www.amazon.com">https://www.amazon.com</ext-link>)</p></list-item><list-item><p>3 M Vet Bond (<ext-link ext-link-type="uri" xlink:href="https://www.amazon.com">https://www.amazon.com</ext-link>)</p></list-item><list-item><p>Scalpel blades - #23 (10023–00; Fine Science Tools)</p></list-item><list-item><p>Kwik-Cast sealant (World Precision Instruments)</p></list-item><list-item><p>Kwik-SIL sealant (World Precision Instruments)</p></list-item><list-item><p>Pacer Technology (Zap) Slo-Zap (Thick) Adhesives, 2 oz (<ext-link ext-link-type="uri" xlink:href="https://www.amazon.com">https://www.amazon.com</ext-link>)</p></list-item><list-item><p>Micro-bulldog clamp for mice (INS600119-2, Kent Scientific)</p></list-item><list-item><p>RelyXUniCem Aplicap Refill A1 20/Bx 3 M ESPE Products (036090–3789981; Henry Schein DBA Butler Animal Health)</p></list-item><list-item><p>Disposable aluminum crinkle dishes with tabs, 8 mL (12577–081; VWR)</p></list-item><list-item><p>3 M Aplicap Applier Activator/Applier Set, 37160</p></list-item><list-item><p>HP 1RF-009 Round Stainless Steel Burs Pk/10 [1RF-009-HP (All4Dentist)]</p></list-item><list-item><p>Small homeothermic blanket system with control unit (Q-21090, Harvard Apparatus)</p></list-item><list-item><p>Gelfoam for cessation of bleeding (NC1061303, Fisher Scientific)</p></list-item><list-item><p>Applicator cotton tipped his non-sterile, 3 inch, wood handle (Henry Schein)</p></list-item><list-item><p>Dumont #3 forceps (11293–00; Fine Science Tools)</p></list-item><list-item><p>Angled 80 deg long probe (10140–03; Fine Science Tools)</p></list-item><list-item><p>Fine scissors, tungsten carbide, straight (14568–09; Fine Science Tools)</p></list-item><list-item><p>Vannas spring scissors – 2.5 mm (15000–08; Fine Science Tools)</p></list-item></list></sec><sec sec-type="appendix" id="s8-3"><title>Cranial window surgery, Protocol III</title><list list-type="simple"><list-item><p><ext-link ext-link-type="uri" xlink:href="https://www.labmaker.org/">https://www.labmaker.org/</ext-link>, “Crystal Skull – One Million Neurons” (Tony Kim, Yanping Zhang and Mark Schnitzer; <ext-link ext-link-type="uri" xlink:href="https://www.labmaker.org/products/crystal-skull?_pos=1&amp;_sid=84c16cded&amp;_ss=r">https://www.labmaker.org/products/crystal-skull?_pos=1&amp;_sid=84c16cded&amp;_ss=r</ext-link>)</p></list-item><list-item><p>TLC International custom cutting, Phoenix-600, 0.21 mm Schott D263T Glass (9849 North 21 Avenue, Phoenix, AZ; A1/V1/M2)</p></list-item><list-item><p>GlasWerk Inc, custom 9–12 mm radius glass bending (29710 Avenida de las banderas, Rancho Santa Margarita, CA; A1/V1/M2)</p></list-item><list-item><p>Loctite 4305 LT cure ad 1oz bottle (LT303389, Krayden)</p></list-item><list-item><p>Dynarex non-woven sponge, N/S 4Ply (amazon.com)</p></list-item><list-item><p>Osada EXL-M40 brushless micromotor system (EXL-M40; Dorado dental supply)</p></list-item><list-item><p>Flow It ALC Flowable Syringe WO Value Pack 6/Pk (726240, Henry Schein)</p></list-item><list-item><p>Micro-point, angled 90 degree long (10065–15; Fine Science Tools)</p></list-item><list-item><p>Micro-point, angled 45 degree long (10066–15; Fine Science Tools)</p></list-item></list></sec><sec sec-type="appendix" id="s8-4"><title>Rig mounting, Protocol IV</title><list list-type="simple"><list-item><p>Mil Spec St Steel Phillips Flat Head Screws, 82 degree (96877 A18, McMaster-CARR)</p></list-item><list-item><p>Dow Corning Sylgard 170 Fast Cure Silicone Encapsulant Black 210 mL (Ellsworth Adhesives)</p></list-item><list-item><p>Posts (Thorlabs; TR6-P5)</p></list-item><list-item><p>Ball joint (Thorlabs; SL20)</p></list-item><list-item><p>Post holder (Thorlabs; PH2E)</p></list-item><list-item><p>Clamp fork (Thorlabs; CF125)</p></list-item><list-item><p>Platform (Thorlabs, MB4)</p></list-item><list-item><p>Motorized linear stage, 25 mm range, 104 mm/s, 48 V (X-LSM025B-E03-KX14A; zaber.com)</p></list-item></list></sec><sec sec-type="appendix" id="s8-5"><title>Multimodal mapping</title><list list-type="simple"><list-item><p>Eyoyo 15.6&quot; inch Gaming Monitor 1920x1080 HDR Display Second (newegg.com)</p></list-item><list-item><p>E-650 Piezo Amplifier for Multilayer bending actuators, 18 W</p></list-item><list-item><p>PICMA multilayer piezo bending actuator, 2000 µm travel range, 45 mm × 11.00 mm×0.55 mm, stranded wires (PL140.11; physik instrumente.store)</p></list-item><list-item><p>Tucker Davis ES1 Free Field Electrostatic Speaker</p></list-item><list-item><p>Tucker Davis ED1 Electrostatic Speaker Driver</p></list-item><list-item><p>Oben BD-0 mini ball head (amazon.com)</p></list-item><list-item><p>Locking ball and socket mount, ¼”–20 threaded (TRB2; Thorlabs)</p></list-item><list-item><p>Mounted LED 470 nm (760 mW, 1000 mA) (M470L4, Thorlabs)</p></list-item><list-item><p>Dichroic (T495lpxr, chroma.com)</p></list-item><list-item><p>Excitation filter (ET470/40 x; chroma.com)</p></list-item><list-item><p>Emission filter (ET525/50 m; chroma.com)</p></list-item><list-item><p>Kinematic fluorescence filter cube (DFM1, Thorlabs)</p></list-item></list></sec><sec sec-type="appendix" id="s8-6"><title>ScanImage 2p acquisition</title><list list-type="simple"><list-item><p>Spectra Physics Mai Tai HP-244 tunable femtosecond laser</p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="https://www.spectra-physics.com/en/f/mai-tai-ultrafast-laser">https://www.spectra-physics.com/en/f/mai-tai-ultrafast-laser</ext-link></p></list-item><list-item><p>Thorlabs Mesoscope</p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="https://www.thorlabs.com/newgrouppage9.cfm?objectgroup_id=10646">https://www.thorlabs.com/newgrouppage9.cfm?objectgroup_id=10646</ext-link></p></list-item><list-item><p>ScanImage 2018–2021 (Vidrio Technologies)</p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="https://vidriotechnologies.com/scanimage/">https://vidriotechnologies.com/scanimage/</ext-link></p></list-item><list-item><p><ext-link ext-link-type="uri" xlink:href="http://scanimage.vidriotechnologies.com/display/SIH/ScanImage+Home">http://scanimage.vidriotechnologies.com/display/SIH/ScanImage+Home</ext-link></p></list-item></list></sec><sec sec-type="appendix" id="s8-7"><title>Computing resources</title><list list-type="simple"><list-item><p>EVGA GeForce RTX 3080 Ti FTW3 Ultra Gaming, 12G-P5-3967-KR, 12 GB GDDR6X, iCX3 Technology, ARGB LED, Metal Backplate (B0922N253, amazon.com)</p></list-item><list-item><p>Tesla V100 PCIe 32 GB GPUs</p></list-item><list-item><p>GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link>; <xref ref-type="bibr" rid="bib54">Vickers, 2024a</xref>.</p></list-item><list-item><p>FigShare +site: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p></list-item></list></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94167.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Grunwald Kadow</surname><given-names>Ilona C</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Bonn</institution><country>Germany</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> paper presents a thoroughly detailed methodology for mesoscale-imaging of extensive areas of the cortex, either from a top or lateral perspective, in behaving mice. The examples of scientific results to be derived with this method offer promising and stimulating insights. Overall, the method and results presented are <bold>convincing</bold> and will be of interest to neuroscientists focused on cortical processing in rodents and beyond.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94167.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors introduce two preparations for observing large-scale cortical activity in mice during behavior. Alongside, they present intriguing preliminary findings utilizing these methods. This paper is poised to be an invaluable resource for researchers engaged in extensive cortical recording in behaving mice.</p><p>Strengths:</p><p>Comprehensive methodological detailing:</p><p>The paper excels in providing an exceptionally detailed description of the methods used. This meticulous documentation includes a step-by-step workflow, complemented by thorough workflow, protocols and list of materials in the supplementary materials.</p><p>Minimal of movement artifacts:</p><p>A notable strength of this study is the remarkably low movement artifacts, with specific strategies outlined to attain this outcome.</p><p>Insightful preliminary data and analysis:</p><p>The preliminary data unveiled in the study reveal interesting heterogeneity in the relationships between neural activity and detailed behavioral features, particularly notable in the lateral cortex. This aspect of the findings is intriguing and suggests avenues for further exploration.</p><p>Weaknesses:</p><p>Clarification about the extent of the method in title:</p><p>The title of the paper, using the term &quot;pan-cortical&quot;, may inadvertently suggest that both the top and lateral view preparations are utilized in the same set of mice, while the authors employ either the dorsal view (which offers limited access to the lateral ventral regions) or the lateral view (which restricts access to the opposite side of the cortex).</p><p>Despite the authors not identifying qualitative effects, tilting the mouse's head could potentially influence behavioral outcomes in certain paradigms.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94167.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors present a comprehensive technical overview of the challenging acquisition of large-scale cortical activity, including surgical procedures and custom 3D-printed headbar designs to obtain neural activity from large parts of the dorsal or lateral neocortex. They then describe technical adjustments for stable head fixation, light shielding, and noise insulation in a 2-photon mesoscope and provide a workflow for multisensory mapping and alignment of the obtained large-scale neural data sets in the Allen CCF framework. Lastly, they show different analytical approaches to relate single-cell activity from various cortical areas to spontaneous activity by using visualization and clustering tools, such as Rastermap, PCA-based cell sorting, and B-SOID behavioral motif detection.</p><p>The study contains a lot of useful technical information that should be of interest to the field. It tackles a timely problem that an increasing number of labs will be facing as recent technical advances allow the activity measurement of an increasing number of neurons across multiple areas in awake mice. Since the acquisition of cortical data with a large field of view in awake animals poses unique experimental challenges, the provided information could be very helpful to promote standard workflows for data acquisition and analysis and push the field forward.</p><p>Strengths:</p><p>The proposed methodology is technically sound and the authors provide convincing data to suggest that they successfully solved various challenging problems, such as motion artifacts of large imaging preparations or high-frequency noise emissions, during 2-photon imaging. Overall, the authors achieved their goal of demonstrating a comprehensive approach for imaging neural data across many cortical areas and providing several examples that demonstrate the validity of their methods and recapitulate and further extend some recent findings in the field. A particular focus of the results is to emphasize the need for imaging large population activity across cortical areas to identify cross-area information processing during active behaviors.</p><p>Weaknesses:</p><p>The manuscript contains a lot of technical details and might be challenging for readers without previous experimental experience. However, the different paragraphs illuminate a large range of technical aspects and challenges of large-scale functional imaging. Therefore, the work should be a valuable source of solutions for a diverse audience.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94167.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>In their manuscript, Vickers and McCormick have demonstrated the potential of leveraging mesoscale two-photon calcium imaging data to unravel complex behavioural motifs in mice. Particularly commendable is their dedication in providing detailed surgical preparations and corresponding design files, a contribution that will greatly benefit the broader neuroscience community as a whole. The quality of the data is high and examples are available to the community. More importantly, the authors have acquired activity-clustered neural ensembles at an unprecedented spatial scale to further correlate with high level behaviour motifs identified by B-SOiD. Such an advancement marks a significant contribution to the field. While the manuscript is comprehensive and the analytical strategy proposed is promising, some technical aspects warrant further clarification. Overall, the authors have presented an invaluable and innovative approach, effectively laying a solid foundation for future research in correlating large scale neural ensembles with behavioural. The implementation of a custom sound insulator for the scanner is a great idea and should be something implemented by others.</p><p>This is a methods paper, but there is no large diagram (in the main figures) that shows how all the parts are connected, communicating and triggering between each other. This is described in the methods and now supplemental figure, but a visual representation would greatly benefit the readers looking to implement something similar as a main figure but I guess they can find it in the methods. No stats for the results shown in Figure 6e, it would be useful to know which of these neural densities for all areas show a clear statistical significance across all the behaviors. While I understand that this is a methods paper, it seems like the authors are aware of the literature surrounding large neuronal recordings during mouse behavior. Indeed, in line 178-179 the authors mention how a significant portion of the variance in neural activity can be attributed to changes in &quot;arousal or self-directed movement even during spontaneous behavior.&quot; Why then did the authors not make an attempt at a simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc). These models are straightforward to implement, and indeed it would benefit this work if the model extracts information on par with what it's known from the literature. We also realize such a model could be done in the future.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94167.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Vickers</surname><given-names>Evan D</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oregon</institution><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>McCormick</surname><given-names>David A</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oregon</institution><addr-line><named-content content-type="city">Oregon</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This valuable paper presents a thoroughly detailed methodology for mesoscale-imaging of extensive areas of the cortex, either from a top or lateral perspective, in behaving mice. While the examples of scientific results to be derived with this method are in the preliminary stages, they offer promising and stimulating insights. Overall, the method and results presented are convincing and will be of interest to neuroscientists focused on cortical processing in rodents.</p></disp-quote><p>Authors’ Response: We thank the reviewers for the helpful and constructive comments. They have helped us plan for significant improvements to our manuscript. Our preliminary response and plans for revision are indicated below.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The authors introduce two preparations for observing large-scale cortical activity in mice during behavior. Alongside this, they present intriguing preliminary findings utilizing these methods. This paper is poised to be an invaluable resource for researchers engaged in extensive cortical recording in behaving mice.</p><p>Strengths:</p><p>-Comprehensive methodological detailing:</p><p>The paper excels in providing an exceptionally detailed description of the methods used. This meticulous documentation includes a step-by-step workflow, complemented by thorough workflow, protocols, and a list of materials in the supplementary materials.</p><p>-Minimal movement artifacts:</p><p>A notable strength of this study is the remarkably low movement artifacts. To further underscore this achievement, a more robust quantification across all subjects, coupled with benchmarking against established tools (such as those from suite2p), would be beneficial.</p></disp-quote><p>Authors’ Response: This is a good suggestion. We have records of the fast-z correction applied by the ScanImage on microscope during acquisition, so we have supplied the online fast-z motion correction .csv files for two example sessions on our GitHub page as supplementary files:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction</ext-link></p><p>These files correspond to Figure S3b (2367_200214_E210_1) and to Figures 5 and 6 (3056_200924_E235_1). These are now also referenced in the main text. See lines ~595, pg 18 and lines ~762, pg 24.</p><p>We have also made minor revisions to the main text of the manuscript with clear descriptions of methods that we have found important for the minimization of movement artifacts, such as fully tightening all mounting devices, implanting the cranial window with proper, evenly applied pressure across its entire extent, and mounting the mouse so that it is not too close or far from the surface of the running wheel. See Line ~309, pg 10.</p><disp-quote content-type="editor-comment"><p>Insightful preliminary data and analysis:</p><p>The preliminary data unveiled in the study reveal interesting heterogeneity in the relationships between neural activity and detailed behavioral features, particularly notable in the lateral cortex. This aspect of the findings is intriguing and suggests avenues for further exploration.</p><p>Weaknesses:</p><p>-Clarification about the extent of the method in the title and text:</p><p>The title of the paper, using the term &quot;pan-cortical,&quot; along with certain phrases in the text, may inadvertently suggest that both the top and lateral view preparations are utilized in the same set of mice. To avoid confusion, it should be explicitly stated that the authors employ either the dorsal view (which offers limited access to the lateral ventral regions) or the lateral view (which restricts access to the opposite side of the cortex). For instance, in line 545, the phrase &quot;lateral cortex with our dorsal and side mount preparations&quot; should be revised to &quot;lateral cortex with our dorsal or side mount preparations&quot; for greater clarity.</p></disp-quote><p>Authors’ Response: We have opted to not change the title of the paper, because we feel that adding the qualifier, “in two preparations,” would add unnecessary complexity. In addition, while the dorsal mount preparation allows for imaging of bilateral dorsal cortex, the side mount preparation does indeed allow for imaging of both dorsal and lateral cortex across the right hemisphere (a bit of contralateral dorsal cortex is also imageable), and the design can be easily “flipped” across a mirror-plane to allow for imaging of left dorsal and lateral cortex. Taken together, we do show preparations that allow for pan-cortical 2-photon imaging.</p><p>We do agree that imprecise reference to the two preparations can sometimes lead to confusion. Therefore, we made several small revisions to the manuscript, including at ~line 545, to make it clearer that we used two imaging preparations to generate our combined 2-photon mesoscope dataset, and that each of those two preparations had both benefits and limitations.</p><disp-quote content-type="editor-comment"><p>-Comparison with existing methods:</p><p>A more detailed contrast between this method and other published techniques would add value to the paper. Specifically, the lateral view appears somewhat narrower than that described in Esmaeili et al., 2021; a discussion of this comparison would be useful.</p></disp-quote><p>Authors’ Response: The preparation by Esmaeili et al. 2021 has some similarities to, but also differences from, our preparation. Our preliminary reading is that their through-the-skull field of view is approximately the same as our through-the-skull field of view that exists between our first (headpost implantation) and second (window implantation) surgeries for our side mount preparation, although our preparation appears to include more anterior areas both near to and on the contralateral side of the midline. We have compared these preparations more thoroughly in the revised manuscript. (See lines ~278.)</p><disp-quote content-type="editor-comment"><p>Furthermore, the number of neurons analyzed seems modest compared to recent papers (50k) - elaborating on this aspect could provide important context for the readers.</p></disp-quote><p>Authors’ response: With respect to the “modest” number of neurons analyzed (between 2000 and 8000 neurons per session for our dorsal and side mount preparations with medians near 4500; See Fig. S2e) we would like to point out that factors such as use of dual-plane imaging or multiple imaging planes, different mouse lines, use of different duration recording sessions (see our Fig S2c), use of different imaging speeds and resolutions (see our Fig S2d), use of different Suite2p run-time parameters, and inclusion of areas with blood vessels and different neuron cell densities, may all impact the count of total analyzed neurons per session. We now mention these various factors and have made clear that we were not, for the purposes of this paper, trying to maximize neuron count at the expense of other factors such as imaging speed and total spatial FOV extent.</p><p>We refer to these issues now briefly in the main text. (See ~line 93, pg 3).</p><disp-quote content-type="editor-comment"><p>-Discussion of methodological limitations:</p><p>The limitations inherent to the method, such as the potential behavioral effects of tilting the mouse's head, are not thoroughly examined. A more comprehensive discussion of these limitations would enhance the paper's balance and depth.</p></disp-quote><p>Authors’ Response: Our mice readily adapted to the 22.5 degree head tilt and learned to perform 2-alternative forced choice (2-AFC) auditory and visual tasks in this configuration (Hulsey et al, 2024; Cell Reports). The advantages and limitations of such a rotation of the mouse, and possible ways to alleviate these limitations, as detailed in the following paragraphs, are now discussed more thoroughly in the revised manuscript at ~line 235, pg. 7.</p><p>One can look at Supplementary Movie 1 for examples of the relatively similar behavior between the dorsal mount (not rotated) and side mount (rotated) preparations. We do not have behavioral data from mice that were placed in both configurations. Our preliminary comparisons across mice indicates that side and dorsal mount mice show similar behavioral variability. We have added brief additional mention of these considerations on ~lines 235-250, pg 7.</p><p>It was in general important to make sure that the distance between the wheel and all four limbs was similar for both preparations. In particular, careful attention must be paid to the positioning of the front limbs in the side mount mice so that they are not too high off the wheel. This can be accomplished by a slight forward angling of the left support arm for side mount mice.</p><p>Although it is possible to image the side mount preparation in the same optical configuration that we do without rotating the mouse, by rotating the objective 20 degrees to the right of vertical, we found that the last 2-3 degrees of missing rotation (our preparation is rotated 22.5 degrees left, which is more than the full available 20 degrees rotation of the Thorlabs mesoscope objective), along with several other factors, made this undesirable. First, it was very difficult to image auditory areas without the additional flexibility to rotate the objective more laterally. Second, it was difficult or impossible to attach the horizontal light shield and to establish a water meniscus with the objective fully rotated. One could use ultrasound gel instead (which we found to be, to some degree, optically inferior to water), but without the horizontal light shield, light from the UV and IR LEDs can reach the PMTs via the objective and contaminate the image or cause tripping of the PMT. Third, imaging the right pupil and face of the mouse is difficult under these conditions because the camera would need the same optical access angle as the 2-photon objective, or would need to be moved downward toward the air table and rotated up at an angle of 20 degrees, in which case its view would be blocked by the running wheel and other objects mounted on the air table.</p><disp-quote content-type="editor-comment"><p>-Preliminary nature of results:</p><p>The results are at a preliminary stage; for example, the B-soid analysis is based on a single mouse, and the validation data are derived from the training data set.</p></disp-quote><p>Authors’ Response: In this methods paper, we have chosen to supply proof of principle examples, without a complete analysis of animal-to-animal variance.</p><p>The B-SOiD analysis that we show in Figure 6 is based on a model trained on 80% of the data from four sessions taken from the same mouse, and then tested on all of a single session from that mouse. Initial attempts to train across sessions from different mice were unsuccessful, probably due to differences in behavioral repertoires across mice. However, we have performed extensive tests with B-SOiD and are confident that these sorts of results are reproducible across mice, although we are not prepared to publish these results at this time.</p><p>We now clarify these points in the main text at ~line 865, pg 27.</p><p>An additional comparison of the results of B-SOiD trained on different numbers of sessions to that of keypoint-MOSEQ (Weinreb et al, 2023, bioRxiv) trained on ~20 sessions can now be found as supplementary material on our GitHub site:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/Figure_SZZ_BSOID_MOSEQ_align.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/Figure_SZZ_BSOID_MOSEQ_align.pdf</ext-link></p><disp-quote content-type="editor-comment"><p>The discrepancy between the maps in Figures 5e and 6e might indicate that a significant portion of the map represents noise. An analysis of variability across mice and a method to assign significance to these maps would be beneficial.</p></disp-quote><p>Authors’ Response: After re-examination of the original analysis output files, we have indeed discovered that some of the Rastermap neuron density maps in Figure 6e were incorrectly aligned with their respective qualitative behaviors due to a discrepancy in file numbering between the images in 6e and the ensembles identified in 6c (each time that Rastermap is run on the same data, at least with the older version available at the time of creation of these figures, the order of the ensembles on the y-axis changes and thus the numbering of the ensembles would change even though the neuron identities within each group stayed the same for a given set of parameters).</p><p>This unfortunate panel alignment / graphical display error present in the original reviewed preprint has been fixed in the current, updated figure (i.e. twitch corresponds to Rastermap groups 2 and 3, whisk to group 6, walk to groups 5 and 4, and oscillate to groups 0 and 1), and in the main text at ~line 925, pg 29. We have also changed the figure legend, which also contained accurate but misaligned information, for Figure 6e to reflect this correction.</p><p>One can now see that, because the data from both figures is from the same session in the same mouse, as you correctly point out, Fig 5d left (walk and whisk) corresponds roughly to Fig 6e group R7, “walk”, and that Fig 5d right (whisk) corresponds roughly to Fig 6e group R4, “twitch”.</p><p>We have double-checked the identity of other CCF map displays of Rastermap neuron density and of mean correlations between neural activity and behavioral primitives in all other figures, and we found no other such alignment or mis-labeling errors.</p><p>We have also added a caveat in the main text at ~lines 925-940, pg. 30, pointing out the preliminary nature of these findings, which are shown here as an example of the viability of the methods. Analysis of the variability of Rastermap alignments across sessions is beyond the scope of the current paper, although it is an issue that we hope to address in upcoming analysis papers.</p><disp-quote content-type="editor-comment"><p>-Analysis details:</p><p>More comprehensive details on the analysis would be beneficial for replicability and deeper understanding. For instance, the statement &quot;Rigid and non-rigid motion correction were performed in Suite2p&quot; could be expanded with a brief explanation of the underlying principles, such as phase correlation, to provide readers with a better grasp of the methodologies employed.</p></disp-quote><p>Authors’ Response: We added a brief explanation of Suite2p motion correction at ~line 136, pg 4. We have also added additional details concerning CCF / MMM alignment and other analysis issues. In general we cite other papers where possible to avoid repeating details of analysis methods that are already published.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors present a comprehensive technical overview of the challenging acquisition of large-scale cortical activity, including surgical procedures and custom 3D-printed headbar designs to obtain neural activity from large parts of the dorsal or lateral neocortex. They then describe technical adjustments for stable head fixation, light shielding, and noise insulation in a 2-photon mesoscope and provide a workflow for multisensory mapping and alignment of the obtained large-scale neural data sets in the Allen CCF framework. Lastly, they show different analytical approaches to relate single-cell activity from various cortical areas to spontaneous activity by using visualization and clustering tools, such as Rastermap, PCA-based cell sorting, and B-SOID behavioral motif detection.</p></disp-quote><p>Authors’ Response: Thank you for this excellent summary of the scope of our paper.</p><disp-quote content-type="editor-comment"><p>The study contains a lot of useful technical information that should be of interest to the field. It tackles a timely problem that an increasing number of labs will be facing as recent technical advances allow the activity measurement of an increasing number of neurons across multiple areas in awake mice. Since the acquisition of cortical data with a large field of view in awake animals poses unique experimental challenges, the provided information could be very helpful to promote standard workflows for data acquisition and analysis and push the field forward.</p></disp-quote><p>Authors’ Response: We very much support the idea that our work here will contribute to the development of standard workflows across the field including those for multiple approaches to large-scale neural recordings.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>The proposed methodology is technically sound and the authors provide convincing data to suggest that they successfully solved various problems, such as motion artifacts or high-frequency noise emissions, during 2-photon imaging. Overall, the authors achieved their goal of demonstrating a comprehensive approach for the imaging of neural data across many cortical areas and providing several examples that demonstrate the validity of their methods and recapitulate and further extend some recent findings in the field.</p><p>Weaknesses:</p><p>Most of the descriptions are quite focused on a specific acquisition system, the Thorlabs Mesoscope, and the manuscript is in part highly technical making it harder to understand the motivation and reasoning behind some of the proposed implementations. A revised version would benefit from a more general description of common problems and the thought process behind the proposed solutions to broaden the impact of the work and make it more accessible for labs that do not have access to a Thorlabs mesoscope. A better introduction of some of the specific issues would also promote the development of other solutions in labs that are just starting to use similar tools.</p></disp-quote><p>Authors’ Response: We have edited the motivations behind the study to clarify the general problems that are being addressed. However, as the 2-photon imaging component of these experiments were performed on a Thorlabs mesoscope, the imaging details necessarily deal specifically with this system.</p><p>We briefly compare the methods and results from our Thorlabs system to that of Diesel-2p, another comparable system, based on what we have been able to glean from the literature on its strengths and weaknesses. See ~lines 206-213, pg 6.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary</p><p>In their manuscript, Vickers and McCormick have demonstrated the potential of leveraging mesoscale two-photon calcium imaging data to unravel complex behavioural motifs in mice. Particularly commendable is their dedication to providing detailed surgical preparations and corresponding design files, a contribution that will greatly benefit the broader neuroscience community as a whole. The quality of the data is high, but it is not clear whether this is available to the community, some datasets should be deposited. More importantly, the authors have acquired activity-clustered neural ensembles at an unprecedented spatial scale to further correlate with high-level behaviour motifs identified by B-SOiD. Such an advancement marks a significant contribution to the field. While the manuscript is comprehensive and the analytical strategy proposed is promising, some technical aspects warrant further clarification. Overall, the authors have presented an invaluable and innovative approach, effectively laying a solid foundation for future research in correlating large-scale neural ensembles with behaviour. The implementation of a custom sound insulator for the scanner is a great idea and should be something implemented by others.</p></disp-quote><p>Authors’ Response: Thank you for the kind words.</p><p>We have made ~500 GB of raw data and preliminary analysis files publicly available on FigShare+ for the example sessions shown in Figures 2, 3, 4, 5, 6, S3, and S6. We ask to be cited and given due credit for any fair use of this data.</p><p>The data is located here: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p><p>We intend to release a complete data set to the public as a Dandiset on the DANDI archive in conjunction with in-depth analysis papers that are currently in preparation.</p><disp-quote content-type="editor-comment"><p>This is a methods paper, but there is no large diagram that shows how all the parts are connected, communicating, and triggering each other. This is described in the methods, but a visual representation would greatly benefit the readers looking to implement something similar.</p></disp-quote><p>Authors’ Response: This is an excellent suggestion. We have included a workflow diagram in the revised manuscript, in the form of a 3-part figure, for the methods (a), data collection (b and c), and analysis (d). This supplementary figure is now located on the GitHub page at the following link:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf</ext-link></p><p>We now reference this figure on ~lines 190-192, pg 6 of the main text, near the beginning of the Results section.</p><disp-quote content-type="editor-comment"><p>The authors should cite sources for the claims stated in lines 449-453 and cite the claim of the mouse's hearing threshold mentioned in lines 463.</p></disp-quote><p>Authors’ Response: For the claim stated in lines 449-453:</p><p>“The unattenuated or native high-frequency background noise generated by the resonant scanner causes stress to both mice and experimenters, and can prevent mice from achieving maximum performance in auditory mapping, spontaneous activity sessions, auditory stimulus detection, and auditory discrimination sessions/tasks”</p><p>,we can provide the following references: (i) for mice: Sadananda et al, 2008 (“Playback of 22-kHz and 50-kHz ultrasonic vocalizations induces differential c-fos expression in rat brain”, Neuroscience Letters, Vol 435, Issue 1, p 17-23), and (ii) for humans: Fletcher et al, 2018 (“Effects of very high-frequency sound and ultrasound on humans. Part I: Adverse symptoms after exposure to audible very-high frequency sound”, J Acoust Soc A, 144, 2511-2520). We will include these references in the revised paper.</p><p>For the claim stated on line 463:</p><p>“i.e. below the mouse hearing threshold at 12.5 kHz of roughly 15 dB”</p><p>,we can provide the following reference: Zheng et al, 1999 (“Assessment of hearing in 80 inbred strains of mice by ABR threshold analyses”, Vol 130, Issues 1-2, p 94-107).</p><p>We have included these two new references in the new, revised version of our paper. Thank you for identifying these citation omissions.</p><disp-quote content-type="editor-comment"><p>No stats for the results shown in Figure 6e, it would be useful to know which of these neural densities for all areas show a clear statistical significance across all the behaviors.</p></disp-quote><p>Authors’ Response: It would be useful if we could provide a statistic similar to what we provide for Fig. S6c and f, in which for each CCF area we compare the observed mean correlation values to a null of 0, or, in this case, the population densities of each Rastermap group within each CCF area to a null value equal to the total number of CCF areas divided by the total number of recorded neurons for that group (i.e. a Rastermap group with 500 neurons evenly distributed across ~30 CCF areas would contain ~17 neurons, or ~3.3% density, per CCF area.) Our current figure legend states the maximums of the scale bar look-up values (reds) for each group, which range from ~8% to 32%.</p><p>However, because the data in panel 6e are from a single session and are being provided as an example of our methods and not for the purpose of claiming a specific result at this point, we choose not to report statistics. It is worth pointing out, perhaps, that Rastermap group densities for a given CCF area close to 3.3% are likely not different from chance, and those closer to ~40%, which is our highest density (for area M2 in Rastermap group 7, which corresponds to the qualitative behavior “walk”), are most likely not due to chance. Without analysis of multiple sessions from the same mouse we believe that making a clear statement of significance for this likelihood would be premature.</p><p>We now clarify this decision and related considerations in the main text at ~line 920, pg 29.</p><disp-quote content-type="editor-comment"><p>While I understand that this is a methods paper, it seems like the authors are aware of the literature surrounding large neuronal recordings during mouse behavior. Indeed, in lines 178-179, the authors mention how a significant portion of the variance in neural activity can be attributed to changes in &quot;arousal or self-directed movement even during spontaneous behavior.&quot; Why then did the authors not make an attempt at a simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc). These models are straightforward to implement, and indeed it would benefit this work if the model extracts information on par with what is known from the literature.</p></disp-quote><p>Authors’ Response: This is an excellent suggestion, but beyond the scope of the current methods paper. We are following up with an in depth analysis of neural activity and corresponding behavior across the cortex during spontaneous and trained behaviors, but this analysis goes well beyond the scope of the present manuscript.</p><p>Here, we prefer to present examples of the types of results that can be expected to be obtained using our methods, and how these results compare with those obtained by others in the field.</p><disp-quote content-type="editor-comment"><p>Specific strengths and weaknesses with areas to improve:</p><p>The paper should include an overall cartoon diagram that indicates how the various modules are linked together for the sampling of both behaviour and mesoscale GCAMP. This is a methods paper, but there is no large diagram that shows how all the parts are connected, communicating, and triggering each other.</p></disp-quote><p>Authors’ Response: This is an excellent suggestion. We have included a workflow diagram in the revised manuscript, in the form of a 3-part figure, for the methods (a), data collection (b and c), and analysis (c). This supplementary figure is now located on the GitHub page at the following link:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/pancortical_workflow_diagrams.pdf</ext-link></p><disp-quote content-type="editor-comment"><p>The paper contains many important results regarding correlations between behaviour and activity motifs on both the cellular and regional scales. There is a lot of data and it is difficult to draw out new concepts. It might be useful for readers to have an overall figure discussing various results and how they are linked to pupil movement and brain activity. A simple linear model that tries to predict the activity of their many thousands of neurons by employing the multitude of regressors at their disposal (pupil, saccades, stimuli, movements, facial changes, etc) may help in this regard.</p></disp-quote><p>Authors’ Response: This is an excellent suggestion, but beyond the scope of the present methods paper. Such an analysis is a significant undertaking with such large and heterogeneous datasets, and we provide proof-of-principle data here so that the reader can understand the type of data that one can expect to obtain using our methods. We will provide a more complete analysis of data obtained using our methodology in the near future in another manuscript.</p><disp-quote content-type="editor-comment"><p>Previously, widefield imaging methods have been employed to describe regional activity motifs that correlate with known intracortical projections. Within the authors' data it would be interesting to perhaps describe how these two different methods are interrelated -they do collect both datasets. Surprisingly, such macroscale patterns are not immediately obvious from the authors' data. Some of this may be related to the scaling of correlation patterns or other factors. Perhaps there still isn't enough data to readily see these and it is too sparse.</p></disp-quote><p>Authors’ Response: Unfortunately, we are unable to directly compare 1-photon widefield GCaMP6s activity with mesoscope 2-photon GCaMP6s activity. During widefield data acquisition, animals were stimulated with visual, auditory, or somatosensory stimuli (i.e. “passive sensory stimulation”), while 2-photon mesoscope data collection occurred during spontaneous changes in behavioral state, without sensory stimulation. The suggested comparison is, indeed, an interesting project for the future.</p><disp-quote content-type="editor-comment"><p>In lines 71-71, the authors described some disadvantages of one-photon widefield imaging including the inability to achieve single-cell resolution. However, this is not true. In recent years, the combination of better surgical preparations, camera sensors, and genetically encoded calcium indicators has enabled the acquisition of single-cell data even using one-photon widefield imaging methods. These methods include miniscopes (Cai et al., 2016), multi-camera arrays (Hope et al., 2023), and spinning disks (Xie et al., 2023).</p><p>Cai, Denise J., et al. &quot;A shared neural ensemble links distinct contextual memories encoded close in time.&quot; Nature 534.7605 (2016): 115-118.</p><p>Hope, James, et al. &quot;Brain-wide neural recordings in mice navigating physical spaces enabled by a cranial exoskeleton.&quot; bioRxiv (2023).</p><p>Xie, Hao, et al. &quot;Multifocal fluorescence video-rate imaging of centimetre-wide arbitrarily shaped brain surfaces at micrometric resolution.&quot; Nature Biomedical Engineering (2023): 1-14.</p></disp-quote><p>Authors’ Response: We have corrected these statements and incorporated these and other relevant references. There are advantages and disadvantages to each chosen technique, such as ease of use, field of view, accuracy, and speed. We will reference the papers you mention without an extensive literature review, but we would like to emphasize the following points:</p><p>Even the best one-photon imaging techniques typically have ~10-20 micrometer resolution in xy (we image at 5 micrometer resolution for our large FOV configuration, but the xy point-spread function for the Thorlabs mesoscope is 0.61 x 0.61 micrometers in xy with 970 nm excitation) and undefined z-resolution (4.25 micrometers for Thorlabs mesoscope). A coarser resolution increases the likelihood that activity related fluorescence from neighboring cells may contaminate the fluorescence observed from imaged neurons. Reducing the FOV and using sparse expression of the indicator lessens this overlap problem.</p><p>We do appreciate these recent advances, however, particularly for use in cases where more rapid imaging is desired over a large field of view (CCD acquisition can be much faster than that of standard 2-photon galvo-galvo or even galvo-resonant scanning, as the Thorlabs mesoscope uses). This being said, there are few currently available genetically encoded Ca2+ sensors that are able to measure fluctuations faster than ~10 Hz, which is a speed achievable on the Thorlabs 2-photon mesoscope with our techniques using the “small, multiple FOV” method (Fig. S2d, e).</p><p>We have further clarified our discussion of these issues in the main text at ~lines 76-80, pg 2.</p><disp-quote content-type="editor-comment"><p>The authors' claim of achieving optical clarity for up to 150 days post-surgery with their modified crystal skull approach is significantly longer than the 8 weeks (approximately 56 days) reported in the original study by Kim et al. (2016). Since surgical preparations are an integral part of the manuscript, it may be helpful to provide more details to address the feasibility and reliability of the preparation in chronic studies. A series of images documenting the progression optical quality of the window would offer valuable insight.</p></disp-quote><p>Authors’ Response: As you suggest, we now include brief supplementary material demonstrating the changes in the window preparation that we observed over the prolonged time periods of our study, for both the dorsal and side mount preparations. The following link to this material is now referenced at ~line 287, pg 9, and at the end of Fig S1:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf</ext-link></p><p>We have also included brief additional details in the main text that we found were useful for facilitating long term use of these preparations. These are located at ~line 287-290, pg 9.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) Sharing raw data and code:</p><p>I strongly encourage sharing some of the raw data from your experiments and all the code used for data analysis (e.g. in a github repository). This would help the reader evaluate data quality, and reproduce your results.</p></disp-quote><p>Authors’ Response: We have made ~500 GB of raw data and preliminary analysis files publicly available on FigShare+ for the example sessions shown in Figures 2, 3, 4, 5, 6, S3, and S6. We ask to be cited and given due credit for any fair use of this data.</p><p>We intend to release a complete data set to the public as a Dandiset on the DANDI archive in conjunction with second and third in-depth analysis papers that are currently in preparation.</p><p>The data is located here: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p><p>We intend to release a complete data set to the public as a Dandiset on the DANDI archive in conjunction with second and third in-depth analysis papers that are currently in preparation.</p><p>Our existing GitHub repository, already referenced in the paper, is located here:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link></p><p>We have added an additional reference in the main text to the existence of these publicly available resources, including the appropriate links, located at ~lines 190-200, pg 6.</p><disp-quote content-type="editor-comment"><p>(2) Use of proprietary software:</p><p>The reliance on proprietary tools like LabView and Matlab could be a limitation for some researchers, given the associated costs and accessibility issues. If possible, consider incorporating or suggesting alternatives that are open-source, to make your methodology more accessible to a broader range of researchers, including those with limited resources.</p></disp-quote><p>Authors’ Response: We are reluctant to recommend open source software that we have not thoroughly tested ourselves. However, we will mention, when appropriate, possible options for the reader to consider.</p><p>Although LabView is proprietary and can be difficult to code, it is particularly useful when used in combination with National Instruments hardware. ScanImage in use with the Thorlabs mesoscope uses National Instruments hardware, and it is convenient to maintain hardware standards across the integrated rig/experimental system. Labview is also useful because it comes with a huge library of device drivers that makes addition of new hardware from basically any source very convenient.</p><p>That being said, there are open source alternatives that could conceivably be used to replace parts of our system. One example is AutoPilot (author: Jonny Saunders), for control of behavioral data acquisition: <ext-link ext-link-type="uri" xlink:href="https://open-neuroscience.com/post/autopilot/">https://open-neuroscience.com/post/autopilot/</ext-link>.</p><p>We are not aware of an alternative to Matlab for control of ScanImage, which is the supported control software for the ThorLabs 2-photon mesoscope.</p><p>Most of our processing and analysis code (see GitHub page: <ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous">https://github.com/vickerse1/mesoscope_spontaneous</ext-link>) is in Python, but some of the code that we currently use remains in Matlab form. Certainly, this could be re-written as Python code. However, we feel like this is outside the scope of the current paper. We have provided commenting to all code in an attempt to aid users in translating it to other languages, if they so desire.</p><disp-quote content-type="editor-comment"><p>(3) Quantifying the effect of tilted head:</p><p>To address the potential impact of tilting the mouse's head on your findings, a quantitative analysis of any systematic differences in the behavior (e.g. Bsoid motifs) could be illuminating.</p></disp-quote><p>Authors’ Response: We have performed DeepLabCut analysis of all sessions from both preparations, across several iterations with different parameters, to extract pose estimates, and we have also performed BSOiD of these sessions. We did not find any obvious qualitative differences in the number of behavioral motifs identified, the dwell times of these motifs, and similar issues, relating to the issue of tilting of the mouse’s head in the side mount preparation. We also did not find any obvious differences in the relative frequencies of high level qualitative behaviors, such as the ones referred to in Fig. 6, between the two preparations.</p><p>Our mice readily adapted to the 22.5 degree head tilt and learned to perform 2-alternative forced choice (2-AFC) auditory and visual tasks in this configuration (Hulsey et al, 2024; Cell Reports). The advantages and limitations of such a rotation of the mouse, and possible ways to alleviate these limitations, as detailed in the following paragraphs, are now discussed more thoroughly in the revised manuscript. (See ~line 235, pg. 7)</p><p>One can look at Supplementary Movie 1 for examples of the relatively similar behavior between the dorsal mount (not rotated) and side mount (rotated) preparations. We do not have behavioral data from mice that were placed in both configurations. Our preliminary comparisons across mice indicates that side and dorsal mount mice show similar behavioral variability. We have added brief additional mention of these considerations on ~lines 235-250, pg 7.</p><p>It was in general important to make sure that the distance between the wheel and all four limbs was similar for both preparations. In particular, careful attention must be paid to the positioning of the front limbs in the side mount mice so that they are not too high off the wheel. This can be accomplished by a slight forward angling of the left support arm for side mount mice.</p><p>Although it would in principle be nearly possible to image the side mount preparation in the same optical configuration that we do without rotating the mouse, by rotating the objective 20 degrees to the right of vertical, we found that the last 2-3 degrees of missing rotation (our preparation is rotated 22.5 degrees left, which is more than the full available 20 degrees rotation of the Thorlabs mesoscope objective), along with several other factors, made this undesirable. First, it was very difficult to image auditory areas without the additional flexibility to rotate the objective more laterally. Second, it was difficult or impossible to attach the horizontal light shield and to establish a water meniscus with the objective fully rotated. One could use gel instead (which we found to be optically inferior to water), but without the horizontal light shield, the UV and IR LEDs can reach the PMTs via the objective and contaminate the image or cause tripping of the PMT. Third, imaging the right pupil and face of the mouse is difficult to impossible under these conditions because the camera would need the same optical access angle as the objective, or would need to be moved down toward the air table and rotated up 20 degrees, in which case its view would be blocked by the running wheel and other objects mounted on the air table.</p><disp-quote content-type="editor-comment"><p>(4) Clarification in the discussion section:</p><p>The paragraph titled &quot;Advantages and disadvantages of our approach&quot; seems to diverge into discussing future directions, rather than focusing on the intended topic. I suggest revisiting this section to ensure that it accurately reflects the strengths and limitations of your approach.</p></disp-quote><p>Authors’ Response: We agree with the reviewer that this section included several potential next steps or solutions for each advantage and disadvantage, which the reviewer refers to as “future directions” and are thus arguably beyond the scope of this section. Therefore we have retitled this section as, “Advantages and disadvantages of our approach (with potential solutions):”.</p><p>Although we believe this to be a logical organization, and we already include a section focused purely on future directions in the Discussion section, we have refocused each paragraph of the advantages/disadvantages subsection to concentrate on the advantages and disadvantages per se. In addition, we have made minor changes to the “future directions” section to make it more succinct and practical. These changes can be found at lines ~1016-1077, pg 33-34.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Below are some more detailed points that will hopefully help to further improve the quality and scope of the manuscript.</p><list list-type="bullet"><list-item><p>While it is certainly favorable for many questions to measure large-scale activity from many brain regions, the introduction appears to suggest that this is a prerequisite to understanding multimodal decision-making. This is based on the argument that combining multiple recordings with movement indicators will 'necessarily obscure the true spatial correlation structures'. However, I don't understand why this is the case or what is meant by 'true spatial correlation structures'. Aren't there many earlier studies that provided important insights from individual cortical areas? It would be helpful to improve the writing to make this argument clearer.</p></list-item></list></disp-quote><p>Authors’ Response: The reviewer makes an excellent point and we have re-worded the manuscript appropriately, to reflect the following clarifications. These changes can be found at ~lines 58-71, pg. 2.</p><p>We believe you are referring to the following passage from the introduction:</p><p>“Furthermore, the arousal dependence of membrane potential across cortical areas has been shown to be diverse and predictable by a temporally filtered readout of pupil diameter and walking speed (Shimoaka et al, 2018). This makes simultaneous recording of multiple cortical areas essential for comparison of the dependence of their neural activity on arousal/movement, because combining multiple recording sessions with pupil dilations and walking bouts of different durations will necessarily obscure the true spatial correlation structures.”</p><p>Here, we do not mean to imply that earlier studies of individual cortical areas are of no value. This argument is provided as an example, of which there are others, of the idea that, for sequences or distributed encoding schemes that simultaneously span many cortical areas that are too far apart to be simultaneously imaged under conventional 2-photon imaging, or are too sparse to be discovered with 1-photon widefield imaging, there are some advantages of our new methods over conventional imaging methods that will allow for truly novel scientific analyses and insights.</p><p>The general idea of the present example, based on the findings of Shimoaka et al, 2018, is that it is not possible to directly combine and/or compare the correlations between behavior and neural activity across regions that were imaged in separate sessions, because the correlations between behavior and neural activity in each region appear to depend on the exact time since the behavior began (Shimoaka et al, 2018), in a manner that differs across regions. So, for example, if one were to record from visual cortex in one session with mostly brief walk bouts, and then from somatosensory cortex in a second session with mostly long walk bouts, any inferred difference between the encoding of walk speed in neural activity between the two areas would run the risk of being contaminated by the “temporal filtering” effect shown in Shimoaka et al, 2018. However, this would not be the case in our recordings, because the distribution of behavior durations corresponding to our recorded neural activity across areas will be exactly the same, because they were recorded simultaneously.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>The text describes different timescales of neural activity but is an imaging rate of 3 Hz fast enough to be seen as operating at the temporal dynamics of the behavior? It appears to me that the sampling rate will impose a hard limit on the speed of correlations that can be observed across regions. While this might be appropriate for relatively slow behaviors and spontaneous fluctuations in arousal, sensory processing and decision formation likely operate on faster time scales below 100ms which would even be problematic at 10 Hz which is proposed as the ideal imaging speed in the manuscript.</p></list-item></list></disp-quote><p>Authors’ Response: Imaging rate is always a concern and the limitations of this have been discussed in other manuscripts. We will remind the reader of these limitations, which must always be kept in mind when interpreting fluorescence based neural activity data.</p><p>Previous studies imaging on a comparable yet more limited spatial scale (Stringer et al, 2019) used an imaging speed of ~1 Hz. With this in view, our work represents an advance both in spatial extent of imaged cortex and in imaging speed. Specifically, we believe that ~1 Hz imaging may be sufficient to capture flip/flop type transitions between low and high arousal states that persist in general for seconds to tens of seconds, and that ~3-5 Hz imaging likely provides additional information about encoding of spontaneous movements and behavioral syllables/motifs.</p><p>Indeed, even 10 Hz imaging would not be fast enough to capture the detailed dynamics of sensory processing and decision formation, although these speeds are likely sufficient to capture “stable” encodings of sensory representations and decisions that must be maintained during a task, for example with delayed match-to-sample tasks.</p><p>In general we are further developing our preparations to allow us to perform simultaneous widefield imaging and Neuropixels recordings, and to perform simultaneous 1.2 x 1.2 mm 2-photon imaging and visually guided patch clamp recordings.</p><p>Both of these techniques will allow us to combine information across both the slow and fast timescales that you refer to in your question.</p><p>We have clarified these points in the Introduction and Discussion sections, at ~lines ~93-105, pg 3, and ~lines 979-983, pg 31 and ~lines 1039-1045, pg 33, respectively.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>The dorsal mount is very close to the crystal skull paper and it was ultimately not clear to me if there are still important differences aside from the headbar design that a reader should be aware of. If they exist, it would be helpful to make these distinctions a bit clearer. Also, the sea shell implants from Ghanbari et al in 2019 would be an important additional reference here.</p></list-item></list></disp-quote><p>Authors’ Response: We have added brief references to these issues in our revised manuscript at ~lines 89-97, pg 3:</p><p>Although our dorsal mount preparation is based on the “crystal skull paper” (Kim et al, 2016), which we reference, the addition of a novel 3-D printable titanium headpost, support arms, light shields, and modifications to the surgical protocols and CCF alignment represent significant advances that made this preparation useable for pan-cortical imaging using the Thorlabs mesoscope. In fact, we were in direct communication with Cris Niell, a UO professor and co-author on the original Kim et al, 2016 paper, during the initial development of our preparation, and he and members of his lab consulted with us in an ongoing manner to learn from our successful headpost and other hardware developments. Furthermore, all of our innovations for data acquisition, imaging, and analysis apply equally to both our dorsal mount and side mount preparations.</p><p>Thank you for mentioning the Ghanbari et al, 2019 paper on the transparent polymer skull method, “See Shells.” We were in fact not aware of this study. However, it should be noted that their preparation seems to, like the crystal skull preparation and our dorsal mount preparation, be limited to bilateral dorsal cortex and not to include, as does our cranial window side mount preparation and the through-the-skull widefield preparation of Esmaeili et al, 2021, a fuller range of lateral cortical areas, including primary auditory cortex.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>When using the lateral mount, rotating the objective, rather than the animal, appears to be preferable to reduce the stress on the animal. I also worry that the rather severe head tilt could be an issue when training animals in more complex behaviors and would introduce an asymmetry between the hemispheres due to the tilted body position. Is there a strong reason why the authors used water instead of an imaging gel to resolve the issue with the meniscus?</p></list-item></list></disp-quote><p>Authors’ Response: Our mice readily adapted to the 22.5 degree head tilt and learned to perform 2-alternative forced choice (2-AFC) auditory and visual tasks in this situation (Hulsey et al, 2024; Cell Reports). The advantages and limitations of such a rotation of the mouse, and possible ways to alleviate these limitations, as detailed in the following paragraphs, are now discussed more thoroughly in the revised manuscript. (See ~line 235, pg. 7)</p><p>One can look at Supplementary Movie 1 for examples of the relatively similar behavior between the dorsal mount (not rotated) and side mount (rotated) preparations. We do not have behavioral data from mice that were placed in both configurations. Our preliminary comparisons across mice indicates that side and dorsal mount mice show similar behavioral variability. We have added brief additional mention of these considerations on ~lines 235-250, pg 7.</p><p>It was in general important to make sure that the distance between the wheel and all four limbs was similar for both preparations. In particular, careful attention must be paid to the positioning of the front limbs in the side mount mice so that they are not too high off the wheel. This can be accomplished by a slight forward angling of the left support arm for side mount mice.</p><p>Although it would in principle be nearly possible to image the side mount preparation in the same optical configuration that we do without rotating the mouse, by rotating the objective 20 degrees to the right of vertical, we found that the last 2-3 degrees of missing rotation (our preparation is rotated 22.5 degrees left, which is more than the full available 20 degrees rotation of the objective), along with several other factors, made this undesirable. First, it was very difficult to image auditory areas without the additional flexibility to rotate the objective more laterally. Second, it was difficult or impossible to attach the horizontal light shield and to establish a water meniscus with the objective fully rotated. One could use gel instead (which we found to be optically inferior to water), but without the horizontal light shield, the UV and IR LEDs can reach the PMTs via the objective and contaminate the image or cause tripping of the PMT. Third, imaging the right pupil and face of the mouse is difficult to impossible under these conditions because the camera would need the same optical access angle as the objective, or would need to be moved down toward the air table and rotated up 20 degrees, in which case its view would be blocked by the running wheel and other objects mounted on the air table.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>In parts, the description of the methods is very specific to the Thorlabs mesoscope which makes it harder to understand the general design choices and challenges for readers that are unfamiliar with that system. Since the Mesoscope is very expensive and therefore unavailable to many labs in the field, I think it would increase the reach of the manuscript to adjust the writing to be less specific for that system but instead provide general guidance that could also be helpful for other systems. For example (but not exclusively) lines 231-234 or lines 371 and below are very Thorlabs-specific.</p></list-item></list></disp-quote><p>Authors’ Response: We have revised the manuscript so that it is more generally applicable to mesoscopic methods.</p><p>We will make revisions as you suggest where possible, although we have limited experience with the other imaging systems that we believe you are referring to. However, please note that we already mentioned at least one other comparable system in the original eLife reviewed pre-print (Diesel 2p, line 209; Yu and Smith, 2021).</p><p>Here are a couple of examples of how we have broadened our description:</p><p>(1) On lines ~231-234, pg 7, we write:</p><p>“However, if needed, the objective of the Thorlabs mesoscope may be rotated laterally up to +20 degrees for direct access to more ventral cortical areas, for example if one wants to use a smaller, flat cortical window that requires the objective to be positioned orthogonally to the target region.”</p><p>Here have modified this to indicate that one may in general rotate their objective lens if their system allows it. Some systems, such as the Thorlabs Bergamo microscope and the Sutter MOM system, allow more than 20 degrees of rotation.</p><p>(2) On line ~371, pg 11, we write:</p><p>“This technique required several modifications of the auxiliary light-paths of the Thorlabs mesoscope”</p><p>Here, we have changed the writing to be more general such as “may require…of one’s microscope.”</p><p>Thank you for these valuable suggestions.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Lines 287-299: Could the authors quantify the variation in imaging depth, for example by quantifying to which extent the imaging depth has to be adjusted to obtain the position of the cortical surface across cortical areas? Given that curvature is a significant challenge in this preparation this would be useful information and could either show that this issue is largely resolved or to what extent it might still be a concern for the interpretation of the obtained results. How large were the required nominal corrections across imaging sites?</p></list-item></list></disp-quote><p>Authors’ Response: This information was provided previously (lines 297-299):</p><p>“In cases where we imaged multiple small ROIs, nominal imaging depth was adjusted in an attempt to maintain a constant relative cortical layer depth (i.e. depth below the pial surface; ~200 micrometer offset due to brain curvature over 2.5 mm of mediolateral distance, symmetric across the center axis of the window).”</p><p>This statement is based on a qualitative assessment of cortical depth based on neuron size and shape, the density of neurons in a given volume of cortex, the size and shape of blood vessels, and known cortical layer depths across regions. A ground-truth measurement of this depth error is beyond the scope of the present study. However, we do specify the type of glass, thickness, and curvature that we use, and the field curvature characterization of the Thorlabs mesoscope is given in Fig. 6 of the Sofroniew et al, 2016 eLife paper.</p><p>In addition, we have provided some documentation of online fast-z correction parameters on our GitHub page at:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/online_fast_z_correction</ext-link></p><p>,and some additional relevant documentation can be found in our publicly available data repository on FigShare+ at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Given the size of the implant and the subsequent work attachments, I wonder to which extent the field of view of the animal is obstructed. Did the authors perform receptive field mapping or some other technique that can estimate the size of the animals' remaining field of view?</p></list-item></list></disp-quote><p>Authors’ Response: The left eye is pointed down ~22.5 degrees, but we position the mouse near the left edge of the wheel to minimize the degree to which this limits their field of view. One may view our Fig. 1 and Suppl Movies 1 and 6 to see that the eyes on the left and right sides are unobstructed by the headpost, light shields, and support arms. However, other components of the experimental setup, such as the speaker, cameras, etc. can restrict a few small portions of the visual field, depending on their exact positioning.</p><p>The facts that mice responded to left side visual stimuli in preliminary recordings during our multimodal 2-AFC task, and that the unobstructed left and right camera views, along with pupillometry recordings, showed that a significant portion of the mouse’s field of view, from either side, remains intact in our preparation.</p><p>We have clarified these points in the text at ~lines 344-346, pg. 11.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 361: What does movie S7 show in this context? The movie seems to emphasize that the observed calcium dynamics are not driven by movement dynamics but it is not clear to me how this relates to the stimulation of PV neurons. The neural dynamics in the example cell are also not very clear. It would be helpful if this paragraph would contain some introduction/motivation for the optogenetic stimulation as it comes a bit out of the blue.</p></list-item></list></disp-quote><p>Authors’ Response: This result was presented for two reasons.</p><p>First, we showed it as a control for movement artifacts, since inhibition of neural activity enhances the relative prominence of non-activity dependent fluorescence that is used to examine the amplitude of movement-related changes in non-activity dependent fluorescence (e.g. movement artifacts). We have included a reference to this point at ~lines 587-588, pg 18.</p><p>Second, we showed it as a demonstration of how one may combine optogenetics with imaging in mesoscopic 2-P imaging. References to this point were already present in the original version of the manuscript (the eLife “ reviewed preprint”).</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Lines 362-370: This paragraph and some of the following text are quite technical and would benefit from a better description and motivation of the general workflow. I have trouble following what exactly is done here. Are the authors using an online method to identify the CCF location of the 2p imaging based on the vessel pattern? Why is it important to do this during the experiment? Wouldn't it be sufficient to identify the areas of interest based on the vessel pattern beforehand and then adjust the 2p acquisition accordingly? Why are they using a dial, shutter, and foot pedal and how does this relate to the working distance of the objective? Does the 'standardized cortical map' refer to the Allen common coordinate framework?</p></list-item></list></disp-quote><p>Authors’ Response: We have revised this section to make it more clear.</p><p>Currently, the general introduction to this section appears in lines 349-361. Starting in line 362, we currently present the technical considerations needed to implement the overall goals stated in that first paragraph of this section.</p><p>In general we use a post-hoc analysis step to confirm the location of neurons recorded with 2-photon imaging. We use “online” juxtaposition of the multimodal map image with overlaid CCF with the 2-photon image by opening these two images next to each other on the ScanImage computer and matching the vasculature patterns “by eye”. We have made this more clear in the text so that the interested reader can more readily implement our methods.</p><p>By use of the phrase “standardized cortical map” in this context, we meant to point out that we had not decided a priori to use the Allen CCF v3.0 when we started working on these issues.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Does Fig. 2c show an example of the online alignment between widefield and 2p data? I was confused here since the use of suite2p suggests that this was done post-recording. I generally didn't understand why the user needed to switch back and forth between the two modes. Doesn't the 2p image show the vessels already? Also, why was an additional motorized dichroic to switch between widefield and 2p view needed? Isn't this the standard in most microscopes (including the Thorlabs scopes)?</p></list-item></list></disp-quote><p>Authors’ Response: We have explained this methodology more clearly in the revised manuscript, both at ~lines 485-500, pg 15-16, and ~lines 534-540, pg 17.</p><p>The motorized dichroic we used replaced the motorized mirror that comes with the Thorlabs mesoscope. We switched to a dichroic to allow for near-simultaneous optogenetic stimulation with 470 nm blue light and 2-photon imaging, so that we would not have to move the mirror back and forth during live data acquisition (it takes a few seconds and makes an audible noise that we wanted to avoid).</p><p>Figure 2c shows an overview of our two step “offline” alignment process. The image at the right in the bottom row labeled “2” is a map of recorded neurons from suite2p, determined post-hoc or after imaging. In Fig. 2d we show what the CCF map looks like when it’s overlaid on the neurons from a single suite2p session, using our alignment techniques. Indeed, this image is created post-hoc and not during imaging. In practice, “online” during imaging, we would have the image at left in the bottom row of Fig. 2c (i.e. the multimodal map image overlaid onto an image of the vasculature also acquired on the widefield rig, with the 22.5 degree rotated CCF map aligned to it based on the location of sensory responses) rotated 90 degrees to the left and flipped over a horizontal mirror plane so that its alignment matches that of the “online” 2-photon acquisition image and is zoomed to the same scale factor. Then, we would navigate based on vasculature patterns “by-eye” to the desired CCF areas, and confirm our successful 2-photon targeting of predetermined regions with our post-hoc analysis.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Why is the widefield imaging done through the skull under anesthesia? Would it not be easier to image through the final window when mice have recovered? Is the mapping needed for accurate window placement?</p></list-item></list></disp-quote><p>Authors’ Response: The headpost and window surgeries are done 3-7 days apart to increase success rate and modularize the workflow. Multimodal mapping by widefield imaging is done through the skull between these two surgeries for two major reasons. First, to make efficient use of the time between surgeries. Second, to allow us to compare the multimodal maps to skull landmarks, such as bregma and lambda, for improved alignment to the CCF.</p><p>Anesthesia was applied to prevent state changes and movements of the mouse, which can produce large, undesired effects on neural responses in primary sensory cortices in the context of these mapping experiments. We sometimes re-imaged multimodal maps on the widefield microscope through the window, roughly every 30-60 days or whenever/if significant changes in vasculature pattern became apparent.</p><p>We have clarified these points in the main text at ~lines 510-522, pg 20-21, and we added a link to our new supplementary material documenting the changes observed in the window preparation over time:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/window_preparation_stability.pdf</ext-link></p><p>Thank you for these questions.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Lines 445 and below: Reducing the noise from resonant scanners is also very relevant for many other 2p experiments so it would be helpful to provide more general guidance on how to resolve this problem. Is the provided solution only applicable to the Thorlabs mesoscope? How hard would it be to adjust the authors' noise shield to other microscopes? I generally did not find many additional details on the Github repo and think readers would benefit from a more general explanation here.</p></list-item></list></disp-quote><p>Authors’ Response: Our revised Github repository has been modified to include more details, including both diagrams and text descriptions of the sound baffle, respectively:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_for_noise_reduction_on_resonant_scanner_devices.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_for_noise_reduction_on_resonant_scanner_devices.pdf</ext-link></p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_methodology_summary.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_methodology_summary.pdf</ext-link></p><p>However, we can not presently disclose our confidential provisional patent application. Complete design information will likely be available in early 2025 when our full utility patent application is filed.</p><p>With respect to your question, yes, this technique is adaptable to any resonant scanner, or, for that matter, any complicated 3D surface that emits sound. We first 3D scan the surface, and then we reverse engineer a solid that fully encapsulates the surface and can be easily assembled in parts with bolts and interior foam that allow for a tight fit, in order to nearly completely block all emitted sound.</p><p>It is this adaptability that has prompted us to apply for a full patent, as we believe this technique will be quite valuable as it may apply to a potentially large number of applications, starting with 2-photon resonant scanners but possibly moving on to other devices that emit unwanted sound.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Does line 458 suggest that the authors had to perform a 3D scan of the components to create the noise reduction shield? If so, how was this done? I don't understand the connection between 3D scanning and printing that is mentioned in lines 464-466.</p></list-item></list></disp-quote><p>Authors’ Response: We do not want to release full details of the methodology until the full utility patent application has been submitted. However, we have now included a simplified text description of the process on our GitHub page and included a corresponding link in the main text:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_methodology_summary.pdf">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/resonant_scanner_baffle/closed_cell_honeycomb_baffle_methodology_summary.pdf</ext-link></p><p>We also clarified in the main text, at the location that you indicate, why the 3D scanning is a critical part of our novel 3D-design, printing, and assembly protocol.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Lines 468 and below: Why is it important to align single-cell data to cortical areas 'directly on the 2-photon microscope'? Is this different from the alignment discussed in the paragraph above? Why not focus on data interpretation after data acquisition? I understand the need to align neural data to cortical areas in general, I'm just confused about the 'on the fly' aspect here and why it seems to be broken out into two separate paragraphs. It seems as if the text in line 485 and below could also be placed earlier in the text to improve clarity.</p></list-item></list></disp-quote><p>Authors’ Response: Here by “such mapping is not routinely possible directly on the 2-photon mesoscope” what we mean is that it is not possible to do multimodal mapping directly on the mesoscope - it needs to be done on the widefield imaging rig (a separate microscope). Then, the CCF is mapped onto the widefield multimodal map, which is overlaid on an image of the vasculature (and sometimes also the skull) that was also acquired on the widefield imaging rig, and the vasculature is used as a sort of Rosetta Stone to co-align the 2-photon image to the multimodal map and then, by a sort of commutative property of alignment, to the CCF, so that each individual neuron in the 2-photon image can be assigned a unique CCF area name and numerical identifier for subsequent analysis.</p><p>We have clarified this in the text, thank you.</p><disp-quote content-type="editor-comment"><p>The Python code for aligning the widefield and 2-photon vessel images would also be of great value for regular 2p users. It would strongly improve the impact of the paper if the repository were better documented and the code would be equally applicable for alignment of imaging data with smaller cranial windows.</p></disp-quote><p>Authors’ Response: All of the code for multimodal map, CCF, and 2-photon image alignment is, in fact, already present on the GitHub page. We have made some minor improvements to the documentation, and readers are more than welcome to contact us for additional help.</p><p>Specifically, the alignment you refer to starts in cell #32 of the meso_pre_proc_1.ipynb notebook. In general the notebooks are meant to be run sequentially, starting with cell #1 of meso_pre_proc_1, then going to the next cell etc…, then moving to meso_pre_proc_2, etc… The purpose of each cell is labeled at the top of the cell in a comment.</p><p>We now include a cleaned, abridged version of the meso_pre_proc_1.pynb notebook that contains only the steps needed for alignment, and included a direct link to this notebook in the main text:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_preprocess_MMM_creation.ipynb">https://github.com/vickerse1/mesoscope_spontaneous/blob/main/python_code/mesoscope_preprocess_MMM_creation.ipynb</ext-link></p><p>Rotated CCF maps are in the CCF map rotation folder, in subfolders corresponding to the angle of rotation.</p><p>Multimodal map creation involves use of the SensoryMapping_Vickers_Jun2520.m script in the Matlab folder.</p><p>We updated the main text to clarify these points and included direct links to scripts relevant to each processing step.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figure 4a: I found it hard to see much of the structure in the Rastermap projection with the viridis colormap - perhaps also because of a red-green color vision impairment. Correspondingly, I had trouble seeing some of the structure that is described in the text or clearer differences between the neuron sortings to PC1 and PC2. Is the point of these panels to show that both PCs identify movement-aligned dynamics or is the argument that they isolate different movement-related response patterns? Using a grayscale colormap as used by Stringer et al might help to see more of the many fine details in the data.</p></list-item></list></disp-quote><p>Authors’ Response: In Fig. 4a the viridis color range is from blue to green to yellow, as indicated in the horizontal scale bar at bottom right. There is no red color in these Rastermap projections, or in any others in this paper. Furthermore, the expanded Rastermap insets in Figs. S4 and S5 provide additional detailed information that may not be clear in Fig 4a and Fig 5a.</p><p>We prefer, therefore, not to change these colormaps, which we use throughout the paper.</p><p>We have provided grayscale png versions of all figures on our GitHub page:</p><p><ext-link ext-link-type="uri" xlink:href="https://github.com/vickerse1/mesoscope_spontaneous/tree/main/grayscale_figures">https://github.com/vickerse1/mesoscope_spontaneous/tree/main/grayscale_figures</ext-link></p><p>In Fig 4a the point of showing both the PC1 and PC2 panels is to demonstrate that they appear to correspond to different aspects of movement (PC1 more to transient walking, both ON and OFF, and PC2 to whisking and sustained ON walk/whisk), and to exhibit differential ability to identify neurons with positive and negative correlations to arousal (PC1 finds both, both PC2 seems to find only the ON neurons).</p><p>We now clarify this in the text at ~lines 696-710, pg 22.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>I find panel 6a a bit too hard to read because the identification and interpretation of the different motifs in the different qualitative episodes is challenging. For example, the text mentions flickering into motif 13 during walk but the majority of that sequence appears to be shaped by what I believe to be motif 11. Motif 11 also occurs prominently in the oscillate state and the unnamed sequence on the left. Is this meaningful or is the emphasis here on times of change between behavioral motifs? The concept of motif flickering should be better explained here.</p></list-item></list></disp-quote><p>Authors’ Response: Here motif 13 corresponds to a syllable that might best be termed “symmetric and ready stance”. This tends to occur just before and after walking, but also during rhythmic wheel balancing movements that appear during the “oscillate” behavior.</p><p>The intent of Fig. 6a is to show that each qualitatively identified behavior (twitch, whisk, walk, and oscillate) corresponds to a period during which a subset of BSOiD motifs flicker back and forth, and that the identity of motifs in this subset differs across the identified qualitative behaviors. This is not to say that a particular motif occurs only during a single identified qualitative behavior. Admittedly, the identification of these qualitative behaviors is a bit arbitrary - future versions of BSOiD (e.g. ASOiD) in fact combine supervised (i.e. arbitrary, top down) and unsupervised (i.e. algorithmic, objective, bottom-up) methods of behavior segmentation in attempt to more reliably identify and label behaviors.</p><p>Flickering appears to be a property of motif transitions in raw BSOiD outputs that have not been temporally smoothed. If one watches the raw video, it seems that this may in fact be an accurate reflection of the manner in which behaviors unfold through time. Each behavior could be thought of, to use terminology from MOSEQ (B Datta), as a series of syllables strung together to make a phrase or sentence. Syllables can repeat over either fast or slow timescales, and may be shared across distinct words and sentences although the order and frequency of their recurrence will likely differ.</p><p>We have clarified these points in the main text at ~lines 917-923, pg 29, and we added motif 13 to the list of motifs for the qualitative behavior labeled “oscillate” in Fig. 6a.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Lines 997-998: I don't understand this argument. Why does the existence of different temporal dynamics make imaging multiple areas 'one of the keys to potentially understanding the nature of their neuronal activity'?</p></list-item></list></disp-quote><p>Authors’ Response: We believe this may be an important point, that comparisons of neurobehavioral alignment across cortical areas cannot be performed by pooling sessions that contain different distributions of dwell times for different behaviors, if in fact that dependence of neural activity on behavior depends on the exact elapsed time since the beginning of the current behavioral “bout”. Again, other reasons that imaging many areas simultaneously would provide a unique advantage over imaging smaller areas one at a time and attempting to pool data across sessions would include the identification of sequences or neural ensembles that span many areas across large distances, or the understanding of distributed coding of behavior (an issue we explore in an upcoming paper).</p><p>We have clarified these points at the location in the Discussion that you have identified. Thank you for your questions and suggestions.</p><disp-quote content-type="editor-comment"><p>Minor</p><p>Line 41: What is the difference between decision, choice, and response periods?</p></disp-quote><p>Authors’ Response: This now reads “...temporal separation of periods during which cortical activity is dominated by activity related to stimulus representation, choice/decision, maintenance of choice, and response or implementation of that choice.”</p><disp-quote content-type="editor-comment"><p>Line 202: What does ambulatory mean in this context?</p></disp-quote><p>Authors’ Response: Here we mean that the mice are able to walk freely on the wheel. In fact they do not actually move through space, so we have changed this to read “able to walk freely on a wheel, as shown in Figs. 1a and 1b”.</p><disp-quote content-type="editor-comment"><p>Is there a reason why 4 mounting posts were used for the dorsal mount but only 1 post was sufficient for the lateral mount?</p></disp-quote><p>Authors’ Response: Here, we assume you mean 2 posts for the side mount and 4 posts for the dorsal mount.</p><p>In general our idea was to use as many posts as possible to provide maximum stability of the preparations and minimize movement artifacts during 2-photon imaging. However, the design of the side mount headpost precluded the straight-forward or easy addition of a right oriented, second arm to its lateral/ventral rim - this would have blocked access of both the 2-photon objective and the right face camera. In the dorsal mount, the symmetrical headpost arms are positioned further back (i.e. posterior), so that the left and right face cameras are not obscured.</p><p>When we created the side mount preparation, we discovered that the 2 vertical 1” support posts were sufficient to provide adequate stability of the preparation and minimize 2-photon imaging movement artifacts. The side mount used two attachment screws on the left side of the headpost, instead of the one screw per side used in the dorsal mount preparation.</p><p>We have included these points/clarifications in the main text at ~lines 217-230, pg 7.</p><disp-quote content-type="editor-comment"><p>Figure S1g appears to be mislabeled.</p></disp-quote><p>Authors’ Response: Yes, on the figure itself that panel was mislabeled as “f” in the original eLife reviewed preprint. We have changed this to read “g”.</p><disp-quote content-type="editor-comment"><p>Line 349 and below: Why is the method called pseudo-widefield imaging?</p></disp-quote><p>Authors’ Response: On the mesoscope, broad spectrum fluorescent light is passed through a series of excitation and emission filters that, based on a series of tests that we performed, allow both reflected blue light and epifluorescence emitted (i.e. Stokes-shifted) green light to reach the CCD camera for detection. Furthermore, the CCD camera (Thorlabs) has a much smaller detector chip than that of the other widefield cameras that we use (RedShirt Imaging and PCO), and we use it to image at an acquisition speed of around 10 Hz maximum, instead of ~30-50 Hz, which is our normal widefield imaging acquisition speed (it also has a slower readout than what we would consider to be a standard or “real” 1-photon widefield imaging camera).</p><p>For these 3 reasons we refer to this as “pseudo-widefield” imaging. We would not use this for sensory activity mapping on the mesoscope - we primarily use it for mapping cortical vasculature and navigating based on our multimodal map to CCF alignment, although it is actually “contaminated” with some GCaMP6s activity during these uses.</p><p>We have briefly clarified this in the text.</p><disp-quote content-type="editor-comment"><p>Figures 4d &amp; e: Do the colors show mean correlations per area? Please add labels and units to the colorbars as done in panel 4a.</p></disp-quote><p>Authors’ Response: For both Figs 4 and 5, we have added the requested labels and units to each scale bar, and have relabeled panels d to say “Rastermap CCF area cell densities”, and panels e to say “mean CCF area corrs w/ neural activity.”</p><p>Thank you for catching these omissions/mislabelings.</p><disp-quote content-type="editor-comment"><p>Line 715: what is superneuron averaging?</p></disp-quote><p>Authors’ Response: This refers to the fact that when Rastermap displays more than ~1000 neurons it averages the activity of each group of adjacent 50 neurons in the sorting to create a single display row, to avoid exceeding the pixel limitations of the display. Each single row representing the average activity of 50 neurons is called a “superneuron” (Stringer et al, 2023; bioRxiv).</p><p>We have modified the text to clarify this point.</p><disp-quote content-type="editor-comment"><p>Line 740: it would be good to mention what exactly the CCF density distribution quantifies.</p></disp-quote><p>Authors’ Response: In each CCF area, a certain percentage of neurons belongs to each Rastermap group. The CCF density distribution is the set of these percentages, or densities, across all CCF areas in the dorsal or side mount preparation being imaged in a particular session. We have clarified this in the text.</p><disp-quote content-type="editor-comment"><p>Line 745: what does 'within each CCF' mean? Does this refer to different areas?</p></disp-quote><p>Authors’ Response: The corrected version of this sentence now reads: “Next, we compared, across all CCF areas, the proportion of neurons within each CCF area that exhibited large positive correlations with walking speed and whisker motion energy.”</p><disp-quote content-type="editor-comment"><p>How were different Rastermap groups identified? Were they selected by hand?</p></disp-quote><p>Authors’ Response: Yes, in Figs. 4, 5, and 6, we selected the identified Rastermap groups “by hand”, based on qualitative similarity of their activity patterns. At the time, there was no available algorithmic or principled means by which to split the Rastermap sort. The current, newer version of Rastermap (Stringer et al, 2023) seems to allow for algorithmic discretization of embedding groups (we have not tested this yet), but it was not available at the time that we performed these preliminary analyses.</p><p>In terms of “correctness” of such discretization or group identification, we intend to address this issue in a more principled manner in upcoming publications. For the purposes of this first paper, we decided that manual identification of groups was sufficient to display the capabilities and outcomes of our methods.</p><p>We clarify this point briefly at several locations in the revised manuscript, throughout the latter part of the Results section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>In &quot;supplementary figures, protocols, methods, and materials&quot;, Figure S1 g is mislabeled as Figure f.</p></disp-quote><p>Authors’ Response: Yes, on the figure itself this panel was mislabeled as “f” in the original reviewed preprint. We have changed this to read “g”.</p><disp-quote content-type="editor-comment"><p>In S1 g, the success rate of the surgical procedure seems quite low. Less than 50% of the mice could be imaged under two-photon. Can the authors elaborate on the criteria and difficulties related to their preparations?</p></disp-quote><p>Authors’ Response: We will elaborate on the difficulties that sometimes hinder success in our preparations in the revised manuscript.</p><p>The success rate indicated to the point of “Spontaneous 2-P imaging (window) reads 13/20, which is 65%, not 50%. The drop to 9/20 by the time one gets to the left edge of “Behavioral Training” indicates that some mice do not master the task.</p><p>Protocol I contains details of the different ways in which mice either die or become unsuitable or “unsuccessful” at each step. These surgeries are rather challenging - they require proper instruction and experience. With the current protocol, our survival rate for the window surgery alone is as high as 75-100%. Some mice can be lost at headpost implantation, in particular if they are low weight or if too much muscle is removed over the auditory areas. Finally, some mice survive windowing but the imageable area of the window might be too small to perform the desired experiment.</p><p>We have added a paragraph detailing this issue in the main text at ~lines 287-320, pg 9.</p><disp-quote content-type="editor-comment"><p>In both Suppl_Movie_S1_dorsal_mount and Suppl_Movie_S1_side_mount provided (Movie S1), the behaviour video quality seems to be unoptimized which will impact the precision of Deeplabcut. As evident, there were multiple instances of mislabeled key points (paws are switched, large jumps of key points, etc) in the videos.</p><p>Many tracked points are in areas of the image that are over-exposed.</p><p>Despite using a high-speed camera, motion blur is obvious.</p><p>Occlusions of one paw by the other paws moving out of frame.</p><p>As Deeplabcut accuracy is key to higher-level motifs generated by BSOi-D, can the authors provide an example of tracking by exclusion/ smoothing of mislabeled points (possibly by the median filtering provided by Deeplabcut), this may help readers address such errors.</p></disp-quote><p>Authors’ Response: We agree that we would want to carefully rerun and carefully curate the outputs of DeepLabCut before making any strong claims about behavioral identification. As the aim of this paper was to establish our methods, we did not feel that this degree of rigor was required at this point.</p><p>It is inevitable that there will be some motion blur and small areas of over-exposure, respectively, when imaging whiskers, which can contain movement components up to ~150 Hz, and when imaging a large area of the mouse, which has planes facing various aspects. For example, perfect orthogonal illumination of both the center of the eye and the surface of the whisker pad on the snout would require two separate infrared light sources. In this case, use of a single LED results in overexposure of areas orthogonal to the direction of the light and underexposure of other aspects, while use of multiple LEDs would partially fix this problem, but still lead to variability in summated light intensity at different locations on the face. We have done our best to deal with these limitations.</p><p>We now briefly point out these limitations in the methods text at ~lines 155-160, pg 5.</p><p>In addition, we have provided additional raw and processed movies and data related to DeepLabCut and BSOiD behavioral analysis in our FigShare+ repository, which is located at:</p><p><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p><disp-quote content-type="editor-comment"><p>In lines 153-154, the authors mentioned that the Deeplabcut model was trained for 650k iterations. In our experience (100-400k), this seems excessive and may result in the model overfitting, yielding incorrect results in unseen data. Echoing point 4, can the authors show the accuracy of their Deeplabut model (training set, validation set, errors, etc).</p></disp-quote><p>Authors’ Response: Our behavioral analysis is preliminary and is included here as an example of our methods, and not to make claims about any specific result. Therefore we believe that the level of detail that you request in our DeepLabCut analysis is beyond the scope of the current paper. However, we would like to point out that we performed many iterations of DeepLabCut runs, across many mice in both preparations, before converging on these preliminary results. We believe that these results are stable and robust.</p><p>We believe that 650k iterations is within the reasonable range suggested by DLC, and that 1 million iterations is given as a reasonable upper bound. This seems to be supported by the literature for example, see Willmore et al, 2022 (“Behavioral and dopaminergic signatures of resilience”, Nature, 124:611, 124-132). Here, in a paper focused squarely on behavioral analysis, DLC training was run with 1.3 million iterations with default parameters.</p><p>We now note, on ~lines 153-154, pg 5, that we used 650K iterations, a number significantly less than the default of 1.03 million, to avoid overfitting.</p><disp-quote content-type="editor-comment"><p>In lines 140-141, the authors mentioned the use of slicing to downsample their data. Have any precautions, such as a low pass filter, been taken to avoid aliasing?</p></disp-quote><p>Authors’ Response: Most of the 2-photon data we present was acquired at ~3 Hz and upsampled to 10 Hz. Most of the behavioral data was downsampled from 5000 Hz to 10 Hz by slicing, as stated. We did not apply any low-pass filter to the behavioral data before sampling. The behavioral variables have heterogeneous real sampling/measurement rates - for example, pupil diameter and whisker motion energy are sampled at 30 Hz, and walk speed is sampled at 100 Hz. In addition, the 2-photon acquisition rate varied across sessions.</p><p>These facts made principled, standardized low-pass filtering difficult to implement. We chose rather to use a common resampling rate of 10 Hz in an unbiased manner. This downsampled 10 Hz rate is also used by B-SOiD to find transitions between behavioral motifs (Hsu and Yttri, 2021).</p><p>We do not think that aliasing is a major factor because the real rate of change of our Ca2+ indicator fluorescence and behavioral variables was, with the possible exception of whisker motion energy, likely at or below 10 Hz.</p><p>We now include a brief statement to this effect in the methods text at ~lines 142-146, pg. 4.</p><disp-quote content-type="editor-comment"><p>Line 288-299, the authors have made considerable effort to compensate for the curvature of the brain which is particularly important when imaging the whole dorsal cortex. Can the authors provide performance metrics and related details on how well the combination of online curvature field correction (ScanImage) and fast-z &quot;sawtooth&quot;/&quot;step&quot; (Sofroniew, 2016)?</p></disp-quote><p>Authors’ Response: We did not perform additional “ground-truth” experiments that would allow us to make definitive statements concerning field curvature, as was done in the initial eLife Thorlabs mesoscope paper (Sofroniew et al, 2016).</p><p>We estimate that we experience ~200 micrometers of depth offset across 2.5 mm - for example, if the objective is orthogonal to our 10 mm radius bend window and centered at the apex of its convexity, a small ROI located at the lateral edge of the side mount preparation would need to be positioned around 200 micrometers below that of an equivalent ROI placed near the apex in order to image neurons at the same cortical layer/depth, and would be at close to the same depth as an ROI placed at or near the midline, at the medial edge of the window. We determined this by examining the geometry of our cranial windows, and by comparing z-depth information from adjacent sessions in the same mouse, the first of which used a large FOV and the second of which used multiple small FOVs optimized so that they sampled from the same cortical layers across areas.</p><p>We have included this brief explanation in the main text at ~lines 300-311, pg 9.</p><disp-quote content-type="editor-comment"><p>In lines 513-515, the authors mentioned that the vasculature pattern can change over the course of the experiment which then requires to re-perform the realignment procedure. How stable is the vasculature pattern? Would laser speckle contrast yield more reliable results?</p></disp-quote><p>Authors’ Response: In general the changes in vasculature we observed were minimal but involved the following: (i) sometimes a vessel was displaced or moved during the window surgery, (ii) sometimes a vessel, in particular the sagittal sinus, enlarged or increased its apparent diameter over time if it is not properly pressured by the cranial window, and (iii) sometimes an area experiencing window pressure that is too low could, over time, show outgrowth of fine vascular endings. The most common of these was (i), and (iii) was perhaps the least common. In general the vasculature was quite stable.</p><p>We have added this brief discussion of potential vasculature changes after cranial window surgery to the main text at ~lines 286-293, pg 9.</p><p>We already mentioned, in the main text of the original eLife reviewed preprint, that we re-imaged the multimodal map (MMM) every 30-60 days or whenever changes in vasculature are observed, in order to maintain a high accuracy of CCF alignment over time. See ~lines 507-511, pg 16.</p><p>We are not very familiar with laser speckle contrast, and it seems like a technique that could conceivably improve the fine-grained accuracy of our MMM-CCF alignment in some instances. We will try this in the future, but for now it seems like our alignments are largely constrained by several large blood vessels present in any given FOV, and so it is unclear how we would incorporate such fine-grained modifications without applying local non-rigid manipulations of our images.</p><disp-quote content-type="editor-comment"><p>In lines 588-598, the authors mentioned that the occasional use of online fast-z corrections yielded no difference. However, it seems that the combination of the online fast-z correction yielded &quot;cleaner&quot; raster maps (Figure S3)?</p></disp-quote><p>Authors’ Response: The Rastermaps in Fig S3a and b are qualitatively similar. We do not believe that any systematic difference exists between their clustering or alignments, and we did not observe any such differences in other sessions that either used or didn’t use online fast-z motion correction.</p><p>We now provide raw data and analysis files corresponding to the sessions shown in Fig S3 (and other data-containing figures) on FigShare+ at:</p><p><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p><disp-quote content-type="editor-comment"><p>Ideally, the datasets contained in the paper should be available on an open repository for others to examine. I could not find a clear statement about data availability. Please include a linked repo or state why this is not possible.</p></disp-quote><p>Authors’ Response: We have made ~500 GB of raw data and preliminary analysis files publicly available on FigShare+ for the example sessions shown in Figures 2, 3, 4, 5, 6, S3, and S6. We ask to be cited and given due credit for any fair use of this data.</p><p>The data is located here:</p><p>Vickers, Evan; A. McCormick, David (2024). Pan-cortical 2-photon mesoscopic imaging and neurobehavioral alignment in awake, behaving mice. Figshare+. Collection:</p><p><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25452/figshare.plus.c.7052513">https://doi.org/10.25452/figshare.plus.c.7052513</ext-link></p><p>We intend to release a complete data set to the public as a Dandiset on the DANDI archive in conjunction with second and third in-depth analysis papers that are currently in preparation.</p></body></sub-article></article>