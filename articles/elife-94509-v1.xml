<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">94509</article-id><article-id pub-id-type="doi">10.7554/eLife.94509</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94509.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Speech and music recruit frequency-specific distributed and overlapping cortical networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>te Rietmolen</surname><given-names>Noémie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5532-6118</contrib-id><email>noemieter@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Mercier</surname><given-names>Manuel R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6358-4734</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Trébuchon</surname><given-names>Agnès</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0049-064X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Schön</surname><given-names>Daniele</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4472-4150</contrib-id><email>daniele.schon@univ-amu.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Institute for Language, Communication, and the Brain, Aix-Marseille University</institution></institution-wrap><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>Aix Marseille Université, INSERM, INS, Institut de Neurosciences des Systèmes</institution></institution-wrap><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05jrr4320</institution-id><institution>APHM, Hôpital de la Timone, Service de Neurophysiologie Clinique</institution></institution-wrap><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>07</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP94509</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-12-03"><day>03</day><month>12</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-11-15"><day>15</day><month>11</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.08.511398"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-08"><day>08</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94509.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-05"><day>05</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94509.2"/></event></pub-history><permissions><copyright-statement>© 2024, te Rietmolen et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>te Rietmolen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-94509-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-94509-figures-v1.pdf"/><abstract><p>To what extent does speech and music processing rely on domain-specific and domain-general neural networks? Using whole-brain intracranial EEG recordings in 18 epilepsy patients listening to natural, continuous speech or music, we investigated the presence of frequency-specific and network-level brain activity. We combined it with a statistical approach in which a clear operational distinction is made between <italic>shared</italic>, <italic>preferred,</italic> and domain-<italic>selective</italic> neural responses. We show that the majority of focal and network-level neural activity is shared between speech and music processing. Our data also reveal an absence of anatomical regional selectivity. Instead, domain-selective neural responses are restricted to distributed and frequency-specific coherent oscillations, typical of spectral fingerprints. Our work highlights the importance of considering natural stimuli and brain dynamics in their full complexity to map cognitive and brain functions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory neuroscience</kwd><kwd>network dynamics</kwd><kwd>language</kwd><kwd>neurophysiology</kwd><kwd>cognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-21-CE28-0010</award-id><principal-award-recipient><name><surname>Schön</surname><given-names>Daniele</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-20-CE28-0007-01</award-id><principal-award-recipient><name><surname>Morillon</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-17-EURE-0029</award-id><principal-award-recipient><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><name><surname>Schön</surname><given-names>Daniele</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-CoG-101043344</award-id><principal-award-recipient><name><surname>Morillon</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-16-CONV-0002</award-id><principal-award-recipient><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><name><surname>Schön</surname><given-names>Daniele</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007586</institution-id><institution>Aix-Marseille Université</institution></institution-wrap></funding-source><award-id>A*MIDEX AMX-19-IET-004</award-id><principal-award-recipient><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><name><surname>Schön</surname><given-names>Daniele</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The neural response to natural speech and music processing is mostly shared between the domains, with additional evidence for selectivity in distributed (i.e. not regional) and frequency-specific neural activity.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The advent of neuroscience continues the longstanding debate on the origins of music and language—that fascinated Rousseau and Darwin (<xref ref-type="bibr" rid="bib47">Kivy, 1959</xref>; <xref ref-type="bibr" rid="bib78">Rousseau, 2009</xref>)—on new biological ground: evidence for the existence of selective and/or shared neural populations involved in their processing. The question on functional selectivity versus domain-general mechanisms is closely related to the question of the nature of the neural code: Are representations <italic>sparse</italic> (and localized) or <italic>distributed</italic>? While the former allows to explicitly represent any stimulus in a small number of neurons, it would require an intractable number of neurons to represent all possible stimuli. Experimental evidence instead suggests that stimulus identification is achieved through a population code, implemented by neural coupling in a distributed dynamical system (<xref ref-type="bibr" rid="bib8">Bizley and Cohen, 2013</xref>; <xref ref-type="bibr" rid="bib76">Rissman and Wagner, 2012</xref>). The question of the nature of the neural code has tremendous implications: it defines an epistemological posture on how to map cognitive and brain functions. This, in turn, affects both the definition of cognitive operations—what is actually computed—as well as the way we look at the data—looking for differences or similarities.</p><p>Neuroimaging studies report mixed evidence of selectivity and resource sharing. On one hand, one can find claims for a clear distinction between brain regions exclusively dedicated to language versus other cognitive processes (<xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib26">Fedorenko and Blank, 2020</xref>; <xref ref-type="bibr" rid="bib31">Friederici, 2020</xref>) and for the existence of specific and separate neural populations for speech, music, and song (<xref ref-type="bibr" rid="bib9">Boebinger et al., 2021</xref>; <xref ref-type="bibr" rid="bib67">Norman-Haignere et al., 2022</xref>). On the other hand, other neuroimaging studies suggest that the brain regions that support language and speech also support nonlinguistic functions (<xref ref-type="bibr" rid="bib1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">Fadiga et al., 2009</xref>; <xref ref-type="bibr" rid="bib48">Koelsch, 2011</xref>; <xref ref-type="bibr" rid="bib57">Menon et al., 2002</xref>; <xref ref-type="bibr" rid="bib77">Robert et al., 2023</xref>; <xref ref-type="bibr" rid="bib83">Schön et al., 2010</xref>). This point is often put forward when interpreting the positive impact music training can have on different levels of speech and language processing (<xref ref-type="bibr" rid="bib27">Flaugnacco et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">François et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Kraus and Chandrasekaran, 2010</xref>; <xref ref-type="bibr" rid="bib82">Schön et al., 2004</xref>).</p><p>Several elements may account for these different findings. The very first may rely on the definition of a brain region. This can be considered as a set of functionally homogeneous but spatially distributed voxels, or, alternatively, as an anatomical landmark as those used in brain atlases (e.g. inferior frontal gyrus). However, observing functional regional selectivity in a distributed pattern is not incompatible with the observation of an absence of anatomical regional selectivity: a selective set of voxels may exist within an anatomically non-selective region. A second element concerns the choice of the stimuli. Some of the studies claiming functional selectivity used rather short auditory stimuli (<xref ref-type="bibr" rid="bib9">Boebinger et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Norman-Haignere et al., 2022</xref>). Besides the low ecological validity of such stimuli that may reduce the generalizability of the findings (<xref ref-type="bibr" rid="bib89">Theunissen et al., 2000</xref>), their comparison further relies on the assumption that speech and music share similar cognitive time constants. However, speech unfolds faster than music (<xref ref-type="bibr" rid="bib19">Ding et al., 2017</xref>), and while a linguistic phrase is typically shorter than a second (<xref ref-type="bibr" rid="bib43">Inbar et al., 2020</xref>), a melodic phrase is an order of magnitude longer. Moreover, balancing the complexity/simplicity of linguistic and musical stimuli can be challenging, and musical stimuli are often reduced to very simple melodies played on a synthesizer. These simple melodies mainly induce pitch processing in associative auditory regions (<xref ref-type="bibr" rid="bib37">Griffiths et al., 2010</xref>) but do not recruit the entire dual-stream auditory pathways (<xref ref-type="bibr" rid="bib93">Zatorre et al., 2007</xref>). Overall, while short and simple stimuli may be sufficient to induce linguistic processing, they might not be cognitively relevant musical stimuli. Finally, another element concerns the data at stake. Most studies that compared language and music processing examined functional MRI data (<xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib64">Nieto-Castañón and Fedorenko, 2012</xref>). Here, we would like to consider cognition as resulting from interactions among functionally specialized but widely distributed brain networks and adopt an approach in which large-scale and frequency-specific neural dynamics are characterized. This approach rests on the idea that the canonical computations that underlie cognition and behavior are anchored in population dynamics of interacting functional modules (<xref ref-type="bibr" rid="bib11">Buzsáki and Vöröslakos, 2023</xref>; <xref ref-type="bibr" rid="bib79">Safaie et al., 2023</xref>) and bound to spectral fingerprints consisting of network- and frequency-specific coherent oscillations (<xref ref-type="bibr" rid="bib84">Siegel et al., 2012</xref>). This framework requires relying on time-resolved neurophysiological recordings (M/EEG) and—rather than focusing only on the amplitude of the high-frequency activity (HFa), a common approach in the literature involving human intracranial EEG recordings (<xref ref-type="bibr" rid="bib53">Martin et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Norman-Haignere et al., 2022</xref>; <xref ref-type="bibr" rid="bib68">Oganian and Chang, 2019</xref>)—to investigate the entire frequency spectrum of neural activity. Indeed, while HFa amplitude is a good proxy of focal neural spiking (<xref ref-type="bibr" rid="bib51">Le Van Quyen et al., 2010</xref>; <xref ref-type="bibr" rid="bib75">Ray and Maunsell, 2011</xref>), large-scale neuronal interactions mainly rely on slower dynamics (<xref ref-type="bibr" rid="bib45">Kayser et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Kopell et al., 2000</xref>; <xref ref-type="bibr" rid="bib84">Siegel et al., 2012</xref>).</p><p>Following the reasoning developed above, we suggest that the study of selectivity of music and language processing should carefully consider the following points: First, the use of ecologically valid stimuli, both in terms of content and duration. Second, a within-subject approach comparing both conditions. Third, aiming for high spatial sensitivity. Fourth, considering not only one type of neural activity (broadband, HFa amplitude) but the entire frequency spectrum of the neurophysiological signal. Fifth, use a broad range of complementary analyses, including connectivity, and take into account individual variability. Finally, we suggest that terms should be operationally defined based on statistical tests, which results in a clear distinction between shared, selective, and preferred activity. That is, let A and B be two investigated cognitive functions, ‘shared’ would be a neural population that (compared to a baseline) significantly and equally contributes to the processing of both A and B; ‘selective’ would be a neural population that exclusively contributes to the processing of A or B (e.g. significant for A but not B); and ‘preferred’ would be a neural population that significantly contributes to the processing of both A and B, but more prominently for A or B (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Concepts, stimuli, and recordings.</title><p>(<bold>A</bold>) Conceptual definition of selective, preferred, and shared neural processes. Illustration of the continua between speech and music selectivity, speech and music preference, and shared resources. ‘Selective’ responses are neural responses significant for one domain but not the other, and with a significant difference between domains (for speech top left; for music bottom right). ‘Preferred’ responses correspond to neural responses that occur during both speech and music processing, but with a significantly stronger response for one domain over the other (striped triangles). Finally, ‘shared’ responses occur when there are no significant differences between domains, and there is a significant neural response to at least one of the two stimuli (visible along the diagonal). If neither domain produces a significant neural response, the difference is not assessed (lower left square). (<bold>B</bold>) Stimuli. Modulation spectrum of the acoustic temporal envelope of the continuous, 10 min long speech and music stimuli. (<bold>C</bold>) Anatomical localization of the stereotactic EEG (sEEG) electrodes for each patient (N=18). (<bold>D</bold>) Anatomical localization of the sEEG electrodes for each anatomical region. Abbreviations according to the Human Brainnetome Atlas (<xref ref-type="bibr" rid="bib24">Fan et al., 2016</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig1-v1.tif"/></fig><p>In an effort to take into account all the above challenges and to precisely quantify the degree of shared, preferred, and selective responses both at the levels of the channels and anatomical regions (<xref ref-type="fig" rid="fig1">Figure 1C and D</xref>), we conducted an experiment on 18 pharmacoresistant epileptic patients explored with stereotactic EEG (sEEG) electrodes. Patients listened to long and ecological audio-recordings of speech and music (10 min each). We investigated stimulus encoding, spectral content of the neural activity, and brain connectivity over the entire frequency spectrum (from 1 to 120 Hz; i.e. delta band to HFa). Finally, we carefully distinguished between the three different categories of neural responses described above: shared, selective, and preferred across the two investigated cognitive domains. Our results reveal that the majority of neural responses are shared between natural speech and music, and they highlight an absence of anatomical regional selectivity. Instead, we found neural selectivity to be restricted to distributed and frequency-specific coherent oscillations, typical of spectral fingerprints.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Anatomical regional neural activity is mostly non-domain selective to speech or music</title><p>To investigate the presence of domain selectivity during ecological perception of speech and music, we first analyzed the neural responses to these two cognitive domains in both a spatially and spectrally resolved manner, with respect to two baseline conditions: one in which patients passively listened to pure tones (each 30 ms in duration), the other in which they passively listened to isolated syllables (/ba/ or /pa/, see Methods). Here, we will report the results using pure tones data as baseline, but note that the results using syllables data as baseline are highly similar (see <xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig6">6</xref> and corresponding figure supplements).We classified, for each canonical frequency band, each channel into one of the categories mentioned above, i.e., shared, selective, or preferred (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), by examining whether speech and/or music differ from baseline and whether they differ from each other. We also considered both activations and deactivations, compared to baseline, as both index a modulation of neural population activity, and both have been linked with cognitive processes (<xref ref-type="bibr" rid="bib73">Pfurtscheller and Lopes da Silva, 1999</xref>; <xref ref-type="bibr" rid="bib74">Proix et al., 2022</xref>). However, because our aim was not to interpret specific increase or decrease with respect to the baseline, we here simply consider significant deviations from the baseline. In other words, when estimating selectivity, it is the strength of the response that matters, not its direction (activation, deactivation). Overall, neural responses are predominantly shared between the two domains, accounting for ~70% of the channels which showed a significant response compared to baseline (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). The preferred category is also systematically present, accounting for 3–15% of significant neural responses, across frequency bands. Selective responses are more present in the lower frequency bands (~30% up to the alpha band), and quite marginal in the HFa band (6–12%).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Power spectrum analyses of activations (speech or music&gt;tones).</title><p>(<bold>A–F</bold>) Neural responses to speech and/or music for the six canonical frequency bands. Only significant activations compared to the baseline condition (pure tones listening) are reported (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for uncategorized, continuous results). Nested pie charts indicate: (1) in the center, the percentage of channels that showed a significant response to speech and/or music. (2) The outer pie indicates the percentage of channels, relative to the center, classified as shared (white), selective (light gray), and preferred (dark gray). (3) The inner pie indicates, for the selective (plain) and preferred (pattern) categories, the proportion of channels that were (more) responsive to speech (red) or music (blue). Brain plots indicate: Distribution of shared (white) and selective (red/blue) stereotactic EEG (sEEG) channels projected on the brain surface. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Power spectrum analyses of activations (speech or music&gt;syllables).</title><p>(<bold>A–F</bold>) Neural responses to speech and/or music for the six canonical frequency bands. Only significant activations compared to the baseline condition (syllables listening) are reported. Same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Power spectrum analyses of activations (speech or music&gt;tones) for each hemisphere separately and for the six frequency bands (<bold>A-F</bold>).</title><p>Same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Contrast between the neural responses to speech and music, for the six (<bold>A-F</bold>) canonical frequency bands (tones baseline).</title><p>Distribution of uncategorized, continuous t-values from the speech vs. music permutation test, projected on the brain surface. Only stereotactic EEG (sEEG) channels for which speech and/or music is significantly stronger (left column) or weaker (right column) than the baseline condition (pure tones listening) are reported. Channels for which the speech vs. music contrast is significant are circled in black. Channels for which the speech vs. music contrast is not significant are circled in white. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig2-figsupp3-v1.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Power spectrum analyses of deactivations (speech or music&lt;tones).</title><p>(<bold>A–F</bold>) Neural responses to speech and/or music for the six canonical frequency bands. Only significant deactivations compared to the baseline condition (pure tones listening) are reported. Same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Power spectrum analyses of deactivations (speech or music&lt;syllables).</title><p>(<bold>A–F</bold>) Neural responses to speech and/or music for the six canonical frequency bands. Only significant deactivations compared to the baseline condition (syllables listening) are reported. Same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Power spectrum analyses of deactivations (speech or music&lt;tones) for each hemisphere separately.</title><p>(<bold>A-F</bold>) Neural responses to speech and/or music for the six canonical frequency bands. Same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Results are significant at q&lt;0.01 (N=18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig3-figsupp2-v1.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cross-frequency channel selectivity for the power spectrum analyses.</title><p>(<bold>A-F</bold>) Percentage of channels that exclusively respond to speech (red) or music (blue) across different frequency bands. For each plot, the first (leftmost) value corresponds to the percentage (%) of channels displaying a selective response in a specific frequency band (either activation or deactivation, compared to the baseline condition of pure tones listening). In the next value, we remove the channels that are significantly responsive in the other domain (i.e. no longer exclusive) for the following frequency band (e.g. in panel <bold>A</bold>: speech selective in delta; speech selective in delta XOR music responsive in theta; speech selective in delta XOR music responsive in theta XOR music responsive in alpha; and so forth). The black dots at the bottom of the graph indicate which frequency bands were successively included in the analysis. Note that channels remaining selective across frequency bands did not necessarily respond selectively in every band. They simply never showed a significant response to the other domain in the other bands.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Cross-frequency channel selectivity for the power spectrum analyses (syllables baseline).</title><p>(<bold>A-F</bold>) Percentage of channels that selectively respond to speech (red) or music (blue) across different frequency bands, compared to the baseline condition (syllable listening). Same conventions as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig4-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Population prevalence for the power spectral analyses of activations (speech or music&gt;tones; N=18).</title><p>(<bold>A–F</bold>) Population prevalence of shared or selective responses for the six canonical frequency bands, per anatomical region (note that preferred responses are excluded). Only significant activations compared to the baseline condition (pure tones listening) are reported. Regions (on the x-axis) were included in the analyses if they had been explored by minimally two patients with minimally two significant channels. Patients were considered to show regional selective processing when all their channels in a given region responded selectively to either speech (red) or music (blue). When regions contained a combination of channels with speech selective, music selective, or shared responses, the patient was considered to show shared (white) processing in this region. The height of the lollipop (y-axis) indicates the percentage of patients over the total number of explored patients in that given region. The size of the lollipop indicates the number of patients. As an example, in panel F (high-frequency activity [HFa] band), most lollipops are white with a height of 100%, indicating that, in these regions, all patients presented a shared response profile. However, in the left inferior parietal lobule (IPL, left) one patient (out of the seven explored) shows speech selective processing (filled red circle). A fully selective region would thus show a fixed-color full height across all frequency bands. Abbreviations according to the Human Brainnetome Atlas (<xref ref-type="bibr" rid="bib24">Fan et al., 2016</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Population prevalence for the power spectral analyses of activations (speech or music&gt;syllables; N=18).</title><p>(<bold>A–F</bold>) Population prevalence of shared or selective responses for the six canonical frequency bands, per anatomical region. Only significant activations compared to the baseline condition (syllables listening) are reported. Same conventions as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig5-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Population prevalence for the power spectral analyses of deactivations (speech or music&lt;tones; N=18).</title><p>(<bold>A–F</bold>) Population prevalence of shared or selective responses for the six canonical frequency bands, per anatomical region. Only significant deactivations compared to the baseline condition (pure tones listening) are reported. Same conventions as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Population prevalence for the power spectral analyses of deactivations (speech or music&lt;syllables; N=18).</title><p>(<bold>A–F</bold>) Population prevalence of shared or selective responses for the six canonical frequency bands, per anatomical region. Only significant deactivations compared to the baseline condition (syllables listening) are reported. Same conventions as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig6-figsupp1-v1.tif"/></fig></fig-group><p>The spatial distribution of the spectrally resolved responses corresponds to the network typically involved in speech and music perception. This network encompasses both ventral and dorsal auditory pathways, extending well beyond the auditory cortex and hence beyond auditory processing that may result from differences in the acoustic properties of our baseline and experimental stimuli. This is the case for overall responses but also when only looking at shared responses. For instance, HFa shared responses represent 74–86% of the overall significant HFa responses, and are visible in the left superior and middle temporal gyri, inferior parietal lobule, and the precentral, middle, and inferior frontal gyri (<xref ref-type="fig" rid="fig2">Figures 2F</xref> and <xref ref-type="fig" rid="fig3">3F</xref>). The left hemisphere appears to be more strongly involved, but this result is biased by the inclusion of a majority of patients with a left hemisphere exploration (<xref ref-type="fig" rid="fig1">Figure 1C and D</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Also, when inspecting left and right hemispheres separately, the patterns of shared, selective, and preferred responses remain similar across hemispheres across frequency bands (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for activation and deactivation, respectively). Both domains displayed a comparable percentage of selective responses across frequency bands (<xref ref-type="fig" rid="fig4">Figure 4</xref>, first values of each plot). When considering separately activation (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and deactivation (<xref ref-type="fig" rid="fig3">Figure 3</xref>) responses, speech and music showed complementary patterns: for low frequencies (&lt;15 Hz) speech selective (and preferred) responses were mostly deactivations and music responses activations compared to baseline, and this pattern reversed for high frequencies (&gt;15 Hz).</p><p>Next, we investigated whether the channels selectivity (to speech or music) observed in a given frequency band was robust across frequency bands (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We estimated the cross-frequency channel selectivity, that is the percentage of channels that selectively respond to speech or music across different frequency bands. We first computed the percentage of total channels selective for speech and music (either activated or deactivated compared to baseline) in a given frequency band. We then verified whether these channels were unresponsive to the other domain in the other frequency bands. This was done by examining each frequency band in turn and deducting any channels that showed a significant neural response to the other domain. When considering the entire frequency spectrum, the percentage of total channels being selective to speech or music is ~4 times less than when considering a single frequency band. For instance, while up to 8% of the total channels are selective for speech (or music) in the theta band, this percentage always drops to ~2% when considering the cross-frequency channel selectivity.</p><p>Critically, we found no evidence of anatomical regional selectivity, i.e., of a simple anatomo-functional spatial code (see <xref ref-type="fig" rid="fig1">Figure 1D</xref> for the definition of anatomical regions). We estimated, for each frequency band, activation/deactivation responses, and anatomical region, the proportion of patients showing selectivity for speech or music, by means of a population prevalence analysis (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>; see Methods). This analysis revealed that, for the majority of patients, first of all, in most regions there were channels that responded to both speech and music (indicative of shared responses at the anatomical regional level), and, second of all, for the minority of anatomical regions for which a selectivity for the same domain (speech or music) was observed across multiple patients, this selectivity does not hold when also considering other frequency bands and activation/deactivation responses. For instance, while the left anterior middle temporal gyrus shows delta activity selective to music (<xref ref-type="fig" rid="fig2">Figures 2A</xref> and <xref ref-type="fig" rid="fig5">5A</xref>), it shows low-gamma activity selective to speech (<xref ref-type="fig" rid="fig2">Figures 2E</xref> and <xref ref-type="fig" rid="fig5">5E</xref>). The left superior temporal gyrus and pSTS (posterior superior temporal sulcus), which show selective activations in the theta and alpha bands for music (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>), show selective deactivations in the same bands for speech (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>) and a majority of shared activations in the HFa (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). This absence of anatomical regional selectivity is also evident when looking at the uncategorized, continuous results (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><p>Overall, these results reveal an absence of regional selectivity to speech or music under ecological conditions. Instead, selective responses coexist in space across different frequency bands. But, while selectivity may not be striking at the level of anatomical regional activity, it may still be present at the network level. To investigate this hypothesis, we explored the connectivity between the auditory cortex and the rest of the brain. And, to functionally define the auditory cortex for each patient, we first investigated the relation between the auditory signal itself and the brain response to identify which sEEG channels (spatial) best encode the dynamics of the auditory stimuli.</p></sec><sec id="s2-2"><title>Low-frequency neural activity best encodes acoustic dynamics</title><p>We linearly modeled the neurophysiological responses to continuous speech and music using temporal response functions (TRFs). Based on previous studies (<xref ref-type="bibr" rid="bib68">Oganian and Chang, 2019</xref>; <xref ref-type="bibr" rid="bib95">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="bib96">Zuk et al., 2021</xref>), we compared four TRF models. From both stimuli, we extracted the continuous, broadband temporal envelope (henceforth ‘envelope’) and the discrete acoustic onset edges (henceforth ‘peakRate’; see Methods) and we quantified how well these two acoustic features are encoded by either the low-frequency (LF) band (1–9 Hz) or the high-frequency amplitude (80–120 Hz) bands. For each model, we estimated the percentage of total channels for which a significant encoding was observed during speech and/or music listening. The model for which most channels significantly encoded speech and/or music acoustic features corresponded to the model in which LF neural activity encoded the <italic>peakRates</italic> (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). In general, the LF activity encodes the acoustic features in significantly more channels than the HFa amplitude (<italic>peakRate</italic> &amp; LF vs. <italic>peakRate</italic> &amp; HFa amplitude comparison: t=13.39, q&lt;0.0001; <italic>peakRate</italic> &amp; LF vs. <italic>envelope</italic> &amp; HFa amplitude comparison: t=9.55, q&lt;0.0001). Note that this effect is not caused by the asymmetric comparison of bandpassed LF to HFa amplitude as model comparisons using the same extraction technique for both signals did not change the results (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). Then, while the <italic>peakRates</italic> are encoded by numerically more channels than the instantaneous envelope, this difference was not significant (<italic>peakRate</italic> &amp; LF vs. <italic>envelope</italic> &amp; LF comparison: t=1.93, q=0.42).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Temporal response function (TRF) analyses.</title><p>(<bold>A</bold>) Model comparison. On the top left a toy model illustrates the use of Venn diagrams comparing the winning model (peakRate in low frequency [LF]) to each of the three other models for speech and music (pooled). Four TRF models were investigated to quantify the encoding of the instantaneous envelope and the discrete acoustic onset edges (peakRate) by either the LF band or the high-frequency amplitude. The ‘peakRate &amp; LF’ model significantly captures the largest proportion of channels, and is, therefore, considered the winning model. The percentages on the top (in bold) indicate the percentage of total channels for which a significant encoding was observed during speech and/or music listening in either of the two compared models. In the Venn diagram, we indicate, out of all significant channels, the percentage that responded in the winning model (left) or in the alternative model (right). The middle part indicates the percentage of channels shared between the winning and the alternative model (percentage not shown). q-Values indicate pairwise model comparisons (Wilcoxon signed-rank test, FDR-corrected). (<bold>B and C</bold>) peakRate &amp; LF model: Spatial distribution of stereotactic EEG (sEEG) channels wherein LF neural activity significantly encodes the speech (red) and music (blue) peakRates. Higher prediction accuracy (Pearson’s r) is indicated by increasing color saturation. All results are significant at q&lt;0.01 (N=18). (<bold>D</bold>) Anatomical localization of the best encoding channel within the left hemisphere for each patient (N=15), as estimated by the ‘peakRate &amp; LF’ model (averaged across speech and music). These channels are all within the auditory cortex and serve as seeds for subsequent connectivity analyses. (<bold>E</bold>) TRFs averaged across the seed channels (N=15), for speech and music. (<bold>F</bold>) Prediction accuracy (Pearson’s r) of the neural activity of each seed channel, for speech and music.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Temporal response function (TRF) model comparison of low-frequency (LF) amplitude and high-frequency activity (HFa) amplitude.</title><p>Models were investigated to quantify the encoding of the instantaneous envelope and the discrete acoustic onset edges (peakRate) by either the LF amplitude or the HFa amplitude. The ‘peakRate &amp; LF amplitude’ model significantly captures the largest proportion of channels, and is, therefore, considered the winning model. Same conventions as in <xref ref-type="fig" rid="fig7">Figure 7A</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig7-figsupp1-v1.tif"/></fig></fig-group><p>Furthermore, we show that the <italic>peakRates</italic> are encoded by the LF neural activity throughout the cortex, for both speech and music (<xref ref-type="fig" rid="fig7">Figure 7B and C</xref>). More precisely, the regions wherein neural activity significantly encodes the acoustic structure of the stimuli go well beyond auditory regions and extend to the temporo-parietal junction, motor cortex, inferior frontal gyrus, and anterior and central sections of the superior and middle temporal gyrus. In particular, the strongest encoding values for speech are observed in the typical left-hemispheric language network, comprising the upper bank of the superior temporal gyrus, the posterior part of the inferior frontal gyrus, and the premotor cortex (<xref ref-type="bibr" rid="bib52">Malik-Moraleda et al., 2022</xref>). Still, as expected, the best cortical tracking of the acoustic structure takes place in the auditory cortex, for both speech and music (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). In other words, the best encoding channels are the same for speech and music and are those located closest to—or in—the primary auditory cortex. While the left hemisphere appears to be more strongly involved, this result is biased by the inclusion of a majority of patients with a left hemisphere exploration (see <xref ref-type="fig" rid="fig1">Figure 1C and D</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Proportionally, we found no difference in the number of significant channels between hemispheres (i.e. speech: 41% and 44% for left and right hemispheres respectively; music: 22% and 24% for left and right hemispheres, respectively). Finally, the <italic>peakRate</italic> &amp; LF model, i.e., the model that captures the largest proportion of significant channels during speech and/or music perception (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), yields for both classes of stimuli a similar TRF shape (<xref ref-type="fig" rid="fig7">Figure 7E</xref>) as well as similar prediction accuracy scores (Pearson’s r), of up to 0.55 (<xref ref-type="fig" rid="fig7">Figure 7F</xref>).</p></sec><sec id="s2-3"><title>Connections of the auditory cortex are also mostly non-domain selective to speech or music</title><p>Seed-based connectivity analyses first revealed that, during speech or music perception, the auditory cortex is mostly connected to the rest of the brain through slow neural dynamics, with ~33% of the channels showing coherence values higher than the surrogate distribution at delta rate, and only ~12% at HFa (<xref ref-type="fig" rid="fig8">Figure 8</xref>, see also <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> for uncategorized, continuous results). Across frequencies, most of the significant connections are shared between the two cognitive domains (~70%), followed by preferred (~15%) and selective connections (~12%). Selectivity is nonetheless homogeneously present in all frequency bands (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Importantly, selectivity is again frequency-specific (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Estimating the cross-frequency channel selectivity, the percentage of total connections being selective to speech or music is at zero for all frequency bands except for the delta range (speech = 0.19%; music = 0.06%). Hence, selectivity is only visible at the level of frequency-specific distributed networks. Finally, here again no anatomical regional selectivity is observed, i.e., not a single cortical region is solely selective to speech or music. Rather, in every cortical region, the majority of patients show shared responses at the regional level, as estimated by the population prevalence analysis (<xref ref-type="fig" rid="fig10">Figure 10</xref>).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Seed-based functional connectivity analyses.</title><p>(<bold>A–F</bold>) Significant coherence responses to speech and/or music for the six canonical frequency bands (see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> for uncategorized, continuous results). The seed was located in the left auditory cortex (see <xref ref-type="fig" rid="fig7">Figure 7D</xref>). Same conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, except for the center percentage in the nested pie charts which, here, reflects the percentage of channels significantly connected to the seed. Results are significant at q&lt;0.01 (N=15).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Contrast between the coherence responses to speech and music, for the six canonical frequency bands (<bold>A-F</bold>).</title><p>Distribution of uncategorized, continuous differences in coherence values between speech and music, projected on the brain surface. Same conventions as in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Results are significant at q&lt;0.01 (N=15).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig8-figsupp1-v1.tif"/></fig></fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Cross-frequency channel selectivity for the connectivity analyses.</title><p>Percentage of channels that showed selective coherence with the primary auditory cortex in speech (red) or music (blue) across different frequency bands (<bold>A-F</bold>). Same conventions as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig9-v1.tif"/></fig><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Population prevalence for the connectivity analyses for the six (<bold>A-F</bold>) canonical frequency bands (N=15).</title><p>Same conventions as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-fig10-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we investigated the existence of domain selectivity for speech and music under ecological conditions. We capitalized on the high spatiotemporal sensitivity of human stereotactic recordings (sEEG) to thoroughly evaluate the presence of selective neural responses—estimated both at the level of individual sEEG channels and anatomical cortical regions—when patients listened to a story or to instrumental music. More precisely, we statistically quantified the extent to which natural speech and music processing is performed by shared, preferred, or domain-selective neural populations. By combining sEEG investigations of HFa with the analyses of other frequency bands (from delta to low-gamma), the neural encoding of acoustic dynamics and spectrally resolved connectivity analyses, we obtained a thorough characterization of the neural dynamics at play during natural and continuous speech and music perception. Our results show that speech and music mostly rely on shared neural resources. Further, while selective responses seem absent at the level of atlas-based cortical regions, selectivity can be observed at the level of frequency-specific distributed networks in both power and connectivity analyses.</p><p>Previous work has reported that written or spoken language selectively activates a left-lateralized functional cortical network (<xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2011</xref>; <xref ref-type="bibr" rid="bib26">Fedorenko and Blank, 2020</xref>; <xref ref-type="bibr" rid="bib52">Malik-Moraleda et al., 2022</xref>). In particular, in previous functional MRI studies, these strong and selective cortical responses were not visible during the presentation of short musical excerpts, and are hypothesized to index linguistic processes (<xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib25">Fedorenko et al., 2011</xref>). Moreover, in the superior temporal gyrus, specific and separate neural populations for speech, music, and song are visible (<xref ref-type="bibr" rid="bib9">Boebinger et al., 2021</xref>; <xref ref-type="bibr" rid="bib67">Norman-Haignere et al., 2022</xref>). These selective responses, not visible in primary cortical regions, seem independent of both low-level acoustic features and higher-order linguistic meaning (<xref ref-type="bibr" rid="bib65">Norman-Haignere et al., 2015</xref>), and could subtend intermediate representations (<xref ref-type="bibr" rid="bib34">Giordano et al., 2023</xref>) such as domain-dependent predictions (<xref ref-type="bibr" rid="bib55">McCarty et al., 2023</xref>; <xref ref-type="bibr" rid="bib81">Sankaran et al., 2023</xref>). Within this framework, the localizationism view applies to highly specialized processes (i.e. functional niches), while general cognitive domains are mostly spatially distributed. Recent studies have shown that some communicative signals (e.g. alarm, emotional, linguistic) can exploit distinct acoustic niches to target specific neural networks and trigger reactions adapted to the intent of the emitter (<xref ref-type="bibr" rid="bib1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Arnal et al., 2019</xref>). Using neurally relevant spectro-temporal representations (MPS), these studies show that different subspaces encode distinct information types: slow temporal modulations for meaning (speech), fast temporal modulations for alarms (screams), and spectral modulations for melodies (<xref ref-type="bibr" rid="bib1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Arnal et al., 2015</xref>; <xref ref-type="bibr" rid="bib4">Arnal et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Flinker et al., 2019</xref>). Which acoustic features—and which neural mechanisms—are necessary and sufficient to route communicative sounds toward selective neural networks remains a promising field of investigation to explore.</p><p>In this context, in the current study we did not observe a single anatomical region for which speech selectivity was present, in any of our analyses. In other words, 10 min of instrumental music was enough to activate cortical regions classically labeled as speech (or language)-selective. On the contrary, we report spatially distributed and frequency-specific patterns of shared, preferred, or selective neural responses and connectivity fingerprints. This indicates that domain-selective brain regions should be considered as a set of functionally homogeneous but spatially distributed voxels, instead of anatomical landmarks. Several non-exclusive explanations may account for this finding. First, our results part with the simple selective versus shared dichotomy and adopt a more biologically valid and continuous framework (<xref ref-type="bibr" rid="bib10">Buzsáki, 2019</xref>; <xref ref-type="bibr" rid="bib94">Zatorre and Gandour, 2008</xref>) by adding a new category that is often neglected in the literature: <italic>preferred</italic> responses (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Indeed, responses in this category are usually reported as shared or selective and most often the statistical approach does not allow a more nuanced view (<xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>). However, preferred responses, namely responses that are stronger to a given class of stimuli but that are also present with other stimuli, are relevant and should not be collapsed into either the selective or shared categories. Introducing this intermediate category refines the epistemological and statistical approach on how to map cognitive and brain functions. It points toward the presence of gradients of neural activity across cognitive domains, instead of all-or-none responses. This framework is more compatible with the notion of distributed representations wherein specific regions are more or less recruited depending on their relative implication in a distributed manifold (<xref ref-type="bibr" rid="bib22">Elman, 1991</xref>; <xref ref-type="bibr" rid="bib76">Rissman and Wagner, 2012</xref>).</p><p>Second, most of the studies that reported regional selectivity are grounded on functional MRI data that lack a precise temporal resolution. Furthermore, the few studies assessing selectivity with intracranial EEG recordings analyzed only the HFa amplitude (<xref ref-type="bibr" rid="bib7">Bellier et al., 2022</xref>; <xref ref-type="bibr" rid="bib66">Norman-Haignere et al., 2020</xref>; <xref ref-type="bibr" rid="bib68">Oganian and Chang, 2019</xref>). However, while this latter reflects local (<xref ref-type="bibr" rid="bib49">Kopell et al., 2000</xref>) and possibly feedforward activity (<xref ref-type="bibr" rid="bib5">Bastos et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Fontolan et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Fries, 2015</xref>), other frequency bands are also constitutive of the cortical dynamics and involved in cognition. For instance, alpha/beta rhythms play a role in predicting upcoming stimuli and modulating sensory processing and associated spiking (<xref ref-type="bibr" rid="bib2">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="bib6">Bastos et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Morillon and Baillet, 2017</xref>; <xref ref-type="bibr" rid="bib80">Saleh et al., 2010</xref>; <xref ref-type="bibr" rid="bib90">van Kerkoerle et al., 2014</xref>). Also slower dynamics in the delta/theta range have been described to play a major role in cognitive processes and in particular for speech perception, contributing to speech tracking, segmentation, and decoding (<xref ref-type="bibr" rid="bib19">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Doelling et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib41">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Keitel et al., 2018</xref>). Importantly, we here addressed both activations and deactivations that can co-occur in the same spatial location across different frequency bands (<xref ref-type="bibr" rid="bib73">Pfurtscheller and Lopes da Silva, 1999</xref>; <xref ref-type="bibr" rid="bib74">Proix et al., 2022</xref>) and indeed observed that the domain selectivity observed within our restricted stimulus set is frequency-specific, meaning that domain selectivity is marginal when considering the entire spectrum of activity of a given sEEG channel. Finally, most studies only investigated local neural activity and did not consider the brain as a distributed system, analyzed through the lens of functional connectivity analyses. While topological approaches are more complex, they also provide more nuanced and robust characterization of brain functions. Critically, our approach reveals the limitation of adopting a reductionist approach—either by considering the brain as a set of independent regions instead of distributed networks, or by overlooking the spectral complexity of the neural signal.</p><p>Third, the ecological auditory stimuli we used are longer and more complex than stimuli used in previous studies and hence more prone to elicit distributed and dynamical neural responses (<xref ref-type="bibr" rid="bib42">Hasson et al., 2010</xref>; <xref ref-type="bibr" rid="bib85">Sonkusare et al., 2019</xref>; <xref ref-type="bibr" rid="bib89">Theunissen et al., 2000</xref>) and they require, in the case of music, for instance, more complex representations of melody and rhythm motifs contributing to stronger representations of meter, tonality, and groove (<xref ref-type="bibr" rid="bib9">Boebinger et al., 2021</xref>). While listening to natural speech and music rests on cognitively relevant neural processes, our analytical approach, extending over a rather long period of time, does not allow to directly isolate specific brain operations. Computational models—which can be as diverse as acoustic (<xref ref-type="bibr" rid="bib14">Chi et al., 2005</xref>), cognitive (<xref ref-type="bibr" rid="bib33">Giordano et al., 2021</xref>), information-theoretic (<xref ref-type="bibr" rid="bib17">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Donhauser and Baillet, 2020</xref>), or self-supervised neural networks (<xref ref-type="bibr" rid="bib21">Donhauser and Baillet, 2020</xref>; <xref ref-type="bibr" rid="bib60">Millet et al., 2022</xref>; <xref ref-type="bibr" rid="bib81">Sankaran et al., 2023</xref>) models—are hence necessary to further our understanding of the type of computations performed by our reported frequency-specific distributed networks. Moreover, incorporating models accounting for musical and linguistic structure can help us avoid misattributing differences between speech and music driven by unmatched sensitivity factors (e.g. arousal, emotion, or attention) as inherent speech or music selectivity (<xref ref-type="bibr" rid="bib54">Mas-Herrero et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Nantais and Schellenberg, 1999</xref>).</p><p>Our modeling approach, although lacking the modeling of melodic and linguistic features, was targeting the temporal dynamics of the speech and music stimuli. Beyond confirming that acoustic dynamics are strongly tracked by auditory neural dynamics, it revealed, investigating the entire cortex, that such neural tracking also occurs well outside of auditory regions—up to motor and inferior frontal areas (<xref ref-type="fig" rid="fig7">Figure 7B</xref>; see also <xref ref-type="bibr" rid="bib12">Chalas et al., 2022</xref>; <xref ref-type="bibr" rid="bib95">Zion Golumbic et al., 2013</xref>). Of note, this spatial map of speech dynamics encoding is very similar to former reports of the brain regions belonging to the language system (<xref ref-type="bibr" rid="bib16">Diachek et al., 2020</xref>). But, here again, adopting an approach that investigates both low and high frequencies of the neural signal—an approach that is not enough embraced in intracranial EEG studies (<xref ref-type="bibr" rid="bib74">Proix et al., 2022</xref>)—reveals that the LF activity clearly better encodes acoustic features than the HFa amplitude (<xref ref-type="fig" rid="fig7">Figure 7A</xref>).</p><p>In conclusion, our results point to a massive amount of shared neural response to speech and music, well beyond the auditory cortex. They also show the interest of considering shared, preferred, and selective responses when investigating domain selectivity. Importantly these three classes of responses should be considered in respect to (1) activation or deactivation patterns compared to a baseline, (2) different frequency bands, and (3) power spectrum (activity) and connectivity approaches. Combining all these points of view gives a richer although possibly more complex view of brain functions. While our data point to an absence of anatomical regional selectivity for speech and music, such a selectivity still exists at the level of a spatially distributed and frequency-specific network. Thus, the inconsistency with previous findings may be limited to the idea that some anatomical regions are selective to speech or music processing. However, the two points of view can be reconciled when considering a fine-grained network approach allowing selectivity to coexist for speech and music within the same anatomical region. Finally, in adopting here a comparative approach of speech and music—the two main auditory domains of human cognition—we only investigated one type of speech and of music during a passive listening task. Future work is needed to investigate for instance whether different sentences or melodies activate the same selective frequency-specific distributed networks and to what extent these results are related to the passive listening context compared to a more active and natural context (e.g. conversation).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>18 patients (10 females, mean age 30 years, range 8–54 years) with pharmacoresistant epilepsy participated in the study. All patients were French native speakers. Neuropsychological assessments carried out before sEEG recordings indicated that all patients had intact language functions and met the criteria for normal hearing. In none of them were the auditory areas part of their epileptogenic zone as identified by experienced epileptologists. Recordings took place at the Hôpital de La Timone (Marseille, France). Patients provided informed consent prior to the experimental session, and the experimental protocol was approved by the Institutional Review Board of the French Institute of Health (IRB00003888).</p></sec><sec id="s4-2"><title>Data acquisition</title><p>The sEEG signal was recorded using depth electrodes shafts of 0.8 mm diameter containing 10–15 electrode contacts (Dixi Medical or Alcis, Besançon, France). The contacts were 2 mm long and were spaced from each other by 1.5 mm. The locations of the electrode implantations were determined solely on clinical grounds. Patients were included in the study if their implantation map covered at least partially the Heschl’s gyrus (left or right). The cohort consists of 13 unilateral implantations (10 left, 3 right) and 5 bilateral implantations, yielding a total of 271 electrodes and 3371 contacts (see <xref ref-type="fig" rid="fig1">Figure 1C and D</xref> for electrodes localization).</p><p>Patients were recorded either in an insulated Faraday cage or in the bedroom. In the Faraday cage, they laid comfortably in a chair, the room was sound attenuated, and data were recorded using a 256-channel amplifier (Brain Products), sampled at 1 kHz and high-pass filtered at 0.016 Hz. In the bedroom, data were recorded using a 256-channel Natus amplifier (DeltaMed system), sampled at 512 Hz, and high-pass filtered at 0.16 Hz.</p></sec><sec id="s4-3"><title>Experimental design</title><p>Patients completed three separate sessions. In one session they completed the main experimental paradigm and the two additional sessions served as baseline for the spectral analysis (see below).</p><p>In the main experimental session, patients passively listened to ~10 min of storytelling (<xref ref-type="bibr" rid="bib38">Gripari, 2004</xref>); 577 s, <italic>La sorcière de la rue Mouffetard</italic> (<xref ref-type="bibr" rid="bib38">Gripari, 2004</xref>) and ~10 min of instrumental music (580 s, <italic>Reflejos del Sur,</italic> <xref ref-type="bibr" rid="bib69">Oneness, 2006</xref>) separated by 3 min of rest. The order of conditions was counterbalanced across patients (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). This session was conducted in the Faraday cage (N=6) or in the bedroom (N=12).</p><p>In the two baseline sessions, patients passively listened to two more basic types of auditory stimuli: (1) 30-ms-long pure tones, presented binaurally at 500 Hz or 1 kHz (with a linear rise and fall time of 0.3 ms) 110 times each, with an ISI of 1030 (±200) ms; and (2) /ba/ or /pa/ syllables, pronounced by a French female speaker and presented binaurally 250 times each, with an ISI of 1030 (±200) ms. These stimuli were designed for a clinical purpose in order to functionally map the auditory cortex. These two recording sessions (lasting ~2 and 4 min) were performed in the Faraday cage.</p><p>In the Faraday cage, a sound Blaster X-Fi Xtreme Audio, an amplifier Yamaha P2040 and Yamaha loudspeakers (NS 10M) were used for sound presentation. In the bedroom, stimuli were presented using a Sennheiser HD 25 headphone set. Sound stimuli were presented at 44.1 kHz sample rate and 16 bits resolution. Speech and music excerpts were presented at ~75 dBA (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p></sec><sec id="s4-4"><title>General preprocessing related to electrodes localization</title><p>To increase spatial sensitivity and reduce passive volume conduction from neighboring regions (<xref ref-type="bibr" rid="bib58">Mercier et al., 2017</xref>; <xref ref-type="bibr" rid="bib59">Mercier et al., 2022</xref>), the signal was offline re-referenced using bipolar montage. That is, for a pair of adjacent electrode contacts, the referencing led to a virtual channel located at the midpoint locations of the original contacts. To precisely localize the channels, a procedure similar to the one used in the iELVis toolbox and in the FieldTrip toolbox was applied (<xref ref-type="bibr" rid="bib40">Groppe et al., 2017</xref>; <xref ref-type="bibr" rid="bib87">Stolk et al., 2018</xref>). First, we manually identified the location of each channel centroid on the post-implant CT scan using the Gardel software (<xref ref-type="bibr" rid="bib56">Medina Villalon et al., 2018</xref>). Second, we performed volumetric segmentation and cortical reconstruction on the pre-implant MRI with the Freesurfer image analysis suite (documented and freely available for download online at <ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>). This segmentation of the pre-implant MRI with SPM12 provides us with both the tissue probability maps (i.e. gray, white, and cerebrospinal fluid [CSF] probabilities) and the indexed-binary representations (i.e. either gray, white, CSF, bone, or soft tissues). This information allowed us to reject electrodes not located in the brain. Third, the post-implant CT scan was coregistered to the pre-implant MRI via a rigid affine transformation and the pre-implant MRI was registered to MNI152 space, via a linear and a non-linear transformation from SPM12 methods (<xref ref-type="bibr" rid="bib72">Penny et al., 2011</xref>), through the FieldTrip toolbox (<xref ref-type="bibr" rid="bib70">Oostenveld et al., 2011</xref>). Fourth, applying the corresponding transformations, we mapped channel locations to the pre-implant MRI brain that was labeled using the volume-based Human Brainnetome Atlas (<xref ref-type="bibr" rid="bib24">Fan et al., 2016</xref>).</p><p>Based on the brain segmentation performed using SPM12 methods through the FieldTrip toolbox, bipolar channels located outside of the brain were removed from the data (3%). The remaining data (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) was then bandpass filtered between 0.1 and 250 Hz, and, following a visual inspection of the power spectral density profile of the data, when necessary, we additionally applied a notch filter at 50 Hz and harmonics up to 200 Hz to remove power line artifacts (N=12). Finally, the data were downsampled to 500 Hz.</p></sec><sec id="s4-5"><title>Artifact rejection</title><p>To define artifacted channel we used both the broadband signal and the amplitude of the HFa. This latter was obtained by computing, with the Hilbert transform, the analytic amplitude of four 10-Hz-wide sub-bands spanning from 80 to 120 Hz. Each sub-band was standardized by dividing it by its mean and, finally, all sub-bands were averaged together (<xref ref-type="bibr" rid="bib71">Ossandón et al., 2012</xref>; <xref ref-type="bibr" rid="bib91">Vidal et al., 2012</xref>). Channels with a variance greater than 2*IQR (interquartile range, i.e. a non-parametric estimate of the standard deviation)—on either the broadband or high-frequency signals—were tagged as artifacted channels (on average 18% of the channels). Then the data were epoched in non-overlapping segments of 5 s (2500 samples). To exclude artifacted epochs, epochs, wherein the maximum amplitude (over time) summed across non-excluded channels was greater than 2*IQR, were tagged as artifacted epochs. Overall, 6% of the speech epochs and 7% of the music epochs were rejected. Channels and epochs defined as artifacted were excluded from subsequent analyses, except if specified otherwise (see TRF analysis section).</p></sec><sec id="s4-6"><title>Spectral analysis</title><p>Six canonical frequency bands were investigated: delta (1–4 Hz), theta (5–8 Hz), alpha (8–12 Hz), beta (18–30 Hz), low-gamma (30–50 Hz), and HFa (80–120 Hz). To prevent edge artifacts, prior to extracting the power spectrum, epochs were zero-padded on both sides with 3.5 s segments which were later removed. For each patient, channel, epoch, and frequency band, the power of the neural signal was calculated using the Welch approach on discrete Fourier transform from the SciPy-Python library (<xref ref-type="bibr" rid="bib92">Virtanen et al., 2020</xref>) and then averaged across the relevant frequencies to obtain these six canonical bands.</p><p>For each canonical band and each channel, we classified the time-averaged neural response as being selective, preferred, or shared across the two investigated cognitive domains (speech, music). We defined these categories by capitalizing on both the simple effects of—and contrast between—the neural responses to speech and music stimuli compared to a baseline condition (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). ‘Selective’ responses are neural responses that are significantly different compared to the baseline for one domain (speech or music) but not the other, and with a significant difference between domains (i.e. speech or music is different from baseline+difference effect between the domains). ‘Preferred’ responses correspond to neural responses that occur during both speech and music processing, but with a significantly stronger response for one domain over the other (i.e. both speech and music are significantly different from baseline+difference effect between the domains). Finally, ‘shared’ responses occur when there are no significant differences between domains, and there is a significant neural response to at least one of the two stimuli (one or two simple effects+no difference). If none of the two domains produces a significant neural response, the difference is not assessed (case ‘neither’ simple effect). In order to explore the full range of possible selective, preferred, or shared responses, we considered both responses greater and smaller than the baseline. Indeed, as neural populations can synchronize or desynchronize in response to sensory stimulation, we estimated these categories separately for significant activations and significant deactivations compared to baseline.</p><p>For each frequency band and channel, the statistical difference between conditions was estimated with paired sample permutation tests based on the t-statistic from the MNE-Python library (<xref ref-type="bibr" rid="bib36">Gramfort et al., 2014</xref>) with 1000 permutations and the <italic>tmax</italic> method to control the family-wise error rate (<xref ref-type="bibr" rid="bib39">Groppe et al., 2011</xref>; <xref ref-type="bibr" rid="bib63">Nichols and Holmes, 2002</xref>). In <italic>tmax</italic> permutation testing, the null distribution is estimated by, for each channel (i.e. each comparison), swapping the condition labels (speech vs music or speech/music vs baseline) between epochs. After each permutation, the most extreme t-scores over channels (<italic>tmax</italic>) are selected for the null distribution. Finally, the t-scores of the observed data are computed and compared to the simulated <italic>tmax</italic> distribution, similar as in parametric hypothesis testing. Because with an increased number of comparisons, the chance of obtaining a large <italic>tmax</italic> (i.e. false discovery) also increases, the test automatically becomes more conservative when making more comparisons, as such correcting for the multiple comparison between channels.</p></sec><sec id="s4-7"><title>TRF analysis</title><p>We used the TRF to estimate the encoding of acoustic features by neural activity. Two acoustic features were extracted from our stimuli (speech, music): the envelope and the peakRate. To estimate the temporal envelope of the two stimuli, the acoustic waveforms were decomposed into 32 narrow frequency bands using a cochlear model, and the absolute value of the Hilbert transform was computed for each of these narrowband signals. The broadband temporal envelope (henceforth ‘envelope’) resulted from the summation of these absolute values. The acoustic onset edges (henceforth ‘peakRate’) were defined as peaks in the rate of change (first derivative) of the envelope (<xref ref-type="bibr" rid="bib20">Doelling et al., 2014</xref>; <xref ref-type="bibr" rid="bib68">Oganian and Chang, 2019</xref>). Finally, both the envelopes and peakRates were downsampled to 100 Hz and z-scored to be mapped to the neural data.</p><p>All computations of the TRF used the pymTRF library (<xref ref-type="bibr" rid="bib86">Steinkamp, 2019</xref>), a Python adaption of the mTRF toolbox (<xref ref-type="bibr" rid="bib15">Crosse et al., 2016</xref>). A TRF is a model that, via linear convolution, serves as a filter to quantify the relationship between two continuous signals, here stimulus features and neural activity. Hence, for this analysis, the entire duration of the recordings were preserved, i.e., no artifacted epochs were excluded. When applied in a forward manner, the TRF approach describes the mapping of stimulus features onto the neural response (henceforth ‘encoding’; <xref ref-type="bibr" rid="bib15">Crosse et al., 2016</xref>). Using ridge regression to avoid overfitting, we examined how well the two different acoustic features—envelope and peakRate—map onto LF activity (1–9 Hz) or the amplitude of the HFa (80–120 Hz, see Artifact rejection section) (<xref ref-type="bibr" rid="bib18">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib95">Zion Golumbic et al., 2013</xref>). Hence, four encoding models were estimated: envelope/peakRate acoustic features * LF/amplitude of HFa neural activity. For each model and patient, the optimal ridge regularization parameter (λ) was estimated using cross-validation on the sEEG channels situated in the auditory cortex. We considered time lags from –150 to 1000 ms for the TRF estimations. 80% of the data was used to derive the TRFs and the remaining 20% was used as a validation set. The quality of the predicted neural response was assessed by computing Pearson’s product moment correlations (Fisher z-scored) between the predicted and actual neural data for each channel and model using the SciPy-Python library (p-values FDR-corrected).</p><p>Models were finally compared in terms of the percentage of channels that significantly encoded the acoustic structure of speech and/or music. This percentage was estimated at the single-subject level and combined with non-parametric Wilcoxon signed-rank tests at the group level to define the winning model. In other words, the winning model is the model for which the percentage of channels significantly encoding speech and/or music acoustic features is the largest. Multiple comparison across pairs of models was controlled for with a FDR correction.</p></sec><sec id="s4-8"><title>Connectivity analysis</title><p>We examined the frequency-specific functional connectivity maps in response to speech and music, between the entire brain and the auditory cortex using a seed-based approach (we dismissed the channels immediately neighboring the seed channel). As seed, we selected, per patient, the channel that best encoded the speech and music acoustic features (see TRF analysis; <xref ref-type="fig" rid="fig7">Figure 7D</xref>). We used spectral coherence as a connectivity measure for all canonical bands (see above) and all analyses were performed using the MNE-Python library (<xref ref-type="bibr" rid="bib36">Gramfort et al., 2014</xref>). Our rationale to use coherence as functional connectivity metric was threefold. First, coherence analysis considers both magnitude and phase information. While the absence of dissociation can be criticized, signals with higher amplitude and/or SNR lead to better time-frequency estimates (which is not the case with a metric that would focus on phase only and therefore would be more likely to include estimates of various SNR). Second, we choose a metric that allows direct comparison between frequencies. As, at high frequencies phase angle changes more quickly, phase alignment/synchronization is less likely in comparison with lower frequencies. Third, we intend to align to previous work which, for the most part, used the measure of coherence most likely for the reasons explained above.</p><p>For each frequency band, we classified each channel into selective, preferred, or shared categories (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>) by examining both the simple effects (i.e. which channels display a significantly coherent signal with the seed during speech and/or music processing) and the difference effects (i.e. is coherence significantly stronger for one domain over the other).</p><p>Statistical significance was assessed for each frequency band and channel using surrogate data with 1000 iterations, which were generated by modifying the temporal structure of the sEEG signal recorded at the seeds (i.e. shuffling the epochs) prior to computing connectivity. This process led to a total of 1000 connectivity values, which were used as null distribution to calculate the probability threshold associated with genuine connectivity.</p></sec><sec id="s4-9"><title>Population prevalence</title><p>For both the spectral and the connectivity analyses, in order to make sure that the results are not driven by the heterogeneity of electrode locations across patients, we examined, for each region, the proportions of patients showing only shared or selective responses. That is, for both the spectral and connectivity results, we examined results representativeness as follows: for each anatomical region wherein at least two patients have at least two significantly responsive channels, we computed the percentage of patients that showed a pattern of selective (i.e. all channels selective to speech or music) or a shared (i.e. a mixture of channels responding to speech and/or music) responses. This approach is inspired by the population prevalence, where an equivalent metric is introduced (i.e. the maximum a posterior estimate; see <xref ref-type="bibr" rid="bib44">Ince et al., 2021</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Formal analysis, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Funding acquisition, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Patients provided informed consent prior to the experimental session, and the experimental protocol was approved by the Institutional Review board of the French Institute of Health (IRB00003888).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Patients description.</title><p>Table provides an overview of patient characteristics and experimental conditions. The table includes 18 patients, with a mix of males and females aged between 8 and 54 years (mean 30). Presentation order (either speech-music or music-speech) was counterbalanced between patients. Recordings took place either at the bedside (room) or in the lab. Hemispheric dominance was mostly typical. All patients had an electrode implanted in the auditory cortex (Heschl’s gyrus; left, right, or bilaterally). The number of depth electrodes is indicated in the far right column.</p></caption><media xlink:href="elife-94509-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-94509-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The raw data investigated in the current manuscript is privileged patient data. Because of this, the conditions of our ethics approval do not permit public archiving of anonymised study data. Readers seeking access to the data should contact Dr. Daniele Schön (daniele.schon@univ-amu.fr). Access will be granted to named individuals in accordance with ethical procedures governing the reuse of clinical data, including completion of a formal data sharing agreement. Data analyses were performed using custom scripts in Python, which are available on GitHub along with preprocessed data necessary to reproduce the figures and results: <ext-link ext-link-type="uri" xlink:href="https://github.com/noemietr/iSpeech">https://github.com/noemietr/iSpeech</ext-link> (copy archived <xref ref-type="bibr" rid="bib88">te Rietmolen, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank all patients for their willingful participation. We thank Patrick Marquis for helping with the data acquisition, and Anne-Catherine Tomei and all colleagues from the Institut de Neuroscience des Systèmes for useful discussions. ANR-20-CE28-0007-01 (to BM), ANR-21-CE28-0010 (to DS), ANR-17-EURE-0029 (NeuroMarseille), and co-funded by the European Union (ERC, SPEEDY, ERC-CoG-101043344). This work, carried out within the Institute of Convergence ILCB, was also supported by grants from France 2030 (ANR-16-CONV-0002), the French government under the Programme 'Investissements d’Avenir', and the Excellence Initiative of Aix-Marseille University (A*MIDEX, AMX-19-IET-004).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albouy</surname><given-names>P</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distinct sensitivity to spectrotemporal modulation supports brain asymmetry for speech and melody</article-title><source>Science</source><volume>367</volume><fpage>1043</fpage><lpage>1047</lpage><pub-id pub-id-type="doi">10.1126/science.aaz3468</pub-id><pub-id pub-id-type="pmid">32108113</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.05.003</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human screams occupy a privileged niche in the communication soundscape</article-title><source>Current Biology</source><volume>25</volume><fpage>2051</fpage><lpage>2056</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.06.043</pub-id><pub-id pub-id-type="pmid">26190070</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>Spinelli</surname><given-names>L</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Mégevand</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The rough sound of salience enhances aversion through neural synchronisation</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3671</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11626-7</pub-id><pub-id pub-id-type="pmid">31413319</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>Bosman</surname><given-names>CA</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Dowdall</surname><given-names>JR</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual areas exert feedforward and feedback influences through distinct frequency channels</article-title><source>Neuron</source><volume>85</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.018</pub-id><pub-id pub-id-type="pmid">25556836</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Waite</surname><given-names>AS</given-names></name><name><surname>Kopell</surname><given-names>N</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Layer and rhythm specificity for predictive routing</article-title><source>PNAS</source><volume>117</volume><fpage>31459</fpage><lpage>31469</lpage><pub-id pub-id-type="doi">10.1073/pnas.2014868117</pub-id><pub-id pub-id-type="pmid">33229572</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bellier</surname><given-names>L</given-names></name><name><surname>Llorens</surname><given-names>A</given-names></name><name><surname>Marciano</surname><given-names>D</given-names></name><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Brunner</surname><given-names>P</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Pasley</surname><given-names>BN</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Encoding and Decoding Analysis of Music Perception Using Intracranial EEG</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.01.27.478085</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The what, where and how of auditory-object perception</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>693</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1038/nrn3565</pub-id><pub-id pub-id-type="pmid">24052177</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boebinger</surname><given-names>D</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Music-selective neural populations arise without musical training</article-title><source>Journal of Neurophysiology</source><volume>125</volume><fpage>2237</fpage><lpage>2263</lpage><pub-id pub-id-type="doi">10.1152/jn.00588.2020</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>The Brain from Inside Out</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oso/9780190905385.001.0001</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Vöröslakos</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Brain rhythms have come of age</article-title><source>Neuron</source><volume>111</volume><fpage>922</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.018</pub-id><pub-id pub-id-type="pmid">37023714</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalas</surname><given-names>N</given-names></name><name><surname>Daube</surname><given-names>C</given-names></name><name><surname>Kluger</surname><given-names>DS</given-names></name><name><surname>Abbasi</surname><given-names>O</given-names></name><name><surname>Nitsch</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multivariate analysis of speech envelope tracking reveals coupling beyond auditory cortex</article-title><source>NeuroImage</source><volume>258</volume><elocation-id>119395</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119395</pub-id><pub-id pub-id-type="pmid">35718023</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Affourtit</surname><given-names>J</given-names></name><name><surname>Ryskin</surname><given-names>R</given-names></name><name><surname>Regev</surname><given-names>TI</given-names></name><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Jouravlev</surname><given-names>O</given-names></name><name><surname>Malik-Moraleda</surname><given-names>S</given-names></name><name><surname>Kean</surname><given-names>H</given-names></name><name><surname>Varley</surname><given-names>R</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The human language system, including its inferior frontal component in “Broca’s area,” does not support music perception</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>7904</fpage><lpage>7929</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhad087</pub-id><pub-id pub-id-type="pmid">37005063</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>604</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diachek</surname><given-names>E</given-names></name><name><surname>Blank</surname><given-names>I</given-names></name><name><surname>Siegelman</surname><given-names>M</given-names></name><name><surname>Affourtit</surname><given-names>J</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The domain-general multiple demand (MD) network does not support core aspects of language comprehension: A large-scale fMRI investigation</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>4536</fpage><lpage>4550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2036-19.2020</pub-id><pub-id pub-id-type="pmid">32317387</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>eLife</source><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id><pub-id pub-id-type="pmid">32122465</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Characterizing neural entrainment to hierarchical linguistic units using electroencephalography (EEG)</article-title><source>Frontiers in Human Neuroscience</source><volume>11</volume><elocation-id>481</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2017.00481</pub-id><pub-id pub-id-type="pmid">29033809</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>NeuroImage</source><volume>85 Pt 2</volume><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id><pub-id pub-id-type="pmid">23791839</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two distinct neural timescales for predictive speech processing</article-title><source>Neuron</source><volume>105</volume><fpage>385</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id><pub-id pub-id-type="pmid">31806493</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed representations, simple recurrent networks, and grammatical structure</article-title><source>Machine Learning</source><volume>7</volume><fpage>195</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1007/BF00114844</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fadiga</surname><given-names>L</given-names></name><name><surname>Craighero</surname><given-names>L</given-names></name><name><surname>D’Ausilio</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Broca’s area in language, action, and music</article-title><source>Annals of the New York Academy of Sciences</source><volume>1169</volume><fpage>448</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04582.x</pub-id><pub-id pub-id-type="pmid">19673823</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Zhuo</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Chu</surname><given-names>C</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Jiang</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human brainnetome atlas: A new brain atlas based on connectional architecture</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3508</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw157</pub-id><pub-id pub-id-type="pmid">27230218</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Behr</surname><given-names>MK</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specificity for high-level linguistic processing in the human brain</article-title><source>PNAS</source><volume>108</volume><fpage>16428</fpage><lpage>16433</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id><pub-id pub-id-type="pmid">21885736</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Blank</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Broca’s area is not a natural kind</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>270</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.01.001</pub-id><pub-id pub-id-type="pmid">32160565</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flaugnacco</surname><given-names>E</given-names></name><name><surname>Lopez</surname><given-names>L</given-names></name><name><surname>Terribili</surname><given-names>C</given-names></name><name><surname>Montico</surname><given-names>M</given-names></name><name><surname>Zoia</surname><given-names>S</given-names></name><name><surname>Schön</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Music training increases phonological awareness and reading skills in developmental dyslexia: A randomized control trial</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0138715</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0138715</pub-id><pub-id pub-id-type="pmid">26407242</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Doyle</surname><given-names>WK</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spectrotemporal modulation provides a unifying framework for auditory cortical asymmetries</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>393</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0548-z</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Liegeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The contribution of frequency-specific activity to hierarchical information processing in the human auditory cortex</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>4694</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms5694</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>François</surname><given-names>C</given-names></name><name><surname>Chobert</surname><given-names>J</given-names></name><name><surname>Besson</surname><given-names>M</given-names></name><name><surname>Schön</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Music training for the development of speech segmentation</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2038</fpage><lpage>2043</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs180</pub-id><pub-id pub-id-type="pmid">22784606</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hierarchy processing in human neurobiology: how specific is it?</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>375</volume><elocation-id>20180391</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2018.0391</pub-id><pub-id pub-id-type="pmid">31735144</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rhythms for cognition: Communication through coherence</article-title><source>Neuron</source><volume>88</volume><fpage>220</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.034</pub-id><pub-id pub-id-type="pmid">26447583</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Whiting</surname><given-names>C</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The representational dynamics of perceived voice emotions evolve from categories to dimensions</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>1203</fpage><lpage>1213</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01073-0</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Esposito</surname><given-names>M</given-names></name><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Intermediate acoustic-to-semantic representations link behavioral and neural responses to natural sounds</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>664</fpage><lpage>672</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id><pub-id pub-id-type="pmid">36928634</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Speech perception from a neurophysiological perspective</chapter-title><person-group person-group-type="editor"><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><source>The Human Auditory Cortex</source><publisher-name>Springer</publisher-name><fpage>225</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-2314-0_9</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><pub-id pub-id-type="pmid">24161808</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Nourski</surname><given-names>KV</given-names></name><name><surname>Kawasaki</surname><given-names>H</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Patterson</surname><given-names>RD</given-names></name><name><surname>Brugge</surname><given-names>JF</given-names></name><name><surname>Howard</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Direct recordings of pitch responses from human auditory cortex</article-title><source>Current Biology</source><volume>20</volume><fpage>1128</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2010.04.044</pub-id><pub-id pub-id-type="pmid">20605456</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gripari</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>La Sorcière de La Rue Mouffetard</source><publisher-name>Gallimard Jeunesse</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname><given-names>DM</given-names></name><name><surname>Urbach</surname><given-names>TP</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mass univariate analysis of event-related brain potentials/fields I: a critical tutorial review</article-title><source>Psychophysiology</source><volume>48</volume><fpage>1711</fpage><lpage>1725</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2011.01273.x</pub-id><pub-id pub-id-type="pmid">21895683</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname><given-names>DM</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Dykstra</surname><given-names>AR</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Mégevand</surname><given-names>P</given-names></name><name><surname>Mercier</surname><given-names>MR</given-names></name><name><surname>Lado</surname><given-names>FA</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data</article-title><source>Journal of Neuroscience Methods</source><volume>281</volume><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.01.022</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hoogenboom</surname><given-names>N</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001752</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id><pub-id pub-id-type="pmid">24391472</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reliability of cortical activity during natural stimulation</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.10.011</pub-id><pub-id pub-id-type="pmid">20004608</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inbar</surname><given-names>M</given-names></name><name><surname>Grossman</surname><given-names>E</given-names></name><name><surname>Landau</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sequences of intonation units form a ~ 1Hz rhythm</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>15846</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-72739-4</pub-id><pub-id pub-id-type="pmid">32985572</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Paton</surname><given-names>AT</given-names></name><name><surname>Kay</surname><given-names>JW</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bayesian inference of population prevalence</article-title><source>eLife</source><volume>10</volume><elocation-id>e62461</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.62461</pub-id><pub-id pub-id-type="pmid">34612811</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Analysis of slow (theta) oscillations as a potential temporal reference frame for information coding in sensory cortices</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002717</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002717</pub-id><pub-id pub-id-type="pmid">23071429</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id><pub-id pub-id-type="pmid">29529019</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kivy</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Charles Darwin on Music</article-title><source>Journal of the American Musicological Society</source><volume>12</volume><fpage>42</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.2307/829516</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Toward A neural basis of music perception - A review and updated model</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>110</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00110</pub-id><pub-id pub-id-type="pmid">21713060</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopell</surname><given-names>N</given-names></name><name><surname>Ermentrout</surname><given-names>GB</given-names></name><name><surname>Whittington</surname><given-names>MA</given-names></name><name><surname>Traub</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Gamma rhythms and beta rhythms have different synchronization properties</article-title><source>PNAS</source><volume>97</volume><fpage>1867</fpage><lpage>1872</lpage><pub-id pub-id-type="doi">10.1073/pnas.97.4.1867</pub-id><pub-id pub-id-type="pmid">10677548</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraus</surname><given-names>N</given-names></name><name><surname>Chandrasekaran</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Music training for the development of auditory skills</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>599</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1038/nrn2882</pub-id><pub-id pub-id-type="pmid">20648064</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Van Quyen</surname><given-names>M</given-names></name><name><surname>Staba</surname><given-names>R</given-names></name><name><surname>Bragin</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>C</given-names></name><name><surname>Valderrama</surname><given-names>M</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name><name><surname>Engel</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Large-scale microelectrode recordings of high-frequency gamma oscillations in human cortex during sleep</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>7770</fpage><lpage>7782</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5049-09.2010</pub-id><pub-id pub-id-type="pmid">20534826</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malik-Moraleda</surname><given-names>S</given-names></name><name><surname>Ayyash</surname><given-names>D</given-names></name><name><surname>Gallée</surname><given-names>J</given-names></name><name><surname>Affourtit</surname><given-names>J</given-names></name><name><surname>Hoffmann</surname><given-names>M</given-names></name><name><surname>Mineroff</surname><given-names>Z</given-names></name><name><surname>Jouravlev</surname><given-names>O</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>An investigation across 45 languages and 12 language families reveals a universal language network</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>1014</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01114-5</pub-id><pub-id pub-id-type="pmid">35856094</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>S</given-names></name><name><surname>Millán</surname><given-names>JDR</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Pasley</surname><given-names>BN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The use of intracranial recordings to decode human language: Challenges and opportunities</article-title><source>Brain and Language</source><volume>193</volume><fpage>73</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2016.06.003</pub-id><pub-id pub-id-type="pmid">27377299</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mas-Herrero</surname><given-names>E</given-names></name><name><surname>Marco-Pallares</surname><given-names>J</given-names></name><name><surname>Lorenzo-Seva</surname><given-names>U</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Rodriguez-Fornells</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Individual differences in music reward experiences</article-title><source>Music Perception</source><volume>31</volume><fpage>118</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1525/mp.2013.31.2.118</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCarty</surname><given-names>MJ</given-names></name><name><surname>Murphy</surname><given-names>E</given-names></name><name><surname>Scherschligt</surname><given-names>X</given-names></name><name><surname>Woolnough</surname><given-names>O</given-names></name><name><surname>Morse</surname><given-names>CW</given-names></name><name><surname>Snyder</surname><given-names>K</given-names></name><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Tandon</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Intraoperative cortical localization of music and language reveals signatures of structural complexity in posterior temporal cortex</article-title><source>iScience</source><volume>26</volume><elocation-id>107223</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2023.107223</pub-id><pub-id pub-id-type="pmid">37485361</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medina Villalon</surname><given-names>S</given-names></name><name><surname>Paz</surname><given-names>R</given-names></name><name><surname>Roehri</surname><given-names>N</given-names></name><name><surname>Lagarde</surname><given-names>S</given-names></name><name><surname>Pizzo</surname><given-names>F</given-names></name><name><surname>Colombet</surname><given-names>B</given-names></name><name><surname>Bartolomei</surname><given-names>F</given-names></name><name><surname>Carron</surname><given-names>R</given-names></name><name><surname>Bénar</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>EpiTools, A software suite for presurgical brain mapping in epilepsy: Intracerebral EEG</article-title><source>Journal of Neuroscience Methods</source><volume>303</volume><fpage>7</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2018.03.018</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menon</surname><given-names>V</given-names></name><name><surname>Levitin</surname><given-names>DJ</given-names></name><name><surname>Smith</surname><given-names>BK</given-names></name><name><surname>Lembke</surname><given-names>A</given-names></name><name><surname>Krasnow</surname><given-names>BD</given-names></name><name><surname>Glazer</surname><given-names>D</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>McAdams</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural correlates of timbre change in harmonic sounds</article-title><source>NeuroImage</source><volume>17</volume><fpage>1742</fpage><lpage>1754</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1295</pub-id><pub-id pub-id-type="pmid">12498748</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname><given-names>MR</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Megevand</surname><given-names>P</given-names></name><name><surname>Groppe</surname><given-names>DM</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Lado</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evaluation of cortical local field potential diffusion in stereotactic electro-encephalography recordings: A glimpse on white matter signal</article-title><source>NeuroImage</source><volume>147</volume><fpage>219</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.037</pub-id><pub-id pub-id-type="pmid">27554533</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname><given-names>MR</given-names></name><name><surname>Dubarry</surname><given-names>AS</given-names></name><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Avanzini</surname><given-names>P</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Cellier</surname><given-names>D</given-names></name><name><surname>Vecchio</surname><given-names>MD</given-names></name><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Hermes</surname><given-names>D</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Llorens</surname><given-names>A</given-names></name><name><surname>Megevand</surname><given-names>P</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Piai</surname><given-names>V</given-names></name><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name><name><surname>Smith</surname><given-names>SE</given-names></name><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Swann</surname><given-names>NC</given-names></name><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Lachaux</surname><given-names>JP</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Advances in human intracranial electroencephalography research, guidelines and good practices</article-title><source>NeuroImage</source><volume>260</volume><elocation-id>119438</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119438</pub-id><pub-id pub-id-type="pmid">35792291</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Millet</surname><given-names>J</given-names></name><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Orhan</surname><given-names>P</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Dunbar</surname><given-names>E</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Toward a Realistic Model of Speech Processing in the Brain with Self-Supervised Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2206.01685">http://arxiv.org/abs/2206.01685</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Motor origin of temporal predictions in auditory attention</article-title><source>PNAS</source><volume>114</volume><fpage>E8913</fpage><lpage>E8921</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705373114</pub-id><pub-id pub-id-type="pmid">28973923</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nantais</surname><given-names>KM</given-names></name><name><surname>Schellenberg</surname><given-names>EG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The Mozart Effect: An artifact of preference</article-title><source>Psychological Science</source><volume>10</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00170</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieto-Castañón</surname><given-names>A</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Subject-specific functional localizers increase sensitivity and functional resolution of multi-subject analyses</article-title><source>NeuroImage</source><volume>63</volume><fpage>1646</fpage><lpage>1669</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.06.065</pub-id><pub-id pub-id-type="pmid">22784644</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical pathways for music and speech revealed by hypothesis-free voxel decomposition</article-title><source>Neuron</source><volume>88</volume><fpage>1281</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id><pub-id pub-id-type="pmid">26687225</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>Feather</surname><given-names>J</given-names></name><name><surname>Boebinger</surname><given-names>D</given-names></name><name><surname>Brunner</surname><given-names>P</given-names></name><name><surname>Ritaccio</surname><given-names>A</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Intracranial Recordings from Human Auditory Cortex Reveal a Neural Population Selective for Song</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/696161</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>Feather</surname><given-names>J</given-names></name><name><surname>Boebinger</surname><given-names>D</given-names></name><name><surname>Brunner</surname><given-names>P</given-names></name><name><surname>Ritaccio</surname><given-names>A</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A neural population selective for song in human auditory cortex</article-title><source>Current Biology</source><volume>32</volume><fpage>1470</fpage><lpage>1484</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.01.069</pub-id><pub-id pub-id-type="pmid">35196507</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oganian</surname><given-names>Y</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A speech envelope landmark for syllable encoding in human superior temporal gyrus</article-title><source>Science Advances</source><volume>5</volume><elocation-id>eaay6279</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aay6279</pub-id><pub-id pub-id-type="pmid">31976369</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Oneness</collab></person-group><year iso-8601-date="2006">2006</year><article-title>Reflejos del Sur</article-title><ext-link ext-link-type="uri" xlink:href="https://reflejosdelsur.cl/">https://reflejosdelsur.cl/</ext-link><date-in-citation iso-8601-date="2024-07-19">July 19, 2024</date-in-citation></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ossandón</surname><given-names>T</given-names></name><name><surname>Vidal</surname><given-names>JR</given-names></name><name><surname>Ciumas</surname><given-names>C</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Hamamé</surname><given-names>CM</given-names></name><name><surname>Dalal</surname><given-names>SS</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Minotti</surname><given-names>L</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Lachaux</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient “pop-out” visual search elicits sustained broadband γ activity in the dorsal attention network</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3414</fpage><lpage>3421</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6048-11.2012</pub-id><pub-id pub-id-type="pmid">22399764</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>WD</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Ashburner</surname><given-names>JT</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Statistical Parametric Mapping: The Analysis of Functional Brain Images</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G</given-names></name><name><surname>Lopes da Silva</surname><given-names>FH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Event-related EEG/MEG synchronization and desynchronization: basic principles</article-title><source>Clinical Neurophysiology</source><volume>110</volume><fpage>1842</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1016/s1388-2457(99)00141-8</pub-id><pub-id pub-id-type="pmid">10576479</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proix</surname><given-names>T</given-names></name><name><surname>Delgado Saa</surname><given-names>J</given-names></name><name><surname>Christen</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>S</given-names></name><name><surname>Pasley</surname><given-names>BN</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Doyle</surname><given-names>WK</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Mégevand</surname><given-names>P</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Imagined speech can be decoded from low- and cross-frequency intracranial EEG features</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>48</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-27725-3</pub-id><pub-id pub-id-type="pmid">35013268</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1000610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000610</pub-id><pub-id pub-id-type="pmid">21532743</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rissman</surname><given-names>J</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Distributed representations in memory: insights from functional brain imaging</article-title><source>Annual Review of Psychology</source><volume>63</volume><fpage>101</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100344</pub-id><pub-id pub-id-type="pmid">21943171</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>R</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Sein</surname><given-names>J</given-names></name><name><surname>Anton</surname><given-names>JL</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Thoret</surname><given-names>E</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Auditory Hemispheric Asymmetry as a Specialization for Actions and Objects</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.04.19.537361</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rousseau</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Essay on the Origin of Languages and Writings Related to Music</source><publisher-name>UPNE</publisher-name></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Safaie</surname><given-names>M</given-names></name><name><surname>Chang</surname><given-names>JC</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Gallego</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Preserved neural dynamics across animals performing similar behaviour</article-title><source>Nature</source><volume>623</volume><fpage>765</fpage><lpage>771</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06714-0</pub-id><pub-id pub-id-type="pmid">37938772</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleh</surname><given-names>M</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Penn</surname><given-names>R</given-names></name><name><surname>Ojakangas</surname><given-names>CL</given-names></name><name><surname>Hatsopoulos</surname><given-names>NG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Fast and slow oscillations in human primary motor cortex predict oncoming behaviorally relevant cues</article-title><source>Neuron</source><volume>65</volume><fpage>461</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.02.001</pub-id><pub-id pub-id-type="pmid">20188651</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sankaran</surname><given-names>N</given-names></name><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Theunissen</surname><given-names>F</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Encoding of Melody in the Human Auditory Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.10.17.562771</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schön</surname><given-names>D</given-names></name><name><surname>Magne</surname><given-names>C</given-names></name><name><surname>Besson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The music of speech: music training facilitates pitch processing in both music and language</article-title><source>Psychophysiology</source><volume>41</volume><fpage>341</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.00172.x</pub-id><pub-id pub-id-type="pmid">15102118</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schön</surname><given-names>D</given-names></name><name><surname>Gordon</surname><given-names>R</given-names></name><name><surname>Campagne</surname><given-names>A</given-names></name><name><surname>Magne</surname><given-names>C</given-names></name><name><surname>Astésano</surname><given-names>C</given-names></name><name><surname>Anton</surname><given-names>JL</given-names></name><name><surname>Besson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Similar cerebral networks in language, music and song perception</article-title><source>NeuroImage</source><volume>51</volume><fpage>450</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.02.023</pub-id><pub-id pub-id-type="pmid">20156575</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spectral fingerprints of large-scale neuronal interactions</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>121</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1038/nrn3137</pub-id><pub-id pub-id-type="pmid">22233726</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonkusare</surname><given-names>S</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Naturalistic stimuli in neuroscience: Critically acclaimed</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>699</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.05.004</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Steinkamp</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Pymtrf: translation of the mtrf-toolbox for matlab</data-title><version designator="dd32a1f">dd32a1f</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/SRSteinkamp/pymtrf">https://github.com/SRSteinkamp/pymtrf</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Griffin</surname><given-names>S</given-names></name><name><surname>van der Meij</surname><given-names>R</given-names></name><name><surname>Dewar</surname><given-names>C</given-names></name><name><surname>Saez</surname><given-names>I</given-names></name><name><surname>Lin</surname><given-names>JJ</given-names></name><name><surname>Piantoni</surname><given-names>G</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integrated analysis of anatomical and electrophysiological human intracranial data</article-title><source>Nature Protocols</source><volume>13</volume><fpage>1699</fpage><lpage>1723</lpage><pub-id pub-id-type="doi">10.1038/s41596-018-0009-6</pub-id><pub-id pub-id-type="pmid">29988107</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>te Rietmolen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>ISpeech</data-title><version designator="swh:1:rev:93e60f6ff70ec09a4daed2733c7fed85ae9337bd">swh:1:rev:93e60f6ff70ec09a4daed2733c7fed85ae9337bd</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e21dbb50084a8108b2c918c82ab0254ee66ffd67;origin=https://github.com/noemietr/iSpeech;visit=swh:1:snp:4ba2e77c80ef1a362e15935ceec73bb24b965867;anchor=swh:1:rev:93e60f6ff70ec09a4daed2733c7fed85ae9337bd">https://archive.softwareheritage.org/swh:1:dir:e21dbb50084a8108b2c918c82ab0254ee66ffd67;origin=https://github.com/noemietr/iSpeech;visit=swh:1:snp:4ba2e77c80ef1a362e15935ceec73bb24b965867;anchor=swh:1:rev:93e60f6ff70ec09a4daed2733c7fed85ae9337bd</ext-link></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Sen</surname><given-names>K</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>2315</fpage><lpage>2331</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-02315.2000</pub-id><pub-id pub-id-type="pmid">10704507</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Dagnino</surname><given-names>B</given-names></name><name><surname>Gariel-Mathis</surname><given-names>MA</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>van der Togt</surname><given-names>C</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>14332</fpage><lpage>14341</lpage><pub-id pub-id-type="doi">10.1073/pnas.1402773111</pub-id><pub-id pub-id-type="pmid">25205811</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname><given-names>JR</given-names></name><name><surname>Freyermuth</surname><given-names>S</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Hamamé</surname><given-names>CM</given-names></name><name><surname>Ossandon</surname><given-names>T</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Minotti</surname><given-names>L</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name><name><surname>Lachaux</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Long-distance amplitude correlations in the high gamma band reveal segregation and integration within the reading network</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>6421</fpage><lpage>6434</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4363-11.2012</pub-id><pub-id pub-id-type="pmid">22573665</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Chen</surname><given-names>JL</given-names></name><name><surname>Penhune</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>When the brain plays music: auditory-motor interactions in music perception and production</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>547</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1038/nrn2152</pub-id><pub-id pub-id-type="pmid">17585307</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Gandour</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural specializations for speech and pitch: moving beyond the dichotomies</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>363</volume><fpage>1087</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2161</pub-id><pub-id pub-id-type="pmid">17890188</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>EM</given-names></name><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Goodman</surname><given-names>RR</given-names></name><name><surname>Emerson</surname><given-names>R</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party.”</article-title><source>Neuron</source><volume>77</volume><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id><pub-id pub-id-type="pmid">23473326</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Murphy</surname><given-names>JW</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009358</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009358</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94509.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Huan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This study presents <bold>valuable</bold> intracranial findings on how two types of natural auditory stimuli - speech and music - are processed in the human brain, and demonstrates that speech and music largely share network-level brain activities, thus challenging the domain-specific processing view. The evidence supporting the claims of the authors is <bold>solid</bold>. The work will be of broad interest to speech and music researchers as well as cognitive scientists in general.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94509.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this study, the authors examined the extent to which processing of speech and music depends on neural networks that are either specific to a domain or general in nature. They conducted comprehensive intracranial EEG recordings on 18 epilepsy patients as they listened to natural, continuous forms of speech and music. This enabled an exploration of brain activity at both the frequency-specific and network levels across a broad spectrum. Utilizing statistical methods, the researchers classified neural responses to auditory stimuli into categories of shared, preferred, and domain-selective types. It was observed that a significant portion of both focal and network-level brain activity is commonly shared between the processing of speech and music. However, neural responses that are selectively responsive to speech or music are confined to distributed, frequency-specific areas. The authors highlight the crucial role of using natural auditory stimuli in research and the need to explore the extensive spectral characteristics inherent in the processing of speech and music.</p><p>Strengths:</p><p>The study's strengths include its high-quality sEEG data from a substantial number of patients, covering a majority of brain regions. This extensive cortical coverage grants the authors the ability to address their research questions with high spatial resolution, marking an advantage over previous studies. They performed thorough analyses across the entire cortical coverage and a wide frequency range of neural signals. The primary analyses, including spectral analysis, temporal response function calculation, and connectivity analysis, are presented straightforwardly. These analyses, as well as figures, innovatively display how neural responses, in each frequency band and region/electrode, are 'selective' (according to the authors' definition) to speech or music stimuli. The findings are summarized in a manner that efficiently communicates information to readers. This research offers valuable insights into the cortical selectivity of speech and music processing, making it a noteworthy reference for those interested in this field. Overall, this research offers a valuable dataset and carries out extensive yet clear analyses, amounting to an impressive empirical investigation into the cortical selectivity of speech and music. It is recommended for readers who are keen on understanding the nuances of selectivity and generality in the processing of speech and music to refer to this study's data and its summarized findings.</p><p>Weaknesses:</p><p>(1) The study employed longer speech and music stimuli, thereby promising improved ecological validity as compared to prior research, a point emphasized by the authors. However, it failed to differentiate between neural responses to the diverse content or local structures within speech and music. The authors considered the potential limitation of treating these extensive speech and music stimuli as stationary signals, neglecting their complex musical or linguistic structural details and temporal variations across local structures such as sentences and phrases. This balanced perspective offered by the authors aids readers in better understanding the context of the study and highlights potential areas for expansion and further considerations.</p><p>(2) In contrast to previous studies that employed short stimulus segments along with various control stimuli to ensure that observed selectivity for speech or music was not merely due to low-level acoustic properties, this study used longer, ecological stimuli. However, the control stimuli used in this study, such as tone or syllable sequences, do not align with the low-level acoustic properties of the speech and music stimuli. This mismatch raises concerns that the differences or selectivity between speech and music observed in this study might be attributable to these basic acoustic characteristics rather than to more complex processing factors specific to speech or music. However, this should not deter readers from recognizing the study's strengths, namely, the use of iEEG recordings that offer high spatial resolution and extensive cortical coverage.</p><p>(3) The concept of selectivity - shared, preferred, and domain-selective - may not present sufficient theoretical accuracy. It is appreciated that the authors put effort into clearly defining their operational measurement on 'selectivity'. Later, the authors further mentioned the specific indication of their analyses. However, the authors' categorization of neural sites/regions as shared, preferred, or domain-selective regarding speech and music processing essentially resembles a traditional ANOVA test with posthoc analysis. While this categorization gives meaningful context to the results, the mere presence of significant differences among control stimuli, a segment of speech, and a piece of music does not present a strong case that a region is specifically selective to a type of stimulus like speech. The narrative of the manuscript could potentially lead to an overgeneralized interpretation of their findings as being broadly applicable to speech or music, if a reader does not delve into the details.</p><p>(4) The authors' approach, akin to mapping a 'receptive field' by correlating stimulus properties with neural responses to ascertain functional selectivity for speech and music, presents potential issues. If cortical regions exhibit heightened responses to one type of stimulus over another, it doesn't automatically imply selectivity or preference for that stimulus. The explanation could lie in functional aspects, such as a region's sensitivity to temporal units of a specific duration, be it music, speech, or even movie segments, and its role in chunking such units (e.g., around 500 ms), which might be more prevalent in music than in speech, or vice versa in the current study. This study does not delve into the functional mechanisms of how speech and music are processed across different musical or linguistic hierarchical levels but merely demonstrates differences in neural responses to various stimuli over a 10-minute span.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94509.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The study investigates whether speech and music processing involve specific or shared brain networks. Using intracranial EEG recordings from 18 epilepsy patients, it examines neural responses to speech and music. The authors found that most neural activity is shared between speech and music processing, without specific regional brain selectivity. Furthermore, domain-selective responses to speech or music are limited to frequency-specific coherent oscillations. The findings challenge the notion of anatomically distinct regions for different cognitive functions in the auditory process.</p><p>Strengths:</p><p>(1) This study uses a relatively large corpus of intracranial EEG data, which provides high spatiotemporal resolution neural recordings, allowing for more precise and dynamic analysis of brain responses. The use of continuous speech and music enhances ecological validity compared to artificial or segmented stimuli.</p><p>(2) This study uses multiple frequency bands in addition to just high-frequency activity (HFA), which has been the focus of many existing studies in the literature. This allows for a more comprehensive analysis of neural processing across the entire spectrum. The heterogeneity across different frequency bands also indicates that different frequency components of the neural activity may reflect different underlying neural computations.</p><p>(3) This study also adds empirical evidence towards distributed representation versus domain-specificity. It challenges the traditional view of highly specialized, anatomically distinct regions for different cognitive functions. Instead, the study suggests a more integrated and overlapping neural network for processing complex stimuli like speech and music.</p><p>Weaknesses:</p><p>While this study is overall convincing, there are still some weaknesses in the methods and analyses that limit the implication of the work.</p><p>The study's main approach, focusing primarily on the grand comparison of response amplitudes between speech and music, may overlook intricate details in neural coding. Speech and music are not entirely orthogonal with each other at different levels of analysis: at the high-level abstraction, these are two different categories of cognitive processes; at the low-level acoustics, they overlap a lot; at intermediate levels, they may also share similar features. For example, the study doesn't adequately address whether purely melodic elements in music correlate with intonations in speech at the neural level. A more granular analysis, dissecting stimuli into distinct features like pitch, phonetics, timbre, and linguistic elements, could unveil more nuanced shared, and unique neural processes between speech and music. Prior research indicates potential overlap in neural coding for certain intermediate features in speech and music (Sankaran et al. 2023), suggesting that a simple averaged response comparison might not fully capture the complexity of neural encoding. Further delineation of phonetic, melodic, linguistic, and other coding, along with an analysis of how different informational aspects (phonetic, linguistic, melodic, etc) are represented in shared neural activities, could enhance our understanding of these processes and strengthen the study's conclusions.</p><p>While classifying electrodes into 3 categories provides valuable insights, it may not fully capture the complexity of the neural response distribution to speech and music. A more nuanced and continuous approach could reveal subtler gradations in neural response, rather than imposing categorical boundaries. This could be done by computing continuous metrics, like unique variances explained by each category or by each acoustic feature, etc. Incorporating such a continuum could enhance our understanding of the neural representation of speech and music, providing a more detailed and comprehensive picture of cortical processing. This goes back to my first comment that the selected set of stimuli may not fully exploit the entire space of speech and music, and there are possible exemplars that violate the preference map here. For example, this study only considered a specific set of multi-instrumental music, it is not clear to me if other types of music would result in different response profiles in individual channels. It is also not clear if a foreign language that the listeners cannot comprehend would evoke similar response profiles. On the contrary, breaking down into the neural coding of more fundamental feature representations that constitute speech and music, and analyzing the unique contribution of each feature would give a more comprehensive understanding.</p><p>The paper's emphasis on shared and overlapping neural activity, as observed through sEEG electrodes, provides valuable insights. It is probably true that domain-specificity for speech and music does not exist at such a macro scale. However, it's important to consider that each electrode records from a large neuronal population, encompassing thousands of neurons. This broad recording scope might mask more granular, non-overlapping feature representations at the single neuron level. Thus, while the study suggests shared neural underpinnings for speech and music perception at a macroscopic level, it cannot definitively rule out the possibility of distinct, non-overlapping neural representations at the microscale of local neuronal circuits for features that are distinctly associated with speech and music. This distinction is crucial for fully understanding the neural mechanisms underlying speech and music perception that merit future endeavors with more advanced large-scale neuronal recordings.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94509.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Te Rietmolen et al., investigated the selectivity of cortical responses to speech and music stimuli using neurosurgical stereo EEG in humans. The authors address two basic questions: 1. Are speech and music responses localized in the brain or distributed; 2. Are these responses selective and domain specific or rather domain general and shared. To investigate this, the study proposes a nomenclature of shared responses (speech and music responses are not significantly different), domain selective (one domain is significant from baseline and the other is not), domain preferred (both are significant from baseline but one is larger than the other and significantly different from each other). The authors employ this framework using neural responses across the spectrum (rather than focusing on high gamma), providing evidence for a low level of selectivity across spectral signatures. To investigate the nature of the underlying representations they use encoding models to predict neural responses (low and high frequency) given a feature space of the stimulus envelope or peak rate (by time delay) and find stronger encoding for both in the low frequency neural responses. The top encoding electrodes are used as seeds for a pair-wise connectivity (coherence) in order to repeat the shared/selective/preferred analysis across the spectra, suggesting low selectivity. Spectral power and connectivity are also analyzed on the level of regional patient population to rule out (and depict) any effects driven by a select few patients. Across analyses the authors consistently show a paucity of domain selective responses and when evident these selective responses were not represented across the entire cortical region. The authors argue that speech and music mostly rely on shared neural resources.</p><p>Strengths:</p><p>I found this manuscript to be rigorous providing compelling and clear evidence towards shared neural signatures for speech and music. The use of intracranial recordings provides an important spatial and temporal resolution that lends itself to the power, connectivity and encoding analyses. The statistics and methods employed are rigorous and reliable, estimated based on permutation approaches and cross-validation/regularization was employed and reported properly. The analysis of measures across the entire spectra in both power, coherence and encoding models provides a comprehensive view of responses that no doubt will benefit the community as an invaluable resource. Analysis on the level of patient population (feasible with their high N) per region also supports the generalizability of the conclusions across a relatively large cohort of patients. Last but not least, I believe the framework of selective, preferred, and shared is a welcome lens through which to investigate cortical function.</p><p>Weaknesses:</p><p>I did not find methodological weaknesses in the current version of the manuscript. I do believe that it is important to highlight that the data is limited to passively listening to naturalistic speech and music. The speech and music stimuli are not completely controlled with varying key acoustic features (inherent to the different domains). Overall, I found the differences in stimulus and lack of attentional controls (passive listening) to be minor weaknesses that would not dramatically change the results or conclusions.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94509.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>te Rietmolen</surname><given-names>Noémie</given-names></name><role specific-use="author">Author</role><aff><institution>Institut de Neurosciences des Systemes</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Mercier</surname><given-names>Manuel R</given-names></name><role specific-use="author">Author</role><aff><institution>Aix-Marseille University</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Trébuchon</surname><given-names>Agnès</given-names></name><role specific-use="author">Author</role><aff><institution>Institut de Neurosciences des Systemes</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><role specific-use="author">Author</role><aff><institution>Aix-Marseille University</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Schön</surname><given-names>Daniele</given-names></name><role specific-use="author">Author</role><aff><institution>Aix-Marseille Université</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>We have specifically addressed the points of uncertainty highlighted in eLife's editorial assessment, which concerned the lack of low-level acoustics control, limitations of experimental design, and in-depth analysis. Regarding “the lack of low-level acoustics control, limitations of experimental design”, in response to Reviewer #1, we clarify that our study aimed to provide a broad perspective —which includes both auditory and higher-level processes— on the similarities and distinctions in processing natural speech and music within an ecological context. Regarding “the lack of in-depth analysis”, in response to Reviewer #1 and #2, we have clarified that while model-based analyzes are valuable, they pose fundamental challenges when comparing speech and music. Non-acoustic features inherently differ between speech and music (such as phonemes and pitch), making direct comparisons reliant on somewhat arbitrary choices. Our approach mitigates this challenge by analyzing the entire neural signal, thereby avoiding potential pitfalls associated with encoding models of non-comparable features. Finally, we provide some additional analyzes suggested by the Reviewers.</p><p>We sincerely appreciate your thoughtful and thorough consideration throughout the review process.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This study presents valuable intracranial findings on how two important types of natural auditory stimuli - speech and music - are processed in the human brain, and demonstrates that speech and music largely share network-level brain activities, thus challenging the domain-specific processing view. The evidence supporting the claims of the authors is solid but somewhat incomplete since although the data analysis is thorough, the results are robust and the stimuli have ecological validity, important considerations such as low-level acoustics control, limitations of experimental design, and in-depth analysis, are lacking. The work will be of broad interest to speech and music researchers as well as cognitive scientists in general.</p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>In this study, the authors examined the extent to which the processing of speech and music depends on neural networks that are either specific to a domain or general in nature. They conducted comprehensive intracranial EEG recordings on 18 epilepsy patients as they listened to natural, continuous forms of speech and music. This enabled an exploration of brain activity at both the frequency-specific and network levels across a broad spectrum. Utilizing statistical methods, the researchers classified neural responses to auditory stimuli into categories of shared, preferred, and domain-selective types. It was observed that a significant portion of both focal and network-level brain activity is commonly shared between the processing of speech and music. However, neural responses that are selectively responsive to speech or music are confined to distributed, frequency-specific areas. The authors highlight the crucial role of using natural auditory stimuli in research and the need to explore the extensive spectral characteristics inherent in the processing of speech and music.</p><p>Strengths:</p><p>The study's strengths include its high-quality sEEG data from a substantial number of patients, covering a majority of brain regions. This extensive cortical coverage grants the authors the ability to address their research questions with high spatial resolution, marking an advantage over previous studies. They performed thorough analyses across the entire cortical coverage and a wide frequency range of neural signals. The primary analyses, including spectral analysis, temporal response function calculation, and connectivity analysis, are presented straightforwardly. These analyses, as well as figures, innovatively display how neural responses, in each frequency band and region/electrode, are 'selective' (according to the authors' definition) to speech or music stimuli. The findings are summarized in a manner that efficiently communicates information to readers. This research offers valuable insights into the cortical selectivity of speech and music processing, making it a noteworthy reference for those interested in this field. Overall, this research offers a valuable dataset and carries out extensive yet clear analyses, amounting to an impressive empirical investigation into the cortical selectivity of speech and music. It is recommended for readers who are keen on understanding the nuances of selectivity and generality in the processing of speech and music to refer to this study's data and its summarized findings.</p><p>Weaknesses:</p><p>The weakness of this study, in my view, lies in its experimental design and reasoning:</p><p>(1) Despite using longer stimuli, the study does not significantly enhance ecological validity compared to previous research. The analyses treat these long speech and music stimuli as stationary signals, overlooking their intricate musical or linguistic structural details and temporal variation across local structures like sentences and phrases. In previous studies, short, less ecological segments of music were used, maintaining consistency in content and structure. However, this study, despite employing longer stimuli, does not distinguish between neural responses to the varied contents or structures within speech and music. Understanding the implications of long-term analyses, such as spectral and connectivity analyses over extended periods of around 10 minutes, becomes challenging when they do not account for the variable, sometimes quasi-periodical or even non-periodical, elements present in natural speech and music. When contrasting this study with prior research and highlighting its advantages, a more balanced perspective would have been beneficial in the manuscript.</p></disp-quote><p>Regarding ecological validity, we respectfully hold a differing perspective from the reviewer. In our view, a one-second music stimulus lacks ecological validity, as real-world music always extends much beyond such a brief duration. While we acknowledge the trade-off in selecting longer stimuli, limiting the diversity of musical styles, we maintain that only long stimuli afford participants an authentic musical listening experience. Conversely, shorter stimuli may lead participants to merely &quot;skip through&quot; musical excerpts rather than engage in genuine listening.</p><p>Regarding the critique that we &quot;did not distinguish between neural responses to the varied contents or structures within speech and music,&quot; we partly concur. Our TRF (temporal response function) analyzes incorporate acoustic content, particularly the acoustic envelope, thereby addressing this concern to some extent. However, it is accurate to note that we did not model non-acoustic features. In acknowledging this limitation, we would like to share an additional thought with the reviewer regarding model comparison for speech and music. Specifically, comparing results from a phonetic (or syntactic) model of speech to a pitch-melodic (or harmonic) model for music is not straightforward, as these models operate on fundamentally different dimensions. In other words, while assuming equivalence between phonemes and pitches may be a reasonable assumption, it in essence relies on a somewhat arbitrary choice. Consequently, comparing and interpreting neuronal population coding for one or the other model remains problematic. In summary, because the models for speech and music are different (except for acoustic models), direct comparison is challenging, although still commendable and of interest.</p><p>Finally, we did take into account the reviewer’s remark and did our best to give a more balanced perspective of our approach and previous studies in the discussion.</p><p>“While listening to natural speech and music rests on cognitively relevant neural processes, our analytical approach, extending over a rather long period of time, does not allow to directly isolate specific brain operations. Computational models -which can be as diverse as acoustic (Chi et al., 2005), cognitive (Giordano et al., 2021), information-theoretic (Di Liberto et al., 2020), or self-supervised neural network (Donhauser &amp; Baillet, 2019 ; Millet et al., 2022) models- are hence necessary to further our understanding of the type of computations performed by our reported frequency-specific distributed networks. Moreover, incorporating models accounting for musical and linguistic structure can help us avoid misattributing differences between speech and music driven by unmatched sensitivity factors (e.g., arousal, emotion, or attention) as inherent speech or music selectivity (Mas-Herrero et al., 2013; Nantais &amp; Schellenberg, 1999).”</p><disp-quote content-type="editor-comment"><p>(2) In contrast to previous studies that employed short stimulus segments along with various control stimuli to ensure that observed selectivity for speech or music was not merely due to low-level acoustic properties, this study used longer, ecological stimuli. However, the control stimuli used in this study, such as tone or syllable sequences, do not align with the low-level acoustic properties of the speech and music stimuli. This mismatch raises concerns that the differences or selectivity between speech and music observed in this study might be attributable to these basic acoustic characteristics rather than to more complex processing factors specific to speech or music.</p></disp-quote><p>We acknowledge the reviewer's concern. Indeed, speech and music differ on various levels, including acoustic and cognitive aspects, and our analyzes do not explicitly distinguish them. The aim of this study was to provide an overview of the similarities and differences between natural speech and music processing, in ecological context. Future work is needed to explore further the different hierarchical levels or networks composing such listening experiences. Of note, however, we report whole-brain results with high spatial resolution (thanks to iEEG recordings), enabling the distinction between auditory, superior temporal gyrus (STG), and higher-level responses. Our findings clearly highlight that both auditory and higher-level regions predominantly exhibit shared responses, challenging the interpretation that our results can be attributed solely to differences in 'basic acoustic characteristics'.</p><p>We have now more clearly pointed out this reasoning in the results section:</p><p>“The spatial distribution of the spectrally-resolved responses corresponds to the network typically involved in speech and music perception. This network encompasses both ventral and dorsal auditory pathways, extending well beyond the auditory cortex and, hence, beyond auditory processing that may result from differences in the acoustic properties of our baseline and experimental stimuli.“</p><disp-quote content-type="editor-comment"><p>(3) The concept of selectivity - shared, preferred, and domain-selective - increases the risks of potentially overgeneralized interpretations and theoretical inaccuracies. The authors' categorization of neural sites/regions as shared, preferred, or domain-selective regarding speech and music processing essentially resembles a traditional ANOVA test with post hoc analysis. While this categorization gives meaningful context to the results, the mere presence of significant differences among control stimuli, a segment of speech, and a piece of music does not necessarily imply that a region is specifically selective to a type of stimulus like speech. The manuscript's narrative might lead to an overgeneralized interpretation that their findings apply broadly to speech or music. However, identifying differences in neural responses to a few sets of specific stimuli in one brain region does not robustly support such a generalization. This is because speech and music are inherently diverse, and specificity often relates more to the underlying functions than to observed neural responses to a limited number of examples of a stimulus type. See the next point.</p></disp-quote><p>Exactly! Here, we present a precise operational definition of these terms, implemented with clear and rigorous statistical methods. It is important to note that in many cognitive neuroscience studies, the term &quot;selective&quot; is often used without a clear definition. By establishing operational definitions, we identified three distinct categories based on statistical testing of differences from baseline and between conditions. This approach provides a framework for more accurate interpretation of experimental findings, as now better outlined in the introduction:</p><p>“Finally, we suggest that terms should be operationally defined based on statistical tests, which results in a clear distinction between shared, selective, and preferred activity. That is, be A and B two investigated cognitive functions, “shared” would be a neural population that (compared to a baseline) significantly and equally contributes to the processing of both A and B; “selective” would be a neural population that exclusively contributes to the processing of A or B (e.g. significant for A but not B); and “preferred” would be a neural population that significantly contributes to the processing of both A and B, but more prominently for A or B (Figure 1A).”</p><p>Regarding the risk of over-generalization, we want to clarify that our manuscript does not claim that a specific region or frequency band is selective to speech or music. As indeed we focus on testing excerpts of speech and music, we employ the reverse logical reasoning: &quot;if 10 minutes of instrumental music activates a region traditionally associated with speech selectivity, we can conclude that this region is NOT speech-selective.&quot; Our conclusions revolve around the absence of selectivity rather than the presence of selective areas or frequency bands. In essence, &quot;one counterexample is enough to disprove a theory.&quot; We now further elaborated on this point in the discussion section:</p><p>“In this context, in the current study we did not observe a single anatomical region for which speech-selectivity was present, in any of our analyzes. In other words, 10 minutes of instrumental music was enough to activate cortical regions classically labeled as speech (or language) -selective. On the contrary, we report spatially distributed and frequency-specific patterns of shared, preferred, or selective neural responses and connectivity fingerprints. This indicates that domain-selective brain regions should be considered as a set of functionally homogeneous but spatially distributed voxels, instead of anatomical landmarks.”</p><disp-quote content-type="editor-comment"><p>(4) The authors' approach, akin to mapping a 'receptive field' by correlating stimulus properties with neural responses to ascertain functional selectivity for speech and music, presents issues. For instance, in the cochlea, different stimuli activate different parts of the basilar membrane due to the distinct spectral contents of speech and music, with each part being selective to certain frequencies. However, this phenomenon reflects the frequency selectivity of the basilar membrane - an important function, not an inherent selectivity for speech or music. Similarly, if cortical regions exhibit heightened responses to one type of stimulus over another, it doesn't automatically imply selectivity or preference for that stimulus. The explanation could lie in functional aspects, such as a region's sensitivity to temporal units of a specific duration, be it music, speech, or even movie segments, and its role in chunking such units (e.g., around 500 ms), which might be more prevalent in music than in speech, or vice versa in the current study. This study does not delve into the functional mechanisms of how speech and music are processed across different musical or linguistic hierarchical levels but merely demonstrates differences in neural responses to various stimuli over a 10-minute span.</p></disp-quote><p>We completely agree with the last statement, as our primary goal was not to investigate the functional mechanisms underlying speech and music processing. However, the finding of a substantial portion of the cortical network as being shared between the two domains constrains our understanding of the underlying common operations. Regarding the initial part of the comment, we would like to clarify that in the framework we propose, if cortical regions show heightened responses to one type of stimulus over another, this falls into the ‘preferred’ category. The ‘selective’ (exclusive) category, on the other hand, would require that the region be unresponsive to one of the two stimuli.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The study investigates whether speech and music processing involve specific or shared brain networks. Using intracranial EEG recordings from 18 epilepsy patients, it examines neural responses to speech and music. The authors found that most neural activity is shared between speech and music processing, without specific regional brain selectivity. Furthermore, domain-selective responses to speech or music are limited to frequency-specific coherent oscillations. The findings challenge the notion of anatomically distinct regions for different cognitive functions in the auditory process.</p><p>Strengths:</p><p>(1) This study uses a relatively large corpus of intracranial EEG data, which provides high spatiotemporal resolution neural recordings, allowing for more precise and dynamic analysis of brain responses. The use of continuous speech and music enhances ecological validity compared to artificial or segmented stimuli.</p><p>(2) This study uses multiple frequency bands in addition to just high-frequency activity (HFA), which has been the focus of many existing studies in the literature. This allows for a more comprehensive analysis of neural processing across the entire spectrum. The heterogeneity across different frequency bands also indicates that different frequency components of the neural activity may reflect different underlying neural computations.</p><p>(3) This study also adds empirical evidence towards distributed representation versus domain-specificity. It challenges the traditional view of highly specialized, anatomically distinct regions for different cognitive functions. Instead, the study suggests a more integrated and overlapping neural network for processing complex stimuli like speech and music.</p><p>Weaknesses:</p><p>While this study is overall convincing, there are still some weaknesses in the methods and analyses that limit the implication of the work.</p><p>The study's main approach, focusing primarily on the grand comparison of response amplitudes between speech and music, may overlook intricate details in neural coding. Speech and music are not entirely orthogonal with each other at different levels of analysis: at the high-level abstraction, these are two different categories of cognitive processes; at the low-level acoustics, they overlap a lot; at intermediate levels, they may also share similar features. The selected musical stimuli, incorporating both vocals and multiple instrumental sounds, raise questions about the specificity of neural activation. For instance, it's unclear if the vocal elements in music and speech engage identical neural circuits. Additionally, the study doesn't adequately address whether purely melodic elements in music correlate with intonations in speech at a neural level. A more granular analysis, dissecting stimuli into distinct features like pitch, phonetics, timbre, and linguistic elements, could unveil more nuanced shared, and unique neural processes between speech and music. Prior research indicates potential overlap in neural coding for certain intermediate features in speech and music (Sankaran et al. 2023), suggesting that a simple averaged response comparison might not fully capture the complexity of neural encoding. Further delineation of phonetic, melodic, linguistic, and other coding, along with an analysis of how different informational aspects (phonetic, linguistic, melodic, etc) are represented in shared neural activities, could enhance our understanding of these processes and strengthen the study's conclusions.</p></disp-quote><p>We appreciate the reviewer's acknowledgment that delving into the intricate details of neural coding of speech and music was beyond the scope of this work. To address some of the more precise issues raised, we have clarified in the manuscript that our musical stimuli do not contain vocals and are purely instrumental. We apologize if this was not clear initially.</p><p>“In the main experimental session, patients passively listened to ~10 minutes of storytelling (Gripari, 2004); 577 secs, La sorcière de la rue Mouffetard, (Gripari, 2004) and ~10 minutes of instrumental music (580 secs, Reflejos del Sur, Oneness, 2006) separated by 3 minutes of rest.”</p><p>Furthermore, we now acknowledge the importance of modeling melodic, phonetic, or linguistic features in the discussion, and we have referenced the work of Sankaran et al. (2024) and McCarty et al. (2023) in this regard. However, we would like to share an additional thought with the reviewer regarding model comparison for speech and music. Specifically, comparing results from a phonetic (or syntactic) model of speech to a pitch-melodic (or harmonic) model for music is not straightforward, as these models operate on fundamentally different dimensions. In other words, while assuming equivalence between phonemes and pitches may be a reasonable assumption, it in essence relies on a somewhat arbitrary choice. Consequently, comparing and interpreting neuronal population coding for one or the other model remains problematic. In summary, because the models for speech and music are different (except for acoustic models), direct comparison is challenging, although still commendable and of interest.</p><p>“These selective responses, not visible in primary cortical regions, seem independent of both low-level acoustic features and higher-order linguistic meaning (Norman-Haignere et al., 2015), and could subtend intermediate representations (Giordano et al., 2023) such as domain-dependent predictions (McCarty et al., 2023; Sankaran et al., 2023).”</p><p>References:</p><p>McCarty, M. J., Murphy, E., Scherschligt, X., Woolnough, O., Morse, C. W., Snyder, K., Mahon, B. Z., &amp; Tandon, N. (2023). Intraoperative cortical localization of music and language reveals signatures of structural complexity in posterior temporal cortex. <italic>iScience, 26(7)</italic>, 107223.</p><p>Sankaran, N., Leonard, M. K., Theunissen, F., &amp; Chang, E. F. (2023). Encoding of melody in the human auditory cortex. <italic>bioRxiv</italic>. https://doi.org/10.1101/2023.10.17.562771</p><disp-quote content-type="editor-comment"><p>The paper's emphasis on shared and overlapping neural activity, as observed through sEEG electrodes, provides valuable insights. It is probably true that domain-specificity for speech and music does not exist at such a macro scale. However, it's important to consider that each electrode records from a large neuronal population, encompassing thousands of neurons. This broad recording scope might mask more granular, non-overlapping feature representations at the single neuron level. Thus, while the study suggests shared neural underpinnings for speech and music perception at a macroscopic level, it cannot definitively rule out the possibility of distinct, non-overlapping neural representations at the microscale of local neuronal circuits for features that are distinctly associated with speech and music. This distinction is crucial for fully understanding the neural mechanisms underlying speech and music perception that merit future endeavors with more advanced large-scale neuronal recordings.</p></disp-quote><p>We appreciate the reviewer's concern, but we do not view this as a weakness for our study's purpose. Every method inherently has limitations, and intracranial recordings currently offer the best possible spatial specificity and temporal resolution for studying the human brain. Studying cell assemblies thoroughly in humans is ethically challenging, and examining speech and music in non-human primates or rats raises questions about cross-species analogy. Therefore, despite its limitations, we believe intracranial recording remains the best option for addressing these questions in humans.</p><p>Regarding the granularity of neural representation, while understanding how computations occur in the central nervous system is crucial, we question whether the single neuron scale provides the most informative insights. The single neuron approach seem more versatile (e.g., in term of cell type or layer affiliation) than the local circuitry they contribute to, which appears to be the brain's building blocks (e.g., like the laminar organization; see Mendoza-Halliday et al.,2024). Additionally, the population dynamics of these functional modules appear crucial for cognition and behavior (Safaie et al. 2023; Buzsáki and Vöröslakos, 2023). Therefore, we emphasize the need for multi-scale research, as we believe that a variety of approaches will complement each other's weaknesses when taken individually. We clarified this in the introduction:</p><p>“This approach rests on the idea that the canonical computations that underlie cognition and behavior are anchored in population dynamics of interacting functional modules (Safaie et al. 2023; Buzsáki and Vöröslakos, 2023) and bound to spectral fingerprints consisting of network- and frequency-specific coherent oscillations (Siegel et al., 2012).”</p><p>Importantly, we focus on the macro-scale and conclude that, at the anatomical region level, no speech or music selectivity can be observed during natural stimulation. This is stated in the discussion, as follow:</p><p>“In this context, in the current study we did not observe a single anatomical region for which speech-selectivity was present, in any of our analyses. In other words, 10 minutes of instrumental music was enough to activate cortical regions classically labeled as speech (or language) -selective. On the contrary, we report spatially distributed and frequency-specific patterns of shared, preferred, or selective neural responses and connectivity fingerprints. This indicates that domain-selective brain regions should be considered as a set of functionally homogeneous but spatially distributed voxels, instead of anatomical landmarks.”</p><p>References :</p><p>Mendoza-Halliday, D., Major, A.J., Lee, N. <italic>et al.</italic> A ubiquitous spectrolaminar motif of local field potential power across the primate cortex. <italic>Nat Neurosci</italic> (2024).</p><p>Safaie, M., Chang, J.C., Park, J. <italic>et al.</italic> Preserved neural dynamics across animals performing similar behaviour. <italic>Nature</italic> <bold>623</bold>, 765–771 (2023).</p><p>Buzsáki, G., &amp; Vöröslakos, M. (2023). Brain rhythms have come of age. <italic>Neuron</italic>, <italic>111</italic>(7), 922-926.</p><disp-quote content-type="editor-comment"><p>While classifying electrodes into 3 categories provides valuable insights, it may not fully capture the complexity of the neural response distribution to speech and music. A more nuanced and continuous approach could reveal subtler gradations in neural response, rather than imposing categorical boundaries. This could be done by computing continuous metrics, like unique variances explained by each category, or ratio-based statistics, etc. Incorporating such a continuum could enhance our understanding of the neural representation of speech and music, providing a more detailed and comprehensive picture of cortical processing.</p></disp-quote><p>To clarify, the metrics we are investigating (coherence, power, linear correlations) are continuous. Additionally, we conduct a comprehensive statistical analysis of these results. The statistical testing, which includes assessing differences from baseline and between the speech and music conditions using a statistical threshold, yields three categories. Of note, ratio-based statistics (a continuous metric) are provided in Figures S9 and S10 (Figures S8 and S9 in the original version of the manuscript).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>Te Rietmolen et al., investigated the selectivity of cortical responses to speech and music stimuli using neurosurgical stereo EEG in humans. The authors address two basic questions: 1. Are speech and music responses localized in the brain or distributed; 2. Are these responses selective and domain-specific or rather domain-general and shared? To investigate this, the study proposes a nomenclature of shared responses (speech and music responses are not significantly different), domain selective (one domain is significant from baseline and the other is not), domain preferred (both are significant from baseline but one is larger than the other and significantly different from each other). The authors employ this framework using neural responses across the spectrum (rather than focusing on high gamma), providing evidence for a low level of selectivity across spectral signatures. To investigate the nature of the underlying representations they use encoding models to predict neural responses (low and high frequency) given a feature space of the stimulus envelope or peak rate (by time delay) and find stronger encoding for both in the low-frequency neural responses. The top encoding electrodes are used as seeds for a pair-wise connectivity (coherence) in order to repeat the shared/selective/preferred analysis across the spectra, suggesting low selectivity. Spectral power and connectivity are also analyzed on the level of the regional patient population to rule out (and depict) any effects driven by a select few patients. Across analyses the authors consistently show a paucity of domain selective responses and when evident these selective responses were not represented across the entire cortical region. The authors argue that speech and music mostly rely on shared neural resources.</p><p>Strengths:</p><p>I found this manuscript to be rigorous providing compelling and clear evidence of shared neural signatures for speech and music. The use of intracranial recordings provides an important spatial and temporal resolution that lends itself to the power, connectivity, and encoding analyses. The statistics and methods employed are rigorous and reliable, estimated based on permutation approaches, and cross-validation/regularization was employed and reported properly. The analysis of measures across the entire spectra in both power, coherence, and encoding models provides a comprehensive view of responses that no doubt will benefit the community as an invaluable resource. Analysis of the level of patient population (feasible with their high N) per region also supports the generalizability of the conclusions across a relatively large cohort of patients. Last but not least, I believe the framework of selective, preferred, and shared is a welcome lens through which to investigate cortical function.</p><p>Weaknesses:</p><p>I did not find methodological weaknesses in the current version of the manuscript. I do believe that it is important to highlight that the data is limited to passively listening to naturalistic speech and music. The speech and music stimuli are not completely controlled with varying key acoustic features (inherent to the different domains). Overall, I found the differences in stimulus and lack of attentional controls (passive listening) to be minor weaknesses that would not dramatically change the results or conclusions.</p></disp-quote><p>Thank you for this positive review of our work. We added these points as limitations and future directions in the discussion section:</p><p>“Finally, in adopting here a comparative approach of speech and music – the two main auditory domains of human cognition – we only investigated one type of speech and of music also using a passive listening task. Future work is needed to investigate for instance whether different sentences or melodies activate the same selective frequency-specific distributed networks and to what extent these results are related to the passive listening context compared to a more active and natural context (e.g. conversation).”</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) The concepts of activation and deactivation within the study's context of selectivity are not straightforward to comprehend. It would be beneficial for the authors to provide more detailed explanations of how these phenomena relate to the selectivity of neural responses to speech and music. Such elaboration would aid readers in better understanding the nuances of how certain brain regions are selectively activated or deactivated in response to different auditory stimuli.</p></disp-quote><p>The reviewer is right that the reported results are quite complex to interpret. The concepts of activation and deactivation are generally complex to comprehend as they are in part defined by an approach (e.g., method and/or metric) and the scale of observation (Pfurtscheller et al., 1999). The power (or the magnitude) of time-frequency estimate is by definition a positive value. Deactivation (or desynchronization) is therefore related to the comparison used (e.g., baseline, control, condition). This is further complexified by the scale of the measurement, for instance, when it comes to a simple limb movement, some brain areas in sensory motor cortex are going to be activated, yet this phenomenon is accompanied at a finer scale by some desynchonization of the mu-activity, and such desynchronization is a relative measure (e.g., before/after motor movement). At a broader scale it is not rare to see some form of balance between brain networks, some being ‘inhibited’ to let some others be activated like the default mode network versus sensory-motor networks. In our case, when estimating selective responses, it is the strength of the signal that matters. The type of selectivity is then defined by the sign/direction of the comparison/subtraction. We now provide additional details about the sign of selectivity between domains and frequencies in the Methods and Results section:</p><p>Methods:</p><p>“In order to explore the full range of possible selective, preferred, or shared responses, we considered both responses greater and smaller than the baseline. Indeed, as neural populations can synchronize or desynchronize in response to sensory stimulation, we estimated these categories separately for significant activations and significant deactivations compared to baseline.”</p><p>Results:</p><p>“We classified, for each canonical frequency band, each channel into one of the categories mentioned above, i.e. shared, selective, or preferred (Figure 1A), by examining whether speech and/or music differ from baseline and whether they differ from each other. We also considered both activations and deactivations, compared to baseline, as both index a modulation of neural population activity, and have been linked with cognitive processes (Pfurtscheller &amp; Lopes da Silva, 1999; Proix et al., 2022). However, because our aim was not to interpret specific increase or decrease with respect to the baseline, we here simply consider significant deviations from the baseline. In other words, when estimating selectivity, it is the strength of the response that matters, not its direction (activation, deactivation).”</p><p>“Both domains displayed a comparable percentage of selective responses across frequency bands (Figure 4, first values of each plot). When considering separately activation (Figure 2) and deactivation (Figure 3) responses, speech and music showed complementary patterns: for low frequencies (&lt;15 Hz) speech selective (and preferred) responses were mostly deactivations and music responses activations compared to baseline, and this pattern reversed for high frequencies (&gt;15 Hz).”</p><p>References :</p><p>J.P. Lachaux, J. Jung, N. Mainy, J.C. Dreher, O. Bertrand, M. Baciu, L. Minotti, D. Hoffmann, P. Kahane,Silence Is Golden: Transient Neural Deactivation in the Prefrontal Cortex during Attentive Reading, <italic>Cerebral Cortex</italic>, Volume 18, Issue 2, February 2008, Pages 443–450</p><p>Pfurtscheller, G., &amp; Da Silva, F. L. (1999). Event-related EEG/MEG synchronization and desynchronization: basic principles. Clinical neurophysiology, 110(11), 1842-1857</p><disp-quote content-type="editor-comment"><p>(2) The manuscript doesn't easily provide information about the control conditions, yet the conclusion significantly depends on these conditions as a baseline. It would be beneficial if the authors could clarify this information for readers earlier and discuss how their choice of control stimuli influences their conclusions.</p></disp-quote><p>We added information in the Results section about the baseline conditions:</p><p>“[...] with respect to two baseline conditions, in which patients passively listened to more basic auditory stimuli: one in which patients passively listened to pure tones (each 30 ms in duration), the other in which patients passively listened to isolated syllables (/ba/ or /pa/, see Methods).”</p><p>Of note, while the choice of different ‘basic auditory stimuli’ as baseline can change the reported results in regions involved in low-level acoustical analyzes (auditory cortex), it will have no impact on the results observed in higher-level regions, which predominantly also exhibit shared responses. We have now more clearly pointed out this reasoning in the results section:</p><p>“The spatial distribution of the spectrally-resolved responses corresponds to the network typically involved in speech and music perception. This network encompasses both ventral and dorsal auditory pathways, extending well beyond the auditory cortex and, hence, beyond auditory processing that may result from differences in the acoustic properties of our baseline and experimental stimuli.“</p><disp-quote content-type="editor-comment"><p>(3) The spectral analyses section doesn't clearly explain how the authors performed multiwise correction. The authors' selectivity categorization appears similar to ANOVAs with posthoc tests, implying the need for certain corrections in the p values or categorization. Could the authors clarify this aspect?</p></disp-quote><p>We apologize that this was not in the original version of the manuscript. In the spectral analyzes, the selectivity categorization depended on both (1) the difference effects between the domains and the baseline, and (2) the difference effect between domains. Channels were marked as selective when there was (1) a significant difference between domains and (2) only one domain significantly differed from the baseline. All difference effects were estimated using the paired sample permutation tests based on the t-statistic from the mne-python library (Gramfort et al., 2014) with 1000 permutations and the build-in <italic>tmax</italic> method to correct for the multiple comparisons over channels (Nichols &amp; Holmes, 2002; Groppe et al. 2011). We have now more clearly explained how we controlled family-wise error in the Methods section:</p><p>“For each frequency band and channel, the statistical difference between conditions was estimated with paired sample permutation tests based on the t-statistic from the mne-python library (Gramfort et al., 2014) with 1000 permutations and the tmax method to control the family-wise error rate (Nichols and Holmes 2002; Groppe et al. 2011). In tmax permutation testing, the null distribution is estimated by, for each channel (i.e. each comparison), swapping the condition labels (speech vs music or speech/music vs baseline) between epochs. After each permutation, the most extreme t-scores over channels (tmax) are selected for the null distribution. Finally, the t-scores of the observed data are computed and compared to the simulated tmax distribution, similar as in parametric hypothesis testing. Because with an increased number of comparisons, the chance of obtaining a large tmax (i.e. false discovery) also increases, the test automatically becomes more conservative when making more comparisons, as such correcting for the multiple comparison between channels.”</p><p>References :</p><p>Gramfort, A., Luessi, M., Larson, E., Engemann, D. A., Strohmeier, D., Brodbeck, C., Parkkonen, L., &amp; Hämäläinen, M. S. (2014). MNE software for processing MEG and EEG data. NeuroImage, 86, 446–460.</p><p>Groppe, D. M., Bickel, S., Dykstra, A. R., Wang, X., Mégevand, P., Mercier, M. R., Lado, F. A., Mehta, A. D., &amp; Honey, C. J. (2017). iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data. Journal of Neuroscience Methods, 281, 40–48.</p><p>Nichols, T. E., &amp; Holmes, A. P. (2002). Nonparametric permutation tests for functional neuroimaging: a primer with examples. Human Brain Mapping, 15(1), 1–25.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Other suggestions:</p><p>(1) The authors need to provide more details on how the sEEG electrodes were localized and selected. Are all electrodes included or only the ones located in the gray matter? If all electrodes were used, how to localize and label the ones that are outside of gray matter? In Figures 1C &amp; 1D it seems that a lot of the electrodes were located in depth locations, how were the anatomical labels assigned for these electrodes</p></disp-quote><p>We apologize that this was not clear in the original version of the manuscript. Our electrode localization procedure was based on several steps described in detail in Mercier et al., 2022. Once electrodes were localized in a post-implant CT-scan and the coordinates projected onto the pre-implant MRI, we were able to obtain the necessary information regarding brain tissues and anatomical region. That is, first, the segmentation of the pre-impant MRI with SPM12 provided both the tissue probability maps (i.e. gray, white, and cerebrospinal fluid (csf) probabilities) and the indexed-binary representations (i.e., either gray, white, csf, bone, or soft tissues) that allowed us to dismiss electrodes outside of the brain and select those in the gray matter. Second, the individual's brain was co-registered to a template brain, which allowed us to back project atlas parcels onto individual’s brain and assign anatomical labels to each electrode. The result of this procedure allowed us to group channels by anatomical parcels as defined by the Brainnetome atlas (Figure 1D), which informed the analyses presented in section Population Prevalence (Methods, Figures 4, 9-10, S4-5). Because this study relies on stereotactic EEG, and not Electro-Cortico-Graphy, recording sites include both gyri and sulci, while depth structures were not retained.</p><p>We have now updated the “General preprocessing related to electrodes localisation” section in the Methods. The relevant part now states:</p><p>“To precisely localize the channels, a procedure similar to the one used in the iELVis toolbox and in the fieldtrip toolbox was applied (Groppe et al., 2017; Stolk et al., 2018). First, we manually identified the location of each channel centroid on the post-implant CT scan using the Gardel software (Medina Villalon et al., 2018). Second, we performed volumetric segmentation and cortical reconstruction on the pre-implant MRI with the Freesurfer image analysis suite (documented and freely available for download online http://surfer.nmr.mgh.harvard.edu/). This segmentation of the pre-implant MRI with SPM12 provides us with both the tissue probability maps (i.e. gray, white, and cerebrospinal fluid (CSF) probabilities) and the indexed-binary representations (i.e., either gray, white, CSF, bone, or soft tissues). This information allowed us to reject electrodes not located in the brain. Third, the post-implant CT scan was coregistered to the pre-implant MRI via a rigid affine transformation and the pre-implant MRI was registered to MNI152 space, via a linear and a non-linear transformation from SPM12 methods (Penny et al., 2011), through the FieldTrip toolbox (Oostenveld et al., 2011). Fourth, applying the corresponding transformations, we mapped channel locations to the pre-implant MRI brain that was labeled using the volume-based Human Brainnetome Atlas (Fan et al., 2016).”</p><p>Reference:</p><p>Mercier, M. R., Dubarry, A.-S., Tadel, F., Avanzini, P., Axmacher, N., Cellier, D., Vecchio, M. D., Hamilton, L. S., Hermes, D., Kahana, M. J., Knight, R. T., Llorens, A., Megevand, P., Melloni, L., Miller, K. J., Piai, V., Puce, A., Ramsey, N. F., Schwiedrzik, C. M., … Oostenveld, R. (2022). Advances in human intracranial electroencephalography research, guidelines and good practices. NeuroImage, 260, 119438.</p><disp-quote content-type="editor-comment"><p>(2) From Figures 5 and 6 (and also S4, S5), is it true that aside from the shared response, lower frequency bands show more music selectivity (blue dots), while higher frequency bands show more speech selectivity (red dots)? I am curious how the authors interpret this.</p></disp-quote><p>The reviewer is right in noticing the asymmetric selective response to music and speech in lower and higher frequency bands. However, while this effect is apparent in the analyzes wherein we inspected stronger synchronization (activation) compared to baseline (Figures 2 and S1), the pattern appears to reverse when examining deactivation compared to baseline (Figures 3 and S2). In other words, there seems to be an overall stronger deactivation for speech in the lower frequency bands and a relatively stronger deactivation for music in the higher frequency bands.</p><p>We now provide additional details about the sign of selectivity between domains and frequencies in the Results section:</p><p>“Both domains displayed a comparable percentage of selective responses across frequency bands (Figure 4, first values of each plot). When considering separately activation (Figure 2) and deactivation (Figure 3) responses, speech and music showed complementary patterns: for low frequencies (&lt;15 Hz) speech selective (and preferred) responses were mostly deactivations and music responses activations compared to baseline, and this pattern reversed for high frequencies (&gt;15 Hz).”</p><p>Note, however, that this pattern of results depends on only a select number of patients, i.e. when ignoring regional selective responses that are driven by as few as 2 to 4 patients, the pattern disappears (Figures 5-6). More precisely, ignoring regions explored by a small number of patients almost completely clears the selective responses for both speech and music. For this reason, we do not feel confident interpreting the possible asymmetry in low vs high frequency bands differently encoding (activation or deactivation) speech and music.</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>(1) P9 L234: Why only consider whether these channels were unresponsive to the other domain in the other frequency bands? What about the responsiveness to the target domain?</p></disp-quote><p>We thank the reviewer for their interesting suggestion. The primary objective of the cross-frequency analyzes was to determine whether domain-selective channels for a given frequency band remain unresponsive (i.e. exclusive) to the other domain across frequency bands, or whether the observed selectivity is confined to specific frequency ranges (i.e.frequency-specific). In other words, does a given channel exclusively respond to one domain and never—in whichever frequency band—to the other domain? The idea behind this question is that, for a channel to be selectively involved in the encoding of one domain, it does not necessarily need to be sensitive to all timescales underlying that domain as long as it remains unresponsive to any timescale in the other domain. However, if the channel is sensitive to information that unfolds slowly in one domain and faster in the other domain, then the channel is no longer globally domain selective, but the selectivity is frequency-specific to each domain.</p><p>The proposed analyzes answer a slightly different, albeit also meaningful, question: how many frequencies (or frequency bands) do selective responses span? From the results presented below, the reviewer can appreciate the overall steep decline in selective response beyond the single frequency band with only few channels remaining selectively responsive across maximally four frequency bands. That is, selective responses globally span one frequency band.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><caption><title>Cross-frequency channel selective responses.</title><p>The top figure shows the results for the spectral analyzes (baselined against the tones condition, including both activation and deactivation). The bottom figure shows the results for the connectivity analyzes. For each plot, the first (leftmost) value corresponds to the percentage (%) of channels displaying a selective response in a specific frequency band. In the next value, we remove the channels that no longer respond selectively to the target domain for the following frequency band. The black dots at the bottom of the graph indicate which frequency bands were successively included in the analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-sa4-fig1-v1.tif"/></fig><p>(2) P21 L623: &quot;Population prevalence.&quot; The subsection title should be in bold.</p><p>Done.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>The authors chose to use pure tone and syllables as baseline, I wonder if they also tried the rest period between tasks and if they could comment on how it differed and why they chose pure tones, (above and beyond a more active auditory baseline).</p></disp-quote><p>This is an interesting suggestion. The reason for not using the baseline between speech and music listening (or right after) is that it will be strongly influenced by the previous stimulus. Indeed, after listening to the story it is likely that patients keep thinking about the story for a while. Similarly after listening to some music, the music remains in “our head” for some time.</p><p>This is why we did not use rest but other auditory stimulation paradigms. Concerning the choice of pure tones and syllables, these happen to be used for clinical purposes to assess functioning of auditory regions. They also corresponded to a passive listening paradigm, simply with more basic auditory stimuli. We clarified this in the Results section:</p><p>“[...] with respect to two baseline conditions, in which patients passively listened to more basic auditory stimuli: one in which patients passively listened to pure tones (each 30 ms in duration), the other in which patients passively listened to isolated syllables (/ba/ or /pa/, see Methods).”</p><disp-quote content-type="editor-comment"><p>Discussion - you might want to address phase information in contrast to power. Your encoding models map onto low-frequency (bandpassed) activity which includes power and phase. However, the high-frequency model includes only power. The model comparison is not completely fair and may drive part of the effects in Figure 7a. I would recommend discussing this, or alternatively ruling out the effect with modeling power separately for the low frequency.</p></disp-quote><p>We thank the reviewer for their recommendation. First, we would like to emphasize that the chosen signal extraction techniques that we used are those most frequently reported in previous papers (e.g. Ding et al., 2012; Di Liberto et al., 2015; Mesgarani and Chang, 2012).</p><p>Low-frequency (LF) phase and high-frequency (HFa) amplitude are also known to track acoustic rhythms in the speech signal in a joint manner (Zion-Golumbic et al., 2013; Ding et al., 2016). This is possibly due to the fact that HFa amplitude and LF phase dynamics have a somewhat similar temporal structure (see Lakatos et al., 2005 ; Canolty and Knight, 2010).</p><p>Still, the reviewer is correct in pointing out the somewhat unfair model comparison and we appreciate the suggestion to rule out a potential confound. We now report in Supplementary Figure S8, a model comparison for LF amplitude vs. HFa amplitude to complement the findings displayed in Figure 7A. Overall, the reviewer can appreciate that using LF amplitude or phase does not change the results: LF (amplitude or phase) always better captures acoustic features than HFa amplitude.</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><caption><title>TRF model comparison of low-frequency (LF) amplitude and high-frequency (HFa) amplitude.</title><p>Models were investigated to quantify the encoding of the instantaneous envelope and the discrete acoustic onset edges (peakRate) by either the low frequency (LF) amplitude or the high frequency (HFa) amplitude. The ‘peakRate &amp; LF amplitude’ model significantly captures the largest proportion of channels, and is, therefore, considered the winning model. Same conventions as in Figure 7A.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94509-sa4-fig2-v1.tif"/></fig><p>References:</p><p>Canolty, R. T., &amp; Knight, R. T. (2010). The functional role of cross-frequency coupling. Trends in Cognitive Sciences, 14(11), 506–515.</p><p>Di Liberto, G. M., O’sullivan, J. A., &amp; Lalor, E. C. (2015). Low-frequency cortical entrainment to speech reflects phoneme-level processing. Current Biology, 25(19), 2457-2465.</p><p>Ding, N., &amp; Simon, J. Z. (2012). Emergence of neural encoding of auditory objects while listening to competing speakers. Proceedings of the National Academy of Sciences, 109(29), 11854-11859.</p><p>Ding, N., Melloni, L., Zhang, H., Tian, X., &amp; Poeppel, D. (2016). Cortical tracking of hierarchical linguistic structures in connected speech. Nature Neuroscience, 19(1), 158–164.</p><p>Golumbic, E. M. Z., Ding, N., Bickel, S., Lakatos, P., Schevon, C. A., McKhann, G. M., ... &amp; Schroeder, C. E. (2013). Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party”. Neuron, 77(5), 980-991.</p><p>Lakatos, P., Shah, A. S., Knuth, K. H., Ulbert, I., Karmos, G., &amp; Schroeder, C. E. (2005). An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex. Journal of Neurophysiology, 94(3), 1904–1911.</p><p>Mesgarani, N., &amp; Chang, E. F. (2012). Selective cortical representation of attended speaker in multi-talker speech perception. Nature, 485(7397), 233-236.</p><disp-quote content-type="editor-comment"><p>Similarly, the Coherence analysis is affected by both power and phase and is not dissociated. i.e. if the authors wished they could repeat the coherence analysis with phase coherence (normalizing by the amplitude). Alternatively, this issue could be addressed in the discussion above</p></disp-quote><p>We agree with the Reviewer. We have now better clarified our choice in the Methods section:</p><p>“Our rationale to use coherence as functional connectivity metric was three fold. First, coherence analysis considers both magnitude and phase information. While the absence of dissociation can be criticized, signals with higher amplitude and/or SNR lead to better time-frequency estimates (which is not the case with a metric that would focus on phase only and therefore would be more likely to include estimates of various SNR). Second, we choose a metric that allows direct comparison between frequencies. As, at high frequencies phase angle changes more quickly, phase alignment/synchronization is less likely in comparison with lower frequencies. Third, we intend to align to previous work which, for the most part, used the measure of coherence most likely for the reasons explained above.“</p></body></sub-article></article>