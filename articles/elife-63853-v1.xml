<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63853</article-id><article-id pub-id-type="doi">10.7554/eLife.63853</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Automated annotation of birdsong with a neural network that segments spectrograms</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-210530"><name><surname>Cohen</surname><given-names>Yarden</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8149-6954</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-211818"><name><surname>Nicholson</surname><given-names>David Aaron</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-211819"><name><surname>Sanchioni</surname><given-names>Alexa</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-211820"><name><surname>Mallaber</surname><given-names>Emily K</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-211821"><name><surname>Skidanova</surname><given-names>Viktoriya</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-211822"><name><surname>Gardner</surname><given-names>Timothy J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1744-3970</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Brain Sciences</institution>, <institution>Weizmann Institute of Science</institution>, <addr-line><named-content content-type="city">Rehovot</named-content></addr-line>, <country>Israel</country></aff><aff id="aff2"><institution content-type="dept">Department of Biology</institution>, <institution>Emory University</institution>, <addr-line><named-content content-type="city">Atlanta</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><institution content-type="dept">Department of Biology</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><institution content-type="dept">Phil and Penny Knight Campus for Accelerating Scientific Impact</institution>, <institution>University of Oregon</institution>, <addr-line><named-content content-type="city">Eugene</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-3674"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewing editor</role><aff><institution>Cornell University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>yarden.j.cohen@weizmann.ac.il</email> (YC);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>timg@uoregon.edu</email> (TG);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>20</day><month>01</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e63853</elocation-id><history><date date-type="received"><day>09</day><month>10</month><year>2020</year></date><date date-type="accepted"><day>19</day><month>01</month><year>2022</year></date></history><permissions><copyright-statement>Â© 2022, Cohen et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Cohen et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63853-v1.pdf"/><abstract><p>Songbirds provide a powerful model system for studying sensory-motor learning. However, many analyses of birdsong require time-consuming, manual annotation of its elements, called syllables. Automated methods for annotation have been proposed, but these methods assume that audio can be cleanly segmented into syllables, or they require carefully tuning multiple statistical models. Here we present TweetyNet: a single neural network model that learns how to segment spectrograms of birdsong into annotated syllables. We show that TweetyNet mitigates limitations of methods that rely on segmented audio. We also show that TweetyNet performs well across multiple individuals from two species of songbirds, Bengalese finches and canaries. Lastly, we demonstrate that using TweetyNet we can accurately annotate very large datasets containing multiple days of song, and that these predicted annotations replicate key findings from behavioral studies. In addition, we provide open-source software to assist other researchers, and a large dataset of annotated canary song that can serve as a benchmark. We conclude that TweetyNet makes it possible to address a wide range of new questions about birdsong.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS104925</award-id><principal-award-recipient><name><surname>Sanchioni</surname><given-names>Alexa</given-names></name><name><surname>Mallaber</surname><given-names>Emily K</given-names></name><name><surname>Skidanova</surname><given-names>Viktoriya</given-names></name><name><surname>Gardner</surname><given-names>Timothy J</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R24NS098536</award-id><principal-award-recipient><name><surname>Sanchioni</surname><given-names>Alexa</given-names></name><name><surname>Mallaber</surname><given-names>Emily K</given-names></name><name><surname>Skidanova</surname><given-names>Viktoriya</given-names></name><name><surname>Gardner</surname><given-names>Timothy J</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS118424</award-id><principal-award-recipient><name><surname>Gardner</surname><given-names>Timothy J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were approved by the Institutional Animal Care and Use Committees of Boston University (protocol numbers 14-028 and 14-029). Song data were collected from adult male canaries (n = 5). Canaries were individually housed for the entire duration of the experiment and kept on a light-dark cycle matching the daylight cycle in Boston (42.3601 N). The birds were not used in any other experiments.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Datasets of annotated Bengalese finch song are available at:https://figshare.com/articles/Bengalese_Finch_song_repository/4805749https://figshare.com/articles/BirdsongRecognition/3470165Datasets of annotated canary song are available at:https://doi.org/10.5061/dryad.xgxd254f4Model checkpoints, logs, and source data files are available at:http://dx.doi.org/10.5061/dryad.gtht76hk4Source data files for figure are in the repository associated with the paper:https://github.com/yardencsGitHub/tweetynet(version 0.4.3, 10.5281/zenodo.3978389).</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Cohen Y</collab></person-group><year iso-8601-date="2020">2020</year><source>Song recordings and annotation files of 3 canaries used to evaluate training of TweetyNet models for birdsong segmentation and annotation</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.xgxd254f4">https://doi.org/10.5061/dryad.xgxd254f4</ext-link><comment>Dryad Digital Repository, doi:10.5061/dryad.xgxd254f4</comment></element-citation><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Nicholson D</collab><collab>Cohen Y</collab></person-group><year iso-8601-date="2020">2020</year><source>Model checkpoints, logs, and source data files</source><ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.gtht76hk4">http://dx.doi.org/10.5061/dryad.gtht76hk4</ext-link><comment>Dryad Digital Repository, doi:10.5061/dryad.gtht76hk4</comment></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Nicholson D</collab><collab>Queen JE</collab><collab>Sober JS</collab></person-group><year iso-8601-date="2017">2017</year><source>Bengalese Finch song repository.</source><ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Bengalese_Finch_song_repository/4805749">https://figshare.com/articles/Bengalese_Finch_song_repository/4805749</ext-link><comment>Figshare, https://doi.org/10.6084/m9.figshare.4805749.v6</comment></element-citation><element-citation id="dataset4" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Koumura</collab><collab>Takuya</collab></person-group><year iso-8601-date="2016">2016</year><source>BirdsongRecognition.</source><ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/media/BirdsongRecognition/3470165">https://figshare.com/articles/media/BirdsongRecognition/3470165</ext-link><comment>Figshare, https://doi.org/10.6084/m9.figshare.3470165.v1</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-63853-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>