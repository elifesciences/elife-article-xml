<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">66526</article-id><article-id pub-id-type="doi">10.7554/eLife.66526</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Natural-gradient learning for spiking neurons</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-221918"><name><surname>Kreutzer</surname><given-names>Elena</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7975-7206</contrib-id><email>kreutzer@pyl.unibe.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-224156"><name><surname>Senn</surname><given-names>Walter</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3622-0497</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="fn1">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-224157"><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2632-0427</contrib-id><email>mihai.petrovici@unibe.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="fn1">‡</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k7v4d05</institution-id><institution>Department of Physiology, University of Bern</institution></institution-wrap><addr-line><named-content content-type="city">Bern</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/038t36y30</institution-id><institution>Kirchhoff-Institute for Physics, Heidelberg University</institution></institution-wrap><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Huguenard</surname><given-names>John R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University School of Medicine</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="other" id="fn1"><label>‡</label><p>Joint senior authorship</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>25</day><month>04</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e66526</elocation-id><history><date date-type="received" iso-8601-date="2021-01-13"><day>13</day><month>01</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-02-21"><day>21</day><month>02</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Kreutzer et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Kreutzer et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-66526-v1.pdf"/><abstract><p>In many normative theories of synaptic plasticity, weight updates implicitly depend on the chosen parametrization of the weights. This problem relates, for example, to neuronal morphology: synapses which are functionally equivalent in terms of their impact on somatic firing can differ substantially in spine size due to their different positions along the dendritic tree. Classical theories based on Euclidean-gradient descent can easily lead to inconsistencies due to such parametrization dependence. The issues are solved in the framework of Riemannian geometry, in which we propose that plasticity instead follows natural-gradient descent. Under this hypothesis, we derive a synaptic learning rule for spiking neurons that couples functional efficiency with the explanation of several well-documented biological phenomena such as dendritic democracy, multiplicative scaling, and heterosynaptic plasticity. We therefore suggest that in its search for functional synaptic plasticity, evolution might have come up with its own version of natural-gradient descent.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>parametrization invariance</kwd><kwd>natural-gradient descent</kwd><kwd>efficient learning</kwd><kwd>homeostasis</kwd><kwd>dendritic learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>European Union 7th Framework Programme</institution></institution-wrap></funding-source><award-id>604102</award-id><principal-award-recipient><name><surname>Senn</surname><given-names>Walter</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>720270</award-id><principal-award-recipient><name><surname>Senn</surname><given-names>Walter</given-names></name><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>785907 and 945539</award-id><principal-award-recipient><name><surname>Senn</surname><given-names>Walter</given-names></name><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Swiss National Fonds</institution></institution-wrap></funding-source><award-id>personal grant 310030L_156863 (WS)</award-id><principal-award-recipient><name><surname>Senn</surname><given-names>Walter</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Swiss National Fonds</institution></institution-wrap></funding-source><award-id>Sinergia grant CRSII5_180316</award-id><principal-award-recipient><name><surname>Senn</surname><given-names>Walter</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Manfred Stärk Foundation</institution></institution-wrap></funding-source><award-id>personal grant</award-id><principal-award-recipient><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A parametrization-invariant synaptic plasticity rule based on natural-gradient descent leads to multiple predictions for the biological plasticity process, some of which relate to well-studied phenomena such as heterosynaptic plasticity.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding the fundamental computational principles underlying synaptic plasticity represents a long-standing goal in neuroscience. To this end, a multitude of top-down computational paradigms have been developed, which derive plasticity rules as gradient descent on a particular objective function of the studied neural network (<xref ref-type="bibr" rid="bib40">Rosenblatt, 1958</xref>; <xref ref-type="bibr" rid="bib43">Rumelhart et al., 1986</xref>; <xref ref-type="bibr" rid="bib37">Pfister et al., 2006</xref>; <xref ref-type="bibr" rid="bib16">D’Souza et al., 2010</xref>; <xref ref-type="bibr" rid="bib18">Friedrich et al., 2011</xref>).</p><p>However, the exact physical quantity to which these synaptic weights correspond often remains unspecified. What is frequently simply referred to as <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (the synaptic weight from neuron <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to neuron <italic>i</italic>) might relate to different components of synaptic interaction, such as calcium concentration in the presynaptic axon terminal, neurotransmitter concentration in the synaptic cleft, receptor activation in the postsynaptic dendrite or the postsynaptic potential (PSP) amplitude in the spine, the dendritic shaft or at the soma of the postsynaptic cell. All these biological processes can be linked by transformation rules, but depending on which of them represents the variable with respect to which performance is optimized, the network behavior during training can be markedly different.</p><p>As an example we consider the parametrization of the synaptic strength either as PSP amplitude in the soma, <inline-formula><mml:math id="inf3"><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:math></inline-formula>, or as PSP amplitude in the dendrite, <inline-formula><mml:math id="inf4"><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msup></mml:math></inline-formula> (see also <xref ref-type="fig" rid="fig1">Figure 1</xref> and Sec. ‘The naive Euclidean gradient is not parametrization-invariant’). Reparametrizing the synaptic strength in this way implies an attenuation factor for each single synapse, but different factors are assigned across the positions on the dendritic tree. As a consequence, the weight vector will follow a different trajectory during learning depending on whether the somatic or dendritic parametrization of the PSP amplitude was chosen.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Classical gradient descent depends on chosen parametrization.</title><p>(<bold>A</bold>) The strength of a synapse can be parametrized in various ways, for example, as the EPSP amplitude at either the soma <inline-formula><mml:math id="inf5"><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:math></inline-formula> or the dendrite <inline-formula><mml:math id="inf6"><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msup></mml:math></inline-formula>. Biological processes such as attenuation govern the relationship between these variables. Depending on the chosen parametrization, Euclidean-gradient descent can yield different results. (<bold>B</bold>) Phenomenological correlates. EPSPs before learning are represented as continuous, after learning as dashed curves. The light blue arrow represents gradient descent on the error as a function of the somatic EPSP <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (also shown in light blue). The resulting weight change leads to an increase <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> in the somatic EPSP after learning. The dark blue arrows track the calculation of the same gradient, but with respect to the dendritic EPSP (also shown in dark blue): (1) taking the attenuation into account in order to compute the error as a function of <inline-formula><mml:math id="inf9"><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msup></mml:math></inline-formula>, (2) calculating the gradient, followed by (3) deriving the associated change in <inline-formula><mml:math id="inf10"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, again considering attenuation. Due to the attenuation <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>f</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> entering the calculation twice, the synaptic weights updates, as well as the associated evolution of a neuron’s output statistics over time, will differ under the two parametrizations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig1-v1.tif"/></fig><p>It certainly could be the case that evolution has favored a parametrization-dependent learning rule, along with one particular parametrization over all others, but this would necessarily imply sub-optimal convergence for all but a narrow set of neuron morphologies and connectome configurations. An invariant learning rule on the other hand would not only be mathematically unambiguous and therefore more elegant, but could also improve learning, thus increasing fitness.</p><p>In some aspects, the question of invariant behavior is related to the principle of relativity in physics, which requires the laws of physics – in our case: the improvement of performance during learning – to be the same in all frames of reference. What if neurons would seek to conserve the way they adapt their behavior regardless of, for example, the specific positioning of synapses along their dendritic tree? Which equations of motion – in our case: synaptic learning rules – are able to fulfill this requirement?</p><p>The solution lies in following the path of steepest descent not in relation to a small change in the synaptic weights (Euclidean-gradient descent), but rather with respect to a small change in the input-output distribution (natural-gradient descent). This requires taking the gradient of the error function with respect to a metric defined directly on the space of possible input-output distributions, with coordinates defined by the synaptic weights. First proposed in <xref ref-type="bibr" rid="bib4">Amari, 1998</xref>, but with earlier roots in information geometry (<xref ref-type="bibr" rid="bib3">Amari, 1987</xref>; <xref ref-type="bibr" rid="bib6">Amari and Nagaoka, 2000</xref>), natural-gradient methods (<xref ref-type="bibr" rid="bib55">Yang and Amari, 1998</xref>; <xref ref-type="bibr" rid="bib39">Rattray and Saad, 1999</xref>; <xref ref-type="bibr" rid="bib34">Park et al., 2000</xref>; <xref ref-type="bibr" rid="bib24">Kakade, 2001</xref>) have recently been rediscovered in the context of deep learning (<xref ref-type="bibr" rid="bib35">Pascanu and Bengio, 2013</xref>; <xref ref-type="bibr" rid="bib31">Martens, 2014</xref>; <xref ref-type="bibr" rid="bib33">Ollivier, 2015</xref>; <xref ref-type="bibr" rid="bib5">Amari et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Bernacchia et al., 2018</xref>). Moreover, <xref ref-type="bibr" rid="bib35">Pascanu and Bengio, 2013</xref> showed that the natural-gradient learning rule is closely related to other machine learning algorithms. However, most of the applications focus on rate-based networks which are not inherently linked to a statistical manifold and have to be equipped with Gaussian noise or a probabilistic output layer interpretation in order to allow an application of the natural gradient. Furthermore, a biologically plausible synaptic plasticity rule needs to make all the required information accessible at the synapse itself, which is usually unnecessary and therefore largely ignored in machine learning.</p><p>The stochastic nature of neuronal outputs in-vivo (see, e.g. <xref ref-type="bibr" rid="bib47">Softky and Koch, 1993</xref>) provides a natural setting for plasticity rules based on information geometry. As a model for biological synapses, natural gradient combines the elegance of invariance with the success of gradient-descent-based learning rules. In this manuscript, we derive a closed-form synaptic learning rule based on natural-gradient descent for spiking neurons and explore its implications. Our learning rule equips the synapses with more functionality compared to classical error learning by enabling them to adjust their learning rate to their respective impact on the neuron’s output. It naturally takes into account relevant variables such as the statistics of the afferent input or their respective positions on the dendritic tree. This allows a set of predictions which are corroborated by both experimentally observed phenomena such as dendritic democracy and multiplicative weight dynamics and theoretically desirable properties such as Bayesian reasoning (<xref ref-type="bibr" rid="bib29">Marceau-Caron and Ollivier, 2007</xref>). Furthermore, and unlike classical error-learning rules, plasticity based on the natural gradient is able to incorporate both homo- and heterosynaptic phenomena into a unified framework. While theoretically derived heterosynaptic components of learning rules are notoriously difficult for synapses to implement due to their non-locality, we show that in our learning rule they can be approximated by quantities accessible at the locus of plasticity. In line with results from machine learning, the combination of these features also enables faster convergence during supervised learning.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The naive Euclidean gradient is not parametrization-invariant</title><p>We consider a cost function <inline-formula><mml:math id="inf12"><mml:mi>C</mml:mi></mml:math></inline-formula> on the neuronal level that, in the sense of cortical credit assignment (see e.g. <xref ref-type="bibr" rid="bib44">Sacramento et al., 2017</xref>), can relate to some behavioral cost of the agent that it serves. The output of the neuron depends on the amplitudes of the somatic PSPs elicited by the presynaptic spikes. We denote these ‘somatic weights’ by <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and may parametrize the neuronal cost as <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>However, dendritic PSP amplitudes <inline-formula><mml:math id="inf15"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msup></mml:math></inline-formula> can be argued to offer a more unmitigated representation of synaptic weights, so we might rather wish to express the cost as <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. These two parametrizations are related by an attenuation factor <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (between 0 and 1):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In general, this attenuation factor depends on the synaptic position and is therefore described by a vector that is multiplied component-wise with the weights. For clarity, we assume a simplified picture in which we neglect, for example, the spread of PSPs as they travel along dendritic cables. However, our observations regarding the effects of reparametrization hold in general.</p><p>It may now seem straightforward to switch between the somatic and dendritic representation of the cost by simply substituting variables, for example<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To derive a plasticity rule for the somatic and dendritic weights, we might consider gradient descent on the cost:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>At first glance, this relation seems reasonable: dendritic weight changes affect the cost more weakly then somatic weight changes, so their respective gradient is more shallow by the factor <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, from a functional perspective, the opposite should be true: dendritic weights should experience a larger change than somatic weights in order to elicit the same effect on the cost. This conflict can be made explicit by considering that somatic weight changes are, themselves, attenuated dendritic weight changes: <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Substituting this into <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> leads to an inconsistency: <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. To solve this conundrum, we need to shift the focus from changing the synaptic input to changing the neuronal output, while at the same time considering a more rigorous treatment of gradient descent (see also <xref ref-type="bibr" rid="bib50">Surace et al., 2020</xref>).</p></sec><sec id="s2-2"><title>The natural-gradient plasticity rule</title><p>We consider a neuron with somatic potential <inline-formula><mml:math id="inf21"><mml:mi>V</mml:mi></mml:math></inline-formula> (above a baseline potential <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>V</mml:mi><mml:mi>rest</mml:mi></mml:msub></mml:math></inline-formula>) evoked by the spikes <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf24"><mml:mi>n</mml:mi></mml:math></inline-formula> presynaptic afferents firing at rates <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The presynaptic spikes of afferent <italic>i</italic> cause a train of weighted dendritic potentials <inline-formula><mml:math id="inf26"><mml:mrow><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msubsup></mml:mpadded><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> locally at the synaptic site. The <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the unweighted synaptic potential (USP) train elicited by the low-pass-filtered spike train <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. At the soma, each dendritic potential is attenuated by a potentially nonlinear function that depends on the synaptic location:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The somatic voltage above baseline thus reads as<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msubsup></mml:mpadded><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msubsup><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We further assume that the neuron’s firing follows an inhomogeneous Poisson process whose rate<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>depends on the current membrane potential through a nonlinear transfer function <inline-formula><mml:math id="inf29"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. In this case, spiking in a sufficiently short interval <inline-formula><mml:math id="inf30"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is Bernoulli-distributed. The probability of a spike occurring in this interval (denoted as <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) is then given by<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which defines our generalized linear neuron model (<xref ref-type="bibr" rid="bib20">Gerstner and Kistler, 2002</xref>). Here, we used <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as a shorthand notation for the USP vector <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The full input-output distribution then reads<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the probability density of the input <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is independent of synaptic weights (see also Sec. ‘Detailed derivation of the natural-gradient learning rule’). In the following, we drop the time indices for better readability.</p><p>In view of this stochastic nature of neuronal responses, we consider a neuron that strives to reproduce a target firing distribution <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The required information is received as teacher spike train <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> that is sampled from <inline-formula><mml:math id="inf37"><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> (see Sec. ‘Sketch for the derivation of the somatic natural-gradient learning rule’ for details). In this context, plasticity may follow a supervised-learning paradigm based on gradient descent. The Kullback-Leibler divergence between the neuron’s current and its target firing distribution<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>∥</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>represents a natural cost function which measures the error between the current and the desired output distribution in an information-theoretic sense. Minimizing this cost function is equivalent to maximizing the log-likelihood of teacher spikes, since <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> does not depend on <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Using naive Euclidean-gradient descent with respect to the synaptic weights (denoted by <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) results in the well-known error-correcting rule (<xref ref-type="bibr" rid="bib37">Pfister et al., 2006</xref>),<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which is a spike-based version of the classical perceptron learning rule (<xref ref-type="bibr" rid="bib40">Rosenblatt, 1958</xref>), whose multilayer version forms the basis of the error-backpropagation algorithm (<xref ref-type="bibr" rid="bib43">Rumelhart et al., 1986</xref>). On the single-neuron level, a possible biological implementation has been suggested by <xref ref-type="bibr" rid="bib51">Urbanczik and Senn, 2014</xref>, who demonstrated how a neuron may exploit its morphology to store errors, an idea that was recently extended to multilayer networks (<xref ref-type="bibr" rid="bib44">Sacramento et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Haider et al., 2021</xref>).</p><p>However, as we argued above, learning based on Euclidean-gradient descent is not unproblematic. It cannot account for synaptic weight (re)parametrization, as caused, for example, by the diversity of synaptic loci on the dendritic tree. <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> represents the learning rule for a somatic parametrization of synaptic weights. For a local computation of Euclidean gradients using a dendritic parametrization, weight updates decrease with increasing distance towards the soma (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), which is likely to harm the convergence speed toward an optimal weight configuration. With the multiplicative USP term <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> being the only manifestation of presynaptic activity, there is no mechanism by which to take into account input variability. This can, in turn, also impede learning, by implicitly assigning equal importance to reliable and unreliable inputs. Furthermore, when compared to experimental evidence, this learning rule cannot explain heterosynaptic plasticity, as it is purely presynaptically gated.</p><p>In general, Euclidean-gradient descent is well-known to exhibit slow convergence in non-isotropic regions of the cost function (<xref ref-type="bibr" rid="bib42">Ruder, 2016</xref>), with such non-isotropy frequently arising or being aggravated by an inadequate choice of parametrization (see <xref ref-type="bibr" rid="bib33">Ollivier, 2015</xref> and <xref ref-type="fig" rid="fig2">Figure 2</xref>). In contrast, natural-gradient descent is, by construction, immune to these problems. The key idea of natural gradient as outlined by Amari is to follow the (locally) shortest path in terms of the neuron’s firing distribution. Argued from a normative point of view, this is the only ‘correct’ path to consider, since plasticity aims to adapt a neuron’s behavior, that is, its input-output relationship, rather than some internal parameter (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In the following, we therefore drop the index from the synaptic weights <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to emphasize the parametrization-invariant nature of the natural gradient.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The natural gradient represents the true gradient direction on the manifold of neuronal input-output distributions.</title><p>(<bold>A</bold>) During supervised learning, the error between the current and the target state is measured in terms of a cost function defined on the neuron’s output space; in our case, this is the manifold formed by the neuronal output distributions <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. As the output of a neuron is determined by the strength of incoming synapses, the cost <inline-formula><mml:math id="inf44"><mml:mi>C</mml:mi></mml:math></inline-formula> depends indirectly on the afferent weight vector <inline-formula><mml:math id="inf45"><mml:mi mathvariant="bold-italic">w</mml:mi></mml:math></inline-formula>. Since the gradient of a function depends on the distance measure of the underlying space, Euclidean-gradient descent, which follows the gradient of the cost as a function of the synaptic weights <inline-formula><mml:math id="inf46"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">∂</mml:mo><mml:mo mathvariant="normal">⁡</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo mathvariant="normal">/</mml:mo><mml:mrow><mml:mo mathvariant="normal">∂</mml:mo><mml:mo mathvariant="normal">⁡</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, is not uniquely defined, but depends on how <inline-formula><mml:math id="inf47"><mml:mi mathvariant="bold-italic">w</mml:mi></mml:math></inline-formula> is parametrized. If, instead, we follow the gradient on the output manifold itself, it becomes independent of the underlying parametrization. Expressed in a specific parametrization, the resulting natural gradient contains a correction term that accounts for the distance distortion between the synaptic parameter space and the output manifold. (<bold>B–C</bold>) Standard gradient descent learning is suited for isotropic (<bold>C</bold>), rather than for non-isotropic (<bold>B</bold>) cost functions. For example, the magnitude of the gradient decreases in valley regions where the cost function is flat, resulting in slow convergence to the target. A non-optimal choice of parametrization can introduce such artefacts and therefore harm the performance of learning rules based on Euclidean-gradient descent. In contrast, natural-gradient learning will locally correct for distortions arising from non-optimal parametrizations (see also <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig2-v1.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Natural-gradient plasticity speeds up learning in a simple regression task.</title><p>(<bold>A</bold>) We tested the performance of the natural gradient rule in a supervised learning scenario, where a single output neuron had to adapt its firing distribution to a target distribution, delivered in form of spikes from a teacher neuron. The latter was modeled as a Poisson neuron firing with a time-dependent instantaneous rate,<inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf49"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="normal">*</mml:mo></mml:msup></mml:math></inline-formula> represents a randomly chosen target weight vector. The input consisted of Poisson spikes from <inline-formula><mml:math id="inf50"><mml:mi>n</mml:mi></mml:math></inline-formula> afferents, half of them firing at 10 Hz and 50 Hz, respectively. For our simulations, we used <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula> afferents, except for the weight path plots in (<bold>D</bold>) and (<bold>E</bold>), where the number of afferents was reduced to <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:math></inline-formula> for illustration purposes. (<bold>B–C</bold>) Spike trains, PSTHs and voltage traces for teacher (orange) and student (red) neuron before (<bold>B</bold>) and after (<bold>C</bold>) learning with natural-gradient plasticity. During learning, the firing patterns of the student neuron align to those of the teacher neuron. The structure in these patterns comes from autocorrelations in this instantaneous rate. These, in turn, are due to mechanisms such as the membrane filter (as seen in the voltage traces) and the nonlinear activation function. (<bold>D–E</bold>) Exemplary weight evolution during Euclidean-gradient (<bold>D</bold>) and natural-gradient (<bold>E</bold>) learning given <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:math></inline-formula> afferents with the same two rates as before. Here, <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>w</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msub></mml:math></inline-formula> corresponds to <inline-formula><mml:math id="inf55"><mml:msup><mml:mi>x</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msup></mml:math></inline-formula> in panel A (10 Hz input) and <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>w</mml:mi><mml:mn mathvariant="normal">2</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf57"><mml:msup><mml:mi>x</mml:mi><mml:mn mathvariant="normal">2</mml:mn></mml:msup></mml:math></inline-formula> (50 Hz input). Thick solid lines represent contour lines of the cost function <inline-formula><mml:math id="inf58"><mml:mi>C</mml:mi></mml:math></inline-formula>. The respective vector fields depict normalized negative Euclidean and natural gradients of the cost <inline-formula><mml:math id="inf59"><mml:mi>C</mml:mi></mml:math></inline-formula>, averaged over 2000 input samples. The thin solid lines represent the paths traced out by the input weights during learning averaged over 500 trials. (<bold>F</bold>) Learning curves for <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula> afferents using natural-gradient and Euclidean-gradient plasticity. The plot shows averages over 1000 trials with initial and target weights randomly chosen from a uniform distribution <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Fixed learning rates were tuned for each algorithm separately to exhibit the fastest possible convergence to a root mean squared error of 0.8 Hz in the student neuron’s output rate.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig3-v1.tif"/></fig><p>For the concept of a locally shortest path to make sense in terms of distributions, we require the choice of a distance measure for probability distributions. Since a parametric statistical model, such as the set of our neuron’s realizable output distributions, forms a Riemannian manifold, a local distance measure can be obtained in form of a Riemannian metric. The Fisher metric (<xref ref-type="bibr" rid="bib38">Rao, 1945</xref>), an infinitesmial version of the <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>D</mml:mi><mml:mi>KL</mml:mi></mml:msub></mml:math></inline-formula>, represents a canonical choice on manifolds of probability distributions (<xref ref-type="bibr" rid="bib6">Amari and Nagaoka, 2000</xref>). On a given parameter space, the Fisher metric may be expressed in terms of a bilinear product with the Fisher information matrix<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The Fisher metric locally measures distances in the <inline-formula><mml:math id="inf63"><mml:mi>p</mml:mi></mml:math></inline-formula>-manifold as a function of the chosen parametrization. We can then obtain the natural gradient (which intuitively may be thought of as ‘<inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’) by correcting the Euclidean gradient <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>:=</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with the distance measure above:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This correction guarantees invariance of the gradient under reparametrization (see also Sec. ‘Reparametrization and the general natural-gradient rule’). The natural-gradient learning rule is then given as <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Calculating the right-hand expression for the case of Poisson-spiking neurons (for details, see Detailed derivation of the natural-gradient learning rule and Inverse of the Fisher Information Matrix), this takes the form<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is an arbitrary weight parametrization that relates to the somatic amplitudes via a component-wise rescaling<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that the attenuation function in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> represents a special case of <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> for dendritic amplitudes.</p><p>We used the shorthand <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> for three scaling factors introduced by the natural gradient that we address in detail below. For easier reading, we use a shorthand notation in which multiplications, divisions and scalar functions of vectors apply component-wise.</p><p><xref ref-type="disp-formula" rid="equ13">Equation 13</xref> represents the complete expression of our natural-gradient rule, which we discuss throughout the remainder of the manuscript. Note that, while having used a standard sigmoidal transfer function throughout the paper, <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> holds for every sufficiently smooth <inline-formula><mml:math id="inf71"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>.</p><p>Natural-gradient learning conserves both the error term <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the USP contribution <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> from classical gradient-descent plasticity. However, by including the relationship between the parametrization of interest <inline-formula><mml:math id="inf74"><mml:mi mathvariant="bold-italic">w</mml:mi></mml:math></inline-formula> and the somatic PSP amplitudes <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, natural-gradient-based plasticity explicitly accounts for reparametrization distortions, such as those arising from PSP attenuation during propagation along the dendritic tree. Furthermore, natural-gradient learning introduces multiple scaling factors and new plasticity components, whose characteristics will be further explored in dedicated sections below (see also Sec. ‘Global scaling factor’ and ‘Empirical Analysis of <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’ for more details).</p><p>First of all, we note the appearance of two scaling factors (more details in ‘Input and output-specific scaling’). On one hand, the size of the synaptic adjustment is modulated by a global scaling factor <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula>, which adjusts synaptic weight updates to the characteristics of the output non-linearity, similarly to the synapse-specific scaling by the inverse of <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Furthermore <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> also depends on the output statistics of the neuron, harmonizing plasticity across different states in the output distribution (see Sec. ‘Global scaling factor’). On the other hand, a second, synapse-specific learning rate scaling accounts for the statistics of the input at the respective synapse, in the form of a normalization by the afferent input rate <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>c</mml:mi><mml:mi>ϵ</mml:mi></mml:msub></mml:math></inline-formula> is a constant that depends on the PSP kernel (see Sec. ‘Neuron model’). Unlike the global modulation introduced by <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula>, this scaling only affects the USP-dependent plasticity component. Just as for Euclidean-gradient-based learning, the latter is directly evoked by the spike trains arriving at the synapse. Therefore, the resulting plasticity is homosynaptic, affecting only synapses which receive afferent input.</p><p>However, in the case of natural-gradient learning, this input-specific adaptation is complemented by two additional forms of heterosynaptic plasticity (Sec. ‘Interplay of homosynaptic and heterosynaptic plasticity’). First, the learning rule has a bias term <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula> which uniformly adjusts all synapses and may be considered homeostatic, as it usually opposes the USP-dependent plasticity contribution. The amplitude of this bias does not exclusively depend on the afferent input at the respective synapse, but is rather determined by the overall input to the neuron. Thus, unlike the USP-dependent component, this heterosynaptic plasticity component equally affects both active and inactive synaptic connections. Furthermore, natural-gradient descent implies the presence of another plasticity component <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> which adapts the synapses depending on their current weight. More specifically, connections that are already strong are subject to larger changes compared to weaker ones. Since the proportionality factor <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> only depends on global variables such as the membrane potential, this component also affects both active and inactive synapses.</p><p>The full expressions for <inline-formula><mml:math id="inf87"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> are functions of the membrane potential, its mean and its variance, which represent synapse-local quantities. In addition, <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> also depend on the total input <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the total instantaneous presynaptic rate <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, under reasonable assumptions such as a high number of presynaptic partners and for a large, diverse set of empirically tested scenarios, we have shown that these factors can be reduced to simple functions of variables that are fully accessible at the locus of individual synapses:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>c</mml:mi><mml:mi>ϵ</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> are constants (Sec. ‘Global scaling factor’ and ‘Empirical Analysis of <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’). The above learning rule along with closed-form expressions for these factors represent the main analytical findings of this paper.</p><p>In the following, we demonstrate that the additional terms introduced in natural-gradient-based plasticity confer important advantages compared to Euclidean-gradient descent, both in terms of of convergence as well as with respect to biological plausibility. More precisely, we show that our plasticity rule improves convergence in a supervised learning task involving an anisotropic cost function, a situation which is notoriously hard to deal with for Euclidean-gradient-based learning rules (<xref ref-type="bibr" rid="bib42">Ruder, 2016</xref>). We then proceed to investigate natural-gradient learning from a biological point of view, deriving a number of predictions that can be experimentally tested, with some of them related to in vivo observations that are otherwise difficult to explain with classical gradient-based learning rules.</p></sec><sec id="s2-3"><title>Natural-gradient plasticity speeds up learning</title><p>Non-isotropic cost landscapes can easily be provoked by non-homogeneous input conditions. In nature, these can arise under a wide range of circumstances, for elementary reasons that boil down to morphology (the position of a synapse along a dendrite can affect the attenuation of its afferent input) and function (different afferents perform different computations and thus behave differently). To evaluate the convergence behavior of our learning rule and compare it to Euclidean-gradient descent, we considered a very generic situation in which a neuron is required to map a diverse set of inputs onto a target output.</p><p>In order to induce a simple and intuitive anisotropy of the error landscape, we divided the afferent population into two equally sized groups of neurons with different firing rates (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, firing rates were chosen as 10 Hz for group one and 50 Hz for group 2). The input spikes were low-pass-filtered with a difference of exponentials (see Sec. ‘Neuron model for details’). This resulted in an asymmetric cost function (KL-divergence between the student and the target firing distribution, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), as visible from the elongated contour lines (<xref ref-type="fig" rid="fig3">Figure 3D and E</xref>). We further chose a realizable teacher by simulating a different neuron with the same input populations connected via a predefined set of target weights <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the weight path plots in <xref ref-type="fig" rid="fig3">Figure 3D and E</xref> fixed target weight <inline-formula><mml:math id="inf99"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>0.3</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.5</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> was chosen, whereas for the learning curve in <xref ref-type="fig" rid="fig3">Figure 3F</xref>, the target weight components were randomly sampled from a uniform distribution on <inline-formula><mml:math id="inf100"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (see Sec. ‘Supervised Learning Task’ for further details). <xref ref-type="fig" rid="fig3">Figure 3B and C</xref> shows that our natural-gradient rule enables the student neuron to adapt its weights to reproduce the teacher voltage <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and thereby its output distribution.</p><p>In the following, we compare learning in two student neurons, one endowed with Euclidean-gradient plasticity (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>, <xref ref-type="fig" rid="fig3">Figure 3D</xref>) and one with our natural-gradient rule (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, <xref ref-type="fig" rid="fig3">Figure 3E</xref>). To better visualize the difference between the two rules, we used a two-dimensional input weight space, that is, one neuron per afferent population. While the negative Euclidean-gradient vectors stand, by definition, perpendicular to the contour lines of <inline-formula><mml:math id="inf102"><mml:mi>C</mml:mi></mml:math></inline-formula>, the negative natural-gradient vectors point directly towards the target weight configuration <inline-formula><mml:math id="inf103"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. Due to the anisotropy of <inline-formula><mml:math id="inf104"><mml:mi>C</mml:mi></mml:math></inline-formula> induced by the different input rates (see also <xref ref-type="fig" rid="fig2">Figure 2B</xref>), Euclidean-gradient learning starts out by mostly adapting the high-rate afferent weight and only gradually begins learning the low-rate afferent. In contrast, natural gradient adapts both synaptic weights homogeneously. This is clearly reflected by paths traced by the synaptic weights during learning.</p><p>Overall, this lead to faster convergence of the natural-gradient plasticity rule compared to Euclidean-gradient descent. In order to enable a meaningful comparison, learning rates were tuned separately for each plasticity rule in order to optimize their respective convergence speed. The faster convergence of natural-gradient plasticity is a robust effect, as evidenced in <xref ref-type="fig" rid="fig3">Figure 3F</xref> by the average learning curves over 1,000 trials.</p><p>In addition to the functional advantages described above, natural-gradient learning also makes some interesting predictions about biology, which we address below.</p></sec><sec id="s2-4"><title>Democratic plasticity</title><p>As discussed in the introduction, classical gradient-based learning rules do not usually account for neuron morphology. Since attenuation of PSPs is equivalent to weight reparametrization and our learning rule is, by construction, parametrization-invariant, it naturally compensates for the distance between synapse and soma. In <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, this is reflected by a component-wise rescaling of the synaptic changes with the inverse of the attenuation function <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is induced by the Fisher information metric (see also <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/66526/figures#fig8">Figure 8</ext-link> and the corresponding section in the Materials and methods). Under the assumption of passive attenuation along the dendritic tree, we have<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>d</italic><sub><italic>i</italic></sub> denotes the distance of the <italic>i</italic>th synapse from the soma. More specifically, <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where λ represents the electrotonic length scale. We can write the natural-gradient rule as<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For functionally equivalent synapses (i.e. with identical input statistics), synaptic changes in distal dendrites are scaled up compared to proximal synapses. As a result, the effect of synaptic plasticity on the neuron’s output is independent of the synapse location, since dendritic attenuation is precisely counterbalanced by weight update amplification.</p><p>We illustrate this effect with simulations of synaptic weight updates at different locations along a dendritic tree in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Such ‘democratic plasticity’, which enables distal synapses to contribute just as effectively to changes in the output as proximal synapses, is reminiscent of the concept of ‘dendritic democracy’ (<xref ref-type="bibr" rid="bib28">Magee and Cook, 2000</xref>). These experiments show increased synaptic amplitudes in the distal dendritic tree of multiple cell types, such as rat hippocampal CA1 neurons; dendritic democracy has therefore been presumed to serve the purpose of giving distal inputs a ‘vote’ on the neuronal output. Still, experiments show highly diverse PSP amplitudes in neuronal somata (<xref ref-type="bibr" rid="bib53">Williams and Stuart, 2002</xref>). Our plasticity rule refines the notion of democracy by asserting that learning itself rather than its end result is rescaled in accordance with the neuronal morphology.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Natural-gradient learning scales synaptic weight updates depending on their distance from the soma.</title><p>We stimulated a single excitatory synapse with Poisson input at 5 Hz, paired with a Poisson teacher spike train at 20 Hz. The distance d from soma was varied between 0 μm and 460 μm and attenuation was assumed to be linear and proportional to the inverse distance from soma. To make weight changes comparable, we scaled dendritic PSP amplitudes with <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>α</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> in order for all of them to produce the same PSP amplitude at the soma. (<bold>A</bold>) Example PSPs before (solid lines) and after (dashed lines) learning for two synapses at 3 μm and 7 μm. Application of our natural-gradient rule results in equal changes for the somatic PSPs. (<bold>B</bold>) Example traces of synaptic weights for the two synapses in (<bold>A</bold>). (<bold>C</bold>) Absolute and relative dendritic amplitude change after 5 s as a function of a synapse’s distance from the soma.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig4-v1.tif"/></fig><p>Whether such democratic plasticity ultimately leads to distal and proximal synapses having the same effective vote at the soma depends on their respective importance towards reaching the target output. In particular, if synapses from multiple afferents that encode the same information are randomly distributed along the dendritic tree, then democratic plasticity also predicts dendritic democracy, as the scaling of weight changes implies a similar scaling of the final learned weights. However, the absence of dendritic democracy does not contradict the presence of democratic plasticity, as afferents from different cortical regions might target specific positions on the dendritic tree (see, e.g., <xref ref-type="bibr" rid="bib30">Markram et al., 2004</xref>). Furthermore, also with democratic plasticity in place, the functional efficacy of synapses at different locations on the dendritic tree will change differently based on different initial conditions. The experimental findings by <xref ref-type="bibr" rid="bib19">Froemke et al., 2005</xref>, who report an inverse relationship between the amount of plasticity at a synapse and its distance from soma, are therefore completely consistent with the predictions of our learning rule, since they compared synapses that were not initially functionally equivalent.</p><p>Note also that dendritic democracy could, in principle, be achieved without democratic plasticity. However, this would be much slower than with natural-gradient learning, especially for distal synapses, as discussed in Sec. ‘Natural-gradient plasticity speeds up learning’.</p></sec><sec id="s2-5"><title>Input and output-specific scaling</title><p>In addition to undoing distortions induced by, for example, attenuation, the natural-gradient rule predicts further modulations of the homosynaptic learning rate. The factor <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> represents an output-dependent global scaling factor (for both homo- and heterosynaptic plasticity):<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Together with the <inline-formula><mml:math id="inf109"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></inline-formula> factor in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, the effective scaling of the learning rule is approximately <inline-formula><mml:math id="inf110"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. In other words, it increases the learning rate in regions where the sigmoidal transfer function is flat (see also Sec. ‘Global scaling factor’). This represents an unmediated reflection of the philosophy of natural-gradient descent, which finds the steepest path for a small change in output, rather than in the numeric value of some parameter. The desired change in the output requires scaling the corresponding input change by the inverse slope of the transfer function.</p><p>Furthermore, synaptic learning rates are inversely correlated to the USP variance <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5</xref>). In particular, for the homosynaptic component, the scaling is exactly equal to<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Natural-gradient learning scales approximately inversely with input variance.</title><p>(<bold>A–C</bold>) Exemplary USPs <inline-formula><mml:math id="inf112"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup></mml:math></inline-formula> and (<bold>D–F</bold>) their distributions for three different scenarios between which the USP variance <inline-formula><mml:math id="inf113"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn mathvariant="normal">2</mml:mn></mml:msup><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is varied. In each scenario, a neuron received a single excitatory input with a given rate <inline-formula><mml:math id="inf114"><mml:mi>r</mml:mi></mml:math></inline-formula> and synaptic time constant <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula>. The soma always received teacher spikes at a rate of 80 Hz. To enable a meaningful comparison, the mean USP was conserved by appropriately rescaling the height <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>ϵ</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> of the USP kernel <inline-formula><mml:math id="inf117"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> (see Sec. ‘Neuron model’). (<bold>A,D</bold>) Reference simulation. (<bold>B,E</bold>) Reduced synaptic time constant, resulting in an increased USP variance <inline-formula><mml:math id="inf118"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn mathvariant="normal">2</mml:mn><mml:mn mathvariant="normal">2</mml:mn></mml:msubsup></mml:math></inline-formula>. (<bold>C,F</bold>) Reduced input rate, resulting in an increased USP variance <inline-formula><mml:math id="inf119"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn mathvariant="normal">3</mml:mn><mml:mn mathvariant="normal">2</mml:mn></mml:msubsup></mml:math></inline-formula>. (<bold>G</bold>) Synaptic weight changes over 5 s for the three scenarios above. (<bold>H</bold>) Total synaptic weight change after <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>5</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of USP variance. Each data point represents a different pair of <inline-formula><mml:math id="inf121"><mml:mi>r</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math></inline-formula>. The three scenarios above are marked with their respective colors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig5-v1.tif"/></fig><p>(see <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> and Sec. ‘Neuron model’). In other words, natural-gradient learning explicitly scales synaptic updates with the (un)reliability – more specifically, with the inverse variance – of their input. To demonstrate this effect in isolation, we simulated the effects of changing the USP variance while conserving its mean. Moreover, to demonstrate its robustness, we independently varied two contributors to the input reliability, namely input rates (which enter <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> directly) and synaptic time constants (which affect the PSP-kernel-dependent scaling constant <inline-formula><mml:math id="inf124"><mml:msub><mml:mi>c</mml:mi><mml:mi>ϵ</mml:mi></mml:msub></mml:math></inline-formula>). <xref ref-type="fig" rid="fig5">Figure 5</xref> shows how unreliable input leads to slower learning, with an inverse dependence of synaptic weight changes on the USP variance. We note that this observation also makes intuitive sense from a Bayesian point of view, under which any information needs to be weighted by the reliability of its source. Furthermore, the approximately inverse scaling with the presynaptic firing rate is qualitatively in line with observations from <xref ref-type="bibr" rid="bib1">Aitchison and Latham, 2014</xref> and <xref ref-type="bibr" rid="bib2">Aitchison et al., 2021</xref>, although our interpretation and precise relationship is different.</p></sec><sec id="s2-6"><title>Interplay of homosynaptic and heterosynaptic plasticity</title><p>One elementary property of update rules based on Euclidean-gradient descent is their presynaptic gating, that is, all weight updates are scaled with their respective synaptic input <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Therefore, they are necessarily restricted to homosynaptic plasticity, as studied in classical LTP and LTD experiments (<xref ref-type="bibr" rid="bib10">Bliss and Lomo, 1973</xref>; <xref ref-type="bibr" rid="bib15">Dudek and Bear, 1992</xref>). As discussed above, natural-gradient learning retains a rescaled version of this homosynaptic contribution, but at the same time predicts the presence of two additional plasticity components. Contrary to homosynaptic plasticity, these components also adapt synapses to currently non-active afferents, given a sufficient level of global input. Due to their lack of input specificity, they give rise to heterosynaptic weight changes, a form of plasticity that has been observed in hippocampus (<xref ref-type="bibr" rid="bib12">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Lynch et al., 1977</xref>), cerebellum (<xref ref-type="bibr" rid="bib22">Ito and Kano, 1982</xref>), and neocortex (<xref ref-type="bibr" rid="bib13">Chistiakova and Volgushev, 2009</xref>), mostly in combination with homosynaptic plasticity. A functional interpretation of heterosynaptic plasticity, to which our learning rule also alludes, is as a prospective adaptation mechanism for temporarily inactive synapses such that, upon activation, they are already useful for the neuronal output.</p><p>Our natural-gradient learning rule <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> can be more summarily rewritten as<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the three additive terms represent the variance-normalized homosynaptic plasticity, the uniform heterosynaptic plasticity and the weight-dependent heterosynaptic plasticity:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mfrac><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>∝</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with the common proportionality factor <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> composed of the learning rate, the output-dependent global scaling factor, the postsynaptic error, a sensitivity factor and the inverse attenuation function, in order of their appearance. The effect of these three components is visualized in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. The homosynaptic term <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is experienced only by stimulated synapses, while the two heterosynaptic terms act on all synapses. The first heterosynaptic term <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> introduces a uniform adjustment to all components by the same amount, depending on the global activity level. For a large number of presynaptic inputs, it can be approximated by a constant (see Sec. ‘Empirical analysis of <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’). Furthermore, it usually opposes the homosynaptic change, which we address in more detail below.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Natural-gradient learning combines multiple forms of plasticity.</title><p>Spike trains to the left of the neuron represent afferent inputs to two of the synapses and teacher input to the soma. The two synapses on the right of the dendritic tree receive no stimulus. The teacher is assumed to induce a positive error. (<bold>A</bold>) The homosynaptic component adapts all stimulated synapses, leaving all unstimulated synapses untouched. (<bold>B</bold>) The uniform heterosynaptic component changes all synapses in the same manner, only depending on global activity levels. (<bold>C</bold>) The proportional heterosynaptic component contributes a weight change that is proportional to the current synaptic strength. The magnitude of this weight change is approximately proportional to a product of the current membrane potential above baseline and the weight vector.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig6-v1.tif"/></fig><p>In contrast, the contribution of the second heterosynaptic term <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is weight-dependent, adapting all synapses in proportion to their current strength. This corresponds to experimental reports such as <xref ref-type="bibr" rid="bib26">Loewenstein et al., 2011</xref>, which found in vivo weight changes in the neocortex to be proportional to the spine size, which itself is correlated with synaptic strength (<xref ref-type="bibr" rid="bib8">Asrican et al., 2007</xref>). Our simulations in Sec. ‘Empirical Analysis of <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’ show that <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is roughly a linear function of the membrane potential (more specifically, its deviation with respect to its baseline), which is reflected by the last approximation in <xref ref-type="disp-formula" rid="equ21">Equation 21</xref>. Since the latter can be interpreted as a scalar product between the afferent input vector and the synaptic weight vector, it implies that spikes transmitted by strong synapses have a larger impact on this heterosynaptic plasticity component compared to spikes arriving via weak synapses. Thus, weaker synapses require more persistent and strong stimulation to induce significant changes and ‘override the status quo’ of the neuron. Since, following a period of learning, afferents connected via weak synapses can be considered uninformative for the neuron’s target output, this mechanism ensures a form of heterosynaptic robustness towards noise.</p><p>The homo- and heterosynaptic terms exhibit an interesting relationship. To illustrate the nature of their interplay, we simulated a simple experiment (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) with varying initial synaptic weights for both active and inactive presynaptic afferents. Stimulated synapses (<xref ref-type="fig" rid="fig7">Figure 7B</xref>) are seen to undergo strong potentiation (LTP) for very small initial weights; the magnitude of weight changes decreases for larger initial amplitudes until the neuron’s output matches its teacher, at which point the sign of the postsynaptic error term flips. For even larger initial weights, potentiation at stimulated synapses therefore turns into depression (LTD), which becoms stronger for higher initial values of the stimulated synapses’ weights. This is in line with the error learning paradigm, in which changes in synaptic weights seek to reduce the difference between a neuron’s target and its output.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Interplay of homo- and heterosynaptic plasticity in natural-gradient learning.</title><p>(<bold>A</bold>) Simulation setup. Five out of 10 inputs received excitatory Poisson input at 5 Hz. In addition, we assumed the presence of tonic inhibition as a balancing mechanism for keeping the neuron’s output within a reasonable regime. Afferent stimulus was paired with teacher spike trains at 20 Hz and plasticity at both stimulated and unstimulated synapses was evaluated in comparison with their initial weights. For simplicity, initial weights within each group were assumed to be equal. (<bold>B</bold>) Weight change of stimulated weights (both homo- and heterosynaptic plasticity are present). These weight changes are independent of unstimulated weights. Equilibrium (dashed black line) is reached when the neuron’s output matches its teacher and the error vanishes. For increasing stimulated weights, potentiation switches to depression at the equilibrium line. (<bold>C</bold>) Weight change of unstimulated weights (only heterosynaptic plasticity is present). For very high activity caused by very large synaptic weights, heterosynaptic plasticity always causes synaptic depression. Otherwise, plasticity at unstimulated synapses behaves exactly opposite to plasticity at stimulated synapses. Increasing the size of initial stimulated weights results in a change from depression to potentiation at the same point where potentiation turns into depression at stimulated synapses. (<bold>D</bold>) Direct comparison of plasticity at stimulated and unstimulated synapses. The light green area (<bold>O1, O2</bold>) represents opposing signs, dark green (<bold>S</bold>) represents the same sign (more specifically, depression). Their shared equilibrium is marked by the dashed green line and represents the switch from positive to negative error. (<bold>E–G</bold>) Relative weight changes of synaptic weights for stimulated and unstimulated synapses during learning, with initial weights picked from the different regimes indicated by the crosses in (<bold>B, C, D</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig7-v1.tif"/></fig><p>For unstimulated synapses (<xref ref-type="fig" rid="fig7">Figure 7C</xref>), we observe a reversed behavior. For small weights, the negative uniform term <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> dominates and plasticity is depressing. As for the homosynaptic case, the sign of plasticity switches when the weights become large enough for the error to switch sign. Therefore, in the regime where stimulated synapses experienced potentiation, unstimulated synapses are depressed and vice-versa. This reproduces various experimental observations: on one hand, potentiation of stimulated synapses has often been found to be accompanied by depression of unstimulated synapses (<xref ref-type="bibr" rid="bib27">Lynch et al., 1977</xref>), such as in the amygdala (<xref ref-type="bibr" rid="bib41">Royer and Paré, 2003</xref>) or the visual cortex (<xref ref-type="bibr" rid="bib7">Arami et al., 2013</xref>); on the other hand, when the postsynaptic error term switches sign, depression at unstimulated synapses transforms into potentiation (<xref ref-type="bibr" rid="bib54">Wöhrl et al., 2007</xref>; <xref ref-type="bibr" rid="bib41">Royer and Paré, 2003</xref>).</p><p>While plasticity at stimulated synapses is unaffected by the initial state of the unstimulated synapses, plasticity at unstimulated synaptic connections depends on both the stimulated and unstimulated weights. In particular, when either of these grow large enough, the proportional term <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> overtakes the uniform term <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:msub><mml:mi mathvariant="normal">t</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and heterosynaptic plasticity switches sign again. Thus, for very large weights (top right corner of <xref ref-type="fig" rid="fig7">Figure 7C</xref>), heterosynaptic potentiation transforms back into depression, in order to more quickly quench excessive output activity. This behavior is useful for both supervised and unsupervised learning scenarios (<xref ref-type="bibr" rid="bib56">Zenke and Gerstner, 2017</xref>), where it was shown that pairing Hebbian terms with heterosynaptic and homeostatic plasticity is crucial for stability.</p><p>In summary, we can distinguish three plasticity regimes for natural-gradient learning (<xref ref-type="fig" rid="fig7">Figure 7D–G</xref>). In two of these regimes, heterosynaptic and homosynaptic plasticity are opposed (O1, O2), whereas in the third, they are aligned and lead to depression (S). The two opposing regimes are separated by the zero-error equilibrium line, at which plasticity switches sign.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>As a consequence of the fundamentally stochastic nature of evolution, it is no surprise that biology withstands confinement to strict laws. Still, physics-inspired arguments from symmetry and invariance can help uncover abstract principles that evolution may have gradually discovered and implemented into our brains. Here, we have considered parametrization invariance in the context of learning, which, in biological terms, translates to the fundamental ability of neurons to deal with diversity in their morphology and input-output characteristics. This requirement ultimately leads to various forms of scaling and heterosynaptic plasticity that are experimentally well-documented, but can not be accounted for by classical paradigms that regard plasticity as Euclidean-gradient descent. In turn, these biological phenomena can now be seen as a means to jointly improve and accelerate error-correcting learning.</p><p>Inspired by insights from information geometry, we applied the framework of natural gradient descent to biologically realistic neurons with extended morphology and spiking output. Compared to classical error-correcting learning rules, our plasticity paradigm requires the presence of several additional ingredients. First, a global factor adapts the learning rate to the particular shape of the voltage-to-spike transfer function and to the desired statistics of the output, thus addressing the diversity of neuronal response functions observed in vivo (<xref ref-type="bibr" rid="bib30">Markram et al., 2004</xref>). Second, the homosynaptic component of plasticity is normalized by the variance of presynaptic inputs, which provides a direct link to Bayesian frameworks of neuronal computation (<xref ref-type="bibr" rid="bib1">Aitchison and Latham, 2014</xref>; <xref ref-type="bibr" rid="bib23">Jordan et al., 2020</xref>). Third, our rule contains a uniform heterosynaptic term that opposes homosynaptic changes, downregulating plasticity and thus acting as a homeostatic mechanism (<xref ref-type="bibr" rid="bib12">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Chistiakova et al., 2015</xref>). Fourth, we find a weight-dependent heterosynaptic term that also accounts for the shape of the neuron’s activation function, while increasing its robustness towards noise. Finally, our natural-gradient-based plasticity correctly accounts for the somato-dendritic reparametrization of synaptic strengths.</p><p>These features enable faster convergence on non-isotropic error landscapes, in line with results for multilayer perceptrons (<xref ref-type="bibr" rid="bib55">Yang and Amari, 1998</xref>; <xref ref-type="bibr" rid="bib39">Rattray and Saad, 1999</xref>) and rate-based deep neural networks (<xref ref-type="bibr" rid="bib35">Pascanu and Bengio, 2013</xref>; <xref ref-type="bibr" rid="bib33">Ollivier, 2015</xref>; <xref ref-type="bibr" rid="bib9">Bernacchia et al., 2018</xref>). Importantly, our learning rule can be formulated as a simple, fully local expression, only requiring information that is available at the locus of plasticity.</p><p>We further note an interesting property of our learning rule, which it inherits directly from the Fisher information metric that underlies natural gradient descent, namely invariance under sufficient statistics (<xref ref-type="bibr" rid="bib11">Cencov, 1972</xref>). This is especially relevant for biological neurons, whose stochastic firing effectively communicates information samples rather than explicit distributions. Thus, downstream computation is likely to require a reliable sample-based, that is, statistically sufficient, estimation of the afferent distribution’s parameters, such as the sample mean and variance. This singles out our natural-gradient approach from other second-order-like methods as a particularly appealing framework for biological learning.</p><p>Many of the biological phenomena predicted by our invariant learning rule are reflected in existing experimental results. Our democratic plasticity can give rise to dendritic democracy, as observed by <xref ref-type="bibr" rid="bib28">Magee and Cook, 2000</xref>. Moreover, it also relates to results by <xref ref-type="bibr" rid="bib25">Letzkus et al., 2006</xref> and <xref ref-type="bibr" rid="bib46">Sjöström and Häusser, 2006</xref> which describe a sign switch of synaptic plasticity between proximal and distal sites that is not easily reconcilable with naive gradient descent. As the authors themselves speculate, the most likely reason is the (partial) failure of action potential backpropagation through the dendritic tree. In some sense, this is a mirror problem to the issue we discuss here: while we consider the attenuation of input signals, these experimental findings could be explained by an attenuation of the output signal. A simple way of incorporating this aspect into our learning rule (but also into Euclidean rules) is to apply a (nonlinear) attenuation function to the teacher signal <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ10 equ13">Equations 10 and 13</xref>). At a certain distance from the soma, this attenuation would become strong enough to switch the sign of the error term <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and thereby also, for example, LTP to LTD. However, addressing this at the normative level of natural gradient descent as opposed to a direct tweak of the learning rule is less straightforward.</p><p>Our rule also requires heterosynaptic plasticity, which has been observed in neocortex, as well as in deeper brain regions such as amygdala and hippocampus (<xref ref-type="bibr" rid="bib27">Lynch et al., 1977</xref>; <xref ref-type="bibr" rid="bib17">Engert and Bonhoeffer, 1997</xref>; <xref ref-type="bibr" rid="bib52">White et al., 1990</xref>; <xref ref-type="bibr" rid="bib41">Royer and Paré, 2003</xref>; <xref ref-type="bibr" rid="bib54">Wöhrl et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Chistiakova and Volgushev, 2009</xref>; <xref ref-type="bibr" rid="bib7">Arami et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Chistiakova et al., 2015</xref>), often in combination with homosynaptic weight changes. Moreover, we find that heterosynaptic plasticity generally opposes homosynaptic plasticity, which qualitatively matches many experimental findings (<xref ref-type="bibr" rid="bib27">Lynch et al., 1977</xref>; <xref ref-type="bibr" rid="bib52">White et al., 1990</xref>; <xref ref-type="bibr" rid="bib41">Royer and Paré, 2003</xref>; <xref ref-type="bibr" rid="bib54">Wöhrl et al., 2007</xref>) and can be functionally interpreted as an enhancement of competition. For very large weights, heterosynaptic plasticity aligns with homosynaptic changes, pushing the synaptic weights back to a sensible range (<xref ref-type="bibr" rid="bib14">Chistiakova et al., 2015</xref>), as shown to be necessary for unsupervised learning (<xref ref-type="bibr" rid="bib56">Zenke and Gerstner, 2017</xref>). In supervised learning it helps speed up convergence by keeping the weights in the operating range.</p><p>These qualitative matches of experimental data to the predictions of our plasticity rule provide a well-defined foundation for future, more targeted experiments that will allow a quantitative exploration of the relationship between heterosynaptic plasticity and natural gradient learning. In contrast to the mostly deterministic protocols used in the referenced literature, such experiments would require a Poisson-like, stochastic stimulation of the student neuron. To further facilitate a quantitative analysis, the experimental setup should, in particular, allow a clear separation between student and teacher, for example via a pairing protocol (see e.g. <xref ref-type="bibr" rid="bib41">Royer and Paré, 2003</xref>, but note that their protocol was deterministic).</p><p>A further prediction that follows from our plasticity rule is the normalization of weight changes by the presynaptic variance. We would thus anticipate that increasing the variance in presynaptic spike trains (through, e.g. temporal correlations) should reduce LTP in standard plasticity induction protocols. Also, we expect to observe a significant dependence of synaptic plasticity on neuronal response functions and output statistics. For example, flatter response functions should correlate with faster learning, in contrast to the inverse correlation predicted by classical learning rules derived from Euclidean-gradient descent. These propositions remain to be tested experimentally.</p><p>By following gradients with respect to the neuronal output rather than the synaptic weights themselves, we were able to derive a parametrization-invariant error-correcting plasticity rule on the single-neuron level. Error-correcting learning rules are an important ingredient in understanding biological forms of error backpropagation (<xref ref-type="bibr" rid="bib44">Sacramento et al., 2017</xref>). In principle, our learning rule can be directly incorporated as a building block into spike-based frameworks of error backpropagation such as (<xref ref-type="bibr" rid="bib48">Sporea and Grüning, 2013</xref>; <xref ref-type="bibr" rid="bib45">Schiess et al., 2016</xref>). Based on these models, top-down feedback can provide a target for the somatic spiking of individual neurons, toward which our learning rule could be used to speed up convergence.</p><p>Explicitly and exactly applying natural gradient at the network level does not appear biologically feasible due to the existence of cross-unit terms in the Fisher information matrix <inline-formula><mml:math id="inf140"><mml:mi>G</mml:mi></mml:math></inline-formula>. These terms introduce non-localities in the natural-gradient learning rule, in the sense that synaptic changes at one neuron depend on the state of other neurons in the network. However, methods such as the unit-wise natural-gradient approach (<xref ref-type="bibr" rid="bib33">Ollivier, 2015</xref>) could be employed to approximate the natural gradient using a block-diagonal form of <inline-formula><mml:math id="inf141"><mml:mi>G</mml:mi></mml:math></inline-formula>. The resulting learning rule would not exhibit biologically implausible co-dependencies between different neurons in the network, while still retaining many of the advantages of full natural-gradient learning. For spiking networks, this would reduce global natural-gradient descent to our local rule for single neurons.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Neuron model</title><p>We chose a Poisson neuron model whose firing rate depends on the somatic membrane potential above the resting potential <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>rest</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>70</mml:mn><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. They relate via a sigmoidal activation function<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a maximal firing rate <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This means the activation function is centered at −60 mV, saturating around −50 mV. Note that the derivation of our learning rule does not depend on the explicit choice of activation function, but holds for arbitrary monotonically increasing, positive functions that are sufficiently smooth. For the sake of simplicity, refractoriness was neglected. For the same reason, we assumed synaptic input to be current-based, such that incoming spikes elicit a somatic membrane potential above baseline given by<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mpadded width="+2.8pt"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>denotes the unweighted synaptic potential (USP) evoked by a spike train<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:msubsup></mml:munder><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>of afferent <italic>i</italic>, and <italic>w</italic><sub><italic>i</italic></sub> is the corresponding synaptic weight. Here, <inline-formula><mml:math id="inf145"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:msubsup></mml:math></inline-formula> denote the firing times of afferent <italic>i</italic>, and the synaptic response kernel <inline-formula><mml:math id="inf146"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> is modeled as<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf147"><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is a scaling factor with units <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi/><mml:mtext/><mml:mrow><mml:mi class="ltx_unit">mV</mml:mi><mml:mtext/><mml:mi class="ltx_unit">ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and unless specified otherwise, we chose <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mtext/><mml:mrow><mml:mi class="ltx_unit">mV</mml:mi><mml:mtext/><mml:mi class="ltx_unit">ms</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For slowly changing input rates, mean and variance of the stationary unweighted synaptic potential are then given as (<xref ref-type="bibr" rid="bib36">Petrovici, 2016</xref>)<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ29 equ30">Equations 29 and 30</xref> are exact for constant input rates. Note that it is <xref ref-type="disp-formula" rid="equ30">Equation 30</xref> that induces the inverse dependence of the homosynaptic term on the input rate discussed around <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>.</p><p>Unless indicated otherwise, simulations were performed with a membrane time constant <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mtext/><mml:mi class="ltx_unit">ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and a synaptic time constant <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mtext/><mml:mi class="ltx_unit">ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Hence, USPs had an amplitude of 60 mV and were normalized with respect to area under the curve and multiplied by the synaptic weights. Initial and target weights were chosen such that the resulting average membrane potential was within operating range of the activation function. As an example, in <xref ref-type="fig" rid="fig3">Figure 3F</xref>, the average initial excitatory weight was 0.005, corresponding to an EPSP amplitude of 300V.</p><p>In <xref ref-type="fig" rid="fig5">Figure 5</xref>, the scaling factor <inline-formula><mml:math id="inf152"><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> was additionally normalized proportionally to the input rate <italic>r</italic><sub><italic>i</italic></sub> at the synapse in order to keep the mean USP constant and allow a comparison based solely on the variance.</p></sec><sec id="s4-2"><title>Sketch for the derivation of the somatic natural-gradient learning rule</title><p>The choice of a Poisson neuron model implies that spiking in a small interval <inline-formula><mml:math id="inf153"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is governed by a Poisson distribution. For sufficiently small interval lengths <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, the probability of having a single spike in <inline-formula><mml:math id="inf155"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> becomes Bernoulli with parameter <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. The aim of supervised learning is to bring this distribution closer to a given target distribution with density <inline-formula><mml:math id="inf157"><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. The latter is delivered in form of a teacher spike train<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">f</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">f</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the probability of having a teacher spike (denoted as <inline-formula><mml:math id="inf158"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) in <inline-formula><mml:math id="inf159"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is Bernoulli with parameter<disp-formula id="equ33">: <label>(33)</label><mml:math id="m33"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that to facilitate the convergence analysis, in <xref ref-type="fig" rid="fig3">Figure 3</xref> we chose a realizable teacher <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> generated by a given set of teacher weights <inline-formula><mml:math id="inf161"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>.</p><p>We measure the error between the desired and the current input-output spike distribution in terms of the Kullback-Leibler divergence given in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, which attains its single minimum when the two distributions are equal. Note that while the <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>D</mml:mi><mml:mi>KL</mml:mi></mml:msub></mml:math></inline-formula> is a standard measure to characterize ‘how far’ two distributions are apart, its behavior can sometimes be slightly unintuitive since it is not a metric. In particular, it is not symmetric and does not satisfy the triangle inequality.</p><p>Classical error learning follows the Euclidean gradient of this cost function, given as the vector of partial derivatives with respect to the synaptic weights. A short calculation (Sec. ‘Detailed derivation of the natural-gradient learning rule’) shows that the resulting Euclidean-gradient-descent learning rule is given by <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>. By correcting the vector of partial derivatives for the distance distortion between the manifold of input-output distributions and the synaptic weight space, given in terms of the Fisher information matrix <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we obtain the natural gradient (<xref ref-type="disp-formula" rid="equ12">Equation 12</xref>). We then followed an approach by <xref ref-type="bibr" rid="bib4">Amari, 1998</xref> to derive an explicit formula for the product on the right hand side of <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>.</p><p>In Sec. ‘Detailed derivation of the natural-gradient learning rule’, we show that given independent input spike trains, the Fisher information matrix defined in <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> can be decomposed with respect to the vector of input rates <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the current weight vector <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here,<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi mathvariant="normal">c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>is the covariance matrix of the unweighted synaptic potentials, and <inline-formula><mml:math id="inf166"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are coefficients (see <xref ref-type="disp-formula" rid="equ82">Equation 82</xref> for their definition) depending on the mean <inline-formula><mml:math id="inf167"><mml:msub><mml:mi>μ</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf168"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the membrane potential, and on the total rate<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>ϵ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Through repeated application of the Sherman-Morrison-Formula, the inverse of <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained as<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here the coefficients <inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which are defined in <xref ref-type="disp-formula" rid="equ103">Equation 102</xref>, are again functions of mean and variance of the membrane potential, and of the total rate. Consequently, the natural-gradient rule in terms of somatic amplitudes is given by<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that the formulas for,<disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mtext> and </mml:mtext><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/></mml:mrow></mml:math></disp-formula></p><p>arise from the product of the inverse Fisher information matrix and the Euclidean gradient, using <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Due to the complicated expressions for <inline-formula><mml:math id="inf173"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ82">Equation 82</xref>), <xref ref-type="disp-formula" rid="equ39">Equation 39</xref> only provides limited information about the behavior of <inline-formula><mml:math id="inf174"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula>. Therefore, we performed an empirical analysis based on simulation data (Sec. ‘Empirical analysis of <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’; <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/66526/figures#fig11">Figure 11</ext-link>). In a stepwise manner we first evaluated <inline-formula><mml:math id="inf178"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> under various conditions, which revealed that the products with <italic>g</italic><sub>2</sub> and <italic>g</italic><sub>3</sub> in <xref ref-type="disp-formula" rid="equ39">Equation 39</xref> are neglible in most cases compared to the other terms, hence<disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mtext> and </mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Furthermore, for a sufficient number of input afferents, we can approximate <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, by the central limit theorem we have<disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>for large <inline-formula><mml:math id="inf181"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Moreover, while the variance of <italic>g</italic><sub>4</sub> across weight samples increases with the number and firing rate of input afferents, its mean stays approximately constant across conditions. This lead to the approximation<disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> is constants across weights, input rates and the number of input afferents.</p><p>To evaluate the quality of our approximations, we tested the performance of learning in the setting of <xref ref-type="fig" rid="fig3">Figure 3</xref> when <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf185"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> were replaced by their approximations (<xref ref-type="disp-formula" rid="equ116 equ117">Equations 112; 113</xref>). The test was performed for several input patterns (Sec. ‘Evaluation of the approximated natural-gradient rule’, Sec. ‘Performance of the approximated learning rule’). It turned out that a convergence behavior very similar to natural-gradient descent could be achieved with <inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula>, which worked much better in practice than <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. For <inline-formula><mml:math id="inf188"><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> a choice of 0.05 which was close to the mean of <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> worked well.</p><p>For these input rate configurations and choices of constants, we additionally sampled the negative gradient vectors for random initial weights and USPs (Sec. ‘Evaluation of the approximated natural-gradient rule’, Sec. ‘Performance of the approximated learning rule’) and compared the angles and length difference between natural-gradient vectors and the approximation to the ones between natural and Euclidean gradient.</p></sec><sec id="s4-3"><title>Reparametrization and the general natural-gradient rule</title><p>To arrive at a more general form of the natural-gradient learning rule, we consider a parametrization <inline-formula><mml:math id="inf190"><mml:mi mathvariant="bold-italic">w</mml:mi></mml:math></inline-formula> of the synaptic weights which is connected to the somatic amplitudes via a smooth component-wise coordinate change <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. A Taylor expansion shows that small weight changes then relate via the derivative of<inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>On the other hand, we can also express the cost function in terms of <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and directly calculate the Euclidean gradient of <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in terms of <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. By the chain rule, we then have<disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Plugging this into <xref ref-type="disp-formula" rid="equ43">Equation 43</xref>, we obtain an inconsistency: <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Hence the predictions of Euclidean-gradient learning depend on our choice of synaptic weight parametrization (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>In order to obtain the natural-gradient learning rule in terms of <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we first express the Fisher Information Matrix in the new parametrization, starting with <xref ref-type="disp-formula" rid="equ60">Equation 60</xref><disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Inserting both <xref ref-type="disp-formula" rid="equ44">Equation 44</xref> and <xref ref-type="disp-formula" rid="equ47">Equation 47</xref> into <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, we obtain the natural-gradient rule in terms of <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>). As illustrated in <xref ref-type="fig" rid="fig8">Figure 8</xref>, unlike for Euclidean-gradient descent, the result is consistent with <xref ref-type="disp-formula" rid="equ43">Equation 43</xref>.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Natural-gradient descent does not depend on chosen parametrization.</title><p>Mathematical derivation and phenomenological correlates. EPSPs before learning are represented as continuous, after learning as dashed curves. The light blue arrow represents gradient descent on the error as a function of the somatic EPSP <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (also shown in light blue). The resulting weight change leads to an increase <inline-formula><mml:math id="inf202"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> in the somatic EPSP after learning. The dark blue arrows track the calculation of the same gradient, but with respect to the dendritic EPSP (also shown in dark blue): (1) taking the attenuation into account in order to compute the error as a function of <inline-formula><mml:math id="inf203"><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msup></mml:math></inline-formula>, (2) calculating the gradient, followed by (3) deriving the associated change in <inline-formula><mml:math id="inf204"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mo mathvariant="normal">⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, again considering attenuation. (<bold>A</bold>) For Euclidean-gradient descent. (<bold>B</bold>) For natural-gradient descent. Unlike for Euclidean-gradient descent, the factor <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is compensated, since its inverse enters via the Fisher information. This leads to the synaptic weights updates, as well as the associated evolution of a neuron’s output statistics over time, being equal under the two parametrizations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig8-v1.tif"/></fig></sec><sec id="s4-4"><title>Simulation details</title><p>All simulations were performed in python and used the numpy and scipy packages. Differential equations were integrated using a forward Euler method with a time step of 0.5 ms.</p><sec id="s4-4-1"><title>Supervised learning task</title><p>A single output neuron was trained to spike according to a given target distribution in response to incoming spike trains from n independently firing afferents. To create an asymmetry in the input, we chose one half of the afferents’ firing rates as 10 Hz, while the remaining afferents fired at 50 Hz. The supervision signal consisted of spike trains from a teacher that received the same input spikes. To allow an easy interpretation of the results, we chose a realizable teacher, firing with rate <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mo>∗</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for some optimal set of weights <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, our theory itself does not include assumptions about the origin and exact form of the teacher spike train.</p><p>For the learning curves in <xref ref-type="fig" rid="fig3">Figure 3F</xref>, initial and target weight components were chosen randomly (but identically for the two curves) from a uniform distribution on <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, corresponding to maximal PSP amplitudes between −600 μV and 600 μV for the simulation with <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> input neurons. Learning curves were averaged over 1000 initial and target weight configurations. In addition, the minimum and maximum values are shown in <xref ref-type="fig" rid="fig9">Figure 9</xref>, as well as the mean Euclidean distance between student and teacher weights and between student and teacher firing rates. We did not enforce Dales’ law, thus about half of the synaptic input was inhibitory at the beginning but sign changes were permitted. This means that the mean membrane potential above rest covered a maximal range [−30 mV, 30 mV]. Learning rates were optimized as <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn><mml:mo>∗</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the natural-gradient-descent algorithm and <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4.5</mml:mn><mml:mo>∗</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for Euclidean-gradient descent, providing the fastest possible convergence to a residual root mean squared error in output rates of 0.8 Hz. To confirm that the convergence behavior of both natural-gradient and Euclidean-gradient learning is robust, we varied the learning rate between <inline-formula><mml:math id="inf213"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf214"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and measured the time until the <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> first reached a value of <inline-formula><mml:math id="inf216"><mml:mrow><mml:mn>5</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Here <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for Euclidean-gradient descent and <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for natural-gradient descent.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Further convergence analysis of natural-gradient-descent learning.</title><p>Unless stated otherwise, all simulation parameters are the same as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>A</bold>) In addition to the average learning curves from <xref ref-type="fig" rid="fig3">Figure 3F</xref> (solid lines), we show the minimum and maximum values (semi-transparent lines) during learning. (<bold>B</bold>) Plot of the mean Euclidean distance between student and teacher weight. Note that a smaller distance in weights does not imply a smaller <inline-formula><mml:math id="inf219"><mml:msub><mml:mi>D</mml:mi><mml:mi mathvariant="normal">KL</mml:mi></mml:msub></mml:math></inline-formula>, nor a smaller distance in firing rates. This is due to the non-linear relationship between weights and firing rates. (<bold>C</bold>) Development of mean Euclidean distance between student and teacher firing rate during learning. (<bold>D</bold>) Robustness of learning against perturbations of the firing rate. We varied the learning rate for natural-gradient and Euclidean-gradient descent relative to the learning rate <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>η</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> used in the simulations for <xref ref-type="fig" rid="fig3">Figure 3F</xref> (EGD: <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4.5</mml:mn><mml:mo>∗</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, NGD: <inline-formula><mml:math id="inf222"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mn mathvariant="normal">6</mml:mn><mml:mo mathvariant="normal">*</mml:mo><mml:msup><mml:mn mathvariant="normal">10</mml:mn><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), and measured the time until the <inline-formula><mml:math id="inf223"><mml:msub><mml:mi>D</mml:mi><mml:mi mathvariant="normal">KL</mml:mi></mml:msub></mml:math></inline-formula> first reached a value of <inline-formula><mml:math id="inf224"><mml:mrow><mml:mn mathvariant="normal">5</mml:mn><mml:mo mathvariant="normal">*</mml:mo><mml:msup><mml:mn mathvariant="normal">10</mml:mn><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig9-v1.tif"/></fig><p>Per trial, the expectation over USPs in the cost function was evaluated on a randomly sampled test set of 50 USPs that resulted from input spike trains of 250 ms. The expectation over output spikes was calculated analytically.</p><p>For the weight path simulation with two neurons (<xref ref-type="fig" rid="fig3">Figure 3D–E</xref>), we chose a fixed initial weight <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>0.3</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.5</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, a fixed target weight <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>0.15</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.15</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and learning rates <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn><mml:mo>∗</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>7.</mml:mn><mml:mo>∗</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Weight paths were averaged over 500 trials of 6000s duration each.</p><p>The vector plots in <xref ref-type="fig" rid="fig3">Figure 3D–E</xref> display the average negative normalized natural and Euclidean-gradient vectors across 2000 USP samples per synapse (<inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>) on a grid of weight positions on <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.4</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.6</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with the first coordinate of the gridpoints in <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.28</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.08</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.26</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.44</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the second in <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.22</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.02</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.26</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each USP sample was the result of a <inline-formula><mml:math id="inf233"><mml:mrow><mml:mn>1</mml:mn><mml:mtext/><mml:mi class="ltx_unit" mathvariant="normal">s</mml:mi></mml:mrow></mml:math></inline-formula> spike train at rate <inline-formula><mml:math id="inf234"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mtext/><mml:mi class="ltx_unit">Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>50</mml:mn><mml:mtext/><mml:mi class="ltx_unit">Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> respectively. The contour lines were obtained from 2000 samples of the <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>D</mml:mi><mml:mi>KL</mml:mi></mml:msub></mml:math></inline-formula> along a grid on <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.4</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.6</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (distance between two grid points in one dimension: <inline-formula><mml:math id="inf238"><mml:mfrac><mml:mn>0.006</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:math></inline-formula>) and displayed at the levels 0.001, 0.003, 0.005, 0.009, 0.015, 0.02, 0.03, 0.04.</p><p>For the plots in <xref ref-type="fig" rid="fig3">Figure 3B–C</xref>, we used initial, final, and target weights from a sample of the learning curve simulation. We then randomly sampled input spike trains of 250 ms length and calculated the resulting USPs and voltages according to <xref ref-type="disp-formula" rid="equ26">Equation 26</xref> and <xref ref-type="disp-formula" rid="equ25">Equation 25</xref>. The output spikes shown in the raster plot were then sampled from a discretized Poisson process with <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>.</mml:mo><mml:mo>*</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We then calculated the PSTH with a bin size of 12.5 ms.</p></sec><sec id="s4-4-2"><title>Distance dependence of amplitude changes</title><p>A single excitatory synapse received Poisson spikes at 5 Hz, paired with Poisson teacher spikes at 20 Hz. The distance from the soma was varied between 0 μm and 460 μm. Learning was switched on for 5 s with an initial weight corresponding to 0.05 at the soma, corresponding to a PSP amplitude of 3 mV. Initial dendritic weights were scaled up with the proportionality factor <inline-formula><mml:math id="inf240"><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> depending on the distance from the soma, in order for input spikes to result in the same somatic amplitude independent of the synaptic position. Example traces are shown for <inline-formula><mml:math id="inf241"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf242"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-4-3"><title>Variance dependence of amplitude changes</title><p>We stimulated a single excitatory synapse with Poisson spikes, while at the same time providing Poisson teacher spike trains at 80 Hz. To change USP variance independently from mean, unlike in the other exercises, the input kernel in <xref ref-type="disp-formula" rid="equ28">Equation 28</xref> was additionally normalized by the input rate. USP variance was varied by either keeping the input rate at 10 Hz while varying the synaptic time constant <inline-formula><mml:math id="inf243"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> between 1 ms and 20 ms, or fixing <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> at 20 ms and varying the input rate between 10 Hz and 50 Hz.</p></sec></sec><sec id="s4-5"><title>Comparison of homo- and heterosynaptic plasticity</title><p>Out of <inline-formula><mml:math id="inf245"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> excitatory synapses of a neuron, we stimulated 5 by Poisson spike trains at 5 Hz, together with teacher spikes at 20 Hz, and measured weight changes after 60 s of learning. To avoid singularities in the homosynaptic term of learning rule, we assumed in the learning rule that unstimulated synapses received input at some infinitesimal rate, thus effectively setting <inline-formula><mml:math id="inf246"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi>hom</mml:mi></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Initial weights for both unstimulated and stimulated synapses were varied between <inline-formula><mml:math id="inf247"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mfrac><mml:mn>5</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:math></inline-formula>. For reasons of simplicity, all stimulated weights were assumed to be equal, and tonic inhibition was assumed by a constant shift in baseline membrane potential of -5 mV. Example weight traces are shown for initial weights of <inline-formula><mml:math id="inf249"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1.6</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>2.5</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf250"><mml:mfrac><mml:mn>4.5</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:math></inline-formula> for both stimulated and unstimulated weights. The learning rate was chosen as <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>For the plots in <xref ref-type="fig" rid="fig7">Figure 7B C</xref>, we chose the diverging colormap matplotlib.cm.seismic in matplotlib. The colormap was inverted such that red indicates negative values representing synaptic depression and blue indicates positive values representing synaptic potentiation. The colormap was linearly discretized into 500 steps. To avoid too bright regions where colors cannot be clearly distinguished, the indices 243–257 were excluded. The colorbar range was manually set to a symmetric range that includes the max/min values of the data. In <xref ref-type="fig" rid="fig7">Figure 7D</xref>, we only distinguish between regions where plasticity at stimulated and at unstimulated synapses have opposite signs (light green), and regions where they have the same sign (dark green).</p><sec id="s4-5-1"><title>Approximation of learning-rule coefficents</title><p>We sampled the values for <inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ82">Equation 82</xref> for different afferent input rates. The input rate <inline-formula><mml:math id="inf253"><mml:mi>r</mml:mi></mml:math></inline-formula> was varied between 5 Hz and 55 Hz for <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> neurons. The coefficients were evaluated for randomly sampled input weights (20 weight samples of dimension <inline-formula><mml:math id="inf255"><mml:mi>n</mml:mi></mml:math></inline-formula>, each component sampled from a uniform distribution <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>In a second simulation, we varied the number <inline-formula><mml:math id="inf257"><mml:mi>n</mml:mi></mml:math></inline-formula> of afferents between 10 and 200 for a fixed input rate of 20 Hz, again for randomly sampled input weights (20 weight samples of dimension <inline-formula><mml:math id="inf258"><mml:mi>n</mml:mi></mml:math></inline-formula>, each component sampled from a uniform distribution <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>In a next step, we compared the sampled values of <italic>g</italic><sub>1</sub> as a function of the total input rate <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>n</mml:mi><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> to the values of the approximation given by <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf262"><mml:mi>r</mml:mi></mml:math></inline-formula> between 5 Hz and 55 Hz, <inline-formula><mml:math id="inf263"><mml:mi>n</mml:mi></mml:math></inline-formula> between 10 and 200 neurons, 20 weight samples of dimension <inline-formula><mml:math id="inf264"><mml:mi>n</mml:mi></mml:math></inline-formula>, each component sampled from a uniform distribution <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>Afterwards, we plotted the sampled values of <inline-formula><mml:math id="inf266"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula> as a function of the approximation <inline-formula><mml:math id="inf267"><mml:mi>s</mml:mi></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ115">Equation 111</xref>, <inline-formula><mml:math id="inf268"><mml:mi>r</mml:mi></mml:math></inline-formula> between 5 Hz and 55 Hz, <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, 20 weight samples of dimension <inline-formula><mml:math id="inf270"><mml:mi>n</mml:mi></mml:math></inline-formula>, each component sampled from a uniform distribution <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, 20 USP-samples of dimension <inline-formula><mml:math id="inf272"><mml:mi>n</mml:mi></mml:math></inline-formula> for each rate/weight-combination).</p><p>Next, we investigated the behavior of <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf275"><mml:mi>r</mml:mi></mml:math></inline-formula> between 5 Hz and 55 Hz, <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, 20 weight samples of dimension <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, each component sampled from a uniform distribution <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, 20 USP-samples of dimension <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for each rate/weight-combination), and in last step, as a function of <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with a constant <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-5-2"><title>Evaluation of the approximated natural-gradient rule</title><p>We evaluated the performance of the approximated natural-gradient rule in <xref ref-type="disp-formula" rid="equ118">Equation 114</xref> (with <inline-formula><mml:math id="inf282"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf283"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) compared to Euclidean-gradient descent and the full rule in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> in the learning task of <xref ref-type="fig" rid="fig3">Figure 3</xref> under different input conditions (n=100, Group 1: <inline-formula><mml:math id="inf284"><mml:mrow><mml:mn>10</mml:mn><mml:mtext/><mml:mi class="ltx_unit">Hz</mml:mi></mml:mrow></mml:math></inline-formula>/ Group 2: 30 Hz, Group 1: 10 Hz/ Group 2: 50 Hz, Group 1: 20 Hz/ Group 2: 20 Hz, Group 1: 20 Hz/ Group 2: 40 Hz). The learning curves were averaged over 1000 trials with input and target weight components randomly chosen from a uniform distribution on <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Learning rate parameters were tuned individually for each learning rule and scenario according to <xref ref-type="table" rid="table1">Table 1</xref>. All other parameters were the same as for <xref ref-type="fig" rid="fig3">Figure 3F</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Learning rates.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><italic>r</italic><sub>1</sub></th><th align="left" valign="bottom"><italic>r</italic><sub>2</sub></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf286"><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf287"><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf288"><mml:msub><mml:mi>η</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">10 Hz</td><td align="char" char="." valign="bottom">30 Hz</td><td align="char" char="." valign="bottom">0.000655</td><td align="char" char="." valign="bottom">0.00055</td><td align="char" char="." valign="bottom">0.00000110</td></tr><tr><td align="char" char="." valign="bottom">10 Hz</td><td align="char" char="." valign="bottom">50 Hz</td><td align="char" char="." valign="bottom">0.000600</td><td align="char" char="." valign="bottom">0.00045</td><td align="char" char="." valign="bottom">0.00000045</td></tr><tr><td align="char" char="." valign="bottom">20 Hz</td><td align="char" char="." valign="bottom">20 Hz</td><td align="char" char="." valign="bottom">0.000650</td><td align="char" char="." valign="bottom">0.00053</td><td align="char" char="." valign="bottom">0.00000118</td></tr><tr><td align="char" char="." valign="bottom">20 Hz</td><td align="char" char="." valign="bottom">40 Hz</td><td align="char" char="." valign="bottom">0.000580</td><td align="char" char="." valign="bottom">0.00045</td><td align="char" char="." valign="bottom">0.00000055</td></tr></tbody></table></table-wrap><p>For the angle histograms in <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/66526/figures#fig12">Figure 12A-B</ext-link>, we simulated the natural, Euclidean and approximated natural weight updates for several input and initial weight conditions. Similar to the setup in <xref ref-type="fig" rid="fig3">Figure 3</xref> we separated the <inline-formula><mml:math id="inf289"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> input afferents in two groups firing at different rates (Group1/Group2: 10 Hz/10 Hz, 10 Hz/30 Hz, 10 Hz/50 Hz, 20 Hz/20 Hz, 20 Hz/40 Hz). For each input pattern, 100 Initial weight components were sampled randomly from a uniform distribution <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, while the target weight was fixed at <inline-formula><mml:math id="inf291"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>0.15</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>0.15</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. For each initial weight, 100-long input spike trains were sampled and the average angle between the natural-gradient weight update and the approximated natural-gradient weight update at <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mtext/><mml:mi class="ltx_unit" mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> was calculated. The same was done for the average angle between the natural and the Euclidean weight update.</p></sec></sec><sec id="s4-6"><title>Detailed derivation of the natural-gradient learning rule</title><p>Here, we summarize the mathematical derivations underlying our natural-gradient learning rule (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>). While all derivations in Sec. ‘Detailed derivation of the natural-gradient learning rule’ and Sec. ‘Inverse of the Fisher Information Matrix’ are made for the somatic parametrization and can then be extended to other weight coordinates as described in Sec. ‘Reparametrization and the general natural-gradient rule’, we drop the index <inline-formula><mml:math id="inf293"><mml:mi mathvariant="normal">s</mml:mi></mml:math></inline-formula> in <inline-formula><mml:math id="inf294"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msup></mml:math></inline-formula> for the sake of readability.</p><p>Supervised learning requires the neuron to adapt its synapses in such a way that its input-output distribution approaches a given target distribution with density <inline-formula><mml:math id="inf295"><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. For a given input spike pattern <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, at each point in time, the probability for a Poisson neuron to fire a spike during the interval <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (denoted as <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) follows a Bernoulli distribution with a parameter <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, depending on the current membrane potential. The probability density of the binary variable <italic>y</italic><sub><italic>t</italic></sub> on {0, 1}, describing whether or not a spike occurred in the interval <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, is therefore given by<disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and we have<disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the probability density of the unweighted synaptic potentials <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Measuring the distance to the target distribution in terms of the Kullback-Leibler divergence, we arrive at<disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Since the target distribution does not depend on the synaptic weights, the negative Euclidean gradient of the <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> equals<disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We may then calculate<disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ54"><label>(54)</label><mml:math id="m54"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ55"><label>(55)</label><mml:math id="m55"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ56"><label>(56)</label><mml:math id="m56"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <xref ref-type="disp-formula" rid="equ55">Equation 55</xref> follows from the fact that <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and for <xref ref-type="disp-formula" rid="equ56">Equation 56</xref> we neglected the term of order <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> which is small compared to the remainder. Plugging <xref ref-type="disp-formula" rid="equ56">Equation 56</xref> into <xref ref-type="disp-formula" rid="equ52">Equation 52</xref> leads to the Euclidean-gradient descent online learning rule, given by<disp-formula id="equ57"><label>(57)</label><mml:math id="m57"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here,<disp-formula id="equ58"><label>(58)</label><mml:math id="m58"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is the teacher spike train. We obtain the negative natural gradient by multiplying <xref ref-type="disp-formula" rid="equ57">Equation 57</xref> with the inverse Fisher information matrix, since<disp-formula id="equ59"><label>(59)</label><mml:math id="m59"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with the Fisher information matrix <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> being defined as<disp-formula id="equ60"><label>(60)</label><mml:math id="m60"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Exploiting that, just like for the target density, the density of the USPs does not depend on <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, so<disp-formula id="equ61"><label>(61)</label><mml:math id="m61"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ62"><label>(62)</label><mml:math id="m62"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>we can insert the previously derived formula <xref ref-type="disp-formula" rid="equ56">Equation 56</xref> for the partial derivative of the log-likelihood. Hence, using the tower property for expectation values and the definition of <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ49">Equation 49</xref>, <xref ref-type="disp-formula" rid="equ50">Equation 50</xref>), <xref ref-type="disp-formula" rid="equ60">Equation 60</xref> transforms to<disp-formula id="equ63"><label>(63)</label><mml:math id="m63"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ64"><label>(64)</label><mml:math id="m64"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ65"><label>(65)</label><mml:math id="m65"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ66"><label>(66)</label><mml:math id="m66"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ67"><label>(67)</label><mml:math id="m67"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In order to arrive at an explicit expression for the natural-gradient learning rule, we further decompose the Fisher information matrix, which will then enable us to find a closed expression for its inverse.</p><p>Inspired by the approach in <xref ref-type="bibr" rid="bib4">Amari, 1998</xref>, we exploit the fact that a positive semi-definite matrix is uniquely defined by its values as a bivariate form on any basis of <inline-formula><mml:math id="inf310"><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math></inline-formula>. Choosing a basis for which the bilinear products with <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are of a particularly simple form, we are able to decompose the Fisher Information Matrix by constructing a sum of matrices whose values as a bivariate form on the basis equal are equal to those of <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Due to the structure of this particular decomposition, we may then apply well-known formulas for matrix inversion to obtain <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Consider the basis <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> such that the vectors <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>,</mml:mo><mml:msqrt><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:msqrt><mml:mspace width="thinmathspace"/><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msqrt><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:msqrt><mml:mspace width="thinmathspace"/><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are orthogonal to each other. Here, <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the covariance matrix of the USPs which in the case of independent Poisson input spike trains is given as<disp-formula id="equ68"><label>(68)</label><mml:math id="m68"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this case, the matrix square root reduces to the component-wise square root. Note that for any <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the random variables <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are uncorrelated, since<disp-formula id="equ69"><label>(69)</label><mml:math id="m69"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ70"><label>(70)</label><mml:math id="m70"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ71"><label>(71)</label><mml:math id="m71"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We make the mild assumptions of having small afferent populations firing at the same input rate, and that the basis <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is constructed in such way that the basis vectors are not too close to the coordinate axes, such that the products <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are not dominated by a single component. Then, for sufficiently large <inline-formula><mml:math id="inf323"><mml:mi>n</mml:mi></mml:math></inline-formula>, every linear combination of the random variables <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is approximately normally distributed, thus, the two random variables follow a joint bivariate normal distribution. Furthermore, uncorrelated random variables that are jointly normally distributed are independent. Since functions of independent random variables are also independent, this allows us to calculate all products of the form <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow><mml:mo>∖</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, as we can transform the expectation of products<disp-formula id="equ72"><label>(72)</label><mml:math id="m72"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>into products of expectations. Taking into account that <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we arrive at<disp-formula id="equ73"><label>(73)</label><mml:math id="m73"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mtext>for </mml:mtext><mml:mspace width="thickmathspace"/><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mtext> and </mml:mtext><mml:mspace width="thickmathspace"/><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>≠</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ74"><label>(74)</label><mml:math id="m74"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mtext>for </mml:mtext><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>≠</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ75"><label>(75)</label><mml:math id="m75"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mtext>for </mml:mtext><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>≠</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ76"><label>(76)</label><mml:math id="m76"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the generalized voltage moments given as<disp-formula id="equ77"><label>(77)</label><mml:math id="m77"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msubsup><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ78"><label>(78)</label><mml:math id="m78"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>u</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ79"><label>(79)</label><mml:math id="m79"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The integral formulas follow from the fact that for a large number of input afferents, and under the mild assumption of all synaptic weights roughly being of the same order of magnitude, the membrane potential approximately follows a normal distribution with mean <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext> mV ms</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by <xref ref-type="disp-formula" rid="equ31">Equation 31</xref>, see Sec. ‘Neuron model’.</p><p>Based on the above calculations, we construct a candidate <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for a decomposition of <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We start with the matrix <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, since its easy to see that<disp-formula id="equ80"><label>(80)</label><mml:math id="m80"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi mathvariant="bold">′</mml:mi></mml:mrow></mml:msup><mml:mtext> for all </mml:mtext><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Exploiting the orthogonal properties according to which we constructed <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to carefully add more terms such that also the other identities in <xref ref-type="disp-formula" rid="equ73">Equation 73</xref> hold, we arrive at<disp-formula id="equ81"><label>(81)</label><mml:math id="m81"><mml:mrow><mml:mrow><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here,<disp-formula id="equ82"><label>(82)</label><mml:math id="m82"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To check that indeed <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, it suffices to check the values of <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>G</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as a bilinear form on the basis <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">B</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-7"><title>Inverse of the Fisher information matrix</title><p>As the expectation of the outer product of a vector with itself, <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is per construction symmetric and positive semidefinite. From the previous calculations, it follows that for elements <inline-formula><mml:math id="inf343"><mml:mi>b</mml:mi></mml:math></inline-formula> of a basis <inline-formula><mml:math id="inf344"><mml:mi class="ltx_font_mathcaligraphic">ℬ</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf345"><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math></inline-formula> the products <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are strictly positive. Hence <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is positive definite and thus invertible. We showed that,<disp-formula id="equ83"><mml:math id="m83"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi mathvariant="bold-italic">w</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="negativethinmathspace"/><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We introduce the notation.<disp-formula id="equ84"><label>(83)</label><mml:math id="m84"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mtext> and </mml:mtext><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Then,<disp-formula id="equ85"><label>(84)</label><mml:math id="m85"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the following calculations, we will repeatedly use the identities<disp-formula id="equ86"><label>(85)</label><mml:math id="m86"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mtext> , </mml:mtext><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>, and </mml:mtext><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>By the Sherman-Morrison-Woodbury formula, the inverse of an invertible rank one correction <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of an invertible matrix A is given by<disp-formula id="equ87"><label>(86)</label><mml:math id="m87"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Applying this to invert <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, as a first step, we consider the term <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and identify <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Its inverse is given by<disp-formula id="equ88"><label>(87)</label><mml:math id="m88"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ89"><label>(88)</label><mml:math id="m89"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Applying the Sherman-Morrison-Woodbury formula a second time, this time with <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, we obtain<disp-formula id="equ90"><label>(89)</label><mml:math id="m90"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here,<disp-formula id="equ91"><label>(90)</label><mml:math id="m91"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ92"><label>(91)</label><mml:math id="m92"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Plugging in the definitions of <italic>M</italic><sub>3</sub> and <italic>M</italic><sub>1</sub>, and using that<disp-formula id="equ93"><label>(92)</label><mml:math id="m93"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>we arrive at<disp-formula id="equ94"><label>(93)</label><mml:math id="m94"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ95"><label>(94)</label><mml:math id="m95"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ96"><label>(95)</label><mml:math id="m96"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ97"><label>(96)</label><mml:math id="m97"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ98"><label>(97)</label><mml:math id="m98"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ99"><label>(98)</label><mml:math id="m99"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ100"><label>(99)</label><mml:math id="m100"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>After resorting the terms and grouping, we obtain the inverse of the Fisher Information Matrix as<disp-formula id="equ101"><label>(100)</label><mml:math id="m101"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ102"><label>(101)</label><mml:math id="m102"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ103"><label>(102)</label><mml:math id="m103"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ104"><label>(103)</label><mml:math id="m104"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ105"><label>(104)</label><mml:math id="m105"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ106"><label>(105)</label><mml:math id="m106"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that instead of applying the Sherman-Morrison-Woodbury formula twice one could also directly use the version for rank-2 corrections, known as the Woodbury identity (<xref ref-type="bibr" rid="bib32">Max, 1950</xref>). This can be seen by rewriting <xref ref-type="disp-formula" rid="equ85">Equation 84</xref> as<disp-formula id="equ107"><label>(106)</label><mml:math id="m107"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ108"><mml:math id="m108"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ109"><mml:math id="m109"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>By the Woodbury identity for rank-2 corrections, we then have<disp-formula id="equ110"><mml:math id="m110"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>U</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Analysis of the learning rule coefficients</title><p>In order to gain an intuitive understanding of <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> and to judge its suitability as an in-vivo plasticity rule, we require insights into the behavior of the coefficients <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> under various circumstances.</p><sec id="s4-8-1"><title>Global scaling factor</title><p>The global scaling factor <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given as<disp-formula id="equ111"><label>(107)</label><mml:math id="m111"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:msubsup><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The above formula reveals that <inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is closely tied to the firing nonlinearity of the neuron, as well as the statistics of output. The scaling by the inverse slope of the output nonlinearity amplifies the synaptic update in regions where a small change in weight and thus in the membrane potential would not lead to a noticeable change in output distribution. A further scaling with <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> additionally amplifies the synaptic change in high-output regimes (<xref ref-type="fig" rid="fig10">Figure 10</xref>). This is in line with the spirit of natural gradient that ensures that the size of the synaptic weight update is homogeneous in terms of <inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> change rather than in absolute weight terms. Furthermore, the rescaling is based on the average output statistics, which the synapse might have access to via backpropagating action potentials (<xref ref-type="bibr" rid="bib49">Stuart et al., 1997</xref>), rather than an instantaneous value.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Global learning rate scaling <inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of the mean membrane potential.</title><p>We sampled the global learning rate factor <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (blue) for various conditions. In line with <xref ref-type="disp-formula" rid="equ111">Equation 107</xref>, <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is boosted in regions where the transfer function is flat, that is, <inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is small. The global scaling factor is additionally increased in regions where the transfer function reaches high absolute values.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig10-v1.tif"/></fig></sec><sec id="s4-8-2"><title>Empirical analysis of <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></title><p>While the formula for <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> provides a rather good intuition of this coefficients’ behavior, from the derivations in the previous sections, it becomes clear that such a straightforward interpretation is not readily available from the formulas for <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Defined as<disp-formula id="equ112"><label>(108)</label><mml:math id="m112"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ113"><label>(109)</label><mml:math id="m113"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are given in <xref ref-type="disp-formula" rid="equ103">Equation 102</xref>, these coefficients depend, apart from the membrane potential and its first and second moments, on the total input and its mean the total rate. This raises the question whether the synapse, which in general only has limited access to global quantities, can implement <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We therefore used an empirical analysis through simulations to obtain a more detailed insight.</p><p>As a starting point, we sampled <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for various input conditions (<xref ref-type="fig" rid="fig11">Figure 11A–D</xref>, refer to Sec. ‘Approximation of learning-rule coefficents’ for simulation details). Here, we varied the afferent input rate <inline-formula><mml:math id="inf375"><mml:mi>r</mml:mi></mml:math></inline-formula> between <inline-formula><mml:math id="inf376"><mml:mrow><mml:mn>5</mml:mn><mml:mtext/><mml:mi class="ltx_unit">Hz</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf377"><mml:mrow><mml:mn>55</mml:mn><mml:mtext/><mml:mi class="ltx_unit">Hz</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons and evaluated the value of the respective coefficient for a randomly sampled input weight. In a second simulation (<xref ref-type="fig" rid="fig11">Figure 11E–H</xref>, Sec. ‘Approximation of learning-rule coefficents’), we varied the number <inline-formula><mml:math id="inf379"><mml:mi>n</mml:mi></mml:math></inline-formula> of afferent inputs between 10 and 200 neurons for fixed input rate <inline-formula><mml:math id="inf380"><mml:mrow><mml:mn>20</mml:mn><mml:mtext/><mml:mi class="ltx_unit">Hz</mml:mi></mml:mrow></mml:math></inline-formula>, again with randomly chosen input weights. This revealed an approximately inverse proportional relationship between the average input rate <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <italic>g</italic><sub>3</sub> respectively. Furthermore, these coefficients seemed to be approximately inversely proportional to the number <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of afferent inputs. However, this was not true for <italic>g</italic><sub>4</sub>, whose mean seemed to stay approximately constants across input rates, although the scattering across the mean value (for different weight samples) increased.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Learning rule coefficients can be approximated by simpler quantities.</title><p>(<bold>A</bold>)-(<bold>D</bold>) Samples values for <inline-formula><mml:math id="inf384"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msub><mml:mo mathvariant="normal">,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for different afferent input rates. (<bold>E</bold>)-(<bold>H</bold>) In a second simulation, we varied the number <inline-formula><mml:math id="inf385"><mml:mi>n</mml:mi></mml:math></inline-formula> of afferent inputs. (<bold>I</bold>) Comparison of the sampled values of <inline-formula><mml:math id="inf386"><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msub></mml:math></inline-formula> (blue) as a function of the total input rate <inline-formula><mml:math id="inf387"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">*</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> to the values of the approximation given by <inline-formula><mml:math id="inf388"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">1</mml:mn></mml:msub><mml:mo mathvariant="normal">≈</mml:mo><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo mathvariant="normal">-</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>J</bold>) Sampled values of <inline-formula><mml:math id="inf389"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula> (blue) as a function of the approximation s (<xref ref-type="disp-formula" rid="equ115">Equation 111</xref>). The proximity of the sampled values to the diagonal indicates that <inline-formula><mml:math id="inf390"><mml:mi>s</mml:mi></mml:math></inline-formula> may indeed serve as an approximation for <inline-formula><mml:math id="inf391"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:math></inline-formula>. (<bold>K</bold>) Sampled values of <inline-formula><mml:math id="inf392"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula> (blue) as a function of <inline-formula><mml:math id="inf393"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">4</mml:mn></mml:msub><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula>. The proximity of the sampled values to the diagonal indicates that <inline-formula><mml:math id="inf394"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">4</mml:mn></mml:msub><mml:mo mathvariant="normal">⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> serves as an approximation for <inline-formula><mml:math id="inf395"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:math></inline-formula>. (<bold>L</bold>) Same as (<bold>K</bold>), but with <inline-formula><mml:math id="inf396"><mml:msub><mml:mi>g</mml:mi><mml:mn mathvariant="normal">4</mml:mn></mml:msub></mml:math></inline-formula> replaced by a constant <inline-formula><mml:math id="inf397"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">0.05</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig11-v1.tif"/></fig><p>While the value of the membrane potential stays bounded also for a large number of inputs (due to the normalization of the synaptic weight range with <inline-formula><mml:math id="inf398"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>), the total sum of USPs increases with an increasing number of inputs. Therefore, for large enough <inline-formula><mml:math id="inf399"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the term <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be neglected, so<disp-formula id="equ114"><label>(110)</label><mml:math id="m114"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>A closer look at the behavior of <italic>g</italic><sub>1</sub> shows that, for a sufficiently large number of neurons or high input rates, <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which we verified by simulations (<xref ref-type="fig" rid="fig11">Figure 11I</xref>, Sec. ‘Approximation of learning-rule coefficents’). In consequence,<disp-formula id="equ115"><label>(111)</label><mml:math id="m115"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To verify this approximation, we sampled the values of <inline-formula><mml:math id="inf402"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for various conditions (<xref ref-type="fig" rid="fig11">Figure 11J</xref>, Sec. ‘Approximation of learning-rule coefficents’) and compared them against the approximation in <xref ref-type="disp-formula" rid="equ115">Equation 111</xref>, which confirmed our approximation. Since the input rate is the mean value of the USP, assuming large enough populations with the same input rate and a sufficient number of input afferents, by the central limit theorem we have<disp-formula id="equ116"><label>(112)</label><mml:math id="m116"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">⟶</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mtext> for </mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, in practice, for <inline-formula><mml:math id="inf404"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mtext/><mml:mrow><mml:mi class="ltx_unit">mV</mml:mi><mml:mtext/><mml:mi class="ltx_unit">ms</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, a learning behavior much closer to natural gradient was obtained when <inline-formula><mml:math id="inf405"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was slightly smaller than 1 such as <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (Sec. ‘Quadratic transfer function’).</p><p>As a starting point to approximate <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we noticed that the mean of <italic>g</italic><sub>4</sub> stayed approximately constant when varying the input rate or the number of input afferents. On the other hand, <italic>g</italic><sub>2</sub> rapidly tends to zero in those cases, so we assumed that <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> stays either constant or goes to zero in the limit of large n. Since <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> seemed to be rather small compared to <italic>g</italic><sub>4</sub>, we hypothesized <inline-formula><mml:math id="inf410"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which was confirmed by simulations (<xref ref-type="fig" rid="fig11">Figure 11K</xref>, Sec. ‘Approximation of learning-rule coefficents’). As a second step (<xref ref-type="fig" rid="fig11">Figure 11L</xref>, Sec. ‘Approximation of learning-rule coefficents’), since <italic>g</italic><sub>4</sub> seemed to be constant in the mean, we approximated<disp-formula id="equ117"><label>(113)</label><mml:math id="m117"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where simulations with <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, close to the mean of <italic>g</italic><sub>4</sub> showed a learning behavior close to natural-gradient learning (<xref ref-type="fig" rid="fig12">Figure 12C–F</xref>). Replacing <inline-formula><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf413"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> by the expressions in <xref ref-type="disp-formula" rid="equ116">Equation 112</xref> and <xref ref-type="disp-formula" rid="equ117">Equation 113</xref>, we obtain the approximated natural-gradient rule<disp-formula id="equ118"><label>(114)</label><mml:math id="m118"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Natural-gradient learning can be approximated by a simpler rule in many scenarios.</title><p>(<bold>A</bold>) Mean Fisher angles between true and approximated weight updates (orange) and between natural and Euclidean weight updates (blue), for <inline-formula><mml:math id="inf414"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula>. Results for several input patterns were pooled (group1/group2: 10 Hz/10 Hz 10 Hz/30 Hz, 10 Hz/50 Hz, 20 Hz/20 Hz, 20 Hz/40 Hz). Initial weights and input spikes were sampled randomly (100 randomly sampled initial weight vectors per input pattern; for each, angles were averaged over 100 input spike train samples per afferent). (<bold>B</bold>) Same as (<bold>A</bold>), but angles measured in the Euclidean metric. (<bold>C–F</bold>) Comparison of learning curves for natural gradient (red), Euclidean gradient (blue) and approximation (orange) for <inline-formula><mml:math id="inf415"><mml:mrow><mml:mi>n</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula> afferents. Simulations were performed in the setting of <xref ref-type="fig" rid="fig3">Figure 3</xref>, under multiple input conditions. (<bold>C</bold>) Group one firing with 10 Hz, group two firing at 30 Hz. (<bold>D</bold>) Group one firing with 10 Hz, group two firing at 50 Hz. (<bold>E</bold>) Group one firing with 20 Hz, group two firing at 20 Hz. (<bold>F</bold>) Group one firing with 20 Hz, group two firing at 40 Hz.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66526-fig12-v1.tif"/></fig></sec><sec id="s4-8-3"><title>Performance of the approximated learning rule</title><p>Simulations of natural, Euclidean and approximated natural-gradient weight updates for several input patterns and randomly sampled initial conditions (Sec. ‘Evaluation of the approximated natural-gradient rule’) showed that the average angles (both in the Euclidean metric and in the Fisher metric) between the true and approximated natural-gradient weight update were small compared to the average angle between Euclidean and natural-gradient weight update (<xref ref-type="fig" rid="fig12">Figure 12A–B</xref>). This was confirmed by the learning curves for several tested input conditions in the setting of <xref ref-type="fig" rid="fig3">Figure 3</xref>, since the performance of the approximation lay in between the natural and the Euclidean gradient’s performance (<xref ref-type="fig" rid="fig12">Figure 12C–F</xref>, simulation details: Sec. ‘Evaluation of the approximated natural-gradient rule’). It can hence be regarded as a trade-off between optimal learning speed, parameter invariance and biological implementability. Note that plugging the above calculations into <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> still keeps invariance under coordinate-wise smooth parameter changes.</p></sec></sec><sec id="s4-9"><title>Quadratic transfer function</title><p>While for all our simulations, we used the sigmoidal transfer function given in <xref ref-type="disp-formula" rid="equ24">Equation 24</xref>, the derivations that lead to <xref ref-type="disp-formula" rid="equ101">Equation 100</xref> also hold for other choices of transfer function. A particularly simple and thereby instructive result can be obtained for a transfer function <inline-formula><mml:math id="inf416"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> that satisfies<disp-formula id="equ119"><label>(115)</label><mml:math id="m119"><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mi>ϕ</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This is the case for a rectified quadratic transfer function<disp-formula id="equ120"><label>(116)</label><mml:math id="m120"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf417"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the Heaviside step function. To prevent issues with division by zero and to ensure that <xref ref-type="disp-formula" rid="equ119">Equation 115</xref> holds for all relevant membrane voltages <inline-formula><mml:math id="inf418"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we assume <inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>≪</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then, <xref ref-type="disp-formula" rid="equ67">Equation 67</xref> reduces to<disp-formula id="equ121"><label>(117)</label><mml:math id="m121"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the right side no longer depends on <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, thus yielding <inline-formula><mml:math id="inf421"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Furthermore, we have<disp-formula id="equ122"><label>(118)</label><mml:math id="m122"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ123"><label>(119)</label><mml:math id="m123"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ124"><label>(120)</label><mml:math id="m124"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and therefore <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ82">Equation 82</xref>). Plugging this into <xref ref-type="disp-formula" rid="equ81">Equation 81</xref>, we obtain<disp-formula id="equ125"><label>(121)</label><mml:math id="m125"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> defined in <xref ref-type="disp-formula" rid="equ35">Equation 35</xref>. Inverting this using the Sherman-Morrison-Woodbury Formula, we arrive at<disp-formula id="equ126"><label>(122)</label><mml:math id="m126"><mml:mrow><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mn mathvariant="bold">1</mml:mn><mml:msup><mml:mn mathvariant="bold">1</mml:mn><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Inserting this version of the Fisher information matrix into <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, we get <inline-formula><mml:math id="inf424"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf425"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ111 equ113">Equation 107 and 109</xref>), thus obtaining a simplified version of the natural-gradient learning rule as<disp-formula id="equ127"><label>(123)</label><mml:math id="m127"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ128"><label>(124)</label><mml:math id="m128"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf426"><mml:mi>q</mml:mi></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="equ36">Equation 36</xref>. This is in line with our empirical approximation for <inline-formula><mml:math id="inf427"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for the case of the sigmoidal transfer function (<xref ref-type="disp-formula" rid="equ115">Equation 111</xref>). It is interesting to note that, for a large number of inputs, we have <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the average of the parenthesis in <xref ref-type="disp-formula" rid="equ127">Equation 123</xref>, taken in isolation, reduces to zero. However, this does not mean that, on average, there is no plasticity. Instead, we can say that, for a quadratic activation regime, the natural gradient is purely driven by the correlation between the postsynaptic error and the (normalized and centered) presynaptic input.</p></sec><sec id="s4-10"><title>Continuous-time limit</title><p>Under the Poisson-process assumption, firing a spike in one of the time bins is independent from the spikes in other bins, therefore the probability for firing in two disjunct time intervals <inline-formula><mml:math id="inf429"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is given as<disp-formula id="equ129"><label>(125)</label><mml:math id="m129"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have<disp-formula id="equ130"><label>(126)</label><mml:math id="m130"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ131"><label>(127)</label><mml:math id="m131"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>so the Fisher information matrix is additive.</p><p>In the continuous-time limit, where the interval <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is decomposed as <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∪</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">⟶</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf436"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:mfrac><mml:mo stretchy="false">⟶</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Therefore,<disp-formula id="equ132"><label>(128)</label><mml:math id="m132"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Π</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Then, under the assumption that <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small and firing rates are approximately constant on <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, for the Fisher information matrix, we have<disp-formula id="equ133"><label>(129)</label><mml:math id="m133"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mi>k</mml:mi><mml:mfrac><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ134"><label>(130)</label><mml:math id="m134"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Joint senior authorship, Conceptualization, Formal analysis, Funding acquisition, Investigation, Resources, Supervision, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Joint senior authorship, Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Supervision, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-66526-transrepform1-v1.docx"/></supplementary-material><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Source data for figures.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-66526-data1-v1.zip"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data used to generate the figures and the simulation code are available on <ext-link ext-link-type="uri" xlink:href="https://github.com/elenakreutzer/Code_Natural-gradient_learning_for_spiking_neurons_eLIFE_2022">GitHub</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3c604a3e934e67a7d520239409622d43805b3ddf;origin=https://github.com/elenakreutzer/Code_Natural-gradient_learning_for_spiking_neurons_eLIFE_2022;visit=swh:1:snp:90605b13cc01c5dd2bb217a65eff2703d65df42a;anchor=swh:1:rev:4da5611348ea969aff461886f85842fe36a8571a">swh:1:rev:4da5611348ea969aff461886f85842fe36a8571a</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work has received funding from the European Union 7th Framework Programme under grant agreement 604102 (HBP), the Horizon 2020 Framework Programme under grant agreements 720270, 785907 and 945539 (HBP) and the SNF under the personal grant 310030L_156863 (WS) and the Sinergia grant CRSII5_180316. Calculations were performed on UBELIX (<ext-link ext-link-type="uri" xlink:href="http://www.id.unibe.ch/hpc">http://www.id.unibe.ch/hpc</ext-link>), the HPC cluster at the University of Bern. We thank Oliver Breitwieser for the figure template, and Simone Surace and João Sacramento, as well as the other members of the Senn and Petrovici groups for many insightful discussions. We are also deeply grateful to the late Robert Urbanczik for many enlightening conversations on gradient descent and its mathematical foundations. Furthermore, we wish to thank the Manfred Stärk Foundation for their ongoing support.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bayesian Synaptic Plasticity Makes Predictions about Plasticity Experiments in Vivo</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1410.1029v2">https://arxiv.org/abs/1410.1029v2</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Jegminat</surname><given-names>J</given-names></name><name><surname>Menendez</surname><given-names>JA</given-names></name><name><surname>Pfister</surname><given-names>J-P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Synaptic plasticity as Bayesian inference</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00809-5</pub-id><pub-id pub-id-type="pmid">33707754</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><chapter-title>Differential geometrical theory of Statistics</chapter-title><person-group person-group-type="editor"><name><surname>Amari</surname><given-names>S</given-names></name></person-group><source>In Differential Geometry in Statistical Inference</source><publisher-loc>Hayward</publisher-loc><publisher-name>Institute of Mathematical Statistics</publisher-name><fpage>19</fpage><lpage>94</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Natural Gradient Works Efficiently in Learning</article-title><source>Neural Computation</source><volume>10</volume><fpage>251</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1162/089976698300017746</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name><name><surname>Karakida</surname><given-names>R</given-names></name><name><surname>Oizumi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fisher information and natural gradient learning in random deep networks</article-title><conf-name>In The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>694</fpage><lpage>702</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name><name><surname>Nagaoka</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Methods of Information Geometry. Translations of Mathematical Monographs</source><publisher-name>American Mathematical Society</publisher-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arami</surname><given-names>MK</given-names></name><name><surname>Sohya</surname><given-names>K</given-names></name><name><surname>Sarihi</surname><given-names>A</given-names></name><name><surname>Jiang</surname><given-names>B</given-names></name><name><surname>Yanagawa</surname><given-names>Y</given-names></name><name><surname>Tsumoto</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reciprocal Homosynaptic and heterosynaptic long-term plasticity of corticogeniculate projection neurons in layer VI of the mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>7787</fpage><lpage>7798</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5350-12.2013</pub-id><pub-id pub-id-type="pmid">23637171</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asrican</surname><given-names>B</given-names></name><name><surname>Lisman</surname><given-names>J</given-names></name><name><surname>Otmakhov</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Synaptic strength of individual spines correlates with bound Ca2+-calmodulin-dependent kinase II</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>14007</fpage><lpage>14011</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3587-07.2007</pub-id><pub-id pub-id-type="pmid">18094239</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bernacchia</surname><given-names>A</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Exact natural gradient in deep linear networks and its application to the nonlinear case</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>5941</fpage><lpage>5950</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bliss</surname><given-names>TV</given-names></name><name><surname>Lomo</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path</article-title><source>The Journal of Physiology</source><volume>232</volume><fpage>331</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1973.sp010273</pub-id><pub-id pub-id-type="pmid">4727084</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cencov</surname><given-names>NN</given-names></name></person-group><year iso-8601-date="1972">1972</year><source>Optimal Decision Rules and Optimal Inference</source><publisher-name>American Mathematical Society, Rhode Island. Translation from Russian</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J-Y</given-names></name><name><surname>Lonjers</surname><given-names>P</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Chistiakova</surname><given-names>M</given-names></name><name><surname>Volgushev</surname><given-names>M</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Heterosynaptic plasticity prevents runaway synaptic dynamics</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>15915</fpage><lpage>15929</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5088-12.2013</pub-id><pub-id pub-id-type="pmid">24089497</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chistiakova</surname><given-names>M</given-names></name><name><surname>Volgushev</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Heterosynaptic plasticity in the neocortex</article-title><source>Experimental Brain Research</source><volume>199</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1859-5</pub-id><pub-id pub-id-type="pmid">19499213</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chistiakova</surname><given-names>M</given-names></name><name><surname>Bannon</surname><given-names>NM</given-names></name><name><surname>Chen</surname><given-names>J-Y</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name><name><surname>Volgushev</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Homeostatic role of heterosynaptic plasticity: models and experiments</article-title><source>Frontiers in Computational Neuroscience</source><volume>9</volume><elocation-id>89</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2015.00089</pub-id><pub-id pub-id-type="pmid">26217218</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudek</surname><given-names>SM</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Homosynaptic long-term depression in area CA1 of hippocampus and effects of N-methyl-D-aspartate receptor blockade</article-title><source>PNAS</source><volume>89</volume><fpage>4363</fpage><lpage>4367</lpage><pub-id pub-id-type="doi">10.1073/pnas.89.10.4363</pub-id><pub-id pub-id-type="pmid">1350090</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Souza</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>SC</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptron learning rule derived from spike-frequency adaptation and spike-time-dependent plasticity</article-title><source>PNAS</source><volume>107</volume><fpage>4722</fpage><lpage>4727</lpage><pub-id pub-id-type="doi">10.1073/pnas.0909394107</pub-id><pub-id pub-id-type="pmid">20167805</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Synapse specificity of long-term potentiation breaks down at short distances</article-title><source>Nature</source><volume>388</volume><fpage>279</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1038/40870</pub-id><pub-id pub-id-type="pmid">9230437</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spatio-temporal credit assignment in neuronal population learning</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002092</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002092</pub-id><pub-id pub-id-type="pmid">21738460</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Poo</surname><given-names>MM</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Spike-timing-dependent synaptic plasticity depends on dendritic location</article-title><source>Nature</source><volume>434</volume><fpage>221</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1038/nature03366</pub-id><pub-id pub-id-type="pmid">15759002</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Kistler</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Spiking Neuron Models: Single Neurons, Populations, Plasticity</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511815706</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Haider</surname><given-names>P</given-names></name><name><surname>Ellenberger</surname><given-names>B</given-names></name><name><surname>Kriener</surname><given-names>L</given-names></name><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Petrovici</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Latent equilibrium: A unified learning theory for arbitrarily fast computation with arbitrarily slow neurons</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Kano</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Long-lasting depression of parallel fiber-Purkinje cell transmission induced by conjunctive stimulation of parallel fibers and climbing fibers in the cerebellar cortex</article-title><source>Neuroscience Letters</source><volume>33</volume><fpage>253</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/0304-3940(82)90380-9</pub-id><pub-id pub-id-type="pmid">6298664</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Petrovici</surname><given-names>MA</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Conductance-based dendrites perform reliability-weighted opinion pooling</article-title><conf-name>NICE ’20</conf-name><pub-id pub-id-type="doi">10.1145/3381755.3381767</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kakade</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A natural policy gradient</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1531</fpage><lpage>1538</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letzkus</surname><given-names>JJ</given-names></name><name><surname>Kampa</surname><given-names>BM</given-names></name><name><surname>Stuart</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Learning rules for spike timing-dependent plasticity depend on dendritic synapse location</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>10420</fpage><lpage>10429</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2650-06.2006</pub-id><pub-id pub-id-type="pmid">17035526</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname><given-names>Y</given-names></name><name><surname>Kuras</surname><given-names>A</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>9481</fpage><lpage>9488</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6130-10.2011</pub-id><pub-id pub-id-type="pmid">21715613</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynch</surname><given-names>GS</given-names></name><name><surname>Dunwiddie</surname><given-names>T</given-names></name><name><surname>Gribkoff</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Heterosynaptic depression: A postsynaptic correlate of long-term potentiation</article-title><source>Nature</source><volume>266</volume><fpage>737</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1038/266737a0</pub-id><pub-id pub-id-type="pmid">195211</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Cook</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Somatic EPSP amplitude is independent of synapse location in hippocampal pyramidal neurons</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>895</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1038/78800</pub-id><pub-id pub-id-type="pmid">10966620</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Marceau-Caron</surname><given-names>G</given-names></name><name><surname>Ollivier</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Natural Langevin dynamics for neural networks</article-title><conf-name>In International Conference on Geometric Science of Information</conf-name><fpage>451</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-68445-1</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Toledo-Rodriguez</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Silberberg</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Interneurons of the neocortical inhibitory system</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>793</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1038/nrn1519</pub-id><pub-id pub-id-type="pmid">15378039</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Martens</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>New Insights and Perspectives on the Natural Gradient Method</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.1193">https://arxiv.org/abs/1412.1193</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Max</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="1950">1950</year><source>Inverting Modified Matrices</source><publisher-name>Statistical Research Group Princeton Univ</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ollivier</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Riemannian metrics for neural networks I: feedforward networks</article-title><source>Information and Inference</source><volume>4</volume><fpage>108</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1093/imaiai/iav006</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Amari</surname><given-names>SI</given-names></name><name><surname>Fukumizu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Adaptive natural gradient learning algorithms for various stochastic models</article-title><source>Neural Networks</source><volume>13</volume><fpage>755</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(00)00051-4</pub-id><pub-id pub-id-type="pmid">11152207</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Revisiting Natural Gradient for Deep Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.3584">https://arxiv.org/abs/1301.3584</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Petrovici</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Form versus Function: Theory and Models for Neuronal Substrates</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-39552-4</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfister</surname><given-names>J-P</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Barber</surname><given-names>D</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</article-title><source>Neural Computation</source><volume>18</volume><fpage>1318</fpage><lpage>1348</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.6.1318</pub-id><pub-id pub-id-type="pmid">16764506</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="1945">1945</year><article-title>Information and the accuracy attainable in the estimation of statistical parameters</article-title><source>Bull. Calcutta Math. Soc</source><volume>37</volume><fpage>81</fpage><lpage>91</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rattray</surname><given-names>M</given-names></name><name><surname>Saad</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Analysis of natural gradient descent for multilayer neural networks</article-title><source>Physical Review E</source><volume>59</volume><fpage>4523</fpage><lpage>4532</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.59.4523</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>The perceptron: A probabilistic model for information storage and organization in the brain</article-title><source>Psychological Review</source><volume>65</volume><fpage>386</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1037/h0042519</pub-id><pub-id pub-id-type="pmid">13602029</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royer</surname><given-names>S</given-names></name><name><surname>Paré</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Conservation of total synaptic weight through balanced synaptic depression and potentiation</article-title><source>Nature</source><volume>422</volume><fpage>518</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/nature01530</pub-id><pub-id pub-id-type="pmid">12673250</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ruder</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An Overview of Gradient Descent Optimization Algorithms</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.04747">https://arxiv.org/abs/1609.04747</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dendritic Error Backpropagation in Deep Cortical Microcircuits</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.00062">https://arxiv.org/abs/1801.00062</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiess</surname><given-names>M</given-names></name><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Somato-dendritic Synaptic Plasticity and Error-backpropagation in Active Dendrites</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004638</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004638</pub-id><pub-id pub-id-type="pmid">26841235</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cooperative switch determines the sign of synaptic plasticity in distal dendrites of neocortical pyramidal neurons</article-title><source>Neuron</source><volume>51</volume><fpage>227</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.017</pub-id><pub-id pub-id-type="pmid">16846857</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Softky</surname><given-names>WR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>334</fpage><lpage>350</lpage><pub-id pub-id-type="pmid">8423479</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporea</surname><given-names>I</given-names></name><name><surname>Grüning</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Supervised learning in multilayer spiking neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>473</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00396</pub-id><pub-id pub-id-type="pmid">23148411</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stuart</surname><given-names>G</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Action potential initiation and backpropagation in neurons of the mammalian CNS</article-title><source>Trends in Neurosciences</source><volume>20</volume><fpage>125</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(96)10075-8</pub-id><pub-id pub-id-type="pmid">9061867</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Surace</surname><given-names>SC</given-names></name><name><surname>Pfister</surname><given-names>J-P</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Brea</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the choice of metric in gradient-based theories of brain function</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007640</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007640</pub-id><pub-id pub-id-type="pmid">32271761</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning by the dendritic prediction of somatic spiking</article-title><source>Neuron</source><volume>81</volume><fpage>521</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.030</pub-id><pub-id pub-id-type="pmid">24507189</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>G</given-names></name><name><surname>Levy</surname><given-names>WB</given-names></name><name><surname>Steward</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Spatial overlap between populations of synapses determines the extent of their associative interaction during the induction of long-term potentiation and depression</article-title><source>Journal of Neurophysiology</source><volume>64</volume><fpage>1186</fpage><lpage>1198</lpage><pub-id pub-id-type="doi">10.1152/jn.1990.64.4.1186</pub-id><pub-id pub-id-type="pmid">2258741</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>SR</given-names></name><name><surname>Stuart</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dependence of EPSP efficacy on synapse location in neocortical pyramidal neurons</article-title><source>Science (New York, N.Y.)</source><volume>295</volume><fpage>1907</fpage><lpage>1910</lpage><pub-id pub-id-type="doi">10.1126/science.1067903</pub-id><pub-id pub-id-type="pmid">11884759</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wöhrl</surname><given-names>R</given-names></name><name><surname>Eisenach</surname><given-names>S</given-names></name><name><surname>Manahan-Vaughan</surname><given-names>D</given-names></name><name><surname>Heinemann</surname><given-names>U</given-names></name><name><surname>von Haebler</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Acute and long-term effects of MK-801 on direct cortical input evoked homosynaptic and heterosynaptic plasticity in the CA1 region of the female rat</article-title><source>The European Journal of Neuroscience</source><volume>26</volume><fpage>2873</fpage><lpage>2883</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2007.05899.x</pub-id><pub-id pub-id-type="pmid">18001284</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>HH</given-names></name><name><surname>Amari</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Complexity issues in natural gradient descent method for training multilayer perceptrons</article-title><source>Neural Computation</source><volume>10</volume><fpage>2137</fpage><lpage>2157</lpage><pub-id pub-id-type="doi">10.1162/089976698300017007</pub-id><pub-id pub-id-type="pmid">9804675</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hebbian plasticity requires compensatory processes on multiple timescales</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160259</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0259</pub-id><pub-id pub-id-type="pmid">28093557</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66526.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The natural gradient has a long and rich history in machine learning. Here, the authors derive a biologically plausible implementation of natural-gradient-based plasticity for spiking neurons, which renders learning invariant under dendritic transformations. This new synaptic learning rule makes several very interesting experimental predictions with respect to the interplay of homo- and heterosynaptic plasticity, and with regard to the scaling of plasticity by the presynaptic variance.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66526.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Natural-gradient learning for spiking neurons&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by a Reviewing Editor and John Huguenard as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>This paper derives a natural gradient learning rule for a spiking neuron in a supervised setting. Unlike conventional gradients in Euclidean space, the natural gradient is invariant under reparameterization, thus achieving fast convergence regardless of the position of synaptic contacts on the dendritic tree. The authors relate their rule to experimentally observed properties of synaptic plasticity, such as heterosynaptic regularization. We're not aware of any other work that applied natural gradient to a spiking neuron in a formal way, and we believe it is an important contribution.</p><p>That said, we do have several comments. All are relatively minor (many have to do with presentation), and, we hope, easy to address. Because this is a combination of three reviews, they're not exactly in order of appearance in the manuscript. But hopefully not too far off.</p><p>1. We're slightly concerned with the fact that the weight update scales inversely with the firing rate. First, presumably for small enough firing rates something in the derivation breaks down. Second, this makes a very strong prediction. The only data we know of that relates to this is in Aitchison and Latham, 2014 (now published in Nature Neuroscience vol 24, pgs 565-571, 2021), where they showed, in one experiment, that learning rate scales as 1/sqrt{r} for moderate firing rates (greater than about 1 Hz), and saturates at small firing rates. This is one experiment, so doesn't rule out the theory here, but these points should be discussed.</p><p>2. The link to biology is often very cursory. References to experiments are often in the form 'so and so found something a bit similar'. Wherever possible, you should try to make precision comparisons to experiments. And on page 10 you say that the weight dependent heterosynaptic terms explains the central weight distributions found in cortex. However, the weight dependent heterosynaptic term is just one term out of three, and the other terms are not weight-dependent. If you want to argue this, you should be more rigorous (and in particular take into account all three terms).</p><p>3. The conclusion of the Zenke and Gerstner paper is, we believe, incorrect: A learning rule with an intrinsic weight dependence, such as Oja's rule, does not need homeostasis. This clearly doesn't affect any of your analysis, but it does mean that the statement &quot;…… (Zenke and Gerstner, 2017), where it was shown that pairing Hebbian terms with heterosynaptic and homeostatic plasticity is crucial for stability&quot; is, we believe, not true.</p><p>4. Figure 1B is pretty incomprehensible, and it doesn't help that the equations are small and gray, making them hard to read. We would suggest dropping it, especially since the point is made very well in the text.</p><p>Along the same lines (but much later), in Sec. ‘The naive Euclidean gradient is not parametrization-invariant’ we don't see any contradiction between Equations 26 and Equation 27; they are just two different definitions of Δ w<sup>s</sup>. It might be an inconsistency, but not a contradiction.</p><p>5. Figure 3: Considering that several approximations were made in the derivation of the learning rule (even for Equation 7), learning performance should be evaluated a bit more carefully. In particular:</p><p>a. Please plot the distance between student weights and teacher weights, on the same timescale as in Figure 3F (either in Euclidean space or in Fisher metric space).</p><p>b. Please plot performance versus learning rate, so we can get a sense of robustness. &quot;Performance&quot; could either be asymptotic loss or time to achieve a particular loss. Also, can you speculate how the learning rate of a neuron can be optimized?</p><p>c. What's the target output firing rate? It might have been stated somewhere, but we missed it. It would be nice if it were in the figure caption, like it is in Figure 4.</p><p>d. In Figure 3, n=2 and n=100 are both used. It should be clear which panel is which.</p><p>e. Panels B and C: what's the timescale? There's only one labeled time point (0.1), so it's impossible to tell. And how many trials did it take to learn? We would have expected considerable learning during 2500 trials.</p><p>f. The direction of the gradient depends on whether or not there's a spike on the teacher neuron. Which means the path in weight space is noisy, and the weights should not converge -- instead, they should exhibit fluctuations forever. But the trajectories in D and F were very smoothed. Is that because they're averaged over a large number of trials? Or was some extra processing done? Please explain.</p><p>6. Figure 4:</p><p>a. Can you reproduce experimental results on dendritic position dependent plasticity (eg. Letzkus JJ, Kampa BM, Stuart GJ, 2006; Sjöström PJ, Häusser M, 2006). These experimental results are inconsistent with vanilla Euclidean gradient; hence they would provide support for biological relevance of the proposed learning rule.</p><p>b. &quot;the distance from soma was varied between 1 microns to 10 microns&quot;. This seems small; the characteristic length of dendritic decay is in order of 100 microns (Williams and Stuart, 2002).</p><p>7. Figure 5: Here, you compared three neurons each receiving single excitatory input with different input characteristics. Do you expect the same result to be true for a single neuron receiving three excitatory inputs? We're curious about it because that's a more relevant scenario. Some casual remarks would be helpful.</p><p>8. Figure 7:</p><p>a. We're somewhat confused that you you can distinguish between the three forms of plasticity in Equations 11-13, at least for the stimulated synapses. Don't all three forms contribute at once? But in panel B you say, &quot;Weight change of stimulated weights (homosynaptic)&quot;. And in panel C, you should have seen homosynaptic plasticity (see next comment). Could you explain how you can single out one form of plasticity? Or else drop the comment. Or maybe we misunderstood?</p><p>b. In the legend of Figure 7, it is mentioned that the rest of synapses received 0.01Hz input, but in Sec. ‘Comparison of homo- and heterosynaptic plasticity’, that information is omitted. Did you actually stimulate these synapses? If so, we would have expected a big jump in synaptic weight when there was a spike, and in 60 s there should have been 3 spikes (0.01x60x5).</p><p>c. 7G: A should be S?</p><p>d. Equation 13 is out of the blue. Could you tell us where this comes from? And from the bottom of page 10, we have</p><p>&quot;…… is roughly a linear function of the membrane potential (more specifically, its deviation with respect to its baseline).&quot;</p><p>What does baseline refer to? We didn't see anything about baseline in Equation 25, which is where Equation 13 comes from.</p><p>e. Could you tell us how the colors in panels B-D were computed? Presumably some assumptions were made. If so, what were they?</p><p>9. Equation 4: we believe that the right hand side should not have a minus sign. More importantly, we should be told that Y* consists of a sum of δ-functions; the statement &quot;Here, Y* denotes a teacher spike train sampled from p*&quot; was unhelpful. It's also important to tell us exactly how Y* is sampled, which is, presumably from</p><p>P(spike from Y* \in [t, t+dt]) = phi(sum<sub>j</sub> w*<sub>j</sub> x<sub>j</sub><sup>ε</sup>) dt.</p><p>If that's not correct, then we're thoroughly confused.</p><p>10. The second paragraph on page 4 makes some strong statements, and we're not sure all of them are true. For instance,</p><p>&quot;Convergence of learning is therefore harmed by the slow adaptation of distal synapses compared to equally important proximal counterparts.&quot;</p><p>Presumably, &quot;adaptation&quot; means &quot;learning&quot;. If so, why is adaptation necessarily slow at distal synapses? That would seem to depend on the biological learning rule. If not, what does &quot;adaptation&quot; mean in this context?</p><p>And,</p><p>&quot;With the multiplicative USP term x<sup>ε</sup> in Equation 4 being the only manifestation of presynaptic activity, there is no mechanism by which to take into account input variability, which can, in turn, also impede learning.&quot;</p><p>This is a bit out of the blue. Why should input variability affect learning? So far the authors have not said anything about this. It comes up later, but right now it's pretty obscure.</p><p>11. Last paragraph on page 4: the motivation for using the Fisher information matrix is a bit obscure. (For instance, what does &quot;it is generally the unique metric that remains invariant under sufficient statistics&quot; mean?) We strongly suspect that we won't be only ones who are lost. So it would be good to motivate it in plain language. And in particular, it would be nice to show, at the very least, that using the Fisher information matrix automatically makes the learning rule invariant with respect to a change of variables. (Which is pretty easy to do.)</p><p>Along the same lines, after Equation 9 you say</p><p>&quot;This represents an unmediated reflection of the philosophy of natural gradient descent, which finds the steepest path for a small change in output, rather than in the numeric value of some parameter.&quot;</p><p>This has been the theme all along, but was it ever shown? As far as we can tell, you just used the inverse of the Fisher information matrix. Is it obvious that it has the above effect?</p><p>12. It would be extremely useful to write down log p<sub>w</sub> in the main text.</p><p>13. The quantities in Equation 7 should be defined immediately -- right now we have to read down half a page to figure out what they are. And you should tell us immediately how γ<sub>u</sub> and γ<sub>w</sub> could be computed locally.</p><p>14. In the section &quot;Natural gradient speeds up learning&quot;, please tell us exactly what the cost function is (which, presumably, means telling us w* and the filters you use to convert incoming spike trains to x). And it would be good if the firing rates were given in the main text, so one doesn't have to look at the figure.</p><p>15. Because of the factor of phi'(V)/phi(V) in Equations 7 and 8, the effective scaling in Equation 9 is, approximately, 1/phi'(V). This doesn't change the point that a shallow slope implies a high learning rate, but thing's aren't quite as bad as Equation 9 implies. This seems worth mentioning.</p><p>16. You mention, on the top of page 8, &quot;that the absence of dendritic democracy does not contradict the presence of democratic plasticity&quot;. It seems worth mentioning, just for completeness, that dendritic democracy can be achieved without democratic plasticity. In particular, as far as we can tell, the Euclidean gradient is also consistent with dendritic democracy, although convergence to the democratic state might be slower that for the natural gradient.</p><p>17. p. 10 &quot;In comparison, input from weak synapses only has……&quot; We couldn't get the logic of this and the following sentences. This non-trivial claim is not explicitly tested in the subsequent paragraph, but reappears in the discussion. You should expand on it, or remove it.</p><p>18. p22: &quot;The integral formulas follow from……&quot;. It would help if you referred to Equations 20 and 21 (in Sec. ‘Neuron model’) here. We were confused by the sudden appearance of ε<sub>o</sub> and c<sub>ε</sub>. Also, you should mention how they are related to dt.</p><p>19. It is somewhat confusing that you set up the problem about dendritic distance, but then first address, in Figure 3, the role of firing rate, before showing the solution to dendritic plasticity. Perhaps starting with Figure 4, contrasting standard gradients to natural gradients would help.</p><p>20. Page 7/8, you say &quot;neurons are not symmetrical geometric objects&quot;. You should make it clear what this means (presumably it means that they have dendrites, and, therefore, some weights are attenuated). In addition, it's not clear that not being a symmetrical geometric objects implies a non-isotropic cost landscapes, since how isotropic the cost landscape is depends on the input and cost function as well as the attenuation in the dendrites. In principle they former could cancel the latter, producing an isotropic cost landscapes even with strongly attenuating neurons. It seems best to drop this point.</p><p>21. A number of studies have recently emphasized that in large networks most machine learning problems have highly degenerate minima (e.g. Belkin et al. et al. PNAS). Does the algorithm generalize to such situations? Your answer may be &quot;we don't know&quot;. But whatever the answer, it would be worth mentioning in the paper.</p><p>22. Page 7, last paragraph: should the reference to Figure 5 be to Figure 4?</p><p>23. The abundance of superscripts, subscripts and decorations in the variables throughout the manuscript make it very hard to read. We would suggest simplifying notation as much as possible. In particular:</p><p>a. The superscript on the gradient operator doesn't help, as it requires extra memorization -- which is hard because it's not standard. You would be much better off putting the superscript on the cost function, and not redefining the gradient. It's especially confusing in Equation 6, since at first glance the superscript n means take n derivatives.</p><p>b. There are two w's: w<sup>d</sup> and w<sub>s</sub>. Except sometimes there's a w without a superscript. It should always be clear which w you're referring to.</p><p>c. The notation in Equation 7 may make for easier reading, but it makes it very difficult to figure out what's actually going on, especially since there's no easy way to tell vectors from scalars. We would strongly recommend using components (dw<sub>i</sub>/dt = ……). It would make it much more clear, with very little cost.</p><p>24. Along the same line, the phrase &quot;unweighted synaptic potential (USP) train&quot; is pretty distracting. We would strongly suggest dropping it, since everybody agrees what a spike train is. Plus, we searched, and &quot;weighted dendritic potentials&quot; was used only once, in the sentence before unweighted synaptic potentials were defined. So &quot;unweighted&quot; seems a bit redundant.</p><p>25. Important equations should, in our opinion, be displayed. That's because readers (OK, some of these readers) always go back and search for important quantities, and they're easier to see if displayed. (This was triggered by the expression for V, a few lines up from Equation 2.)</p><p>26. The setup should be made clear up front: the neuron is receiving a teacher spike train, y<sub>t</sub>, and using those spike trains to update its synapses. The statement before Equation 3, &quot;Assuming that the neuron strives to reproduce a target firing distribution p*(y|x)&quot; was not helpful (I couldn't really make sense of it, so I kind of ignored it). And so it took me forever to figure out what was going on.</p><p>27. Along the same lines, wouldn't it make sense to just say the neuron is trying to maximize the log probability of the teacher spikes? It gives the same cost function, and it's a lot less obscure than saying the cost function is the KL distance. Clearly a matter of taste, but log probability seems more sensible.</p><p>28. On the top of page 9, it says that learning rates are inversely correlated to the variance of σ<sup>2</sup>(x<sup>ε</sup>). It needs to be clear why the equations say that; right now it's pretty much out of the blue.</p><p>29. End of discussion (page 12): &quot;error-correcting plasticity rule&quot;. Why is it error correcting?</p><p>30. Very end of discussion:</p><p>&quot;Explicitly and exactly applying natural gradient at the network level does not appear biologically feasible due to the existence of cross-unit terms in the Fisher information matrix G. However, methods such as the unit-wise natural-gradient approach (Ollivier, 2015) could be employed to approximate the natural gradient using a block-diagonal form of G. For spiking networks, this would reduce global natural-gradient descent to our local rule for single neurons.&quot;</p><p>We didn't understand that -- it should be unpacked.</p><p>31. Given that this is <italic>eLife</italic>, and there's no page limit, we would strongly urge you to get rid of the Appendix and put all the analysis in Methods. We suspect anybody willing to read Methods will want to see the algebra in the Appendix. In any case, all relevant quantities (e.g., g<sub>1</sub>, ……) should be in Methods; the reader shouldn't have to hunt them down in what will potentially be another document. And certainly, the gradient, which is central to the whole endeavor, should be in Methods. As should Equation S18, so that the reader knows where G(w) comes from.</p><p>32. There's a much easier derivation of G(w), Equation 31:</p><p>int dx exp(h dot t) N(x; mu, Σ) f(a dot x)</p><p>is easy to evaluate (here x is a vector and N(x; mu, Σ) is a Gaussian with mean mu and covariance Σ). Take two derivatives, and, voila, you have.. Same answer, but easier on the reader.</p><p>In addition, the derivation of its inverse can be more concise. G(w) is a sum of a diagonal matrix and a rank-2 modulation. Denoting</p><p>V = (r w') and C = [[c1 e<sup>2</sup>, c2 e], [c2 e, c3]],</p><p>we may write</p><p>G(w) = c1 Σ + V C V<sup>T</sup>.</p><p>Applying the Woodbury identity,</p><p>G<sup>-1</sup>(w) = Σ<sup>-1</sup>/c1 – V' (C<sup>-1</sup> + V Σ<sup>-1</sup> V<sup>T</sup>)<sup>-1</sup> V'<sup>T</sup>,</p><p>where</p><p>V' = (r'/e w).</p><p>Thus,</p><p>g = [[g1 g3], [g2 g4]]</p><p>is given as</p><p>g = – (C<sup>-1</sup> + V Σ<sup>-1</sup> V<sup>T</sup>) <sup>-1</sup>.</p><p>33. Aren't Equations S12-13 a repeat of arguments made above, on the same page? Or is this something new? Either way, it should be clear (and if it's a repeat, maybe it can be dropped).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Natural-gradient learning for spiking neurons&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by John Huguenard (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The good news: The reviewers are quite positive, and there are no major concerns.</p><p>The bad news: Reviewer 1 has many comments requiring clarification. But almost all should be relatively straightforward to address in a timely way.</p><p><italic>Reviewer #1:</italic></p><p>The manuscript is much improved, although it's still a bit hard to read. Some of which we think could be easily fixed. Specific comments follow.</p><p>1. p 1: &quot;It certainly could be the case that evolution has favored one particular parametrization over all others during its gradual tuning of synaptic plasticity, but this would necessarily imply sub-optimal convergence for all but a narrow set of neuron morphologies and connectome configurations.&quot;</p><p>We found this confusing, since parametrization and learning rules are not directly coupled. Did you mean &quot;evolution has favored one particular parametrization over all others, and updates weights using Euclidean gradient descent ……&quot;?</p><p>2. One more attempt to kill USP: why not call x<sup>ε</sup> filtered spike trains, since that's what they are? &quot;Unweighted synaptic potential&quot; is completely unfamiliar, and with a paper this complicated, that's the last thing one needs -- every single time we read that phrase, we had to mentally translate it to filtered spike trains.</p><p>3. Technically, x<sub>iε</sub> should depend on distance to the soma: because of dendritic filtering, the farther a psp travels along a dendrite the more it spreads out (in time). You should mention this, and speculate on whether it will make much difference to your analysis. It seems like it might, since c<sub>ε</sub> scales with the timescale of the psp.</p><p>4. In both places where you mention spikes (x<sub>i</sub>, above Equation 4 and Y<sup>*</sup>, above Equation 9) you should point out that they are a sum of δ functions. &quot;Spikes&quot; isn't so well defined in this field.</p><p>5. If you want anybody besides hard core theorists do understand what you're doing, you'll need to be explicit about what p* is: it's Equation 7 with phi replaced by phi*, the target firing rate as a function of voltage. We strongly suggest you do that. In particular, the description &quot;a target firing distribution p*(y∣x<sup>ε</sup>)&quot; doesn't help much, since a firing rate distribution is usually a distribution over firing rates. What you mean, though, is that p* gives you the true probability of a spike in a time interval.</p><p>5. It would be very helpful, in Equation 9, to add</p><p>\approx dt[ (phi – phi*) – phi* log(phi/phi*) ]</p><p>(and point out that it's exact in the limit dt --&gt; 0). Then Equation 10 is easy -- just say that phi* is replaced by Y*.</p><p>6. Starting in Equation 10, you switch to a weight without a superscript. We found this very confusing, since the learning rule you wrote down in Equation 10 is exactly the one for w<sup>s</sup>. We strongly suggest you never use w. But if there is a reason to do so, you should be crystal clear about what it is. As far as we could tell, this switch to w is completely unsignalled. So maybe it's a typo?</p><p>7. p 4: &quot;which is likely to harm the convergence speed towards an optimal weight configuration.&quot;</p><p>The word &quot;likely&quot; is not all that convincing. Can you be a lot stronger? Are there relevant refs? Or you could point to the simulations in Figure 3.</p><p>8. p 4: &quot;In the following, we therefore drop the index from the synaptic weights w to emphasize the parametrization-invariant nature of the natural gradient.&quot;</p><p>In fact, what's really going on is that you're deriving the natural gradient learning rule (Equation 13) with V given by</p><p>V = sum<sub>i</sub> f(w<sub>i</sub>) x<sub>iε</sub></p><p>which is actually very confusing, since f was used previously to relate w<sup>s</sup> to w<sup>d</sup> (Equation 4). Is the f in Equation 14 the same as the one in Equation 4? If so, it should be w<sup>d</sup> on the right hand side of Equation 14. If not, you should use a different symbol for the function.</p><p>As you can see, your notation is confusing us. Our suggestion would be to always use superscripts s or d, and never use w by itself.</p><p>9. Figure 2:</p><p>&quot;(A) During supervised learning, the error between the current and the target state is measured in terms of a cost function defined on the neuron's output space; in our case, this is the manifold formed by the neuronal output distributions p(y, x).&quot;</p><p>What does &quot;defined on the neuron's output space&quot; mean? Doesn't the cost function depend only on the weights, since it's an average over the input-output function? Which is more or less what you say in the next sentence,</p><p>&quot;As the output of a neuron is determined by the strength of incoming synapses, the cost C is an implicit function of the afferent weight vector w.!</p><p>Although we're not sure why you say &quot;implicit&quot; function; isn't it a very explicit function (see point 5 above).</p><p>&quot;If, instead, we follow the gradient on the output manifold itself, it becomes independent of the underlying parametrization.&quot;</p><p>Since we don't know what the output manifold is (presumably the statistical manifold in panel A? not that that helps), this sentence doesn't make sense to us.</p><p>&quot;In contrast, natural-gradient learning will locally correct for distortions arising from non-optimal parametrizations.&quot;</p><p>This is a highly nontrivial statement, and there's nothing in the manuscript so far that makes this obvious. I think a reference is needed here. Or point the reader to Figure 3 for an example?</p><p>10. Equation 13: You may not like components, but it would be extremely helpful to put them in Equation 13, or maybe add an equation with components to make it clear. There is a precedent; you use components in Equation 4. And let's face it, anybody who isn't mathematically competent will have been lost long ago, and anybody who is mathematically competent will want to be sure what's going on. And things are actually ambiguous, since it's really hard to tell what's a vector and what's a scalar. In particular, γ<sub>u</sub> should be treated as a vector, even though it's not bold, so it's not immediately clear whether γ<sub>s</sub> should also be treated as a vector. If nothing else, you should use γ<sub>u</sub> {\bf 1}; that would help a little. Also, the convention these days when writing f(w) is for f not to be bold, but instead define it as a pointwise nonlinearity. You don't have to follow conventions, but it does make it easier on the reader.</p><p>The same comment applies to Equation 17.</p><p>11. In Equation 14, presumably w<sup>s</sup> on the left hand side should be bold? And why not use indices, as in Equation 4? Much more clear, and fewer symbols.</p><p>12. p 6, typo: &quot;inactive inactive&quot;.</p><p>13. Two questions and one comment about Figure 3F. First, why don't the orange and blue lines start in the same place? Second, the blue line doesn't appear to be saturating. Can the Euclidean gradient do better than the natural gradient? If so, that's important to point out. But maybe it's not true, and is a by-product of your optimization method?</p><p>And the comment: presumably the D<sub>KL</sub> term on the y-axis has a factor of dt. That makes it pretty much impossible to interpret, since we don't know what dt is (probably it's somewhere, but it's not in an obvious place). We suggest removing the factor of dt, and making it clear that when phi is close to phi*, what's being plotted is &lt;(phi-phi*)^2/2 phi*&gt;. That will make the plot much easier to interpret.</p><p>14. p 8: &quot;As a result, the effect of synaptic plasticity on the neuron's output is independent of the synapse location, since dendritic attenuation is precisely counterbalanced by weight update amplification.&quot;</p><p>Because of the term γ<sub>w</sub> w<sup>d</sup>, this is true only if w<sup>d</sup> \propto 1/α(d). But, as you point out later, this doesn't have to be the case. So this statement needs to be modified. Maybe it's approximately true?</p><p>15. Figure 5, several comments:</p><p>In the equation in panel H, r should be in the numerator, not the denominator, and a factor of ε<sub>02</sub> is missing. In addition, the term on the right hand side can be simplified:</p><p>σ<sup>2</sup>(x<sup>ε</sup>) = r ε<sub>02</sub>/2(tau<sub>1</sub> + tau<sub>2</sub>).</p><p>Also it would be a good idea to switch to tau<sub>s</sub> and tau<sub>m</sub> here rather than tau<sub>1</sub> and tau<sub>2</sub>, to be consistent with Methods, and to not clash with the tau's in panels A-B.</p><p>And in panels A-B, presumably it should be tau<sub>si</sub> rather than tau<sub>i</sub>?. Also, tau<sub>m</sub> should be reported here.</p><p>It would make it a lot easier on the reader if you wrote down an equation for cε, which isn't so complicated: c<sub>ε</sub>/r<sub>i</sub> = 2 (tau<sub>m</sub> + tau<sub>s</sub>)/ ε<sub>02</sub> r<sub>i</sub>. Equation 19 is a natural place to do that.</p><p>Because of the other two terms in Equation 17, it's not true that &quot;Natural-gradient learning scales inversely with input variance.&quot; It should be clear that this is approximate.</p><p>16. In Equation 19, it should be x<sub>iε</sub> on the left hand side, not x<sup>ε</sup>.</p><p>17. p 10: &quot;Furthermore, it is also consistent with data from Aitchison and Latham (2014) and Aitchison et al.et al. (2021), as well as with their observation of an inverse dependence on presynaptic firing rates, although our interpretation is different from theirs.&quot;</p><p>There is no justification for this statement: in Aitchison et al.et al. a plot of Δ w/w versus r showed 1/sqrt(r) scaling. That would be consistent with the theory in this paper only if the weight scales as 1/sqrt(r). It might, but without checking, you can't make that claim.</p><p>That data is weak, so the fact that you don't fit it is hardly the end of the world. However, what's more important is to point out that the theory here makes very different predictions that the theory in Aitchison et al.et al. That way, experimentalists will have something to do.</p><p>Finally, the inverse scaling with firing rate must eventually saturate, since there's a limit to how big weight changes can be. You should comment on this, if briefly.</p><p>18. Figure 7, we're pretty lost, for several reasons:</p><p>a. Because homosynaptic scales as 1/r, it's not possible to have unstimulated synapses (for which r=0); at least not with the current derivation. If you want to have unstimulated synapses, you need to show that it is indeed possible, by computing G when r<sub>i=0</sub> for some of the i's.</p><p>b. From the bottom of page 27, γ<sub>u</sub> \approx ε<sub>0</sub> c<sub>ε</sub>. Thus, the first two plasticity terms are</p><p>c<sub>ε</sub> (x<sup>ε</sup>/r – ε<sub>0</sub>).</p><p>Because = r ε<sub>0</sub>, these two terms approximately cancel. Which means everything should be driven by the last term, c<sub>w</sub> V f(w). This doesn't seem consistent with the explanation in Figure 7.</p><p>c. Because of the term c<sub>w</sub> V f(w), shouldn't there be a strong dependence on the unstimulated weights in panels B-D?</p><p>All this should be clarified.</p><p>19. p 13: &quot;We further note an interesting property of our learning rule, which it inherits directly from the Fisher information metric that underlies natural gradient descent, namely invariance under sufficient statistics (Cencov, 1972).&quot; What does &quot;invariance under sufficient statistics&quot; refer to?</p><p>20. p 13: &quot;A further prediction that follows from our plasticity rule is the normalization of weight changes by the presynaptic variance. We would thus anticipate that increasing the jitter in presynaptic spike trains should reduce LTP in standard plasticity induction protocols.&quot;</p><p>Presumably this statement comes from the fact that the learning rate scales inversely with the variance of the filtered spike train. However, it's not clear that jittering spike trains increases the variance. What would definitely increase the variance, though, is temporal correlations. Maybe this could be swapped in for jittering? Either that, or explain why jittering increases variance. And also, it would be nice to refer the reader to the dependence of the learning rate on variance.</p><p>21. You should comment on where you think the teacher spike train comes from. Presumably it's delivered by PSPs at the soma, but those don't propagate back to synapses. How do you envision the teacher spike trains communicating with the synapses? Even if the answer is &quot;we don't know&quot;, it's important to inform the reader -- this may simply be an avenue for future research.</p><p>22. In Equations 29 and 30, you should explain why you use \approx. Presumably because that's because you assumed a constant firing rate?</p><p>23. Equation 31: it would be extremely useful to point out that c<sub>ε</sub> = 2(tau<sub>m</sub> + tau<sub>s</sub>)/ε<sub>02</sub>, along with a reference (or calculate it yourself somewhere).</p><p>24. After Equation 31, &quot;Unless indicated otherwise, simulations were performed with a membrane time constant tau<sub>m</sub> = 10 ms and a synaptic time constant τs = 3 ms. Hence, USPs had an amplitude of 60 mV&quot;.</p><p>Doesn't the amplitude of the USPs depend on ε<sub>0</sub>? And 60 mV seems pretty big. Is that a typo?</p><p>25. Equation 34: Missing parentheses around Σ<sub>USP</sub> w<sup>s</sup> in the second term in parentheses.</p><p>26. Equation 24 and the line above it: should w<sub>i</sub> be w<sub>id</sub>? If not, you shouldn't use f, since that was used to translate somatic to dendritic amplitude.</p><p>27. Equation 115: Should it be Theta(V-theta)?</p><p>28. Equation 117 is identical to Equation 67. Did you mean to drop the term (phi')<sup>2</sup>/phi?</p><p>Also, (phi')<sup>2</sup>/phi = 1 when V &gt; theta. But presumably it's equal to 0 when V &lt; theta. If so, Equation 117 should be</p><p>G(w) = E(dt Theta(V-theta) x xT).</p><p>if that's correct, then the integrals I<sub>1</sub>-I<sub>3</sub> do not reduce to the values given in Equations 118-120.</p><p><italic>Reviewer #2:</italic></p><p>The authors have done a good job in the revision.</p><p>A number of small suggestions remaining:</p><p>– Equation 13. Wouldn't it be easier to write this for a single component <inline-formula><mml:math id="sa1m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>……</p><p>– Figure 3 shows temporal structure in the teacher signal but this is nowhere explained in the main text.</p><p>– Fig3D+E perhaps the axis or caption can indicate whether w1,2 is 10 or 50Hz.</p><p>– Figure 4A: Shouldn't the purple top solid curve (dendritic voltage) be taller than the solid orange curve?</p><p>– Fig5D+E+F might look better with the position of the y-axis left instead of right.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66526.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>This paper derives a natural gradient learning rule for a spiking neuron in a supervised setting. Unlike conventional gradients in Euclidean space, the natural gradient is invariant under reparameterization, thus achieving fast convergence regardless of the position of synaptic contacts on the dendritic tree. The authors relate their rule to experimentally observed properties of synaptic plasticity, such as heterosynaptic regularization. We're not aware of any other work that applied natural gradient to a spiking neuron in a formal way, and we believe it is an important contribution.</p><p>That said, we do have several comments many have to do with presentation, and, we hope, easy to address. Because this is a combination of three reviews, they're not exactly in order of appearance in the manuscript. But hopefully not too far off.</p><p>1. We're slightly concerned with the fact that the weight update scales inversely with the firing rate. First, presumably for small enough firing rates something in the derivation breaks down. Second, this makes a very strong prediction. The only data we know of that relates to this is in Aitchison and Latham, 2014 (now published in Nature Neuroscience vol 24, pgs 565-571, 2021), where they showed, in one experiment, that learning rate scales as 1/sqrt{r} for moderate firing rates (greater than about 1 Hz), and saturates at small firing rates. This is one experiment, so doesn't rule out the theory here, but these points should be discussed.</p></disp-quote><p>The inverse scaling comes from the first term in the Fisher information matrix, which is proportional to the input’s covariance matrix. This fits nicely into the philosophy of natural gradient descent and the resulting synaptic democracy, as isotropization of the cost with respect to the weights also implies a renormalization of the inputs with respect to their variance. For the particular case of Poisson firing discussed in our manuscript, the variance of an input is proportional to its rate, hence the inverse dependence of our learning rule w.r.t. the input rate. The rule is well-behaved as long as the input rates are not exactly zero, which is arguably the case in a non-pathological brain. Of course, depending on the exact experimental scenario and stimulation protocol, the other heterosynaptic components could override this scaling effect and even invert it. We also referenced Aitchison et al. as our prediction is, indeed, similar to theirs, and also compatible with their data, but made clear that our interpretation is different (lines 270-272 in the revised manuscript with tracked changes).</p><disp-quote content-type="editor-comment"><p>2. The link to biology is often very cursory. References to experiments are often in the form 'so and so found something a bit similar'. Wherever possible, you should try to make precision comparisons to experiments. And on page 10 you say that the weight dependent heterosynaptic terms explains the central weight distributions found in cortex. However, the weight dependent heterosynaptic term is just one term out of three, and the other terms are not weight-dependent. If you want to argue this, you should be more rigorous (and in particular take into account all three terms).</p></disp-quote><p>We tried to clarify the relationship to experimental data, while remaining cautious in order to not overstate our claims, since most experiments have not been performed in the exact same settings as those in which we tested our model. For example, many experiments from the referenced literature on heterosynaptic plasticity rely on deterministic stimulation protocols rather than on Poisson input (e.g., Royer and Paré, 2003, White et al. 1990). However, the qualitative matches of our results to this data represent encouraging evidence, while also providing an inspiration for future, more targeted experiments that also allow a quantitative comparison. To emphasize this point, we included an additional paragraph in the discussion, outlining potential directions of future experiments.</p><p>Furthermore, as highlighted by Häusser, data on the dependence of plasticity on synaptic location is rare, “as most such studies to date have implicitly assumed that synaptic function at distal and proximal synapses is identical.” (Häusser, 2001, p. R12). This puts the findings of Froemke, Poo, and Dan, 2005 in relation to our predictions on democratic plasticity, as we discuss at the end of Section 2.4 (lines 245-249).</p><p>As to the last point, we argue that the existence of the het<sub>w</sub> term corresponds to the observation made by Loewenstein et al. (2011) about changes in spine size being proportional to the spine size itself (in light of Asrican et al. (2007), who show that synaptic strength is positively correlated with spine size). However, we agree that the remark about heterosynaptic plasticity “explaining” the central weight distribution in cortex might have been phrased too strongly – “correspondence” is, in this case, certainly a better descriptor than “explanation”.</p><disp-quote content-type="editor-comment"><p>3. The conclusion of the Zenke and Gerstner paper is, we believe, incorrect: A learning rule with an intrinsic weight dependence, such as Oja's rule, does not need homeostasis. This clearly doesn't affect any of your analysis, but it does mean that the statement &quot;… (Zenke and Gerstner, 2017), where it was shown that pairing Hebbian terms with heterosynaptic and homeostatic plasticity is crucial for stability&quot; is, we believe, not true.</p></disp-quote><p>You are absolutely right about the role of the weight-dependent term in Oja’s rule, but we don’t think that this necessarily contradicts Zenke and Gernstner. It might only be a matter of wording, but we would argue that in Oja’s rule, the negative, weight-dependent term is, in addition to being heterosynaptic, also homeostatic, because it stabilizes (normalizes) the weight vector by punishing too large weights.</p><disp-quote content-type="editor-comment"><p>4. Figure 1B is pretty incomprehensible, and it doesn't help that the equations are small and gray, making them hard to read. We would suggest dropping it, especially since the point is made very well in the text.</p></disp-quote><p>Agreed, we moved Figure 1B to the corresponding Figure 8 (Section 6.3 of the revised manuscript) in the Methods.</p><disp-quote content-type="editor-comment"><p>Along the same lines (but much later), in Sec. ‘The naive Euclidean gradient is not parametrization-invariant’ we don't see any contradiction between Equations 26 and Equation 27; they are just two different definitions of \Δ w<sup>s</sup>. It might be an inconsistency, but not a contradiction.</p></disp-quote><p>Agreed, done (line 621).</p><disp-quote content-type="editor-comment"><p>5. Figure 3: Considering that several approximations were made in the derivation of the learning rule (even for Equation 7), learning performance should be evaluated a bit more carefully. In particular:</p><p>a. Please plot the distance between student weights and teacher weights, on the same timescale as in Figure 3F (either in Euclidean space or in Fisher metric space).</p></disp-quote><p>Good point, we added another Figure (#9) in the methods. We also plotted the distance between student and teacher firing rates, as well as the “minimum” and “maximum” learning curves.</p><disp-quote content-type="editor-comment"><p>b. Please plot performance versus learning rate, so we can get a sense of robustness. &quot;Performance&quot; could either be asymptotic loss or time to achieve a particular loss. Also, can you speculate how the learning rate of a neuron can be optimized?</p></disp-quote><p>Our simulations show (panel D in the new Figure 9) that the convergence behavior is robust against perturbations of the learning rate.</p><p>Meta-learning is, in general, a complex subject of active research, and we are aware of several related experimental (e.g., https://elifesciences.org/articles/51439) and algorithmic (e.g., https://elifesciences.org/articles/66273) studies. For our purposes, the optimization of eta<sub>0</sub> is a relatively simple case of univariate optimization, so our simple grid search was sufficient.</p><disp-quote content-type="editor-comment"><p>c. What's the target output firing rate? It might have been stated somewhere, but we missed it. It would be nice if it were in the figure caption, like it is in Figure 4.</p></disp-quote><p>The teacher was modeled as a Poisson neuron that fires with a time-dependent rate, given by \phi (\sum<sub>i=1</sub><sup>n</sup> w*<sub>i</sub> \xepsi), with a randomly sampled target weight w<sup>*</sup>. We added a sentence in the caption of Figure 3 to clarify this.</p><disp-quote content-type="editor-comment"><p>d. In Figure 3, n=2 and n=100 are both used. It should be clear which panel is which.</p></disp-quote><p>Absolutely, we adapted the caption accordingly.</p><disp-quote content-type="editor-comment"><p>e. Panels B and C: what's the timescale? There's only one labeled time point (0.1), so it's impossible to tell. And how many trials did it take to learn? We would have expected considerable learning during 2500 trials.</p></disp-quote><p>We added a second labeled time point so that the time scale in the panel becomes clear. Furthermore, we renamed the ordinate of the scatter plots into PSTH, since the label “trial” might have been misleading. The scatter plots in B and C show repeated sampling of the teacher and student firing distribution before and after learning, for one of the sample traces in 3F. This means they do not depict the learning process itself, but illustrate the firing distribution of the student and teacher at fixed points in the learning process.</p><disp-quote content-type="editor-comment"><p>f. The direction of the gradient depends on whether or not there's a spike on the teacher neuron. Which means the path in weight space is noisy, and the weights should not converge -- instead, they should exhibit fluctuations forever. But the trajectories in D and F were very smoothed. Is that because they're averaged over a large number of trials? Or was some extra processing done? Please explain.</p></disp-quote><p>You are correct, the weight paths were averaged over 500 trials. We included this information in the figure caption so it is easier to find.</p><disp-quote content-type="editor-comment"><p>6. Figure 4:</p><p>a. Can you reproduce experimental results on dendritic position dependent plasticity (eg. Letzkus JJ, Kampa BM, Stuart GJ, 2006; Sjöström PJ, Häusser M, 2006). These experimental results are inconsistent with vanilla Euclidean gradient; hence they would provide support for biological relevance of the proposed learning rule.</p></disp-quote><p>Thank you for raising this interesting point. It is certainly true that this is conceptually related to the issue we discuss here – in some sense, it’s a mirror problem, namely the attenuation of output signals versus the attenuation of input signals. We have added a paragraph in the discussion where we describe the relationship between these findings and the predictions of our learning rule (lines 366-376).</p><disp-quote content-type="editor-comment"><p>b. &quot;the distance from soma was varied between 1 microns to 10 microns&quot;. This seems small; the characteristic length of dendritic decay is in order of 100 microns (Williams and Stuart, 2002).</p></disp-quote><p>Thank you for pointing this out, this is a mistake in the caption of Figure 4 that appeared during the final editing of the paper. It is not the distance from soma that is varied between 1 and 10 microns, but rather the inverse attenuation factor α(d) which is varied between 1 and 10. Since α(d)=exp(-d/λ), this means that d/λ is varied between 0 and ln(10) ≅ 2,3. With an electrotonic length scale of approximately 200 μm, as described in Williams and Stuart (2002), this corresponds to a distance d from soma between 0 and 460 μm. We corrected this in the revised manuscript (caption of Figure 4).</p><disp-quote content-type="editor-comment"><p>7. Figure 5: Here, you compared three neurons each receiving single excitatory input with different input characteristics. Do you expect the same result to be true for a single neuron receiving three excitatory inputs? We're curious about it because that's a more relevant scenario. Some casual remarks would be helpful.</p></disp-quote><p>When the activity of inputs does not overlap in time (at any point in time, only one is active), the results will hold exactly as in Figure 5. We chose this scenario because it is the only one where a clear-cut comparison can be made, without additional complications induced by their interaction.</p><p>In the case of simultaneously active inputs, we still expect the variance to strongly impact the learning rate in an inverse-proportional manner, since Equation 34 of the revised manuscript (line 585) shows that the input variance is an important contributor to the Fisher information matrix, whose inverse plays an essential role in the natural gradient update (see Equation 12 of the revised manuscript, line 134). From Equation 37 (revised manuscript, line 591), it becomes clear that the homosynaptic component of our learning rule exactly scales with the inverse variance of the unweighted synaptic potential. We also ran computer simulations for this scenario, but the resulting data is much more difficult to study, since due to the interactions of the different inputs, the behavior of the heterosynaptic components becomes much more complicated.</p><disp-quote content-type="editor-comment"><p>8. Figure 7:</p><p>a. We're somewhat confused that you you can distinguish between the three forms of plasticity in Equations 11-13, at least for the stimulated synapses. Don't all three forms contribute at once? But in panel B you say, &quot;Weight change of stimulated weights (homosynaptic)&quot;. And in panel C, you should have seen homosynaptic plasticity (see next comment). Could you explain how you can single out one form of plasticity? Or else drop the comment. Or maybe we misunderstood?</p></disp-quote><p>You are right, stimulated synapses are affected by all three terms simultaneously, while unstimulated synapses are only affected by the heterosynaptic terms. The confusion resulted from our overloading of the terms “homosynaptic” and “heterosynaptic”. In literature, they are often used synonymously with “stimulated” and “unstimulated”. Here, we use the terms to describe the different components of the learning rule. We have revised the caption and text to avoid the resulting ambiguity.</p><disp-quote content-type="editor-comment"><p>b. In the legend of Figure 7, it is mentioned that the rest of synapses received 0.01Hz input, but in Sec. ‘Comparison of homo- and heterosynaptic plasticity’, that information is omitted. Did you actually stimulate these synapses? If so, we would have expected a big jump in synaptic weight when there was a spike, and in 60 s there should have been 3 spikes (0.01x60x5).</p></disp-quote><p>You are right, and we addressed this misunderstanding in the revised caption of Figure 7 and Sec. ‘Comparison of homo- and heterosynaptic plasticity’ (lines 677-679). In the simulation, the synapses received no input spikes at all. The assumption of infinitesimally small input rates was simply an argument that allowed us to avoid division by zero in the homosynaptic term. Biologically, this would simply correspond to zero homosynaptic plasticity for zero input.</p><disp-quote content-type="editor-comment"><p>c. 7G: A should be S?</p></disp-quote><p>Yes, you are right. We corrected it in the figure.</p><disp-quote content-type="editor-comment"><p>d. Equation 13 is out of the blue. Could you tell us where this comes from?</p></disp-quote><p>Indeed, this was not sufficiently clear in the previous version. Furthermore, Equation 13 ( = Equation 23 of the revised manuscript, line 285) contained a typo as the last identity only holds approximately. We corrected it and added a reference to the section in the Methods (former SI) where the empirical analysis of γ<sub>w</sub> is explained.</p><disp-quote content-type="editor-comment"><p>And from the bottom of page 10, we have</p><p>&quot;… is roughly a linear function of the membrane potential (more specifically, its deviation with respect to its baseline).&quot;</p><p>What does baseline refer to? We didn't see anything about baseline in Equation 25, which is where Equation 13 comes from.</p></disp-quote><p>By “baseline” we refer to the somatic membrane potential V<sub>rest</sub> of the neuron in the absence of afferent input. Throughout the paper, we assumed a value of -70 mV for V<sub>rest</sub> (see Sec. ‘Neuron model’). In contrast, the variable V captures the part of the somatic membrane potential which is evoked by afferent input (Equation 5, line 84). Hence, the total somatic membrane potential is given as V<sub>rest</sub> + V. The definitions of both V<sub>rest</sub> and V can also be found in the first paragraph of the Methods section (Sec. ‘Neuron model’ of the revised manuscript, lines 542 and 547 / Equation 25). We now explicitly define both V<sub>rest</sub> and V at the very beginning of Section 2.2 (line 79).</p><disp-quote content-type="editor-comment"><p>e. Could you tell us how the colors in panels B-D were computed? Presumably some assumptions were made. If so, what were they?</p></disp-quote><p>Yes, we made the following assumptions, which we have now also included in the Methods, Sec. ‘Comparison of homo- and heterosynaptic plasticity’ (lines 683-689).</p><p>For the plots in 7B and 7C, we chose the diverging colormap matplotlib.cm.seismic in matplotlib. The colormap was inverted such that red indicates negative values representing synaptic depression and blue indicates positive values representing synaptic potentiation. The colormap was linearly discretized into 500 steps. To avoid too bright regions where colors cannot be clearly distinguished, the indices 243-257 were excluded. The colorbar range was manually set to a symmetric range that includes the max/min values of the data.</p><p>In Figure 7D, we only distinguish between regions where plasticity at stimulated and at unstimulated synapses have opposite signs (light green), and regions where they have the same sign (dark green).</p><disp-quote content-type="editor-comment"><p>9. Equation 4: we believe that the right hand side should not have a minus sign. More importantly, we should be told that Y* consists of a sum of δ-functions; the statement &quot;Here, Y* denotes a teacher spike train sampled from p*&quot; was unhelpful. It's also important to tell us exactly how Y* is sampled, which is, presumably from</p><p>P(spike from Y* \in [t, t+dt]) = phi(sum<sub>j</sub> w*<sub>j</sub> x<sub>jε</sub>) dt.</p><p>If that's not correct, then we're thoroughly confused.</p></disp-quote><p>You are right about the minus sign, we corrected this typo (line 101 / Equation 10 of the revised manuscript).</p><p>We also added a reference to the Methods where we provide more information regarding the teacher spike train. For brevity and readability, and since the main text is already quite heavy on equations, we opted for having these equations in the Methods.</p><p>You are also right that for the simulations in Figure 3 we choose a realizable teacher, i.e., one that we know can be learned; as you say, its spikes are sampled from</p><p>P(spike from Y* \in [t, t+dt]) = phi(sum<sub>j</sub> w*<sub>j</sub> x<sub>jε</sub>) dt.</p><p>We made this assumption to facilitate the analysis of our learning rule, but please note that Equation 13 (line 137) of the revised manuscript can be derived in a more general setting where the teacher spikes are generated from an arbitrary (inhomogeneous) Poisson process.</p><disp-quote content-type="editor-comment"><p>10. The second paragraph on page 4 makes some strong statements, and we're not sure all of them are true. For instance,</p><p>&quot;Convergence of learning is therefore harmed by the slow adaptation of distal synapses compared to equally important proximal counterparts.&quot;</p><p>Presumably, &quot;adaptation&quot; means &quot;learning&quot;. If so, why is adaptation necessarily slow at distal synapses? That would seem to depend on the biological learning rule. If not, what does &quot;adaptation&quot; mean in this context?</p></disp-quote><p>Yes, “adaptation” means “learning” in this context. And you are right, the statement was not precise enough. We were referring specifically to a dendritic parametrization, which might appear natural for local computation. We have corrected the text accordingly (lines 109-113).</p><disp-quote content-type="editor-comment"><p>And,</p><p>&quot;With the multiplicative USP term x<sup>ε</sup> in Equation 4 being the only manifestation of presynaptic activity, there is no mechanism by which to take into account input variability, which can, in turn, also impede learning.&quot;</p><p>This is a bit out of the blue. Why should input variability affect learning? So far the authors have not said anything about this. It comes up later, but right now it's pretty obscure.</p></disp-quote><p>You are right, we should have at least provided some intuition for the reader until they reach the corresponding section in the manuscript. We have amended the text accordingly (lines 114-115).</p><disp-quote content-type="editor-comment"><p>11. Last paragraph on page 4: the motivation for using the Fisher information matrix is a bit obscure. (For instance, what does &quot;it is generally the unique metric that remains invariant under sufficient statistics&quot; mean?) We strongly suspect that we won't be only ones who are lost. So it would be good to motivate it in plain language.</p></disp-quote><p>Indeed, invariance under sufficient statistics is not a commonplace property and needs explanation. To not disrupt the flow in this already dense section, we now simply refer to some classical literature on information geometry to motivate the canonical nature of the Fisher metric (line 129). We then specifically explain invariance under sufficient statistics and address its relevance for neuronal computation in the Discussion section (lines 358-364).</p><disp-quote content-type="editor-comment"><p>And in particular, it would be nice to show, at the very least, that using the Fisher information matrix automatically makes the learning rule invariant with respect to a change of variables. (Which is pretty easy to do.)</p></disp-quote><p>We agree that at this point in the manuscript it would be useful to state this invariance explicitly. To not disrupt the flow of reading, we added a corresponding sentence in the main manuscript (line 135), along with a reference to the detailed derivation and figure in the Methods.</p><disp-quote content-type="editor-comment"><p>Along the same lines, after Equation 9 you say</p><p>&quot;This represents an unmediated reflection of the philosophy of natural gradient descent, which finds the steepest path for a small change in output, rather than in the numeric value of some parameter.&quot;</p><p>This has been the theme all along, but was it ever shown? As far as we can tell, you just used the inverse of the Fisher information matrix. Is it obvious that it has the above effect?</p></disp-quote><p>We used this argument when introducing the Fisher metric in the first place (line 125 onwards) and tried to visualize it in Figure 2:</p><p>“The key idea of natural gradient as outlined by Amari is to follow the (locally) shortest path in terms of the neuron's firing distribution. Argued from a normative point of view, this is the only &quot;correct&quot; path to consider, since plasticity aims to adapt a neuron's behavior, i.e., its input-output relationship, rather than some internal parameter (Figure 2).”</p><p>Building on your other suggestions, we made a number of modifications to Section 2.2. to better explain different aspects of the theory. We hope that now readers are provided with sufficient intuition before reaching the “unmediated reflection” remark in Section 2.5.</p><disp-quote content-type="editor-comment"><p>12. It would be extremely useful to write down log p<sub>w</sub> in the main text.</p></disp-quote><p>Done, but without the log, for brevity (line 90 / Equation 8 of the revised manuscript).</p><disp-quote content-type="editor-comment"><p>13. The quantities in Equation 7 should be defined immediately -- right now we have to read down half a page to figure out what they are. And you should tell us immediately how γ<sub>u</sub> and γ<sub>w</sub> could be computed locally.</p></disp-quote><p>Agreed. We referenced the factors briefly directly after the equations (lines 139-140) and added expressions for γ<sub>u</sub> and γ<sub>w</sub> to the main manuscript (line 174 / Equation 15 of the revised manuscript).</p><disp-quote content-type="editor-comment"><p>14. In the section &quot;Natural gradient speeds up learning&quot;, please tell us exactly what the cost function is (which, presumably, means telling us w* and the filters you use to convert incoming spike trains to x). And it would be good if the firing rates were given in the main text, so one doesn't have to look at the figure.</p></disp-quote><p>Done. We have included more information in the text and also added references to the corresponding sections in the methods (lines 195-204).</p><disp-quote content-type="editor-comment"><p>15. Because of the factor of phi'(V)/phi(V) in Equations 7 and 8, the effective scaling in Equation 9 is, approximately, 1/phi'(V). This doesn't change the point that a shallow slope implies a high learning rate, but thing's aren't quite as bad as Equation 9 implies. This seems worth mentioning.</p></disp-quote><p>Good idea, done (lines 256-257).</p><disp-quote content-type="editor-comment"><p>16. You mention, on the top of page 8, &quot;that the absence of dendritic democracy does not contradict the presence of democratic plasticity&quot;. It seems worth mentioning, just for completeness, that dendritic democracy can be achieved without democratic plasticity. In particular, as far as we can tell, the Euclidean gradient is also consistent with dendritic democracy, although convergence to the democratic state might be slower that for the natural gradient.</p></disp-quote><p>Done (lines 250-251).</p><disp-quote content-type="editor-comment"><p>17. p. 10 &quot;In comparison, input from weak synapses only has…&quot; We couldn't get the logic of this and the following sentences. This non-trivial claim is not explicitly tested in the subsequent paragraph, but reappears in the discussion. You should expand on it, or remove it.</p></disp-quote><p>We understand how our previous formulation may have caused some confusion here, so we modified the text in the hope of avoiding this misunderstanding (lines 296-304). In short: Assuming a single spike arrives via a strong synapse, it will cause a larger PSP (and therefore a larger V compared to the baseline) than the same spike arriving via a weak synapse. Thus, on average, weak synapses require more stimulus than strong synapses for the same weight change to occur.</p><disp-quote content-type="editor-comment"><p>18. p22: &quot;The integral formulas follow from…&quot;. It would help if you referred to Equations 20 and 21 (in Sec. ‘Neuron model’) here. We were confused by the sudden appearance of ε<sub>o</sub> and c<sub>ε</sub>. Also, you should mention how they are related to dt.</p></disp-quote><p>Done (lines 762-763).</p><disp-quote content-type="editor-comment"><p>19. It is somewhat confusing that you set up the problem about dendritic distance, but then first address, in Figure 3, the role of firing rate, before showing the solution to dendritic plasticity. Perhaps starting with Figure 4, contrasting standard gradients to natural gradients would help.</p></disp-quote><p>Indeed, we asked ourselves the same question when writing the manuscript. In the end, we decided to present our results in this order for several reasons. First, Figure 3 is intended to be the classical “it works” figure, before delving into the details and implications. Second, it does portray some fundamental properties of the natural gradient, namely the isotropization of the cost function close to the optimum. Third, it addresses the purely functional perspective (not only does it work, but it also can surpass the standard Euclidean approach) early on, which we believe to be important. Fourth, if it came after Figure 4, we feel it would disrupt the flow by switching from physiology to function then to physiology again. So while we certainly agree that swapping the figures would carry some benefit, we believe that it would also come with certain downsides and would therefore prefer the current order.</p><disp-quote content-type="editor-comment"><p>20. Page 7/8, you say &quot;neurons are not symmetrical geometric objects&quot;. You should make it clear what this means (presumably it means that they have dendrites, and, therefore, some weights are attenuated). In addition, it's not clear that not being a symmetrical geometric objects implies a non-isotropic cost landscapes, since how isotropic the cost landscape is depends on the input and cost function as well as the attenuation in the dendrites. In principle they former could cancel the latter, producing an isotropic cost landscapes even with strongly attenuating neurons. It seems best to drop this point.</p></disp-quote><p>You are absolutely right that the different factors that affect the shape of a cost function could, in principle, cancel out. We thought that at this point it would be instructive to convey the intuition that not only are cost landscapes often anisotropic, but that, given the diversity of morphologies and input signals, it would be an incredible coincidence if cost landscapes were isotropic in the first place. We modified the text (lines 188-192) to make it more clear, following your suggestions.</p><disp-quote content-type="editor-comment"><p>21. A number of studies have recently emphasized that in large networks most machine learning problems have highly degenerate minima (e.g. Belkin et al. PNAS). Does the algorithm generalize to such situations? Your answer may be &quot;we don't know&quot;. But whatever the answer, it would be worth mentioning in the paper.</p></disp-quote><p>The generalization of our learning rule to larger networks is certainly a research direction that we are very interested to explore. Variants of natural-gradient learning have already been successfully applied to rate-based deep networks, and their convergence behavior has often been found to compare favorably to methods based on Euclidean gradient descent (see e.g. Bernacchia et al. 2018, Ollivier 2015, Pascanu and Bengio 2013).</p><p>We are therefore optimistic that also for the case of Poisson-spiking neurons, natural-gradient learning will generalize well to the large network case. In particular, since our learning rule relies on local information, we do not expect it to be affected by the presence of degenerate minima in a different way than standard backpropagation.</p><p>A detailed analysis of this scenario is however beyond the scope of the current paper, since there are still many open questions on how to best train stochastically spiking deep networks with gradient methods, even for Euclidean gradient descent.</p><disp-quote content-type="editor-comment"><p>22. Page 7, last paragraph: should the reference to Figure 5 be to Figure 4?</p></disp-quote><p>You are right, we corrected this typo (line 231).</p><disp-quote content-type="editor-comment"><p>23. The abundance of superscripts, subscripts and decorations in the variables throughout the manuscript make it very hard to read. We would suggest simplifying notation as much as possible. In particular:</p><p>a. The superscript on the gradient operator doesn't help, as it requires extra memorization -- which is hard because it's not standard. You would be much better off putting the superscript on the cost function, and not redefining the gradient. It's especially confusing in Equation 6, since at first glance the superscript n means take n derivatives.</p></disp-quote><p>We very much agree with the aim of simplifying notation, and we tried our best to do so. Moreover, the point about the nth derivative is absolutely correct. We tried to address it by capitalizing the superscript and making it non-italic. However, we think that the superscript must remain on the gradient, because it would otherwise suggest that it is the cost function that changes. While it is true that it is useful to view the natural gradient as “isotropizing the cost function”, strictly speaking, the cost function does not change. For the exact same cost function under the exact same parametrization, the two gradients predict different trajectories in weight space.</p><disp-quote content-type="editor-comment"><p>b. There are two w's: w<sup>d</sup> and w<sup>s</sup>. Except sometimes there's a w without a superscript. It should always be clear which w you're referring to.</p></disp-quote><p>Agreed, and we have amended the manuscript accordingly. We use all three of these notations to emphasize the difference between somatic, dendritic and parametrization-invariant learning. In Equation 10 (revised manuscript, line 101), the somatic amplitude is learned, and the text below now makes it explicit. As we move to the natural gradient, the learning becomes parametrization-invariant, and the superscript-free version of the weights is used to make this point. We added a sentence above Equation 11 in which we say this explicitly (lines 123-124).</p><disp-quote content-type="editor-comment"><p>c. The notation in Equation 7 may make for easier reading, but it makes it very difficult to figure out what's actually going on, especially since there's no easy way to tell vectors from scalars. We would strongly recommend using components (dw<sub>i</sub>/dt = …). It would make it much more clear, with very little cost.</p></disp-quote><p>Throughout the paper, we chose to use vectors for easier reading and have consistently used bold notation for vectors. In particular, it greatly reduces the number of indices, which is already rather high, due to the complexity of the problem, as you have pointed out above. We fear that a switch of notation around one particular equation might be confusing to the reader. Alternatively, using component-wise notation everywhere would make the equations significantly more difficult to parse. We would therefore kindly ask you to consider keeping the current notation.</p><disp-quote content-type="editor-comment"><p>24. Along the same line, the phrase &quot;unweighted synaptic potential (USP) train&quot; is pretty distracting. We would strongly suggest dropping it, since everybody agrees what a spike train is. Plus, we searched, and &quot;weighted dendritic potentials&quot; was used only once, in the sentence before unweighted synaptic potentials were defined. So &quot;unweighted&quot; seems a bit redundant.</p></disp-quote><p>We understand the possible confusion and hope that writing out the voltage equation (see changes in main text, Equation 5 of the revised manuscript, line 84) helps clarify this point. The spike trains are denoted as x<sub>i</sub>, whereas the filtered spike trains (with the PSP kernel \ε) are denoted by x<sub>iε</sub>. These are what we call USPs, and they are quite important, as they appear explicitly in the learning rule (and its derivation).</p><disp-quote content-type="editor-comment"><p>25. Important equations should, in our opinion, be displayed. That's because readers (OK, some of these readers) always go back and search for important quantities, and they're easier to see if displayed. (This was triggered by the expression for V, a few lines up from Equation 2.)</p></disp-quote><p>We absolutely agree and have displayed more equations inline in the revised manuscript, both in the main text and in the supplement. Being aware that different readers have different preferences regarding equations, we tried to reduce the number of displayed equations to a minimum, but your comments encouraged us to increase the value of this minimum.</p><disp-quote content-type="editor-comment"><p>26. The setup should be made clear up front: the neuron is receiving a teacher spike train, y<sub>t</sub>, and using those spike trains to update its synapses. The statement before Equation 3, &quot;Assuming that the neuron strives to reproduce a target firing distribution p*(y|x)&quot; was not helpful (I couldn't really make sense of it, so I kind of ignored it). And so it took me forever to figure out what was going on.</p></disp-quote><p>Agreed. We modified the text accordingly (lines 93-97).</p><disp-quote content-type="editor-comment"><p>27. Along the same lines, wouldn't it make sense to just say the neuron is trying to maximize the log probability of the teacher spikes? It gives the same cost function, and it's a lot less obscure than saying the cost function is the KL distance. Clearly a matter of taste, but log probability seems more sensible.</p></disp-quote><p>We agree, and added a sentence about the equivalence of minimizing the KL and maximizing the log-likelihood of teacher spikes (lines 99-100).</p><disp-quote content-type="editor-comment"><p>28. On the top of page 9, it says that learning rates are inversely correlated to the variance of σ<sup>2</sup>(x<sup>ε</sup>). It needs to be clear why the equations say that; right now it's pretty much out of the blue.</p></disp-quote><p>Indeed, this statement referred more to the empirical observation provided in Figure 5. The more precise mathematical reasoning is given in the next sentence (line 262): “In particular, for the homosynaptic component, the scaling is exactly [inversely proportional].”</p><disp-quote content-type="editor-comment"><p>29. End of discussion (page 12): &quot;error-correcting plasticity rule&quot;. Why is it error correcting?</p></disp-quote><p>This is due to the multiplicative error term [Y* – phi(V)] in the learning rule. We address this term explicitly in Section 2.2, as well as in Section 2.6 and Figure 7, where plasticity switches sign at the zero-error line.</p><disp-quote content-type="editor-comment"><p>30. Very end of discussion:</p><p>&quot;Explicitly and exactly applying natural gradient at the network level does not appear biologically feasible due to the existence of cross-unit terms in the Fisher information matrix G. However, methods such as the unit-wise natural-gradient approach (Ollivier, 2015) could be employed to approximate the natural gradient using a block-diagonal form of G. For spiking networks, this would reduce global natural-gradient descent to our local rule for single neurons.&quot;</p><p>We didn't understand that -- it should be unpacked.</p></disp-quote><p>We have revised and extended this paragraph to hopefully improve its clarity (lines 407-414). In particular, it now explicitly addresses the non-localities induced by applying natural-gradient descent at the network level and explains the benefits of the unit-wise solution more clearly.</p><disp-quote content-type="editor-comment"><p>31. Given that this is eLife, and there's no page limit, we would strongly urge you to get rid of the Appendix and put all the analysis in Methods. We suspect anybody willing to read Methods will want to see the algebra in the Appendix. In any case, all relevant quantities (e.g., g<sub>1</sub>, …) should be in Methods; the reader shouldn't have to hunt them down in what will potentially be another document. And certainly, the gradient, which is central to the whole endeavor, should be in Methods. As should Equation S18, so that the reader knows where G(w) comes from.</p></disp-quote><p>Agreed, done, and thank you for the suggestion! We struggled with this decision ourselves and tried to make the manuscript more reader-friendly by shifting some of the more detailed content into an SI, but are definitely happy with the new structure.</p><disp-quote content-type="editor-comment"><p>32. There's a much easier derivation of G(w), Equation 31:</p><p>int dx exp(h dot t) N(x; mu, Σ) f(a dot x)</p><p>is easy to evaluate (here x is a vector and N(x; mu, Σ) is a Gaussian with mean mu and covariance Σ). Take two derivatives, and, voila, you have. Same answer, but easier on the reader.</p></disp-quote><p>We really appreciate the detailed feedback, especially given how complicated some of our derivations are and we are certainly welcoming suggestions for simplification. In this case, however, we must apologize but the derivation of Equation 31 (now Equation 48) already seems quite straightforward to us – which, admittedly, might very much be a matter of taste.</p><disp-quote content-type="editor-comment"><p>In addition, the derivation of its inverse can be more concise. G(w) is a sum of a diagonal matrix and a rank-2 modulation. Denoting</p><p>V = (r w') and C = [[c1 e<sup>2</sup>, c2 e], [c2 e, c3]],</p><p>we may write</p><p>G(w) = c1 \Σ + V C V<sup>T</sup>.</p><p>Applying the Woodbury identity,</p><p>G{<sup>-1</sup>}(w) = \Σ{<sup>-1</sup>}/c1 – V' (C{<sup>-1</sup>} + V \Σ{<sup>-1</sup>} V<sup>T</sup>) {<sup>-1</sup>} V'<sup>T</sup>,</p><p>where</p><p>V' = (r'/e w).</p><p>Thus,</p><p>g = [[g1 g3], [g2 g4]]</p><p>is given as</p><p>g = – (C{<sup>-1</sup>} + V \Σ{<sup>-1</sup>} VT) {<sup>-1</sup>}.</p></disp-quote><p>Thank you for this suggestion! Indeed, the idea of working directly with a rank-2 correction is more elegant and concise than our iterative application of Sherman-Morrison, and certainly more appealing to a reader interested in understanding the principle behind our method. We therefore added this idea to our derivation in the Methods (line 779-780). However, since the result remains the same, we have conserved our original derivation in order to avoid an extensive and error-prone change in notation.</p><disp-quote content-type="editor-comment"><p>33. Aren't Equations S12-13 a repeat of arguments made above, on the same page? Or is this something new? Either way, it should be clear (and if it's a repeat, maybe it can be dropped).</p></disp-quote><p>Assuming you are referring to either Equation S3 or Equation S4 (now Equations 51 and 52, lines 733-735), both of them are similar to the arguments made in Eq. S12 and Eq. S13 -(now Equations 61 and 62, line 742), but not exactly the same.</p><p>1. In Equation S3 (now Equation 51), the density over the USPs vanishes, since it appears in both the counter and the denominator of the fraction.</p><p>2. In Equation S4 (now Equation 52), the density of the target firing distribution vanishes when taking the derivative, since it does not depend on w.</p><p>3. In contrast, in EquationS12 and S13 (now Equations 61 and 62), the density over the USPs vanishes when taking the derivative, since it also does not depend on w.</p><p>We have adapted the text to make this difference more clear.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The manuscript is much improved, although it's still a bit hard to read. Some of which we think could be easily fixed. Specific comments follow.</p><p>1. p 1: &quot;It certainly could be the case that evolution has favored one particular parametrization over all others during its gradual tuning of synaptic plasticity, but this would necessarily imply sub-optimal convergence for all but a narrow set of neuron morphologies and connectome configurations.&quot;</p><p>We found this confusing, since parametrization and learning rules are not directly coupled. Did you mean &quot;evolution has favored one particular parametrization over all others, and updates weights using Euclidean gradient descent ……&quot;?</p></disp-quote><p>We adapted the sentence to address this issue.</p><disp-quote content-type="editor-comment"><p>2. One more attempt to kill USP: why not call x<sup>ε</sup> filtered spike trains, since that's what they are? &quot;Unweighted synaptic potential&quot; is completely unfamiliar, and with a paper this complicated, that's the last thing one needs -- every single time we read that phrase, we had to mentally translate it to filtered spike trains.</p></disp-quote><p>We believe that USP is a more exact description for x<sup>ε</sup>, because it implicitly specifies the filter (same filter as the membrane, difference of exponentials) and the lack of synaptic weighting (see our reply to #24) whereas “filtered spike trains” does not (it could just be the synaptic current, for example). The first paragraph of Sec. ‘The natural-gradient plasticity rule’ and especially Equation 5 should leave no room for interpretation.</p><disp-quote content-type="editor-comment"><p>3. Technically, x<sub>iε</sub> should depend on distance to the soma: because of dendritic filtering, the farther a psp travels along a dendrite the more it spreads out (in time). You should mention this, and speculate on whether it will make much difference to your analysis. It seems like it might, since c<sub>ε</sub> scales with the timescale of the psp.</p></disp-quote><p>It is true that our model does not capture all the details of biological neurons, but our observations regarding the effects of weight reparametrization hold in general. We clarified this in a footnote above Equation 1.</p><disp-quote content-type="editor-comment"><p>4. In both places where you mention spikes (x<sub>i</sub>, above Equation 4 and Y<sup>*</sup>, above Equation 9) you should point out that they are a sum of δ functions. &quot;Spikes&quot; isn't so well defined in this field.</p></disp-quote><p>We agree, which is why we stated this explicitly in Equation 27.</p><disp-quote content-type="editor-comment"><p>5. If you want anybody besides hard core theorists do understand what you're doing, you'll need to be explicit about what p* is: it's Equation 7 with phi replaced by phi*, the target firing rate as a function of voltage. We strongly suggest you do that. In particular, the description &quot;a target firing distribution p*(y∣x<sup>ε</sup>)&quot; doesn't help much, since a firing rate distribution is usually a distribution over firing rates. What you mean, though, is that p* gives you the true probability of a spike in a time interval.</p></disp-quote><p>We agree, which is why we stated this explicitly in Sec. ‘Sketch for the derivation of the somatic natural-gradient learning rule‘, Equations 32 and 33.</p><disp-quote content-type="editor-comment"><p>5. It would be very helpful, in Equation 9, to add</p><p>\approx dt[ (phi – – phi*) – – phi* log(phi/phi*) ]</p><p>(and point out that it's exact in the limit dt --&gt; 0). Then Equation 10 is easy -- just say that phi* is replaced by Y*.</p></disp-quote><p>We agree, but have decided – in line with suggestions from the reviewers – to keep the main manuscript as light as possible on mathematical details/derivations and rather focus on the key arguments and insights. (In this particular case, since Equation 10 is not our result and appears often in other literature, we point to Pfister et al.et al. (2006) for more details.)</p><disp-quote content-type="editor-comment"><p>6. Starting in Equation 10, you switch to a weight without a superscript. We found this very confusing, since the learning rule you wrote down in Equation 10 is exactly the one for w<sup>s</sup>. We strongly suggest you never use w. But if there is a reason to do so, you should be crystal clear about what it is. As far as we could tell, this switch to w is completely unsignalled. So maybe it's a typo?</p></disp-quote><p>We agree, which is why we have stated this explicitly in the text above Equation 11: “In the following, we therefore drop the index from the synaptic weights w to emphasize the parametrization-invariant nature of the natural gradient.” In particular, this means that Equation 13 holds in a more general setting than only for the somatic parametrization. If we use w<sub>s</sub> instead of w, we lose this generality.</p><disp-quote content-type="editor-comment"><p>7. p 4: &quot;which is likely to harm the convergence speed towards an optimal weight configuration.&quot;</p><p>The word &quot;likely&quot; is not all that convincing. Can you be a lot stronger? Are there relevant refs? Or you could point to the simulations in Figure 3.</p></disp-quote><p>We provide an explanation under Equation 3, which is explicitly referenced in the sentence you mention: “However, from a functional perspective, the opposite should be true: dendritic weights should experience a larger change than somatic weights in order to elicit the same effect on the cost.” We used “likely” simply because we want to be cautious. It is possible to imagine (arguably pathological) cases where the somatic learning rate is so large that it prevents convergence, so a dendritic parametrization can help by decreasing the learning rate.</p><disp-quote content-type="editor-comment"><p>8. p 4: &quot;In the following, we therefore drop the index from the synaptic weights w to emphasize the parametrization-invariant nature of the natural gradient.&quot;</p><p>In fact, what's really going on is that you're deriving the natural gradient learning rule (Equation 13) with V given by</p><p>V = sum<sub>i</sub> f(w<sub>i</sub>) x<sub>iε</sub></p><p>which is actually very confusing, since f was used previously to relate w<sup>s</sup> to w<sup>d</sup> (Equation 4). Is the f in Eq 14 the same as the one in Equation 4? If so, it should be w<sup>d</sup> on the right hand side of Equation 14. If not, you should use a different symbol for the function.</p></disp-quote><p>The f here is a generalization of the f in Equation 4. It describes the relationship of an arbitrary weight parametrization to the somatic amplitude parametrization. Since Equation 4 is actually a special case for the dendritic amplitudes, we feel that using f in both cases is justified. We added a sentence below Equation 14 that states this explicitly.</p><disp-quote content-type="editor-comment"><p>As you can see, your notation is confusing us. Our suggestion would be to always use superscripts s or d, and never use w by itself.</p><p>9. Figure 2:</p><p>&quot;(A) During supervised learning, the error between the current and the target state is measured in terms of a cost function defined on the neuron's output space; in our case, this is the manifold formed by the neuronal output distributions p(y, x).&quot;</p><p>What does &quot;defined on the neuron's output space&quot; mean? Doesn't the cost function depend only on the weights, since it's an average over the input-output function? Which is more or less what you say in the next sentence,</p><p>&quot;As the output of a neuron is determined by the strength of incoming synapses, the cost C is an implicit function of the afferent weight vector w.!</p><p>Although we're not sure why you say &quot;implicit&quot; function; isn't it a very explicit function (see point 5 above).</p></disp-quote><p>The cost function does indeed depend on the weights, but only indirectly, via the output. If two particular weight configurations give the same output, the cost function will be the same. This is illustrated in Figure 2A. We agree that “implicit” was not the right word here and replaced it with “indirectly”. C is defined on the space of p, but the p themselves depend on w, so C depends indirectly on w.</p><disp-quote content-type="editor-comment"><p>&quot;If, instead, we follow the gradient on the output manifold itself, it becomes independent of the underlying parametrization.&quot;</p><p>Since we don't know what the output manifold is (presumably the statistical manifold in panel A? not that that helps), this sentence doesn't make sense to us.</p></disp-quote><p>In the paragraphs directly above Equation 11 we discuss the output manifold in detail. In particular, we say explicitly that “the set of our neuron’s realizable output distributions[,] forms a Riemannian manifold” and, in the first sentence under Figure 2, that “[the] cost function [is] defined on the neuron's output space; in our case, this is the manifold formed by the neuronal output distributions p(y,x)”. These distributions are, in turn, defined in Equation 7 and 8.</p><disp-quote content-type="editor-comment"><p>&quot;In contrast, natural-gradient learning will locally correct for distortions arising from non-optimal parametrizations.&quot;</p><p>This is a highly nontrivial statement, and there's nothing in the manuscript so far that makes this obvious. I think a reference is needed here. Or point the reader to Figure 3 for an example?</p></disp-quote><p>Since natural gradient multiplies the Euclidean gradient by the inverse Fisher information matrix G<sup>-1</sup> (and G is a Riemannian metric), provides the relationship between distances on the parameter space and the output manifold, we believe this statement is justified. We added a reference to Figure 3 as suggested.</p><disp-quote content-type="editor-comment"><p>10. Equation 13: You may not like components, but it would be extremely helpful to put them in Equation 13, or maybe add an equation with components to make it clear. There is a precedent; you use components in Equation 4. And let's face it, anybody who isn't mathematically competent will have been lost long ago, and anybody who is mathematically competent will want to be sure what's going on. And things are actually ambiguous, since it's really hard to tell what's a vector and what's a scalar. In particular, γ<sub>u</sub> should be treated as a vector, even though it's not bold, so it's not immediately clear whether γ<sub>s</sub> should also be treated as a vector. If nothing else, you should use γ<sub>u</sub> {\bf 1}; that would help a little. Also, the convention these days when writing f(w) is for f not to be bold, but instead define it as a pointwise nonlinearity. You don't have to follow conventions, but it does make it easier on the reader.</p><p>The same comment applies to Equation 17.</p></disp-quote><p>Thank you for pointing this out – yes, we should have multiplied γ with the unit vector explicitly, as we did in other parts of the document. The vector notation was a conscious choice on our part, with one important reason being the avoidance of stacked indices. Given this decision, we used bold notation for vector-valued functions such as <bold>f</bold> to help the reader differentiate them from scalar-valued ones such as C.</p><disp-quote content-type="editor-comment"><p>11. In Equation 14, presumably w<sup>s</sup> on the left hand side should be bold? And why not use indices, as in Equation 4? Much more clear, and fewer symbols.</p></disp-quote><p>Good catch, thank you!</p><disp-quote content-type="editor-comment"><p>12. p 6, typo: &quot;inactive inactive&quot;.</p></disp-quote><p>Good catch, thank you!</p><disp-quote content-type="editor-comment"><p>13. Two questions and one comment about Figure 3F. First, why don't the orange and blue lines start in the same place? Second, the blue line doesn't appear to be saturating. Can the Euclidean gradient do better than the natural gradient? If so, that's important to point out. But maybe it's not true, and is a by-product of your optimization method?</p></disp-quote><p>We describe this in the simulation details (section 6.4.1), but we added a few clarifying words: “For the learning curves in Figure 3F, initial and target weight components were chosen randomly (but identically for the two curves) from a uniform distribution on U(-1/n,1/n), corresponding to maximal PSP amplitudes between -600 µV and 600 µV for the simulation with n=100 input neurons. Learning curves were averaged over 1000 initial and target weight configurations.” This implies that the orange and blue line do indeed start at the same point, but it is just a bit difficult to see since they partly overlap.</p><p>Furthermore, the Euclidean gradient descent does converge, which is a bit more obvious in Figure 9A. However, we wanted to highlight the difference between natural-gradient and Euclidean-gradient descent and therefore chose to focus on the most relevant part of the learning curves. Please note also Figure 12C-F, which shows the convergence behavior displayed in Figure 3F is consistent over multiple input scenarios.</p><p>Since we did not provide a formal proof that natural-gradient-descent learning outperforms Euclidean-gradient-descent learning, there could be special scenarios where Euclidean-gradient-descent learning exhibits, on average, a more favorable convergence behavior than natural gradient. However, we did not observe such a case in our simulations, which is consistent with the findings from other studies of natural-gradient descent.</p><disp-quote content-type="editor-comment"><p>And the comment: presumably the D<sub>KL</sub> term on the y-axis has a factor of dt. That makes it pretty much impossible to interpret, since we don't know what dt is (probably it's somewhere, but it's not in an obvious place). We suggest removing the factor of dt, and making it clear that when phi is close to phi*, what's being plotted is &lt;(phi-phi*)^2/2 phi*&gt;. That will make the plot much easier to interpret.</p></disp-quote><p>We see how this relates to you point #5, but we think that Figure 9C (distance of firing rates) should provide additional context to interpret Figure 3F, should the need arise.</p><disp-quote content-type="editor-comment"><p>14. p 8: &quot;As a result, the effect of synaptic plasticity on the neuron's output is independent of the synapse location, since dendritic attenuation is precisely counterbalanced by weight update amplification.&quot;</p><p>Because of the term γ<sub>w</sub> w<sub>d</sub>, this is true only if w<sup>d</sup> \propto 1/α(d). But, as you point out later, this doesn't have to be the case. So this statement needs to be modified. Maybe it's approximately true?</p></disp-quote><p>This might be a misunderstanding. Equation 17 is a special case of Equation 13 for exactly the case where Equation 16 holds. Please note the general form of the last term in Equation 13 and refer to Figure 8B for a detailed explanation on how attenuation is canceled out for natural-gradient-descent learning.</p><disp-quote content-type="editor-comment"><p>15. Figure 5, several comments:</p><p>In the equation in panel H, r should be in the numerator, not the denominator, and a factor of ε<sub>02</sub> is missing. In addition, the term on the right hand side can be simplified:</p><p>σ<sup>2</sup>(x<sup>ε</sup>) = r ε<sub>02</sub>/2(tau<sub>1</sub> + tau<sub>2</sub>).</p><p>Also it would be a good idea to switch to tau<sub>s</sub> and tau<sub>m</sub> here rather than tau<sub>1</sub> and tau<sub>2</sub>, to be consistent with Methods, and to not clash with the tau's in panels A-B.</p><p>And in panels A-B, presumably it should be tau{<sub>si</sub>} rather than tau<sub>i</sub>?. Also, tau<sub>m</sub> should be reported here.</p></disp-quote><p>Good ideas, thank you, we have incorporated all of them into Figure 5.</p><disp-quote content-type="editor-comment"><p>It would make it a lot easier on the reader if you wrote down an equation for cε, which isn't so complicated: c<sub>ε</sub>/r<sub>i</sub> = 2 (tau<sub>m</sub> + tau<sub>s</sub>)/ε<sub>02</sub> r<sub>i</sub>. Equation 19 is a natural place to do that.</p></disp-quote><p>Agreed, we added this expression to Equation 31 (see also # 23).</p><disp-quote content-type="editor-comment"><p>Because of the other two terms in Equation 17, it's not true that &quot;Natural-gradient learning scales inversely with input variance.&quot; It should be clear that this is approximate.</p></disp-quote><p>You are certainly right, which is why we said “inversely” rather than “inversely proportional”. We added an “approximately” to make this unequivocal.</p><disp-quote content-type="editor-comment"><p>16. In Equation 19, it should be x<sub>iε</sub> on the left hand side, not x<sup>ε</sup>.</p></disp-quote><p>Good catch, thank you!</p><disp-quote content-type="editor-comment"><p>17. p 10: &quot;Furthermore, it is also consistent with data from Aitchison and Latham (2014) and Aitchison et al.et al. (2021), as well as with their observation of an inverse dependence on presynaptic firing rates, although our interpretation is different from theirs.&quot;</p><p>There is no justification for this statement: in Aitchison et al.et al. a plot of Δ w/w versus r showed 1/sqrt(r) scaling. That would be consistent with the theory in this paper only if the weight scales as 1/sqrt(r). It might, but without checking, you can't make that claim.</p><p>That data is weak, so the fact that you don't fit it is hardly the end of the world. However, what's more important is to point out that the theory here makes very different predictions that the theory in Aitchison et al.et al. That way, experimentalists will have something to do.</p><p>Finally, the inverse scaling with firing rate must eventually saturate, since there's a limit to how big weight changes can be. You should comment on this, if briefly.</p></disp-quote><p>We suspect that this is simply a matter of wording, as we also pointed out above, under #15. We used “inverse” in an approximate, qualitative sense, and not as “inversely proportional”. It is meant to be taken as “if the presynaptic rate becomes larger, the weight changes become smaller”. We believe the statement to be correct and in line with Aitchison et al.et al., who write “the learning rate […] increases as the synapse’s uncertainty […] increases” and “the synapse’s uncertainty should fall as the presynaptic firing rate increases”. We adapted the corresponding sentence in our manuscript to mitigate the potential misunderstanding that our model makes the same quantitative predictions.</p><disp-quote content-type="editor-comment"><p>18. Figure 7, we're pretty lost, for several reasons:</p><p>a. Because homosynaptic scales as 1/r, it's not possible to have unstimulated synapses (for which r=0); at least not with the current derivation. If you want to have unstimulated synapses, you need to show that it is indeed possible, by computing G when r<sub>i</sub>=0 for some of the i's.</p></disp-quote><p>We already addressed this in our response to point 8b of the previous review: “You are right, and we addressed this misunderstanding in the revised caption of Figure 7 and Sec. ‘Comparison of homo- and heterosynaptic plasticity’ (lines 677-679). In the simulation, the synapses received no input spikes at all. The assumption of infinitesimally small input rates [in the simulated learning rule] was simply an argument that allowed us to avoid division by zero in the homosynaptic term. Biologically, this would simply correspond to zero homosynaptic plasticity for zero input.” Please note that line numbers and highlighted text changes are now, necessarily, different; the referenced sentence is the second one in the first paragraph of Sec. ‘Comparison of homo- and heterosynaptic plasticity’.</p><disp-quote content-type="editor-comment"><p>b. From the bottom of page 27, γ<sub>u</sub> \approx ε<sub>0</sub> c<sub>ε</sub>. Thus, the first two plasticity terms are</p><p>c<sub>ε</sub> (x<sup>ε</sup>/r – ε<sub>0</sub>).</p><p>Because = r ε<sub>0</sub>, these two terms approximately cancel. Which means everything should be driven by the last term, c<sub>w</sub> V f(w). This doesn't seem consistent with the explanation in Figure 7.</p></disp-quote><p>If we understand correctly, you are looking at the average weight change &lt;\Δ w&gt;. This is an average over a product between the postsynaptic error and the parenthesis containing the sum over the three terms (cf. Equation 13). The average of this product is not the product of the two averages, because the error is correlated with the input, and plasticity is driven by this very correlation (and of course the last term c<sub>w</sub> V f(w)).</p><disp-quote content-type="editor-comment"><p>c. Because of the term c<sub>w</sub> V f(w), shouldn't there be a strong dependence on the unstimulated weights in panels B-D?</p></disp-quote><p>The purpose of these panels is to address this exact question. Figure 7B shows the change in the stimulated weights, so there is no dependence on the unstimulated weights (since the influence of the term c<sub>w</sub> V f(w) is component-wise). In contrast, for the unstimulated weights (Figure 7C), we see a clear effect of this term which particularly manifests itself in the switch from potentiation to depression in the upper right corner of the panel. This dependence on the unstimulated weight is also clearly visible in the upper right corner of Figure 7D.</p><disp-quote content-type="editor-comment"><p>All this should be clarified.</p><p>19. p 13: &quot;We further note an interesting property of our learning rule, which it inherits directly from the Fisher information metric that underlies natural gradient descent, namely invariance under sufficient statistics (Cencov, 1972).&quot; What does &quot;invariance under sufficient statistics&quot; refer to?</p></disp-quote><p>A parameter of a probability distribution can often be described by a statistic (such as the sample mean) which is sufficient if it contains all necessary information about the parameter. The distributions of a sufficient statistic form themselves a parametric model and therefore a Riemannian manifold, with a one-to-one relationship between the original parametric model and the one for the sufficient statistic. Due to the invariance property of the Fisher metric, the distances of corresponding points will be equal up to a constant factor. We felt the need to mention this distinguishing property of the Fisher metric, since it makes it stand out from other Riemannian metrics on statistical manifolds and because it is relevant for biological neurons, as explained in the paragraph. Since we believe that the discussion is not a good place to elaborate on mathematical details – again, to not distract the reader from the main message – we refer to the original literature, in this case Cencov, 1972.</p><disp-quote content-type="editor-comment"><p>20. p 13: &quot;A further prediction that follows from our plasticity rule is the normalization of weight changes by the presynaptic variance. We would thus anticipate that increasing the jitter in presynaptic spike trains should reduce LTP in standard plasticity induction protocols.&quot;</p><p>Presumably this statement comes from the fact that the learning rate scales inversely with the variance of the filtered spike train. However, it's not clear that jittering spike trains increases the variance. What would definitely increase the variance, though, is temporal correlations. Maybe this could be swapped in for jittering? Eiththeyer that, or explain why jittering increases variance. And also it would be nice to refer the reader to the dependence of the learning rate on variance.</p></disp-quote><p>We adapted the sentence as requested.</p><disp-quote content-type="editor-comment"><p>21. You should comment on where you think the teacher spike train comes from. Presumably it's delivered by PSPs at the soma, but those don't propagate back to synapses. How do you envision the teacher spike trains communicating with the synapses? Even if the answer is &quot;we don't know&quot;, it's important to inform the reader - this may simply be an avenue for future research.</p></disp-quote><p>We did, directly below Equation 10, where the teacher spike train first appears in the postsynaptic error term: “On the single-neuron level, a possible biological implementation has been suggested by Urbanczik and Senn (2014), who demonstrated how a neuron may exploit its morphology to store errors, an idea that was recently extended to multilayer networks (Sacramento et al.et al., 2017; Haider et al.et al., 2021a).”</p><disp-quote content-type="editor-comment"><p>22. In Equations 29 and 30, you should explain why you use \approx. Presumably because that's because you assumed a constant firing rate?</p></disp-quote><p>Exactly, “\approx” becomes “=” for constant input rates. We added an explanatory sentence below Equation 31.</p><disp-quote content-type="editor-comment"><p>23. Equation 31: it would be extremely useful to point out that c<sub>ε</sub> = 2(tau<sub>m</sub> + tau<sub>s</sub>)/ε<sub>0</sub>^2, along with a reference (or calculate it yourself somewhere).</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>24. After Equation 31, &quot;Unless indicated otherwise, simulations were performed with a membrane time constant tau<sub>m</sub> = 10 ms and a synaptic time constant τs = 3 ms. Hence, USPs had an amplitude of 60 mV&quot;.</p><p>Doesn't the amplitude of the USPs depend on ε<sub>0</sub>? And 60 mV seems pretty big. Is that a typo?</p></disp-quote><p>Yes, it does, and we explicitly say that ε<sub>0</sub> = 1 mV ms just a few lines above. The size of the USP does not matter – the U stands for unweighted, hence our insistence on keeping the acronym to make this explicit (see our reply to #2). Just one sentence further down from the one you cite, we discuss the actual size of PSPs. (But also in other places, for example Sec. ‘Distance dependence of amplitude changes’.)</p><disp-quote content-type="editor-comment"><p>25. Equation 34: Missing parentheses around Σ<sub>USP</sub> w<sup>s</sup> in the second term in parentheses.</p></disp-quote><p>Good catch, thank you!</p><disp-quote content-type="editor-comment"><p>26. Equation 24 and the line above it: should w<sub>i</sub> be w<sub>id</sub>? If not, you shouldn't use f, since that was used to translate somatic to dendritic amplitude.</p></disp-quote><p>We are not sure what you mean, since Equation 24 does not contain a w<sub>i</sub>. Could this be a typo? Maybe our reply to # 8 regarding the use of f is helpful in this context.</p><disp-quote content-type="editor-comment"><p>27. Equation 115: Should it be Theta(V-theta)?</p></disp-quote><p>Good catch, thank you!</p><disp-quote content-type="editor-comment"><p>28. Equation 117 is identical to Equation 67. Did you mean to drop the term (phi')<sub>2</sub>/phi?</p></disp-quote><p>Yes, thank you!</p><disp-quote content-type="editor-comment"><p>Also, (phi')<sup>2</sup>/phi = 1 when V &gt; theta. But presumably it's equal to 0 when V &lt; theta. If so, Equation 117 should be</p><p>G(w) = E(dt Theta(V-theta) x x<sup>T</sup>).</p><p>if that's correct, then the integrals I<sub>1</sub>-I<sub>3</sub> do not reduce to the values given in Equations 118-120.</p></disp-quote><p>You are right and thank you for pointing this out. We think that everything should work out fine if we assume that the mass of the membrane potential distribution is above theta. While this is not necessarily biologically realistic, we think that this result is still instructive due to its simplicity. We adapted the text for more clarity.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The authors have done a good job in the revision.</p><p>A number of small suggestions remaining:</p><p>– Equation 13. Wouldn't it be easier to write this for a single component <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>……</p></disp-quote><p>We agree that sometimes this can be easier, but in this case, we decided to use vector notation to avoid index clutter, see also our reply to #10 of Reviewer 1.</p><disp-quote content-type="editor-comment"><p>– Figure 3 shows temporal structure in the teacher signal but this is nowhere explained in the main text.</p></disp-quote><p>We added a few clarifying sentences to the caption of Figure 3: “During learning, the firing patterns of the student neuron align to those of the teacher neuron. The structure in these patterns comes from autocorrelations in this instantaneous rate. These, in turn, are due to mechanisms such as the membrane filter (as seen in the voltage traces) and</p><p>the nonlinear activation function.”</p><disp-quote content-type="editor-comment"><p>– Fig3D+E perhaps the axis or caption can indicate whether w1,2 is 10 or 50Hz.</p></disp-quote><p>Good idea, thank you! We added a corresponding sentence to the caption.</p><disp-quote content-type="editor-comment"><p>– Figure 4A: Shouldn't the purple top solid curve (dendritic voltage) be taller than the solid orange curve?</p></disp-quote><p>Yes, absolutely, we adapted the sketch to make this clearer.</p><disp-quote content-type="editor-comment"><p>– Fig5D+E+F might look better with the position of the y-axis left instead of right.</p></disp-quote><p>We chose the axes on the left so the histograms look more familiar and just rotated by 90° CCW.</p></body></sub-article></article>