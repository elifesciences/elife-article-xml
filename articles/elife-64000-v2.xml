<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64000</article-id><article-id pub-id-type="doi">10.7554/eLife.64000</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Ecology</subject></subj-group></article-categories><title-group><article-title>TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-210470"><name><surname>Walter</surname><given-names>Tristan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8604-7229</contrib-id><email>twalter@ab.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-154185"><name><surname>Couzin</surname><given-names>Iain D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8556-4558</contrib-id><email>icouzin@ab.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Max Planck Institute of Animal Behavior</institution><addr-line><named-content content-type="city">Radolfzell</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Centre for the Advanced Study of Collective Behaviour, University of Konstanz</institution><addr-line><named-content content-type="city">Konstanz</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Department of Biology, University of Konstanz</institution><addr-line><named-content content-type="city">Konstanz</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Lentink</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Rutz</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution>University of St Andrews</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>02</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e64000</elocation-id><history><date date-type="received" iso-8601-date="2020-10-13"><day>13</day><month>10</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-02-25"><day>25</day><month>02</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Walter and Couzin</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Walter and Couzin</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64000-v2.pdf"/><abstract><p>Automated visual tracking of animals is rapidly becoming an indispensable tool for the study of behavior. It offers a quantitative methodology by which organisms’ sensing and decision-making can be studied in a wide range of ecological contexts. Despite this, existing solutions tend to be challenging to deploy in practice, especially when considering long and/or high-resolution video-streams. Here, we present TRex, a fast and easy-to-use solution for tracking a large number of individuals simultaneously using background-subtraction with real-time (60 Hz) tracking performance for up to approximately 256 individuals and estimates 2D visual-fields, outlines, and head/rear of bilateral animals, both in open and closed-loop contexts. Additionally, TRex offers highly accurate, deep-learning-based visual identification of up to approximately 100 unmarked individuals, where it is between 2.5 and 46.7 times faster, and requires 2–10 times less memory, than comparable software (with relative performance increasing for more organisms/longer videos) and provides interactive data-exploration within an intuitive, platform-independent graphical user-interface.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>tracking</kwd><kwd>s. gregaria</kwd><kwd>c. cyphergaster</kwd><kwd>posture estimation</kwd><kwd>visual field</kwd><kwd>p. reticulata</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd><kwd>Zebrafish</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000154</institution-id><institution>Division of Integrative Organismal Systems</institution></institution-wrap></funding-source><award-id>IOS-1355061</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-19-1-2556</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>EXC 2117-422037984</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Struktur- und Innovationsfunds fuer die Forschung of the State of Baden-Wuerttemberg</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Being fast, memory efficient, easy to use, and with powerful integrated tools, TRex will lower barriers of entry into, and enable more ambitious approaches to, the quantitative study of behavior.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Tracking multiple moving animals (and multiple objects, generally) is important in various fields of research such as behavioral studies, ecophysiology, biomechanics, and neuroscience (<xref ref-type="bibr" rid="bib18">Dell et al., 2014</xref>). Many tracking algorithms have been proposed in recent years (<xref ref-type="bibr" rid="bib52">Ohayon et al., 2013</xref>, <xref ref-type="bibr" rid="bib22">Fukunaga et al., 2015</xref>, <xref ref-type="bibr" rid="bib11">Burgos-Artizzu et al., 2012</xref>, <xref ref-type="bibr" rid="bib63">Rasch et al., 2016</xref>), often limited to/only tested with a particular organism (<xref ref-type="bibr" rid="bib29">Hewitt et al., 2018</xref>, <xref ref-type="bibr" rid="bib9">Branson et al., 2009</xref>) or type of organism (e.g. protists, <xref ref-type="bibr" rid="bib54">Pennekamp et al., 2015</xref>; fly larvae and worms, <xref ref-type="bibr" rid="bib64">Risse et al., 2017</xref>). Relatively few have been tested with a range of organisms and scenarios (<xref ref-type="bibr" rid="bib58">Pérez-Escudero et al., 2014</xref>, <xref ref-type="bibr" rid="bib70">Sridhar et al., 2019</xref>, <xref ref-type="bibr" rid="bib66">Rodriguez et al., 2018</xref>). Furthermore, many existing tools only have a specialized set of features, struggle with very long or high-resolution (≥4 K) videos, or simply take too long to yield results. Existing fast algorithms are often severely limited with respect to the number of individuals that can be tracked simultaneously; for example, xyTracker (<xref ref-type="bibr" rid="bib63">Rasch et al., 2016</xref>) allows for real-time tracking at 40 Hz while accurately maintaining identities, and thus is suitable for closed-loop experimentation (experiments where stimulus presentation can depend on the real-time behaviors of the individuals, for example <xref ref-type="bibr" rid="bib3">Bath et al., 2014</xref>, <xref ref-type="bibr" rid="bib10">Brembs and Heisenberg, 2000</xref>, <xref ref-type="bibr" rid="bib6">Bianco and Engert, 2015</xref>), but has a limit of being able to track only five individuals simultaneously. ToxTrac (<xref ref-type="bibr" rid="bib66">Rodriguez et al., 2018</xref>), a software comparable to xyTracker in it’s set of features, is limited to 20 individuals and relatively low frame-rates (≤25fps). Others, while implementing a wide range of features and offering high-performance tracking, are costly and thus limited in access (<xref ref-type="bibr" rid="bib51">Noldus et al., 2001</xref>). Perhaps with the exception of proprietary software, one major problem at present is the severe fragmentation of features across the various software solutions. For example, experimentalists must typically construct work-flows from many individual tools: One tool might be responsible for estimating the animal’s positions, another for estimating their posture, another one for reconstructing visual fields (which in turn probably also estimates animal posture, but does not export it in any way) and one for keeping identities – correcting results of other tools post-hoc. It can take a very long time to make them all work effectively together, adding what is often considerable overhead to behavioral studies.</p><p>TRex, the software released with this publication (available at trex.run under an Open-Source license), has been designed to address these problems, and thus to provide a powerful, fast and easy to use tool that will be of use in a wide range of behavioral studies. It allows users to track moving objects/animals, as long as there is a way to separate them from the background (e.g. static backgrounds, custom masks, as discussed below). In addition to the positions of individuals, our software provides other per-individual metrics such as body shape and, if applicable, head-/tail-position. This is achieved using a basic posture analysis, which works out of the box for most organisms, and, if required, can be easily adapted for others. Posture information, which includes the body center-line, can be useful for detecting for example courtship displays and other behaviors that might not otherwise be obvious from mere positional data. Additionally, with the visual sense often being one of the most important modalities to consider in behavioral research, we include the capability for users to obtain a computational reconstruction of the visual fields of all individuals (<xref ref-type="bibr" rid="bib71">Strandburg-Peshkin et al., 2013</xref>; <xref ref-type="bibr" rid="bib68">Rosenthal et al., 2015</xref>). This not only reveals which individuals are visible from an individual’s point-of-view, as well as the distance to them, but also which parts of others’ bodies are visible.</p><p>Included in the software package is a task-specific tool, TGrabs, that is employed to pre-process existing video files and which allows users to record directly from cameras capable of live-streaming to a computer (with extensible support from generic webcams to high-end machine vision cameras). It supports most of the above-mentioned tracking features (positions, posture, visual field) and provides access to results immediately while continuing to record/process. This not only saves time, since tracking results are available immediately after the trial, but makes closed-loop support possible for large groups of individuals (≤ 128 individuals). TRex and TGrabs are written in <monospace>C++</monospace> but, as part of our closed-loop support, we are providing a <monospace>Python</monospace>-based general scripting interface which can be fully customized by the user without the need to recompile or relaunch. This interface allows for compatibility with external programs (e.g. for closed-loop stimulus-presentation) and other custom extensions.</p><p>The fast tracking described above employs information about the kinematics of each organism in order to try to maintain their identities. This is very fast and useful in many scenarios, for example where general assessments about group properties (group centroid, alignment of individuals, density, etc.) are to be made. However, when making conclusions about <italic>individuals</italic> instead, maintaining identities perfectly throughout the video is a critical requirement. Every tracking method inevitably makes mistakes, which, for small groups of two or three individuals or short videos, can be corrected manually – at the expense of spending much more time on analysis, which rapidly becomes prohibitive as the number of individuals to be tracked increases. To make matters worse, when multiple individuals stay out of view of the camera for too long (such as if individuals move out of frame, under a shelter, or occlude one another) there is no way to know who is whom once they re-emerge. With no baseline truth available (e.g. using physical tags as in <xref ref-type="bibr" rid="bib2">Alarcón‐Nieto et al., 2018</xref>, <xref ref-type="bibr" rid="bib50">Nagy et al., 2013</xref>; or marker-less methods as in <xref ref-type="bibr" rid="bib58">Pérez-Escudero et al., 2014</xref>, <xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref>, <xref ref-type="bibr" rid="bib63">Rasch et al., 2016</xref>), these mistakes cannot be corrected and accumulate over time, until eventually all identities are fully shuffled. To solve this problem (and without the need to mark, or add physical tags to individuals), TRex can, at the cost of spending more time on analysis (and thus not during live-tracking), automatically learn the identity of up to approximately 100 unmarked individuals based on their visual appearance. This machine-learning-based approach, herein termed <italic>visual identification</italic>, provides an independent source of information on the identity of individuals, which is used to detect and correct potential tracking mistakes without the need for human supervision.</p><p>In this paper, we evaluate the most important functions of our software in terms of speed and reliability using a wide range of experimental systems, including termites, fruit flies, locusts, and multiple species of schooling fish (although we stress that our software is not limited to such species).</p><p>Specifically regarding the visual identification of unmarked individuals in groups, <monospace>idtracker.ai</monospace> is currently state-of-the-art, yielding high-accuracy (&gt; 99% in most cases) in maintaining consistent identity assignments across entire videos (<xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref>). Similarly to TRex, this is achieved by training an artificial neural network to visually differentiate between individuals, and using identity predictions from this network to avoid/correct tracking mistakes. Both approaches work without human supervision, and are limited to approximately 100 individuals. Given that <monospace>idtracker.ai</monospace> is the only currently available tool with visual identification for such large groups of individuals, and also because of the quality of results, we will use it as a benchmark for our visual identification system. Results will be compared in terms of both accuracy and computation speed, showing TRex’ ability to achieve the same high level of accuracy but typically at far higher speeds, and with a much reduced memory requirement.</p><p>TRex is platform-independent and runs on all major operating systems (Linux, Windows, macOS) and offers complete batch processing support, allowing users to efficiently process entire sets of videos without requiring human intervention. All parameters can be accessed either through settings files, from within the graphical user interface (or <italic>GUI</italic>), or using the command-line. The user interface supports off-site access using a built-in web-server (although it is recommended to only use this from within a secure VPN environment). Available parameters are explained in the documentation directly as part of the GUI and on an external website (see below). Results can be exported to independent data-containers (NPZ, or CSV for plain-text type data) for further analyses in software of the user’s choosing. We will not go into detail regarding the many GUI functions since albeit being of great utility to the researcher, they are only the means to easily apply the features presented herein. Some examples will be given in the main text and appendix, but a comprehensive collection of all of them, as well as detailed documentation, is available in the up-to-date online-documentation which can be found at trex.run/docs.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Our software package consists of two task-specific tools, TGrabs and TRex, with different specializations. TGrabs is primarily designed to connect to cameras and to be very fast. It employs the same program code as TRex to achieve real-time online tracking, such as could be employed for closed-loop experiments (the user can launch TGrabs from the opening dialog of TRex). However, its focus on speed comes at the cost of not having access to the rich GUI or more sophisticated (and thus slower) processing steps, such as deep-learning-based identification, that TRex provides. TRex focusses on the more time-consuming tasks, as well as visual data exploration, re-tracking existing results – but sometimes it simply functions as an easier-to-use graphical interface for tracking and adjusting parameters. Together they provide a wide range of capabilities to the user and are often used in sequence as part of the same work-flow. Typically, such a sequence can be summarized in four stages (see also <xref ref-type="fig" rid="fig1">Figure 1</xref> for a flow diagram):</p><list list-type="order"><list-item><p>Segmentation in TGrabs. When recording a video or converting a previously recorded file (e.g. MP4, .AVI, etc.), it is segmented into background and foreground-objects (<monospace>blobs</monospace>), the latter typically being the entities to be tracked. Results are saved to a custom, non-proprietary video format (<monospace>PV</monospace>) (<xref ref-type="fig" rid="fig2">Figure 2a</xref>).</p></list-item><list-item><p>Tracking the video, either directly in TGrabs, or in TRex after pre-processing, with access to customizable visualizations and the ability to change tracking parameters on-the-fly. Here, we will describe two types of data available within TRex, 2D posture- and visual-field estimation, as well as real-time applications of such data (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p></list-item><list-item><p>Automatic identity correction (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), a way of utilizing the power of a trained neural network to perform visual identification of individuals, is available in TRex only. This step may not be necessary in many cases, but it is the only way to guarantee consistent identities throughout the video. It is also the most processing-heavy (and thus usually the most time-consuming) step, as well as the only one involving machine learning. All previously collected posture- and other tracking-related data are utilized in this step, placing it late in a typical workflow.</p></list-item><list-item><p>Data visualization is a critical component of any research project, especially for unfamiliar datasets, but manually crafting one for every new experiment can be very time-consuming. Thus, TRex offers a universal, highly customizable, way to make all collected data available for interactive exploration (<xref ref-type="fig" rid="fig2">Figure 2d</xref>) – allowing users to change many display options and recording video clips for external playback. Tracking parameters can be adjusted on the fly (many with visual feedback) – important for example when preparing a closed-loop feedback with a new species or setup.</p></list-item></list><p>Below we assess the performance of our software regarding three properties that are most important when using it (or in fact any tracking software) in practice: (i) The time it takes to perform tracking (ii) the time it takes to perform automatic identity correction and (iii) the peak memory consumption when correcting identities (since this is where memory consumption is maximal), as well as (iv) the accuracy of the produced trajectories after visual identification.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Videos are typically processed in four main stages, illustrated here each with a list of prominent features.</title><p>Some of them are accessible from both TRex and TGrabs, while others are software specific (as shown at the very top). (<bold>a</bold>) The video is either recorded directly with our software (TGrabs), or converted from a pre-recorded video file. Live-tracking enables users to perform closed-loop experiments, for which a virtual testing environment is provided. (<bold>b</bold>) Videos can be tracked and parameters adjusted with visual feedback. Various exploration and data presentation features are provided and customized data streams can be exported for use in external software. (<bold>c</bold>) After successful tracking, automatic visual identification can, optionally, be used to refine results. An artificial neural network is trained to recognize individuals, helping to automatically correct potential tracking mistakes. In the last stage, many graphical tools are available to users of TRex, a selection of which is listed in (<bold>d</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig1-v2.tif"/></fig><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>An overview of the interconnection between TRex, TGrabs and their data in- and output formats, with titles on the left corresponding to the stages in 1.</title><p>Starting at the top of the figure, video is either streamed to TGrabs from a file or directly from a compatible camera. At this stage, preprocessed data are saved to a .pv file which can be read by TRex later on. Thanks to its integration with parts of the TRex code, TGrabs can also perform online tracking for limited numbers of individuals, and save results to a .results file (that can be opened by TRex) along with individual tracking data saved to numpy data-containers (.npz) or standard CSV files, which can be used for analysis in third-party applications. If required, videos recorded directly using TGrabs can also be streamed to a .mp4 video file which can be viewed in commonly available video players like VLC.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig2-v2.tif"/></fig><media id="fig2video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-64000-fig2-video1.mp4"><label>Figure 2—video 1.</label><caption><title>This video shows an overview of the typical chronology of operations when using our software.</title><p>Starting with the raw video, segmentation using TGrabs (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) is the first and only step that is not optional. Tracking (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) and posture estimation (both also available for live-tracking in TGrabs) are usually performed in that order, but can be partly parallelized (e.g. performing posture estimation in parallel for all individuals). Visual identification (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) is only available in TRex due to relatively long processing times. All clips from this composite video have been recorded directly in TRex. <ext-link ext-link-type="uri" xlink:href="https://youtu.be/g9EOi7FZHM0">https://youtu.be/g9EOi7FZHM0</ext-link>.</p></caption></media></fig-group><p>While accuracy is an important metric and specific to identification tasks, time and memory are typically of considerable practical importance for all tasks. For example, tracking-speed may be the difference between only being able to run a few trials or producing more reliable results with a much larger number of trials. In addition, tracking speed can make a major difference as the number of individuals increases. Furthermore, memory constraints can be extremely prohibitive making tracking over long video sequences and/or for a large number of individuals extremely time-consuming, or impossible, for the user.</p><p>In all of our tests, we used a relatively modest computer system, which could be described as a mid-range consumer or gaming PC:</p><list list-type="bullet"><list-item><p>Intel Core i9-7900X CPU</p></list-item><list-item><p>NVIDIA Geforce 1080 Ti</p></list-item><list-item><p>64 GB RAM</p></list-item><list-item><p>NVMe PCIe x4 hard-drive</p></list-item><list-item><p>Debian bullseye (<ext-link ext-link-type="uri" xlink:href="https://www.debian.org/devel/debian-installer/">debian.org</ext-link>)</p></list-item></list><p>As can be seen in the following sections (memory consumption, processing speeds, etc.) using a high-end system is not necessary to run TRex and, anecdotally, we did not observe noticeable improvements when using a solid state drive versus a normal hard drive. A video card (presently an NVIDIA card due to the requirements of TensorFlow) is recommended for tasks involving visual identification as such computations will take much longer without it – however, it is not required. We decided to employ this system due to having a relatively cheap, compatible graphics card, as well as to ensure that we have an easy way to produce direct comparisons with <monospace>idtracker.ai</monospace> – which according to their website requires large amounts of RAM (32 – 128 GB, idtrackerai online documentation) and a fast solid-state drive.</p><p><xref ref-type="table" rid="table1">Table 1</xref> shows the entire set of videos used in this paper, which have been obtained from multiple sources (credited under the table) and span a wide range of different organisms, demonstrating TRex’ ability to track anything as long as it moves occasionally. Videos involving a large number (&gt; 100) of individuals are all the same species of fish since these were the only organisms we had available in such quantities. However, this is not to say that only fish could be tracked efficiently in these quantities. We used the full dataset with up to 1024 individuals in one video (Video 0) to evaluate raw tracking speed without visual identification and identity corrections (next sub-section). However, since such numbers of individuals exceed the capacity of the neural network used for automatic identity corrections (compare also <xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref> who used a similar network), we only used a subset of these videos (videos 7 through 16) to look specifically into the quality of our visual identification in terms of keeping identities and its memory consumption.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>A list of the videos used in this paper as part of the evaluation of TRex, along with the species of animals in the videos and their common names, as well as other video-specific properties.</title><p>Videos are given an incremental ID, to make references more efficient in the following text, which are sorted by the number of individuals in the video. Individual quantities are given accurately, except for the videos with more than 100 where the exact number may be slightly more or less. These videos have been analyzed using TRex’ dynamic analysis mode that supports unknown quantities of animals. Videos 7 and 8, as well as 13–11, are available as part of the original idtracker paper (<xref ref-type="bibr" rid="bib58">Pérez-Escudero et al., 2014</xref>). Many of the videos are part of yet unpublished data: Guppy videos have been recorded by A. Albi, videos with sunbleak (Leucaspius delineatus) have been recorded by D. Bath. The termite video has been kindly provided by H. Hugo and the locust video by F. Oberhauser. Due to the size of some of these videos (&gt;150 GB per video), they have to be made available upon specific request. Raw versions of these videos (some trimmed), as well as full preprocessed versions, are available as part of the dataset published alongside this paper (<xref ref-type="bibr" rid="bib75">Walter et al., 2020</xref>).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>ID</th><th>Species</th><th>Common</th><th># ind.</th><th>Fps (Hz)</th><th>Duration</th><th>Size (Px<sup>2</sup>) (<inline-formula><mml:math id="inf1"><mml:msup><mml:mi>px</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>)</th></tr></thead><tbody><tr><td>0</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>1024</td><td>40</td><td>8 min 20 s</td><td>3866 × 4048</td></tr><tr><td>1</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>512</td><td>50</td><td>6 min 40 s</td><td>3866 × 4140</td></tr><tr><td>2</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>512</td><td>60</td><td>5 min 59 s</td><td>3866 × 4048</td></tr><tr><td>3</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>256</td><td>50</td><td>6 min 40 s</td><td>3866 × 4140</td></tr><tr><td>4</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>256</td><td>60</td><td>5 min 59 s</td><td>3866 × 4048</td></tr><tr><td>5</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>128</td><td>60</td><td>6 min</td><td>3866 × 4048</td></tr><tr><td>6</td><td><italic>Leucaspius delineatus</italic></td><td>Sunbleak</td><td>128</td><td>60</td><td>5 min 59 s</td><td>3866 × 4048</td></tr><tr><td>7</td><td><italic>Danio rerio</italic></td><td>Zebrafish</td><td>100</td><td>32</td><td>1 min</td><td>3584 × 3500</td></tr><tr><td>8</td><td><italic>Drosophila melanogaster</italic></td><td>Fruit-fly</td><td>59</td><td>51</td><td>10 min</td><td>2306 × 2306</td></tr><tr><td>9</td><td><italic>Schistocerca gregaria</italic></td><td>Locust</td><td>15</td><td>25</td><td>1hr 0 min</td><td>1880 × 1881</td></tr><tr><td>10</td><td><italic>Constrictotermes cyphergaster</italic></td><td>Termite</td><td>10</td><td>100</td><td>10 min 5 s</td><td>1920 × 1080</td></tr><tr><td>11</td><td><italic>Danio rerio</italic></td><td>Zebrafish</td><td>10</td><td>32</td><td>10 min 10 s</td><td>3712 × 3712</td></tr><tr><td>12</td><td><italic>Danio rerio</italic></td><td>Zebrafish</td><td>10</td><td>32</td><td>10 min 3 s</td><td>3712 × 3712</td></tr><tr><td>13</td><td><italic>Danio rerio</italic></td><td>zebrafish</td><td>10</td><td>32</td><td>10 min 3 s</td><td>3712 × 3712</td></tr><tr><td>14</td><td><italic>Poecilia reticulata</italic></td><td>Guppy</td><td>8</td><td>30</td><td>3 hr 15 min 22 s</td><td>3008 × 3008</td></tr><tr><td>15</td><td><italic>Poecilia reticulata</italic></td><td>Guppy</td><td>8</td><td>25</td><td>1 hr 12 min</td><td>3008 × 300</td></tr><tr><td>16</td><td><italic>Poecilia reticulata</italic></td><td>Guppy</td><td>8</td><td>35</td><td>3 hr 18 min 13 s</td><td>3008 × 3008</td></tr><tr><td>17</td><td><italic>Poecilia reticulata</italic></td><td>Guppy</td><td>1</td><td>140</td><td>1 hr 9 min 32 s</td><td>1312 × 1312</td></tr></tbody></table></table-wrap><sec id="s2-1"><title>Tracking: speed and accuracy</title><p>In evaluating the 4.2 Tracking portion of TRex, the main focus lies with processing speed, while accuracy in terms of keeping identities is of secondary importance. Tracking is required in all other parts of the software, making it an attractive target for extensive optimization. Especially with regard to closed-loop, and live-tracking situations, there may be no room even to lose a millisecond between frames and thus risk dropping frames. We therefore designed TRex to support the simultaneous tracking of many (≥256) individuals <italic>quickly</italic> and achieve reasonable <italic>accuracy</italic> for up to 100 individuals – which are the two suppositions we will investigate in the following.</p><p>Trials were run without posture/visual-field estimation enabled, where tracking generally, and consistently, reaches speeds faster than real-time (processing times of 1.5 – 40 % of the video duration, 25 – 100 Hz) even for a relatively large number of individuals (77 – 94.77 % for up to 256 individuals, see <xref ref-type="table" rid="app4table1">Appendix 4—table 1</xref>). Videos with more individuals (&gt; 500) were still tracked within reasonable time of 235– 358 % of the video duration. As would be expected from these results, we found that combining tracking and recording in a single step generally leads to higher processing speeds. The only situation where this was not the case was a video with 1024 individuals, which suggests that live-tracking (in TGrabs) handles cases with many individuals slightly worse than offline tracking (in TRex). Otherwise, 5– 35 % shorter total processing times were measured (14.55 % on average, see <xref ref-type="table" rid="app4table4">Appendix 4—table 4</xref>), compared to running TGrabs separately and then tracking in TRex. These percentage differences, in most cases, reflect the ratio between the video duration and the time it takes to track it, suggesting that most time is spent – by far – on the conversion of videos. This additional cost can be avoided in practice when using TGrabs to record videos, by directly writing to a custom format recognized by TRex, and/or using its live-tracking ability to export tracking data immediately after the recording is stopped.</p><p>We also investigated trials that were run with posture estimation <italic>enabled</italic> and we found that real-time speed could be achieved for videos with ≤128 individuals (see column ‘tracking’ in <xref ref-type="table" rid="app4table4">Appendix 4—table 4</xref>). Tracking speed, when posture estimation is enabled, depends more strongly on the size of individuals in the image.</p><p>Generally, tracking software becomes slower as the number of individuals to be tracked increases, as a result of an exponentially growing number of combinations to consider during matching. TRex uses a novel tree-based algorithm by default (see Tracking), but circumvents problematic situations by falling back on using the <italic>Hungarian method</italic> (also known as the <italic>Kuhn-Munkres algorithm</italic>, <xref ref-type="bibr" rid="bib40">Kuhn, 1955</xref>) when necessary. Comparing our mixed approach (see Tracking) to purely using the Hungarian method shows that, while both perform similarly for few individuals, the Hungarian method is easily outperformed by our algorithm for larger groups of individuals (as can be seen in <xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3</xref>). This might be due to custom optimizations regarding local cliques of individuals, whereby we ignore objects that are too far away, and also as a result of our optimized pre-sorting. The Hungarian method has the advantage of not leading to combinatorical explosions in some situations – and thus has a lower <italic>maximum</italic> complexity while proving to be less optimal in the <italic>average</italic> case. For further details, see the appendix: Appendix D Matching an object to an object in the next frame.</p><p>In addition to speed, we also tested the accuracy of our tracking method, with regard to the consistency of identity assignments, comparing its results to the manually reviewed data (the methodology of which is described in the next section). In order to avoid counting follow-up errors as ‘new’ errors, we divided each trajectory in the uncorrected data into ‘uninterrupted’ segments of frames, instead of simply comparing whole trajectories. A segment is interrupted when an individual is lost (for any of the reasons given in 4.3.1 Preparing Tracking-Data) and starts again when it is reassigned to another object later on. We term these (re-)assignments <italic>decisions</italic> here. Each segment of every individual can be uniquely assigned to a similar/identical segment in the baseline data and its identity. Following one trajectory in the uncorrected data, we can detect these wrong decisions by checking whether the baseline identity associated with one segment of that trajectory changes in the next. We found that roughly 80 % of such decisions made by the tree-based matching were correct, even with relatively high numbers of individuals (100). For trajectories where no manually reviewed data were available, we used automatically corrected trajectories as a base for our comparison – we evaluate the accuracy of these automatically corrected trajectories in the following section. Even though we did not investigate accuracy in situations with more than 100 individuals, we suspect similar results since the property with the strongest influence on tracking accuracy – individual density – is limited physically and most of the investigated species school tightly in either case.</p></sec><sec id="s2-2"><title>Visual identification: accuracy</title><p>Since the goal of using visual identification is to generate consistent identity assignments, we evaluated the accuracy of our method in this regard. As a benchmark, we compare it to manually reviewed datasets as well as results from <monospace>idtracker.ai</monospace> for the same set of videos (where possible). In order to validate trajectories exported by either software, we manually reviewed multiple videos with the help from a tool within TRex that allows to view each crossing and correct possible mistakes in-place. Assignments were deemed incorrect, and subsequently corrected by the reviewer, if the centroid of a given individual was not contained within the object it was assigned to (e.g. the individual was not part of the correct object). Double assignments per object are impossible due to the nature of the tracking method. Individuals were also forcibly assigned to the correct objects in case they were visible but not detected by the tracking algorithm. After manual corrections had been applied, ‘clean’ trajectories were exported – providing a per-frame baseline truth for the respective videos. A complete table of reviewed videos, and the percentage of reviewed frames per video, can be found in <xref ref-type="table" rid="table2">Table 2</xref>. For longer videos (&gt; 1 hr), we relied entirely on a comparison between results from <monospace>idtracker.ai</monospace> and TRex. Their paper (<xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref>) suggests a very high accuracy of over 99.9 % correctly identified individual images for most videos, which should suffice for most relevant applications and provide a good baseline truth. As long as both tools produce sufficiently similar trajectories, we therefore know they have found the correct solution.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Results of the human validation for a subset of videos.</title><p>Validation was performed by going through all problematic situations (e.g. individuals lost) and correcting mistakes manually, creating a fully corrected dataset for the given videos. This dataset may still have missing frames for some individuals, if they could not be detected in certain frames (as indicated by ‘of that interpolated’). This was usually a very low percentage of all frames, except for Video 9, where individuals tended to rest on top of each other – and were thus not tracked – for extended periods of time. This baseline dataset was compared to all other results obtained using the automatic visual identification by TRex (<inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) and <monospace>idtracker.ai</monospace> (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>) to estimate correctness. We were not able to track Videos 9 and 10 with <monospace>idtracker.ai</monospace>, which is why correctness values are not available.</p><p><supplementary-material id="table2sdata1"><label>Table 2—source data 1.</label><caption><title>A table of positions for each individual of each manually approved and corrected trial.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64000-table2-data1-v2.zip"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Video metrics</th><th colspan="2">Review stats</th><th colspan="2">% correct</th></tr><tr><th>Video</th><th># ind.</th><th>Reviewed (%)</th><th>Of that interpolated (%)</th><th>TRex</th><th><monospace>idtracker.ai</monospace></th></tr></thead><tbody><tr><td>7</td><td>100</td><td>100.0</td><td>0.23</td><td>99.07 ± 0.013</td><td>98.95 ± 0.146</td></tr><tr><td>8</td><td>59</td><td>100.0</td><td>0.15</td><td>99.68 ± 0.533</td><td>99.94 ± 0.0</td></tr><tr><td>9</td><td>15</td><td>22.2</td><td>8.44</td><td>95.12 ± 6.077</td><td>N/A</td></tr><tr><td>10</td><td>10</td><td>100.0</td><td>1.21</td><td>99.7 ± 0.088</td><td>N/A</td></tr><tr><td>13</td><td>10</td><td>100.0</td><td>0.27</td><td>99.98 ± 0.0</td><td>99.96 ± 0.0</td></tr><tr><td>12</td><td>10</td><td>100.0</td><td>0.59</td><td>99.94 ± 0.006</td><td>99.63 ± 0.0</td></tr><tr><td>11</td><td>10</td><td>100.0</td><td>0.5</td><td>99.89 ± 0.009</td><td>99.34 ± 0.002</td></tr></tbody></table></table-wrap><p>A direct comparison between <monospace>TRex</monospace> and <monospace>idtracker.ai</monospace> was not possible for Videos 9 and 10, where idtracker.ai frequently exceeded hardware memory-limits and caused the application to be terminated, or did not produce usable results within multiple days of run-time. However, we were able to successfully analyze these videos with TRex and evaluate its performance by comparing to manually reviewed trajectories (see below in Visual identification: accuracy). Due to the stochastic nature of machine learning, and thus the inherent possibility of obtaining different results in each run, as well as other potential factors influencing processing time and memory consumption, both <monospace>TRex</monospace> and <monospace>idtracker.ai</monospace> have been executed repeatedly (<monospace>5x TRex, 3x idtracker.ai</monospace>).</p><p>The trajectories exported by both <monospace>idtracker.ai</monospace> and <monospace>TRex</monospace> were very similar throughout (see <xref ref-type="table" rid="table3">Table 3</xref>). While occasional disagreements happened, similarity scores were higher than 98 % in all and higher than 99 % in most cases (i.e. less than 1 % of individuals have been differently assigned in each frame on average). Most difficulties that <italic>did</italic> occur were, after manual review, attributable to situations where multiple individuals cross over excessively within a short time-span. In each case that has been manually reviewed, identities switched back to the correct individuals – even after temporary disagreement. We found that both solutions occasionally experienced these same problems, which often occur when individuals repeatedly come in and out of view in quick succession (e.g. overlapping with other individuals). Disagreements were expected for videos with many such situations due to the way both algorithms deal differently with them: <monospace>idtracker.ai</monospace> assigns identities only based on the network output. In many cases, individuals continue to partly overlap even while already being tracked, which results in visual artifacts and can lead to unstable predictions by the network and causing <monospace>idtracker.ai’s</monospace> approach to fail. Comparing results from both <monospace>idtracker.ai</monospace> and <monospace>TRex</monospace> to manually reviewed data (see <xref ref-type="table" rid="table2">Table 2</xref>) shows that both solutions consistently provide high-accuracy results of above 99.5 % for most videos, but that TRex is slightly improved in all cases while also having a better overall frame coverage per individual (99.65 % versus <monospace>idtracker.ai’s</monospace> 97.93 %, where 100 % would mean that all individuals are tracked in every frame; not shown). This suggests that the splitting algorithm (see appendix, Appendix K Algorithm for splitting touching individuals) is working to TRex’ advantage here.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Evaluating comparability of the automatic visual identification between <monospace>idtracker.ai</monospace> and <monospace>TRex</monospace>.</title><p>Columns show various video properties, as well as the associated uniqueness score (see Guiding the training process) and a similarity metric. Similarity (<italic>% similar individuals</italic>) is calculated based on comparing the positions for each identity exported by both tools, choosing the closest matches overall and counting the ones that are differently assigned per frame. An individual is classified as ‘wrong’ in that frame, if the euclidean distance between the matched solutions from <monospace>idtracker.ai</monospace> and TRex exceeds 1 % of the video width. The column ‘% similar individuals’ shows percentage values, where a value of 99% would indicate that, on average, 1 % of the individuals are assigned differently. To demonstrate how uniqueness corresponds to the quality of results, the last column shows the average uniqueness achieved across trials. A file containing all X and Y positions for each trial and each software combined into one very large table is available from <xref ref-type="bibr" rid="bib75">Walter et al., 2020</xref>, along with the data in different formats.</p><p><supplementary-material id="table3sdata1"><label>Table 3—source data 1.</label><caption><title>Assignments between identities from multiple solutions, as calculated by a bipartite-graph matching algorithm.</title><p>For each permutation of trials from TRex and idtracker.ai for the same video, the algorithm sought to match the trajectories of the same physical individuals in both trials with each other by finding the ones with the smallest mean euclidean distance per frame between them. Available from <xref ref-type="bibr" rid="bib75">Walter et al., 2020</xref> as T2_source_data.zip.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-64000-table3-data1-v2.csv"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Video</th><th># ind.</th><th>N TRex</th><th>% similar individuals</th><th>Final uniqueness</th></tr></thead><tbody><tr><td>7</td><td>100</td><td>5</td><td>99.8346 ± 0.5265</td><td>0.9758 ± 0.0018</td></tr><tr><td>8</td><td>59</td><td>5</td><td>98.6885 2.1145</td><td>0.9356 ± 0.0358</td></tr><tr><td>13</td><td>10</td><td>5</td><td>99.9902 0.3737</td><td>0.9812 ± 0.0013</td></tr><tr><td>11</td><td>10</td><td>5</td><td>99.9212 ± 1.1208</td><td>0.9461 ± 0.0039</td></tr><tr><td>12</td><td>10</td><td>5</td><td>99.9546 ± 0.8573</td><td>0.9698 ± 0.0024</td></tr><tr><td>14</td><td>8</td><td>5</td><td>98.8356 ± 5.8136</td><td>0.9192 ± 0.0077</td></tr><tr><td>15</td><td>8</td><td>5</td><td>99.2246 ± 4.4486</td><td>0.9576 ± 0.0023</td></tr><tr><td>162</td><td>8</td><td>5</td><td>99.7704 ± 2.1994</td><td>0.9481 ± 0.0025</td></tr></tbody></table></table-wrap><p>Additionally, while TRex could successfully track individuals in all videos without tags, we were interested to see the effect of tags (in this case QR tags attached to locusts, see <xref ref-type="fig" rid="fig3">Figure 3a</xref>) on network training. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, we visualize differences in network activation, depending on the visual features available for the network to learn from, which are different between species (or due to physically added tags, as mentioned above). The ‘hot’ regions indicate larger between-class differences for that specific pixel (values are the result of activation in the last convolutional layer of the trained network, see figure legend). Differences are computed separately within each group and are not directly comparable between trials/species in value. However, the distribution of values – reflecting the network’s reactivity to specific parts of the image – is. Results show that the most apparent differences are found for the stationary parts of the body (not in absolute terms, but following normalization, as shown in <xref ref-type="fig" rid="fig4">Figure 4c</xref>), which makes sense seeing as this part (i) is the easiest to learn due to it being in exactly the same position every time, (ii) larger individuals stretch further into the corners of a cropped image, making the bottom right of each image a source of valuable information (especially in <xref ref-type="fig" rid="fig3">Figure 3a and b</xref>) and (iii) details that often occur in the head-region (like distance between the eyes) which can also play a role here. ‘Hot’ regions in the bottom right corner of the activation images (e.g. in <xref ref-type="fig" rid="fig3">Figure 3d</xref>) suggest that also pixels are reacted to which are explicitly <italic>not</italic> part of the individual itself but of other individuals – likely this corresponds to the network making use of size/shape differences between them.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Activation differences for images of randomly selected individuals from four videos, next to a median image of the respective individual – which hides thin extremities, such as legs in (a) and (c).</title><p>The captions in (a-d) detail the species per group and number of samples per individual. Colors represent the relative activation differences, with hotter colors suggesting bigger magnitudes, which are computed by performing a forward-pass through the network up to the last convolutional layer (using keract). The outputs for each identity are averaged and stretched back to the original image size by cropping and scaling according to the network architecture. Differences shown here are calculated per cluster of pixels corresponding to each filter, comparing average activations for images from the individual’s class to activations for images from other classes.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Code, as well as images/weights needed to produce this figure (see README).</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64000-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig3-v2.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>An overview of TRex’ the main interface, which is part of the documentation at trex.run/docs.</title><p>Interface elements are sorted into categories in the four corners of the screen (labelled here in black). The omni-box on the bottom left corner allows users to change parameters on-the-fly, helped by a live auto-completion and documentation for all settings. Only some of the many available features are displayed here. Generally, interface elements can be toggled on or off using the bottom-left display options or moved out of the way with the cursor. Users can customize the tinting of objects (e.g. sourcing it from their speed) to generate interesting effect and can be recorded for use in presentations. Additionally, all exportable metrics (such as border-distance, size, x/y, etc.) can also be shown as an animated graph for a number of selected objects. Keyboard shortcuts are available for select features such as loading, saving, and terminating the program. Remote access is supported and offers the same graphical user interface, for example in case the software is executed without an application window (for batch processing purposes).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig4-v2.tif"/></fig><p>As would be expected, distinct patterns can be recognized in the resulting activations after training as soon as physical tags are attached to individuals (as in <xref ref-type="fig" rid="fig3">Figure 3a</xref>). While other parts of the image are still heavily activated (probably to benefit from size/shape differences between individuals), tags are always at least a large part of where activations concentrate. The network seemingly makes use of the additional information provided by the experimenter, where that has occurred. This suggests that, while definitely not being necessary, adding tags probably does not worsen, and likely may even improve, training accuracy, for difficult cases allowing networks to exploit any source of inter-individual variation.</p></sec><sec id="s2-3"><title>Visual identification: memory consumption</title><p>In order to generate comparable results between both tested software solutions, the same external script has been used to measure shared, private and swap memory of <monospace>idtracker.ai</monospace> and <monospace>TRex</monospace>, respectively. There are a number of ways with which to determine the memory usage of a process. For automation purposes, we decided to use a tool called syrupy, which can start and save information about a specified command automatically. We modified it slightly, so we could obtain more accurate measurements for Swap, Shared and Private separately, using ps_mem.</p><p>As expected, differences in memory consumption are especially prominent for long videos (4-7x lower maximum memory), and for videos with many individuals (2-3x lower). Since we already experienced significant problems tracking a long video (&gt; 3 hr) of only eight individuals with <monospace>idtracker.ai</monospace>, we did not attempt to further study its behavior in long videos with many individuals. However, we would expect <monospace>idtracker.ai</monospace> memory usage to increase even more rapidly than is visible in <xref ref-type="fig" rid="fig5">Figure 5</xref> since it retains a lot of image data (segmentation/pixels) in memory and we already had to ‘allow’ it to relay to hard-disk in our efforts to make it work for Videos 8, 14, and 16 (which slows down analysis). The maximum memory consumption across all trials was on average 5.01±2.54 times higher in <monospace>idtracker.ai</monospace>, ranging from 1.81 to 10.85 times the maximum memory consumption of TRex for the same video (see <xref ref-type="table" rid="table4">Table 4</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The maximum memory by TRex and <monospace>idtracker.ai</monospace> when tracking videos from a subset of all videos (the same videos as in <xref ref-type="table" rid="table3">Table 3</xref>).</title><p>Results are plotted as a function of video length (min) multiplied by the number of individuals. We have to emphasize here that, for the videos in the upper length regions of multiple hours (<italic>2</italic>, <italic>2</italic>), we had to set <monospace>idtracker.ai</monospace> to store segmentation information on disk – as opposed to in RAM. This uses less memory, but is also slower. For the video with flies we tried out both and also settled for on-disk, since otherwise the system ran out of memory. Even then, the curve still accelerates much faster for <monospace>idtracker.ai</monospace>, ultimately leading to problems with most computer systems. To minimize the impact that hardware compatibility has on research, we implemented switches limiting memory usage while always trying to maximize performance given the available data. TRex can be used on modern laptops and normal consumer hardware at slightly lower speeds, but without any <italic>fatal</italic> issues.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Each data-point from <xref ref-type="fig" rid="fig5">Figure 5</xref> as plotted, indexed by video and software used.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-64000-fig5-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig5-v2.tif"/></fig><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Both TRex and <monospace>idtracker.ai</monospace> analyzed the same set of videos, while continuously logging their memory consumption using an external tool.</title><p>Rows have been sorted by <inline-formula><mml:math id="inf4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>video</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>length</mml:mi></mml:mrow><mml:mo>*</mml:mo><mml:mi mathvariant="normal">#</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>individuals</mml:mi></mml:mrow></mml:math></inline-formula>, which seems to be a good predictor for the memory consumption of both solutions. <monospace>idtracker.ai</monospace> has mixed mean values, which, at low individual densities are similar to TRex’ results. Mean values can be misleading here, since more time spent in low-memory states skews results. The maximum, however, is more reliable since it marks the memory that is necessary to run the system. Here, <monospace>idtracker.ai</monospace> clocks in at significantly higher values (almost always more than double) than TRex.</p><p><supplementary-material id="table4sdata1"><label>Table 4—source data 1.</label><caption><title>Data from log files for all trials as a single table, where each row is one sample.</title><p>The total memory of each sample is calculated as <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>SWAP</mml:mi><mml:mo>+</mml:mo><mml:mi>PRIVATE</mml:mi><mml:mo>+</mml:mo><mml:mi>SHARED</mml:mi></mml:mrow></mml:math></inline-formula>. Each row indicates at which exact time, by which software, and as part of which trial it was taken. </p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64000-table4-data1-v2.zip"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Video</th><th>#ind.</th><th>Length</th><th>Max.consec.</th><th>TRex memory (GB)</th><th>Idtracker.ai memory (GB)</th></tr></thead><tbody><tr><td>12</td><td>10</td><td>10 min</td><td>26.03s</td><td>4.88 ± 0.23, max 6.31</td><td>8.23 ± 0.99, max 28.85</td></tr><tr><td>13</td><td>10</td><td>10 min</td><td>36.94s</td><td>4.27 ± 0.12, max 4.79</td><td>7.83 ± 1.05, max 29.43</td></tr><tr><td>11</td><td>10</td><td>10 min</td><td>28.75s</td><td>4.37 ± 0.32, max 5.49</td><td>6.53 ± 4.29, max 29.32</td></tr><tr><td>7</td><td>100</td><td>1 min</td><td>5.97s</td><td>9.4 ± 0.47, max13.45</td><td>15.27 ± 1.05, max 24.39</td></tr><tr><td>15</td><td>8</td><td>72 min</td><td>79.4s</td><td>5.6 ± 0.22, max 8.41</td><td>35.2 ± 4.51, max 91.26</td></tr><tr><td>10</td><td>10</td><td>10 min</td><td>1391s</td><td>6.94 ± 0.27, max 10.71</td><td>N/A</td></tr><tr><td>9</td><td>15</td><td>60 min</td><td>7.64s</td><td>13.81 ± 0.53, max 16.99</td><td>N/A</td></tr><tr><td>8</td><td>59</td><td>10 min</td><td>102.35s</td><td>12.4 ± 0.56, max 17.41</td><td>35.3 ± 0.92, max 50.26</td></tr><tr><td>14</td><td>8</td><td>195 min</td><td>145.77s</td><td>12.44 ± 0.8, max 21.99</td><td>35.08 ± 4.08, max 98.04</td></tr><tr><td>16</td><td>8</td><td>198 min</td><td>322.57s</td><td>16.15 ± 1.6, max 28.62</td><td>49.24 ± 8.21, max 115.37</td></tr></tbody></table></table-wrap><p>Overall memory consumption for TRex also contains posture data, which contributes a lot to RAM usage. Especially with longer videos, disabling posture can lower the hardware needs for running our software. If posture is to be retained, the user can still (more slightly) reduce memory requirements by changing the outline re-sampling scale (one by default), which adjusts the outline resolution between sub- and super-pixel accuracy. While analysis will be faster – and memory consumption lower – when posture is disabled (only limited by the matching algorithm, see <xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3</xref>), users of the visual identification might experience a decrease in training accuracy or speed (see <xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Convergence behavior of the network training for three different normalization methods.</title><p>This shows the maximum achievable validation accuracy after 100 epochs for 100 individuals (Video 7), when sub-sampling the number of examples per individual. Tests were performed using a manually corrected training dataset to generate the images in three different ways, using the same, independent script (see <xref ref-type="fig" rid="fig8">Figure 8</xref>): Using no normalization (blue), using normalization based on image moments (green, similar to <monospace>idtracker.ai</monospace>), and using posture information (red, as in TRex). Higher numbers of samples per individual result in higher maximum accuracy overall, but – unlike the other methods – posture-normalized runs already reach an accuracy above the 90 % mark for ≥75 samples. This property can help significantly in situations with more crossings, when longer global segments are harder to find.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Raw data-points as plotted in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64000-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig6-v2.tif"/></fig></sec><sec id="s2-4"><title>Visual identification: processing time</title><p>Automatically correcting the trajectories (to produce consistent identity assignments) means that additional time is spent on the training and application of a network, specifically for the video in question. Visual identification builds on some of the other methods described in this paper (tracking and posture estimation), naturally making it by far the most complex and time-consuming process in TRex – we thus evaluated how much time is spent on the entire sequence of all required processes. For each run of TRex and<monospace>idtracker.ai,</monospace> we saved precise timing information from start to finish. Since <monospace>idtracker.ai</monospace> reads videos <italic>directly</italic> and preprocesses them again each run, we used the same starting conditions with our software for a direct comparison:</p><p>A trial starts by converting/preprocessing a video in TGrabs and then immediately opening it in TRex, where automatic identity corrections were applied. TRex terminated automatically after satisfying a correctness criterion (high uniqueness value) according to equation (Accumulation of additional segments and stopping-criteria). It then exported trajectories, as well as validation data (similar to <monospace>idtracker.ai</monospace>), concluding the trial. The sum of time spent within TGrabs and TRex gives the total amount of time for that trial. For the purpose of this test it would not have been fair to compare only TRex processing times to <monospace>idtracker.ai</monospace>, but it is important to emphasize that conversion could be skipped entirely by using TGrabs to record videos directly from a camera instead of opening an existing video file.</p><p>In <xref ref-type="table" rid="table5">Table 5</xref>, we can see that video length and processing times (in TRex) did not correlate directly. Indeed, a 1 min video (Video 8) took significantly longer than one that was 60 min long (Video 15). The reason for this, initially counterintuitive, result is that the process of learning identities requires sufficiently long video sequences: longer samples have a higher likelihood of capturing more of the total possible intra-individual variance which helps the algorithm to more comprehensively represent each individual’s appearance. Longer videos naturally provide more material for the algorithm to choose from and, simply due to their length, have a higher probability of containing at least one higher quality segment that allows higher uniqueness-regimes to be reached more quickly (see Guiding the training process and H.2 Stopping-criteria). Thus, it is important to use sufficiently long video sequences for visual identification, and longer sequences can lead to better results – both in terms of quality and processing time.</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Evaluating time-cost for automatic identity correction – comparing to results from <monospace>idtracker.ai</monospace>.</title><p>Timings consist of preprocessing time in TGrabs plus network training in TRex, which are shown separately as well as combined (<italic>ours (min)</italic>, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). The time it takes to analyze videos strongly depends on the number of individuals and how many usable samples per individual the initial segment provides. The length of the video factors in as well, as does the stochasticity of the gradient descent (training). <monospace>idtracker.ai</monospace> timings (<inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>) contain the whole tracking and training process from start to finish, using its terminal_mode (v3). Parameters have been manually adjusted per video and setting, to the best of our abilities, spending at most one hour per configuration. For videos 16 and 14, we had to set <monospace>idtracker.ai</monospace> to storing segmentation information on disk (as compared to in RAM) to prevent the program from being terminated for running out of memory.</p><p><supplementary-material id="table5sdata1"><label>Table 5—source data 1.</label><caption><title>Preprocessed log files (see also notebooks.zip in <xref ref-type="bibr" rid="bib75">Walter et al., 2020</xref>) in a table format.</title><p>The total processing time (s) of each trial is indexed by video and software used – TGrabs for conversion and TRex and <monospace>idtracker.ai</monospace> for visual identification. This data is also used in <xref ref-type="table" rid="app4table4">Appendix 4—table 4</xref>.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-64000-table5-data1-v2.csv"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Video</th><th># ind.</th><th>Length</th><th>Sample</th><th>TGrabs (min)</th><th>TRex (min)</th><th>Ours (min)</th><th><monospace>idtracker.ai</monospace> (min)</th></tr></thead><tbody><tr><td>7</td><td>100</td><td>1min</td><td>1.61s</td><td>2.03 ± 0.02</td><td>74.62 ± 6.75</td><td>76.65</td><td>392.22 ± 119.43</td></tr><tr><td>8</td><td>59</td><td>10min</td><td>19.46s</td><td>9.28 ± 0.08</td><td>96.7 ± 4.45</td><td>105.98</td><td>495.82 ± 115.92</td></tr><tr><td>9</td><td>15</td><td>60min</td><td>33.81s</td><td>13.17 ± 0.12</td><td>101.5 ± 1.85</td><td>114.67</td><td>N/A</td></tr><tr><td>11</td><td>10</td><td>10min</td><td>12.31s</td><td>8.8 ± 0.12</td><td>21.42 ± 2.45</td><td>30.22</td><td>127.43 ± 57.02</td></tr><tr><td>12</td><td>10</td><td>10min</td><td>10.0s</td><td>8.65 ± 0.07</td><td>23.37 ± 3.83</td><td>32.02</td><td>82.28 ± 3.83</td></tr><tr><td>13</td><td>10</td><td>10min</td><td>36.91s</td><td>8.65 ± 0.047</td><td>12.47 ± 1.27</td><td>21.12</td><td>79.42 ± 4.52</td></tr><tr><td>10</td><td>10</td><td>10min</td><td>16.22s</td><td>4.43 ± 0.05</td><td>35.05 ± 1.45</td><td>39.48</td><td>N/A</td></tr><tr><td>14</td><td>8</td><td>195min</td><td>67.97s</td><td>109.97 ± 0.05</td><td>70.48 ± 3.67</td><td>180.45</td><td>707.0 ± 27.55</td></tr><tr><td>15</td><td>8</td><td>72min</td><td>79.36s</td><td>32.1 ± 0.42</td><td>30.77 ± 6.28</td><td>62.87</td><td>291.42 ± 16.83</td></tr><tr><td>16</td><td>8</td><td>198min</td><td>134.07s</td><td>133.1 ± 2.28</td><td>68.85 ± 13.12</td><td>201.95</td><td>1493.83 ± 27.75</td></tr></tbody></table></table-wrap><p>Compared to <monospace>idtracker.ai</monospace>, TRex (conversion + visual identification) shows both considerably lower computation times (<inline-formula><mml:math id="inf8"><mml:mrow><mml:mn>2.57</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf9"><mml:mrow><mml:mn>46.74</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> faster for the same video), as well as lower variance in the timings (79% lower for the same video on average).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have designed TRex to be a versatile and fast program that can enable researches to track animals (and other mobile objects) in a wide range of situations. It maintains identities of up to 100 un-tagged individuals and produces corrected tracks, along with posture estimation, visual-field reconstruction, and other features that enable the quantitative study of animal behavior. Even videos that cannot be tracked by other solutions, such as videos with over 500 animals, can now be tracked within the same day of recording.</p><p>While all options are available from the command-line and a screen is not required, TRex offers a rich, yet straight-foward to use, interface to local as well as remote users. Accompanied by the integrated documentation for all parameters, each stating purpose, type and value ranges, as well as a comprehensive online documentation, new users are provided with all the information required for a quick adoption of our software. Especially to the benefit of new users, we evaluated the parameter space using videos of diverse species (fish, termites, locusts) and determined which parameters work best in most use-cases to set their default values.</p><p>The interface is structured into groups (see <xref ref-type="fig" rid="fig5">Figure 5</xref>), categorized by the typical use-case:</p><list list-type="order"><list-item><p>The main menu, containing options for loading/saving, options for the timeline and reanalysis of parts of the video</p></list-item><list-item><p>Timeline and current video playback information</p></list-item><list-item><p>Information about the selected individual</p></list-item><list-item><p>Display options and an interactive ‘omni-box’ for viewing and changing parameters</p></list-item><list-item><p>General status information about TRex and the Python integration.</p></list-item></list><p>The tracking accuracy of TRex is at the state-of-the-art while typically being <inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>2.57</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf11"><mml:mrow><mml:mn>46.74</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> faster than comparable software and having lower hardware requirements – especially RAM. In addition to visual identification and tracking, it provides a rich assortment of additional data, including body posture, visual fields, and other kinematic as well as group-related information (such as derivatives of position, border and mean neighbor distance, group compactness, etc); even in live-tracking and closed-loop situations.</p><p>Raw tracking speeds (without visual identification) still achieved roughly 80 % accuracy per decision (as compared to &gt; 99% with visual identification). We have found that real-time performance can be achieved, even on relatively modest hardware, for all numbers of individuals ≤256 without posture estimation (≤ 128 with posture estimation). More than 256 individuals can be tracked as well, remarkably still delivering frame-rates at about 10 – 25 frames per second using the same settings.</p><p>Not only does the increased processing-speeds benefit researchers, but the contributions we provide to data exploration should not be underestimated as well – merely making data more easily accessible right out-of-the-box, such as visual fields and live-heatmaps (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), has the potential to reveal features of group- and individual behavior which have not been visible before. TRex makes information on multiple timescales of events available simultaneously, and sometimes this is the only way to detect interesting properties (e.g. trail formation in termites).</p><p>Since the software is already actively used within the Max Planck Institute of Animal Behavior, reported issues have been taken into consideration during development. However, certain theoretical, as well as practically observed, limitations remain:</p><list list-type="bullet"><list-item><p>Posture: While almost all shapes can be detected correctly (by adjusting parameters), some shapes – especially round shapes – are hard to interpret in terms of ‘tail’ or ‘head’. This means that only the other image alignment method (moments) can be used. However, it does introduce some limitations for example calculating visual fields is impossible.</p></list-item><list-item><p>Tracking: Predictions, if the wrong direction is assumed, might go really far away from where the object is. Objects are then ‘lost’ for a fixed amount of time (parameter). This can be ‘fixed’ by shortening this time-period, though this leads to different problems when the software does not wait long enough for individuals to reappear.</p></list-item><list-item><p>General: Barely visible individuals have to be tracked with the help of deep learning (e.g. using <xref ref-type="bibr" rid="bib12">Caelles et al., 2017</xref>) and a custom-made mask per video frame, prepared in an external program of the users choosing.</p></list-item><list-item><p>Visual identification: All individuals have to be <italic>visible</italic> and <italic>separate</italic> at the same time, at least once, for identification to work at all. Visual identification, for example with very high densities of individuals, can thus be very difficult. This is a hard restriction to any software since finding consecutive global segments is the underlying principle for the successful recognition of individuals.</p></list-item></list><p>We will continue updating the software, increasingly addressing the above issues (and likely others), as well as potentially adding new features. During development, we noticed a couple of areas where improvements could be made, both theoretical and practical in nature. Specifically, incremental improvements in analysis speed could be made regarding visual identification by using the trained network more sporadically – for example it is not necessary to predict every image of very long consecutive segments, since, even with fewer samples, prediction values are likely to converge to a certain value early on. A likely more potent change would be an improved ‘uniqueness’ algorithm, which, during the accumulation phase, is better at predicting which consecutive segment will improve training results the most. This could be done, for example, by taking into account the variation between images of the same individual. Other planned extensions include:</p><list list-type="bullet"><list-item><p>(Feature): We want to have a more general interface available to users, so they can create their own plugins. Working with the data in live-mode, while applying their own filters. As well as specifically being able to write a plugin that can detect different species/annotate them in the video.</p></list-item><list-item><p>(Crossing solver): Additional method optimized for splitting overlapping, solid-color objects. The current method, simply using a threshold, is effective for many species but often produces large holes when splitting objects consisting of largely the same color.</p></list-item></list><p>To obtain the most up-to-date version of TRex, please download it at trex.run or update your existing installation according to our instructions listed on trex.run/docs/install.html.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>In the following sections, we describe the methods implemented in TRex and TGrabs, as well as their most important features in a typical order of operations (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for a flow diagram), starting out with a raw video. We will then describe how trajectories are obtained and end with the most technically involved features.</p><sec id="s4-1"><title>Segmentation</title><p>When an image is first received from a camera (or a video file), the objects of interest potentially present in the frame must be found and cropped out. Several technologies are available to separate the foreground from the background (segmentation). Various machine learning algorithms are frequently used to great effect, even for the most complex environments (<xref ref-type="bibr" rid="bib33">Hughey et al., 2018</xref>; <xref ref-type="bibr" rid="bib65">Robie et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Francisco et al., 2019</xref>). These more advanced approaches are typically beneficial for the analysis of field-data or organisms that are very hard to see in video (e.g. very transparent or low contrast objects/animals in the scene). In these situations, where integrated methods might not suffice, it is possible to segment objects from the background using external, for example deep-learning based, tools (see next paragraph). However, for most laboratory experiments, simpler (and also much faster), classical image-processing methods yield satisfactory results. Thus, we provide as a generically useful capability <italic>background-subtraction</italic>, which is the default method by which objects are segmented. This can be used immediately in experiments where the background is relatively static. Backgrounds are generated automatically by uniformly sampling images from the source video(s) – different modes are available (min/max, mode and mean) for the user to choose from. More advanced image-processing techniques like luminance equalization (which is useful when lighting varies between images), image undistortion, and brightness/contrast adjustments are available in TGrabs and can enhance segmentation results – but come at the cost of slightly increased processing time. Importantly, since many behavioral studies rely on ≥4 K resolution videos, we heavily utilize the GPU (if available) to speed up most of the image-processing, allowing TRex to scale well with increasing image resolution.</p><p>TGrabs can generally find any object in the video stream, and subsequently pass it on to the tracking algorithm (next section), as long as either (i) the background is relatively static while the objects move at least occasionally, (ii) the objects/animals of interest have enough contrast to the background, or (iii) the user provides an additional binary mask per frame which is used to separate the objects of interest from the background, the typical means of doing this being by deep-learning based segmentation (e.g. <xref ref-type="bibr" rid="bib12">Caelles et al., 2017</xref>). These masks are expected to be in a video-format themselves and correspond 1:1 in length and dimensions to the video that is to be analyzed. They are expected to be binary, marking individuals in white and background in black. Of course, these binary videos could be used on their own, but would not retain grey-scale information of the objects. There are a lot of possible applications where this could be useful; but generally, whenever individuals are really hard to detect visually and need to be recognized by a different software (e.g. a machine-learning-based segmentation like <xref ref-type="bibr" rid="bib46">Maninis et al., 2018</xref>). Individual frames can then be connected using our software as a second step.</p><p>The detected objects are saved to a custom non-proprietary compressed file format (Preprocessed Video or PV, see appendix Appendix G The PV file format), that stores only the most essential information from the original video stream: the objects and their pixel positions and values. This format is optimized for quick random index access by the tracking algorithm (see next section) and stores other meta-information (like frame timings) utilized during playback or analysis. When recording videos directly from a camera, they can also be streamed to an additional and independent MP4 container format (plus information establishing the mapping between PV and MP4 video frames).</p></sec><sec id="s4-2"><title>Tracking</title><p>Once animals (or, more generally, termed ‘objects’ henceforth) have been successfully segmented from the background, we can either use the live-tracking feature in TGrabs or open a pre-processed file in TRex, to generate the trajectories of these objects. This process uses information regarding an object’s movement (i.e. its kinematics) to follow it across frames, estimating future positions based on previous velocity and angular speed. It will be referred to as ‘tracking’ in the following text, and is a required step in all workflows.</p><p>Note that this approach alone is very fast, but, as will be shown, is subject to error with respect to maintaining individual identities. If that is required, there is a further step, outlined in Automatic visual identification based on machine learning below, which can be applied at the cost of processing speed. First, however, we will discuss the general basis of tracking, which is common to approaches that do, and do not, require identities to be maintained with high-fidelity. Tracking can occur for two distinct categories, which are handled slightly differently by our software:</p><list list-type="order"><list-item><p>There is a known number of objects</p></list-item><list-item><p>There is an unknown number of objects</p></list-item></list><p>The first case assumes that the number of tracked objects in a frame cannot exceed a certain expected number of objects (calculated automatically, or set by the user). This allows the algorithm to make stronger assumptions, for example regarding noise, where otherwise ‘valid’ objects (conforming to size expectations) are ignored due to their positioning in the scene (e.g. too far away from previously lost individuals). In the second case, new objects may be generated until all viable objects in a frame are assigned. While being more susceptible to noise, this is useful for tracking a large number of objects, where counting objects may not be possible, or where there is a highly variable number of objects to be tracked.</p><p>For a given video, our algorithm processes every frame sequentially, extending existing trajectories (if possible) for each of the objects found in the current frame. Every object can only be assigned to one trajectory, but some objects may not be assigned to any trajectory (e.g. in case the number of objects exceeds the allowed number of individuals) and some trajectories might not be assigned to any object (e.g. while objects are out of view). To estimate object identities across frames, we use an approach akin to the popular Kalman filter (<xref ref-type="bibr" rid="bib38">Kalman, 1960</xref>) which makes predictions based on multiple noisy data streams (here, positional history and posture information). In the initial frame, objects are simply assigned from top-left to bottom-right. In all other frames, assignments are made based on probabilities (see appendix Appendix D Matching an object to an object in the next frame) calculated for every combination of object and trajectory. These probabilities represent the degree to which the program believes that ‘it makes sense’ to extend an existing trajectory with an object in the current frame, given its position and speed. Our tracking algorithm only considers assignments with probabilities larger than a certain threshold, generally constrained to a certain proximity around an object assigned in the previous frame.</p><p>Matching a set of objects in one frame with a set of objects in the next frame is representative of a typical assignment problem, which can be solved in polynomial time (e.g. using the Hungarian method <xref ref-type="bibr" rid="bib40">Kuhn, 1955</xref>). However, we found that, in practice, the computational complexity of the Hungarian method can constrain analysis speed to such a degree that we decided to implement a custom algorithm, which we term tree-based matching, which has a better <italic>average-case</italic> performance (see evaluation), even while having a comparatively bad <italic>worst-case</italic> complexity. Our algorithm constructs a tree of all possible object/trajectory combinations in the frame and tries to find a compatible (such that no objects/trajectories are assigned twice) set of choices, maximizing the sum of probabilities amongst these choices (described in detail in the appendix Appendix D Matching an object to an object in the next frame). Problematic are situations where a large number of objects are in close proximity of one another, since then the number of possible sets of choices grows exponentially. These situations are avoided by using a mixed approach: tree-based matching is used most of the time, but as soon as the combinatorical complexity of a certain situation becomes too great, our software falls back on using the Hungarian method. If videos are known to be problematic throughout (e.g. with &gt; 100 individuals consistently very close to each other), the user may choose to use an approximate method instead (described in the appendix Appendix D), which simply iterates through all objects and assigns each to the trajectory for which it has the highest probability and subsequently does not consider whether another object has an even higher probability for that trajectory. While the approximate method scales better with an increasing number of individuals, it is ‘wrong’ (seeing as it does not consider all possible combinations) – which is why it is not recommended unless strictly necessary. However, since it does not consider all combinations, making it more sensitive to parameter choice, it scales better for very large numbers of objects and produces results good enough for it to be useful in very large groups (see <xref ref-type="table" rid="app4table2">Appendix 4—table 2</xref>).</p><p>Situations where objects/individuals are touching, partly overlapping, or even completely overlapping, is an issue that all tracking solutions have to deal with in some way. The first problem is the <italic>detection</italic> of such an overlap/crossing, the second is its <italic>resolution</italic>. <monospace>idtracker.ai</monospace>, for example, deals only with the first problem: It trains a neural network to detect crossings and essentially ignores the involved individuals until the problem is resolved by movement of the individuals themselves. However, using such an image-based approach can never be fully independent of the species or even video (it has to be retrained for each specific experiment) while also being time-costly to use. In some cases the size of objects might indicate that they contain multiple overlapping objects, while other cases might not allow for such an easy distinction – for example when sexually dimorphic animals (or multiple species) are present at the same time. We propose a method, similar to <monospace>xyTracker</monospace> in that it uses the object’s movement history to detect overlaps. If there are fewer objects in a region than would be expected by looking at previous frames, an attempt is made to split the biggest ones in that area. The size of that area is estimated using the maximal speed objects are allowed to travel per frame (parameter, see documentation track_max_speed). This, of course, requires relatively good predictions or, alternatively, high frame-rates relative to the object’s movement speeds (which are likely necessary anyway to observe behavior at the appropriate time-scales).</p><p>By default, objects suspected to contain overlapping individuals are split by thresholding their background-difference image (see appendix Appendix K), continuously increasing the threshold until the expected number (or more) similarly sized objects are found. Grayscale values and, more generally, the shading of three-dimensional objects and animals often produces a natural gradient (see for example <xref ref-type="fig" rid="fig4">Figure 4</xref>) making this process surprisingly effective for many of the species we tested with. Even when there is almost no visible gradient and thresholding produces holes inside objects, objects are still successfully separated with this approach. Missing pixels from inside the objects can even be regenerated afterwards. The algorithm fails, however, if the remaining objects are too small or are too different in size, in which case the overlapping objects will not be assigned to any trajectory until all involved objects are found again separately in a later frame.</p><p>After an object is assigned to a specific trajectory, two kinds of data (posture and visual-fields) are calculated and made available to the user, which will each be described in one of the following subsections. In the last subsection, we outline how these can be utilized in real-time tracking situations.</p><sec id="s4-2-1"><title>Posture analysis</title><p>Groups of animals are often modeled as systems of simple particles (<xref ref-type="bibr" rid="bib35">Inada and Kawachi, 2002</xref>; <xref ref-type="bibr" rid="bib13">Cavagna et al., 2010</xref>; <xref ref-type="bibr" rid="bib59">Perez-Escudero and de Polavieja, 2011</xref>), a reasonable simplification which helps to formalize/predict behavior. However, intricate behaviors, like courtship displays, can only be fully observed once the body shape and orientation are considered (e.g. using tools such as DeepPoseKit, <xref ref-type="bibr" rid="bib27">Graving et al., 2019</xref>, LEAP <xref ref-type="bibr" rid="bib55">Pereira et al., 2019</xref>/SLEAP <xref ref-type="bibr" rid="bib56">Pereira et al., 2020</xref>, and DeepLabCut, <xref ref-type="bibr" rid="bib47">Mathis et al., 2018</xref>). TRex does not track individual body parts apart from the head and tail (where applicable), but even the included simple and fast 2D posture estimator already allows for deductions to be made about how an animal is positioned in space, bent and oriented – crucial for example when trying to estimate the position of eyes/antennae as part of an analysis, where this is required (e.g. <xref ref-type="bibr" rid="bib71">Strandburg-Peshkin et al., 2013</xref>; <xref ref-type="bibr" rid="bib68">Rosenthal et al., 2015</xref>). When detailed tracking of all extremities is required, TRex offers an option that allows it to interface with third-party software like DeepPoseKit (<xref ref-type="bibr" rid="bib27">Graving et al., 2019</xref>), SLEAP (<xref ref-type="bibr" rid="bib56">Pereira et al., 2020</xref>), or DeepLabCut (<xref ref-type="bibr" rid="bib47">Mathis et al., 2018</xref>). This option (output_image_per_tracklet), when set to true, exports cropped and (optionally) normalized videos per individual that can be imported directly into these tools – where they might perform better than the raw video. Normalization, for example, can make it easier for machine-learning algorithms in these tools to learn where body-parts are likely to be (see <xref ref-type="fig" rid="fig6">Figure 6</xref>) and may even reduce the number of clicks required during annotation.</p><p>In TRex, the 2D posture of an animal consists of (i) an outline around the outer edge of a blob, (ii) a center-line (or midline for short) that curves with the body and (iii) positions on the outline that represent the front and rear of the animal (typically head and tail). Our only assumptions here are that the animal is bilateral with a mirror-axis through its center and that it has a beginning and an end, and that the camera-view is roughly perpendicular to this axis. This is true for most animals, but may not hold for example for jellyfish (with radial symmetry) or animals with different symmetries (e.g. radiolaria (protozoa) with spherical symmetry). Still, as long as the animal is not exactly circular from the perspective of the camera, the midline will follow its longest axis and a posture can be estimated successfully. The algorithm implemented in our software is run for every (cropped out) image of an individual and processes it as follows:</p><list list-type="roman-lower"><list-item><p>A tree-based approach follows edge pixels around an object in a clock-wise manner. Drawing the line <italic>around</italic> pixels, as implemented here, instead of through their centers, as done in comparable approaches, helps with very small objects (e.g. one single pixel would still be represented as a valid outline, instead of a single point).</p></list-item><list-item><p>The pointiest end of the outline is assumed, by default, to be either the tail or the head (based on curvature and area between the outline points in question). Assignment of head vs. tail can be set by the user, seeing as some animals might have ‘pointier’ heads than tails (e.g. termite workers, one of the examples we employ). Posture data coming directly from an image can be very noisy, which is why the program offers options to simplify outline shapes using an Elliptical Fourier Transform (EFT, see <xref ref-type="bibr" rid="bib36">Iwata et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Kuhl and Giardina, 1982</xref>) or smoothing via a simple weighted average across points of the curve (inspired by common subdivision techniques, see <xref ref-type="bibr" rid="bib76">Warren and Weimer, 2001</xref>). The EFT allows for the user to set the desired level of approximation detail (via the number of elliptic fourier descriptors, EFDs) and thus make it ‘rounder’ and less jittery. Using an EFT with just two descriptors is equivalent to fitting an ellipse to the animal’s shape (as, for example, <monospace>xyTracker</monospace> does), which is the simplest supported representation of an animal’s body.</p></list-item><list-item><p>The reference-point chosen in (ii) marks the start for the midline-algorithm. It walks both left and right from this point, always trying to move approximately the same distance on the outline (with limited wiggle-room), while at the same time minimizing the distance from the left to the right point. This works well for most shapes and also automatically yields distances between a midline point and its corresponding two points on the outline, estimating thickness of this object’s body at this point.</p></list-item></list><p>Compared to the tracking itself, posture estimation is a time-consuming process and can be disabled. It is, however, required to estimate – and subsequently normalize – an animal’s orientation in space (e.g. required later in Automatic visual identification based on machine learning), or to reconstruct their visual field as described in the following sub-section.</p></sec><sec id="s4-2-2"><title>Reconstructing 2D visual fields</title><p>Visual input is an important modality for many species (e.g. fish <xref ref-type="bibr" rid="bib71">Strandburg-Peshkin et al., 2013</xref>, <xref ref-type="bibr" rid="bib7">Bilotta and Saszik, 2001</xref> and humans <xref ref-type="bibr" rid="bib16">Colavita, 1974</xref>). Due to its importance in widely used model organisms like zebrafish (<italic>Danio rerio</italic>), we decided to include the capability to conduct a two-dimensional reconstruction of each individual’s visual field as part of the software. The requirements for this are successful posture estimation and that individuals are viewed from above, as is usually the case in laboratory studies.</p><p>The algorithm makes use of the fact that outlines have already been calculated during posture estimation. Eye positions are estimated to be evenly distanced from the ‘snout’ and will be spaced apart depending on the thickness of the body at that point (the distance is based on a ratio, relative to body-size, which can be adjusted by the user). Eye orientation is also adjustable, which influences the size of the stereoscopic part of the visual field. We then use ray-casting to intersect rays from each of the eyes with all other individuals as well as the focal individual itself (self-occlusion). Individuals not detected in the current frame are approximated using the last available posture. Data are organized as a multi-layered 1D-image of fixed size for each frame, with each image prepresenting angles from <inline-formula><mml:math id="inf12"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf13"><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> for the given frame. Simulating a limited field-of-view would thus be as simple as cropping parts of these images off the left and right sides. The different layers per pixel encode:</p><list list-type="order"><list-item><p>identity of the occluder</p></list-item><list-item><p>distance to the occluder</p></list-item><list-item><p>body-part that was hit (distance from the head on the outline in percent).</p></list-item></list><p>While the individuals viewed from above on a computer screen look two-dimensional, one major disadvantage of any 2D approach is, of course, that it is merely a projection of the 3D scene. Any visual field estimator has to assume that, from an individual’s perspective, other individuals act as an occluder in all instances (see <xref ref-type="fig" rid="fig7">Figure 7</xref>). This may only be partly true in the real world, depending on the experimental design, as other individuals may be able to move slightly below, or above, the focal individuals line-of-sight, revealing otherwise occluded conspecifics behind them. We therefore support multiple occlusion-layers, allowing second-order and <italic>N</italic>th-order occlusions to be calculated for each individual.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Visual field estimate of the individual in the center (zoomed in, the individuals are approximately 2 – 3 cm long, Video 15).</title><p>Right (blue) and left (orange) fields of view intersect in the binocular region (pink). Most individuals can be seen directly by the focal individual (1, green), which has a wide field of view of <inline-formula><mml:math id="inf14"><mml:msup><mml:mn>260</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> per eye. Individual three on the top-left is not detected by the focal individual directly and not part of its first-order visual field. However, second-order intersections (visualized by gray lines here) are also saved and accessible through a separate layer in the exported data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig7-v2.tif"/></fig><media id="fig7video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-64000-fig7-video1.mp4"><label>Figure 7—video 1.</label><caption><title>A clip from Video 15, showing TRex’ visual-field estimation for Individual 1 <ext-link ext-link-type="uri" xlink:href="https://youtu.be/yEO_3lpZIzU">https://youtu.be/yEO_3lpZIzU</ext-link>.</title></caption></media></fig-group></sec><sec id="s4-2-3"><title>Realtime tracking option for closed-loop experiments</title><p>Live tracking is supported, as an option to the user, during the recording, or conversion, of a video in TGrabs. When closed-loop feedback is enabled, TGrabs focusses on maintaining stable recording frame-rates and may not track recorded frames if tracking takes too long. This is done to ensure that the recorded file can later be tracked again in full/with higher accuracy (thus no information is lost) if required, and to help the closed-loop feedback to stay synchronized with real-world events.</p><p>During development we worked with a mid-range gaming computer and Basler cameras at 90fps and 2048<sup>2</sup>px resolution, where drawbacks did not occur. Running the program on hardware with specifications below our recommendations (see 2 Results), however, may affect frame-rates as described below.</p><p>TRex loads a prepared <monospace>Python</monospace> script, handing down an array of data per individual in every frame. Which data fields are being generated and sent to the script is selected by the script. Available fields are:</p><list list-type="bullet"><list-item><p>Position</p></list-item><list-item><p>Midline information</p></list-item><list-item><p>Visual field.</p></list-item></list><p>If the script (or any other part of the recording process) takes too long to execute in one frame, consecutive frames may be dropped until a stable frame-rate can be achieved. This scales well for all computer-systems, but results in fragmented tracking data, causing worse identity assignment, and reduces the number of frames and quality of data available for closed-loop feedback. However, since even untracked frames are saved to disk, these inaccuracies can be fixed in TRex later. Alternatively, if live-tracking is enabled but closed-loop feedback is disabled, the program maintains detected objects in memory and tracks them in an asynchronous thread (potentially introducing wait time after the recording stops). When the program terminates, the tracked individual’s data are exported – along with a <monospace>results</monospace> file that can be loaded by the <monospace>tracker</monospace> at a later time.</p><p>In order to make this interface easy to use for prototyping and to debug experiments, the script may be changed during its run-time and will be reloaded if necessary. Errors in the <monospace>Python</monospace> code lead to a temporary pause of the closed-loop part of the program (not the recording) until all errors have been fixed.</p><p>Additionally, thanks to <monospace>Python</monospace> being a fully-featured scripting language, it is also possible to call and send information to other programs during real-time tracking. Communication with other external programs may be necessary whenever easy-to-use <monospace>Python</monospace> interfaces are not available for for example hardware being used by the experimenter.</p></sec></sec><sec id="s4-3"><title>Automatic visual identification based on machine learning</title><p>Tracking, when it is only based on individual’s positional history, can be very accurate under good circumstances and is currently the fastest way to analyze video recordings or to perform closed-loop experiments. However, such tracking methods simply do not have access to enough information to allow them to ensure identities are maintained for the duration of most entire trials – small mistakes can and will happen. There are cases, for example when studying polarity (only based on short trajectory segments), or other general group-level assessments, where this is acceptable and identities do not have to be maintained perfectly. However, consistent identities are required in many individual-level assessments, and with no baseline truth available to correct mistakes, errors start accumulating until eventually all identities are fully shuffled. Even a hypothetical, <italic>perfect</italic> tracking algorithm will not be able to yield correct results in all situations as multiple individuals might go out of view at the same time (e.g. hiding under cover or just occluded by other animals). There is no way to tell who is whom, once they re-emerge.</p><p>The only way to solve this problem is by providing an independent source of information from which to infer identity of individuals, which is of course a principle we make use of all the time in our everyday lives: Facial identification of con-specifics is something that is easy for most humans, to an extent where we sometimes recognize face-like features where there aren’t any. Our natural tendency to find patterns enables us to train experts on recognizing differences between animals, even when they belong to a completely different taxonomic order. Tracking individuals is a demanding task, especially with large numbers of moving animals (<xref ref-type="bibr" rid="bib45">Liu et al., 2009</xref> shows humans to be effective for up to four objects). Human observers are able to solve simple memory recall tasks for 39 objects at only 92 % correct (see <xref ref-type="bibr" rid="bib34">Humphrey and Khan, 1992</xref>), where the presented objects do not even have to be identified individually (just classified as old/new) and contain more inherent variation than most con-specific animals would. Even with this being true, human observers are still the most efficient solution in some cases (e.g. for long-lived animals in complex habitats). Enhancing visual inter-individual differences by attaching physical tags is an effective way to make the task easier and more straight-forward to automate. RFID tags are useful in many situations, but are also limited since individuals have to be in very close proximity to a sensor in order to be detected (<xref ref-type="bibr" rid="bib8">Bonter and Bridge, 2011</xref>). Attaching fiducial markers (such as QR codes) to animals allows for a very large number (thousands) of individuals to be uniquely identified at the same time (see <xref ref-type="bibr" rid="bib25">Gernat et al., 2018</xref>; <xref ref-type="bibr" rid="bib79">Wild et al., 2020</xref>; <xref ref-type="bibr" rid="bib48">Mersch et al., 2013</xref>; <xref ref-type="bibr" rid="bib17">Crall et al., 2015</xref>) – and over a much greater distance than RFID tags. Generating codes can also be automated, generating tags with optimal visual inter-marker distances (<xref ref-type="bibr" rid="bib24">Garrido-Jurado et al., 2016</xref>), making it feasible to identify a large number of individuals with minimal tracking mistakes.</p><p>While physical tagging is often an effective method by which to identify individuals, it requires animals to be caught and manipulated, which can be difficult (<xref ref-type="bibr" rid="bib48">Mersch et al., 2013</xref>) and is subject to the physical limitations of the respective system. Tags have to be large enough so a program can recognize it in a video stream. Even worse, especially with increased relative tag-size, the animal’s behavior may be affected by the presence of the tag or during its application (<xref ref-type="bibr" rid="bib19">Dennis et al., 2008</xref>; <xref ref-type="bibr" rid="bib53">Pankiw and Page, 2003</xref>; <xref ref-type="bibr" rid="bib69">Sockman and Schwabl, 2001</xref>), and there might be no way for experimenters to necessarily know that it did (unless with considerable effort, see <xref ref-type="bibr" rid="bib73">Switzer and Combes, 2016</xref>). In addition, for some animals, like fish and termites, attachment of tags that are effective for discriminating among a large number of individuals can be problematic, or impossible.</p><p>Recognizing such issues, (<xref ref-type="bibr" rid="bib58">Pérez-Escudero et al., 2014</xref>) first proposed an algorithm termed <italic>idtracker</italic>, generalizing the process of pattern recognition for a range of different species. Training an expert program to tell individuals apart, by detecting slight differences in patterning on their bodies, allows the correction of identities without any human involvement. Even while being limited to about 15 individuals per group, this was a very promising approach. It became much improved upon only a few years later by the same group in their software <monospace>idtracker.ai</monospace> (<xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref>), implementing a paradigm shift from explicit, hard-coded, color-difference detection to using more general machine learning methods instead – increasing the supported group size by an order of magnitude.</p><p>We employ a method for visual identification in TRex that is similar to the one used in <monospace>idtracker.ai</monospace>, where a neural network is trained to visually recognize individuals and is used to correct tracking mistakes automatically, without human intervention – the network layout (see <italic>1</italic> c) is almost the same as well (differing only by the addition of a pre-processing layer and using 2D- instead of 1D-dropout layers). However, in TRex, processing speed and chances of success are improved (the former being greatly improved) by (i) minimizing the variance landscape of the problem and (ii) exploring the landscape to our best ability, optimally covering all poses and lighting-conditions an individual can be in, as well as (iii) shortening the training duration by significantly altering the training process – for example choosing new samples more adaptively and using different stopping-criteria (accuracy, as well as speed, are part of the later evaluation).</p><p>While 4.2 Tracking already <italic>tries</italic> to (within each trajectory) consistently follow the same individual, there is no way to ensure/check the validity of this process without providing independent identity information. Generating this source of information, based on the visual appearance of individuals, is what the algorithm for visual identification, described in the following subsections, aims to achieve. Re-stated simply, the goal of using automatic visual identification is to obtain reliable predictions of the identities of all (or most) objects in each frame. Assuming these predictions are of sufficient quality, they can be used to detect and correct potential mistakes made during 4.2 Tracking by looking for identity switches within trajectories. Ensuring that predicted identities within trajectories are consistent, by proxy, also ensures that each trajectory is consistently associated with a single, real individual. In the following, before describing the four stages of that algorithm, we will point out key aspects of how tracking/image data are processed and how we addressed the points (i)-(iii) above and especially highlight the features that ultimately improved performance compared to other solutions.</p><sec id="s4-3-1"><title>Preparing tracking-data</title><p>Visual identification starts out only with the trajectories that the 4.2 Tracking provides. Tracking, on its own, is already an improvement over other solutions, especially since (unlike e.g. <monospace>idtracker.ai</monospace>) TRex makes an effort to separate overlapping objects (see the Appendix K Algorithm for splitting touching individuals) and thus is able to keep track of individuals for longer (see <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>). Here, we – quite conservatively – assume that, after every problematic situation (defined in the list below), the assignments made by our tracking algorithm are wrong. Whenever a problematic situation is encountered as part of a trajectory, we split the trajectory at that point. This way, all trajectories of all individuals in a video become an assortment of trajectory snippets (termed ‘segments’ from here on), which are clear of problematic situations, and for each of which the goal is to find the correct identity (‘correct’ meaning that identities are consistently assigned to the same <italic>real</italic> individual throughout the video). Situations are considered ‘problematic’, and cause the trajectory to be split, when:</p><list list-type="bullet"><list-item><p>The individual has been lost for at least one frame. For example when individuals are moving unexpectedly fast, are occluded by other individuals/the environment, or simply not present anymore (e.g. eaten).</p></list-item><list-item><p>Uncertainty of assignment was too high (<inline-formula><mml:math id="inf15"><mml:mrow><mml:mi/><mml:mo mathvariant="normal">&gt;</mml:mo><mml:mrow><mml:mn mathvariant="normal">50</mml:mn><mml:mo lspace="0pt" mathvariant="normal" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) for example due to very high movement speeds or extreme variation in size between frames. With simpler tracking tasks in mind, these segments are kept as <italic>connected</italic> tracks, but regarded as separate ones here.</p></list-item><list-item><p>Timestamps suggest skipped frames. Missing frames in the video may cause wrong assignments and are thus treated as if the individuals have been lost. This distinction can only be made if accurate frame timings are available (when recording using TGrabs or provided alongside the video files in separate npz files).</p></list-item></list><p>Unless one of the above conditions becomes true, a segment is assumed to be consecutive and connected; that is, throughout the whole segment, no mistakes have been made that lead to identities being switched. Frames where all individuals are currently within one such segment at the same time will henceforth be termed <italic>global segments</italic>.</p><p>Since we know that there are no problematic situations inside each per-individual segment, and thus also not across individuals within the range of a global segment, we can choose any global segment as a basis for an initial, arbitrary assignment of identities to trajectories. One of the most important steps of the identification algorithm then becomes deciding which global segment is the best starting point for the training. If a mistake is made here, consecutive predictions for other segments will fail and/or produce unreliable results in general.</p><p>Only a limited set of global segments is kept – striking a balance between respecting user-given constraints and capturing as much of the variance as possible. In many of the videos used for evaluation, we found that only few segments had to be considered – however, computation time is ultimately bounded by reducing the number of qualifying segments. While this is true, it is also beneficial to avoid auto-correlation by incorporating samples from all sections of the video instead of only sourcing them from a small portion – to help achieve a balance, global segments are binned by their middle frame into four bins (each quarter of the video being a bin) and then reducing the number of segments inside each bin. With that goal in mind, we sort the segments within bins by their ‘quality’ – a combination of two factors:</p><list list-type="order"><list-item><p>To capture as much as possible the variation due to an individual’s own movement, as well as within the background that it moves across, a ‘good’ segment should be a segment where all individuals move as much as possible and also travel as large a distance as possible. Thus, we derive a per-individual <italic>spatial coverage descriptor</italic> for the given segment by dissecting the arena (virtually) into a grid of equally sized, rectangular ‘cells’ (depending on the aspect ratio of the video). Each time an individual’s center-point moves from one cell to the next, a counter is incremented for that individual. To avoid situations where, for example, all individuals but one are moving, we only use the lowest per-individual spatial coverage value to represent a given segment.</p></list-item><list-item><p>It is beneficial to have more examples for the network to learn from. Thus, as a second sorting criterion, we use the average number of samples per individual.</p></list-item></list><p>After being sorted according to these two metrics, the list of segments per bin is reduced, according to a user-defined variable (four by default), leaving only the most viable options per quarter of video.</p><p>The number of visited cells may, at first, appear to be essentially equivalent to a spatially normalized <italic>distance travelled</italic> (as used in <monospace>idtracker.ai</monospace>). In edge cases, where individuals never stop or always stop, both metrics can be very similar. However, one can imagine an individual continuously moving around in the same corner of the arena, which would be counted as an equally good segment for that individual as if it had traversed the whole arena (and thus capturing all variable environmental factors). In most cases, using highly restricted movement for training is problematic, and worse than using a shorter segment of the individual moving diagonally through the entire space, since the latter captures more of the variation within background, lighting conditions and the animals movement in the process.</p></sec><sec id="s4-3-2"><title>Minimizing the variance landscape by normalizing samples</title><p>A big strength of machine learning approaches is their resistance to noise in the data. Generally, any machine learning method will likely still converge – even with noisy data. Eliminating unnecessary noise and degrees of freedom in the dataset, however, will typically help the network to converge much more quickly: Tasks that are easier to solve will of course also be solved more accurately within similar or smaller timescales. This is due to the optimizer not having to consider various parts of the possible parameter-space during training, or, put differently, shrinking the overall parameter-space to the smallest possible size without losing important information. The simplest such optimization included in most tracking and visual identification approaches is to segment out the objects and centering the individuals in the cropped out images. This means that (i) the network does not have to consider the whole image, (ii) needs only to consider one individual at a time and (iii) the corners of the image can most likely be neglected.</p><p>Further improving on this, approaches like <monospace>idtracker.ai</monospace> align all objects along their most-elongated axis, essentially removing global orientation as a degree of freedom. The orientation of an arbitrary object can be calculated for example using an approach often referred to as image-moments (<xref ref-type="bibr" rid="bib30">Hu, 1962</xref>), yielding an angle within <inline-formula><mml:math id="inf16"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mn>180</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>. Of course, this means that:</p><list list-type="order"><list-item><p>circular objects have a random (noisy) orientation</p></list-item><list-item><p>elongated objects (e.g. fish) can be either head-first or flipped by <inline-formula><mml:math id="inf17"><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> and there is no way to discriminate between those two cases (see second row, <xref ref-type="fig" rid="fig8">Figure 8</xref>)</p></list-item><list-item><p>a C-shaped body deformation, for example, results in a slightly bent axis, meaning that the head will not be in exactly the same position as with a straight posture of the animal.</p></list-item></list><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Comparison of different normalization methods.</title><p>Images all stem from the same video and belong to the same identity. The video has previously been automatically corrected using the visual identification. Each object visible here consists of <italic>N</italic> images <inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that have been accumulated into a single image using <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, with <italic>min</italic> being the element-wise minimum across images. The columns represent same samples from the same frames, but normalized in three different ways: In (a), images have not been normalized at all. Images in (b) have been normalized by aligning the objects along their main axis (calculated using <italic>image-moments</italic>), which only gives the axis within 0– 180 degrees. In (c), all images have been aligned using posture information generated during the tracking process. As the images become more and more recognizable to <italic>us</italic> from left to right, the same applies to a network trying to tell identities apart: Reducing noise in the data speeds up the learning process.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-fig8-v2.tif"/></fig><p>Each of these issues adds to the things the network has to learn to account for, widening the parameter-space to be searched and increasing computation time. However, barring the first point, each problem can be tackled using the already available posture information. Knowing head and tail positions and points along the individual’s center-line, the individual’s heads can be locked roughly into a single position. This leaves room only for their rear end to move, reducing variation in the data to a minimum (see <xref ref-type="fig" rid="fig8">Figure 8</xref>). In addition to faster convergence, this also results in better generalization right from the start and even with a smaller number of samples per individual (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). For further discussion of highly deformable bodies, such as of rodents, please see Appendix (Appendix L Posture and Visual Identification of Highly-Deformable Bodies).</p></sec><sec id="s4-3-3"><title>Guiding the training process</title><p>Per batch, the stochastic gradient descent is directed by the local accuracy (a fraction of correct/total predictions), which is a simple and commonly used metric that has no prior knowledge of where the samples within a batch come from. This has the desirable consequence that no knowledge about the temporal arrangement of images is necessary in order to train and, more importantly, to apply the network later on.</p><boxed-text id="box1"><label>Box 1.</label><caption><p>Calculating uniqueness for a frame.</p></caption><p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><tbody><tr valign="top"><td colspan="2">Algorithm 1: The algorithm used to calculate the uniqueness score for an individual frame. <break/>Probabilities <inline-formula><mml:math id="inf20"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> are predictions by the pre-trained network. During the accumulation these predictions will gradually improve proportional to the global training quality. Multiplying the unique percentage <inline-formula><mml:math id="inf21"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>uids</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by the (scaled) mean probability deals with cases of low accuracy, where individuals switch every frame (but uniquely).</td></tr><tr valign="top"><td colspan="2"><bold>Data</bold>: frame x</td></tr><tr valign="top"><td colspan="2"><bold>Result</bold>:Uniqueness score for frame x</td></tr><tr valign="top"><td colspan="2">uids = map{}</td></tr><tr valign="top"><td colspan="2"><inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the probability of blobb to be identity i</td></tr><tr valign="top"><td colspan="2">f(x) returns a list of the tracked objects in frame </td></tr><tr valign="top"><td colspan="2">x <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a shift of roughly <inline-formula><mml:math id="inf24"><mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and non-linear scaling of values <inline-formula><mml:math id="inf25"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>v</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</td></tr><tr valign="top"><td colspan="2"/></tr><tr valign="top"><td colspan="2"><bold>for each</bold> object <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold></td></tr><tr valign="top"><td colspan="2">    <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>identities</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr valign="top"><td colspan="2">    <bold>if</bold> maxid <inline-formula><mml:math id="inf29"><mml:mo>∈</mml:mo></mml:math></inline-formula> uids <bold>then</bold></td></tr><tr valign="top"><td colspan="2">       <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td colspan="2">    <bold>else</bold></td></tr><tr valign="top"><td colspan="2">       <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr valign="top"><td colspan="2">    <bold>end</bold></td></tr><tr valign="top"><td colspan="2"><bold>end</bold></td></tr><tr valign="top"><td colspan="2"><bold>return</bold> <inline-formula><mml:math id="inf32"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>uids</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>uids</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>uids</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>uids</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr></tbody></table></table-wrap></p></boxed-text><p>In order to achieve accurate results quickly across batches, while at the same time making it possible to indicate to the user potentially problematic sequences within the video, we devised a metric that can be used to estimate local as well as global training quality: We term this uniqueness and it combines information about objects within a frame, following the principle of non-duplication; images of individuals within the same frame are required to be assigned different identities by the networks predictions.</p><p>The program generates image data for evenly spaced frames across the entire video. All images of tracked individuals within the selected frames are, after every epoch of the training, passed on to the network. It returns a vector of probabilities <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each image <italic>i</italic> to be identity <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with <italic>N</italic> being the number of individuals. Based on these probabilities, uniqueness can be calculated as in Box 1, evenly covering the entire video. The magnitude of this probability vector per image is taken into account, rewarding strong predictions of <inline-formula><mml:math id="inf35"><mml:mrow><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and punishing weak predictions of <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Uniqueness is not integrated as part of the loss function, but it is used as a global gradient before and after each training unit in order to detect global improvements. Based on the average uniqueness calculated before and after a training unit, we can determine whether to stop the training, or whether training on the current segment made our results worse (faulty data). If uniqueness is consistently high throughout the video, then training has been successful and we may terminate early. Otherwise, valleys in the uniqueness curve indicate bad generalization and thus currently missing information regarding some of the individuals. In order to detect problematic sections of the video we search for values below <inline-formula><mml:math id="inf37"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mn>0.5</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, meaning that the section potentially contains new information we should be adding to our training data. Using accuracy per-batch and then using uniqueness to determine global progress, we get the best of both worlds: A context-free prediction method that is trained on global segments that are strategically selected by utilizing local context information.</p><p>The closest example of such a procedure in <monospace>idtracker.ai</monospace> is the termination criterion after <italic>protocol 1</italic>, which states that individual segments have to be consistent and certain enough in all global segments in order to stop iterating. While this seems to be similar at first, the way accuracy is calculated and the terminology here are quite different: (i) Every metric in <monospace>idtracker.ai's</monospace> final assessment after <italic>protocol one</italic> is calculated at segment-level, not utilizing per-frame information. <italic>Uniqueness</italic> works per-frame, not per segment, and considers individual frames to be entirely independent from each other. It can be considered a much stronger constraint set upon the network’s predictive ability, seeing as it basically counts the number of times mistakes are estimated to have happened within single frames. Averaging only happens <italic>afterwards</italic>. (ii) The terminology of identities being unique is only used in <monospace>idtracker.ai</monospace> once after <italic>procotol one</italic> and essentially as a binary value, not recognizing its potential as a descendable gradient. Images are simply added until a certain percentage of images has been reached, at which point accumulation is terminated. (iii) Testing uniqueness is much faster than testing network accuracy across segments, seeing as the same images are tested over and over again (meaning they can be cached) and the testing dataset can be much smaller due to its locality. <italic>Uniqueness</italic> thus provides a stronger gradient estimation, while at the same time being more local (meaning it can be used independently of whether images are part of global segments), as well as more manageable in terms of speed and memory size.</p><p>In the next four sections, we describe the training phases of our algorithm (1-3), and how the successfully trained network can be used to automatically correct trajectories based on its predictions (4).</p><sec id="s4-3-3-1"><title>The initial training unit</title><p>All global segments are considered and sorted by the criteria listed below in Accumulation of additional segments and stopping-criteria. The best suitable segment from the beginning of that set of segments is used as the initial dataset for the network. Images are split into a training and a validation set (4:1 ratio). Efforts are made to equalize the sample sizes per class/identity beforehand, but there has to always be a trade-off between similar sample sizes (encouraging unbiased priors) and having as many samples as possible available for the network to learn from. Thus, in order to alleviate some of the severity of dealing with imbalanced datasets, the performance during training iterations is evaluated using a categorical focal loss function (<xref ref-type="bibr" rid="bib43">Lin et al., 2020</xref>). Focal loss down-weighs classes that are already reliably predicted by the network and in turn emphasizes neglected classes. An Adam optimizer (<xref ref-type="bibr" rid="bib4">Bengio et al., 2015</xref>) is used to traverse the loss landscape towards the global (or to at least a local) minimum.</p><p>The network layout used for the classification in TRex (see <xref ref-type="fig" rid="fig1">Figure 1c</xref>) is a typical Convolutional Neural Network (CNN). The concepts of ‘convolutional’ and ‘downsampling’ layers, as well as the back-propagation used during training, are not new. They were introduced in <xref ref-type="bibr" rid="bib23">Fukushima, 1988</xref>, inspired originally by the work of Hubel and Wiesel on cats and rhesus monkeys (<xref ref-type="bibr" rid="bib31">Hubel and Wiesel, 1959</xref>; <xref ref-type="bibr" rid="bib32">Hubel and Wiesel, 1963</xref>; <xref ref-type="bibr" rid="bib78">Wiesel and Hubel, 1966</xref>), describing receptive fields and their hierarchical structure in the visual cortex. Soon afterward, in <xref ref-type="bibr" rid="bib42">LeCun et al., 1989</xref>, CNNs, in combination with back-propagation, were already successfully used to recognize handwritten ZIP codes – for the first time, the learning process was fully automated. A critical step towards making their application practical, and the reason they are popular today.</p><p>The network architecture used in our software is similar to the identification module of the network in <xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref>, and is, as in most typical CNNs, (reverse-)pyramid-like. However, key differences between TRex’ and <monospace>idtracker.ai's</monospace> procedure lie with the way that training data is prepared (see previous sections) and how further segments are accumulated/evaluated (see next section). Furthermore, contrary to <monospace>idtracker.ai's</monospace> approach, images in TRex are augmented (during training) before being passed on to the network. While this augmentation is relatively simple (random shift of the image in x-direction), it can help to account for positional noise introduced by for example the posture estimation or the video itself when the network is used for predictions later on <xref ref-type="bibr" rid="bib57">Perez and Wang, 2017</xref>. We do not flip the image in this step, or rotate it, since this would defeat the purpose of using orientation normalization in the first place (as in Minimizing the variance landscape by normalizing samples, see <xref ref-type="fig" rid="fig8">Figure 8</xref>). Here, in fact, normalization of object orientation (during training and predictions) could be seen as a superior alternative to data augmentation.</p><p>The input data for TRex’ network is a single, cropped grayscale image of an individual. This image is first passed through a ‘lambda’ layer (blue) that normalizes the pixel values, dividing them by half the value limit of <inline-formula><mml:math id="inf38"><mml:mrow><mml:mrow><mml:mn>255</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mn>127.5</mml:mn></mml:mrow></mml:math></inline-formula> and subtracting 1 – this moves them into the range of <inline-formula><mml:math id="inf39"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. From then on, sections are a combination of convolutional layers (kernel sizes of 16, 64, and 100 pixels), each followed by a 2D (2 × 2) max-pooling and a 2D spatial dropout layer (with a rate of 0.25). Within each of these blocks the input data is reduced further, focussing it down to information that is deemed important. Toward the end, the data are flattened and flow into a densely connected layer (100 units) with exactly as many outputs as the number of classes. The output is a vector with values between 0 and 1 for all elements of the vector, which, due to softmax-activation, sum to 1.</p><p>Training commences by performing a stochastic gradient descent (using the Adam optimizer, see <xref ref-type="bibr" rid="bib4">Bengio et al., 2015</xref>), which iteratively minimizes the error between network predictions and previously known associations of images with identities – the original assignments within the initial frame segment. The optimizer’s behavior in the last five epochs is continuously observed and training is terminated immediately if one of the following criteria is met:</p><list list-type="bullet"><list-item><p>the maximum number of iterations is reached (150 by default, but can be set by the user)</p></list-item><list-item><p>a plateau is achieved at a high per-class accuracy</p></list-item><list-item><p>overfitting/overly optimizing for the training data at the loss of generality</p></list-item><list-item><p>no further improvements can be made (due to the accuracy within the current training data already being 1).</p></list-item></list><p>The initial training unit is also by far the most important as it determines the predicted identities within further segments that are to be added. It is thus less risky to overfit than it is important to get high-quality training results, and the algorithm has to be relatively conservative regarding termination criteria. Later iterations, however, are only meant to extend an already existing dataset and thus (with computation speed in mind) allow for additional termination criteria to be added:</p><list list-type="bullet"><list-item><p>plateauing at/circling around a certain val_loss level</p></list-item><list-item><p>plateauing around a certain uniqueness level.</p></list-item></list></sec><sec id="s4-3-3-2"><title>Accumulation of additional segments and stopping-criteria</title><p>If necessary, initial training results can be improved by adding more samples to the active dataset. This could be done manually by the user, always trying to select the most promising segment next, but requiring such manual work is not acceptable for high-throughput processing. Instead, in order to translate this idea into features that can be calculated automatically, the following set of metrics is re-generated per (yet inactive) segment after each successful step:</p><list list-type="order"><list-item><p>Average uniqueness index (rounded to an integer percentage in 5 % steps)</p></list-item><list-item><p>Minimal distance to regions that have previously been trained on (rounded to the next power of two), larger is better as it potentially includes samples more different from the already known ones</p></list-item><list-item><p>Minimum <italic>cells visited</italic> per individual (larger is better for the same reason as 2)</p></list-item><list-item><p>Minimum average samples per individual (larger is better)</p></list-item><list-item><p>Whether its image data has already been generated before (mostly for saving memory)</p></list-item><list-item><p>The uniqueness value is smaller than <inline-formula><mml:math id="inf40"><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> after five steps, with <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> being the best uniqueness value previous to the current accumulation step.</p></list-item></list><p>With the help of these values, the segment list is sorted and the best segment selected to be considered next. Adding a segment to a set of already active samples requires us to correct the identities inside it, potentially switching temporary identities to represent the same <italic>real</italic> identities as in our previous data. This is done by predicting identities for the new samples using the network that has been trained on the old samples. Making mistakes here can lead to significant subsequent problems, so merely plausible segments will be added – meaning only those samples are accepted for which the predicted IDs are <italic>unique</italic> within each unobstructed sequence of frames for every temporary identity. If multiple temporary individuals are predicted to be the same real identity, the segment is saved for later and the search continues.</p><p>If multiple additional segments are found, the program tries to actively improve local uniqueness valleys by adding samples first from regions with comparatively <italic>low</italic> accuracy predictions. Seeing as low accuracy regions will also most likely fail to predict unique identities, it is important to emphasize here that this is generally not a problem for the algorithm: Failed segments are simply ignored and can be inserted back into the queue later. Smoothing the curve also makes sure to prefer regions close to valleys, making the algorithm follow the valley walls upwards in both directions.</p><p>Finishing a training unit does not necessarily mean that it was successful. Only the network states improving upon results from previous units are considered and saved. Any training result – except the initial one – may be rejected after training in case the uniqueness score has not improved globally, or at least remained within 99 % of the previous best value. This ensures stability of the process, even with tracking errors present (which can be corrected for later on, see next section). If a segment is rejected, the network is restored to the best recorded state.</p><p>Each new segment is always combined with regularly sampled data from previous steps, ensuring that identities don’t switch back and forth between steps due to uncertain predictions. If switching did occur, then the uniqueness and accuracy values can never reach high value regimes – leading to the training unit being discarded as a result. The contribution of each previously added segment <italic>R</italic> is limited to <inline-formula><mml:math id="inf42"><mml:mrow><mml:mo stretchy="false">⌈</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>samples</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>max</mml:mi></mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⌉</mml:mo></mml:mrow></mml:math></inline-formula> samples, with <italic>N</italic> as the total number of frames in global segments for this individual and <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>samples</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:math></inline-formula> a constant that is calculated using image size and memory constraints (or 1 GB by default). <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>R</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula> is the actual <italic>usable</italic> number of images in segment <italic>R</italic>. This limitation is an attempt to not bias the priors of the network by sub-sampling segments according to their contribution to the total number of frames in global segments.</p><p>Training is considered to be successful globally, as soon as either (i) accumulative individual gaps between sampled regions is less than 25 % of the video length for all individuals, or (ii) uniqueness has reached a value higher than <inline-formula><mml:math id="inf45"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mn>0.5</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>id</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></inline-formula> (1) so that almost all detected identities are present exactly once per frame. Otherwise, training will be continued as described above with additional segments – each time extending the percentage of images seen by the network further.</p><p>Training accuracy/consistency could potentially be further improved by letting the program add an arbitrary amount of segments, however we found this not to be necessary in any of our test-cases. Users are allowed to set a custom limit if required in their specific cases.</p></sec><sec id="s4-3-3-3"><title>The final training unit</title><p>After the accumulation phase, one last training step is performed. In previous steps, validation data has been kept strictly separate from the training set to get a better gauge on how generalizable the results are to unseen parts of the video. This is especially important during early training units, since ‘overfitting’ is much more likely to occur in smaller datasets and we still potentially need to add samples from different parts of the video. Now that we are not going to extend our training dataset anymore, maintaining generalizibility is no longer the main objective – so why not use <italic>all</italic> of the available data? The entire dataset is simply merged and sub-sampled again, according to the memory strategy used. Network training is started, with a maximum of <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo>;</mml:mo><mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>epochs</mml:mi></mml:mrow><mml:mo>*</mml:mo><mml:mn>0.25</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> iterations (max_epochs is 150 by default). During this training, the same stopping-criteria apply as during the initial step.</p><p>Even if we tolerate the risk of potentially overfitting on the training data, there is still a way to detect overfitting if it occurs: Only training steps that lead to improvements in mean uniqueness across the video are saved. Often, if prediction results become worse (e.g. due to overfitting), multiple individuals in a single frame are predicted to be the same identity – precisely the problem which our uniqueness metric was designed to detect.</p><p>For some videos, this is the step where most progress is made (e.g. Video 9). The reason being that this is the first time when all the training data from all segments is considered at once (instead of mostly the current segment plus fewer samples from previously accepted segments), and samples from all parts of the video have an equal likelihood of being used in training after possible reduction due to memory-constraints.</p></sec><sec id="s4-3-3-4"><title>Assigning identities based on network predictions</title><p>After the network has been successfully trained, all parts of the video which were not part of the training are packaged together and the network calculates predictive probabilities for each image of each individual to be any of the available identities. The vectors returned by the network are then averaged per consecutive segment per individual. The average probability vectors for all overlapping segments are weighed against each other – usually forcing assignment to the most likely identity (ID) for each segment, given that no other segments have similar probabilities. When referring to segments here, meant is simply a number of consecutive frames of one individual that the tracker is fairly sure does <italic>not</italic> contain any mix-ups. We implemented a way to detect tracking mistakes, which is mentioned later.</p><p>If an assignment is ambiguous, meaning that multiple segments <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> overlapping in time have the same maximum probability index <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mspace width="thickmathspace"/><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the segment to belong to a certain identity (i), a decision has to be made. Assignments are deferred if the ratio<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi mathvariant="normal">∀</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>between any two maximal probabilities is <italic>larger than</italic> 0.6 for said <italic>i</italic> (<inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is inverted if it is greater than 1). In such a case, we rely on the general purpose tracking algorithm to pick a sensible option – other identities might even be successfully assigned (using network predictions) in following frames, which is a complexity we do not have to deal with here. In case all ratios are <italic>below</italic> 0.6, when the best choices per identity are not too ambiguous, the following steps are performed to resolve remaining conflicts:</p><list list-type="order"><list-item><p>Count the number of samples <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the current segment, and the number of samples <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the other segment that this segment is compared to</p></list-item><list-item><p>Calculate average probability vectors <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></p></list-item><list-item><p>If <inline-formula><mml:math id="inf54"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, then assign the current segment with the ID in question. Otherwise assign the ID to the other segment. Where:</p><list list-type="simple"><list-item><p><disp-formula id="equ2"><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></list-item></list></list-item></list><p>This procedure prefers segments with larger numbers of samples over segments with fewer samples, ensuring that identities are not switched around randomly whenever a short segment (e.g. of noisy data) is predicted to be the given identity for a few frames – at least as long as a better alternative is available. The non-linearity in <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> exaggerates differences between lower values and dampens differences between higher values: For example, the quality of a segment with 4000 samples is barely different from a segment with 5000 samples; however, there is likely to be a significant quality difference between segments with 10 and 100 samples.</p><p>In case something goes wrong during the tracking, for example an individual is switched with another individual without the program knowing that it might have happened, the training might still be successful (for example if that particular segment has not been used for training). In such cases, the program tries to correct for identity switches mid-segment by calculating a running-window median identity throughout the whole segment. If the identity switches for a significant length of time, before identities are assigned to segments, the segment is split up at the point of the first change within the window and the two parts are handled as separate segments from then on.</p></sec></sec></sec><sec id="s4-4"><title>Software and licenses</title><p>TRex is published under the GNU GPLv3 license (see <ext-link ext-link-type="uri" xlink:href="https://choosealicense.com/licenses/gpl-3.0/">here</ext-link> for permissions granted by GPLv3). All the codes have been written by the first author of this paper (a few individual lines of code from other sources have been marked inside the code). While none of these libraries are distributed alongside TRex (they have to be provided separately), the following libraries are used: OpenCV (<ext-link ext-link-type="uri" xlink:href="https://opencv.org/about/">opencv.org</ext-link>) is a core library, used for all kinds of image manipulation. GLFW (<ext-link ext-link-type="uri" xlink:href="https://www.glfw.org/">glfw.org</ext-link>) helps with opening application windows and maintaining graphics contexts, while DearImGui (<ext-link ext-link-type="uri" xlink:href="http://www.oberhumer.com/opensource/lzo/#minilzo">github.com/ocornut/imgui</ext-link>) helps with some more abstractions regarding graphics. pybind11 (<xref ref-type="bibr" rid="bib37">Jakob et al., 2017</xref>) for Python integration within a C++ environment. miniLZO (<ext-link ext-link-type="uri" xlink:href="http://www.oberhumer.com/opensource/lzo/">oberhumer.com/opensource/lzo</ext-link>) is used for compression of PV frames. Optional bindings are available to FFMPEG (<ext-link ext-link-type="uri" xlink:href="http://ffmpeg.org/">ffmpeg.org</ext-link>) and libpng libraries, if available. (optional) GNU Libmicrohttpd (<ext-link ext-link-type="uri" xlink:href="https://www.gnu.org/software/libmicrohttpd/">gnu.org/software/libmicrohttpd</ext-link>), if available, can be used for an HTTP interface of the software, but is non-essential.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank A Albi, F Nowak, H Hugo, D E Bath, F Oberhauser, H Naik, J Graving, I Etheredge for helping with their insights, by providing videos, for comments on the manuscript, testing the software and for frequent coffee breaks during development. The development of this software would not have been possible without them. We thank D Mink and M Groettrup providing additional video material of mice. We thank the reviewers and editors for their constructive and useful comments and suggestions. IDC acknowledges support from the NSF (IOS-1355061), the Office of Naval Research grant (ONR, N00014-19-1-2556), the Struktur- und Innovationsfunds für die Forschung of the State of Baden-Württemberg, the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy–EXC 2117 – 422037984, and the Max Planck Society.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: We herewith confirm that the care and use of animals described in this work is covered by the protocols 35-9185.81/G-17/162, 35-9185.81/G-17/88 and 35-9185.81/G-16/116 granted by the Regional Council of the State of Baden-Württemberg, Freiburg, Germany, to the Max Planck Institute of Animal Behavior in accordance with the German Animal Welfare Act (TierSchG) and the Regulation for the Protection of Animals Used for Experimental or Other Scientific Purposes (Animal Welfare Regulation Governing Experimental Animals - TierSchVersV).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-64000-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Video data that has been used in the evaluation of TRex has been deposited in MPG Open Access Data Repository (Edmond), under the Creative Commons BY 4.0 license, at <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17617/3.4y">https://dx.doi.org/10.17617/3.4y</ext-link> Most raw videos have been trimmed, since original files are each up to 200GB in size. Pre-processed versions (in PV format) are included, so that all steps after conversion can be reproduced directly (conversion speeds do not change with video length, so proportional results are reproducible as well). Full raw videos are made available upon reasonable request. All analysis scripts, scripts used to process the original videos, and the source code/pre-compiled binaries (linux-64) that have been used, are archived in this repository. Most intermediate data (PV videos, log files, tracking data, etc.) are included, and the binaries along with the scripts can be used to automatically generate all intermediate steps. The application source code is available for free under <ext-link ext-link-type="uri" xlink:href="https://github.com/mooch443/trex">https://github.com/mooch443/trex</ext-link>. Videos 11, 12 and 13 are part of idtracker.ai's example videos: URL <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/file/d/1pAR6oJjrEn7jf_OU2yMdyT2UJZMTNoKC/view?usp=sharing">https://drive.google.com/file/d/1pAR6oJjrEn7jf_OU2yMdyT2UJZMTNoKC/view?usp=sharing</ext-link> (10_zebrafish.tar.gz) [Francisco Romero, 2018, Examples for idtracker.ai, Online, Accessed 23-Oct-2020]; Video 7 (video_example_100fish_1min.avi): URL <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/file/d/1Tl64CHrQoc05PDElHvYGzjqtybQc4g37/view?usp=sharing">https://drive.google.com/file/d/1Tl64CHrQoc05PDElHvYGzjqtybQc4g37/view?usp=sharing</ext-link> [Francisco Romero, 2018, Examples for idtracker.ai, Online, Accessed 23-Oct-2020]; V1 from Appendix 12: <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/drive/folders/1Nir2fzgxofz-fcojEiG_JCNXsGQXj_9k">https://drive.google.com/drive/folders/1Nir2fzgxofz-fcojEiG_JCNXsGQXj_9k</ext-link> [Francisco Romero, 2018, Examples for idtracker.ai, Online, Accessed 09-Feb-2021].</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Walter</surname><given-names>T</given-names></name><name><surname>Albi</surname><given-names>A</given-names></name><name><surname>Bath</surname><given-names>D</given-names></name><name><surname>Hugo</surname><given-names>H</given-names></name><name><surname>Oberhauser</surname><given-names>F</given-names></name><name><surname>Mink</surname><given-names>D</given-names></name><name><surname>Groettrup</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Reproduction Data for: TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</data-title><source>MPDL Edmond</source><pub-id assigning-authority="EMDB" pub-id-type="accession" xlink:href="https://edmond.mpdl.mpg.de/imeji/collection/eVbVH0_57TwQsAe8">eVbVH0_57TwQsAe8</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>AbuBaker</surname> <given-names>A</given-names></name> <name><surname>Qahwaji</surname> <given-names>R</given-names></name> <name><surname>Ipson</surname> <given-names>S</given-names></name> <name><surname>Saleh</surname> <given-names>M</given-names></name> </person-group><year iso-8601-date="2007">2007</year><article-title>One scan connected component labeling technique</article-title><conf-name>2007 IEEE International Conference on Signal Processing and Communications</conf-name><fpage>1283</fpage><lpage>1286</lpage><pub-id pub-id-type="doi">10.1109/ICSPC.2007.4728561</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Alarcón‐Nieto</surname> <given-names>G</given-names></name> <name><surname>Graving</surname> <given-names>JM</given-names></name> <name><surname>Klarevas‐Irby</surname> <given-names>JA</given-names></name> <name><surname>Maldonado‐Chaparro</surname> <given-names>AA</given-names></name> <name><surname>Mueller</surname> <given-names>I</given-names></name> <name><surname>Farine</surname> <given-names>DR</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title>An automated barcode tracking system for behavioural studies in birds</article-title><source>Methods in Ecology and Evolution</source><volume>9</volume><fpage>1536</fpage><lpage>1547</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.13005</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Bath</surname> <given-names>DE</given-names></name> <name><surname>Stowers</surname> <given-names>JR</given-names></name> <name><surname>Hörmann</surname> <given-names>D</given-names></name> <name><surname>Poehlmann</surname> <given-names>A</given-names></name> <name><surname>Dickson</surname> <given-names>BJ</given-names></name> <name><surname>Straw</surname> <given-names>AD</given-names></name> </person-group><year iso-8601-date="2014">2014</year><article-title>FlyMAD: rapid thermogenetic control of neuronal activity in freely walking <italic>Drosophila</italic></article-title><source>Nature Methods</source><volume>11</volume><fpage>756</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2973</pub-id><pub-id pub-id-type="pmid">24859752</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"> <name><surname>Bengio</surname> <given-names>Y</given-names></name> <name><surname>Kingma</surname> <given-names>DP</given-names></name> <name><surname>Ba</surname> <given-names>J</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>Adam: a method for stochastic optimization</article-title><conf-name>3rd International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Bertsekas</surname> <given-names>DP</given-names></name> </person-group><year iso-8601-date="1981">1981</year><article-title>A new algorithm for the assignment problem</article-title><source>Mathematical Programming</source><volume>21</volume><fpage>152</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1007/BF01584237</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Bianco</surname> <given-names>IH</given-names></name> <name><surname>Engert</surname> <given-names>F</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>Visuomotor transformations underlying hunting behavior in zebrafish</article-title><source>Current Biology</source><volume>25</volume><fpage>831</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.01.042</pub-id><pub-id pub-id-type="pmid">25754638</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Bilotta</surname> <given-names>J</given-names></name> <name><surname>Saszik</surname> <given-names>S</given-names></name> </person-group><year iso-8601-date="2001">2001</year><article-title>The zebrafish as a model visual system</article-title><source>International Journal of Developmental Neuroscience</source><volume>19</volume><fpage>621</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1016/S0736-5748(01)00050-8</pub-id><pub-id pub-id-type="pmid">11705666</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Bonter</surname> <given-names>DN</given-names></name> <name><surname>Bridge</surname> <given-names>ES</given-names></name> </person-group><year iso-8601-date="2011">2011</year><article-title>Applications of radio frequency identification (RFID) in ornithological research: a review</article-title><source>Journal of Field Ornithology</source><volume>82</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1111/j.1557-9263.2010.00302.x</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Branson</surname> <given-names>K</given-names></name> <name><surname>Robie</surname> <given-names>AA</given-names></name> <name><surname>Bender</surname> <given-names>J</given-names></name> <name><surname>Perona</surname> <given-names>P</given-names></name> <name><surname>Dickinson</surname> <given-names>MH</given-names></name> </person-group><year iso-8601-date="2009">2009</year><article-title>High-throughput ethomics in large groups of <italic>Drosophila</italic></article-title><source>Nature Methods</source><volume>6</volume><fpage>451</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id><pub-id pub-id-type="pmid">19412169</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Brembs</surname> <given-names>B</given-names></name> <name><surname>Heisenberg</surname> <given-names>M</given-names></name> </person-group><year iso-8601-date="2000">2000</year><article-title>The operant and the classical in conditioned orientation of <italic>Drosophila melanogaster</italic> at the flight simulator</article-title><source>Learning &amp; Memory</source><volume>7</volume><fpage>104</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1101/lm.7.2.104</pub-id><pub-id pub-id-type="pmid">10753977</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Burgos-Artizzu</surname> <given-names>XP</given-names></name> <name><surname>Dollár</surname> <given-names>P</given-names></name> <name><surname>Lin</surname> <given-names>D</given-names></name> <name><surname>Anderson</surname> <given-names>DJ</given-names></name> <name><surname>Perona</surname> <given-names>P</given-names></name> </person-group><year iso-8601-date="2012">2012</year><article-title>Social behavior recognition in continuous video</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2012.6247817</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Caelles</surname> <given-names>S</given-names></name> <name><surname>Maninis</surname> <given-names>K</given-names></name> <name><surname>Pont-Tuset</surname> <given-names>J</given-names></name> </person-group><year iso-8601-date="2017">2017</year><article-title>One-shot video object segmentation</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>5320</fpage><lpage>5329</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.565</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Cavagna</surname> <given-names>A</given-names></name> <name><surname>Cimarelli</surname> <given-names>A</given-names></name> <name><surname>Giardina</surname> <given-names>I</given-names></name> <name><surname>Parisi</surname> <given-names>G</given-names></name> <name><surname>Santagati</surname> <given-names>R</given-names></name> <name><surname>Stefanini</surname> <given-names>F</given-names></name> <name><surname>Tavarone</surname> <given-names>R</given-names></name> </person-group><year iso-8601-date="2010">2010</year><article-title>From empirical data to inter-individual interactions: unveiling the rules of collective animal behavior</article-title><source>Mathematical Models and Methods in Applied Sciences</source><volume>20</volume><fpage>1491</fpage><lpage>1510</lpage><pub-id pub-id-type="doi">10.1142/S0218202510004660</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"> <name><surname>Chang</surname> <given-names>F</given-names></name> <name><surname>Chen</surname> <given-names>C</given-names></name> </person-group><year iso-8601-date="2003">2003</year><article-title>A component-labeling algorithm using contour tracing technique</article-title><conf-name>2013 12th International Conference on Document Analysis and Recognition</conf-name><elocation-id>741</elocation-id></element-citation></ref><ref id="bib15"><element-citation publication-type="web"><person-group person-group-type="author"> <name><surname>Clausen</surname> <given-names>J</given-names></name> </person-group><year iso-8601-date="1999">1999</year><article-title>Branch and bound algorithms-principles and examples </article-title><ext-link ext-link-type="uri" xlink:href="http://www2.imm.dtu.dk/courses/04232/TSPtext.pdf">http://www2.imm.dtu.dk/courses/04232/TSPtext.pdf</ext-link><date-in-citation iso-8601-date="2020-10-22">October 22, 2020</date-in-citation></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Colavita</surname> <given-names>FB</given-names></name> </person-group><year iso-8601-date="1974">1974</year><article-title>Human sensory dominance</article-title><source>Perception &amp; Psychophysics</source><volume>16</volume><fpage>409</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.3758/BF03203962</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Crall</surname> <given-names>JD</given-names></name> <name><surname>Gravish</surname> <given-names>N</given-names></name> <name><surname>Mountcastle</surname> <given-names>AM</given-names></name> <name><surname>Combes</surname> <given-names>SA</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>BEEtag: a Low-Cost, Image-Based tracking system for the study of animal behavior and locomotion</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0136487</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0136487</pub-id><pub-id pub-id-type="pmid">26332211</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Dell</surname> <given-names>AI</given-names></name> <name><surname>Bender</surname> <given-names>JA</given-names></name> <name><surname>Branson</surname> <given-names>K</given-names></name> <name><surname>Couzin</surname> <given-names>ID</given-names></name> <name><surname>de Polavieja</surname> <given-names>GG</given-names></name> <name><surname>Noldus</surname> <given-names>LP</given-names></name> <name><surname>Pérez-Escudero</surname> <given-names>A</given-names></name> <name><surname>Perona</surname> <given-names>P</given-names></name> <name><surname>Straw</surname> <given-names>AD</given-names></name> <name><surname>Wikelski</surname> <given-names>M</given-names></name> <name><surname>Brose</surname> <given-names>U</given-names></name> </person-group><year iso-8601-date="2014">2014</year><article-title>Automated image-based tracking and its application in ecology</article-title><source>Trends in Ecology &amp; Evolution</source><volume>29</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2014.05.004</pub-id><pub-id pub-id-type="pmid">24908439</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Dennis</surname> <given-names>RL</given-names></name> <name><surname>Newberry</surname> <given-names>RC</given-names></name> <name><surname>Cheng</surname> <given-names>HW</given-names></name> <name><surname>Estevez</surname> <given-names>I</given-names></name> </person-group><year iso-8601-date="2008">2008</year><article-title>Appearance matters: artificial marking alters aggression and stress</article-title><source>Poultry Science</source><volume>87</volume><fpage>1939</fpage><lpage>1946</lpage><pub-id pub-id-type="doi">10.3382/ps.2007-00311</pub-id><pub-id pub-id-type="pmid">18809854</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"> <name><surname>Francisco</surname> <given-names>FA</given-names></name> <name><surname>Nührenberg</surname> <given-names>P</given-names></name> <name><surname>Jordan</surname> <given-names>AL</given-names></name> </person-group><year iso-8601-date="2019">2019</year><article-title>A low-cost, open-source framework for tracking and behavioural analysis of animals in aquatic ecosystems</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/571232</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Fredman</surname> <given-names>ML</given-names></name> <name><surname>Tarjan</surname> <given-names>RE</given-names></name> </person-group><year iso-8601-date="1987">1987</year><article-title>Fibonacci heaps and their uses in improved network optimization algorithms</article-title><source>Journal of the ACM</source><volume>34</volume><fpage>596</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1145/28869.28874</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Fukunaga</surname> <given-names>T</given-names></name> <name><surname>Kubota</surname> <given-names>S</given-names></name> <name><surname>Oda</surname> <given-names>S</given-names></name> <name><surname>Iwasaki</surname> <given-names>W</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>GroupTracker: video tracking system for multiple animals under severe occlusion</article-title><source>Computational Biology and Chemistry</source><volume>57</volume><fpage>39</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.compbiolchem.2015.02.006</pub-id><pub-id pub-id-type="pmid">25736254</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Fukushima</surname> <given-names>K</given-names></name> </person-group><year iso-8601-date="1988">1988</year><article-title>Neocognitron: a hierarchical neural network capable of visual pattern recognition</article-title><source>Neural Networks</source><volume>1</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(88)90014-7</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Garrido-Jurado</surname> <given-names>S</given-names></name> <name><surname>Muñoz-Salinas</surname> <given-names>R</given-names></name> <name><surname>Madrid-Cuevas</surname> <given-names>FJ</given-names></name> <name><surname>Medina-Carnicer</surname> <given-names>R</given-names></name> </person-group><year iso-8601-date="2016">2016</year><article-title>Generation of fiducial marker dictionaries using mixed integer linear programming</article-title><source>Pattern Recognition</source><volume>51</volume><fpage>481</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2015.09.023</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Gernat</surname> <given-names>T</given-names></name> <name><surname>Rao</surname> <given-names>VD</given-names></name> <name><surname>Middendorf</surname> <given-names>M</given-names></name> <name><surname>Dankowicz</surname> <given-names>H</given-names></name> <name><surname>Goldenfeld</surname> <given-names>N</given-names></name> <name><surname>Robinson</surname> <given-names>GE</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title>Automated monitoring of behavior reveals bursty interaction patterns and rapid spreading dynamics in honeybee social networks</article-title><source>PNAS</source><volume>115</volume><fpage>1433</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1073/pnas.1713568115</pub-id><pub-id pub-id-type="pmid">29378954</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"> <name><surname>Glorot</surname> <given-names>X</given-names></name> <name><surname>Bengio</surname> <given-names>Y</given-names></name> </person-group><year iso-8601-date="2010">2010</year><article-title>Understanding the difficulty of training deep feedforward neural networks</article-title><conf-name>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</conf-name><fpage>249</fpage><lpage>250</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Graving</surname> <given-names>JM</given-names></name> <name><surname>Chae</surname> <given-names>D</given-names></name> <name><surname>Naik</surname> <given-names>H</given-names></name> <name><surname>Li</surname> <given-names>L</given-names></name> <name><surname>Koger</surname> <given-names>B</given-names></name> <name><surname>Costelloe</surname> <given-names>BR</given-names></name> <name><surname>Couzin</surname> <given-names>ID</given-names></name> </person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>He</surname> <given-names>L</given-names></name> <name><surname>Chao</surname> <given-names>Y</given-names></name> <name><surname>Suzuki</surname> <given-names>K</given-names></name> <name><surname>Wu</surname> <given-names>K</given-names></name> </person-group><year iso-8601-date="2009">2009</year><article-title>Fast connected-component labeling</article-title><source>Pattern Recognition</source><volume>42</volume><fpage>1977</fpage><lpage>1987</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2008.10.013</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Hewitt</surname> <given-names>BM</given-names></name> <name><surname>Yap</surname> <given-names>MH</given-names></name> <name><surname>Hodson-Tole</surname> <given-names>EF</given-names></name> <name><surname>Kennerley</surname> <given-names>AJ</given-names></name> <name><surname>Sharp</surname> <given-names>PS</given-names></name> <name><surname>Grant</surname> <given-names>RA</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title>A novel automated rodent tracker (ART), demonstrated in a mouse model of amyotrophic lateral sclerosis</article-title><source>Journal of Neuroscience Methods</source><volume>300</volume><fpage>147</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.04.006</pub-id><pub-id pub-id-type="pmid">28414047</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname> <given-names>M-K</given-names></name> </person-group><year iso-8601-date="1962">1962</year><article-title>Visual pattern recognition by moment invariants</article-title><source>IRE Transactions on Information Theory</source><volume>8</volume><fpage>179</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1109/TIT.1962.1057692</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Hubel</surname> <given-names>DH</given-names></name> <name><surname>Wiesel</surname> <given-names>TN</given-names></name> </person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat's striate cortex</article-title><source>The Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id><pub-id pub-id-type="pmid">14403679</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Hubel</surname> <given-names>DH</given-names></name> <name><surname>Wiesel</surname> <given-names>TN</given-names></name> </person-group><year iso-8601-date="1963">1963</year><article-title>Receptive fields of cells in striate cortex of very young, visually inexperienced kittens</article-title><source>Journal of Neurophysiology</source><volume>26</volume><fpage>994</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1152/jn.1963.26.6.994</pub-id><pub-id pub-id-type="pmid">14084171</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Hughey</surname> <given-names>LF</given-names></name> <name><surname>Hein</surname> <given-names>AM</given-names></name> <name><surname>Strandburg-Peshkin</surname> <given-names>A</given-names></name> <name><surname>Jensen</surname> <given-names>FH</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title>Challenges and solutions for studying collective animal behaviour in the wild</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>373</volume><elocation-id>20170005</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2017.0005</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Humphrey</surname> <given-names>GK</given-names></name> <name><surname>Khan</surname> <given-names>SC</given-names></name> </person-group><year iso-8601-date="1992">1992</year><article-title>Recognizing novel views of three-dimensional objects</article-title><source>Canadian Journal of Psychology/Revue Canadienne De Psychologie</source><volume>46</volume><fpage>170</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1037/h0084320</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Inada</surname> <given-names>Y</given-names></name> <name><surname>Kawachi</surname> <given-names>K</given-names></name> </person-group><year iso-8601-date="2002">2002</year><article-title>Order and flexibility in the motion of fish schools</article-title><source>Journal of Theoretical Biology</source><volume>214</volume><fpage>371</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1006/jtbi.2001.2449</pub-id><pub-id pub-id-type="pmid">11846596</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Iwata</surname> <given-names>H</given-names></name> <name><surname>Ebana</surname> <given-names>K</given-names></name> <name><surname>Uga</surname> <given-names>Y</given-names></name> <name><surname>Hayashi</surname> <given-names>T</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>Genomic prediction of biological shape: elliptic fourier analysis and kernel partial least squares (PLS) regression applied to grain shape prediction in rice (Oryza sativa L.)</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0120610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0120610</pub-id><pub-id pub-id-type="pmid">25825876</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"> <name><surname>Jakob</surname> <given-names>W</given-names></name> <name><surname>Rhinelander</surname> <given-names>J</given-names></name> <name><surname>Moldovan</surname> <given-names>D</given-names></name> </person-group><year iso-8601-date="2017">2017</year><source>Pybind11 – Seamless Operability Between C++11 and Python</source><ext-link ext-link-type="uri" xlink:href="https://github.com/pybind/pybind11">https://github.com/pybind/pybind11</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Kalman</surname> <given-names>RE</given-names></name> </person-group><year iso-8601-date="1960">1960</year><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Kuhl</surname> <given-names>FP</given-names></name> <name><surname>Giardina</surname> <given-names>CR</given-names></name> </person-group><year iso-8601-date="1982">1982</year><article-title>Elliptic fourier features of a closed contour</article-title><source>Computer Graphics and Image Processing</source><volume>18</volume><fpage>236</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/0146-664X(82)90034-X</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Kuhn</surname> <given-names>HW</given-names></name> </person-group><year iso-8601-date="1955">1955</year><article-title>The hungarian method for the assignment problem</article-title><source>Naval Research Logistics Quarterly</source><volume>2</volume><fpage>83</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1002/nav.3800020109</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname> <given-names>AH</given-names></name> <name><surname>Doig</surname> <given-names>AG</given-names></name> </person-group><year iso-8601-date="2010">2010</year><source>An Automatic Method for Solving Discrete Programming Problems</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-540-68279-0_5</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>LeCun</surname> <given-names>Y</given-names></name> <name><surname>Boser</surname> <given-names>B</given-names></name> <name><surname>Denker</surname> <given-names>JS</given-names></name> <name><surname>Henderson</surname> <given-names>D</given-names></name> <name><surname>Howard</surname> <given-names>RE</given-names></name> <name><surname>Hubbard</surname> <given-names>W</given-names></name> <name><surname>Jackel</surname> <given-names>LD</given-names></name> </person-group><year iso-8601-date="1989">1989</year><article-title>Backpropagation applied to handwritten zip code recognition</article-title><source>Neural Computation</source><volume>1</volume><fpage>541</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Lin</surname> <given-names>TY</given-names></name> <name><surname>Goyal</surname> <given-names>P</given-names></name> <name><surname>Girshick</surname> <given-names>R</given-names></name> <name><surname>He</surname> <given-names>K</given-names></name> <name><surname>Dollar</surname> <given-names>P</given-names></name> </person-group><year iso-8601-date="2020">2020</year><article-title>Focal loss for dense object detection</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>42</volume><fpage>318</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2858826</pub-id><pub-id pub-id-type="pmid">30040631</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Little</surname> <given-names>JDC</given-names></name> <name><surname>Murty</surname> <given-names>KG</given-names></name> <name><surname>Sweeney</surname> <given-names>DW</given-names></name> <name><surname>Karel</surname> <given-names>C</given-names></name> </person-group><year iso-8601-date="1963">1963</year><article-title>An algorithm for the traveling salesman problem</article-title><source>Operations Research</source><volume>11</volume><fpage>972</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1287/opre.11.6.972</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"> <name><surname>Liu</surname> <given-names>T</given-names></name> <name><surname>Chen</surname> <given-names>W</given-names></name> <name><surname>Xuan</surname> <given-names>Y</given-names></name> <name><surname>Fu</surname> <given-names>X</given-names></name> </person-group><year iso-8601-date="2009">2009</year><article-title>The effect of object features on multiple object tracking and identification</article-title><conf-name>International Conference on Engineering Psychology and Cognitive Ergonomics</conf-name><fpage>206</fpage><lpage>212</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maninis</surname> <given-names>K-K</given-names></name> <name><surname>Caelles</surname> <given-names>S</given-names></name> <name><surname>Chen</surname> <given-names>Y</given-names></name> <name><surname>Pont-Tuset</surname> <given-names>J</given-names></name> <name><surname>Leal-Taixé</surname> <given-names>L</given-names></name> <name><surname>Cremers</surname> <given-names>D</given-names></name> <name><surname>Van Gool</surname> <given-names>L</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title>Video object segmentation without temporal information</article-title><conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name><fpage>1515</fpage><lpage>1530</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2838670</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Mathis</surname> <given-names>A</given-names></name> <name><surname>Mamidanna</surname> <given-names>P</given-names></name> <name><surname>Cury</surname> <given-names>KM</given-names></name> <name><surname>Abe</surname> <given-names>T</given-names></name> <name><surname>Murthy</surname> <given-names>VN</given-names></name> <name><surname>Mathis</surname> <given-names>MW</given-names></name> <name><surname>Bethge</surname> <given-names>M</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Mersch</surname> <given-names>DP</given-names></name> <name><surname>Crespi</surname> <given-names>A</given-names></name> <name><surname>Keller</surname> <given-names>L</given-names></name> </person-group><year iso-8601-date="2013">2013</year><article-title>Tracking individuals shows spatial fidelity is a key regulator of ant social organization</article-title><source>Science</source><volume>340</volume><fpage>1090</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1126/science.1234316</pub-id><pub-id pub-id-type="pmid">23599264</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Munkres</surname> <given-names>J</given-names></name> </person-group><year iso-8601-date="1957">1957</year><article-title>Algorithms for the assignment and transportation problems</article-title><source>Journal of the Society for Industrial and Applied Mathematics</source><volume>5</volume><fpage>32</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1137/0105003</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Nagy</surname> <given-names>M</given-names></name> <name><surname>Vásárhelyi</surname> <given-names>G</given-names></name> <name><surname>Pettit</surname> <given-names>B</given-names></name> <name><surname>Roberts-Mariani</surname> <given-names>I</given-names></name> <name><surname>Vicsek</surname> <given-names>T</given-names></name> <name><surname>Biro</surname> <given-names>D</given-names></name> </person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent hierarchies in pigeons</article-title><source>PNAS</source><volume>110</volume><fpage>13049</fpage><lpage>13054</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305552110</pub-id><pub-id pub-id-type="pmid">23878247</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Noldus</surname> <given-names>LPJJ</given-names></name> <name><surname>Spink</surname> <given-names>AJ</given-names></name> <name><surname>Tegelenbosch</surname> <given-names>RAJ</given-names></name> </person-group><year iso-8601-date="2001">2001</year><article-title>EthoVision: a versatile video tracking system for automation of behavioral experiments</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>33</volume><fpage>398</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.3758/BF03195394</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Ohayon</surname> <given-names>S</given-names></name> <name><surname>Avni</surname> <given-names>O</given-names></name> <name><surname>Taylor</surname> <given-names>AL</given-names></name> <name><surname>Perona</surname> <given-names>P</given-names></name> <name><surname>Roian Egnor</surname> <given-names>SE</given-names></name> </person-group><year iso-8601-date="2013">2013</year><article-title>Automated multi-day tracking of marked mice for the analysis of social behaviour</article-title><source>Journal of Neuroscience Methods</source><volume>219</volume><fpage>10</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.05.013</pub-id><pub-id pub-id-type="pmid">23810825</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Pankiw</surname> <given-names>T</given-names></name> <name><surname>Page</surname> <given-names>RE</given-names></name> </person-group><year iso-8601-date="2003">2003</year><article-title>Effect of pheromones, hormones, and handling on sucrose response thresholds of honey bees (Apis mellifera L.)</article-title><source>Journal of Comparative Physiology A</source><volume>189</volume><fpage>675</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1007/s00359-003-0442-y</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Pennekamp</surname> <given-names>F</given-names></name> <name><surname>Schtickzelle</surname> <given-names>N</given-names></name> <name><surname>Petchey</surname> <given-names>OL</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>BEMOVI, software for extracting behavior and morphology from videos, illustrated with analyses of microbes</article-title><source>Ecology and Evolution</source><volume>5</volume><fpage>2584</fpage><lpage>2595</lpage><pub-id pub-id-type="doi">10.1002/ece3.1529</pub-id><pub-id pub-id-type="pmid">26257872</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Pereira</surname> <given-names>TD</given-names></name> <name><surname>Aldarondo</surname> <given-names>DE</given-names></name> <name><surname>Willmore</surname> <given-names>L</given-names></name> <name><surname>Kislin</surname> <given-names>M</given-names></name> <name><surname>Wang</surname> <given-names>SS</given-names></name> <name><surname>Murthy</surname> <given-names>M</given-names></name> <name><surname>Shaevitz</surname> <given-names>JW</given-names></name> </person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"> <name><surname>Pereira</surname> <given-names>TD</given-names></name> <name><surname>Tabris</surname> <given-names>N</given-names></name> <name><surname>Li</surname> <given-names>J</given-names></name> <name><surname>Ravindranath</surname> <given-names>S</given-names></name> <name><surname>Papadoyannis</surname> <given-names>ES</given-names></name> <name><surname>Wang</surname> <given-names>ZY</given-names></name> <name><surname>Turner</surname> <given-names>DM</given-names></name> <name><surname>McKenzie-Smith</surname> <given-names>G</given-names></name> <name><surname>Kocher</surname> <given-names>SD</given-names></name> <name><surname>Falkner</surname> <given-names>AL</given-names></name> <name><surname>Shaevitz</surname> <given-names>JW</given-names></name> <name><surname>Murthy</surname> <given-names>M</given-names></name> </person-group><year iso-8601-date="2020">2020</year><article-title>Sleap: multi-animal pose tracking</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.31.276246</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"> <name><surname>Perez</surname> <given-names>L</given-names></name> <name><surname>Wang</surname> <given-names>J</given-names></name> </person-group><year iso-8601-date="2017">2017</year><article-title>The effectiveness of data augmentation in image classification using deep learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1712.04621">https://arxiv.org/abs/1712.04621</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Pérez-Escudero</surname> <given-names>A</given-names></name> <name><surname>Vicente-Page</surname> <given-names>J</given-names></name> <name><surname>Hinz</surname> <given-names>RC</given-names></name> <name><surname>Arganda</surname> <given-names>S</given-names></name> <name><surname>de Polavieja</surname> <given-names>GG</given-names></name> </person-group><year iso-8601-date="2014">2014</year><article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title><source>Nature Methods</source><volume>11</volume><fpage>743</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2994</pub-id><pub-id pub-id-type="pmid">24880877</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Perez-Escudero</surname> <given-names>A</given-names></name> <name><surname>de Polavieja</surname> <given-names>G</given-names></name> </person-group><year iso-8601-date="2011">2011</year><article-title>Collective animal behavior from bayesian estimation and probability matching</article-title><source>Nature Precedings</source><volume>7</volume><elocation-id>e1002282</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002282</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Pesant</surname> <given-names>G</given-names></name> <name><surname>Quimper</surname> <given-names>C</given-names></name> <name><surname>Zanarini</surname> <given-names>A</given-names></name> </person-group><year iso-8601-date="2012">2012</year><article-title>Counting-Based search: branching heuristics for constraint satisfaction problems</article-title><source>Journal of Artificial Intelligence Research</source><volume>43</volume><fpage>173</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1613/jair.3463</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="report"><person-group person-group-type="author"> <name><surname>Ramshaw</surname> <given-names>L</given-names></name> <name><surname>Tarjan</surname> <given-names>RE</given-names></name> </person-group><year iso-8601-date="2012">2012a</year><source>On Minimum-Cost Assignments in Unbalanced Bipartite Graphs</source><publisher-name>Technical Report</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ramshaw</surname> <given-names>L</given-names></name> <name><surname>Tarjan</surname> <given-names>RE</given-names></name> </person-group><year iso-8601-date="2012">2012b</year><article-title>A weight-scaling algorithm for min-cost imperfect matchings in bipartite graphs</article-title><conf-name>2012 IEEE 53rd Annual Symposium on Foundations of Computer Science</conf-name><fpage>581</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1109/FOCS.2012.9</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"> <name><surname>Rasch</surname> <given-names>MJ</given-names></name> <name><surname>Shi</surname> <given-names>A</given-names></name> <name><surname>Ji</surname> <given-names>Z</given-names></name> </person-group><year iso-8601-date="2016">2016</year><article-title>Closing the loop: tracking and perturbing behaviour of individuals in a group in real-time</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/071308</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Risse</surname> <given-names>B</given-names></name> <name><surname>Berh</surname> <given-names>D</given-names></name> <name><surname>Otto</surname> <given-names>N</given-names></name> <name><surname>Klämbt</surname> <given-names>C</given-names></name> <name><surname>Jiang</surname> <given-names>X</given-names></name> </person-group><year iso-8601-date="2017">2017</year><article-title>FIMTrack: an open source tracking and locomotion analysis software for small animals</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005530</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005530</pub-id><pub-id pub-id-type="pmid">28493862</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Robie</surname> <given-names>AA</given-names></name> <name><surname>Seagraves</surname> <given-names>KM</given-names></name> <name><surname>Egnor</surname> <given-names>SER</given-names></name> <name><surname>Branson</surname> <given-names>K</given-names></name> </person-group><year iso-8601-date="2017">2017</year><article-title>Machine vision methods for analyzing social interactions</article-title><source>The Journal of Experimental Biology</source><volume>220</volume><fpage>25</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1242/jeb.142281</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Rodriguez</surname> <given-names>A</given-names></name> <name><surname>Zhang</surname> <given-names>H</given-names></name> <name><surname>Klaminder</surname> <given-names>J</given-names></name> <name><surname>Brodin</surname> <given-names>T</given-names></name> <name><surname>Andersson</surname> <given-names>PL</given-names></name> <name><surname>Andersson</surname> <given-names>M</given-names></name> </person-group><year iso-8601-date="2018">2018</year><article-title><italic>ToxTrac</italic> : A fast and robust software for tracking organisms</article-title><source>Methods in Ecology and Evolution</source><volume>9</volume><fpage>460</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.12874</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Romero-Ferrero</surname> <given-names>F</given-names></name> <name><surname>Bergomi</surname> <given-names>MG</given-names></name> <name><surname>Hinz</surname> <given-names>RC</given-names></name> <name><surname>Heras</surname> <given-names>FJH</given-names></name> <name><surname>de Polavieja</surname> <given-names>GG</given-names></name> </person-group><year iso-8601-date="2019">2019</year><article-title>Idtracker.ai: tracking all individuals in small or large collectives of unmarked animals</article-title><source>Nature Methods</source><volume>16</volume><fpage>179</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id><pub-id pub-id-type="pmid">30643215</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Rosenthal</surname> <given-names>SB</given-names></name> <name><surname>Twomey</surname> <given-names>CR</given-names></name> <name><surname>Hartnett</surname> <given-names>AT</given-names></name> <name><surname>Wu</surname> <given-names>HS</given-names></name> <name><surname>Couzin</surname> <given-names>ID</given-names></name> </person-group><year iso-8601-date="2015">2015</year><article-title>Revealing the hidden networks of interaction in mobile animal groups allows prediction of complex behavioral contagion</article-title><source>PNAS</source><volume>112</volume><fpage>4690</fpage><lpage>4695</lpage><pub-id pub-id-type="doi">10.1073/pnas.1420068112</pub-id><pub-id pub-id-type="pmid">25825752</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Sockman</surname> <given-names>KW</given-names></name> <name><surname>Schwabl</surname> <given-names>H</given-names></name> </person-group><year iso-8601-date="2001">2001</year><article-title>Plasma corticosterone in nestling american kestrels: effects of age, handling stress, yolk androgens, and body condition</article-title><source>General and Comparative Endocrinology</source><volume>122</volume><fpage>205</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1006/gcen.2001.7626</pub-id><pub-id pub-id-type="pmid">11316426</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Sridhar</surname> <given-names>VH</given-names></name> <name><surname>Roche</surname> <given-names>DG</given-names></name> <name><surname>Gingins</surname> <given-names>S</given-names></name> </person-group><year iso-8601-date="2019">2019</year><article-title>Tracktor: image‐based automated tracking of animal movement and behaviour</article-title><source>Methods in Ecology and Evolution</source><volume>10</volume><fpage>815</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.13166</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Strandburg-Peshkin</surname> <given-names>A</given-names></name> <name><surname>Twomey</surname> <given-names>CR</given-names></name> <name><surname>Bode</surname> <given-names>NW</given-names></name> <name><surname>Kao</surname> <given-names>AB</given-names></name> <name><surname>Katz</surname> <given-names>Y</given-names></name> <name><surname>Ioannou</surname> <given-names>CC</given-names></name> <name><surname>Rosenthal</surname> <given-names>SB</given-names></name> <name><surname>Torney</surname> <given-names>CJ</given-names></name> <name><surname>Wu</surname> <given-names>HS</given-names></name> <name><surname>Levin</surname> <given-names>SA</given-names></name> <name><surname>Couzin</surname> <given-names>ID</given-names></name> </person-group><year iso-8601-date="2013">2013</year><article-title>Visual sensory networks and effective information transfer in animal groups</article-title><source>Current Biology</source><volume>23</volume><fpage>R709</fpage><lpage>R711</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.07.059</pub-id><pub-id pub-id-type="pmid">24028946</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Suzuki</surname> <given-names>K</given-names></name> <name><surname>Horiba</surname> <given-names>I</given-names></name> <name><surname>Sugie</surname> <given-names>N</given-names></name> </person-group><year iso-8601-date="2003">2003</year><article-title>Linear-time connected-component labeling based on sequential local operations</article-title><source>Computer Vision and Image Understanding</source><volume>89</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/S1077-3142(02)00030-9</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Switzer</surname> <given-names>CM</given-names></name> <name><surname>Combes</surname> <given-names>SA</given-names></name> </person-group><year iso-8601-date="2016">2016</year><article-title>bombus impatiens (Hymenoptera: apidae) display reduced pollen foraging behavior when marked with bee tags vs. paint</article-title><source>Journal of Melittology</source><volume>62</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.17161/jom.v0i62.5679</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="thesis"><person-group person-group-type="author"> <name><surname>Thomas</surname> <given-names>DJ</given-names></name> </person-group><year iso-8601-date="2016">2016</year><article-title>Matching problems with additional resource constraints</article-title><publisher-name>Doctoral Thesis</publisher-name></element-citation></ref><ref id="bib75"><element-citation publication-type="data"><person-group person-group-type="author"> <name><surname>Walter</surname> <given-names>T</given-names></name> <name><surname>Albi</surname> <given-names>A</given-names></name> <name><surname>Bath</surname> <given-names>DE</given-names></name> <name><surname>Hugo</surname> <given-names>H</given-names></name> <name><surname>Oberhauser</surname> <given-names>F</given-names></name> <name><surname>Groettrup</surname> <given-names>M</given-names></name> <name><surname>Mink</surname> <given-names>D</given-names></name> </person-group><year iso-8601-date="2020">2020</year><data-title>Reproduction data for: TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</data-title><source>Max Planck Society</source><pub-id pub-id-type="doi">10.17617/3.4y</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="book"><person-group person-group-type="author"> <name><surname>Warren</surname> <given-names>J</given-names></name> <name><surname>Weimer</surname> <given-names>H</given-names></name> </person-group><year iso-8601-date="2001">2001</year><source>Subdivision Methods for Geometric Design: A Constructive Approach</source><publisher-loc>San Francisco, CA, USA</publisher-loc><publisher-name>Morgan Kaufmann Publishers Inc</publisher-name></element-citation></ref><ref id="bib77"><element-citation publication-type="report"><person-group person-group-type="author"> <name><surname>Weixiong</surname> <given-names>Z</given-names></name> </person-group><year iso-8601-date="1996">1996</year><source>Branch-and-Bound Search Algorithms and Their Computational Complexity</source><publisher-name>Technical Report, ISI/RR-96-443, [Online</publisher-name></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"> <name><surname>Wiesel</surname> <given-names>TN</given-names></name> <name><surname>Hubel</surname> <given-names>DH</given-names></name> </person-group><year iso-8601-date="1966">1966</year><article-title>Spatial and chromatic interactions in the lateral geniculate body of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>29</volume><fpage>1115</fpage><lpage>1156</lpage><pub-id pub-id-type="doi">10.1152/jn.1966.29.6.1115</pub-id><pub-id pub-id-type="pmid">4961644</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"> <name><surname>Wild</surname> <given-names>B</given-names></name> <name><surname>Dormagen</surname> <given-names>DM</given-names></name> <name><surname>Zachariae</surname> <given-names>A</given-names></name> <name><surname>Smith</surname> <given-names>ML</given-names></name> <name><surname>Traynor</surname> <given-names>KS</given-names></name> <name><surname>Brockmann</surname> <given-names>D</given-names></name> <name><surname>Couzin</surname> <given-names>ID</given-names></name> <name><surname>Landgraf</surname> <given-names>T</given-names></name> </person-group><year iso-8601-date="2020">2020</year><article-title>Social networks predict the life and death of honey bees</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.06.076943</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"> <name><surname>Williams</surname> <given-names>L</given-names></name> </person-group><year iso-8601-date="1978">1978</year><article-title>Casting Curved Shadows on Curved Surfaces</article-title><conf-name>Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><fpage>270</fpage><lpage>274</lpage></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Installation requirements and usage</title><p>Compiled, ready-to-use binaries are available for all major operating systems (Windows, Linux, MacOS). However, it should be possible to compile the software yourself for any Unix- or Windows-based system (≥8), possibly with minor adjustments. Tested setups include:</p><list list-type="bullet"><list-item><p>Windows, Linux, MacOS</p></list-item><list-item><p>A computer with <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mrow><mml:mn>16</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> RAM is recommended</p></list-item><list-item><p>OpenCV(<ext-link ext-link-type="uri" xlink:href="https://opencv.org/about/">opencv.org</ext-link>) libraries <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mrow><mml:mi>v3</mml:mi><mml:mo>⁢</mml:mo><mml:mn>.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Python libraries <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mrow><mml:mi>v3</mml:mi><mml:mo>⁢</mml:mo><mml:mn>.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, as well as additional packages such as:</p></list-item><list-item><p>Keras <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mrow><mml:mi>v2</mml:mi><mml:mo>⁢</mml:mo><mml:mn>.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> with one of the following backends installed</p><list list-type="simple"><list-item><p>–Tensorflow <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi/><mml:mo>&lt;</mml:mo><mml:mi>v2</mml:mi></mml:mrow></mml:math></inline-formula> (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">tensorflow.org</ext-link>) (either CPU-based, or GPU-based)</p></list-item><list-item><p>–Theano (<ext-link ext-link-type="uri" xlink:href="http://deeplearning.net/software/theano/">deeplearning.net</ext-link>)</p></list-item></list></list-item><list-item><p>GPU-based recognition requires an NVIDIA graphics-card and drivers (see Tensorflow documentation).</p></list-item></list><p>For detailed download/installation instructions and up-to-date requirements, please refer to the documentation at trex.run/install.</p></sec><sec id="s9" sec-type="appendix"><title>Workflow</title><p>TRex can be opened in one of two ways: (i) Simply starting the application (e.g. using the operating systems’ file-browser), (ii) using the command-line. If the user simply opens the application, a file opening dialog displays a list of compatible files as well as information on a selected files content. Certain startup parameters can be adjusted from within the graphical user-interface (see <xref ref-type="fig" rid="fig8">Figure 8</xref>, &quot;interactive settings box&quot;), before confirming and loading up the file (see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Users with more command-line experience, or the intent of running TRex in batch-mode, can append necessary parameter values without adding them to a settings file.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Using the interactive heatmap generator within TRex, the foraging trail formation of <italic>Constrictotermes cyphergaster</italic> (termites) can be visualized during analysis, as well as other potentially interesting metrics (based on posture- as well basic positional data).</title><p>This is generalizable to all output data fields available in TRex, for example also making it possible to visualize ‘time’ as a heatmap and showing where individuals were more likely to be located during the beginning or towards end of the video. <italic>Video: H. Hugo</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>The file opening dialog.</title><p>On the left is a list of compatible files in the current folder. The center column shows meta-information provided by the video file, including its frame-rate and resolution – or some of the settings used during conversion and the timestamp of conversion. The column on the right provides an easy interface for adjusting the most important parameters before starting up the software. Most parameters can be changed later on from within TRex as well.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app1-fig2-v2.tif"/></fig><p>To acquire video-files that can be opened using TRex, one needs to first run TGrabs in one way or another. It is possible to use a webcam (generic USB camera) for recording, but TGrabs can also be compiled with Basler Pylon5 support The <ext-link ext-link-type="uri" xlink:href="http://baslerweb.com/">baslerweb.com</ext-link> Pylon SDK is required to be installed to support Basler USB cameras. TGrabs can also convert existing videos and write to a more suitable format for TRex to interact with (a static background with moving objects clearly separated in front of it). It can be started just like TRex, although most options are either set via the command-line, or a web-interface. TGrabs can perform basic tracking tasks on the fly, offering closed-loop support as well.</p><p>For automatic visual recognition, one might need to adjust some parameters. Mostly, these adjustments consist of changing the following parameters:</p><list list-type="bullet"><list-item><p>blob_size_ranges: Setting one (or multiple) size thresholds for individuals, by giving lower and upper limit value pairs.</p></list-item><list-item><p>track_max_individuals: Sets the number of individuals expected in a trial. This number needs to be known for recognition tasks (and will be guessed if not provided), but can be set to 0 for unknown numbers of individuals.</p></list-item><list-item><p>track_max_speed: Sets the maximum speed (cm/s) that individuals are expected to travel at. This is influenced by meta information provided to TGrabs by the user (e.g. the width of the tank), as well as frame timings.</p></list-item><list-item><p>track_threshold: Even TRex can threshold images of individuals, so it is beneficial to not threshold away too many pixels during conversion/recording and do finer-grade adjustments in the tracker itself.</p></list-item><list-item><p>outline_resample: A factor that is <italic>gt</italic> <sub>0</sub>, by which the number of points in the outline is essentially ‘divided’. Smaller resample rates lead to more points on the outline (good for very small shapes).</p></list-item></list><p>Training can be started once the user is satisfied with the basic tracking results. Consecutive segments are highlighted in the time-line and suggest better or worse tracking, based on their quantity and length. Problematic segments of the video are highlighted using yellow bars in that same time-line, giving another hint to the user as to the tracking quality. To start the training, the user just clicks on ‘train network’ in the main menu – triggering the accumulation process immediately. After training, the user can click on ‘auto correct’ in the menu and let TRex correct the tracks automatically (this will re-track the video). The entire process can be automated by adding the ‘auto_train’ parameter to the command-line, or selecting it in the interface.</p></sec><sec id="s10" sec-type="appendix"><title>Output</title><p>Once finished, the user may export the data in the desired format. Which parts of the data are exported is up to the user as well. By default, almost all the data is exported and saved in NPZ files in the output folder.</p><p>Output folders are structured in this way:</p><list list-type="simple"><list-item><p>output folder:</p><list list-type="simple"><list-item><p>–Settings files</p></list-item><list-item><p>–Training weights</p></list-item><list-item><p>–Saved program states</p></list-item><list-item><p>–data folder:</p><list list-type="simple"><list-item><p>–Statistics</p></list-item><list-item><p>* All exported NPZ files (named [video_name]_fish[number].npz – the prefix ‘fish’ can be changed).</p></list-item><list-item><p>*…</p></list-item></list></list-item><list-item><p>–frames folder (contains video clips recorded in the GUI, for example for presentations):</p><list list-type="simple"><list-item><p>* [video name] folder</p><list list-type="simple"><list-item><p>⋅ clip[index].avi</p></list-item><list-item><p>⋅ …</p></list-item></list></list-item><list-item><p>* …</p></list-item></list></list-item></list></list-item></list><p>At any point in time (except during training), the user can save the current program state and return to it at a later time (e.g. after a computer restart).</p><sec id="s10-1"><title>Export options</title><p>After individuals have been assigned by the matching algorithm, various metrics are calculated (depending on settings):</p><list list-type="bullet"><list-item><p>Angle: The angle of an individual can be calculated without any context using image moments (<xref ref-type="bibr" rid="bib30">Hu, 1962</xref>). However, this angle is only reliable within 0 to 180 degrees – not the full 360. Within these 180 degrees it is probably more accurate than is movement direction.</p></list-item><list-item><p>Position: Centroid information on the current, as well as the previous position of the individual are maintained. Based on previous positions, velocity as well as acceleration are calculated. This process is based on information sourced from the respective video file or camera on the time passed between frames. The centroid of an individual is calculated based on the mass center of the pixels that the object comprises. Angles calculated in the previous steps are corrected (flipped by 180 degrees) if the angle difference between movement direction and angle + 180 degrees is smaller than with the raw angle.</p></list-item><list-item><p>Posture: A large part of the computational complexity comes from calculating the posture of individuals. While this process is relatively fast in TRex, it is still the main factor (except with many individuals, where the matching process takes longest). We dedicated a subsection to it below.</p></list-item><list-item><p>Visual Field: Based on posture, rays can be cast to detect which animal is visible from the position of another individual. We also dedicated a subsection to visual field further down.</p></list-item><list-item><p>Other features can be computed, such as inter-individual distances or distance to the tank border. These are optional and will only be computed if necessary when exporting the data. A (non-comprehensive) list of metrics that can be exported follows:</p><list list-type="simple"><list-item><p>–Time: The time of the current frame (relative to the start of the video) in seconds.</p></list-item><list-item><p>–Frame: Index of the frame in the PV video file.</p></list-item><list-item><p>–Individual components of position its derivatives (as well as their magnitudes, e.g. speed)</p></list-item><list-item><p>–Midline offset: The center-line, for example of a beating fish-tail, is normalized to be roughly parallel to the x-axis (from its head to a user-defined percentage of a body). The y-offset of its last point is exported as a ‘midline offset’. This is useful, for example to detect burst-and-glide events.</p></list-item><list-item><p>–Midline variance: Variance in midline offset, for example for detection of irregular postures or increased activity.</p></list-item><list-item><p>–Border distance</p></list-item><list-item><p>–Average neighbor distance: Could be used to detect individuals who prefer to be located far away from the others or are avoided by them.</p></list-item></list></list-item></list><p>Additionally, tracks of individuals can be exported as a series of cropped-out images – a very useful tool if they are to be used with an external posture estimator or tag-recognition. This series of images can be either every single image, or the median of multiple images (the time-series is down-sampled).</p></sec></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s11" sec-type="appendix"><title>From video frame to blobs</title><p>Video frames can originate either from a camera, or from a pre-recorded video file saved on disk. TGrabs treats both sources equally, the only exception being some minor details and that pre-recorded videos have a well-defined end (which only has an impact on MP4 encoding). Multiple formats are supported, but the full list of supported codecs depends on the specific system and OpenCV version installed. TGrabs saves images in RAW quality, but does not store complete images. Merely the objects of interest, defined by common tracking parameters such as size, will actually be written to a file. Since TGrabs is mostly meant for use with stable backgrounds (except when contrast is good or a video-mask is provided), the rest of the area can be approximated by a static background image generated in the beginning of the process (or previously).</p><p>Generally, every image goes through a number of steps before it can be tracked in TRex:</p><list list-type="order"><list-item><p>Images are decoded by either (i) a camera driver, or (ii) OpenCV. They consist of an array of values between 0 and 255 (grayscale). Color images will be converted to grayscale images (color channel or ‘hue’ can be chosen).</p></list-item><list-item><p>Timing information is saved and images are appended to a queue of images to be processed</p></list-item><list-item><p>All operations from now on are performed on the GPU if available. Once images are in the queue, they are picked one-by-one by the processing thread, which performs operations on them based on user-defined parameters:</p><list list-type="bullet"><list-item><p>Cropping</p></list-item><list-item><p>–Inverting</p></list-item><list-item><p>Contrast/brightness and lighting corrections</p></list-item><list-item><p>–Undistortion (see OpenCV Tutorial)</p></list-item></list></list-item><list-item><p>(optional) Background subtraction (<inline-formula><mml:math id="inf61"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with <italic>f</italic> being the image and <italic>b</italic> the background image), leaving a difference image containing only the objects. This can be an absolute difference <inline-formula><mml:math id="inf62"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> or a signed one, which has different effects on the following step. Otherwise <inline-formula><mml:math id="inf63"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Thresholding to obtain a binary image, with all pixels either being 1 or 0:</p><list list-type="simple"><list-item><p><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>where <inline-formula><mml:math id="inf64"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>T</mml:mi><mml:mo>≤</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:math></inline-formula> is the threshold constant.</p></list-item></list></list-item><list-item><p>Options are available for further adjustment of the binary image: Dilation, Erosion and Closing are used to close gaps in the shapes, which are filled up by successive dilation and erosion operations (see <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>). If there is an imbalance of dilation and erosion commands, noise can be removed or shapes made more inclusive.</p></list-item><list-item><p>The original image is multiplied by the thresholded image, obtaining a masked grayscale image: <inline-formula><mml:math id="inf65"><mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where · is the element-wise multiplication operator.</p></list-item></list><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Example of morphological operations on images: ‘Erosion’.</title><p>Blue pixels denote on-pixels with color values greater than zero, white pixels are ‘off-pixels’ with a value equal to zero. A mask is moved across the original image, with its center (dot) being the focal pixel. A focal pixel is <italic>retained</italic> if all the on-pixels within the structure element/mask are on top of on-pixels in the original image. Otherwise the focal pixel is set to 0. The type of operation performed is entirely determined by the structure element.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app2-fig1-v2.tif"/></fig><p>At this point, the masked image is returned to the CPU, where connected components (objects) are detected. A connected component is a number of adjacent pixels with color values greater than zero. Algorithms for connected-component labeling either use a 4-neighborhood or an 8-neighborhood, which considers diagonal neighbors to be adjacent as well. Many such algorithms are available (<xref ref-type="bibr" rid="bib1">AbuBaker et al., 2007</xref>, <xref ref-type="bibr" rid="bib14">Chang and Chen, 2003</xref>, and many others), even capable of real-time speeds (<xref ref-type="bibr" rid="bib72">Suzuki et al., 2003</xref>, <xref ref-type="bibr" rid="bib28">He et al., 2009</xref>). However, since we want to use a compressed representation throughout our solution, as well as transfer over valuable information to integrate it with posture analysis, we needed to implement our own (see Appendix C Connected components algorithm).</p><p>MP4 encoding has some special properties, since its speed is mainly determined by the external encoding software. Encoding at high-speed frame-rates can be challenging, since we are also encoding to a PV-file simultaneously. Videos are encoded in a separate thread, without muxing, and will be remuxed after the recording is stopped. For very high frame-rates or resolutions, it may be necessary to limit the duration of videos since all of the images have to be kept in RAM until they have been encoded. RAW images in RAM can take up a lot of space (<inline-formula><mml:math id="inf66"><mml:mrow><mml:mrow><mml:mn>1024</mml:mn><mml:mo>*</mml:mo><mml:mn>1024</mml:mn><mml:mo>*</mml:mo><mml:mn>1000</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>048</mml:mn><mml:mo>,</mml:mo><mml:mn>576</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> bytes for 1000 images quite low in resolution). If there a recording length is defined prior to starting the program, or a video is converted to PV and streamed to MP4 at the same time (though it is unclear why that would be necessary), TGrabs is able to automatically determine which frame-rate can be maintained reliably and without filling the memory.</p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s12" sec-type="appendix"><title>Connected components algorithm</title><p>Pixels are not represented individually in TRex. Instead, they are saved as connected horizontal line segments. For each of these lines, only y- as well as start- and end-position are saved (<inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>x</italic> <sub>1</sub>). This representation is especially suited for objects stretching out along the x-axis, but of course its worst-case is a straight, vertical line – in which case space requirements are <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <italic>N</italic> pixels. Especially for big objects, however, only a fraction of coordinates has to be kept in memory (with a space requirement of <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>*</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> being width and height of the object).</p><p>Extracting these connected horizontal line segments from an image can be parallelized easily by cutting the image into full-width pieces and running the following algorithm repeatedly for each row:</p><list list-type="order"><list-item><p>From 0 to <italic>W</italic>, iterate all pixels. Always maintain the previous value (binary), as well as the current value. We start out with our previous value of <inline-formula><mml:math id="inf72"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (the border is considered not to be an object).</p></list-item><list-item><p>Now repeat for every pixel <italic>p</italic> <sub><italic>i</italic></sub> in the current row:</p><list list-type="alpha-lower"><list-item><p>If <inline-formula><mml:math id="inf73"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is one and <italic>p</italic> <sub><italic>i</italic></sub> is 0, set <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>:=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and save the position as the end of a line segment <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>If <inline-formula><mml:math id="inf76"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is 0 and <italic>p</italic> <sub><italic>i</italic></sub> is 1, we did not have a previous line segment and a new one starts. We save it as our current line segment with <italic>x</italic> <sub>0</sub> and <italic>y</italic> equal to the current row. Set <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list></list-item><list-item><p>After each row, if we have a valid current line, we save it in our array of lines. If <inline-formula><mml:math id="inf78"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> was set, and the line segment ended at the border <italic>W</italic> of the image, we first set its end position to <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mi>W</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>We keep the array of extracted lines sorted by their y-coordinate, as well as their x-coordinates in the order we encountered them. To extract connected components, we now just need to walk through all extracted rows and detect changes in the y-coordinate. The only information needed are the current row and the previous row, as well as a list of active preliminary ‘blobs’ (or connected components). A blob is simply a collection of ordered horizontal line segments belonging to a single connected component. These blobs are preliminary until the whole image has been processed, since they might be merged into a single blob further down despite currently being separate (see <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>).</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>An example array of pixels, or image, to be processed by the connected components algorithm.</title><p>This figure should be read from top to bottom, just as the connected components algorithm would do. When this image is analyzed, the red and blue objects will temporarily stay separate within different ‘blobs’. When the green pixels are reached, both objects are combined into one identity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app3-fig1-v2.tif"/></fig><p>‘Rows’ are an array of horizontal lines with the same y-coordinate, ordered by their x-coordinates (increasing). The following algorithm only considers pairs of previous row <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and current row <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. We start by inserting all separate horizontal line segments of the very first row into the pool of active blobs, each assigned their own blob. Lines within row <inline-formula><mml:math id="inf82"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are <inline-formula><mml:math id="inf83"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Coordinates of <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> will be denoted as <inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Our current index in row <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is <italic>j</italic> and our index in row <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is <italic>k</italic>. We initialize <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>:=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Now for each pair of rows, three different actions may be required depending on the case at hand. All three actions are hierarchically ordered and mutually exclusive (like a typical if/else structure would be), meaning that cases 0 – 2 can be true at the same time while no other combination can be simultaneously true:</p><list list-type="order"><list-item><p>Case 0,1 and 2: We have to create a new blob. This is the case if (0) the line in <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> ends before the line in <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> starts (<inline-formula><mml:math id="inf93"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), or (1) y-coordinates of <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are farther apart than 1 (<inline-formula><mml:math id="inf96"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), or (2) there are no lines left in <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to match the current line in <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to (<inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>j</mml:mi><mml:mo>≥</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is assigned with a new blob.</p></list-item><list-item><p>Case 3: Segment in the previous row ends before the segment in the current row starts. If <inline-formula><mml:math id="inf101"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, then we just have to <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>:=</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Case 4: Segment in the previous row and segment in the current row intersect in x-coordinates. If <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is no yet assigned with a blob, assign it with the one from <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Otherwise, both blobs have to be merged. This is done in a sub-routine, which guarantees that lines within blobs stay properly sorted during merging. This means that (i) y-coordinates increase or stay the same and (ii) x-coordinates increase monotonically. Afterwards, we increase either <italic>k</italic> or <italic>j</italic> based on which one associated line ends earlier: If <inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, then we increase <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>:=</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; otherwise <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>:=</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>After the previous algorithm has been executed on a pair of <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, we increase <italic>i</italic> by one <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>:=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. This process is continued until <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>, at which point all connected components are contained within the active blob array.</p><p>Retaining information about pixel values adds slightly more complexity to the algorithm, but is straight-forward to implement. In TRex, horizontal line segments comprise <italic>y</italic>, <italic>x</italic> <sub>0</sub> and <italic>x</italic> <sub>1</sub> values plus an additional pointer. It points to the start of a line within array of all pixels (or an image matrix), adding only little computational complexity overall.</p><p>Based on the horizontal line segments and their order, posture analysis can be sped up when properly integrated. Another advantage is that detection of connected components within arrays of horizontal line segments is supported due to the way the algorithm functions – we can just get rid of the extraction phase.</p></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s13" sec-type="appendix"><title>Matching an object to an object in the next frame</title><sec id="s13-1"><title>Terminology</title><p>A graph is a mathematical structure commonly used in many fields of research, such as computer science, biology and linguistics. Graphs are made up of vertices, which in turn are connected by edges. Below we define relevant terms that we are going to use in the following section:</p><list list-type="bullet"><list-item><p>Directed graph: Edges have a direction assigned to them</p></list-item><list-item><p>Weighted edges: Edges have a weight (or cost) assigned to them</p></list-item><list-item><p>Adjacent nodes: Nodes which are connected immediately by an edge</p></list-item><list-item><p>Path: A path is a sequence of edges, where each edges starting vertex is the end vertex of the previous edge</p></list-item><list-item><p>Acyclic graph: The graph contains no path in which the same vertex appears more than once</p></list-item><list-item><p>Connected graph: There are no vertices without edges, there is a path from any vertex to any other vertex in the graph</p></list-item><list-item><p>Bipartite graph: Vertices can be sorted into two distinct groups, without an edge from any vertex to elements of its own group – only to the other group</p></list-item><list-item><p>Tree: A tree is a connected, undirected, acyclic graph, in which any two vertices are only connected by exactly one path</p></list-item><list-item><p>Rooted, directed out-tree: A tree where one vertex has been defined to be the root and directed edges, with all edges flowing away from the root</p></list-item><list-item><p>Visited vertex: A vertex that is already part of the current path</p></list-item><list-item><p>Leaf: A vertex which has only one edge arriving, but none going out (in a tree this are the bottom-most vertices)</p></list-item><list-item><p>Depth-first/breadth-first and best-first search: Different strategies to pick the next vertex to explore for a set of paths with traversable edges. Depth-first prefers to first go deeper inside a graph/tree, before going on to explore other edges of the same vertex. Breadth-first is the opposite of depth-search. Best-first search uses strategies to explore the most promising path first.</p></list-item></list></sec><sec id="s13-2"><title>Background</title><p>The transportation problem is one of the fundamental problems in computer science. It solves the problem of transporting a finite number of <italic>goods</italic> to a finite number of <italic>factories</italic>, where each possible transport route is associated with a <italic>cost</italic> (or weight). Every factory has a <italic>demand</italic> for goods and every good has a limited <italic>supply</italic>. The sum of this cost has to be minimized (or benefits maximized), while remaining within the constraints given by supply and demand. In the special case where demand by each factory and supply for each good are exactly equal to 1, this problem reduces to the <italic>assignment problem</italic>.</p><p>The assignment problem can be further separated into two distinct cases: the <italic>balanced</italic> and the <italic>unbalanced</italic> assignment problem. In the balanced case, net-supply and demand are the same – meaning that the number of factories matches exactly the number of suppliers. While the balanced case can be solved slightly more efficiently, most practical problems are usually unbalanced (<xref ref-type="bibr" rid="bib62">Ramshaw and Tarjan, 2012b</xref>). Thankfully, unbalanced assignments can be reduced to balanced assignments, for example using graph-duplication methods or by adding nodes (<xref ref-type="bibr" rid="bib61">Ramshaw and Tarjan, 2012a</xref>, <xref ref-type="bibr" rid="bib61">Ramshaw and Tarjan, 2012a</xref>). This makes the widely used Hungarian method (<xref ref-type="bibr" rid="bib40">Kuhn, 1955</xref>; <xref ref-type="bibr" rid="bib49">Munkres, 1957</xref>) a viable solution to both, with a computational complexity of <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. It can be further improved using Fibonacci heaps (not implemented in TRex), resulting in <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> time-complexity (<xref ref-type="bibr" rid="bib21">Fredman and Tarjan, 1987</xref>), with <italic>m</italic> being the number of possible connections/edges, <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>s</mml:mi><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> the number of factories to be supplied and <italic>n</italic> the number of factories. Re-balancing, by adding nodes or other structures, also adds computational cost – especially when <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>s</mml:mi><mml:mo>≪</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib62">Ramshaw and Tarjan, 2012b</xref>).</p></sec><sec id="s13-3"><title>Adaptation for our matching problem</title><p>1382 assigning individuals to objects in the frame is, in the worst case, exactly that: an unbalanced assignment problem – potentially with <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>r</mml:mi><mml:mo>≠</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>. During development, we found that we can achieve better average-complexity by combining an approach commonly used to solve <italic>NP-hard</italic> problems. This is a class problems for which it is (probably) not possible to find a polynomial-time solution. In order to motivate our usage of a less stable algorithm than for example the Hungarian method, let us first introduce a more general algorithm, following along with remarks for adapting it to our special case. The next subsection concludes with considerations regarding its complexity in comparison to the more stable Hungarian method.</p><p><italic>Branch and Bound</italic> (or BnB, <xref ref-type="bibr" rid="bib41">Land and Doig, 2010</xref>, formalized in <xref ref-type="bibr" rid="bib44">Little et al., 1963</xref>) is a very general approach to traversing the large search spaces of <italic>NP-hard</italic> problems, traditionally represented by a tree. Branching and bounding gives optimal solutions by traversing the entire search space if necessary, but stopping along the way to evaluate its options, always trying to choose better branches of the tree to explore next or skip unnecessary ones. BnB always consists of three main ingredients:</p><list list-type="order"><list-item><p>Branching: The division of our problem into smaller, partial problems</p></list-item><list-item><p>Bounding: Estimate the upper/lower limits of the probability/cost gain to be expected by traversing a given edge</p></list-item><list-item><p>Selection: Determining the next node to be processed.</p></list-item></list><p>Finding good strategies is essential and can have a big impact on overall computation time. Strategies can only be worked out with insight into the specific problem, but <italic>bounding</italic> is generally the dominating factor here – in that choosing good selection and branching techniques cannot make up for a bad bounding function (<xref ref-type="bibr" rid="bib15">Clausen, 1999</xref>). A bounding function estimates an upper (or lower) limit for the quality of results that can be achieved within a given sub-problem (current branch of the tree).</p><p>The ‘problem’ is the entire assignment problem located at the root node of the tree. The further down we go in the tree, the smaller the partial problems become until we reach a leaf. Any graph can be represented as a tree by duplicating nodes when necessary (<xref ref-type="bibr" rid="bib77">Weixiong, 1996</xref>, ‘Graph vs. tree’). So even if the bipartite assignment graph (an example sketched in <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>) is a more ‘traditional’ representation of the assignment problem, we can translate it into a rooted, directed out-tree <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with weighted edges. Here, <italic>U</italic> are individuals and <italic>V</italic> are objects in the current frame that are potentially assigned to identities in <italic>U. E</italic> are edges mapping from <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>U</mml:mi><mml:mo>→</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula>, while <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>F</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>→</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. It is quite visible from <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>, that the representation as a tree (b) is much more verbose than a bipartite graph (a). However, its structure is very simple:</p><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>A bipartite graph (a) and its equivalent tree-representation (b).</title><p>It is <italic>bipartite</italic> since nodes can be sorted into two disjoint and independent sets (<inline-formula><mml:math id="inf120"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>), where no nodes have edges to other nodes within the same set. (a) is a straight-forward way of depicting an assignment problem, with the identities on the left side and objects being assigned to the identities on the right side. Edge weights are, in TRex and this example, probabilities for a given identity to be the object in question. This graph is also an example for an unbalanced assignment problem, since there are fewer objects (orange) available than individuals (blue). The optimal solution in this case, using weight-maximization, is to assign <inline-formula><mml:math id="inf122"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>→</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>→</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and leave one unassigned. Invalid edges have been pruned from the tree in (b), enforcing the rule that objects can only appear once in each path. The optimal assignments have been highlighted in red.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app4-fig1-v2.tif"/></fig><fig id="app4fig2" position="float"><label>Appendix 4—figure 2.</label><caption><title>The same set of videos as in <xref ref-type="table" rid="table5">Table 5</xref> pooled together, we evaluate the efficiency of our crossings solver.</title><p><italic>Consecutive frame segments</italic> are sequences of frames without gaps, for example due to crossings or visibility issues. We find these <italic>consecutive frame segments</italic> in data exported by TRex, and compare the distribution of segment-lengths to <monospace>idtracker.ai's</monospace> results (as a reference for an algorithm without a way to resolve crossings). In <monospace>idtracker.ai's</monospace> case, we segmented the non-interpolated tracks by missing frames, assuming tracks to be correct in between. The Y-axis shows the percentage of <inline-formula><mml:math id="inf123"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi>video</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>length</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>*</mml:mo><mml:mi mathvariant="normal">#</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>individuals</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in <italic>V</italic> videos that one column makes up for – the overall coverage for TRex was 98%, while <monospace>idtracker.ai</monospace> was slightly worse with 95.17%. Overall, the data distribution suggests that, probably due to it attempting to resolve crossings, TRex seems to produce longer consecutive segments.</p><p><supplementary-material id="app4fig2sdata1"><label>Appendix 4—figure 2—source data 1.</label><caption><title>A list of all consecutive frame segments used in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>.</title><p>In the table, they are indexed by their length, the software they were produced by, the video they originate from, as well as they bin they belong to.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-64000-app4-fig2-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="app4fig2sdata2"><label>Appendix 4—figure 2—source data 2.</label><caption><title>The raw data-points as plotted in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-64000-app4-fig2-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app4-fig2-v2.tif"/></fig><fig id="app4fig3" position="float"><label>Appendix 4—figure 3.</label><caption><title>Mean values of processing-times and 5 %/95 % percentiles for video frames of all videos in the <italic>speed dataset</italic> (<xref ref-type="table" rid="table1">Table 1</xref>), comparing two different matching algorithms.</title><p>Parameters were kept identical, except for the matching mode, and posture was turned off to eliminate its effects on performance. Our tree-based algorithm is shown in green and the Hungarian method in red. Grey numbers above the graphs show the number of samples within each bin, per method. Differences between the algorithms increase very quickly, proportional to the number of individuals. Especially the Hungarian method quickly becomes very computationally intensive, while our tree-based algorithm shows a much shallower curve. Some frames could not be solved in reasonable time by the tree-based algorithm alone, at which point it falls back to the Hungarian algorithm. Data-points belonging to these frames (<inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>79</mml:mn></mml:mrow></mml:math></inline-formula>) have been excluded from the results for both algorithms. One main advantage of the Hungarian method is that, with its bounded worst-case complexity (see Appendix D Matching an object to an object in the next frame), no such combinatorical explosions can happen. However, even given this advantage the Hungarian method still leads to significantly lower processing speed overall (see also <xref ref-type="table" rid="app4table3">Appendix 4—table 3</xref>).</p><p><supplementary-material id="app4fig3sdata1"><label>Appendix 4—figure 3—source data 1.</label><caption><title>Raw data for producing this figure and <xref ref-type="table" rid="app4table3">Appendix 4—table 3</xref>.</title><p>Each sample is represented as a row here, indexed by method (tree, approximate, hungarian), video and the bin (horizontal line in this figure).</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64000-app4-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app4-fig3-v2.tif"/></fig><p>Looking at the tree in <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1(b)</xref>, individuals (blue) are found along the y-axis/deeper into the tree while objects in the frame (orange) are listed along on the x-axis. This includes a ‘null’ case per individual, representing the possibility that it is <italic>not</italic> assigned to any object – ensuring that every individual has at least one edge.</p><p>Tree is never generated in its entirety (except in extreme cases), but it represents all <italic>possible</italic> combinations of individuals and objects. Overall, the set <italic>Q</italic> of every complete and valid path from top to bottom would be exactly the same as the set of every valid permutation of pairings between objects (plus null) and individuals. Edge weights in <italic>E</italic> are equal to the probability <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ36">Equation 7</xref>), abbreviated to <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> here since we are only ever looking at one time-step. <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is an object and <italic>i</italic> is an individual, so we can rewrite it in the current context as <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf129"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>U</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>We are maximizing the objective function<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>∈</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> is an element of all valid paths within <italic>T</italic>.</p><p>The simplest approach would be to traverse every edge in the graph and accumulate a sum of probabilities along each path, guaranteeing to find the optimal solution eventually. Since the number of possible combinations <inline-formula><mml:math id="inf131"><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> grows rapidly with the number of edges, this is not realistic – even with few individuals. Thus, at least the <italic>typical</italic> number of visited edges has to be minimized. While we do not know the exact solution to our problem before traversing the graph, we can make very good guesses. For example, we may order nodes in such a way that branching (visiting a node leads to <italic>gt</italic> <sub>1</sub> new edges to be visited) is reduced in most cases. To do that, we first need to calculate the <italic>degree</italic> of each individual. The degree <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math></inline-formula> of individual <italic>u</italic>, which is exactly equivalent to the maximum number of edges going out from that individual, we define as<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow><mml:mo>:=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The maximally probable edge per individual also has to be computed beforehand, defined as<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Nodes are sorted first by their degree (ascending) and secondly by <inline-formula><mml:math id="inf133"><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> (descending). We call this ordered set <italic>S</italic>. Sorting by degree ensures that the nodes with the fewest outgoing edges are visited <italic>first</italic>, causing severe branching to only happen in the lower regions of the tree. This is preferable, because a new branch in the bottom layer merely results in a few more options. If this happens at the top, the tree is essentially duplicated <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>C</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math></inline-formula> times – in one step drastically increasing the overall number of items to be kept in memory. This process is, fittingly, called <italic>node sorting</italic> (<xref ref-type="bibr" rid="bib77">Weixiong, 1996</xref>). Sorting by <inline-formula><mml:math id="inf135"><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is only applied whenever nodes of the same degree have to be considered.</p><p>We always follow the most promising paths first (the one with the highest accumulated probability), which is called ‘best-first search’ (BFS) – our selection strategy for (1.) in D.2.1. BFS is implemented using a queue maintaining the list of all currently expanded nodes.</p><p>Regarding (2.) in D.2.1, we utilize <inline-formula><mml:math id="inf136"><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> as an approximation for the upper bound to the achievable probability in each vertex. For each layer with vertices of <italic>U</italic>, we calculate an accumulative sum <inline-formula><mml:math id="inf137"><mml:mrow><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> being indices into our ordered set <italic>S</italic> of individuals and <italic>i</italic> being the current depth in the graph (only counting vertices of <italic>U</italic>). This hierarchical upper limit for the expected value does not consider whether the respective edges are still <italic>viable</italic>, so they could have been eliminated already by assigning the object of <italic>V</italic> to another vertex of <italic>U</italic> above the current one. Any edge with <inline-formula><mml:math id="inf139"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>current</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is skipped since it can not improve upon our previous best value <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:math></inline-formula>. If we do find an edge with a better value, we replace <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:math></inline-formula> with the new value and continue.</p><p>As an example, let us traverse the tree in <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1b</xref>:</p><list list-type="bullet"><list-item><p>We first calculate <inline-formula><mml:math id="inf142"><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> for every <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.85</mml:mn><mml:mo>;</mml:mo><mml:mover><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mo>;</mml:mo><mml:mover><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), as well as the hierarchical probability table <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each index <inline-formula><mml:math id="inf146"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf147"><mml:mrow><mml:mrow><mml:mn>0.9</mml:mn><mml:mo>+</mml:mo><mml:mn>0.75</mml:mn></mml:mrow><mml:mo>;</mml:mo><mml:mn>0.75</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Individual 0 (the root) is expanded, which has one edge with probability <inline-formula><mml:math id="inf149"><mml:mrow><mml:mrow><mml:mn>0.85</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to object 3 (plus the null case) and is the only node with a degree of 1. We know that our now expanded node is the best, since it has the largest probability due to sorting, plus also is the deepest. In fact, this is true for all expanded nodes exactly in the order they are expanded (depth-first search <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula> best-first search for our case). We set <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mn>0.85</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The edge to NIL is added to our queue.</p></list-item><list-item><p>Objects in <italic>V</italic> are only virtual and always have zero-probability connections to the next individual in an ordered set (<inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>), so they do not add to the overall probability sum. We skip to the next node.</p></list-item><list-item><p>Individual 2 branches off into one or two different edges, depending on which edges have been chosen previously.</p></list-item><list-item><p>We first explore the edge towards object four with a probability of <inline-formula><mml:math id="inf153"><mml:mrow><mml:mrow><mml:mn>0.9</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.65</mml:mn><mml:mo>≥</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and add it to <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Only one possibility is left and we arrive at a leaf with an accumulated probability of <inline-formula><mml:math id="inf155"><mml:mrow><mml:mrow><mml:mn>0.85</mml:mn><mml:mo>+</mml:mo><mml:mn>0.9</mml:mn><mml:mo>+</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.75</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>We now perform backtracking, meaning we look at every expanded node in our queue, each time observing <inline-formula><mml:math id="inf156"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><list list-type="simple"><list-item><p>–NIL (from node 2) would be added to the front of our queue, however its probability <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mn>0.85</mml:mn><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.6</mml:mn><mml:mo>&lt;</mml:mo><mml:mn>1.75</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, so it is discarded.</p></list-item><list-item><p>–NIL (from node 0) would be added now, but its probability of <inline-formula><mml:math id="inf158"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>upper</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>limit</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.65</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>best</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, so it is also discarded.</p></list-item></list></list-item></list><p>We can see that with increased depth, we have to keep track of more and more possibilities. Since our nodes and edges are pre-sorted, our path through the tree is optimal after exactly <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> node expansions (not counting <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> expansions since they are only ‘virtual’).</p></sec><sec id="s13-4"><title>Complexity</title><p>Utilizing these techniques, we can achieve very good average-case complexity. Of course having a good worst-case complexity is important (such as the Hungarian method), but the impact of a good average-case complexity can be significant as well. This is illustrated nicely by the timings measured in Table <xref ref-type="table" rid="app4table3">Appendix 4—table 3</xref>, where our method consistently surpasses the Hungarian method in terms of performance – especially for very large groups of animals – despite having worse worst-case complexity. Usually, even in situations with over 1000 individuals present, the average number of leaves visited was approximately 1.112 (see Table <xref ref-type="table" rid="app4table5">Appendix 4—table 5</xref>) and each visit was a global improvement (not shown). The number of nodes visited per frame were around 2844 to <inline-formula><mml:math id="inf161"><mml:mrow><mml:mn>19</mml:mn><mml:mo>,</mml:mo><mml:mn>804</mml:mn><mml:mo>,</mml:mo><mml:mn>880</mml:mn></mml:mrow></mml:math></inline-formula> in the same video, which, given the maximal number of possible combinations <inline-formula><mml:math id="inf162"><mml:msup><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:msup></mml:math></inline-formula> for <italic>M</italic> edges and <italic>N</italic> individuals (<xref ref-type="bibr" rid="bib74">Thomas, 2016</xref>), is quite moderate. Especially considering the number of calculations that the Hungarian method has to perform in every step, which, according to its complexity, will be in the range of <inline-formula><mml:math id="inf163"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>≈</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mo>⁢</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula> individuals.</p><p>The average complexity of a solution using best-first-search BnB is given by <xref ref-type="bibr" rid="bib77">Weixiong, 1996</xref>. It depends on the probability of encountering a ‘zero-cost edge’ <italic>p</italic> <sub>0</sub>, as well as the mean branching factor <italic>b</italic> of the tree:</p><list list-type="order"><list-item><p><inline-formula><mml:math id="inf165"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>N</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf166"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>β</mml:mi><mml:mo>≤</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> and <italic>N</italic> is the depth of the tree</p></list-item><list-item><p><inline-formula><mml:math id="inf168"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf169"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><inline-formula><mml:math id="inf170"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>⇔</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>as <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>In our case, the depth of the tree is exactly the number of individuals <italic>N</italic>, which we have already substituted here. This is the number of nodes that have to be visited in the best case. A ‘zero-cost edge’ is an edge that does not add any cost to the current path. We are maximizing (not minimizing) so in our case this would be ‘an edge with a probability of 1’. While reaching exactly one is improbable, it is (in our case) equivalent to ‘having only one viable edge arriving at an object’. <italic>p</italic> <sub>0</sub> depends very much on the settings, specifically the maximum movement speed allowed, and behavior of individuals, which is why in scenarios with <italic>gt</italic> <sub>100</sub> individuals the maximum speed should always be adjusted first. To put it another way: If there are only few branching options available for the algorithm to explore per individual, which seems to be the case even in large groups, we can assume our graph to have a probability <italic>p</italic> <sub>0</sub> within <inline-formula><mml:math id="inf173"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≪</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The mean branching factor <italic>b</italic> is given by the mean number of edges arriving at an object (not an individual). Averaging at around <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>b</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> being the average number of assignable blobs per individual (roughly 1.005 in Video 0) and one the null-case, we can assume <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to be <italic>gt</italic> <sub>1</sub> on average. An average complexity of <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as long as <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>b</mml:mi><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is even better than the complexity of the Hungarian method (which is also <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the average-case, <xref ref-type="bibr" rid="bib5">Bertsekas, 1981</xref>), giving a possible explanation for the good results achieved using tree-based matching in TRex on average (Table <xref ref-type="table" rid="app4table3">Appendix 4—table 3</xref>).</p><p>Further optimizations could be implemented, for example using impact-based heuristics (as an example of dynamic variable ordering) instead of the static and coarse maximum probability estimate used here. Such heuristics first choose the vertex ‘triggering the largest search space reduction’ (<xref ref-type="bibr" rid="bib60">Pesant et al., 2012</xref>). In our case, assigning an individual first if, for example, it has edges to many objects that each only one other individual is connected to.</p><table-wrap id="app4table1" position="float"><label>Appendix 4—table 1.</label><caption><title>Showing quantiles for frame timings for videos of the <italic>speed dataset</italic> (without posture enabled).</title><p>Video 15, 16, and 14 each contain a short sequence of taking out the fish, causing a lot of big objects and noise in the frame. This leads to relatively high spikes in these segments of the video, resulting in high peak processing timings here. Generally, processing time is influenced by a lot of factors involving not only TRex, but also the operating system as well as other programs. While we did try to control for these, there is no way to make sure. However, having sporadic spikes in the timings per frame does not significantly influence overall processing time, since it can be compensated for by later frames. We can see that videos of all quantities ≤256 individuals can be processed faster than they could be recorded. Videos that can not be processed faster than real-time are underlaid in gray.</p><p><supplementary-material id="app4table1sdata1"><label>Appendix 4—table 1—source data 1.</label><caption><title>Raw samples for this table and <xref ref-type="table" rid="app4table5">Appendix 4—table 5</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-64000-app4-table1-data1-v2.zip"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3">Video characteristics</th><th colspan="5">Ms / frame (processing)</th><th>Processing time</th></tr><tr><th>Video</th><th># ind.</th><th>Ms / frame</th><th>5%</th><th>Mean</th><th>95 %</th><th>Max</th><th>&gt; real-time</th><th>% video length</th></tr></thead><tbody><tr><td>0</td><td>1024</td><td>25.0</td><td>46.93</td><td>62.96</td><td>119.54</td><td>849.16</td><td>100.0%</td><td>358.12</td></tr><tr><td>1</td><td>512</td><td>20.0</td><td>19.09</td><td>29.26</td><td>88.57</td><td>913.52</td><td>92.11%</td><td>259.92</td></tr><tr><td>2</td><td>512</td><td>16.67</td><td>17.51</td><td>26.53</td><td>36.72</td><td>442.12</td><td>97.26%</td><td>235.39</td></tr><tr><td>3</td><td>256</td><td>20.0</td><td>8.35</td><td>11.28</td><td>13.25</td><td>402.54</td><td>1.03%</td><td>77.18</td></tr><tr><td>4</td><td>256</td><td>16.67</td><td>8.04</td><td>11.62</td><td>13.48</td><td>394.75</td><td>1.13%</td><td>94.77</td></tr><tr><td>5</td><td>128</td><td>16.67</td><td>3.54</td><td>5.14</td><td>5.97</td><td>367.92</td><td>0.41%</td><td>40.1</td></tr><tr><td>6</td><td>128</td><td>16.67</td><td>3.91</td><td>5.64</td><td>6.89</td><td>381.51</td><td>0.51%</td><td>44.38</td></tr><tr><td>7</td><td>100</td><td>31.25</td><td>2.5</td><td>3.57</td><td>5.19</td><td>316.75</td><td>0.1%</td><td>28.35</td></tr><tr><td>8</td><td>59</td><td>19.61</td><td>1.43</td><td>2.29</td><td>3.93</td><td>2108.77</td><td>0.19%</td><td>16.33</td></tr><tr><td>9</td><td>15</td><td>40.0</td><td>0.4</td><td>0.52</td><td>1.67</td><td>4688.5</td><td>0.01%</td><td>2.96</td></tr><tr><td>10</td><td>10</td><td>10.0</td><td>0.28</td><td>0.33</td><td>0.57</td><td>283.7</td><td>0.07%</td><td>8.08</td></tr><tr><td>11</td><td>10</td><td>31.25</td><td>0.21</td><td>0.25</td><td>0.65</td><td>233.7</td><td>0.01%</td><td>3.48</td></tr><tr><td>12</td><td>10</td><td>31.25</td><td>0.23</td><td>0.27</td><td>0.75</td><td>225.63</td><td>0.02%</td><td>2.82</td></tr><tr><td>13</td><td>10</td><td>31.25</td><td>0.22</td><td>0.25</td><td>0.54</td><td>237.32</td><td>0.02%</td><td>2.64</td></tr><tr><td>14</td><td>8</td><td>33.33</td><td>0.24</td><td>0.29</td><td>0.66</td><td>172.8</td><td>0.02%</td><td>1.8</td></tr><tr><td>15</td><td>8</td><td>40.0</td><td>0.22</td><td>0.26</td><td>0.88</td><td>244.88</td><td>0.01%</td><td>1.5</td></tr><tr><td>16</td><td>8</td><td>28.57</td><td>0.18</td><td>0.21</td><td>0.51</td><td>1667.14</td><td>0.02%</td><td>1.38</td></tr><tr><td>17</td><td>1</td><td>7.14</td><td>0.03</td><td>0.04</td><td>0.06</td><td>220.81</td><td>0.01%</td><td>1.56</td></tr></tbody></table></table-wrap><table-wrap id="app4table2" position="float"><label>Appendix 4—table 2.</label><caption><title>A quality assessment of assignment decisions made by the general purpose tracking system without the aid of visual recognition – comparing results of two accurate tracking algorithms with the assignments made by an approximate method.</title><p>Here, <italic>decisions</italic> are reassignments of an individual after it has been lost, or the tracker was too ‘unsure’ about an assignment. Decisions can be either correct or wrong, which is determined by comparing to reference data generated using automatic visual recognition: Every segment of frames between decisions is associated with a corresponding ‘baseline-truth’ identity from the reference data. If this association changes after a decision, then that decision is counted as wrong. Analysing a decision may fail if no good match can be found in the reference data (which is not interpolated). Failed decisions are ignored. Comparative values for the Hungarian algorithm (<xref ref-type="bibr" rid="bib40">Kuhn, 1955</xref>) are always exactly the same as for our tree-based algorithm, and are therefore not listed separately. Left-aligned <italic>total</italic>, <italic>excluded</italic> and <italic>wrong</italic> counts in each column are results achieved by an accurate algorithm, numbers to their right are the corresponding results using an approximate method. Raw data of trial runs using the hungarian and tree-based matching algorithms, as well as baseline data from manually or automatically corrected trials used in this table is available for download from <xref ref-type="bibr" rid="bib75">Walter et al., 2020</xref> (in A4T2_source_data.zip).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Video</th><th># ind.</th><th>Length</th><th colspan="2">Total</th><th colspan="2">Excluded</th><th colspan="2">Wrong</th></tr></thead><tbody><tr><td>7</td><td>100</td><td>1 min</td><td>717</td><td>755</td><td>22</td><td>22</td><td>45 (6.47%)</td><td>65 (8.87%)</td></tr><tr><td>8</td><td>59</td><td>10 min</td><td>279</td><td>312</td><td>146</td><td>100</td><td>55 (41.35%)</td><td>32 (16.09%)</td></tr><tr><td>9</td><td>15</td><td>1h0min</td><td>838</td><td>972</td><td>70</td><td>111</td><td>100 (13.02%)</td><td>240 (27.87%)</td></tr><tr><td>13</td><td>10</td><td>10min3s</td><td>331</td><td>337</td><td>22</td><td>22</td><td>36 (11.65%)</td><td>54 (17.14%)</td></tr><tr><td>12</td><td>10</td><td>10min3s</td><td>382</td><td>404</td><td>42</td><td>43</td><td>83 (24.41%)</td><td>130 (36.01%)</td></tr><tr><td>11</td><td>10</td><td>10min10s</td><td>1067</td><td>1085</td><td>50</td><td>52</td><td>73 (7.18%)</td><td>92 (8.91%)</td></tr><tr><td>14</td><td>8</td><td>3h15min22s</td><td>7424</td><td>7644</td><td>1428</td><td>1481</td><td>1174 (19.58%)</td><td>1481 (24.03%)</td></tr><tr><td>15</td><td>8</td><td>1h12min</td><td>3538</td><td>3714</td><td>427</td><td>517</td><td>651 (20.93%)</td><td>962 (30.09%)</td></tr><tr><td>16</td><td>8</td><td>3h18min13s</td><td>2376</td><td>3305</td><td>136</td><td>206</td><td>594 (26.52%)</td><td>1318 (42.53%)</td></tr><tr><td colspan="3">sum</td><td>16952</td><td>16754</td><td>-2343</td><td>-2554</td><td>2811 (19.24%)</td><td>4374 (27.38%)</td></tr></tbody></table></table-wrap><table-wrap id="app4table3" position="float"><label>Appendix 4—table 3.</label><caption><title>Comparing computation speeds of the tree-based tracking algorithm with the widely established Hungarian algorithm <xref ref-type="bibr" rid="bib40">Kuhn, 1955</xref>, as well as an approximate version optimized for large quantities of individuals.</title><p>Posture estimation has been disabled, focusing purely on the assignment problem in our timing measurements. The tree-based algorithm is programmed to fall back on the Hungarian method whenever the current problem ‘explodes’ computationally – these frames were excluded. Listed are relevant video metrics on the left and mean computation speeds on the right side for three different algorithms: (1) The tree-based and (2) the approximate algorithm presented in this paper, and (3) the Hungarian algorithm. Speeds listed here are percentages of real-time (the videos’ fps), demonstrating usability in closed-loop applications and overall performance. Results show that increasing the number of individuals both increases the time-cost, as well as producing much larger relative standard deviation values. (1) is almost always fast than (3), while becoming slower than (2) with increasing individual numbers. In our implementation, all algorithms produce faster than real-time speeds with 256 or fewer individuals (see also appendix Table <xref ref-type="table" rid="app4table1">Appendix 4—table 1</xref>), with (1) and (2) even getting close for 512 individuals.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4">Video metrics</th><th colspan="3">% real-time</th></tr><tr><th>Video</th><th># ind.</th><th>Fps (Hz)</th><th>Size (px<sup>2</sup>)</th><th>Tree</th><th>Approximate</th><th>Hungarian</th></tr></thead><tbody><tr><td>0</td><td>1024</td><td>40</td><td>3866 × 4048</td><td>35.49 ± 65.94</td><td>38.69 ± 65.39</td><td>12.05 ± 18.72</td></tr><tr><td>1</td><td>512</td><td>50</td><td>3866 × 4140</td><td>51.18 ± 180.08</td><td>75.02 ± 193.0</td><td>28.92 ± 29.12</td></tr><tr><td>2</td><td>512</td><td>60</td><td>3866 × 4048</td><td>59.66 ± 121.4</td><td>65.58 ± 175.51</td><td>23.18 ± 26.83</td></tr><tr><td>3</td><td>256</td><td>50</td><td>3866 × 4140</td><td>174.02 ± 793.12</td><td>190.62 ± 743.54</td><td>127.86 ± 9841.21</td></tr><tr><td>4</td><td>256</td><td>60</td><td>3866 × 4048</td><td>140.73 ± 988.15</td><td>155.9 ± 760.05</td><td>108.48 ± 2501.06</td></tr><tr><td>5</td><td>128</td><td>60</td><td>3866 × 4048</td><td>318.6 ± 347.8</td><td>353.58 ± 291.63</td><td>312.05 ± 337.71</td></tr><tr><td>6</td><td>128</td><td>60</td><td>3866 × 4048</td><td>286.13 ± 330.08</td><td>314.91 ± 303.53</td><td>232.33 ± 395.21</td></tr><tr><td>7</td><td>100</td><td>32</td><td>3584 × 3500</td><td>572.46 ± 98.21</td><td>611.5 ± 96.46</td><td>637.87 ± 97.03</td></tr><tr><td>8</td><td>59</td><td>51</td><td>2306 × 2306</td><td>744.98 ± 364.43</td><td>839.45 ± 257.56</td><td>864.01 ± 223.47</td></tr><tr><td>9</td><td>15</td><td>25</td><td>1880 × 1881</td><td>4626 ± 424.8</td><td>4585.08 ± 378.64</td><td>4508.08 ± 404.56</td></tr><tr><td>10</td><td>10</td><td>100</td><td>1920 × 1080</td><td>2370.35 ± 303.94</td><td>2408.27 ± 297.83</td><td>2362.42 ± 296.99</td></tr><tr><td>11</td><td>10</td><td>32</td><td>3712 × 3712</td><td>6489.12 ± 322.59</td><td>6571.28 ± 306.34</td><td>6472.0 ± 322.03</td></tr><tr><td>12</td><td>10</td><td>32</td><td>3712 × 3712</td><td>6011.59 ± 318.12</td><td>6106.12 ± 305.96</td><td>55.49.25 ± 318.21</td></tr><tr><td>13</td><td>10</td><td>32</td><td>3712 × 3712</td><td>6717.12 ± 325.37</td><td>6980.12 ± 316.59</td><td>6726.46 ± 316.87</td></tr><tr><td>14</td><td>8</td><td>30</td><td>3008 × 3008</td><td>8752.2 ± 2141.03</td><td>8814.63 ± 2140.4</td><td>8630.73 ± 2177.16</td></tr><tr><td>15</td><td>8</td><td>25</td><td>3008 × 3008</td><td>9786.68 ± 1438.08</td><td>10118.04 ± 1380.2</td><td>9593.44 ± 1439.28</td></tr><tr><td>16</td><td>8</td><td>35</td><td>3008 × 3008</td><td>6861.42 ± 1424.91</td><td>10268.82 ± 1339.8</td><td>9680.68 ± 1387.14</td></tr><tr><td>17</td><td>1</td><td>140</td><td>1312 × 1312</td><td>15323.05 ± 637.17</td><td>15250.39 ± 639.2</td><td>15680.93 ± 640.99</td></tr></tbody></table></table-wrap><table-wrap id="app4table4" position="float"><label>Appendix 4—table 4.</label><caption><title>Comparing the time-cost for tracking and converting videos in two steps with doing both of those tasks at the same time.</title><p>The columns <italic>prepare</italic> and <italic>tracking</italic> show timings for the tasks when executed separately, while <italic>live</italic> shows the time when both of them are performed at the same time using the live-tracking feature of TGrabs. The column <italic>win</italic> shows the time ‘won’ by combining tracking and preprocessing as the percentage <inline-formula><mml:math id="inf180"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>prepare</mml:mi><mml:mo>+</mml:mo><mml:mi>tracking</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>live</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>prepare</mml:mi><mml:mo>+</mml:mo><mml:mi>tracking</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The process is more complicated than simply adding up timings of the tasks. Memory and the interplay of work-loads have a huge effect here. Posture is enabled in all variants.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4">Video metrics</th><th colspan="3">Minutes</th><th/></tr><tr><th>Video</th><th># ind.</th><th>Length</th><th>Fps (Hz)</th><th>Prepare</th><th>Tracking</th><th>Live</th><th>Win (%)</th></tr></thead><tbody><tr><td>0</td><td>1024</td><td>8.33min</td><td>40</td><td>10.96 ± 0.3</td><td>41.11 ± 0.34</td><td>65.72 ± 1.35</td><td>-26.23</td></tr><tr><td>1</td><td>512</td><td>6.67min</td><td>50</td><td>11.09 ± 0.24</td><td>24.43 ± 0.2</td><td>33.67 ± 0.58</td><td>5.24</td></tr><tr><td>2</td><td>512</td><td>5.98min</td><td>60</td><td>11.72 ± 0.2</td><td>20.86 ± 0.47</td><td>31.1 ± 0.62</td><td>4.55</td></tr><tr><td>3</td><td>256</td><td>6.67min</td><td>50</td><td>11.09 ± 0.21</td><td>7.99 ± 0.17</td><td>12.32 ± 0.17</td><td>35.26</td></tr><tr><td>4</td><td>256</td><td>5.98min</td><td>60</td><td>11.76 ± 0.26</td><td>9.04 ± 0.26</td><td>15.08 ± 0.13</td><td>27.46</td></tr><tr><td>6</td><td>128</td><td>5.98min</td><td>60</td><td>11.77 ± 0.29</td><td>4.74 ± 0.13</td><td>12.13 ± 0.32</td><td>26.49</td></tr><tr><td>5</td><td>128</td><td>6.0min</td><td>60</td><td>11.74 ± 0.26</td><td>4.54 ± 0.1</td><td>12.08 ± 0.25</td><td>25.79</td></tr><tr><td>7</td><td>100</td><td>1.0min</td><td>32</td><td>1.92 ± 0.02</td><td>0.47 ± 0.01</td><td>2.03 ± 0.02</td><td>14.88</td></tr><tr><td>8</td><td>59</td><td>10.0min</td><td>51</td><td>6.11 ± 0.07</td><td>7.68 ± 0.12</td><td>9.28 ± 0.08</td><td>32.7</td></tr><tr><td>9</td><td>15</td><td>60.0min</td><td>25</td><td>12.59 ± 0.18</td><td>5.32 ± 0.07</td><td>13.17 ± 0.12</td><td>26.47</td></tr><tr><td>11</td><td>10</td><td>10.17min</td><td>32</td><td>8.58 ± 0.04</td><td>0.74 ± 0.01</td><td>8.8 ± 0.12</td><td>5.66</td></tr><tr><td>12</td><td>10</td><td>10.05min</td><td>32</td><td>8.68 ± 0.04</td><td>0.75 ± 0.01</td><td>8.65 ± 0.07</td><td>8.3</td></tr><tr><td>13</td><td>10</td><td>10.05min</td><td>32</td><td>8.67 ± 0.03</td><td>0.71 ± 0.01</td><td>8.65 ± 0.07</td><td>7.76</td></tr><tr><td>102</td><td>10</td><td>10.08min</td><td>100</td><td>4.17 ± 0.06</td><td>2.02 ± 0.02</td><td>4.43 ± 0.05</td><td>28.3</td></tr><tr><td>14</td><td>8</td><td>195.37min</td><td>30</td><td>110.51 ± 2.32</td><td>8.99 ± 0.22</td><td>109.97 ± 2.05</td><td>7.98</td></tr><tr><td>15</td><td>8</td><td>72.0min</td><td>25</td><td>31.84 ± 0.53</td><td>3.26 ± 0.07</td><td>32.1 ± 0.42</td><td>8.55</td></tr><tr><td>16</td><td>8</td><td>198.22min</td><td>35</td><td>133.45 ± 2.22</td><td>11.38 ± 0.28</td><td>1.33 ± 2.28</td><td>8.1</td></tr><tr><td colspan="7">mean</td><td>14.55 %</td></tr></tbody></table></table-wrap><table-wrap id="app4table5" position="float"><label>Appendix 4—table 5.</label><caption><title>Statistics for running the tree-based matching algorithm with the videos of the speed dataset.</title><p>We achieve low leaf and node visits across the board – this is especially interesting in videos with high numbers of individuals. High values for ’# nodes visited’ are only impactful if they make up a large portion of the assignments. These are the result of too many choices for assignments – the weak point of the tree-based algorithm – and lead to combinatorical ‘explosions’ (the method will take a really long time to finish). If such an event is detected, TRex automatically switches to a more computationally bounded algorithm like the Hungarian method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Video characteristics</th><th colspan="3">Matching stats</th></tr><tr><th>Video</th><th># ind.</th><th># nodes visited (5,50,95,100%)</th><th># leafs visited</th><th># improvements</th></tr></thead><tbody><tr><td>0</td><td>1024</td><td>[1535; 2858; 83243; 18576918]</td><td>1.113 ± 0.37</td><td>1.113</td></tr><tr><td>1</td><td>512</td><td>[1060; 8156; 999137; 19811558]</td><td>1.247 ± 0.61</td><td>1.247</td></tr><tr><td>2</td><td>512</td><td>[989; 2209; 56061; 8692547]</td><td>1.159 ± 0.47</td><td>1.159</td></tr><tr><td>3</td><td>256</td><td>[452; 479; 969; 205761]</td><td>1.064 ± 0.29</td><td>1.064</td></tr><tr><td>4</td><td>256</td><td>[475; 496; 584; 608994]</td><td>1.028 ± 0.18</td><td>1.028</td></tr><tr><td>5</td><td>128</td><td>[233; 245; 258; 7149]</td><td>1.012 ± 0.12</td><td>1.012</td></tr><tr><td>6</td><td>128</td><td>[237; 259; 510; 681702]</td><td>1046 ± 0.25</td><td>1.046</td></tr><tr><td>7</td><td>100</td><td>[195; 199; 199; 13585]</td><td>1.014 ± 0.14</td><td>1.014</td></tr><tr><td>8</td><td>59</td><td>[117; 117; 117; 16430]</td><td>1.014 ± 0.2</td><td>1.014</td></tr><tr><td>9</td><td>15</td><td>[24; 29; 29; 635]</td><td>1.027 ± 0.22</td><td>1.027</td></tr><tr><td>10</td><td>10</td><td>[17; 19; 19; 56]</td><td>1.001 ± 0.02</td><td>1.001</td></tr><tr><td>11</td><td>10</td><td>[19; 19; 19; 129]</td><td>1.006 ± 0.1</td><td>1.006</td></tr><tr><td>12</td><td>10</td><td>[19; 19; 19; 1060]</td><td>1.023 ± 0.23</td><td>1.023</td></tr><tr><td>13</td><td>10</td><td>[19; 19; 19; 106]</td><td>1.001 ± 0.04</td><td>1.001</td></tr><tr><td>14</td><td>8</td><td>[11; 15; 15; 893]</td><td>1.003 ± 0.08</td><td>1.003</td></tr><tr><td>15</td><td>8</td><td>[13; 15; 15; 597]</td><td>1.024 ± 0.23</td><td>1.024</td></tr><tr><td>16</td><td>8</td><td>[15; 15; 15; 2151]</td><td>1.009 ± 0.17</td><td>1.009</td></tr><tr><td>17</td><td>1</td><td>[1; 1; 1; 1]</td><td>1.0 ± 0.02</td><td>1.0</td></tr></tbody></table></table-wrap></sec></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s14" sec-type="appendix"><title>Posture</title><p>Estimating an animals orientation and body pose in space is a diverse topic, where angle and pose can mean many different things. We are not estimating the individual positions of many legs and antennae in TRex, we simply want to know where the front- and the back-end of the animal are. Ultimately, the goal here is to be able to align animals using an arbitrary axis with their head extending in one direction and their tail roughly in the opposite direction. In order to achieve this, we are required to follow a series of steps to acquire all the necessary information:</p><list list-type="order"><list-item><p>Locate objects in the image</p></list-item><list-item><p>Detect the edge of objects</p></list-item><list-item><p>Find an ordered set of points (the outline), which in sequence approximate the outer edge of an object in the scene. This is done for each object (as well as for holes).</p></list-item><list-item><p>Calculate a center-line based on local curvature of the outline.</p></list-item><list-item><p>Calculate head and tail positions.</p></list-item></list><p>The first point is a given at this point (see Appendix C Connected components algorithm). We can utilize the format in which connected components are computed in TRex (an ordered array of horizontal line segments), which reduces redundancy by avoiding to look at every individual pixel. These line segments also contain information about edges since every start and end has to be an edge-pixel, too.</p><p>Even though we already have a list of edge-pixels, retrieving an <italic>ordered</italic> set of points is crucial and requires much more effort. Without information about a pixels connectivity, we cannot differentiate between inner and outer shapes (holes vs. outlines) and we cannot calculate local curvature.</p></sec><sec id="s15" sec-type="appendix"><title>Connecting pixels to form an outline</title><p>We implemented an algorithm based on horizontal line segments, which only ever retains three consecutive rows of pixels (<italic>p</italic> previous, <italic>c</italic> current, and <italic>n</italic> next). These horizontal line segments always stem from a ‘blob’ (or connected component). Rows contain (i) their y-value in pixels, (ii) <inline-formula><mml:math id="inf181"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> values describing the first and last ‘on’-pixel that has been found in it, (iii) a set of detected border pixels (identified by their x-coordinate). A row is valid, whenever the <italic>y</italic> coordinate is not -1 – all three rows are initialized to an invalid <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the previous row. Using <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as a function <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> returns one for on-pixels at that x-coordinate, and 0 for off-pixels.</p><p>For each line, <italic>l</italic> in the sorted list of horizontal line segments, we detect border pixels:</p><list list-type="order"><list-item><p>Subtract the blobs position (minimum of all <inline-formula><mml:math id="inf185"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>l</italic> <sub><italic>y</italic></sub> separately) from <italic>l</italic></p></list-item><list-item><p>If <inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, a row has ended and a new one starts: call finalize else if <inline-formula><mml:math id="inf187"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>∧</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, we either skipped a few pixels in <italic>n</italic> or <italic>l</italic> starts before <italic>c</italic> even had valid pixels. This means that all pixels <italic>x</italic> between <inline-formula><mml:math id="inf188"><mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are border pixels in <italic>c</italic>.</p></list-item><list-item><p>If <inline-formula><mml:math id="inf189"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, or <italic>c</italic> is invalid, then line <italic>l</italic> ends before the previous row (<italic>c</italic>) even has any ‘on’-pixels. All pixels <italic>x</italic> between <inline-formula><mml:math id="inf190"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are border pixels in <italic>n</italic>. else (a) <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>s</mml:mi><mml:mo>≔</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>(b) if <inline-formula><mml:math id="inf192"><mml:mrow><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, then lines are overlapping in <italic>c</italic> and <italic>n</italic> (line <italic>l</italic>). We can fill <italic>n</italic> up with border while <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Set <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>:=</mml:mo><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. else if <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>∧</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, then <italic>l</italic> starts at the image border (which is an automatic border pixel) or there is a gap before <italic>l</italic>. Set <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>:=</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (c) All pixels at <italic>x</italic>-coordinates <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>s</mml:mi><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are border in <italic>n</italic>, if they are either (i) beyond <italic>c</italic>’s bounds (<inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), or (ii) <inline-formula><mml:math id="inf201"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Set <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>After iterating through all lines, we need two additional calls to finalize to populate the lines currently in <italic>c</italic> and <italic>n</italic> through.</p><p>A graph is updated each time a row is finalized. This graph stores all border ‘nodes’, as well as all a maximum of two edges per node (since this is the maximum number of neighbors for a line vertex). More on that below. The following procedure (finalize) prepares a row (<italic>c</italic>) to be integrated into the graph, using two parameters: A triplet of rows <inline-formula><mml:math id="inf203"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the first line <italic>l</italic>, which started the new row to be added.</p><list list-type="order"><list-item><p>If <italic>n</italic> is invalid, continue to the next operation. else if <inline-formula><mml:math id="inf204"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, then we skipped at least one row between <italic>n</italic> and the new row – making all on-pixels in <italic>n</italic> border pixels. else we have consecutive rows where <inline-formula><mml:math id="inf205"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. All on-pixels <italic>x</italic> in <italic>n</italic> between <inline-formula><mml:math id="inf206"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> are border pixels.</p></list-item><list-item><p>Now the current row (<italic>c</italic>) is certainly finished, as it will in the following become the previous row (<italic>p</italic>), which is read-only at that point. We can add every border-pixel of <italic>c</italic> to our graph (see below).</p></list-item><list-item><p>It then discards <italic>p</italic> and moves <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>c</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>n</mml:mi><mml:mo>→</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>, as well as reading a new row to assign to <italic>n</italic>, setting <inline-formula><mml:math id="inf209"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>The graph consists of nodes (border pixels), indexed by their x and y coordinates (integers) and containing a list of all on-pixels around them (8-neighborhood with top-left, top, left, bottom-left, etc.). This information is available when finalize is called, since the middle row (<italic>c</italic>) is fully defined at that point (its entire neighborhood has been cached).</p><p>After all rows have been processed, an additional step is needed to connect all nodes and produce a connected, clockwise ordered outline. We already marked all pixels that have at least one border. We can also already mark TOP, RIGHT, BOTTOM, and LEFT borders per node if no neighboring pixel is present in that direction, since these major directions will definitely get a ‘line’ in the end. So all we have left to do now, is check the diagonals. The points that will be returned, are located half-way along the outer edges of pixels. In the end, each pixel can potentially have four border lines (if it is a singular pixel without connections to other pixels, see yellow ‘hole’ in <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1b</xref>). The half-edge-points for each node are generated as follows:</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>The original image is displayed on the left.</title><p>Each square represents one pixel. The processed image on the right is overlaid with lines of different colors, each representing one connected component detected by our outline estimation algorithm. Dots in the centers of pixels are per-pixel-identities returned by OpenCVs findContours function (for reference) coded in the same colors as ours. Contours calculated by OpenCVs algorithm can not be used to estimate the one-pixel-wide ‘tail’ of the 9-like shape seen here, since it becomes a 1D line without sub-pixel accuracy. Our algorithm also detects diagonal lines of pixels, which would otherwise be an aliased line when scaled up.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app5-fig1-v2.tif"/></fig><list list-type="order"><list-item><p>A nodes list of border pixels is a sparse, ordered list of directions (top, top-right, …, top-left). Each major direction of these (TOP, RIGHT, BOTTOM, LEFT), if present, check the face of their square to the left of them (own direction - 1, or − 45°). For example, TOP would check top-left.</p></list-item><list-item><p>if the checked neighbor is on, we add an edge between our face (e.g. TOP) and its 90° rotated face (e.g. own direction + 2 = RIGHT). else check the face an additional 45° to the left (e.g. LEFT). (a) if it there is an on-pixel attached to this face, add an edge between the two faces (of the focal and its left pixel) in the same direction (e.g. TOP → TOP). (b) else we do not seem to have a neighbor to either side, so this must be a corner pixel. Add an edge from the focal face (e.g. TOP) to the side 90° to the left of itself (e.g. LEFT).</p></list-item></list><p>Each time an edge is added, more and more of the half-edges are becoming fully connected (meaning they have two of the allowed two edges). To generate the final result, all we have to do is to start somewhere in the graph and walk strictly in clockwise direction. ‘Walking’ is done using a queue and edges are followed using depth-first search (see Appendix D Matching an object to an object in the next frame): Each time a node is visited, all its yet unexplored edges are added to the front of the queue (in clockwise order). Already visited edges are marked (or pruned) and will not be traversed again – their floating-point positions (somewhere on an edge of its parent pixel) are added to an array.</p><p>After a path ended, meaning that no more edges can be reached from our current node, the collected floating-point positions are pushed to another array and a different, yet unvisited, starting node is sought. This way, we can accumulate all available outlines in a given image one-by-one – including holes.</p><p>These outlines will usually be further processed using an Elliptical Fourier Transform (or EFT, <xref ref-type="bibr" rid="bib39">Kuhl and Giardina, 1982</xref>), as mentioned in the main-text. Outlines can also be smoothed using a weighted average of the <italic>N</italic> points around a given point, or resampled to either reduce or (virtually) increase resolution.</p></sec><sec id="s16" sec-type="appendix"><title>Finding the tail</title><p>Given an ordered outline, curvature can be calculated locally (per index <italic>i</italic>):<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>∗</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>∗</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>∗</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf210"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℕ</mml:mi></mml:mrow></mml:math></inline-formula> is a parameter, which effectively leads to more smoothing when increased. Triangle area can be calculated as follows:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To find the ‘tail’, or the pointy end of the shape, we employ a method closely related to scipys find_peaks function: We find local maxima using discrete curve differentiation and then generate a hierarchy of these extrema. The only major difference to normal differentiation is that we assume periodicity to achieve our results – values wrap around in both directions, since we are dealing with an outline here. We then find the peak with the largest integral, meaning we detect both very wide and very high peaks (just not very slim ones). The center of this peak is the ‘tail’.</p><p>To find the head as well, we now have to search for the peak that has the largest (index-) distance to the tail-peak. This is a periodic distance, too, meaning that <italic>N</italic> is one of the closest neighbors of 0.</p><p>The entire outline array is then rotated, so that the head is always the first point in it. Both indexes are saved.</p></sec><sec id="s17" sec-type="appendix"><title>Calculating the center-line</title><p>A center-line, for a given outline, can be calculated by starting out at the head and walking in both directions from there – always trying to find a pair of points with minimal distance to each other on both sides. Two indices are used: <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> for left and right. We also allow some ‘wiggle-room’ for the algorithm to find the best-matching points on each side. This is limited by a maximum offset of ω points which is set to <inline-formula><mml:math id="inf212"><mml:mrow><mml:mn>0.025</mml:mn><mml:mo>*</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> by default, where <italic>N</italic> is the number of points in the outline. <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> gives the point on in outline at position <italic>i</italic>.</p><p>Starting from <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>:=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>:=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> we continue while <inline-formula><mml:math id="inf215"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:</p><list list-type="order"><list-item><p>Find <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>:=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>ω</mml:mi><mml:mo>∧</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. If no valid <italic>m</italic> can be found, abort. Otherwise set <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>r</mml:mi><mml:mo>≔</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Find <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>:=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>ω</mml:mi><mml:mo>∧</mml:mo><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. If no valid <italic>k</italic> can be found, abort. Otherwise set <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>l</mml:mi><mml:mo>≔</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Our segment now consists of points <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with a center vector of <inline-formula><mml:math id="inf222"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Push it to the center-line array. We can also calculate the width of the body at that point using <inline-formula><mml:math id="inf223"><mml:mrow><mml:mo fence="true">∥</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo fence="true">∥</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Set <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>:=</mml:mo><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Set <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>:=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>Head and tail positions can be switched now, for example for animals where the wider part is the head. We may also want to start at the slimmest peak first, which ever that is, since there we have not as much space for floating-point errors regarding where <italic>exactly</italic> the peak was. These options depend on the specific settings used in each video.</p><p>The angle of the center-line is calculated using <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for a vector between the first point and one point at an offset from it. The specific offset is determined by a midline stiffness parameter, which offers some additional stability – despite for example potentially noisy peak detection.</p></sec></boxed-text></app><app id="appendix-6"><title>Appendix 6</title><boxed-text><sec id="s18" sec-type="appendix"><title>Visual field estimation</title><p>Visual fields are calculated by casting rays and intersecting them with other individuals and the focal individual (for self-occlusion). An example of this can be seen in <xref ref-type="fig" rid="fig6">Figure 6</xref>. The following procedure requires posture for all individuals in a frame. In case an individual does not have a valid posture in the given frame, its most recent posture and position are used as an approximation. The field is internally represented as a discretized vector of multi-dimensional pixel values. Depending on the resolution parameter (<inline-formula><mml:math id="inf227"><mml:msub><mml:mi>F</mml:mi><mml:mi>res</mml:mi></mml:msub></mml:math></inline-formula>), which sets the number of pixels, each index in the array represents step-sizes of <inline-formula><mml:math id="inf228"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>res</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> radians. The <italic>F</italic> values are constants setting the minimum and maximum field of view (<inline-formula><mml:math id="inf229"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mn>130</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf230"><mml:msup><mml:mn>130</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> by default, which gives a range of <inline-formula><mml:math id="inf231"><mml:msup><mml:mn>260</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>). Each pixel consists of multiple data-streams: The distance to the other individual, the identity of the other individual and the body-part that the ray intersected with.</p><p>Eyes are simulated to be located on the outline of the focal individual, near the head. The distance to the head can be set by the user as a percentage of midline-length. To find the exact eye position, the program calculates intersections between vectors going left/right from that midline point, perpendicular to the midline, and the individual’s outline. In order to be able to simulate different types of binocular and monocular sight, a parameter for eye separation <inline-formula><mml:math id="inf232"><mml:msub><mml:mi>E</mml:mi><mml:mi>sep</mml:mi></mml:msub></mml:math></inline-formula> (radians) controls the offset from the head angle <inline-formula><mml:math id="inf233"><mml:msub><mml:mi>H</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> per eye. Left and right eye are looking in directions <inline-formula><mml:math id="inf234"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>sep</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>sep</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively.</p><p>We iterate through all available postures in a given frame and use a procedure which is very similar to depth-maps (<xref ref-type="bibr" rid="bib80">Williams, 1978</xref>) in for example OpenGL. In the case of 2D visual fields, this depth-map is 1D. Each pixel holds a floating-point value (initialized to <inline-formula><mml:math id="inf236"><mml:mi mathvariant="normal">∞</mml:mi></mml:math></inline-formula>) which is continuously compared to new samples for the same position – if the new sample is closer to the ‘camera’ than the reference value, the reference value is replaced. This way, after all samples have been evaluated, we generate a map of the objects closest to the ‘camera’ (in this case the eye of the focal individual). For that to work we also have to keep the identity in each of these discrete slots maintained. So each time a depth value is replaced, the same goes for all the other data-streams (such as identity and head-position). When an existing value is replaced, values in deeper layers of occlusion are pushed downwards alongside the old value for the first layer.</p><p>Position of the intersecting object’s top-left corner is located at <inline-formula><mml:math id="inf237"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf238"><mml:msub><mml:mi>E</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> be the position of each eye, relative to <inline-formula><mml:math id="inf239"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. For each point <inline-formula><mml:math id="inf240"><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> (coordinates relative to <inline-formula><mml:math id="inf241"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>) of the outline, check the distance between <inline-formula><mml:math id="inf242"><mml:msub><mml:mi>E</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> and the outline segments (<inline-formula><mml:math id="inf243"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). For each eye <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>E</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>:</p><p>1. Project angles ranging from <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf246"><mml:msub><mml:mi>α</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> is the eye orientation, using:<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">z</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula><inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>angle</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>normalize</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> normalizes beta to be between <inline-formula><mml:math id="inf248"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>2. If either <inline-formula><mml:math id="inf249"><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is inside the visual field (<inline-formula><mml:math id="inf251"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>):</p><p>(a) We call the first angle satisfying the condition β.</p><p>(b) Then the search range becomes <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">⌊</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo fence="false" stretchy="false">⌉</mml:mo><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">⌊</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo fence="false" stretchy="false">⌉</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the elements in <italic>R</italic> are integers.</p><p>(c) Let <inline-formula><mml:math id="inf253"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true">∥</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mo fence="true">∥</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the distance between outline point at <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and the eye (interpolation could be done here).</p><p>(d) Let index <inline-formula><mml:math id="inf255"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℕ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> be our index into the first layer of the depth-map <italic>depth</italic> <sub>0</sub>:</p><p>(e) if <inline-formula><mml:math id="inf256"><mml:mrow><mml:mrow><mml:msub><mml:mi>depth</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: Calculate all properties <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and push values at <italic>k</italic> in layer 0 to layer 1.</p><p>(f) Otherwise, if <inline-formula><mml:math id="inf258"><mml:mrow><mml:mrow><mml:msub><mml:mi>depth</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, calculate properties for layer one instead and move data from layer one further down, etc.</p><p>The data-streams are calculated individually with the following equations:</p><list list-type="bullet"><list-item><p>Distance: Given already in <inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mi>depth</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In practice, values are cut off at the maximum distance (size of the video squared) and normalized to <inline-formula><mml:math id="inf260"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>255</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Identity: Is assigned alongside <inline-formula><mml:math id="inf261"><mml:mrow><mml:msub><mml:mi>depth</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each element that successfully replacing another in the map.</p></list-item><list-item><p>Body-part: Let <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> tail index, <inline-formula><mml:math id="inf263"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> number of points in left/right side of the outline (given by tail- and head-indexes):</p><list list-type="simple"><list-item><p>(a) if <inline-formula><mml:math id="inf264"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="inf265"><mml:mrow><mml:mrow><mml:mi>head</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>distance</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>(b) else: <inline-formula><mml:math id="inf266"><mml:mrow><mml:mrow><mml:mi>head</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>distance</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></list-item></list></sec></boxed-text></app><app id="appendix-7"><title>Appendix 7</title><boxed-text><sec id="s19" sec-type="appendix"><title>The PV file format</title><p>Since we are using a custom file format to save videos recorded using TGrabs (MP4 video can be saved alongside PV for a limited time or frame-rate), the following is a short overview of PV6 contents and structure. This description is purely technical and concise. It is mainly intended for users who wish to implement a loader for the file format (e.g. in Python) or are curious.</p></sec><sec id="s20" sec-type="appendix"><title>Structure</title><p>Generally, the file is built as a header (containing meta information on the video) followed by a long data section and an index table plus a settings string at the end. The header at the start of the file can be read as follows:</p><list list-type="order"><list-item><p>version (string): ‘PV6’</p></list-item><list-item><p>channels (uint8): Hard-coded to 1</p></list-item><list-item><p>width and height (uint16): Video size</p></list-item><list-item><p>crop offsets (4x uint16): Offsets from original image</p></list-item><list-item><p>size of HorizontalLine struct (uchar)</p></list-item><list-item><p># frames (uint32)</p></list-item><list-item><p>index offset (uint64): Byte offset pointing to the index table for</p></list-item><list-item><p>timestamp (uint64): time since 1970 in microseconds of recording (or conversion time if unavailable)</p></list-item><list-item><p>empty string</p></list-item><list-item><p>background image (byte*): An array of uint8 values of size width * height * channels.</p></list-item><list-item><p>mask image size (uint64): 0 if no mask image was used, otherwise size in bytes followed by a byte* array of that size.</p></list-item></list><p>Followed by the data section, where information is saved per frame. This information can either be in a zip-compressed format, or raw (determined by size), see below:</p><list list-type="order"><list-item><p>compression flag (uint8): one if compression was used, 0 otherwise</p></list-item><list-item><p>if compressed:(a) original size (uint32)(b) compressed size (uint32)(c) lzo1x compressed data (byte*) in the format of the uncompressed variant (below)</p></list-item><list-item><p>if uncompressed:(a) timestamp since start time in header (uint32)(b) number of images in frame (uint16)(c) for each image in frame:</p><list list-type="roman-lower"><list-item><p>number of HorizontalLines (uint16)</p></list-item><list-item><p>data of HorizontalLine (byte*)</p></list-item><list-item><p>pixel data for each pixel in the previous array (byte*).</p></list-item></list></list-item></list><p>Files are concluded by the index table, which gives a byte offset for each video frame in the file, and a settings string. This index is used for quick frame skipping in TRex as well as random access. It consists of exactly one uint64 index per video frame (as determined by the number of video frames read earlier). After that map ends, a string follows, which contains a JSON style string of all metadata associated by the user (or program) with the video (such as species or size of the tank).</p></sec></boxed-text></app><app id="appendix-8"><title>Appendix 8</title><boxed-text><sec id="s21" sec-type="appendix"><title>Automatic visual identification</title><sec id="s21-1"><title>Network layout and training procedure</title><p>Network layout is sketched in <italic>1</italic> c. Using version 2.2.4 of Keras See <ext-link ext-link-type="uri" xlink:href="https://keras.io/api/layers/initializers/#glorotuniform-class">keras.io</ext-link> documentation for default arguments, weights of densely connected layers as well as convolutional layers are initialized using Xavier-initialization (<xref ref-type="bibr" rid="bib26">Glorot and Bengio, 2010</xref>). Biases are used and initialized to 0. The default image size in TRex is <inline-formula><mml:math id="inf267"><mml:mrow><mml:mn>80</mml:mn><mml:mo>×</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula>, but can be changed to any size in order to retain more detail or improve computation speed.</p><p>During training, we use the Adam optimizer (<xref ref-type="bibr" rid="bib4">Bengio et al., 2015</xref>) to traverse the loss landscape, which is generated by categorical focal loss. <italic>Categorical</italic> focal loss is an adaptation of the original <italic>binary</italic> focal loss (<xref ref-type="bibr" rid="bib43">Lin et al., 2020</xref>) for multiple classes:<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf268"><mml:msub><mml:mi mathvariant="bold">𝐏</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the prediction vector component returned by the network for class <italic>c</italic> in image <italic>j</italic>. <inline-formula><mml:math id="inf269"><mml:mi mathvariant="bold">𝐕</mml:mi></mml:math></inline-formula> is a set of validation images, which remains the same throughout the training process. It comprises 25 % of the images available per individual. Images are marked <italic>globally</italic> when becoming part of the validation dataset and are not used for training in the current or any of the following steps.</p><p>After each epoch, predictions are generated by performing a forward-pass through the network layers. Returned are the softmax-activations <inline-formula><mml:math id="inf270"><mml:msub><mml:mi mathvariant="bold">𝐏</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the last layer for each image <italic>j</italic> in the validation dataset. Simply calculating the mean of<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>gives the mean accuracy of the network. <italic>M</italic> is the number of images in the validation dataset, where <inline-formula><mml:math id="inf271"><mml:msub><mml:mi mathvariant="bold">𝐕</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> are the expected probability vectors per image <italic>j</italic>. However, much more informative is the per-class (per-identity) accuracy of the network among the set of images <italic>i</italic> belonging to class <italic>c</italic>, which is<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>;</mml:mo><mml:mpadded width="+5pt"><mml:mi>where</mml:mi></mml:mpadded><mml:msub><mml:mi>𝐕</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>given that all vectors in <italic>V</italic> are one-hot vectors – meaning the vector has length <italic>N</italic> with <inline-formula><mml:math id="inf272"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐕</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:mrow><mml:mo>≠</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐕</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Another constant, across training units – not just across epochs, is the set of images used to calculate mean uniqueness <inline-formula><mml:math id="inf274"><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> (see Box 1, as well as Guiding the Training Process). Values generated in each epoch <italic>t</italic> of every training unit are kept in memory and used to calculate their derivative <inline-formula><mml:math id="inf275"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s21-2"><title>Stopping-criteria</title><p>A training unit can be interrupted if one of the following conditions becomes true:</p><p>1. Training commenced for at least <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> epochs, but uniqueness value <inline-formula><mml:math id="inf277"><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> was never above<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>best</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf278"><mml:msub><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>best</mml:mi></mml:msub></mml:math></inline-formula> is the best mean uniqueness currently achieved by any training unit (initialized with zero). This prevents to train on faulty segments after a first successful epoch.</p><p>2. The worst accuracy value per class has been ‘good enough’ in the last three epochs:<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.97</mml:mn></mml:mrow></mml:math></disp-formula></p><p>3. The global uniqueness value has been plateauing for more than 10 epochs.<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></disp-formula></p><p>4. Overfitting: Change in loss is very low on average after more than five epochs. Mean loss is calculated as follows:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:msub><mml:mi>cFL</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mi>cFL</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Now if the difference between the current loss and the previous loss is below a threshold:<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">⌊</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mi>cFL</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow><mml:mo maxsize="120%" minsize="120%">⌉</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn><mml:mo>∗</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>5. Maximum number of epochs has been reached. User-defined option limiting the amount of time that training can take per unit. By default this limit is set to 150 epochs.</p><p>6. Loss is zero. No further improvements are possible within the current training unit, so we terminate and continue with the next.</p><p>A high per-class accuracy over multiple consecutive epochs is usually an indication that everything that can be learned from the given data has already been learned. No further improvements should be expected from this point, unless the training data is extended by adding samples from a different part of the video. The same applies to scenarios with consistently zero or very low change in loss. Even if improvements are still possible, they are more likely to happen during the final (overfitting) step where all of the data is combined.</p></sec></sec></boxed-text></app><app id="appendix-9"><title>Appendix 9</title><boxed-text><sec id="s22" sec-type="appendix"><title>Data used in this paper and reproducibility</title><p>All of the data, as well as the figures and tables showing the data, have been generated automatically. We provide the scripts that have been used, as well as the videos if requested. ‘Data’ refers to converted video-files, as well as log- and NPZ-files. Analysis has been done in Python notebooks, using mostly matplotlib and pandas, as well as numpy to load the data. Since TRex and TGrabs, as well as <monospace>idtracker.ai</monospace> have been run on a Linux system, we were able to run everything from two separate bash files:</p><list list-type="order"><list-item><p>run.bash</p></list-item><list-item><p>run_idtracker.bash</p></list-item></list><p>where (1) encompasses all trials run using TRex and TGrabs, both for the speed- and recognition-datasets. (2) runs <monospace>idtracker.ai</monospace> in its own dedicated Python environment, using only the recognition-dataset. The parameters we picked for <monospace>idtracker.ai</monospace> vary between videos and are hand-crafted, saved in individual .json files (see Table <xref ref-type="table" rid="app9table1">Appendix 9—table 1</xref> for a list of settings used). We ran multiple trials for each combination of tools and data with <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> where necessary:</p><list list-type="bullet"><list-item><p>3x TGrabs [speed-dataset]</p></list-item><list-item><p>5x TRex + recognition [recognition-dataset]</p></list-item><list-item><p>3x <monospace>idtracker.ai</monospace> [recognition-dataset]</p></list-item><list-item><p>TRex without recognition enabled [speed-dataset]:</p><list list-type="simple"><list-item><p>–3x for testing the tree-based, approximate and Hungarian methods (4.2 Tracking), without posture enabled – testing raw speeds (see Table <xref ref-type="table" rid="app4table3">Appendix 4—table 3</xref>)</p></list-item><list-item><p>–3x testing accuracy of basic tracking (see Table <xref ref-type="table" rid="app4table2">Appendix 4—table 2</xref>), with posture enabled.</p></list-item></list></list-item></list><p>A Python script used for <xref ref-type="fig" rid="fig5">Figure 5</xref>, which is run only once. It generates a series of results for the same video (Video 7 with 100 individuals) with different sample-sizes. It uses a single set of training samples and then – after equalizing the numbers of images per individual – generates multiple virtual subsets with fewer images. They span 15 different sample-sizes per individual, saving a history of accuracies for each run. We repeated the same procedure with for the different normalization methods (no normalization, moments and posture), each repeated five times.</p><p>As described in the main text, we recorded memory usage with an external tool (syrupy) and used it to measure both software solutions. This tool saves a log-file for each run, which is appropriately renamed and stored alongside the other files of that trial.</p><p>All runs of TRex are preceded by running a series of TGrabs commands first, in order to convert the videos in the datasets. We chose to keep these trials separately and load whenever possible, to avoid data-duplication. Since subsequent results of TGrabs are always identical (with the exception of timings), we only keep one version of the PV files (Appendix G The PV file format) as well as only one version of the results files generated using live-tracking. However, multiple runs of TGrabs were recorded in the form of log-files to get a measure of variance between runs in terms of speed and memory.</p><sec id="s22-1"><title>Human validation</title><p>To ensure that results from the automatic evaluation (in Visual identification: accuracy) are plausible, we manually reviewed part of the data. Specifically, the table in <xref ref-type="table" rid="table3">Table 3</xref> shows an overview of the individual events reviewed and percentages of wrongly assigned frames. Due to the length of videos and the numbers of individuals inside the videos, we did not review all videos in their entirety, as shown in the table. Using the reviewing tools integrated in TRex, we focused on crossings that were automatically detected. These tools allow the user to jump directly to points in the video that it deems problematic. Detecting problematic situations is equivalent to detecting the end of individual segments (see Automatic visual identification based on machine learning). While iterating through these situations, we corrected individuals that have been assigned to the wrong object, generating a clean and corrected baseline dataset. We assumed that an assignment is correct, as long as the individual is at least part of the object that the identity has been assigned to. Misassignments were typically fixed after a few frames. Identities always returned to the correct individuals afterward (thus not causing a chain of follow-up errors).</p></sec><sec id="s22-2"><title>Comparison between trajectories from different softwares, or multiple runs of the same software</title><p>In our tests, the same individuals may have been given different IDs (or ‘names’) by each software (and in each run of each software for the same video), so, as a first step in every test where this was relevant, we had to determine the optimal pairing between identities of two datasets we wished to compare. This was done using a square distance matrix containing overall euclidean distances between identities is calculated by summing their per-frame distances. Optimally, this number would be zero for one and greater than zero for every other pairing, but temporary tracking mistakes and differences in the calculation of centroids may introduce noise. Thus, we solved the matching problem (see Appendix D Matching an object to an object in the next frame) for identities between each two datasets and paired individuals with the smallest accumulative distance between them. This was done for all results presented, where a direct comparison between two datasets was required.</p><table-wrap id="app9table1" position="float"><label>Appendix 9—table 1.</label><caption><title>Settings used for <monospace>idtracker.ai</monospace> trials, as saved inside the json files used for tracking.</title><p>The minimum intensity was always set to 0 and background subtraction was always enabled. An ROI is an area of interest in the form of an array of 2D vectors, typically a convex polygon containing the area of the tank (e.g. for fish or locusts). Since this format is quite lengthy, we only indicate here whether we limited the area of interest or not.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Video</th><th>length (# frames)</th><th>Nblobs</th><th>Area</th><th>Max. intensity</th><th>Roi</th></tr></thead><tbody><tr><td>7</td><td>1921</td><td>100</td><td>[165, 1500]</td><td>170</td><td>Yes</td></tr><tr><td>8</td><td>30,626</td><td>59</td><td>[100, 2500]</td><td>160</td><td>Yes</td></tr><tr><td>11</td><td>19,539</td><td>10</td><td>[200, 1500]</td><td>10</td><td>Yes</td></tr><tr><td>13</td><td>19,317</td><td>10</td><td>[200, 1500]</td><td>10</td><td>Yes</td></tr><tr><td>12</td><td>19,309</td><td>10</td><td>[200, 1500]</td><td>10</td><td>Yes</td></tr><tr><td>9</td><td>90,001</td><td>8</td><td>[190, 4000]</td><td>147</td><td>Yes</td></tr><tr><td>16</td><td>416,259</td><td>8</td><td>[200, 2500]</td><td>50</td><td>No</td></tr><tr><td>14</td><td>351,677</td><td>8</td><td>[200, 2500]</td><td>50</td><td>No</td></tr><tr><td>15</td><td>108,000</td><td>8</td><td>[250, 2500]</td><td>10</td><td>No</td></tr></tbody></table></table-wrap></sec></sec></boxed-text></app><app id="appendix-10"><title>Appendix 10</title><boxed-text><sec id="s23" sec-type="appendix"><title>Matching probabilities</title><p>One of the most important steps, when matching objects in one frame with objects in the next frame, is to calculate a numerical landscape that can then be traversed by a maximization algorithm to find the optimal combination. This landscape, which can be expressed as an <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi mathvariant="bold">𝐏</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, contains the probability values between <inline-formula><mml:math id="inf282"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> for each assignment between individuals <italic>i</italic> and objects <inline-formula><mml:math id="inf283"><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Below are definitions used in the following text:</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="inf284"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi></mml:msub></mml:math></inline-formula> is the typical time between frames (s), which depends on the video</p></list-item><list-item><p><inline-formula><mml:math id="inf285"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is most recent frame assigned to individual <italic>i</italic> previous to the current frame <italic>t</italic></p></list-item><list-item><p><inline-formula><mml:math id="inf286"><mml:msub><mml:mi>P</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:math></inline-formula> is the minimally allowed probability for the matching algorithm, underneath which the probabilities are assumed to be zero (and respective combination of object and individual is ignored). This value is set to 0.1 by default.</p></list-item><list-item><p><inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mi>ℕ</mml:mi></mml:mrow></mml:math></inline-formula> is the frame number associated with the time <italic>t</italic> (s)</p></list-item><list-item><p><inline-formula><mml:math id="inf288"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒯</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> is the time in seconds of frame <italic>f</italic>, with <inline-formula><mml:math id="inf289"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒯</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><inline-formula><mml:math id="inf290"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> indicates that <italic>x</italic> is a vector</p></list-item><list-item><p><inline-formula><mml:math id="inf291"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐔</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo fence="true">∥</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo fence="true">∥</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>Some values necessary for the following calculations are independent of the objects in the current frame and merely depend on data from previous frames. They can be re-used per frame and individual in the spirit of dynamic programming, reducing computational complexity in later steps:<disp-formula id="equ20"><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>δ</mml:mi><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mrow><mml:msub><mml:mi>𝐚</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>δ</mml:mi><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>𝐯</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Velocity <inline-formula><mml:math id="inf292"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and acceleration <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are simply the first and second derivatives of the individuals position at time <italic>t</italic>. <inline-formula><mml:math id="inf294"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is almost the same as the raw velocity, but its length is limited to the maximally allowed travel distance per second (<inline-formula><mml:math id="inf295"><mml:msub><mml:mi>D</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:math></inline-formula>, parameter track_max_speed).</p><p>These are then further processed, combining and smoothing across values of multiple previous frames (the last five valid ones). Here, <inline-formula><mml:math id="inf296"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates that the resulting value uses data from multiple frames.<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>is the speed at which the individual has travelled at recently. The mean direction of movement is expressed as<disp-formula id="equ24"><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>with the corresponding direction of acceleration<disp-formula id="equ25"><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The predicted position for individual <italic>i</italic> at time <italic>t</italic> is calculated as follows:<disp-formula id="equ26"><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>with weights for each considered time-step of<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a decay rate (parameter track_speed_decay) at which the impact of previous positions on the predicted position decreases with distance in time. With its value approaching 1, the resulting curve becomes steeper – giving less weight previous positions the farther away they are from the focal frame.</p><p>In order to locate an individual <italic>i</italic> in the current frame <inline-formula><mml:math id="inf298"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, a probability is calculated for each object <inline-formula><mml:math id="inf299"><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> found in the current frame resulting in the matrix:<disp-formula id="equ28"><label>(3)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Probabilities <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for all potential connections between blobs <inline-formula><mml:math id="inf301"><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and identities <italic>i</italic> at time <italic>t</italic> are calculated by first predicting the expected position <inline-formula><mml:math id="inf302"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐩</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each individual in the current frame <inline-formula><mml:math id="inf303"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This allows the program to focus on a small region of where the individual is expected to be located, instead of having to search the whole arena each time.</p><p>Based on the individual’s recent speed <inline-formula><mml:math id="inf304"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, direction <inline-formula><mml:math id="inf305"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold">𝐝</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, acceleration <inline-formula><mml:math id="inf306"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and angular momentum <inline-formula><mml:math id="inf307"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the individual’s projected position <inline-formula><mml:math id="inf308"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">𝐩</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is usually not far away from its last seen location for small time-steps. Only when <inline-formula><mml:math id="inf309"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> increases, if the individual has been lost for more than one frame or frame-rates are low, does it really play a role.</p><p>The actual probability values in <inline-formula><mml:math id="inf310"><mml:mrow><mml:mi mathvariant="bold">𝐏</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are then calculated by combining three metrics - each describing different aspects of potential concatenation of object <italic>b</italic> at time <italic>t</italic> to the already existing track for individual <italic>i</italic>:</p><p>The time metric <inline-formula><mml:math id="inf311"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which does not depend on the blob the individual is trying to be matched to. It merely reflects the recency of the individuals last occurence in a way that recently seen individual will always be preferred over individuals that have been lost for longer.<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>∧</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒯</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ31"><label>(4)</label><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the speed that it would take to travel from the individuals position to the blobs position in the given time (which might be longer than one frame), inverted and normalized to a value between 0 and 1.<disp-formula id="equ32"><label>(5)</label><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>and the angular difference metric <inline-formula><mml:math id="inf313"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, describing how close in angle the resulting vector of connecting blob and individual to a track would be to the previous direction vector:<disp-formula id="equ33"><mml:math id="m33"><mml:mrow><mml:mi>𝐚</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>𝐩</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>𝐩</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ34"><mml:math id="m34"><mml:mrow><mml:mi>𝐛</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>𝐩</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>𝐩</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ35"><label>(6)</label><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>∧</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The conditional ensures that the individual travelled a long enough distance, as the <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> function used to determine angular difference here lacks numerical precision for very small magnitudes. This is, however, an unproblematic case in this situation as the positions are in pixel-coordinates and anything below a movement of one pixel is likely to be due to noise anyway.</p><p>Combining (4 , 5) and (6) into a weighted probability product yields:<disp-formula id="equ36"><label>(7)</label><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Results from <xref ref-type="disp-formula" rid="equ36">equation (7)</xref> can now easily be used in a matching algorithm, in order to determine the best combination of objects and individuals as in Appendix D Matching an object to an object in the next frame. <inline-formula><mml:math id="inf315"><mml:msub><mml:mi>ω</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is usually set to 0.1, <inline-formula><mml:math id="inf316"><mml:msub><mml:mi>ω</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is set to 0.25 by default.</p></sec></boxed-text></app><app id="appendix-11"><title>Appendix 11</title><boxed-text><sec id="s24" sec-type="appendix"><title>Algorithm for splitting touching individuals</title><p><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><tbody><tr valign="top"><td colspan="2">  Algorithm 2 The algorithm used whenever two individuals touch, which is detected by a history-based method. <break/>This history-based method also provides <inline-formula><mml:math id="inf317"><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>, the number of expected objects within the current (big) object. <inline-formula><mml:math id="inf318"><mml:msub><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> is the starting threshold constant parameter, as set by the user.</td></tr><tr valign="top"><td colspan="2"><bold>Data</bold>: image of a blob, <inline-formula><mml:math id="inf319"><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> number of expected blobs</td></tr><tr valign="top"><td colspan="2"><bold>Result</bold>:<inline-formula><mml:math id="inf320"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> smaller image-segments, or error</td></tr><tr valign="top"><td colspan="2"><bold>while</bold> <inline-formula><mml:math id="inf321"><mml:mrow><mml:mi>threshold</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:math></inline-formula><bold>do</bold></td></tr><tr valign="top"><td colspan="2">    <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>;</mml:mo><mml:mspace width="thickmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>;</td></tr><tr valign="top"><td colspan="2">    <bold>if</bold> <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> <bold>then</bold></td></tr><tr valign="top"><td colspan="2">            <bold>break</bold>;</td></tr><tr valign="top"><td colspan="2">    <bold>end</bold></td></tr><tr valign="top"><td colspan="2">    <bold>if </bold> <inline-formula><mml:math id="inf324"><mml:mrow><mml:mrow><mml:mo fence="true">∥</mml:mo><mml:mi>blobs</mml:mi><mml:mo fence="true">∥</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> <bold>then</bold></td></tr><tr valign="top"><td colspan="2">            sort blobs by size in decreasing fashion;</td></tr><tr valign="top"><td colspan="2">            loop through all blobs <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> up to <inline-formula><mml:math id="inf326"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and detect whether the size-ratio between them is roughly even. <break/>            until then, we keep iterating.;</td></tr><tr valign="top"><td colspan="2">            <bold>if</bold> <inline-formula><mml:math id="inf327"><mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:msub><mml:mi>ratio</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> <bold>then</bold></td></tr><tr valign="top"><td colspan="2">                <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>;</td></tr><tr valign="top"><td colspan="2">                <bold>continue</bold> ;</td></tr><tr valign="top"><td colspan="2">            <bold>else</bold></td></tr><tr valign="top"><td colspan="2">                <bold>return</bold> <italic>blobs</italic>;</td></tr><tr valign="top"><td colspan="2">            <bold>end</bold></td></tr><tr valign="top"><td colspan="2">    <bold>else</bold></td></tr><tr valign="top"><td colspan="2">             <inline-formula><mml:math id="inf329"><mml:mrow><mml:mi>threshold</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>threshold</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</td></tr><tr valign="top"><td colspan="2">    <bold>end</bold></td></tr><tr valign="top"><td colspan="2"><bold>end</bold></td></tr><tr valign="top"><td colspan="2"><bold>return fail</bold>;</td></tr></tbody></table></table-wrap></p></sec></boxed-text></app><app id="appendix-12"><title>Appendix 12</title><boxed-text><sec id="s25" sec-type="appendix"><title>Posture and visual identification of highly deformable bodies</title><p>To evaluate further whether TRex’s posture and visual identification algorithms are broadly applicable, such as to mammals (e.g. rodents) – which have highly deformable bodies and thus increased variance per individual, we conducted additional analyses on videos of groups of four freely behaving mice (four C57BL/6 mice provided by D. Mink and M. Groettrup, and four ‘black mice’ from <xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref> provided to us by G.G. de Polavieja, and now linked under <ext-link ext-link-type="uri" xlink:href="https://idtrackerai.readthedocs.io/en/latest/data.html">idtrackerai.readthedocs.io</ext-link>).</p><p>Both videos, listed in <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref> and previewed in <xref ref-type="fig" rid="app12fig1">Appendix 12—figure 1</xref>, were analyzed using the same scripts used to generate <xref ref-type="table" rid="table3">Table 3</xref>, although each video has only been automatically tracked once (since accuracy of tracking is very high, as detailed below). We manually generated verified trajectories for both videos in full, following the same procedure described in I.1 Human validation, and compared them to the automatically generated trajectories. As can be seen in <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref>, TRex provides highly accurate results for both videos (<inline-formula><mml:math id="inf330"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mrow><mml:mn>99.6</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><table-wrap id="app12table1" position="float"><label>Appendix 12—table 1.</label><caption><title>Analogous to our analysis in <xref ref-type="table" rid="table3">Table 3</xref>, we compared automatically generated trajectories for two videos with manually verified ones.</title><p>Unlike the table in the main text, the sample size per video is only one here, which is why the standard deviation is zero in both cases. Results show very high accuracy for both videos, but relatively high numbers of interpolated frames compared to <xref ref-type="table" rid="table3">Table 3</xref>, where only the results for Video 9 showed more than 8 % interpolation and all others remained below 1 %.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Video</th><th># ind.</th><th>Reviewed (%)</th><th>Interpolated (%)</th><th>TRex</th></tr></thead><tbody><tr><td>(V1) <xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref></td><td>4</td><td>100.0</td><td>6.41</td><td>99.6 ± 0.0</td></tr><tr><td>(V2) D. Mink, M. Groettrup</td><td>4</td><td>100.0</td><td>1.74</td><td>99.82 ± 0.0</td></tr></tbody></table></table-wrap><p>Tracking, in theory and in practice as per our results here, is not generally impacted by the shape of individuals. However, individuals of some species tend to stay close/on top of con-specifics, which may render them impossible to track during periods where traditional image processing methods are unable to separate them. This explains the <inline-formula><mml:math id="inf331"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mn>6</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> interpolated frames in V1 (see <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref>), and also gives a reason why there is similarity between Video 9 and V1 in that respect – the locusts in Video 9 also spend much time either on top of others, or in places where they are harder to see.</p><p>Very short segments of mistaken identities (with a maximum length of less than 200 ms) occurred whenever individuals 'œappear' only for a short moment and the segment does not contain enough data to be properly matched with a learned identity. Correct identities were reassigned in all cases after the individuals could be visually separated from each other again, and such events only make up <inline-formula><mml:math id="inf332"><mml:mrow><mml:mi/><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the tracked data.</p><p>Furthermore, we found that our method for posture estimation works well despite the more deformable bodies and complex 3D-postures of mice. Head and tail may switch occasionally, especially when animals shrink to 'œa circle' from the viewpoint of the camera. Overall, however, by far most samples are normalized correctly – as can be seen in <xref ref-type="fig" rid="app12fig2">Appendix 12—figure 2</xref> and <xref ref-type="fig" rid="app12fig3">Appendix 12—figure 3</xref>.</p><fig-group><fig id="app12fig1" position="float"><label>Appendix 12—figure 1.</label><caption><title>Screenshots from videos V1 and V2 listed in <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref>.</title><p>Left (V1), video of four ‘black mice’ (17 min, 1272 × 909 px resolution) from <xref ref-type="bibr" rid="bib67">Romero-Ferrero et al., 2019</xref>. Right (V2), four C57BL/6 mice (1: 08 min, 1280 × 960 px resolution) by M. Groettrup, D. Mink.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app12-fig1-v2.tif"/></fig><media id="app12fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-64000-app12-fig1-video1.mp4"><label>Appendix 12—figure 1—video 1.</label><caption><title>A clip of the tracking results from V1, played back at normal speed.</title><p>Although it succumbs to noise in some frames (e.g. around 13 s), posture estimation remains remarkably robust to it throughout the video – sometimes even through periods where individuals overlap (e.g. at 27 s). Identity assignments are near perfect here, confirming our results in <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref>. <ext-link ext-link-type="uri" xlink:href="https://youtu.be/UnqRNKrYiR4">https://youtu.be/UnqRNKrYiR4</ext-link>.</p></caption></media><media id="app12fig1video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-64000-app12-fig1-video2.mp4"><label>Appendix 12—figure 1—video 2.</label><caption><title>Tracking results from V2, played back at two times normal speed.</title><p>Since resolution per animal in V2 is lower than V1, and contrast is lower, posture estimation in V2 is also slightly worse than in V1. Importantly, however, identity assignment is very stable and accurate. <ext-link ext-link-type="uri" xlink:href="https://youtu.be/OTP4dVSc7Es">https://youtu.be/OTP4dVSc7Es</ext-link>.</p></caption></media></fig-group><fig id="app12fig2" position="float"><label>Appendix 12—figure 2.</label><caption><title>Median of all normalized images (N = 7161, 7040, 7153, 7076) for each of the four individuals from V1 in <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref>.</title><p>Posture information was used to normalize each image sample, which was stable enough â€” also for TRex â€” to tell where the head is, and even to make out the ears on each side (brighter spots).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app12-fig2-v2.tif"/></fig><fig id="app12fig3" position="float"><label>Appendix 12—figure 3.</label><caption><title>Median of all normalized images (N = 1593, 1586, 1620, 1538) for each of the four individuals from V2 in <xref ref-type="table" rid="app12table1">Appendix 12—table 1</xref>.</title><p>Resolution per animal is lower than in V1, but ears are still clearly visible.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64000-app12-fig3-v2.tif"/></fig></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64000.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Lentink</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Pujades</surname><given-names>Sergi</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The paper by Walter and Couzin describes new open-source software to track individual animals, such as fish and insects, moving and interacting within groups in a quasi-two-dimensional plane. The method assumes controlled lighting conditions and a stationary camera, which facilitates ease of use and fast data analysis. The software is useful for animal behaviour, neurobiology and comparative biomechanics research. The authors report assessments of accuracy, run-time, and memory consumption, which illustrate this open-source solution is state-of-the-art. The invaluable utility of this open-source tool is enabled by how many standard and robust algorithms are combined in a single functional package. A particular strength of the present framework is the unusually large number of individuals that can be tracked simultaneously in real-time, enabling new virtual-reality manipulative studies of collective behaviour.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Christian Rutz as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Sergi Pujades (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this decision letter to help you prepare a revised submission. Please address the comments and suggestions to the best of your ability, mark all changes in the revised manuscript using a blue font, and provide a point-by-point response to the issues raised. This will significantly facilitate the Reviewing Editor's evaluation of your revision.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>The paper by Walter and Couzin describes new open-source software to track individual animals such as fish and insects moving and interacting within groups in a quasi-two-dimensional plane. The method assumes controlled lighting conditions and a stationary camera, which facilitates ease of use and fast data analysis. The software is useful for animal behaviour, neurobiology and comparative biomechanics research. The authors demonstrate this utility based on assessments of accuracy, runtime, and memory consumption, which illustrate this open-source solution is state-of-the-art. While the authors rightfully point out that their system is applicable to essentially any animal species, their tests were mainly on fish and insects, which have relatively limited deformations and appearance changes compared to many mammals and other organisms with dynamically morphing body shapes. The invaluable utility of this open-source tool is enabled by how many standard and robust algorithms are combined in a single functional package. A particular strength of the present framework is the unusually large number of individuals that can be tracked simultaneously in real-time, enabling new virtual-reality manipulative studies of collective behaviour.</p><p>Essential revisions:</p><p>Abstract: Please mention that background subtraction is the key default segmentation approach. Clarify that limb position of highly deformable bodies is not tracked.</p><p>To put the contribution of the software into perspective, it would be helpful to add in the conclusions the assumptions made on the shape of the animals (subsection “Posture Analysis”), and please provide examples of animals which do not fall into this category, to help the general <italic>eLife</italic> readership comprehend both the promise and limits of the new method.</p><p>Subsection “Realtime Tracking Option for Closed-Loop Experiments”: Please clarify what &quot;very outdated &quot; or &quot;low-end&quot; is in this context. Specs are given in the Results. This could be rephrased in the line of: &quot;Problems of using a lower hardware as the recommended one (ref) lead to.… frame-rates, fragmented data and bad identity assignments.&quot; Please address.</p><p>&quot;QR codes&quot; are more correctly called &quot;fiducial markers&quot; (and mostly QR codes themselves are not used).</p><p>Subsection “Automatic Visual Identification Based on Machine Learning”: To our understanding, James Crall and Stacey Combes performed a thorough study documenting the effects of these fiducial markers (&quot;QR codes&quot;) for tracking and quantifying bee behaviour. It would be useful to cite this work, because it shows such documentation can be done although it is not trivial.</p><p>The final training:</p><p>Is a validation set kept to avoid overfitting? The procedure to check on &quot;uniqueness improvement&quot; is not clearly described. This could be improved.</p><p>Figure 1B: The text in the figure seems to imply that Trex would do real-time online tracking, but isn't this the case for TGrabs only? Please double-check the implication of the text in this figure to make sure it is what you wish to communicate to the reader.</p><p>Figure 2: Exports: Does TRex export to.csv? It seems .npz only, correct? Also in the Introduction.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64000.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>The paper by Walter and Couzin describes new open-source software to track individual animals such as fish and insects moving and interacting within groups in a quasi-two-dimensional plane. The method assumes controlled lighting conditions and a stationary camera, which facilitates ease of use and fast data analysis. The software is useful for animal behaviour, neurobiology and comparative biomechanics research. The authors demonstrate this utility based on assessments of accuracy, runtime, and memory consumption, which illustrate this open-source solution is state-of-the-art. While the authors rightfully point out that their system is applicable to essentially any animal species, their tests were mainly on fish and insects, which have relatively limited deformations and appearance changes compared to many mammals and other organisms with dynamically morphing body shapes. The invaluable utility of this open-source tool is enabled by how many standard and robust algorithms are combined in a single functional package. A particular strength of the present framework is the unusually large number of individuals that can be tracked simultaneously in real-time, enabling new virtual-reality manipulative studies of collective behaviour.</p></disp-quote><p>Thank you for your detailed review of our method, and for your helpful suggestions. We agree that we should have been more clear regarding the ability of our software to accurately track, and maintain identities of animals that have highly deformable bodies, such as rodents. We have now added an additional Appendix (please see Appendix 12) where we show TRex can readily be used to track mice which, as can be seen from the included videos, have highly-deformable body shapes.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Abstract: Please mention that background subtraction is the key default segmentation approach. Clarify that limb position of highly deformable bodies is not tracked.</p><p>To put the contribution of the software into perspective, it would be helpful to add in the conclusions the assumptions made on the shape of the animals (subsection “Posture Analysis”), and please provide examples of animals which do not fall into this category, to help the general eLife readership comprehend both the promise and limits of the new method.</p></disp-quote><p>We have now made it clear in the Abstract that the default is “using background-subtraction”. We have also replaced “postures” with “visual-fields, outlines, and head/rear of bilateral animals” to clarify what the specific assumptions/limits of our method are – as per our main text, where we write <italic>“</italic>TRex does not track individual body parts apart from the head and tail of the animal”. While it is possible, however, to derive limb information from the outline generated by TRex, we also added a sentence in the main-text to make clear that output from TRex can be utilised in complimentary markerless tracking software such as our DeepPoseKit (Graving et al., 2020), and DeepLabCut (Mathis et al., 2018). The paragraph now reads as follows:</p><p>“When detailed tracking of all extremities is required, TRex offers an option that allows it to interface with third-party software like DeepPoseKit (Graving et al., 2019), SLEAP (Pereira et al., 2020), or DeepLabCut (Mathis et al., 2018). […] Normalisation, for example, can make it easier for machine-learning algorithms in these tools to learn where body-parts are likely to be (see Figure 5) and may even reduce the number of clicks required during annotation.”</p><disp-quote content-type="editor-comment"><p>Subsection “Realtime Tracking Option for Closed-Loop Experiments”: Please clarify what &quot;very outdated &quot; or &quot;low-end&quot; is in this context. Specs are given in the Results. This could be rephrased in the line of: &quot;Problems of using a lower hardware as the recommended one (ref) lead to.… frame-rates, fragmented data and bad identity assignments.&quot; Please address.</p></disp-quote><p>We have rephrased the sentence to:</p><p>“Running the program on hardware with specifications below our recommendations (see Results), however, may affect frame-rates as described below”,</p><p>and added clarifications to the paragraph as follows:</p><p>“If the script (or any other part of the recording process) takes too long to execute in one frame, consecutive frames may be dropped until a stable frame-rate can be achieved. […] Alternatively, if live-tracking is enabled but closed-loop feedback is disabled, the program maintains detected objects in memory and tracks them in an asynchronous thread (potentially introducing wait time after the recording stops).”</p><disp-quote content-type="editor-comment"><p>&quot;QR codes&quot; are more correctly called &quot;fiducial markers&quot; (and mostly QR codes themselves are not used).</p></disp-quote><p>We changed this line, and also added citations for recent papers:</p><p>“Attaching fiducial markers (such as QR codes) to animals allows for a very large number (thousands) of individuals to be uniquely identified at the same time (see Gernat et al., 2018, Wild et al., 2020, Mersch et al., 2013, Crall et al., 2015) – and over a much greater distance than RFID tags.”</p><disp-quote content-type="editor-comment"><p>Subsection “Automatic Visual Identification Based on Machine Learning”: To our understanding, James Crall and Stacey Combes performed a thorough study documenting the effects of these fiducial markers (&quot;QR codes&quot;) for tracking and quantifying bee behaviour. It would be useful to cite this work, because it shows such documentation can be done although it is not trivial.</p></disp-quote><p>This is a good point. We searched extensively and could not find a paper by these authors that had thoroughly evaluated the effects of fiducial markers (we are certainly keen to add it and are sorry if we have missed it). We have already cited another paper by Crall and Combes (Crall et al., 2015) elsewhere, but this does not study how tags impacted the animals' behavior. However, we have now cited a related paper, “Switzer and Combes, 2016”, which is a comparative study on the effects of using RFID tags vs. paint to mark animals. It shows that the effects of different kinds of tagging can, with considerable effort, be documented. The paragraph now reads:</p><p>“While physical tagging is often an effective method by which to identify individuals, it requires animals to be caught and manipulated, which can be difficult (Mersch et al., 2013) and is subject to the physical limitations of the respective system. […] In addition, for some animals, like fish and termites, attachment of tags that are effective for discriminating among a large number of individuals can be problematic, or impossible.”</p><disp-quote content-type="editor-comment"><p>The final training:</p><p>Is a validation set kept to avoid overfitting? The procedure to check on &quot;uniqueness improvement&quot; is not clearly described. This could be improved.</p></disp-quote><p>Yes. In addition to using Dropout layers in our network layout, a validation set is kept to avoid overfitting in the early training units. This is necessary since 1. overfitting is much more likely to happen in smaller datasets, and 2. at early stages we still care about how well our network generalises (especially to segments far away from the current segment). This restriction is much less important in the final training unit (except maybe for a use-case involving transfer learning, where it can be disabled) simply because it is the end of the procedure and we aim to (1) include all parts of the video, and (2) get as close to 1/0 probability predictions as we can (anecdotally speaking, however, we do not observe such overfitting effects in our results – likely thanks to Dropout layers – and extensive, but as-yet unpublished, work by A. Albi from our lab involves transfer learning and works equally well with/without this last step).</p><p>We have changed the text to reflect what we have described here:</p><p>“After the accumulation phase, one last training step is performed. […] The reason being that this is the first time when all of the training data from all segments is considered at once (instead of mostly the current segment plus fewer samples from previously accepted segments), and samples from all parts of the video have an equal likelihood of being used in training after possible reduction due to memory-constraints.”</p><disp-quote content-type="editor-comment"><p>Figure 1B: The text in the figure seems to imply that Trex would do real-time online tracking, but isn't this the case for TGrabs only? Please double-check the implication of the text in this figure to make sure it is what you wish to communicate to the reader.</p></disp-quote><p>You are absolutely correct. We kept the introductory bullet-points from the Materials and methods when moving the Results to the front of the document, but changed the beginning of that section to:</p><p>“Our software package consists of two task-specific tools, TGrabs and TRex, with different specializations. […] Typically, such a sequence can be summarized in four stages (see also Figure 2 for a flow diagram)”.</p><p>To better reflect this we have also changed Figure 1B/Tracking section.</p><disp-quote content-type="editor-comment"><p>Figure 2: Exports: Does TRex export to.csv? It seems .npz only, correct? Also in the Introduction.</p></disp-quote><p>Yes, TRex also exports to CSV. In the main text we now write:</p><p>“Results can be exported to independent data-containers (NPZ, or CSV for plain-text type data) for further analyses in software of the user’s choosing.”</p><p>The user can select the output format, which depends on a settings parameter (output_format), which accepts the values “npz” or “csv” (see <ext-link ext-link-type="uri" xlink:href="https://trex.run/docs/parameters_trex.html#output_format">https://trex.run/docs/parameters_trex.html#output_format</ext-link>). This can be changed from within the GUI, or via the command-line (as all parameters can). Some specific data cannot be exported as plain-text-CSV because they are generally very large, such as visual field data (essentially a stream of floating-point images) and the full posture data – thus being able to export all data in both formats does not offer any additional benefit to users. We have added “for plain-text type data” to reflect this minor restriction, excluding larger binary outputs.</p><p>We also changed the text under Figure 2 to:</p><p>“Thanks to its integration with parts of the TRex code, TGrabs can also perform online tracking for limited numbers of individuals, and save results to a .results file (that can be opened by TRex) along with individual tracking data saved to numpy data-containers (.npz) or standard CSV files, which can be used for analysis in third-party applications.”</p></body></sub-article></article>