<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56972</article-id><article-id pub-id-type="doi">10.7554/eLife.56972</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Shared and modality-specific brain regions that mediate auditory and visual word comprehension</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-179272"><name><surname>Keitel</surname><given-names>Anne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4498-0146</contrib-id><email>a.keitel@dundee.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-141443"><name><surname>Gross</surname><given-names>Joachim</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-5124"><name><surname>Kayser</surname><given-names>Christoph</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7362-5704</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Psychology, University of Dundee</institution><addr-line><named-content content-type="city">Dundee</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Institute of Neuroscience and Psychology, University of Glasgow</institution><addr-line><named-content content-type="city">Glasgow</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Institute for Biomagnetism and Biosignalanalysis, University of Münster</institution><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Department for Cognitive Neuroscience, Faculty of Biology, Bielefeld University</institution><addr-line><named-content content-type="city">Bielefeld</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name><role>Reviewing Editor</role><aff><institution>Imperial College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>24</day><month>08</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e56972</elocation-id><history><date date-type="received" iso-8601-date="2020-03-16"><day>16</day><month>03</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-08-18"><day>18</day><month>08</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Keitel et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Keitel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56972-v2.pdf"/><abstract><p>Visual speech carried by lip movements is an integral part of communication. Yet, it remains unclear in how far visual and acoustic speech comprehension are mediated by the same brain regions. Using multivariate classification of full-brain MEG data, we first probed where the brain represents acoustically and visually conveyed word identities. We then tested where these sensory-driven representations are predictive of participants’ trial-wise comprehension. The comprehension-relevant representations of auditory and visual speech converged only in anterior angular and inferior frontal regions and were spatially dissociated from those representations that best reflected the sensory-driven word identity. These results provide a neural explanation for the behavioural dissociation of acoustic and visual speech comprehension and suggest that cerebral representations encoding word identities may be more modality-specific than often upheld.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech decoding</kwd><kwd>visual speech</kwd><kwd>MEG</kwd><kwd>lip reading</kwd><kwd>auditory pathways</kwd><kwd>word classification</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/L027534/1</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name><name><surname>Kayser</surname><given-names>Christoph</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2014-CoG (grant No 646657)</award-id><principal-award-recipient><name><surname>Kayser</surname><given-names>Christoph</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>Joint Senior Investigator Grant (No 098433)</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>GR 2024/5-1</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009379</institution-id><institution>Interdisziplinäres Zentrum für Klinische Forschung, Universitätsklinikum Würzburg</institution></institution-wrap></funding-source><award-id>Gro3/001/19</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The comprehension of acoustic and visual speech depends on modality-specific pathways in the brain, which explains why auditory speech abilities and lip reading are not associated in typical adults.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Acoustic and visual speech signals are both elemental for everyday communication. While acoustic speech consists of temporal and spectral modulations of sound pressure, visual speech consists of movements of the mouth, head, and hands. Movements of the mouth, lips and tongue in particular provide both redundant and complementary information to acoustic cues (<xref ref-type="bibr" rid="bib39">Hall et al., 2005</xref>; <xref ref-type="bibr" rid="bib77">Peelle and Sommers, 2015</xref>; <xref ref-type="bibr" rid="bib83">Plass et al., 2019</xref>; <xref ref-type="bibr" rid="bib101">Summerfield, 1992</xref>), and can help to enhance speech intelligibility in noisy environments and in a second language (<xref ref-type="bibr" rid="bib68">Navarra and Soto-Faraco, 2007</xref>; <xref ref-type="bibr" rid="bib99">Sumby and Pollack, 1954</xref>; <xref ref-type="bibr" rid="bib110">Yi et al., 2013</xref>). While a plethora of studies have investigated the cerebral mechanisms underlying speech in general, we still have a limited understanding of the networks specifically mediating visual speech perception, that is lip reading (<xref ref-type="bibr" rid="bib5">Bernstein and Liebenthal, 2014</xref>; <xref ref-type="bibr" rid="bib16">Capek et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Crosse et al., 2015</xref>). In particular, it remains unclear whether visual speech signals are largely represented in dedicated regions, or whether these signals are encoded by the same networks that mediate auditory speech perception.</p><p>Behaviourally, our ability to understand acoustic speech seems to be independent from our ability to understand visual speech. In the typical adult population, performance in auditory/verbal and visual speech comprehension tasks are uncorrelated (<xref ref-type="bibr" rid="bib19">Conrad, 1977</xref>; <xref ref-type="bibr" rid="bib47">Jeffers and Barley, 1980</xref>; <xref ref-type="bibr" rid="bib66">Mohammed et al., 2006</xref>; <xref ref-type="bibr" rid="bib100">Summerfield, 1991</xref>; <xref ref-type="bibr" rid="bib101">Summerfield, 1992</xref>). Moreover, large inter-individual differences in lip reading skills contrast with the low variability seen in auditory speech tests (<xref ref-type="bibr" rid="bib101">Summerfield, 1992</xref>). In contrast to this behavioural dissociation, neuroimaging and neuroanatomical studies have suggested the convergence of acoustic and visual speech information in specific brain regions (<xref ref-type="bibr" rid="bib13">Calvert, 1997</xref>; <xref ref-type="bibr" rid="bib15">Campbell, 2008</xref>; <xref ref-type="bibr" rid="bib86">Ralph et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Simanova et al., 2014</xref>). Prevalent models postulate a fronto-temporal network mediating acoustic speech representations, comprising a word-meaning pathway from auditory cortex to inferior frontal areas, and an articulatory pathway that extends from auditory to motor regions (<xref ref-type="bibr" rid="bib32">Giordano et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib37">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Hickok, 2012</xref>; <xref ref-type="bibr" rid="bib45">Huth et al., 2016</xref>; <xref ref-type="bibr" rid="bib67">Morillon et al., 2019</xref>). Specifically, a number of anterior-temporal and frontal regions have been implied in implementing a-modal semantic representations (<xref ref-type="bibr" rid="bib63">MacSweeney et al., 2008</xref>; <xref ref-type="bibr" rid="bib86">Ralph et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Simanova et al., 2014</xref>) and in enhancing speech perception in adverse environments, based on the combination of acoustic and visual signals (<xref ref-type="bibr" rid="bib32">Giordano et al., 2017</xref>).</p><p>Yet, when it comes to representing visual speech signals themselves, our understanding becomes much less clear. That is, we know relatively little about which brain regions mediate lip reading. Previous studies have shown that visual speech activates ventral and dorsal visual pathways and bilateral fronto-temporal circuits (<xref ref-type="bibr" rid="bib5">Bernstein and Liebenthal, 2014</xref>; <xref ref-type="bibr" rid="bib13">Calvert, 1997</xref>; <xref ref-type="bibr" rid="bib15">Campbell, 2008</xref>; <xref ref-type="bibr" rid="bib16">Capek et al., 2008</xref>). Some studies have explicitly suggested that auditory regions are also involved in lip reading (<xref ref-type="bibr" rid="bib13">Calvert, 1997</xref>; <xref ref-type="bibr" rid="bib14">Calvert and Campbell, 2003</xref>; <xref ref-type="bibr" rid="bib16">Capek et al., 2008</xref>; <xref ref-type="bibr" rid="bib58">Lee and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib78">Pekkola et al., 2005</xref>), for example by receiving signals from visual cortices that can be exploited to establish coarse-grained acoustic representations (<xref ref-type="bibr" rid="bib8">Bourguignon et al., 2020</xref>). While these findings can be seen to suggest that largely the same brain regions represent acoustic and visual speech, neuroimaging studies have left the nature and the functional specificity of these visual speech representations unclear (<xref ref-type="bibr" rid="bib5">Bernstein and Liebenthal, 2014</xref>; <xref ref-type="bibr" rid="bib22">Crosse et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Ozker et al., 2018</xref>). This is in part because most studies focused on mapping activations rather than specific semantic or lexical speech content. Indeed, alternative accounts have been proposed, which hold that visual and auditory speech representations are largely distinct (<xref ref-type="bibr" rid="bib5">Bernstein and Liebenthal, 2014</xref>; <xref ref-type="bibr" rid="bib26">Evans et al., 2019</xref>).</p><p>When investigating how speech is encoded in the brain, it is important to distinguish purely stimulus driven neural activity (e.g. classic ‘activation’) from activity specifically representing a stimulus and contributing to the participant’s percept on an individual trial (<xref ref-type="bibr" rid="bib9">Bouton et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Panzeri et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Tsunada et al., 2016</xref>). That is, it is important to differentiate the representations of sensory inputs per se from those representations of sensory information that directly contribute to, or at least correlate with, the single-trial behavioural outcome. Recent neuroimaging studies have suggested that those cerebral representations representing the physical speech are partly distinct from those reflecting the actually perceived meaning. For example, syllable identity can be decoded from temporal, occipital and frontal areas, but only focal activity in the inferior frontal gyrus (IFG) and posterior superior temporal gyrus (pSTG) mediates perceptual categorisation (<xref ref-type="bibr" rid="bib9">Bouton et al., 2018</xref>). Similarly, the encoding of the acoustic speech envelope is seen widespread in the brain, but correct word comprehension correlates only with focal activity in temporal and motor regions (<xref ref-type="bibr" rid="bib95">Scott, 2019</xref>; <xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>). In general, activity in lower sensory pathways seems to correlate more with the actual physical stimulus, while activity in specific higher-tier regions correlates with the subjective percept (<xref ref-type="bibr" rid="bib21">Crochet et al., 2019</xref>; <xref ref-type="bibr" rid="bib91">Romo et al., 2012</xref>). However, this differentiation poses a challenge for data analysis, and studies on sensory perception are only beginning to address this systematically (<xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Panzeri et al., 2017</xref>; <xref ref-type="bibr" rid="bib89">Ritchie et al., 2015</xref>).</p><p>We here capitalise on this functional differentiation of cerebral speech representations that simply reflect the physical stimulus, from those representations of the sensory inputs that correlate with the perceptual outcome, to identify the comprehension-relevant encoding of auditory and visual word identity in the human brain. That is, we ask where and to what degree comprehension-relevant representations of auditory and visual speech overlap. To this end, we exploited a paradigm in which participants performed a comprehension task based on individual sentences that were presented either acoustically or visually (lip reading), while brain activity was recorded using MEG (<xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>). We then extracted single-trial word representations and applied multivariate classification analysis geared to quantify (i) where brain activity correctly encodes the actual word identity regardless of behavioural outcome, and (ii) where the quality of the cerebral representation of word identity (or its experimentally obtained readout) is predictive of the participant’s comprehension. Note that the term ‘word identity’ in the present study refers to the semantic, as well as the phonological form of a word (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for an exploratory semantic and phonological analysis).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioural performance</title><p>On each trial, the 20 participants viewed or listened to visually or acoustically presented sentences (presented in blocks), and performed a comprehension task on a specific target word (4-alternative forced-choice identification of word identity). The 18 target words, which always occurred in the third or second last position of the sentence, each appeared in 10 different auditory and visual sentences to facilitate the use of classification-based data analysis (see table in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for all used target words). Acoustic sentences were presented mixed with background noise, to equalise performance between visual and auditory trials. On average, participants perceived the correct target word in approximately 70% of trials across auditory and visual conditions (chance level was 25%). The behavioural performance did not differ significantly between these conditions (<italic>M</italic><sub>auditory</sub> = 69.7%, SD = 7.1%, <italic>M</italic><sub>visual</sub> = 71.7%, SD = 20.0%; <italic>t</italic>(19) = −0.42, p=0.68; <xref ref-type="fig" rid="fig1">Figure 1</xref>), demonstrating that the addition of acoustic background noise indeed equalised performance between conditions. Still, the between-subject variability in performance was larger in the visual condition (between 31.7% and 98.3%), in line with the notion that lip reading abilities vary considerably across individuals (<xref ref-type="bibr" rid="bib5">Bernstein and Liebenthal, 2014</xref>; <xref ref-type="bibr" rid="bib101">Summerfield, 1992</xref>; <xref ref-type="bibr" rid="bib104">Tye-Murray et al., 2014</xref>). Due to the near ceiling performance (above 95% correct), the data from three participants in the visual condition had to be excluded from the neuro-behavioural analysis. Participants also performed the task with auditory and visual stimuli presented at the same time (audiovisual condition), but because performance in this condition was near ceiling (<italic>M</italic><sub>audiovisual</sub> = 96.4%, SD = 3.3%), we present the corresponding data only in the supplementary material (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Trial structure and behavioural performance.</title><p>(<bold>A</bold>) Trial structure was identical in the auditory and visual conditions. Participants listened to stereotypical sentences while a fixation dot was presented (auditory condition) or watched videos of a speaker saying sentences (visual condition). The face of the speaker is obscured for visualisation here only. After each trial, a prompt on the screen asked which adjective (or number) appeared in the sentence and participants chose one of four alternatives by pressing a corresponding button. Target words (here ‘beautiful’) occupied the 2<sup>nd</sup> or 3<sup>rd</sup> last position in the sentence. (<bold>B</bold>) Participants’ behavioural performance in auditory (blue) and visual (orange) conditions, and their individual SNR values (grey) used for the auditory condition. Dots represent individual participants (n = 20), boxes denote median and interquartile ranges, whiskers denote minima and maxima (no outliers present). MEG data of two participants (shaded in a lighter colour) were not included in neural analyses due to excessive artefacts. Participants exceeding a performance of 95% correct (grey line) were excluded from the neuro-behavioural analysis (which was the case for three participants in the visual condition). (<bold>C</bold>) Example sentence with target adjective marked in blue.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Explorative representational similarity analysis (RSA) of the behavioural data (n = 20).</title><p>Density plots show the distribution of within-participant correlations between behavioural representational dissimilarity matrices (RDMs) and RDMs obtained from phonological and semantic representations of the 18 target words (medians, interquartile ranges and densities). Behavioural RDMs were obtained by computing the confusion matrix from participants’ behavioural responses. Pairwise phonological distances for the stimulus material were computed using Phonological Corpus Tools (V1.4.0), and semantic distances were computed using fastText vector representations (see Materials and methods for details). Representational similarity was computed using Spearman rank correlations. Overall, phonological and semantic features both influenced participants’ responses. A repeated-measurements ANOVA (2 (conditions) x 2 (features)) yielded a main effect of condition (F(1,19) = 7.53, p=0.013; mean correlations: M<sub>auditory</sub> = 0.38, SEM = 0.01; M<sub>visual</sub> = 0.43, SEM = 0.02) and a main effect of features (F(1,19) = 20.98, p = &lt;0.001, M<sub>phon</sub> = 0.43, SEM = 0.01; M<sub>sem</sub> = 0.37, SEM = 0.01). A post-hoc comparison revealed that in both conditions phonological features influenced behaviour stronger than semantic features (Wilcoxon Signed-ranks test; Z<sub>auditory</sub> = 151, p=0.037, Z<sub>visual</sub> = 189, p=0.002). Examples for close phonological and semantic relationships between two words are given in image.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Data preparation and classification procedures.</title><p>Data preparation: Raw data were first de-noised and SQUID jumps were removed (preprocessing). Eye and heart artefacts and noisy channels were removed via visual inspection and ICA. Clean data were then down-sampled to 100 Hz and bandpass-filtered for further analysis. Data were projected into source space using an LCMV beamformer after a semi-automatic co-registration of MEG and individual anatomical MRIs. In source space, data were spatially smoothed (FWHM = 3 mm) and the time series for each grid point and trial was <italic>z</italic>-scored. Classification: Neural activity during hearing/seeing the target words was extracted for each grid point, using a 500-ms windows aligned to the onset of the target word. For each trial, this activity (during hearing “beautiful” in the above example) was compared with activity to the same target word in other trials (within-target distances), and with the activity to the three alternative words in other trials (between-target distances). Distances were quantified using Pearson correlations of the spatio-temporal searchlight activity and were averaged across repetitions of a word (<inline-formula><mml:math id="inf1"><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula>). The target word was then classified as that word for which the average correlation was strongest.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig1-figsupp2-v2.tif"/></fig></fig-group><p>An explorative representational similarity analysis (RSA) (<xref ref-type="bibr" rid="bib27">Evans and Davis, 2015</xref>; <xref ref-type="bibr" rid="bib56">Kriegeskorte et al., 2008</xref>) indicated that participants’ behavioural responses were influenced by both semantic and phonological features in both conditions (see Materials and methods, and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). A repeated-measurements ANOVA yielded a main effect of condition (<italic>F</italic>(1,19) = 7.53, p=0.013; mean correlations: <italic>M</italic><sub>auditory</sub> = 0.38, SEM = 0.01; <italic>M</italic><sub>visual</sub> = 0.43, SEM = 0.02) and a main effect of features (<italic>F</italic>(1,19) = 20.98, p&lt;0.001, <italic>M</italic><sub>phon</sub> = .43, SEM = 0.01; <italic>M</italic><sub>sem</sub> = 0.37, SEM = 0.01). A post-hoc comparison revealed that in both conditions phonological features influenced behaviour stronger than semantic features (Wilcoxon Signed-ranks test; <italic>Z<sub>auditory</sub></italic> = 151, p=0.037, <italic>Z<sub>visual</sub></italic> = 189, p=0.002). While the small number of distinct word identities used here (n = 9 in two categories) precludes a clear link between these features and the underlying brain activity, these results suggest that participants’ performance was also driven by non-semantic information.</p></sec><sec id="s2-2"><title>Decoding word identity from MEG source activity</title><p>Using multivariate classification, we quantified how well the single-trial identity of the target words (18 target words, each repeated 10 times) could be correctly predicted from source-localised brain activity (‘stimulus classifier’). Classification was computed in source space at the single-subject level in a 500 ms window aligned to the onset of the respective target word. Importantly, for each trial we computed classification performance within the subset of the four presented alternative words in each trial, on which participants performed their behavioural judgement. We did this to be able to directly link neural representations of word identity with perception in a later analysis. We first quantified how well brain activity encoded the word identity regardless of behaviour (‘stimulus-classification’; c.f. Materials and methods). The group-level analysis (n = 18 participants with usable MEG, cluster-based permutation statistics, corrected at p=0.001 FWE) revealed significant stimulus classification performance in both conditions within a widespread network of temporal, occipital and frontal regions (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Word classification based on MEG activity regardless of behavioural performance (‘stimulus classification’).</title><p>Surface projections show areas with significant classification performance at the group level (n = 18; cluster-based permutation statistics, corrected at p&lt;0.001 FWE). Results show strongest classification in temporal regions for the auditory condition (<bold>A</bold>) and occipital areas for the visual condition (<bold>B</bold>). Cluster peaks are marked with dots. Panel (<bold>C</bold>) overlays the significant effects from both conditions, with the overlap shown in green. The overlap contains bilateral regions in middle and inferior temporal gyrus, the inferior frontal cortex and dorsal regions of the postcentral and supramarginal gyrus (SMG). The peak of the overlap is in the postcentral gyrus. (<bold>D</bold>) Grid point-wise Bayes factors for a difference between auditory and visual word classification performance for all grid points in the ROIs characterised by a significant contribution to word classification in at least one modality in panel A or B (red: evidence for a difference between conditions, that is in favour of H1 [alternative hypothesis]; blue: evidence for no difference between conditions, that is in favour of H0 [null hypothesis]. RO – Rolandic Operculum; POST – postcentral gyrus; IFG – inferior frontal gyrus; OCC – occipital gyrus).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Whole-brain statistical maps for the comparison between auditory and visual word classification (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>(<bold>A</bold>) Results of a cluster-based permutation analysis (n = 18; 3000 within-subject permutations, corrected at p&lt;0.05 FWE). Shown are only those grid points that exhibit significant word classification in at least one condition. Auditory classification was significantly stronger in frontotemporal cortex (left: <italic>T</italic><sub>sum</sub> = 348.47, p&lt;0.001; right: <italic>T</italic><sub>sum</sub> = 269.62, p&lt;0.001). Visual classification was significantly stronger in a large bilateral occipital cluster (<italic>T</italic><sub>sum</sub> = −1755.58, p&lt;0.001). Not shown are two clusters in which visual classification was stronger than auditory classification but which themselves did not exhibit significant classification versus baseline (a right frontopolar region: <italic>T</italic><sub>sum</sub> = −156.86, p&lt;0.001; a left precentral region: <italic>T</italic><sub>sum</sub> = −31.50, p=0.002). (<bold>B</bold>) Results of a Bayes factor analysis derived from a comparison of the auditory and visual word classification (n = 18; red: evidence for significant modality difference, that is in favour of H1 [alternative hypothesis]; blue: evidence for no modality difference, that is in favour of H0 [null hypothesis]). The resulting Bayes factors show that for many grid points there was either substantial evidence for no difference between conditions, or inconclusive evidence. There was substantial or strong evidence for a modality difference in auditory and visual cortices and some frontal grid points.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Results of the audiovisual condition.</title><p>(<bold>A</bold>) Behavioural performance of all 20 participants. Scaling of the figure is identical to <xref ref-type="fig" rid="fig2">Figure 2</xref>. Dots represent individual participants, boxes denote median and interquartile ranges, whiskers denote minimum and maximum (no outliers present). MEG data of two participants (shaded in a lighter colour) were not included in neural analyses due to excessive artifacts. (<bold>B</bold>) Word classification regardless of behavioural performance (‘stimulus classification’, n = 18). Surface projections show areas with significant classification performance at the group level (surface projection of the cluster-based permutation statistics, corrected at p&lt;0.001 FWE). Strongest classification performance was observed in right auditory and bilateral visual sensory areas, with performance ranging from 25.63% to 33.43% (with a chance level of 25%). Statistical analysis yielded two clusters: a large bilateral cluster covering occipital and temporal regions that peaked in the right inferior occipital gyrus (right OCC; <italic>T</italic><sub>sum</sub> = 461.36, p&lt;0.001) and a left-hemispheric cluster that peaked in the middle temporal pole (left TP; Tsum = 12.62, p&lt;0.001). Cluster peaks are marked with dots TP – temporal pole; OCC – occipital gyrus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Cross-classification between auditory, visual conditions and audiovisual conditions (n = 18).</title><p>(<bold>A</bold>) Results of a group-level <italic>t</italic>-test based on cluster-based permutation. Left panel: No significant cross-classification performance between the auditory and visual conditions was found (n = 18; neither at p&lt;0.001, nor at a more lenient p&lt;0.05), supporting the notion that auditory and visual word identities are largely represented in different networks. Right panel: Areas where word identity in the auditory (upper panel) or visual (lower panel) conditions can be predicted significantly based on word representations obtained from the audiovisual condition. Auditory word identities can be significantly classified from audiovisual word representations in a small region in right temporal and supramarginal gyrus (<italic>T</italic><sub>sum</sub> = 2.61, p&lt;0.001). Visual word identities can be classified from audiovisual word presentations in a large cluster in bilateral occipital cortex (<italic>T</italic><sub>sum</sub> = 224.62, p&lt;0.001) and a small left-hemispheric cluster in the left inferior temporal gyrus (<italic>T</italic><sub>sum</sub> = 7.05, p&lt;0.001). Colour scale is adapted from <xref ref-type="fig" rid="fig2">Figure 2</xref>, to allow a comparison of results. (<bold>B</bold>) Results of a Bayes factor analysis derived from a comparison of the actual cross-classification performance to a distribution of performance values after data randomisation (red: evidence for significant cross-decoding, that is in favour of H1 [alternative hypothesis]; blue: evidence for no significant cross-decoding, that is in favour of H0 [null hypothesis]). The resulting Bayes factors show that for most grid points there was substantial evidence for no cross-classification between the auditory and visual conditions, while there was substantial or strong evidence for cross-classification between the auditory (visual) and the audiovisual condition.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig2-figsupp3-v2.tif"/></fig></fig-group><p>Auditory speech was represented bilaterally in fronto-temporal areas, extending into intra-parietal regions within the left hemisphere (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; <xref ref-type="table" rid="table1">Table 1</xref>). Cluster-based permutation statistics yielded two large clusters: a left-lateralised cluster peaking in inferior postcentral gyrus (left POST; <italic>T</italic><sub>sum</sub> = 230.42, p&lt;0.001), and a right-lateralised cluster peaking in the Rolandic operculum (right RO; <italic>T</italic><sub>sum</sub> = 111.17, p&lt;0.001). Visual speech was represented bilaterally in occipital areas, as well as in left parietal and frontal areas (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), with classification performance between 25.9% and 33.9%. There were three clusters: a large bilateral posterior cluster that peaked in the left calcarine gyrus (left OCC; <italic>T</italic><sub>sum</sub> = 321.78, p&lt;0.001), a left-hemispheric cluster that peaked in the inferior frontal gyrus (left IFG; <italic>T</italic><sub>sum</sub> = 10.98, p&lt;0.001), and a left-hemispheric cluster that peaked in the postcentral gyrus (left POST; <italic>T</italic><sub>sum</sub> = 35.83, p&lt;0.001). The regions representing word identity in both visual and auditory conditions overlapped in the middle and inferior temporal gyrus, the postcentral and supramarginal gyri, and the left inferior frontal gyrus (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; overlap in green). MNI coordinates of cluster peaks and the corresponding classification values are given in <xref ref-type="table" rid="table1">Table 1</xref>. Results for the audiovisual condition essentially mirror the unimodal findings and exhibit significant stimulus classification in bilateral temporal and occipital regions (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Peak effects of stimulus classification performance based on MEG activity.</title><p>Labels are taken from the AAL atlas (<xref ref-type="bibr" rid="bib105">Tzourio-Mazoyer et al., 2002</xref>). For each peak, MNI coordinates, and classification performance (mean and SEM) are presented. Chance level for classification was 25%. Abbreviations as used in <xref ref-type="fig" rid="fig2">Figure 2</xref> are given in parentheses.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" valign="top">Atlas label</th><th align="center" colspan="3" valign="top">MNI coordinates</th><th rowspan="2" valign="top"><italic>Classification %</italic> (SEM)</th></tr><tr><th align="right" valign="top">X</th><th align="right" valign="top">Y</th><th align="right" valign="top">Z</th></tr></thead><tbody><tr><td valign="top">Auditory peaks</td><td align="right" colspan="3" valign="top"/><td valign="top"/></tr><tr><td valign="top">Rolandic Oper R (RO)</td><td align="right" valign="top">41 </td><td align="right" valign="top">–14</td><td align="right" valign="top">20</td><td valign="top">28.89 (0.78)</td></tr><tr><td valign="top">Postcentral L (POST)</td><td align="right" valign="top">−48</td><td align="right" valign="top">–21</td><td align="right" valign="top">25</td><td valign="top">29.04 (1.00)</td></tr><tr><td valign="top">Visual peaks</td><td align="right" colspan="3" valign="top"/><td valign="top"/></tr><tr><td valign="top">Calcarine L (OCC)</td><td align="right" valign="top">−5</td><td align="right" valign="top">–101</td><td align="right" valign="top">−7</td><td valign="top">33.92 (1.53)</td></tr><tr><td valign="top">Frontal Inf Tri L (IFG)</td><td align="right" valign="top">−48</td><td align="right" valign="top">23</td><td align="right" valign="top">1</td><td valign="top">26.70 (0.83)</td></tr><tr><td valign="top">Postcentral L (POST)</td><td align="right" valign="top">−51</td><td align="right" valign="top">–24</td><td align="right" valign="top">47</td><td valign="top">26.85 (1.02)</td></tr><tr><td valign="top">Peak of overlap</td><td align="right" colspan="3" valign="top"/><td valign="top"/></tr><tr><td valign="top">Postcentral L (POST)</td><td align="right" valign="top">−47</td><td align="right" valign="top">–15</td><td align="right" valign="top">52</td><td valign="top">26.50 (0.67)</td></tr></tbody></table></table-wrap><p>To directly investigate whether regions differed in their classification performance between visual and auditory conditions, we performed two analyses. First, we investigated the evidence for or against the null hypothesis of no condition difference for all grid points contributing to word classification in at least one modality (i.e. the combined clusters derived from <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>). The respective Bayes factors for each ROI (from a group-level <italic>t</italic>-test) are shown in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. These revealed no conclusive evidence for many grid points within these clusters (1∕3 &lt; bf<sub>10</sub>&lt;3). However, both auditory clusters and the occipital visual cluster contained grid points with substantial or strong (bf<sub>10</sub> &gt; 3 and bf<sub>10</sub> &gt; 10, respectively) evidence for a significant modality difference. In contrast, the visual postcentral region (POST), the IFG and the overlap region contained many grid points with substantial evidence for no difference between modalities (1∕10 &lt; bf<sub>10</sub>&lt;1∕3). Second, we performed a full-brain cluster-based permutation test for a modality difference. The respective results, masked by the requirement of significant word classification in at least one modality, are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>. Auditory classification was significantly better in clusters covering left and right auditory cortices, while visual classification was significantly better in bilateral visual sensory areas. Full-brain Bayes factors confirm that, apart from sensory areas exhibiting strong evidence for a modality difference, many grid points show substantial evidence for no modality difference, or inconclusive evidence (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>).</p><p>Given that most clusters were found in only one hemisphere, we performed a direct test on whether these effects are indeed lateralised in a statistical sense (c.f. Materials and methods). We found evidence for a statistically significant lateralisation for both auditory clusters (left cluster peaking in POST: <italic>t</italic>(17) = 5.15, <italic>p</italic>FDR &lt;0.001; right cluster peaking in RO: <italic>t</italic>(17) = 4.26, <italic>p</italic>FDR &lt;0.01). In the visual condition, the lateralisation test for the two left clusters reached only marginal significance (left cluster peaking in IFG: <italic>t</italic>(17) = 2.19, <italic>p</italic>FDR = 0.058; left cluster peaking in POST: <italic>t</italic>(17) = 1.87, <italic>p</italic>FDR = 0.078). Note that the large occipital cluster in the visual condition is bilateral and we therefore did not test this for a lateralisation effect. Collectively, these analyses provide evidence that distinct frontal, occipital and temporal regions represent word identity specifically for visual and acoustic speech, while also providing evidence that regions within inferior temporal and frontal cortex, the SMG and dorsal post-central cortex reflect word identities in both modalities.</p></sec><sec id="s2-3"><title>Cerebral speech representations that are predictive of comprehension</title><p>The above analysis leaves it unclear which of these cerebral representations of word identity are actually relevant for single-trial word comprehension. That is, it remains unclear, which cerebral activations reflect the word identity in a manner that directly contributes to, or at least correlates with, participants' performance on the task. To directly address this, we computed an index of how strongly the evidence for a specific word identity in the neural single-trial word representations is predictive of the participant’s response. We regressed the evidence in the cerebral classifier for word identity against the participants’ behaviour (see Materials and methods). The resulting neuro-behavioural weights (regression <italic>betas</italic>) were converted into <italic>t</italic>-values for group-level analysis. The results in <xref ref-type="fig" rid="fig3">Figure 3</xref> (two-sided cluster-based permutation statistics, corrected at p=0.05 FWE) reveal several distinct regions in which neural representations of word identity are predictive of behaviour. In the auditory condition, we found five distinct clusters. Three were in the left hemisphere, peaking in the left inferior temporal gyrus (left ITG; <italic>T</italic><sub>sum</sub> = 469.55, p&lt;0.001), the inferior frontal gyrus (left IFG; <italic>T</italic><sub>sum</sub> = 138.70, p&lt;0.001), and the middle occipital gyrus (left MOG; <italic>T</italic><sub>sum</sub> = 58.44, p&lt;0.001). In the right hemisphere, the two significant clusters were in the supplementary motor area (right SMA; <italic>T</italic><sub>sum</sub> = 312.48, p&lt;0.001) and in the angular gyrus (right AG; <italic>T</italic><sub>sum</sub> = 68.59, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). In the visual condition, we found four clusters: A left-hemispheric cluster in the inferior frontal gyrus (left IFG; <italic>T</italic><sub>sum</sub> = 144.15, p&lt;0.001) and three clusters with right-hemispheric peaks, in the superior temporal gyrus (right STG; <italic>T</italic><sub>sum</sub> = 168.68, p&lt;0.001), the superior frontal gyrus (right SFG; <italic>T</italic><sub>sum</sub> = 158.39, p&lt;0.001) and the angular gyrus (right AG; <italic>T</italic><sub>sum</sub> = 37.42, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). MNI coordinates of cluster peaks and the corresponding <italic>beta</italic> and <italic>t</italic>-values are given in <xref ref-type="table" rid="table2">Table 2</xref>. Interestingly, these perception-relevant (i.e. predictive) auditory and visual representations did not overlap (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), although some of them occurred in adjacent regions in the IFG and AG.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cortical areas in which neural word representations predict participants’ response.</title><p>Coloured areas denote significant group-level effects (surface projection of the cluster-based permutation statistics, corrected at p&lt;0.05 FWE). (<bold>A</bold>) In the auditory condition (n = 18), we found five clusters (cluster peaks are marked with dots). Three were in left ventral regions, in the inferior frontal gyrus, the inferior temporal gyrus, and the occipital gyrus, the other two were in the right hemisphere, in the angular gyrus and the supplementary motor area. (<bold>B</bold>) In the visual condition (n = 15; three participants were excluded due to near ceiling performance), we found four clusters: In the left (dorsal) inferior frontal gyrus, the right anterior cingulum stretching to left dorsal frontal regions, in the right angular gyrus and the right superior temporal gyrus (all peaks are marked with dots). Panel (<bold>C</bold>) overlays the significant effects from both conditions. There was no overlap. However, both auditory and visual effects were found in adjacent regions within the left IFG and the right AG. Panel (<bold>D</bold>) shows distributions of grid point-wise Bayes factors for a difference between auditory and visual conditions for these clusters (red: evidence for differences between conditions, that is in favour of H1 [alternative hypothesis]; blue: evidence for no difference between conditions, that is in favour of H0 [null hypothesis]).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Whole-brain statistical maps for the comparison between auditory and visual neurobehavioural prediction (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</title><p>(<bold>A</bold>) Results of a cluster-based permutation analysis (n = 18; 3000 within-subject permutations, corrected at p&lt;0.05 FWE). Shown are only those grid points that exhibit significant word classification in at least one condition. Behavioural prediction was statistically stronger in the auditory condition in left middle occipital gyrus (<italic>T</italic><sub>sum</sub> = 114.57, p&lt;0.001), left calcarine gyrus (<italic>T</italic><sub>sum</sub> = 47.45, p&lt;0.001), right posterior angular gyrus (<italic>T</italic><sub>sum</sub> = 31.02, p=0.006), and bilateral supplementary motor area (<italic>T</italic><sub>sum</sub> = 75.98, p&lt;0.001, not visible due to medial location). Not shown is a cluster in which prediction in the visual condition was stronger than in the auditory condition but that did not show significant prediction versus baseline (a right middle frontal cluster: <italic>T</italic><sub>sum</sub> = −35.78, p=0.001). (<bold>B</bold>) Results of a Bayes factor analysis derived from a comparison of the auditory and visual neurobehavioural prediction (n = 15; red: evidence for significant modality difference, that is in favour of H1 [alternative hypothesis]; blue: evidence for no modality difference, that is in favour of H0 [null hypothesis]). The resulting Bayes factors show that for most grid points there was either substantial evidence for no difference between the auditory and visual conditions, or inconclusive evidence. There was substantial or strong evidence for a modality difference in left middle occipital and calcarine gyrus, as well as in right angular gyrus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Correlations between word classification and behavioural indices.</title><p>(<bold>A</bold>) Surface projection of rho-values from correlations between neural classification and behaviour. No significant clusters were found at an alpha-level of 0.05, supporting that stimulus classification performance alone does not predict behaviour. Upper panel: Correlation between auditory (visual) word classification and participants’ performance in the auditory (visual) conditions. Lower panel: Correlation between auditory (visual) word classification and participant-specific SNR values used for the auditory condition. (<bold>B</bold>) Results of a Bayes factor analysis for correlation coefficients (red: evidence for significant correlation, that is in favour of H1 [alternative hypothesis]; blue: evidence for no significant correlation, that is in favour of H0 [null hypothesis]). For the large majority of brain regions, there is substantial evidence for no correlation. Exceptions are scattered grid points, most notably in right occipital regions for a (negative) correlation between visual word classification and individual auditory SNR values across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig3-figsupp2-v2.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Peak effects for the neuro-behavioural analysis.</title><p>Labels are taken from the AAL atlas (<xref ref-type="bibr" rid="bib105">Tzourio-Mazoyer et al., 2002</xref>). For each local peak, MNI coordinates, regression <italic>beta</italic> (mean and SEM across participants) and corresponding <italic>t</italic>-value are presented. Abbreviations as used in <xref ref-type="fig" rid="fig3">Figure 3</xref> are given in parentheses.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" valign="top">Atlas label</th><th align="center" colspan="3" valign="top">MNI coordinates</th><th rowspan="2" valign="top"><italic>Beta</italic> (SEM)</th><th rowspan="2" valign="top"><italic>t</italic>-value</th></tr><tr><th align="right" valign="top">X</th><th align="right" valign="top">Y</th><th align="right" valign="top">Z</th></tr></thead><tbody><tr><td valign="top">Auditory</td><td align="right" colspan="3" valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Temporal Inf L (ITG)</td><td align="right" valign="top">−41</td><td align="right" valign="top">– 23</td><td align="right" valign="top">−26</td><td valign="top">0.106 (0.024)</td><td valign="top">4.40</td></tr><tr><td valign="top">Frontal Inf Orb L (IFG)</td><td align="right" valign="top">−28</td><td align="right" valign="top">25</td><td align="right" valign="top">–9</td><td valign="top">0.082 (0.031)</td><td valign="top">2.66</td></tr><tr><td valign="top">Occipital Mid L, Occipital Inf L (MOG)</td><td align="right" valign="top">−46</td><td align="right" valign="top">–83</td><td align="right" valign="top">−4</td><td valign="top">0.079 (0.029)</td><td valign="top">2.75</td></tr><tr><td valign="top">Supp Motor Area R (SMA)</td><td align="right" valign="top">3</td><td align="right" valign="top">11</td><td align="right" valign="top">52</td><td valign="top">0.089 (0.027)</td><td valign="top">3.33</td></tr><tr><td valign="top">Angular R (AG)</td><td align="right" valign="top">49</td><td align="right" valign="top">–67</td><td align="right" valign="top">40</td><td valign="top">0.079 (0.027)</td><td valign="top">2.87</td></tr><tr><td valign="top">Visual</td><td align="right" colspan="3" valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Frontal Inf Tri L (IFG)</td><td align="right" valign="top">−57</td><td align="right" valign="top">30</td><td align="right" valign="top">4</td><td valign="top">0.075 (0.017)</td><td valign="top">4.34</td></tr><tr><td valign="top">Frontal Sup Medial R, Cingulum Ant R (SFG)</td><td align="right" valign="top">9</td><td align="right" valign="top">47</td><td align="right" valign="top">15</td><td valign="top">0.080 (0.028)</td><td valign="top">2.86</td></tr><tr><td valign="top">Temporal Sup R (STG)</td><td align="right" valign="top">38</td><td align="right" valign="top">–30</td><td align="right" valign="top">10</td><td valign="top">0.086 (0.023)</td><td valign="top">3.77</td></tr><tr><td valign="top">Angular R (AG)</td><td align="right" valign="top">60</td><td align="right" valign="top">–55</td><td align="right" valign="top">34</td><td valign="top">0.073 (0.020)</td><td valign="top">3.55</td></tr></tbody></table></table-wrap><p>IFG – inferior frontal gyrus; MOG – middle occipital gyrus; AG – angular gyrus; SMA – supplementary motor area; ITG – inferior temporal gyrus; IFG – inferior frontal gyrus; STG – superior temporal gyrus; SFG – superior frontal gyrus.</p><p>Again, we asked whether the behavioural relevance of these regions exhibit a significant bias towards either modality by investigating the between-condition contrast for all clusters that are significantly predictive of behaviour (Bayes factors derived from the group-level <italic>t</italic>-test; <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Three auditory clusters contained grid points that differed substantially or strongly (bf<sub>10</sub> &gt; 3 and bf<sub>10</sub> &gt; 10, respectively) between modalities (left ITG, left MOG, and right AG). In addition, in two regions the majority of grid points provided substantial evidence for no difference between modalities (IFG and AG from the visual condition). A separate full-brain cluster-based permutation test (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>) provided evidence for a significant modality specialisation for auditory words in four clusters in the left middle occipital gyrus, left calcarine gyrus, right posterior angular gyrus, and bilateral supplementary motor area. The corresponding full-brain Bayes factors (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>) support this picture but also provide no evidence for a modality preference, or inconclusive results, in many other regions. Importantly, those grid points containing evidence for a significant modality difference in the full brain analysis (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>) correspond to those auditory ROIs derived in <xref ref-type="fig" rid="fig3">Figure 3A</xref> (MOG, posterior AG and SMA). On the other hand, regions predictive of visual word comprehension did not show a significant modality preference. Regarding the lateralisation of these clusters, we found that corresponding <italic>betas</italic> in the contralateral hemisphere were systematically smaller in all clusters but did not differ significantly (all <italic>p</italic><sub>FDR</sub> ≥0.15), hence providing no evidence for a strict lateralisation.</p><p>To further investigate whether perception-relevant auditory and visual representations are largely distinct, we performed a cross-decoding analysis, in which we directly quantified whether the activity patterns of local speech representations are the same across modalities. At the whole-brain level, we found no evidence for significant cross-classification (two-sided cluster-based permutation statistics, neither at a corrected p=0.001 nor a more lenient p=0.05; <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>, left panel). That significant cross-classification is possible in principle from the data, is shown by the significant results for the audiovisual condition (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A</xref>, right panel). An analysis of the Bayes factors for this cross-classification test confirmed that most grid points contained substantial evidence for no cross-classification between the auditory and visual conditions (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3B</xref>, left panel). On the other hand, there was strong evidence for significant cross-classification between the uni- and multisensory conditions in temporal and occipital regions (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3B</xref>, right panel).</p></sec><sec id="s2-4"><title>Strong sensory representations do not necessarily predict behaviour</title><p>The above results suggest that the brain regions in which sensory representations shape speech comprehension are mostly distinct from those allowing the best prediction of the actual stimulus (see <xref ref-type="fig" rid="fig4">Figure 4</xref> for direct visualisation of the results from both analyses). Only parts of the left inferior temporal gyrus (auditory condition), the right superior temporal gyrus (visual condition) and the left inferior frontal gyrus (both conditions) feature high stimulus classification and a driving role for comprehension. In other words, the accuracy by which local activity reflects the physical stimulus is generally not predictive of the impact of this local word representation on behaviour. To test this formally, we performed within-participant robust regression analyses between the overall stimulus classification performance and the perceptual weight of each local representation across all grid points. Group-level statistics of the participant-specific <italic>beta</italic> values provided no support for a consistent relationship between these (auditory condition: <italic>b</italic> = 0.05 ± 0.11 [M ± SEM], <italic>t</italic>(17) = 0.50, <italic>p</italic><sub>FDR</sub> = 0.81; visual condition: <italic>b</italic> = 0.03 ± 0.11 [M ± SEM], <italic>t</italic>(14) = 0.25, <italic>p</italic><sub>FDR</sub> = 0.81; participant-specific regression slopes are depicted in <xref ref-type="fig" rid="fig4">Figure 4B</xref>). A Bayes factor analysis also provided substantial evidence for no consistent relationship (bf<sub>10</sub> = 0.27 and bf<sub>10</sub> = 0.27, for auditory and visual conditions, respectively).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Largely distinct regions provide strong stimulus classification and mediate behavioural relevance.</title><p>(<bold>A</bold>) Areas with significant stimulus classification (from <xref ref-type="fig" rid="fig2">Figure 2</xref>) are shown in yellow, those with significant neuro-behavioural results (from <xref ref-type="fig" rid="fig3">Figure 3</xref>) in green, and the overlap in blue. The overlap in the auditory condition (<italic>N</italic> = 14 grid points) comprised parts of the left inferior and middle temporal gyrus (ITG), and the orbital part of the left inferior frontal gyrus (IFG). The overlap in the visual condition (<italic>N</italic> = 27 grid points) comprised the triangular part of the inferior frontal gyrus (IFG), and parts of the superior temporal gyrus (STG), extending dorsally to the Rolandic operculum. (<bold>B</bold>) Results of a regression between word classification performance and neurobehavioural weights. Individual participant’s slopes and all individual data points are shown. A group-level <italic>t</italic>-test on betas yielded no consistent relationship (both <italic>t</italic>s &lt;0.50, both <italic>p</italic>s = 81). Corresponding Bayes factors (both bf<sub>10</sub>s &lt; 1/3) provide substantial evidence for no consistent relationship.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-fig4-v2.tif"/></fig><p>Still, this leaves it unclear whether variations in the strength of neural speech representations (i.e. the ‘stimulus classification’) can explain variations in the behavioural differences <italic>between</italic> participants. We therefore correlated the stimulus classification performance for all grid points with participants’ behavioural data, such as auditory and lip-reading performance, and the individual SNR value. We found no significant clusters (all <italic>p</italic>s &gt; 0.11, two-sided cluster-based permutation statistics, uncorrected across the four tests), indicating that stimulus classification performance was not significantly correlated with behavioural performance across participants (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). The corresponding Bayes factors confirm that in the large majority of brain regions, there is substantial evidence for no correlation (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Acoustic and visual speech are represented in distinct brain regions</title><p>Our results show that the cerebral representations of auditory and visual speech are mediated by both modality-specific and overlapping (potentially amodal) representations. While several parietal, temporal and frontal regions were engaged in the encoding of both acoustically and visually conveyed word identities (‘stimulus classification’), comprehension in both sensory modalities was driven mostly by distinct networks. Only the inferior frontal and anterior angular gyrus contained adjacent regions that contributed to both auditory and visual comprehension.</p><p>This multi-level organisation of auditory and visual speech is supported by several of our findings. First, we found a partial intersection of the sensory information, where significant word classification performance overlapped in bilateral postcentral regions, inferior temporal and frontal regions and the SMG. On the other hand, auditory and visual cortices represent strongly modality-specific word identities. Second, it is also supported by the observation that anterior angular and inferior frontal regions facilitate both auditory and visual comprehension, while distinct regions support modality-specific comprehension. In particular, our data suggest that middle occipital and posterior angular representations specifically drive auditory comprehension. In addition, superior frontal and temporal regions are engaged in lip reading, although we did not find strong evidence for a modality preference of these regions. None of these comprehension-relevant regions was strictly lateralised, in line with the notion that speech comprehension is largely a bilateral process (<xref ref-type="bibr" rid="bib54">Kennedy-Higgins et al., 2020</xref>).</p><p>The inability to cross-classify auditory and visual speech from local activity further supports the conclusion that the nature of local representations of acoustic and visual speech is relatively distinct. It is important to note that cross-classification probes not only the spatial overlap of two representations but also asks whether the local spatio-temporal activity patterns encoding word identity in the two sensory modalities are the same. It could be that local activity encodes a given word based on acoustic or visual evidence but using distinct activity patterns. Representations could therefore spatially overlap without using the same ‘neural code’. Our results hence provide evidence that the activity patterns by which auditory and visual speech are encoded may be partly distinct, even within regions that represent both acoustically and visually mediated word information, such as the inferior frontal and anterior angular gyrus.</p><p>While we found strongest word classification performance in sensory areas, significant classification also extended into central, frontal and parietal regions. This suggests that the stimulus-domain classifier used here may also capture processes potentially related to attention or motor preparation. While we cannot rule out that levels of attention differed between conditions, we ensured by experimental design that comprehension performance did not differ between modalities. In addition, the relevant target words were placed not at the end of the sentence to prevent motor planning and preparation during their presentation (see Stimuli).</p></sec><sec id="s3-2"><title>The encoding of visual speech</title><p>The segregation of comprehension-relevant auditory and visual representations provides a possible explanation for the finding that auditory or verbal skills and visual lip reading are uncorrelated in normal-hearing adults (<xref ref-type="bibr" rid="bib47">Jeffers and Barley, 1980</xref>; <xref ref-type="bibr" rid="bib66">Mohammed et al., 2006</xref>; <xref ref-type="bibr" rid="bib101">Summerfield, 1992</xref>). Indeed, it has been suggested that individual differences in lip reading represent something other than normal variation in speech perceptual abilities (<xref ref-type="bibr" rid="bib101">Summerfield, 1992</xref>). For example, lip reading skills are unrelated to reading abilities in the typical adult population (<xref ref-type="bibr" rid="bib1">Arnold and Köpsel, 1996</xref>; <xref ref-type="bibr" rid="bib66">Mohammed et al., 2006</xref>), although a relationship is sometimes found in deaf or dyslexic children (<xref ref-type="bibr" rid="bib1">Arnold and Köpsel, 1996</xref>; <xref ref-type="bibr" rid="bib23">de Gelder and Vroomen, 1998</xref>; <xref ref-type="bibr" rid="bib57">Kyle et al., 2016</xref>).</p><p>Previous imaging studies suggested that silent lip reading engages similar auditory regions as engaged by acoustic speech (<xref ref-type="bibr" rid="bib8">Bourguignon et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Calvert, 1997</xref>; <xref ref-type="bibr" rid="bib14">Calvert and Campbell, 2003</xref>; <xref ref-type="bibr" rid="bib16">Capek et al., 2008</xref>; <xref ref-type="bibr" rid="bib62">MacSweeney et al., 2000</xref>; <xref ref-type="bibr" rid="bib76">Paulesu et al., 2003</xref>; <xref ref-type="bibr" rid="bib78">Pekkola et al., 2005</xref>), implying a direct route for visual speech into the auditory pathways and an overlap of acoustic and visual speech representations in these regions (<xref ref-type="bibr" rid="bib5">Bernstein and Liebenthal, 2014</xref>). Studies comparing semantic representations from different modalities also supported large modality-independent networks (<xref ref-type="bibr" rid="bib28">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="bib96">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="bib97">Simanova et al., 2014</xref>). Yet, most studies have focused on mapping activation strength rather than the encoding of <italic>word identity</italic> by cerebral speech representations. Hence, it could be that visual speech may activate many regions in an unspecific manner, without engaging specific semantic or lexical representations, maybe as a result of attentional engagement or feed-back (<xref ref-type="bibr" rid="bib2">Balk et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Ozker et al., 2018</xref>). Support for this interpretation comes from lip reading studies showing that auditory cortical areas are equally activated by visual words and pseudo-words (<xref ref-type="bibr" rid="bib13">Calvert, 1997</xref>; <xref ref-type="bibr" rid="bib76">Paulesu et al., 2003</xref>), and studies demonstrating cross-modal activations in early sensory regions also for simplistic stimuli (<xref ref-type="bibr" rid="bib30">Ferraro et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Ibrahim et al., 2016</xref>; <xref ref-type="bibr" rid="bib80">Petro et al., 2017</xref>).</p><p>Our results suggest that visual speech comprehension is mediated by parietal and inferior frontal regions that likely contribute to both auditory and visual speech comprehension, but also engage superior temporal and superior frontal regions. Thereby our results support a route of visual speech into auditory cortical and temporal regions but provide no evidence for an overlap of speech representations in the temporal lobe that would facilitate both lip-reading and acoustic speech comprehension, in contrast to recent suggestions from a lesion-based approach (<xref ref-type="bibr" rid="bib43">Hickok et al., 2018</xref>).</p><p>Two specific regions mediating lip-reading comprehension were the IFG and the anterior angular gyrus. Our results suggest that these facilitate both auditory and visual speech comprehension, in line with previous suggestions (<xref ref-type="bibr" rid="bib97">Simanova et al., 2014</xref>). Previous work has also implicated these regions in the visual facilitation of auditory speech-in-noise perception (<xref ref-type="bibr" rid="bib32">Giordano et al., 2017</xref>) and lip-reading itself (<xref ref-type="bibr" rid="bib8">Bourguignon et al., 2020</xref>). Behavioural studies have shown that lip-reading drives the improvement of speech perception in noise (<xref ref-type="bibr" rid="bib61">Macleod and Summerfield, 1987</xref>), hence suggesting that the representations of visual speech in these regions may be central for hearing in noisy environments. Interestingly, these regions resemble the left-lateralised dorsal pathway activated in deaf signers when seeing signed verbs (<xref ref-type="bibr" rid="bib25">Emmorey et al., 2011</xref>). Our results cannot directly address whether these auditory and visual speech representations are the same as those that mediate the multisensory facilitation of speech comprehension in adverse environments (<xref ref-type="bibr" rid="bib7">Bishop and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib32">Giordano et al., 2017</xref>). Future work needs to directly contrast the degree of which multisensory speech representations overlap locally to the ability of these regions to directly fuse this information.</p></sec><sec id="s3-3"><title>Cross-modal activations in visual cortex</title><p>We also found that acoustic comprehension was related to occipital brain activity (c.f. <xref ref-type="fig" rid="fig3">Figure 3</xref>). Previous work has shown that salient sounds activate visual cortices (<xref ref-type="bibr" rid="bib29">Feng et al., 2014</xref>; <xref ref-type="bibr" rid="bib65">McDonald et al., 2013</xref>), with top-down projections providing visual regions with semantic information, for example about object categories (<xref ref-type="bibr" rid="bib80">Petro et al., 2017</xref>; <xref ref-type="bibr" rid="bib87">Revina et al., 2018</xref>). The acoustic speech in the present study was presented in noise, and performing the task hence required attentional effort. Attention may therefore have automatically facilitated the entrance of top-down semantic information into occipital regions that, in a multisensory context, would encode the lip-movement trajectory, in order to maximise task performance (<xref ref-type="bibr" rid="bib65">McDonald et al., 2013</xref>). The lack of significant cross-classification performance suggests that the nature of this top-down induced representation differs from that induced by direct lip-movement information.</p></sec><sec id="s3-4"><title>Sub-optimal sensory representations contribute critically to behaviour</title><p>To understand which cerebral representations of sensory information guide behaviour, it is important to dissociate those that mainly correlate with the stimulus from those that encode sensory information and guide behavioural choice. At the single neuron level some studies have proposed that only those neurons encoding the specific stimulus optimally are driving behaviour (<xref ref-type="bibr" rid="bib11">Britten et al., 1996</xref>; <xref ref-type="bibr" rid="bib82">Pitkow et al., 2015</xref>; <xref ref-type="bibr" rid="bib85">Purushothaman and Bradley, 2005</xref>; <xref ref-type="bibr" rid="bib103">Tsunada et al., 2016</xref>), while others suggest that ‘plain’ sensory information and sensory information predictive of choice can be decoupled across neurons (<xref ref-type="bibr" rid="bib93">Runyan et al., 2017</xref>). Theoretically, these different types of neural representations can be dissected by considering the intersection of brain activity predictive of stimulus and choice (<xref ref-type="bibr" rid="bib73">Panzeri et al., 2017</xref>), that is, the neural representations that are informative about the sensory environment and are used to guide behaviour. While theoretically attractive, this intersection is difficult to quantify for high-dimensional data, in part as direct estimates of this intersection, for example based on information-theoretic approaches, are computationally costly (<xref ref-type="bibr" rid="bib81">Pica et al., 2017</xref>). Hence, in the past most studies, also on speech, have focused on either studying sensory encoding (e.g. by classifying stimuli), or behaviourally predictive activity only (e.g. by classifying responses). However, the former type of cerebral representation may not guide behaviour at all, while the latter may also capture brain activity that drives perceptual errors due to intrinsic fluctuations in sensory pathways, the decision process, or even noise in the motor system (<xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>).</p><p>To directly quantify where auditory or visual speech is represented and this representation is used to guide comprehension we capitalised on the use of a stimulus-classifier to first pinpoint brain activity carrying relevant word-level information and to then test where the quality of the single-trial word representation is predictive of participants’ comprehension (<xref ref-type="bibr" rid="bib18">Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib89">Ritchie et al., 2015</xref>). This approach directly follows the idea to capture processes related to the encoding of external (stimulus-driven) information and to then ask whether these representations correlate over trials with the behavioural outcome or report. Although one has to be careful in interpreting this as causally driving behaviour, our results reveal that brain regions allowing for a sub-optimal read-out of the actual stimulus are predictive of the perceptual outcome, whereas those areas allowing the best read-out not necessarily predict behaviour. This dissociation is emerging in several recent studies on the neural basis underlying perception (<xref ref-type="bibr" rid="bib9">Bouton et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Hasson et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>). Importantly, it suggests that networks mediating speech comprehension can neither be understood by mapping speech representations during passive perception nor during task performance, if the analysis itself is not geared towards directly revealing the perception-relevant representations.</p><p>On a technical level, it is important to keep in mind that the insights derived from any classification analysis are limited by the quality of the overall classification performance. Classification performance was highly significant and reached about 10% above the respective chance level, a number that is in accordance with other neuroimaging studies on auditory pathways (<xref ref-type="bibr" rid="bib3">Bednar et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Correia et al., 2015</xref>). Yet, more refined classification techniques, or data obtained using significantly larger stimulus sets and more repetitions of individual target words may be able to provide even more refined insights. In addition, by design of our experiment (four response options) and data analysis, the neurobehavioral analysis was primary driven by trials in which the respective brain activity encoded the sensory stimulus correctly. We cannot specifically link the incorrect encoding of a stimulus with behaviour. This is in contrast to studies using only two stimulus or response options, where evidence for one option directly provides evidence against the other (<xref ref-type="bibr" rid="bib31">Frühholz et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Petro et al., 2013</xref>).</p><p>One factor that may shape the behavioural relevance of local sensory representations is the specific task imposed (<xref ref-type="bibr" rid="bib44">Hickok and Poeppel, 2007</xref>). In studies showing the perceptual relevance of optimally encoding neurons, the tasks were mostly dependent on low-level features (<xref ref-type="bibr" rid="bib82">Pitkow et al., 2015</xref>; <xref ref-type="bibr" rid="bib103">Tsunada et al., 2016</xref>), while studies pointing to a behavioural relevance of high level regions were relying on high-level information such as semantics or visual object categories (<xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>). One prediction from our results is therefore that if the nature of the task was changed from speech comprehension to an acoustic task, the perceptual relevance of word representations would shift from left anterior regions to strongly word encoding regions in the temporal and supramarginal regions. Similarly, if the task would concern detecting basic kinematic features of the visual lip trajectory, activity within early visual cortices tracking the stimulus dynamics should be more predictive of behavioural performance (<xref ref-type="bibr" rid="bib24">Di Russo et al., 2007</xref>; <xref ref-type="bibr" rid="bib53">Keitel et al., 2019</xref>; <xref ref-type="bibr" rid="bib102">Tabarelli et al., 2020</xref>). This suggests that a discussion of the relevant networks underlying speech perception should always be task-focused.</p></sec><sec id="s3-5"><title>Conclusion</title><p>These results suggest that cerebral representations of acoustic and visual speech might be more modality-specific than often assumed and provide a neural explanation for why acoustic speech comprehension is a poor predictor of lip-reading skills. Our results also suggest that those cerebral speech representations that directly drive comprehension are largely distinct from those best representing the physical stimulus, strengthening the notion that neuroimaging studies need to more specifically quantify the cerebral mechanisms driving single-trial behaviour.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Part of the dataset analysed in the present study has been used in a previous publication (<xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>). The data analysis performed here is entirely different from the previous work and includes unpublished data.</p><sec id="s4-1"><title>Participants and data acquisition</title><p>Twenty healthy, native volunteers participated in this study (nine female, age 23.6 ± 5.8 y [<italic>M</italic> ± <italic>SD</italic>]). The sample size was set based on previous recommendations (<xref ref-type="bibr" rid="bib6">Bieniek et al., 2016</xref>; <xref ref-type="bibr" rid="bib84">Poldrack et al., 2017</xref>; <xref ref-type="bibr" rid="bib98">Simmons et al., 2011</xref>). MEG data of two participants had to be excluded due to excessive artefacts. Analysis of MEG data therefore included 18 participants (seven female), whereas the analysis of behavioural data included 20 participants. An exception to this is the neurobehavioral analysis in the visual condition, where three participants performed at ceiling and had to be excluded (resulting in n = 15 participants in <xref ref-type="fig" rid="fig3">Figure 3B</xref>). All participants were right-handed (Edinburgh Handedness Inventory; <xref ref-type="bibr" rid="bib70">Oldfield, 1971</xref>), had normal hearing (Quick Hearing Check; <xref ref-type="bibr" rid="bib55">Koike et al., 1994</xref>), and normal or corrected-to-normal vision. Participants had no self-reported history of neurological or language disorders. All participants provided written informed consent prior to testing and received monetary compensation of £10/h. The experiment was approved by the ethics committee of the College of Science and Engineering, University of Glasgow (approval number 300140078), and conducted in compliance with the Declaration of Helsinki.</p><p>MEG was recorded with a 248-magnetometers, whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging) at a sampling rate of 1 KHz. Head positions were measured at the beginning and end of each run, using five coils placed on the participants’ head. Coil positions were co-digitised with the head-shape (FASTRAK, Polhemus Inc, VT, USA). Participants sat upright and fixated a fixation point projected centrally on a screen. Visual stimuli were displayed with a DLP projector at 25 frames/second, a resolution of 1280 × 720 pixels, and covered a visual field of 25 × 19 degrees. Sounds were transmitted binaurally through plastic earpieces and 370 cm long plastic tubes connected to a sound pressure transducer and were presented stereophonically at a sampling rate of 22,050 Hz. Stimulus presentation was controlled with Psychophysics toolbox (<xref ref-type="bibr" rid="bib10">Brainard, 1997</xref>) for MATLAB (The MathWorks, Inc) on a Linux PC.</p></sec><sec id="s4-2"><title>Stimuli</title><p>The experiment featured three conditions: auditory only (A), visual only (V), and a third condition in which the same stimulus material was presented audiovisually (AV). This condition could not be used for the main analyses as participants performed near ceiling level in the behavioural task (correct trials: <italic>M</italic> = 96.5%, SD = 3.4%; see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref> for results). The stimulus material consisted of 180 sentences, based on a set of 18 target words derived from two categories (nine numbers and nine adjectives), each repeated 10 times in a different sentence. Sentences were spoken by a trained, male, native British actor. Sentences were recorded with a high-performance camcorder (Sony PMW-EX1) and external microphone. The speaker was instructed to speak clearly and naturally. Each sentence had the same linguistic structure (<xref ref-type="bibr" rid="bib52">Keitel et al., 2018</xref>). An example is: ‘<italic>Did you notice</italic> (filler phase), <italic>on Sunday night</italic> (time phrase) <italic>Graham</italic> (name) <italic>offered</italic> (verb) <italic>ten</italic> (number) <italic>fantastic</italic> (adjective) <italic>books</italic> (noun)”. In total, 18 possible names, verbs, numbers, adjectives, and nouns were each repeated ten times. Sentence elements were re-combined within a set of 180 sentences. As a result, sentences made sense, but no element could be semantically predicted from the previous material. To measure comprehension performance, a target word was selected that was either the adjective in one set of sentences (‘fantastic’ in the above example) or the number in the other set (for example, ‘thirty-two’). These were always the second or third last word in a sentence, ensuring that the cerebral processes encoding these words were independent from the behavioural (motor) response. All adjective target words had a positive valence (<xref ref-type="bibr" rid="bib94">Scott et al., 2019</xref>; see table in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for all possible target words). The duration of sentences ranged from 4.2 s to 6.5 s (5.4 ± 0.4 s [<italic>M</italic> ± <italic>SD</italic>]). Noise/video onset and offset was approximately 1 s before and after the speech, resulting in stimulus lengths of 6.4 s to 8.2 s (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The durations of target words ranged from 419 ms to 1,038 ms (679 ± 120 ms [<italic>M</italic> ± <italic>SD</italic>]). After the offset of the target words, the stimulus continued for 1.48 s to 2.81 s (1.98 ± 0.31 s [<italic>M</italic> ± <italic>SD</italic>]) before the end of the sentence.</p><p>The acoustic speech was embedded in noise to match performance between auditory and visual conditions. The noise consisted of ecologically valid, environmental sounds (traffic, car horns, talking), combined into a uniform mixture of 50 different background noises. The individual noise level for each participant was determined with a one-up-three-down (3D1U) staircase procedure that targets the 79.4% probability correct level (<xref ref-type="bibr" rid="bib50">Karmali et al., 2016</xref>). For the staircase procedure, only the 18 possible target words (i.e. adjectives and numbers) were used instead of whole sentences. Participants were presented with a single target word embedded in noise and had to choose between two alternatives. Note that due to the necessary differences between staircase procedure (single words and two-alternative-forced-choice) and behavioural experiment (sentences and four-alternative forced-choice), the performance in the behavioural task was lower than 79.4%. The signal-to-noise ratio across participants ranged from −7.75 dB to −3.97 dB (−5.96 ± 1.06 dB [<italic>M</italic> ± <italic>SD</italic>]; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p></sec><sec id="s4-3"><title>Experimental design</title><p>The 180 sentences were each presented in three conditions (A, V, AV), each consisting of four blocks with 45 sentences each. In each block, participants either reported the comprehended adjective or number, resulting in two ‘adjective blocks’ and two ‘number blocks’. The order of sentences and blocks was randomised for each participant. The first trial of each block was a ‘dummy’ trial that was discarded for subsequent analysis; this trial was repeated at the end of the block.</p><p>During the presentation of the sentence, participants fixated either a dot (auditory condition) or a small cross on the speaker’s mouth (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for depiction of trial structure). After each sentence, participants were presented with four target words (either adjectives or written numbers) on the screen and had to indicate which one they perceived by pressing one of four buttons on a button box. After 2 s, the next trial started automatically. Each block lasted approximately 10 min. The two separate sessions were completed within one week.</p></sec><sec id="s4-4"><title>MEG pre-processing</title><p>Pre-processing of MEG data was carried out in MATLAB (The MathWorks, Inc) using the Fieldtrip toolbox (<xref ref-type="bibr" rid="bib71">Oostenveld et al., 2011</xref>). All experimental blocks were pre-processed separately. Single trials were extracted from continuous data starting 2 s before sound/video onset and until 10 s after onset. MEG data were denoised using a reference signal. Known faulty channels (<italic>N</italic> = 7) were removed before further pre-processing. Trials with SQUID jumps (on average 3.86% of trials) were detected and removed using Fieldtrip procedures with a cut-off <italic>z</italic>-value of 30. Before further artifact rejection, data were filtered between 0.2 and 150 Hz (fourth order Butterworth filters, forward and reverse) and down-sampled to 300 Hz. Data were visually inspected to find noisy channels (4.95 ± 5.74 on average across blocks and participants) and trials (0.60 ± 1.24 on average across blocks and participants). There was no indication for a statistical difference between the number of rejected channels or trials between conditions (two-sided <italic>t</italic>-tests; p&gt;0.48 for channels, p&gt;0.40 for trials). Finally, heart and eye movement artifacts were removed by performing an independent component analysis with 30 principal components (2.5 components removed on average). Data were further down-sampled to 150 Hz and bandpass-filtered between 0.8 and 30 Hz (fourth order Butterworth filters, forward and reverse).</p></sec><sec id="s4-5"><title>Source reconstruction</title><p>Source reconstruction was performed using Fieldtrip, SPM8, and the Freesurfer toolbox. We acquired T1-weighted structural magnetic resonance images (MRIs) for each participant. These were co-registered to the MEG coordinate system using a semi-automatic procedure (<xref ref-type="bibr" rid="bib37">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Keitel et al., 2017</xref>). MRIs were then segmented and linearly normalised to a template brain (MNI space). A forward solution was computed using a single-shell model (<xref ref-type="bibr" rid="bib69">Nolte, 2003</xref>). We projected sensor-level timeseries into source space using a frequency-specific linear constraint minimum variance (LCMV) beamformer (<xref ref-type="bibr" rid="bib107">Van Veen et al., 1997</xref>) with a regularisation parameter of 7% and optimal dipole orientation (singular value decomposition method). Covariance matrices for source were based on the whole length of trials (<xref ref-type="bibr" rid="bib12">Brookes et al., 2008</xref>). Grid points had a spacing of 6 mm, resulting in 12,337 points covering the whole brain. For subsequent analyses, we selected grid points that corresponded to cortical regions only (parcellated using the AAL atlas; <xref ref-type="bibr" rid="bib105">Tzourio-Mazoyer et al., 2002</xref>). This resulted in 6490 grid points in total.</p><p>Neural time series were spatially smoothed (<xref ref-type="bibr" rid="bib37">Gross et al., 2013</xref>) and normalised in source space. For this, the band-pass filtered time series for the whole trial (i.e. the whole sentence) were projected into source space and smoothed using SPM8 routines with a Full-Width Half Max (FWHM) value of 3 mm. The time series for each grid point and trial was then <italic>z</italic>-scored.</p></sec><sec id="s4-6"><title>Classification analysis</title><p>We used multi-variate single-trial classification to localise cerebral representations of the target words in source activity (<xref ref-type="bibr" rid="bib35">Grootswagers et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Guggenmos et al., 2018</xref>). Each target word was presented in ten different trials per condition. We extracted the 500 ms of activity following the onset of each target word and re-binned the source activity at 20 ms resolution. This choice of the analysis time window as made based on the typical duration of target words (<italic>M</italic> = 679 ms length, see <bold>Stimuli</bold>). Because the words following the target word differed in each sentence, choosing a longer window would have contaminated the specific classification of target word identity. We therefore settled on a 500 ms window, which has been shown to be sufficient for word decoding (<xref ref-type="bibr" rid="bib17">Chan et al., 2011</xref>) and does not include the beginning of the following word in most sentences (94%). Importantly, this analysis window did not capture post-sentence or repose periods. Classification was performed on spatial searchlights of 1.2 cm radius. The typical searchlight contained 31 neighbours (median value), with 95% of searchlights containing 17 to 33 grid points. The (leave-one-trial-out) classifier computed, for a given trial, the Pearson correlation of the spatio-temporal searchlight activity in this test-trial with the activities for the same word in all other trials (within-target distances), and with the activities of the three alternative words in all trials (between-target distances). That is, each trial was classified within the sub-set of words that was available to the participant as potential behavioural choices (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for illustration). We then averaged correlations within the four candidate words and decoded the target trial as the word identity with the strongest average correlation (that is, smallest classifier distance). This classification measure is comparable to previous studies probing how well speech can be discriminated based on patterns of dynamic brain activity (<xref ref-type="bibr" rid="bib60">Luo and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib88">Rimmele et al., 2015</xref>). Classification performance was averaged across blocks with numbers and adjectives as task-relevant words. For cross-condition classification (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), we classified the single-trial activity from the auditory (visual) condition against all trials with the same word alternatives from the other condition, or from the audiovisual condition.</p></sec><sec id="s4-7"><title>Selection of parameters and classifier procedures</title><p>We initially tested a number of different classifiers, including linear-discriminant and diagonal-linear classifiers, and then selected a correlation-based nearest-neighbour classifier as this performed slightly better than the others (although we note that the difference in peak classification performance was only on the range of 2–3% between different classifiers). We focussed on linear classifiers here because these have been shown to often perform equally well than more complex non-linear classifiers, while also offering insights that are more readily interpretable (<xref ref-type="bibr" rid="bib41">Haxby et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Kamitani and Tong, 2005</xref>; <xref ref-type="bibr" rid="bib90">Ritchie et al., 2019</xref>).</p><p>To assess the impact of the temporal binning of MEG activity, we probed classification performance based on bins of 3.3, 20, 40 and 60 ms length. Classification performance dropped slightly when sampling the data at a resolution lower than 20 ms, particularly for auditory classification (for 3.3, 20, 40 and 60 ms bins, the mean performance of the 10% grid points with the highest values in the benchmark 20 ms classification was: auditory, 27.19 ± 0.48%, 26.81 ± 0.86%, 26.54 ± 1.00% and 25.92 ± 0.85%; visual, 28.71 ± 1.55%, 28.68 ± 1.73%, 28.54 ± 1.68% and 27.85 ± 2.04% [M ± SD]).</p><p>We also probed the influence of the spatial searchlight by (i) including each neighbouring spatial grid point into the searchlight, or (ii) averaging across grid points, and (iii) by not including a searchlight altogether. Ignoring the spatial pattern by averaging grid points led to a small drop in classification performance (individual vs average grid points: auditory, 27.19 ± 0.48 vs 26.72 ± 0.67; visual, 28.71 ± 1.55 vs 27.71 ± 1.25 [M ± SD]). Performance also dropped slightly when no searchlight was included (auditory, 26.77 ± 1.93; visual, 27.86 ± 2.50 [M ± SD]).</p><p>For the main analysis, we therefore opted for a classifier based on the MEG source data represented as spatial searchlight including each grid point within a 1.2 cm radius, and binned at 20 ms resolution.</p></sec><sec id="s4-8"><title>Quantifying the behavioural relevance of speech representations</title><p>To quantify the degree to which the classifier evidence obtained from local speech representations in favour of a specific word identity is predictive of participants' comprehension, we extracted an index of how well the classifier separated the correct word identity from the three false alternatives: the distance of the single trial classifier evidence to a decision bound (<xref ref-type="bibr" rid="bib18">Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib89">Ritchie et al., 2015</xref>). This representational distance was defined as the average correlation with trials of the same (within-target distances) word identity minus the mean of the correlation with the three alternatives (between-target distances; see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). If a local cerebral representation allows a clear and robust classification of a specific word identity, this representational distance would be large, while if a representation allows only for poor classification, or mis-classifies a trial, this distance will be small or negative. We then quantified the statistical relation between participants performance (accuracy) and these single-trial representational distances (<xref ref-type="bibr" rid="bib18">Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Grootswagers et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Panzeri et al., 2017</xref>; <xref ref-type="bibr" rid="bib81">Pica et al., 2017</xref>; <xref ref-type="bibr" rid="bib89">Ritchie et al., 2015</xref>). This analysis was based on a regularised logistic regression (<xref ref-type="bibr" rid="bib75">Parra et al., 2005</xref>), which was computed across all trials per participant. To avoid biasing, the regression model was computed across randomly selected subsets of trials with equal numbers of correct and wrong responses, averaging betas across 50 randomly selected trials. The resulting <italic>beta</italic> values were averaged across blocks with numbers and adjectives as targets and were entered into a group-level analysis. Given the design of the task (four response options, around 70% correct performance), this analysis capitalises on the relation between correctly encoded words (positive representational distance) and their relation to performance. Conversely, it is not directly able to capture how a wrongly encoded word identity relates to performance.</p></sec><sec id="s4-9"><title>Quantifying the role of phonological and semantic features to perception</title><p>For each pair of words we computed their phonological distance using the Phonological Corpus Tools (V1.4.0) based on the phonetic string similarity (’phonological edit distance’) derived from the transcription tier, using the Irvine Phonotactic Online Dictionary (<xref ref-type="bibr" rid="bib106">Vaden et al., 2009</xref>). We also computed pairwise semantic distances using the fastTExt vector representation of English words trained on <ext-link ext-link-type="uri" xlink:href="https://commoncrawl.org/"><italic>Common Crawl</italic></ext-link> and <ext-link ext-link-type="uri" xlink:href="https://www.wikipedia.org/"><italic>Wikipedia</italic></ext-link> obtained online (file cc.en.300.vec) (<xref ref-type="bibr" rid="bib34">Grave et al., 2018</xref>). The individual word vectors (300 dimensions) were length-normalised and cosine distances were computed. For each participant, we obtained a behavioural representational dissimilarity matrix (RDM) as the pair-wise behavioural confusion matrix from their behavioural data. We then implemented a representational similarity analysis (RSA) (<xref ref-type="bibr" rid="bib56">Kriegeskorte et al., 2008</xref>) between phonological (semantic) representations and participants’ performance. Specifically, behavioural and semantic (phonetic) RDMs were compared using Spearman’s rank correlation. The resulting correlations were <italic>z</italic>-scored and averaged across adjectives and numbers (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s4-10"><title>Statistical analyses</title><p>To test the overall stimulus classification performance, we averaged the performance per grid point across participants and compared this group-averaged value to a group-average permutation distribution obtained from 3000 within-subject permutations derived with random trial labels. Cluster-based permutation was used to correct for multiple comparisons (<xref ref-type="bibr" rid="bib64">Maris and Oostenveld, 2007</xref>). Significant clusters were identified based on a first-level significance derived from the 99.95<sup>th</sup> percentile of the permuted distribution (family-wise error [FWE] of p=0.001), using the summed statistics (<italic>T</italic><sub>sum</sub>) across grid points within a cluster, and by requiring a minimal cluster size of 10 grid points. The resulting clusters were considered if they reached a <italic>p</italic>-value smaller than 0.05.</p><p>For the neuro-behavioural analyses, the regression <italic>betas</italic> obtained from the logistic regression were transformed into group-level <italic>t</italic>-values. These were compared with a surrogate distribution of <italic>t</italic>-values obtained from 3000 within-subject permutations using shuffled trial labels and using cluster-based permutations as above. The first-level significance threshold (at p&lt;0.05) was determined per condition based on the included sample size (<italic>t</italic>-value of <italic>t</italic> = 2.1 for the 18 participants in the auditory condition and <italic>t</italic> = 2.2 for 15 participants in the visual condition), and the resulting clusters were considered significant if they reached a p-value smaller than 0.05.</p><p>Resulting clusters were tested for lateralisation (<xref ref-type="bibr" rid="bib59">Liégeois et al., 2002</xref>; <xref ref-type="bibr" rid="bib74">Park and Kayser, 2019</xref>). For this, we extracted the participant-specific classification performance (or regression <italic>betas</italic>, respectively) for each cluster and for the corresponding contralateral grid points. These values were averaged within each hemisphere and the between-hemispheric difference was computed using a group-level, two-sided <italic>t</italic>-test. Resulting <italic>p</italic>-values were corrected for multiple comparisons by controlling the FDR at p≤0.05 (<xref ref-type="bibr" rid="bib4">Benjamini and Hochberg, 1995</xref>). We only use the term ‘lateralised’ if the between-hemispheric difference is statistically significant.</p><p>To determine whether individual local effects (e.g. stimulus classification or behavioural prediction) were specific to either condition, we implemented a direct contrast between conditions. For each grid point, we computed a group-level <italic>t</italic>-test. We then subjected these to the same full-brain cluster-based permutation approach as described above. In addition, we converted the group-level <italic>t</italic>-values to a JZS Bayes factor using a default scale factor of 0.707 (<xref ref-type="bibr" rid="bib92">Rouder et al., 2009</xref>). We then quantified the number of grid points per region of interest that exhibited a specific level of evidence in favour of the null hypothesis of no effect versus the alternative hypothesis (H<sub>0</sub> vs H<sub>1</sub>) (<xref ref-type="bibr" rid="bib48">Jeffreys, 1998</xref>). Using previous conventions (<xref ref-type="bibr" rid="bib108">Wagenmakers et al., 2011</xref>), the Bayes factors were interpreted as showing evidence for H<sub>1</sub> if they exceeded a value of 3, and evidence for H<sub>0</sub> if they were below ⅓, with the intermediate range yielding inconclusive results. We also calculated Bayes factors from Pearson correlation coefficients (for a control analysis between classification performance and behavioural data), using the same conventions (<xref ref-type="bibr" rid="bib109">Wetzels and Wagenmakers, 2012</xref>).</p><p>To investigate the relationship between stimulus classification and neurobehavioral results, we performed a robust linear regression within each participant for all grid points. The participant-specific <italic>beta</italic> values were then tested against zero using a two-sided <italic>t</italic>-test (<xref ref-type="bibr" rid="bib51">Keitel et al., 2017</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the UK Biotechnology and Biological Sciences Research Council (BBSRC, BB/L027534/1). CK is supported by the European Research Council (ERC-2014-CoG; grant No 646657); JG by the Wellcome Trust (Joint Senior Investigator Grant, No 098433), DFG (GR 2024/5-1) and IZKF (Gro3/001/19). The authors declare no competing financial interests. We are grateful to Lea-Maria Schmitt for guiding the semantic distance analysis.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: All participants provided written informed consent prior to testing and received monetary compensation of £10/h. The experiment was approved by the ethics committee of the College of Science and Engineering, University of Glasgow (approval number 300140078), and conducted in compliance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Target words used in this study (9 adjectives and nine numbers, each presented in 10 different sentences).</title><p>Note that adjectives were comparable with regard to their positive valence (<xref ref-type="bibr" rid="bib94">Scott et al., 2019</xref>).</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-56972-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-56972-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All relevant data and stimuli lists have been deposited to Dryad, under the <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.zkh18937w">https://doi.org/10.5061/dryad.zkh18937w</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Shared and modality-specific brain regions that mediate auditory and visual word comprehension</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.zkh18937w</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Data from: Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.1qq7050</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname> <given-names>P</given-names></name><name><surname>Köpsel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Lipreading, reading and memory of hearing and hearing-impaired children</article-title><source>Scandinavian Audiology</source><volume>25</volume><fpage>13</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.3109/01050399609047550</pub-id><pub-id pub-id-type="pmid">8658020</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balk</surname> <given-names>MH</given-names></name><name><surname>Kari</surname> <given-names>H</given-names></name><name><surname>Kauramäki</surname> <given-names>J</given-names></name><name><surname>Ahveninen</surname> <given-names>J</given-names></name><name><surname>Sams</surname> <given-names>M</given-names></name><name><surname>Autti</surname> <given-names>T</given-names></name><name><surname>Jääskeläinen</surname> <given-names>IP</given-names></name><collab>Brain and Mind Laboratory, Department of Biomedical Engineering and Computational Science (BECS), Aalto University School of Science, Espoo, Finland</collab></person-group><year iso-8601-date="2013">2013</year><article-title>Silent lipreading and covert speech production suppress processing of non-linguistic sounds in auditory cortex</article-title><source>Open Journal of Neuroscinece</source><volume>3</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.13055/ojns_3_1_1.130206</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bednar</surname> <given-names>A</given-names></name><name><surname>Boland</surname> <given-names>FM</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Different spatio-temporal electroencephalography features drive the successful decoding of binaural and monaural cues for sound localization</article-title><source>European Journal of Neuroscience</source><volume>45</volume><fpage>679</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.1111/ejn.13524</pub-id><pub-id pub-id-type="pmid">28102912</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.2307/2346101</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>LE</given-names></name><name><surname>Liebenthal</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural pathways for visual speech perception</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>386</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00386</pub-id><pub-id pub-id-type="pmid">25520611</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bieniek</surname> <given-names>MM</given-names></name><name><surname>Bennett</surname> <given-names>PJ</given-names></name><name><surname>Sekuler</surname> <given-names>AB</given-names></name><name><surname>Rousselet</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A robust and representative lower bound on object processing speed in humans</article-title><source>European Journal of Neuroscience</source><volume>44</volume><fpage>1804</fpage><lpage>1814</lpage><pub-id pub-id-type="doi">10.1111/ejn.13100</pub-id><pub-id pub-id-type="pmid">26469359</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CW</given-names></name><name><surname>Miller</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A multisensory cortical network for understanding speech in noise</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1790</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21118</pub-id><pub-id pub-id-type="pmid">18823249</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname> <given-names>M</given-names></name><name><surname>Baart</surname> <given-names>M</given-names></name><name><surname>Kapnoula</surname> <given-names>EC</given-names></name><name><surname>Molinaro</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Lip-Reading enables the brain to synthesize auditory features of unknown silent speech</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>1053</fpage><lpage>1065</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1101-19.2019</pub-id><pub-id pub-id-type="pmid">31889007</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouton</surname> <given-names>S</given-names></name><name><surname>Chambon</surname> <given-names>V</given-names></name><name><surname>Tyrand</surname> <given-names>R</given-names></name><name><surname>Guggisberg</surname> <given-names>AG</given-names></name><name><surname>Seeck</surname> <given-names>M</given-names></name><name><surname>Karkar</surname> <given-names>S</given-names></name><name><surname>van de Ville</surname> <given-names>D</given-names></name><name><surname>Giraud</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Focal versus distributed temporal cortex activity for speech sound category assignment</article-title><source>PNAS</source><volume>115</volume><fpage>E1299</fpage><lpage>E1308</lpage><pub-id pub-id-type="doi">10.1073/pnas.1714279115</pub-id><pub-id pub-id-type="pmid">29363598</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname> <given-names>KH</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Celebrini</surname> <given-names>S</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A relationship between behavioral choice and the visual responses of neurons in macaque MT</article-title><source>Visual Neuroscience</source><volume>13</volume><fpage>87</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1017/S095252380000715X</pub-id><pub-id pub-id-type="pmid">8730992</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brookes</surname> <given-names>MJ</given-names></name><name><surname>Vrba</surname> <given-names>J</given-names></name><name><surname>Robinson</surname> <given-names>SE</given-names></name><name><surname>Stevenson</surname> <given-names>CM</given-names></name><name><surname>Peters</surname> <given-names>AM</given-names></name><name><surname>Barnes</surname> <given-names>GR</given-names></name><name><surname>Hillebrand</surname> <given-names>A</given-names></name><name><surname>Morris</surname> <given-names>PG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Optimising experimental design for MEG beamformer imaging</article-title><source>NeuroImage</source><volume>39</volume><fpage>1788</fpage><lpage>1802</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.050</pub-id><pub-id pub-id-type="pmid">18155612</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Activation of auditory cortex during silent lipreading</article-title><source>Science</source><volume>276</volume><fpage>593</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1126/science.276.5312.593</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname> <given-names>GA</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Reading speech from still and moving faces: the neural substrates of visible speech</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>57</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1162/089892903321107828</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The processing of audio-visual speech: empirical and neural bases</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>363</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2155</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Capek</surname> <given-names>CM</given-names></name><name><surname>Macsweeney</surname> <given-names>M</given-names></name><name><surname>Woll</surname> <given-names>B</given-names></name><name><surname>Waters</surname> <given-names>D</given-names></name><name><surname>McGuire</surname> <given-names>PK</given-names></name><name><surname>David</surname> <given-names>AS</given-names></name><name><surname>Brammer</surname> <given-names>MJ</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical circuits for silent speechreading in deaf and hearing people</article-title><source>Neuropsychologia</source><volume>46</volume><fpage>1233</fpage><lpage>1241</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.11.026</pub-id><pub-id pub-id-type="pmid">18249420</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname> <given-names>AM</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Decoding word and category-specific spatiotemporal representations from MEG and EEG</article-title><source>NeuroImage</source><volume>54</volume><fpage>3028</fpage><lpage>3039</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.10.073</pub-id><pub-id pub-id-type="pmid">21040796</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Jozwik</surname> <given-names>KM</given-names></name><name><surname>van den Bosch</surname> <given-names>JJF</given-names></name><name><surname>Charest</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural dynamics of real-world object vision that guide behaviour</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/147298</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conrad</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Lip-reading by deaf and hearing children</article-title><source>British Journal of Educational Psychology</source><volume>47</volume><fpage>60</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8279.1977.tb03001.x</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Correia</surname> <given-names>JM</given-names></name><name><surname>Jansma</surname> <given-names>B</given-names></name><name><surname>Hausfeld</surname> <given-names>L</given-names></name><name><surname>Kikkert</surname> <given-names>S</given-names></name><name><surname>Bonte</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>EEG decoding of spoken words in bilingual listeners: from words to language invariant semantic-conceptual representations</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>71</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00071</pub-id><pub-id pub-id-type="pmid">25705197</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crochet</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>SH</given-names></name><name><surname>Petersen</surname> <given-names>CCH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural circuits for Goal-Directed sensorimotor transformations</article-title><source>Trends in Neurosciences</source><volume>42</volume><fpage>66</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.08.011</pub-id><pub-id pub-id-type="pmid">30201180</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Crosse</surname> <given-names>MJ</given-names></name><name><surname>ElShafei</surname> <given-names>HA</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Investigating the temporal dynamics of auditory cortical activation to silent lipreading</article-title><conf-name>Paper Presented at the 2015 7th International IEEE/EMBS Conference on Neural Engineering</conf-name><pub-id pub-id-type="doi">10.1109/NER.2015.7146621</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gelder</surname> <given-names>B</given-names></name><name><surname>Vroomen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Impaired speech perception in poor readers: evidence from hearing and speech reading</article-title><source>Brain and Language</source><volume>64</volume><fpage>269</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1006/brln.1998.1973</pub-id><pub-id pub-id-type="pmid">9743542</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Russo</surname> <given-names>F</given-names></name><name><surname>Pitzalis</surname> <given-names>S</given-names></name><name><surname>Aprile</surname> <given-names>T</given-names></name><name><surname>Spitoni</surname> <given-names>G</given-names></name><name><surname>Patria</surname> <given-names>F</given-names></name><name><surname>Stella</surname> <given-names>A</given-names></name><name><surname>Spinelli</surname> <given-names>D</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatiotemporal analysis of the cortical sources of the steady-state visual evoked potential</article-title><source>Human Brain Mapping</source><volume>28</volume><fpage>323</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1002/hbm.20276</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emmorey</surname> <given-names>K</given-names></name><name><surname>McCullough</surname> <given-names>S</given-names></name><name><surname>Mehta</surname> <given-names>S</given-names></name><name><surname>Ponto</surname> <given-names>LL</given-names></name><name><surname>Grabowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sign language and pantomime production differentially engage frontal and parietal cortices</article-title><source>Language and Cognitive Processes</source><volume>26</volume><fpage>878</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1080/01690965.2010.492643</pub-id><pub-id pub-id-type="pmid">21909174</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Gutierrez-Sigut</surname> <given-names>E</given-names></name><name><surname>MacSweeney</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence for shared conceptual representations for sign and speech</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/623645</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical organization of auditory and motor representations in speech perception: evidence from searchlight similarity analysis</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4772</fpage><lpage>4788</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv136</pub-id><pub-id pub-id-type="pmid">26157026</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname> <given-names>SL</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain regions that represent amodal conceptual knowledge</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>10552</fpage><lpage>10558</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0051-13.2013</pub-id><pub-id pub-id-type="pmid">23785167</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname> <given-names>W</given-names></name><name><surname>Störmer</surname> <given-names>VS</given-names></name><name><surname>Martinez</surname> <given-names>A</given-names></name><name><surname>McDonald</surname> <given-names>JJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sounds activate visual cortex and improve visual discrimination</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>9817</fpage><lpage>9824</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4869-13.2014</pub-id><pub-id pub-id-type="pmid">25031419</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferraro</surname> <given-names>S</given-names></name><name><surname>Van Ackeren</surname> <given-names>MJ</given-names></name><name><surname>Mai</surname> <given-names>R</given-names></name><name><surname>Tassi</surname> <given-names>L</given-names></name><name><surname>Cardinale</surname> <given-names>F</given-names></name><name><surname>Nigri</surname> <given-names>A</given-names></name><name><surname>Bruzzone</surname> <given-names>MG</given-names></name><name><surname>D'Incerti</surname> <given-names>L</given-names></name><name><surname>Hartmann</surname> <given-names>T</given-names></name><name><surname>Weisz</surname> <given-names>N</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stereotactic electroencephalography in humans reveals multisensory signal in early visual and auditory cortices</article-title><source>Cortex</source><volume>126</volume><fpage>253</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.12.032</pub-id><pub-id pub-id-type="pmid">32092494</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname> <given-names>S</given-names></name><name><surname>van der Zwaag</surname> <given-names>W</given-names></name><name><surname>Saenz</surname> <given-names>M</given-names></name><name><surname>Belin</surname> <given-names>P</given-names></name><name><surname>Schobert</surname> <given-names>AK</given-names></name><name><surname>Vuilleumier</surname> <given-names>P</given-names></name><name><surname>Grandjean</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural decoding of discriminative auditory object features depends on their socio-affective valence</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>11</volume><fpage>1638</fpage><lpage>1649</lpage><pub-id pub-id-type="doi">10.1093/scan/nsw066</pub-id><pub-id pub-id-type="pmid">27217117</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname> <given-names>BL</given-names></name><name><surname>Ince</surname> <given-names>RAA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title><source>eLife</source><volume>6</volume><elocation-id>e24763</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.24763</pub-id><pub-id pub-id-type="pmid">28590903</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Grave</surname> <given-names>E</given-names></name><name><surname>Bojanowski</surname> <given-names>P</given-names></name><name><surname>Gupta</surname> <given-names>P</given-names></name><name><surname>Joulin</surname> <given-names>A</given-names></name><name><surname>Mikolov</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning word vectors for 157 languages</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.06893">https://arxiv.org/abs/1802.06893</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoding dynamic brain patterns from evoked responses: a tutorial on multivariate pattern analysis applied to time series neuroimaging data</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>677</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id><pub-id pub-id-type="pmid">27779910</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Finding decodable information that can be read out in behaviour</article-title><source>NeuroImage</source><volume>179</volume><fpage>252</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.022</pub-id><pub-id pub-id-type="pmid">29886145</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Hoogenboom</surname> <given-names>N</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Schyns</surname> <given-names>P</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Belin</surname> <given-names>P</given-names></name><name><surname>Garrod</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001752</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id><pub-id pub-id-type="pmid">24391472</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname> <given-names>M</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multivariate pattern analysis for MEG: a comparison of dissimilarity measures</article-title><source>NeuroImage</source><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.02.044</pub-id><pub-id pub-id-type="pmid">29499313</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>DA</given-names></name><name><surname>Fussell</surname> <given-names>C</given-names></name><name><surname>Summerfield</surname> <given-names>AQ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reading fluent speech from talking faces: typical brain networks and individual differences</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>939</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1162/0898929054021175</pub-id><pub-id pub-id-type="pmid">15969911</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Skipper</surname> <given-names>JI</given-names></name><name><surname>Nusbaum</surname> <given-names>HC</given-names></name><name><surname>Small</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Abstract coding of audiovisual speech: beyond sensory representation</article-title><source>Neuron</source><volume>56</volume><fpage>1116</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.037</pub-id><pub-id pub-id-type="pmid">18093531</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding neural representational spaces using multivariate pattern analysis</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>435</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170325</pub-id><pub-id pub-id-type="pmid">25002277</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cortical organization of speech processing: Feedback control and predictive coding the context of a dual-stream model</article-title><source>Journal of Communication Disorders</source><volume>45</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1016/j.jcomdis.2012.06.004</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Rogalsky</surname> <given-names>C</given-names></name><name><surname>Matchin</surname> <given-names>W</given-names></name><name><surname>Basilakos</surname> <given-names>A</given-names></name><name><surname>Cai</surname> <given-names>J</given-names></name><name><surname>Pillay</surname> <given-names>S</given-names></name><name><surname>Ferrill</surname> <given-names>M</given-names></name><name><surname>Mickelsen</surname> <given-names>S</given-names></name><name><surname>Anderson</surname> <given-names>SW</given-names></name><name><surname>Love</surname> <given-names>T</given-names></name><name><surname>Binder</surname> <given-names>J</given-names></name><name><surname>Fridriksson</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural networks supporting audiovisual integration for speech: A large-scale lesion study</article-title><source>Cortex</source><volume>103</volume><fpage>360</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.03.030</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>de Heer</surname> <given-names>WA</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><volume>532</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname> <given-names>LA</given-names></name><name><surname>Mesik</surname> <given-names>L</given-names></name><name><surname>Ji</surname> <given-names>XY</given-names></name><name><surname>Fang</surname> <given-names>Q</given-names></name><name><surname>Li</surname> <given-names>HF</given-names></name><name><surname>Li</surname> <given-names>YT</given-names></name><name><surname>Zingg</surname> <given-names>B</given-names></name><name><surname>Zhang</surname> <given-names>LI</given-names></name><name><surname>Tao</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cross-Modality sharpening of visual cortical processing through Layer-1-Mediated inhibition and disinhibition</article-title><source>Neuron</source><volume>89</volume><fpage>1031</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.027</pub-id><pub-id pub-id-type="pmid">26898778</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffers</surname> <given-names>J</given-names></name><name><surname>Barley</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1980">1980</year><source>Speechreading (Lipreading): Charles C</source><publisher-name>Thomas Publisher</publisher-name></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffreys</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>The Theory of Probability</source><publisher-name>OUP Oxford Press</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname> <given-names>Y</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Decoding the visual and subjective contents of the human brain</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>679</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1038/nn1444</pub-id><pub-id pub-id-type="pmid">15852014</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karmali</surname> <given-names>F</given-names></name><name><surname>Chaudhuri</surname> <given-names>SE</given-names></name><name><surname>Yi</surname> <given-names>Y</given-names></name><name><surname>Merfeld</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Determining thresholds using adaptive procedures and psychometric fits: evaluating efficiency using theory, simulations, and human experiments</article-title><source>Experimental Brain Research</source><volume>234</volume><fpage>773</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1007/s00221-015-4501-8</pub-id><pub-id pub-id-type="pmid">26645306</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Ince</surname> <given-names>RAA</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</article-title><source>NeuroImage</source><volume>147</volume><fpage>32</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.062</pub-id><pub-id pub-id-type="pmid">27903440</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id><pub-id pub-id-type="pmid">29529019</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname> <given-names>C</given-names></name><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Benwell</surname> <given-names>CSY</given-names></name><name><surname>Daube</surname> <given-names>C</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stimulus-Driven brain rhythms within the alpha band: the Attentional-Modulation conundrum</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3119</fpage><lpage>3129</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1633-18.2019</pub-id><pub-id pub-id-type="pmid">30770401</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy-Higgins</surname> <given-names>D</given-names></name><name><surname>Devlin</surname> <given-names>JT</given-names></name><name><surname>Nuttall</surname> <given-names>HE</given-names></name><name><surname>Adank</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The causal role of left and right superior temporal gyri in speech perception in noise: a transcranial magnetic stimulation study</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>1092</fpage><lpage>1103</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01521</pub-id><pub-id pub-id-type="pmid">31933438</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koike</surname> <given-names>KJ</given-names></name><name><surname>Hurst</surname> <given-names>MK</given-names></name><name><surname>Wetmore</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Correlation between the American-Academy-of-Otolaryngology-Head-and-Neck-Surgery 5-minute hearing test and standard audiological data</article-title><source>Otolaryngology-Head and Neck Surgery</source><volume>111</volume><fpage>625</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1177/019459989411100514</pub-id><pub-id pub-id-type="pmid">7970802</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kyle</surname> <given-names>FE</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>MacSweeney</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The relative contributions of speechreading and vocabulary to deaf and hearing children's reading ability</article-title><source>Research in Developmental Disabilities</source><volume>48</volume><fpage>13</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.ridd.2015.10.004</pub-id><pub-id pub-id-type="pmid">26524726</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Physical and perceptual factors shape the neural mechanisms that integrate audiovisual signals in speech comprehension</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>11338</fpage><lpage>11350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6510-10.2011</pub-id><pub-id pub-id-type="pmid">21813693</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname> <given-names>F</given-names></name><name><surname>Connelly</surname> <given-names>A</given-names></name><name><surname>Salmond</surname> <given-names>CH</given-names></name><name><surname>Gadian</surname> <given-names>DG</given-names></name><name><surname>Vargha-Khadem</surname> <given-names>F</given-names></name><name><surname>Baldeweg</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A direct test for lateralization of language activation using fMRI: comparison with invasive assessments in children with epilepsy</article-title><source>NeuroImage</source><volume>17</volume><fpage>1861</fpage><lpage>1867</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1327</pub-id><pub-id pub-id-type="pmid">12498760</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id><pub-id pub-id-type="pmid">17582338</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname> <given-names>A</given-names></name><name><surname>Summerfield</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Quantifying the contribution of vision to speech perception in noise</article-title><source>British Journal of Audiology</source><volume>21</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.3109/03005368709077786</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname> <given-names>M</given-names></name><name><surname>Amaro</surname> <given-names>E</given-names></name><name><surname>Calvert</surname> <given-names>GA</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>David</surname> <given-names>AS</given-names></name><name><surname>McGuire</surname> <given-names>P</given-names></name><name><surname>Williams</surname> <given-names>SC</given-names></name><name><surname>Woll</surname> <given-names>B</given-names></name><name><surname>Brammer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Silent speechreading in the absence of scanner noise: an event-related fMRI study</article-title><source>Neuroreport</source><volume>11</volume><fpage>1729</fpage><lpage>1733</lpage><pub-id pub-id-type="doi">10.1097/00001756-200006050-00026</pub-id><pub-id pub-id-type="pmid">10852233</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname> <given-names>M</given-names></name><name><surname>Capek</surname> <given-names>CM</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>Woll</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The signing brain: the neurobiology of sign language</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>432</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.07.010</pub-id><pub-id pub-id-type="pmid">18805728</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname> <given-names>JJ</given-names></name><name><surname>Störmer</surname> <given-names>VS</given-names></name><name><surname>Martinez</surname> <given-names>A</given-names></name><name><surname>Feng</surname> <given-names>W</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Salient sounds activate human visual cortex automatically</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>9194</fpage><lpage>9201</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5902-12.2013</pub-id><pub-id pub-id-type="pmid">23699530</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohammed</surname> <given-names>T</given-names></name><name><surname>Campbell</surname> <given-names>R</given-names></name><name><surname>Macsweeney</surname> <given-names>M</given-names></name><name><surname>Barry</surname> <given-names>F</given-names></name><name><surname>Coleman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Speechreading and its association with reading among deaf, hearing and dyslexic individuals</article-title><source>Clinical Linguistics &amp; Phonetics</source><volume>20</volume><fpage>621</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1080/02699200500266745</pub-id><pub-id pub-id-type="pmid">17056494</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname> <given-names>B</given-names></name><name><surname>Arnal</surname> <given-names>LH</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Keitel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prominence of Delta oscillatory rhythms in the motor cortex and their relevance for auditory and speech perception</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>107</volume><fpage>136</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.09.012</pub-id><pub-id pub-id-type="pmid">31518638</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarra</surname> <given-names>J</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hearing lips in a second language: visual articulatory information enables the perception of second language sounds</article-title><source>Psychological Research</source><volume>71</volume><fpage>4</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1007/s00426-005-0031-5</pub-id><pub-id pub-id-type="pmid">16362332</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Physics in Medicine and Biology</source><volume>48</volume><fpage>3637</fpage><lpage>3652</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The assessment and analysis of handedness: the edinburgh inventory</article-title><source>Neuropsychologia</source><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id><pub-id pub-id-type="pmid">5146491</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozker</surname> <given-names>M</given-names></name><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Frontal cortex selects representations of the talker’s mouth to aid in speech perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e30387</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.30387</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Piasini</surname> <given-names>E</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Fellin</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cracking the neural code for sensory perception by combining statistics, intervention, and behavior</article-title><source>Neuron</source><volume>93</volume><fpage>491</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.036</pub-id><pub-id pub-id-type="pmid">28182905</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>H</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration in humans</article-title><source>eLife</source><volume>8</volume><elocation-id>47001</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47001</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parra</surname> <given-names>LC</given-names></name><name><surname>Spence</surname> <given-names>CD</given-names></name><name><surname>Gerson</surname> <given-names>AD</given-names></name><name><surname>Sajda</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Recipes for the linear analysis of EEG</article-title><source>NeuroImage</source><volume>28</volume><fpage>326</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.05.032</pub-id><pub-id pub-id-type="pmid">16084117</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulesu</surname> <given-names>E</given-names></name><name><surname>Perani</surname> <given-names>D</given-names></name><name><surname>Blasi</surname> <given-names>V</given-names></name><name><surname>Silani</surname> <given-names>G</given-names></name><name><surname>Borghese</surname> <given-names>NA</given-names></name><name><surname>De Giovanni</surname> <given-names>U</given-names></name><name><surname>Sensolo</surname> <given-names>S</given-names></name><name><surname>Fazio</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A functional-anatomical model for lipreading</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>2005</fpage><lpage>2013</lpage><pub-id pub-id-type="doi">10.1152/jn.00926.2002</pub-id><pub-id pub-id-type="pmid">12750414</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Sommers</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction and constraint in audiovisual speech perception</article-title><source>Cortex</source><volume>68</volume><fpage>169</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.006</pub-id><pub-id pub-id-type="pmid">25890390</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pekkola</surname> <given-names>J</given-names></name><name><surname>Ojanen</surname> <given-names>V</given-names></name><name><surname>Autti</surname> <given-names>T</given-names></name><name><surname>J????skel??inen</surname> <given-names>IP</given-names></name><name><surname>M??tt??nen</surname> <given-names>R</given-names></name><name><surname>Tarkiainen</surname> <given-names>A</given-names></name><name><surname>Sams</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Primary auditory cortex activation by visual speech: an fMRI study at 3???T</article-title><source>NeuroReport</source><volume>16</volume><fpage>125</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1097/00001756-200502080-00010</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petro</surname> <given-names>LS</given-names></name><name><surname>Smith</surname> <given-names>FW</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Decoding face categories in diagnostic subregions of primary visual cortex</article-title><source>European Journal of Neuroscience</source><volume>37</volume><fpage>1130</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1111/ejn.12129</pub-id><pub-id pub-id-type="pmid">23373719</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petro</surname> <given-names>LS</given-names></name><name><surname>Paton</surname> <given-names>AT</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contextual modulation of primary visual cortex by auditory signals</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160104</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0104</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pica</surname> <given-names>G</given-names></name><name><surname>Piasini</surname> <given-names>E</given-names></name><name><surname>Safaai</surname> <given-names>H</given-names></name><name><surname>Runyan</surname> <given-names>C</given-names></name><name><surname>Harvey</surname> <given-names>C</given-names></name><name><surname>Diamond</surname> <given-names>M</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Quantifying how much sensory information in a neural code is relevant for behavior</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Liu</surname> <given-names>S</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>How can single sensory neurons predict behavior?</article-title><source>Neuron</source><volume>87</volume><fpage>411</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.033</pub-id><pub-id pub-id-type="pmid">26182422</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Plass</surname> <given-names>J</given-names></name><name><surname>Brang</surname> <given-names>D</given-names></name><name><surname>Suzuki</surname> <given-names>S</given-names></name><name><surname>Grabowecky</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Vision perceptually restores auditory spectral dynamics in speech</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://psyarxiv.com/t954p">https://psyarxiv.com/t954p</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname> <given-names>RA</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Durnez</surname> <given-names>J</given-names></name><name><surname>Gorgolewski</surname> <given-names>KJ</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name><name><surname>Munafò</surname> <given-names>MR</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Poline</surname> <given-names>JB</given-names></name><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Scanning the horizon: towards transparent and reproducible neuroimaging research</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>115</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.167</pub-id><pub-id pub-id-type="pmid">28053326</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purushothaman</surname> <given-names>G</given-names></name><name><surname>Bradley</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural population code for fine perceptual decisions in area MT</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/nn1373</pub-id><pub-id pub-id-type="pmid">15608633</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname> <given-names>MA</given-names></name><name><surname>Jefferies</surname> <given-names>E</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Rogers</surname> <given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural and computational bases of semantic cognition</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id><pub-id pub-id-type="pmid">27881854</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Revina</surname> <given-names>Y</given-names></name><name><surname>Petro</surname> <given-names>LS</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical feedback signals generalise across different spatial frequencies of feedforward inputs</article-title><source>NeuroImage</source><volume>180</volume><fpage>280</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.047</pub-id><pub-id pub-id-type="pmid">28951158</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname> <given-names>JM</given-names></name><name><surname>Zion Golumbic</surname> <given-names>E</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The effects of selective attention and speech acoustics on neural speech-tracking in a multi-talker scene</article-title><source>Cortex</source><volume>68</volume><fpage>144</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2014.12.014</pub-id><pub-id pub-id-type="pmid">25650107</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname> <given-names>JB</given-names></name><name><surname>Tovar</surname> <given-names>DA</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Emerging object representations in the visual system predict reaction times for categorization</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004316</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004316</pub-id><pub-id pub-id-type="pmid">26107634</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname> <given-names>JB</given-names></name><name><surname>Kaplan</surname> <given-names>DM</given-names></name><name><surname>Klein</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoding the brain: neural representation and the limits of multivariate pattern analysis in cognitive neuroscience</article-title><source>The British Journal for the Philosophy of Science</source><volume>70</volume><fpage>581</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1093/bjps/axx023</pub-id><pub-id pub-id-type="pmid">31086423</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Lemus</surname> <given-names>L</given-names></name><name><surname>de Lafuente</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sense, memory, and decision-making in the somatosensory cortical network</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>914</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.08.002</pub-id><pub-id pub-id-type="pmid">22939031</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname> <given-names>JN</given-names></name><name><surname>Speckman</surname> <given-names>PL</given-names></name><name><surname>Sun</surname> <given-names>D</given-names></name><name><surname>Morey</surname> <given-names>RD</given-names></name><name><surname>Iverson</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian t tests for accepting and rejecting the null hypothesis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>225</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.2.225</pub-id><pub-id pub-id-type="pmid">19293088</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runyan</surname> <given-names>CA</given-names></name><name><surname>Piasini</surname> <given-names>E</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Distinct timescales of population coding across cortex</article-title><source>Nature</source><volume>548</volume><fpage>92</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1038/nature23020</pub-id><pub-id pub-id-type="pmid">28723889</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>GG</given-names></name><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Becirspahic</surname> <given-names>M</given-names></name><name><surname>Yao</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Glasgow norms: ratings of 5,500 words on nine scales</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>1258</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1099-3</pub-id><pub-id pub-id-type="pmid">30206797</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From speech and talkers to the social world: The neural processing of human spoken language</article-title><source>Science</source><volume>366</volume><fpage>58</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1126/science.aax0288</pub-id><pub-id pub-id-type="pmid">31604302</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinkareva</surname> <given-names>SV</given-names></name><name><surname>Malave</surname> <given-names>VL</given-names></name><name><surname>Mason</surname> <given-names>RA</given-names></name><name><surname>Mitchell</surname> <given-names>TM</given-names></name><name><surname>Just</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Commonality of neural representations of words and pictures</article-title><source>NeuroImage</source><volume>54</volume><fpage>2418</fpage><lpage>2425</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.10.042</pub-id><pub-id pub-id-type="pmid">20974270</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simanova</surname> <given-names>I</given-names></name><name><surname>Hagoort</surname> <given-names>P</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Modality-independent decoding of semantic information from the human brain</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>426</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs324</pub-id><pub-id pub-id-type="pmid">23064107</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname> <given-names>JP</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title><source>Psychological Science</source><volume>22</volume><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumby</surname> <given-names>WH</given-names></name><name><surname>Pollack</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Visual contribution to speech intelligibility in noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>26</volume><fpage>212</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1121/1.1907309</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title><italic>Visual perception of phonetic gestures.</italic> paper presented at the modularity and the motor theory of speech perception</article-title><conf-name>A Conference to Honor Alvin M. Liberman</conf-name></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Lipreading and audiovisual Speech-Perception</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>335</volume><fpage>71</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0009</pub-id><pub-id pub-id-type="pmid">1348140</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabarelli</surname> <given-names>D</given-names></name><name><surname>Keitel</surname> <given-names>C</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Baldauf</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatial attention enhances cortical tracking of quasi-rhythmic visual stimuli</article-title><source>NeuroImage</source><volume>208</volume><elocation-id>116444</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116444</pub-id><pub-id pub-id-type="pmid">31816422</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunada</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>AS</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Cohen</surname> <given-names>YE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal contribution of primate auditory cortex to auditory perceptual decision-making</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/nn.4195</pub-id><pub-id pub-id-type="pmid">26656644</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tye-Murray</surname> <given-names>N</given-names></name><name><surname>Hale</surname> <given-names>S</given-names></name><name><surname>Spehar</surname> <given-names>B</given-names></name><name><surname>Myerson</surname> <given-names>J</given-names></name><name><surname>Sommers</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Lipreading in School-Age children: the roles of age, hearing status, and cognitive ability</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>57</volume><fpage>556</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1044/2013_JSLHR-H-12-0273</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname> <given-names>N</given-names></name><name><surname>Landeau</surname> <given-names>B</given-names></name><name><surname>Papathanassiou</surname> <given-names>D</given-names></name><name><surname>Crivello</surname> <given-names>F</given-names></name><name><surname>Etard</surname> <given-names>O</given-names></name><name><surname>Delcroix</surname> <given-names>N</given-names></name><name><surname>Mazoyer</surname> <given-names>B</given-names></name><name><surname>Joliot</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title><source>NeuroImage</source><volume>15</volume><fpage>273</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id><pub-id pub-id-type="pmid">11771995</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vaden</surname> <given-names>KI</given-names></name><name><surname>Halpin</surname> <given-names>H</given-names></name><name><surname>Hickok</surname> <given-names>GS</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Irvine Phonotactic Online Dictionary</source><version designator="2.0">2.0</version><ext-link ext-link-type="uri" xlink:href="http://www.iphod.com/">http://www.iphod.com/</ext-link></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname> <given-names>BD</given-names></name><name><surname>Van Drongelen</surname> <given-names>W</given-names></name><name><surname>Yuchtman</surname> <given-names>M</given-names></name><name><surname>Suzuki</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1109/10.623056</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name><name><surname>Wetzels</surname> <given-names>R</given-names></name><name><surname>Borsboom</surname> <given-names>D</given-names></name><name><surname>van der Maas</surname> <given-names>HLJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Why psychologists must change the way they analyze their data: the case of psi: comment on bem (2011)</article-title><source>Journal of Personality and Social Psychology</source><volume>100</volume><fpage>426</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1037/a0022790</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wetzels</surname> <given-names>R</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A default Bayesian hypothesis test for correlations and partial correlations</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><fpage>1057</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0295-x</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname> <given-names>A</given-names></name><name><surname>Wong</surname> <given-names>W</given-names></name><name><surname>Eizenman</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Gaze patterns and audiovisual speech enhancement</article-title><source>Journal of Speech, Language, and Hearing Research</source><volume>56</volume><fpage>471</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2012/10-0288)</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56972.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name><role>Reviewing Editor</role><aff><institution>Imperial College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Reichenbach</surname><given-names>Tobias</given-names> </name><role>Reviewer</role><aff><institution>Imperial College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Davis</surname><given-names>Matthew H</given-names></name><role>Reviewer</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>We can understand speech not only through hearing the sound, but also through reading from somebody's lips. In this study, Keitel et al. show that the identity of words is encoded by similar neural networks, whether they are presented through auditory or through visual signals. However, the comprehension of speech in the two modalities involves largely different brain areas, suggesting that the neural mechanisms for auditory and for visual speech comprehension differ more than previously believed.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your work entitled &quot;Largely distinct networks mediate perceptually-relevant auditory and visual speech representations&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Tobias Reichenbach as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Matthew H Davis (Reviewer #2); John Plass (Reviewer #3).</p><p>Our decision has been reached after consultation between the reviewers. The individual reviews below and the discussion have identified significant revisions and additional analysis that are required for the manuscript to be published in <italic>eLife</italic>. Because this additional work would likely take more than the two months that we allow for revisions, we regret to inform you we must formally decline your manuscript at this time.</p><p>However, we would consider a new submission in which the concerns raised below have been addressed. If you choose to submit a new version of the manuscript to <italic>eLife</italic>, we will make every effort for the new manuscript to be assessed by the same reviewers and Reviewing Editor.</p><p><italic>Reviewer #1:</italic></p><p>The authors investigate audiovisual speech processing using MEG. They present participants with speech in noise, as well as with videos of talking faces. In both conditions, subjects understand about 70% of the speech. The authors then investigate which brain areas decode word identity, as well as which brain areas predict the subject's actual comprehension. They find largely distinct areas for encoding of words as well as for predicting subject performance. Moreover, for both aspects of speech processing, the brain areas differ largely between the auditory and the visual stimulus presentation.</p><p>The paper is well written, and the obtained results shed new light on audio-visual speech processing. In particular, they show a large degree of modality-specific processing, as well as a dissociation between the brain areas for speech representation and for encoding comprehension. I am therefore in favour of publication. But I have two major comments that I would like the authors to address in a revised version.</p><p>1) I don't see the point of presenting the supplementary Figure 1. Since the behavioural results are at ceiling, only 8 subjects (those whose performance is away from the ceiling) are included in the MEG analysis. But this number of subjects is too small to draw conclusive results. The authors account for this to a degree by labelling the results as preliminary. But since the results cannot be considered conclusive, I think they should either be left out or modified to be conclusive.</p><p>2) The correlation between stimulus classification and the behavioural performance is only done for the visual condition ( – subsection “Strong sensory representations do not necessarily predict behaviour”). The authors state that this correlation can't be performed in the auditory condition because the comprehension scores there were 70%. But Figure 1B shows significant subject-to-subject variability in speech comprehension around 70%. While the variation is less than for lip reading, I don't see a reason why this performance cannot be related to the stimulus classification as well. Please add that.</p><p><italic>Reviewer #2:</italic></p><p>This paper describes an MEG study in which neural responses to unimodal auditory and visual (lip-read) sentences are analysed to determine the spatial locations at which brain responses provide information to distinguish between words, whether and how these responses are associated with correct perceptual identification of words, and the degree to which common neural representations in overlapping brain regions contribute to perception of auditory and visual speech.</p><p>Results show largely distinct networks of auditory, visual brain areas that convey information on word identity in heard and lip-read speech and that contribute to perception (inferred by increased information in neural representations for correctly identified words). There was some, limited neural overlap seen in higher-order language areas (such as inferior frontal gyrus, and temporal pole). However, attempts at cross-decoding – i.e. testing for common neural representations between words that were identified in visual and auditory speech – were non-significant. This is despite significant cross-decoding of both auditory and visual speech using audio-visual presentation.</p><p>Results are interpreted as showing separate, modality-specific neural representations of auditory and visual word identities, and taken to explain the independence (i.e. non-correlation) between individual differences in measures of auditory and visual speech perception abilities.</p><p>This paper addresses an important and interesting topic. Individual differences in auditory and visual speech perception are well established, and the lack of correlation between these abilities in the population appears to be a well-replicated, but currently under-explained, finding. Indeed, this basic observation goes against the dominant thrust of research on audio-visual speech perception which is largely concerned with the convergence of auditory and visual speech information. The authors' use of source-localised MEG classification analyses to address this issue is novel and the results presented build on a sufficient number of existing findings (e.g. the ability of auditory and visual responses to identify spoken and lip-read words, respectively), for me to find the two more surprising results: (1) limited overlap between audio and visual decoding, and (2) no significant cross-decoding of audio and visual speech to be intriguing and well worth publishing.</p><p>However, at the same time, I had some significant concerns that both of the key results that I have identified here depend on null findings in whole brain analyses of source-localised MEG responses. The authors must surely be aware that the absence of results reaching corrected significance cannot be taken to indicate that these effects are definitely absent. Absence of evidence is not evidence of absence, and this is particularly true when; (1) effects are tested using conventional null hypothesis significance testing, and (2) whole-brain correction for multiple comparisons are required which substantially reduce statistical sensitivity.</p><p>Only by directly subtracting statistical maps of auditory and visual word classification can the authors conclude that there is a reliable difference between the brain regions that contribute to visual and auditory word identification. Furthermore, the authors' presentation of the regression betas for peak regions from auditory and visual classification (in Figure 3D) are misleading. Given how these peaks are defined (from whole-brain search for regions showing unimodal classification), it's inevitable that maxima from auditory classification will be less reliable when tested on visual classification (and vice-versa).</p><p>These problems become particularly acute for concluding – as I think the authors wish to – that auditory areas don't contribute to word classification in visual speech. This requires confirming a null hypothesis. This can only be achieved with a Bayesian analysis which quantifies the likelihood of the null hypothesis in different brain regions. Only by performing this analysis can the authors be confident that there is not reliable classification in auditory areas that would be detected had they performed a study with greater statistical power. The authors might wish to make use of independent data – such as from the audiovisual speech condition presented in the supporting information – to define ROIs and/or expected effect sizes based on independent data.</p><p>The same problem with interpretation of null effects arises in the cross-modality decoding analyses. It is striking that these analyses are reliable for audiovisual speech but not for auditory speech. However, while this shows that the method the authors are using can detect reliable effects of a certain size, the analyses presented cannot be used to infer whether the most likely interpretation is that cross-decoding of unimodal auditory and visual speech is absent, or that it is present, but fails to reach a stringent level of significance. The authors wish to conclude that this effect is absent, they say: &quot;The inability to cross-classify auditory and visual speech from local brain activity further supports the conclusion that acoustic and visual speech representations are largely distinct&quot;, but do not have a sound statistical basis on which to draw this conclusion.</p><p>I think some substantial additional analysis, and careful consideration of which conclusions are, or are not supported by direct statistical analyses are required if this work is to be published with the current framing. This is not to say that word identity decoding in auditory and visual speech is uninteresting, or that the seeming lack of correlation between these abilities is not of interest. Only that I found the fundamental claim in the paper – that there's no common phonological representation of auditory and visual speech – to be insufficiently supported by the data presented in the current manuscript.</p><p><italic>Reviewer #3:</italic></p><p>In this manuscript, Keital et al. use MEG pattern classification to compare the cortical regions involved in the encoding and comprehension of auditory and visual speech. The article is well-written, addresses a scientifically valuable topic, and extends the current literature by employing multivariate approaches. In their stimulus classification analysis, the authors found that visually-presented words were best classified on the basis of signals localized to occipital regions, while auditory words were best classified in perisylvian regions. In their neuro-behavioral decoding analysis, the authors found that classification strength predicted behavioral accuracy in largely distinct regions during auditory versus visual stimulation. They conclude that perceptually-relevant speech representations are largely modality specific.</p><p>While I largely agreed with the authors' rationale in employing these techniques, I had some concerns regarding the statistical/mathematical details of their approach. First and most importantly, the neuro-behavioral decoding analysis presented in Figure 3A and 3C does not directly test the hypothesis that forms their primary conclusion (i.e., that &quot;Largely distinct networks mediate perceptually-relevant auditory and visual speech representations&quot;). To test this hypothesis directly, it would be necessary to compare neuro-behavioral decoding accuracy between the auditory and visual conditions for each vertex and then multiple-comparison correct across vertices. That is, rather than comparing decoding accuracy against the null separately for each condition, decoder accuracy should be compared directly across conditions. The authors perform a similar analysis in Figure 3D, but this analysis may inflate Type-I error because it involves pre-selecting peaks identified in each single-condition analysis.</p><p>Second, throughout the manuscript, a wide variety of statistical techniques are used which are sometimes internally inconsistent, weakly or not explicitly justified, or interpreted in a manner that is not fully consistent with their mathematical implications. For example, for different analyses, different multiple comparison correction procedures are used (FDR vs. cluster-based FWE control), without a clear justification. It would be best to use similar corrections across analyses to ensure similar power. Also, the authors report that they: &quot;Tested a number of different classifiers, […] then selected a correlation-based nearest-neighbour classifier as this performed slightly better than the others&quot;. Absent an a priori justification for the chosen approach, it would be helpful to know whether the reported results are robust to these design decisions. Finally, it is not clear to me that the neuro-behavioral decoding technique employed here is particularly well-suited for identifying regions that represent participants' percepts. It seems the most natural way to perform such an analysis would be to train a classifier to predict participants' trial-wise responses. By contrast, the technique employed here compares (binary) trial-wise accuracy with &quot;representational distance&quot;, computed as the difference between &quot;the average correlation with trials of the same (correct) word identity and the mean of the correlation with the three alternatives.&quot; Thus, the classifier would not be expected to identify regions with response patterns that predict participants' percepts, but those which exhibit a target-like pattern when the participant responds accurately. It is therefore perhaps best conceived of as an &quot;accuracy classifier&quot; rather than a &quot;percept classifier.&quot; This may be problematic for the authors' interpretation because activity in areas unrelated to perceptual representations (e.g., areas involved in unimodal attention) could also be predictive of accuracy.</p><p>Finally, in some cases, the methods description was not self-sufficient, leaving the reader to consult references to fully understand. One critical question is how the spatial and temporal dimensions of the data were used in the classifier. If classification is primarily driven by the temporal dimension, it is not clear that successful classification really relies on pattern similarity in population responses, rather than inter-trial temporal covariation produced by, e.g., phase-resetting or entrainment to stimulus dynamics. In the cited methods articles, spatial classification is performed separately for different time bins, alleviating this concern. It would be important to critically consider this detail in interpreting these results.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Largely distinct networks mediate perceptually-relevant auditory and visual speech representations&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Tobias Reichenbach as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Matthew H Davis (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is 'in revision at <italic>eLife</italic>'. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>The revised paper presents a better-fitting analysis, and does a more nuanced job in discussing the results, than the original manuscript. However, there are still a few major criticisms that we have for the analysis, detailed below.</p><p>Essential revisions:</p><p>1) Brain-wide, multiple-comparison corrected tests comparing auditory versus visual decoding are still lacking. The authors have now provided vertex-wise Bayes factors within areas that showed significant decoding in each individual condition. Unfortunately, this is not satisfactory, because these statistics are (1) potentially circular because ROIs were pre-selected based on an analysis of individual conditions, (2) not multiple-comparison corrected, and (3) rely on an arbitrary prior that is not calibrated to the expected effect size. Still, ignoring these issues, the only area that appears to contain vertices with &quot;strong evidence&quot; for a difference in neuro-behavioral decoding is the MOG, which wouldn't really support the claim of &quot;largely distinct networks&quot; supporting audio vs. visual speech representation.</p><p>The authors may address these issues, for instance, by</p><p>i) presenting additional whole-brain results – e.g. for a direct comparison of auditory and visual classification (in Figure 2) and of perceptual prediction (in Figure 3).</p><p>ii) presenting voxel-wise maps of Bayesian evidence values (as in Figure 2—figure supplement 3) for the statistical comparisons shown in Figure 2D, and Figure 3D</p><p>iii) in the text included in Figure 2D and 3D making clear what hypotheses correspond to the null hypothesis and to the alternative hypothesis (i.e. auditory = visual, auditory &lt;&gt; visual).</p><p>2) As noted before by reviewer 3, the classifiers used in this study do not discriminate between temporal versus spatial dimensions of decoding accuracy. This leaves it unclear whether the reported results are driven by (dis)similarity of spatial patterns of activity (as in fMRI-based MVPA), temporal patterns of activity (e.g., oscillatory &quot;tracking&quot; of the speech signal), or some combination. As these three possibilities could lead to very different interpretations of the data, it seems critical to distinguish between them. For example, the authors write &quot;the encoding of the acoustic speech envelope is seen widespread in the brain, but correct word comprehension correlates only with focal activity in temporal and motor regions,&quot; but, as it stands, their results could be partly driven by this non-specific entrainment to the acoustic envelope.</p><p>In their response, the authors show that classifier accuracy breaks down when spatial or temporal information is degraded, but it would be more informative to show how these two factors interact. For example, the methods article cited by the authors (Grootswagers, Wardle and Carlson, 2017) shows classification accuracy for successive time bins after stimulus onset (i.e., they train different classifiers for each time bin 0-100 ms, 100-200 ms, etc.). The timing of decoding accuracy in different areas could also help to distinguish between different plausible explanations of the results.</p><p>Finally, it is somewhat unclear how spatial and temporal information are combined in the current classifier. Figure 1—figure supplement 2 creates the impression that the time-series for each vertex within a spotlight were simply concatenated. However, this would conflate within-vertex (temporal) and across-vertex (spatial) variance.</p><p>3) The concern that the classifier could conceivably index factors influencing &quot;accuracy&quot; rather than the perceived stimulus does not appear to be addressed sufficiently. Indeed, the classifier is referred to as identifying &quot;sensory representations&quot; throughout the manuscript, when it could just as well identify areas involved in any other functions (e.g., attention, motor function) that would contribute to accurate behavioral performance. This limitation should be acknowledged in the manuscript. The authors could consider using the timing of decoding accuracy in different areas to disambiguate these explanations.</p><p>The authors state in their response that classifying based on the participant's reported stimulus (rather than response accuracy) could &quot;possibly capture representations not related to speech encoding but relevant for behaviour only (e.g. pre-motor activity). These could be e.g. brain activity that leads to perceptual errors based on intrinsic fluctuations in neural activity in sensory pathways, noise in the decision process favouring one alternative response among four choices, or even noise in the motor system that leads to a wrong button press without having any relation to sensory representations at all.&quot;</p><p>But, it seems that all of these issues would also effect the accuracy-based classifier as well. Moreover, it seems that intrinsic fluctuations in sensory pathways, or possibly noise in the decision process, are part of what the authors are after. If noise in a sensory pathway can be used to predict particular inaccurate responses, isn't that strong evidence that it encodes behaviorally-relevant sensory representations? For example, intrinsic noise in V1 has been found to predict responses in a simple visual task in non-human primates, with false alarm trials exhibiting noise patterns that are similar to target responses (Seidemann and Geisler, 2018). Showing accurate trial-by-trial decoding of participants' incorrect responses could similarly provide stronger evidence that a certain area contributes to behavior.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56972.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><p>We would like to thank the reviewers and editors for their very helpful and constructive comments. Based on these we have substantially improved the manuscript, both by refining the statistical approach and the interpretation of the results.</p><p>A main concern shared by several reviewers was that some of our conclusions were based on statistical null results, and these were not sufficiently supported by quantitative analysis (e.g. Bayes factors). With this in mind, we have revised the entire analysis pipeline. First, we ensured that all analyses are based on the same statistical corrections for multiple comparisons (which was another concern). Second, we implemented direct condition contrasts as requested and now provide Bayes factors to substantiate the evidence for or against the respective null hypotheses. Third, we implemented additional control analyses to understand whether and how spatial and temporal response patterns contribute to the overall classification performance. And fourth, we analysed semantic and phonological stimulus features to obtain a better understanding of the sensory features driving comprehension performance.</p><p>All analyses were computed de novo for the revision. Due to changes in the statistical procedures some of the specific details in the results have changed (e.g. the number of clusters emerging in specific analyses). However, the main conclusions put forward in the previous version still hold and thanks to the additional analyses we can now provide a more refined interpretation of these in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The authors investigate audiovisual speech processing using MEG. They present participants with speech in noise, as well as with videos of talking faces. In both conditions, subjects understand about 70% of the speech. The authors then investigate which brain areas decode word identity, as well as which brain areas predict the subject's actual comprehension. They find largely distinct areas for encoding of words as well as for predicting subject performance. Moreover, for both aspects of speech processing, the brain areas differ largely between the auditory and the visual stimulus presentation.</p><p>The paper is well written, and the obtained results shed new light on audio-visual speech processing. In particular, they show a large degree of modality-specific processing, as well as a dissociation between the brain areas for speech representation and for encoding comprehension. I am therefore in favour of publication. But I have two major comments that I would like the authors to address in a revised version.</p><p>1) I don't see the point of presenting the supplementary Figure 1. Since the behavioural results are at ceiling, only 8 subjects (those whose performance is away from the ceiling) are included in the MEG analysis. But this number of subjects is too small to draw conclusive results. The authors account for this to a degree by labelling the results as preliminary. But since the results cannot be considered conclusive, I think they should either be left out or modified to be conclusive.</p></disp-quote><p>We agree that the number of participants who performed below ceiling in the audiovisual condition is too small for conclusive results. Hence, we have removed the respective data from the manuscript but decided to keep the audiovisual results that included all participants in Figure 2—figure supplement 2. There we show the AV data regarding behavioural performance (<italic>N</italic> = 20) and classification performance (N = 18). We also kept the cross-classification analysis between A/V and the AV conditions to demonstrate that cross-classification is possible in principle (Figure 2—figure supplement 3).</p><disp-quote content-type="editor-comment"><p>2) The correlation between stimulus classification and the behavioural performance is only done for the visual condition (subsection “Strong sensory representations do not necessarily predict behaviour”). The authors state that this correlation can't be performed in the auditory condition because the comprehension scores there were 70%. But Figure 1B shows significant subject-to-subject variability in speech comprehension around 70%. While the variation is less than for lip reading, I don't see a reason why this performance cannot be related to the stimulus classification as well. Please add that.</p></disp-quote><p>We have added the respective correlation for the auditory condition, which is now presented in Figure 3—figure supplement 2A. We have also added a correlation between auditory/visual classification and individual auditory SNR values (please see response to reviewer 2 below). The statistical analysis for these correlations was adapted to be consistent with analyses in the rest of the manuscript. A cluster-based permutation test did not yield any significant results. We also report respective Bayes factors, which supported overall results (Figure 3—figure supplement 2B).</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>This paper describes an MEG study in which neural responses to unimodal auditory and visual (lip-read) sentences are analysed to determine the spatial locations at which brain responses provide information to distinguish between words, whether and how these responses are associated with correct perceptual identification of words, and the degree to which common neural representations in overlapping brain regions contribute to perception of auditory and visual speech.</p><p>Results show largely distinct networks of auditory, visual brain areas that convey information on word identity in heard and lip-read speech and that contribute to perception (inferred by increased information in neural representations for correctly identified words). There was some, limited neural overlap seen in higher-order language areas (such as inferior frontal gyrus, and temporal pole). However, attempts at cross-decoding – i.e. testing for common neural representations between words that were identified in visual and auditory speech – were non-significant. This is despite significant cross-decoding of both auditory and visual speech using audio-visual presentation.</p><p>Results are interpreted as showing separate, modality-specific neural representations of auditory and visual word identities, and taken to explain the independence (i.e. non-correlation) between individual differences in measures of auditory and visual speech perception abilities.</p><p>This paper addresses an important and interesting topic. Individual differences in auditory and visual speech perception are well established, and the lack of correlation between these abilities in the population appears to be a well-replicated, but currently under-explained, finding. Indeed, this basic observation goes against the dominant thrust of research on audio-visual speech perception which is largely concerned with the convergence of auditory and visual speech information. The authors' use of source-localised MEG classification analyses to address this issue is novel and the results presented build on a sufficient number of existing findings (e.g. the ability of auditory and visual responses to identify spoken and lip-read words, respectively), for me to find the two more surprising results: (1) limited overlap between audio and visual decoding, and (2) no significant cross-decoding of audio and visual speech to be intriguing and well worth publishing.</p><p>However, at the same time, I had some significant concerns that both of the key results that I have identified here depend on null findings in whole brain analyses of source-localised MEG responses. The authors must surely be aware that the absence of results reaching corrected significance cannot be taken to indicate that these effects are definitely absent. Absence of evidence is not evidence of absence, and this is particularly true when; (1) effects are tested using conventional null hypothesis significance testing, and (2) whole-brain correction for multiple comparisons are required which substantially reduce statistical sensitivity.</p><p>Only by directly subtracting statistical maps of auditory and visual word classification can the authors conclude that there is a reliable difference between the brain regions that contribute to visual and auditory word identification. Furthermore, the authors' presentation of the regression betas for peak regions from auditory and visual classification (in Figure 3D) are misleading. Given how these peaks are defined (from whole-brain search for regions showing unimodal classification), it's inevitable that maxima from auditory classification will be less reliable when tested on visual classification (and vice-versa).</p></disp-quote><p>The reviewer points out a critical shortcoming in our previous submission: the lack of evidence for statistical null results. We have addressed this point using additional data analyses, whereby we now provide direct between-condition contrasts for all grid-points in the significant clusters, both for the stimulus classification (Figure 2) and the neuro-behavioural analysis (Figure 3). For each of these we computed a direct contrast between the A and V conditions and derived the associated Bayes factors to substantiate the evidence for or against the respective null hypothesis. These new results are presented in Figures 2D and 3D and support our conclusion that stimulus information for acoustic and visual speech is provided both by potentially amodal regions (e.g. post-central regions) as well as by regions that contain (significant) information only about a single modality (e.g. occipital regions). Furthermore, they support the conclusion that auditory and visual comprehension are driven in large by distinct networks, but also are facilitated by an overlap of auditory and visual representations in angular and frontal regions. We revised the Discussion and conclusions in the light of this refined analysis.</p><p>The reviewer also points out that our presentation of the effects obtained at local peak voxels was misleading. We did not intend to present these as an (indeed) circular statistical analysis, but rather as post-hoc visualisation and quantification of the underlying effects. In the revised manuscript, we now avoid this potentially misleading step, and derive significant clusters from the respective full-brain maps and simply report local peak results in Tables 1 and 2.</p><disp-quote content-type="editor-comment"><p>These problems become particularly acute for concluding – as I think the authors wish to – that auditory areas don't contribute to word classification in visual speech. This requires confirming a null hypothesis. This can only be achieved with a Bayesian analysis which quantifies the likelihood of the null hypothesis in different brain regions. Only by performing this analysis can the authors be confident that there is not reliable classification in auditory areas that would be detected had they performed a study with greater statistical power. The authors might wish to make use of independent data – such as from the audiovisual speech condition presented in the supporting information – to define ROIs and/or expected effect sizes based on independent data.</p></disp-quote><p>Following this comment, we now provide a systematic analysis of statistical contrasts and associated Bayes factors (Figures 2D and 3D). These new results allow us to directly support some of our original conclusions, but also provide a more nuanced picture. Starting from those regions where cerebral word representations are significantly predictive of comprehension (Figure 3A and B) we find that in some regions many grid points exhibit evidence for a differential contribution to auditory and visual comprehension, while in other regions (e.g. IFG, AG) many grid points exhibit evidence for no modality specificity (Figure 3D). We have revised the Discussion to fully reflect these results.</p><p>We would like to point out that our data indeed suggest the involvement of temporal regions in visual speech comprehension (right STG in Figure 3B), but that this region does not predict auditory speech comprehension. Our main conclusion is therefore that regions predictive of auditory and visual comprehension are largely distinct.</p><p>For example: “Thereby our results support a route of visual speech into auditory cortical and temporal regions but provide no evidence for an overlap of speech representations in the temporal lobe that would facilitate both lip-reading and acoustic speech comprehension.” A more elaborate summary of our conclusions can be found in the Discussion.</p><disp-quote content-type="editor-comment"><p>The same problem with interpretation of null effects arises in the cross-modality decoding analyses. It is striking that these analyses are reliable for audiovisual speech but not for auditory speech. However, while this shows that the method the authors are using can detect reliable effects of a certain size, the analyses presented cannot be used to infer whether the most likely interpretation is that cross-decoding of unimodal auditory and visual speech is absent, or that it is present, but fails to reach a stringent level of significance. The authors wish to conclude that this effect is absent, they say: &quot;The inability to cross-classify auditory and visual speech from local brain activity further supports the conclusion that acoustic and visual speech representations are largely distinct&quot;, but do not have a sound statistical basis on which to draw this conclusion.</p></disp-quote><p>We addressed this comment in different ways. First, by revising the significance testing of the cross-decoding performance, for which we now report brain-wide cluster-based permutation statistics. This new statistical analysis (which is now consistent with the cluster statistics in the rest of the manuscript), did not yield any significant effects (Figure 2—figure supplement 3A). Second, we now also provide Bayes factors based on t-values, derived from a comparison with a random distribution. The topographical maps (Figure 2—figure supplement 3B) show that for the large majority of brain regions, cross-classification is not possible (Bayes factors supporting the H<sub>0</sub>), with the exception of some irregular grid points. An exploratory cluster analysis (only for the purpose of this response) found that, even with a minimum cluster size of 1 grid point, no significant clusters occurred. We are therefore confident that cross-classification is not possible in a meaningful way between the auditory and visual conditions for the present data.</p><disp-quote content-type="editor-comment"><p>I think some substantial additional analysis, and careful consideration of which conclusions are, or are not supported by direct statistical analyses are required if this work is to be published with the current framing. This is not to say that word identity decoding in auditory and visual speech is uninteresting, or that the seeming lack of correlation between these abilities is not of interest. Only that I found the fundamental claim in the paper – that there's no common phonological representation of auditory and visual speech – to be insufficiently supported by the data presented in the current manuscript.</p></disp-quote><p>The revised paper now clearly spells out which results are substantiated by appropriate statistical evidence for modality specific results, and which not. In doing so, we can now provide a more detailed and nuanced view on which regions contribute perceptually-relevant word encoding for acoustic and visual speech.</p><p>For example, in the first paragraph of the Discussion we state: “Our results show that the cerebral representations of auditory and visual speech are mediated by both modality specific and overlapping (potentially amodal) representations. While several parietal, temporal and frontal regions were engaged in the encoding of both acoustically and visually conveyed word identities (“stimulus classification”), comprehension in both sensory modalities was largely driven by distinct networks. Only the inferior frontal and angular gyrus contained regions that contributed similarly to both auditory and visual comprehension.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>In this manuscript, Keital et al. use MEG pattern classification to compare the cortical regions involved in the encoding and comprehension of auditory and visual speech. The article is well-written, addresses a scientifically valuable topic, and extends the current literature by employing multivariate approaches. In their stimulus classification analysis, the authors found that visually-presented words were best classified on the basis of signals localized to occipital regions, while auditory words were best classified in perisylvian regions. In their neuro-behavioral decoding analysis, the authors found that classification strength predicted behavioral accuracy in largely distinct regions during auditory versus visual stimulation. They conclude that perceptually-relevant speech representations are largely modality specific.</p><p>While I largely agreed with the authors' rationale in employing these techniques, I had some concerns regarding the statistical/mathematical details of their approach. First and most importantly, the neuro-behavioral decoding analysis presented in Figure 3A and 3C does not directly test the hypothesis that forms their primary conclusion (i.e., that &quot;Largely distinct networks mediate perceptually-relevant auditory and visual speech representations&quot;). To test this hypothesis directly, it would be necessary to compare neuro-behavioral decoding accuracy between the auditory and visual conditions for each vertex and then multiple-comparison correct across vertices. That is, rather than comparing decoding accuracy against the null separately for each condition, decoder accuracy should be compared directly across conditions. The authors perform a similar analysis in Figure 3D, but this analysis may inflate Type-I error because it involves pre-selecting peaks identified in each single-condition analysis.</p></disp-quote><p>As reported in the reply to reviewer 2, we now provide direct statistical contrasts between A and V conditions and report Bayes factors for the null hypotheses to support our conclusions. These analyses in large support our previous claims, but also highlight a more nuanced picture, as reflected in the revised Discussion. Please see also comments above. e.g. in the first paragraph of the Discussion we now write: “Our results show that the cerebral representations of auditory and visual speech are mediated by both modality-specific and overlapping (potentially amodal) representations. While several parietal, temporal and frontal regions were engaged in the encoding of both acoustically and visually conveyed word identities (“stimulus classification”), comprehension in both sensory modalities was largely driven by distinct networks. Only the inferior frontal and angular gyrus contained regions that contributed similarly to both auditory and visual comprehension.”</p><disp-quote content-type="editor-comment"><p>Second, throughout the manuscript, a wide variety of statistical techniques are used which are sometimes internally inconsistent, weakly or not explicitly justified, or interpreted in a manner that is not fully consistent with their mathematical implications. For example, for different analyses, different multiple comparison correction procedures are used (FDR vs. cluster-based FWE control), without a clear justification. It would be best to use similar corrections across analyses to ensure similar power. Also, the authors report that they: &quot;Tested a number of different classifiers, […] then selected a correlation-based nearest-neighbour classifier as this performed slightly better than the others&quot;. Absent an a priori justification for the chosen approach, it would be helpful to know whether the reported results are robust to these design decisions.</p></disp-quote><p>We have revised all statistical analyses and now use the same correction methods for all fullbrain analyses. In a preliminary analysis we had tested different classifiers in their ability to classify the stimulus set (e.g. a nearest neighbour classifier based on the Euclidean distance, and correlation decoders based on different temporal sampling). Based on the overall performance across all three conditions (A,V,AV) we decided on the classifier and parameter used, although we note that the differences between version of the classifier were small (23%). In part, we now directly address this point by reporting how the stimulus classification performance depends on the spatial searchlight and the temporal resolution. We have repeated the stimulus classification using a range of spatial and temporal parameters for the searchlight. In particular, we removed the spatial information and systematically reduced the temporal resolution.</p><p>These complementary results are now mentioned in the manuscript.</p><p>“We also probed classification performance based on a number of spatio-temporal searchlights, including temporal binning of the data at 3.3, 20, 40 and 60ms, and including each neighbouring spatial grid point into the searchlight or averaging across grid points. Comparing classification performance revealed that in particular the auditory condition was sensitive to the choice of searchlight. Classification performance dropped when ignoring the spatial configuration or sampling the data at a resolution lower than 20 ms (median performance of the 10% grid points with highest performance based on each searchlight: 27.0%, 26.9% 26.5% and 25.8% for 3.3, 20, 40 and 60-ms bins and the full spatial searchlight; and 26.4% at 20ms and ignoring the spatial pattern). We hence opted for a classifier based on the source data represented as spatial searchlight (1.2-cm radius) and sampled at 20-ms resolution.”</p><p>In <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> is an illustration of these results.</p><fig id="sa2fig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Finally, it is not clear to me that the neuro-behavioral decoding technique employed here is particularly well-suited for identifying regions that represent participants' percepts. It seems the most natural way to perform such an analysis would be to train a classifier to predict participants' trial-wise responses. By contrast, the technique employed here compares (binary) trial-wise accuracy with &quot;representational distance&quot;, computed as the difference between &quot;the average correlation with trials of the same (correct) word identity and the mean of the correlation with the three alternatives.&quot; Thus, the classifier would not be expected to identify regions with response patterns that predict participants' percepts, but those which exhibit a target-like pattern when the participant responds accurately. It is therefore perhaps best conceived of as an &quot;accuracy classifier&quot; rather than a &quot;percept classifier.&quot; This may be problematic for the authors' interpretation because activity in areas unrelated to perceptual representations (e.g., areas involved in unimodal attention) could also be predictive of accuracy.</p></disp-quote><p>The reviewer touches on an important issue concerning the interpretation of the mapped representations. Our study was motivated by the notion of intersection information, that is the search for cerebral representations of a stimulus that are used for the respective single trial behaviour (Panzeri et al., 2017; Pica et al., 2017). This intersection information can be formalised theoretically and can be measured using information theoretic approaches. However, these principled approaches are still computationally inefficient. Following the neuroimaging field, we hence opted for a distanceto-bound method, where the amount of sensory evidence captured in a classifier is regressed against behaviour using a linear model (see, e.g. Grootswagers, Cichy and Carlson, 2018). This method is computationally cheaper and allows for full-brain permutation statistics. In contrast to this approach, a classifier trained on participants response (choice) would, at least on error trials, possibly capture representations not related to speech encoding but relevant for behaviour only (e.g. pre-motor activity). These could be e.g. brain activity that leads to perceptual errors based on intrinsic fluctuations in neural activity in sensory pathways, noise in the decision process favouring one alternative response among four choices, or even noise in the motor system that leads to a wrong button press without having any relation to sensory representations at all. The latter example highlights the need to base any analysis of behaviourally-relevant sensory representations (i.e. the intersection information) on classifiers which are firstly trained to discriminate the relevant sensory information and which are then probed as to how behaviourally-relevant the classifier output is. We have revised the Discussion to better explain the rationale of our approach in this respect.</p><disp-quote content-type="editor-comment"><p>Finally, in some cases, the methods description was not self-sufficient, leaving the reader to consult references to fully understand. One critical question is how the spatial and temporal dimensions of the data were used in the classifier. If classification is primarily driven by the temporal dimension, it is not clear that successful classification really relies on pattern similarity in population responses, rather than inter-trial temporal covariation produced by, e.g., phase-resetting or entrainment to stimulus dynamics. In the cited methods articles, spatial classification is performed separately for different time bins, alleviating this concern. It would be important to critically consider this detail in interpreting these results.</p></disp-quote><p>We have revised the Materials and methods in many instances for ensure that all procedures are described clearly. We have added more information about the spatio-temporal dimensions of the classifier (see also above), noting that both spatial and temporal patterns of local brain activity were contributing to the stimulus classification. Whether the neural “mechanisms” mentioned, such as phase-resetting, indeed contribute to the cerebral representations studied here is a question that is surely beyond the scope of this study.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Brain-wide, multiple-comparison corrected tests comparing auditory versus visual decoding are still lacking. The authors have now provided vertex-wise Bayes factors within areas that showed significant decoding in each individual condition. Unfortunately, this is not satisfactory, because these statistics are (1) potentially circular because ROIs were pre-selected based on an analysis of individual conditions, (2) not multiple-comparison corrected, and (3) rely on an arbitrary prior that is not calibrated to the expected effect size. Still, ignoring these issues, the only area that appears to contain vertices with &quot;strong evidence&quot; for a difference in neuro-behavioral decoding is the MOG, which wouldn't really support the claim of &quot;largely distinct networks&quot; supporting audio vs. visual speech representation.</p><p>The authors may address these issues, for instance, by</p><p>i) presenting additional whole-brain results – e.g. for a direct comparison of auditory and visual classification (in Figure 2) and of perceptual prediction (in Figure 3).</p><p>ii) presenting voxel-wise maps of Bayesian evidence values (as in Figure 2—figure supplement 3) for the statistical comparisons shown in Figure 2D, and Figure 3D</p><p>iii) in the text included in Figure 2D and 3D making clear what hypotheses correspond to the null hypothesis and to the alternative hypothesis (i.e. auditory = visual, auditory &lt;&gt; visual).</p></disp-quote><p>We addressed this comment using additional data analysis to ensure that all claims are supported by sufficient statistical evidence.</p><p>i) In the revised manuscript, we now provide the suggested full-brain cluster-corrected contrasts between auditory and visual conditions for both main analyses (Figure 2—figure supplement 1A, Figure 3—figure supplement 1A). However, we caution against the interpretation of condition-wise differences at grid points that do not exhibit significant evidence for the primary “function” of interest. We therefore refrain from interpreting condition-wise differences in word classification performance (or the prediction of comprehension) at grid points that do not exhibit significant word classification (prediction of comprehension) in at least one condition (vs. the randomization null). Hence, we masked the full-brain cluster-corrected condition differences against all grid points contributing to word classification (or the prediction of comprehension) in at least one modality (while also reporting all clusters in the figure caption).</p><p>Importantly, and in line with the general concerns raised about interpreting null results, these full-brain condition differences can provide evidence in favour of a condition difference, and therefore support the existence of modality specific regions. However, they cannot provide evidence in favour of a null finding of no modality specialisation. The analysis of Bayes factors provided in the previous revision, which we retained in Figures 2D,3D, in contrast, can provide such evidence in favour of a null result. Hence in the revised manuscript we kept the Bayes factors in the main figure, while now also providing the full-brain cluster-based statistics in the supplemental material. Importantly, we base all interpretations of the results on the combined evidence provided by the full-brain condition differences and these Bayes factors.</p><p>ii) We now also provide full-brain maps with Bayes factors for contrasts between the two modalities (in Figure 2—figure supplement 1B, Figure 3—figure supplement 1B, alongside the full-brain cluster maps).</p><p>iii) We have added the specific hypotheses to Figures 2 and 3 as suggested.</p><p>The results of these additional statistical tests do not affect our main findings, but they support the conclusions derived from the ROI-based analysis. We carefully ensured that the revised manuscript clearly acknowledges that the individual analyses are inconclusive for some parts of the brain or offer specific evidence for no difference between modalities in other parts. For example:</p><p>“A separate full-brain cluster-based permutation test (Figure 3—figure supplement 1A) provided evidence for a significant modality specialisation for auditory words in four clusters in the left middle occipital gyrus, left calcarine gyrus, right posterior angular gyrus, and bilateral supplementary motor area. The corresponding full-brain Bayes factors (Figure 3—figure supplement 1B) support this picture but also provide no evidence for a modality preference, or inconclusive results, in many other regions.”</p><p>To acknowledge that there is a large number of grid points that do not show a modality difference, we have also revised the title of this manuscript to “Shared and modality-specific brain regions that mediate auditory and visual word comprehension”</p><p>The comment also suggests that the analyses of ROI-specific comparisons may be circular. First, we now provide the full brain results for the Bayes factors in Figure 2—figure supplement 1B and Figure 3—figure supplement 1B. Concerning the ROI-based results in Figures 2,3, it is important to note that we compare condition-wise differences (as Bayes factors) within regions pre-selected to show an effect in at least one of the two modalities, hence independent of the contrast. We do so, as the interpretation of a condition-wise difference (e.g. in word classification) in a brain region not exhibiting significant classification in any condition is difficult, if not impossible. Such pre-selection of electrodes or regions of interest is very common (Cheung et al., 2016, Giordano et al., 2017, Karas et al., 2019, Mihai et al., 2019, Ozker, Yoshor and Beauchamp, 2018) and supported by the use of orthogonal contrasts for selection and comparison.</p><p>The comment further notes that we relied on an “arbitrary prior” to calculate Bayes factors. We would like to emphasize that the JZS Bayes factor, while not being data-driven by the present study, is not arbitrary. It has been advocated heavily by a number of studies for situations where no specific information about the expected effects sizes is available 2009(Rouder et al., 2012; ) and is highly accepted in the current literature (Guitard and Cowan, 2020, Mazor, Friston and Fleming, 2020, Puvvada and Simon, 2017, Kimel, Ahissar and Lieder, 2020). The default scale of <inline-formula><mml:math id="inf2"><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mn>2</mml:mn></mml:mfrac></mml:math></inline-formula> corresponds to the assumption that the prior of effect sizes follows a Cauchy distribution with 50% of probability mass placed on effect sizes smaller than <inline-formula><mml:math id="inf3"><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mn>2</mml:mn></mml:mfrac></mml:math></inline-formula>, and 50% larger than this number (Schönbrodt and Wagenmakers, 2018). Looking at our data, the effect sizes in the neurobehavioral analysis (Table 2) seem to follow such a pattern rather well (4 out of 9 effect sizes, calculated as Cohen’s D, are smaller than <inline-formula><mml:math id="inf4"><mml:mfrac><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mn>2</mml:mn></mml:mfrac></mml:math></inline-formula>, while 5 are larger than this). Hence, taking this one specific statistical contrast from our study as evidence, it makes sense to assume that similar effect sizes are expected also in the other tests, such as the condition-wise differences for which we report the Bayes factors in Figures 2D, 3D. Of course, in an ideal case one would base the choice of the prior on pre-existing and independent data, however, unfortunately such data were not available.</p><disp-quote content-type="editor-comment"><p>2) As noted before by reviewer 3, the classifiers used in this study do not discriminate between temporal versus spatial dimensions of decoding accuracy. This leaves it unclear whether the reported results are driven by (dis)similarity of spatial patterns of activity (as in fMRI-based MVPA), temporal patterns of activity (e.g., oscillatory &quot;tracking&quot; of the speech signal), or some combination. As these three possibilities could lead to very different interpretations of the data, it seems critical to distinguish between them. For example, the authors write &quot;the encoding of the acoustic speech envelope is seen widespread in the brain, but correct word comprehension correlates only with focal activity in temporal and motor regions,&quot; but, as it stands, their results could be partly driven by this non-specific entrainment to the acoustic envelope.</p><p>In their response, the authors show that classifier accuracy breaks down when spatial or temporal information is degraded, but it would be more informative to show how these two factors interact. For example, the methods article cited by the authors (Grootswagers, Wardle and Carlson, 2017) shows classification accuracy for successive time bins after stimulus onset (i.e., they train different classifiers for each time bin 0-100 ms, 100-200 ms, etc.). The timing of decoding accuracy in different areas could also help to distinguish between different plausible explanations of the results.</p></disp-quote><p>This comment raises a number of interesting questions, which we (partly) addressed in the previous round. Unfortunately, maybe, our treatment of this question in the previous round was not fully comprehensive, and the results were reported only in a single sentence in the Materials and methods. To address these points in full, we have done a series of additional analyses and report these results more extensively in the manuscript and in this reply.</p><p>We start noting that the use of a spatio-temporal searchlight in MEG source analysis is common in the literature (Cao et al., 2019, Giordano, et al., 2017, Kocagoncu et al., 2017, Su et al., 2012), and is analogous to the inclusion of all M/EEG sensors in sensory-based analyses relying on classification methods or RSA analysis (Cichy and Pantazis, 2017, Guggenmos, Sterzer and Cichy, 2018, Kaiser, Azzalini and Peelen, 2016 ). However, and maybe unlike the use of spatial searchlights in many fMRI studies, the spatial component (on the scale of 1.2 cm as used here) in MEG source space is expected to add only minor information, given the natural smoothness of MEG source data. Hence, the emphasis for the present analysis was on the temporal domain.</p><p>Concerning the duration of the chosen time window in the main analysis (500ms), we note that this has been adapted to the specifics of the experimental design and task (Figure 1). In particular, this window was chosen to cover the different target words as much as possible without including the following word in the sentence (which is different in every sentence and would therefore have contaminated the decoding analysis). Spoken words are temporally extended, and to study their cerebral encoding it makes sense to use a time window that covers most of the stimulus duration. Otherwise one runs the risk of capturing processes related to lexical exploration/competition or word predictions (Klimovich-Gray et al., 2019, Kocagoncu, et al., 2017, Marslen-Wilson and Welsh, 1978). We therefore chose a longer time window. This is possibly in contrast to studies on visual object encoding, where stimuli are often flashed for a few tens of milliseconds only, and the encoding window often extends this by a certain, but ambiguous amount. In contrast, the choice of a 500-ms window here is parsimonious given the nature of the stimuli and task.</p><p>Importantly, by task design, the sentence continued beyond the target word, which was the 2nd or 3rd last word in the sentence (Figure 1). Hence, the sentence stimuli continued beyond the analysis window (for 1.48 s to 2.81s (1.98 ± 0.31 s [<italic>M</italic> ± <italic>SD</italic>])) and any motor response, or the relevant response options presented to the participants, followed much later. Using this design, we ensured that motor preparation is very unlikely to emerge within the analysed time window.</p><p>To address this comment using data analysis, we first compared the classification performance with and without the spatial dimension in the searchlight. As noted above, the natural expectation is that the spatial dimension adds only little additional information in comparison to the time domain. In the additional analysis, we quantified whether and by how much the spatial dimension adds to the ability to classify word identities (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>). The average classification performance (within the 10% grid points with the highest performance in the classification including searchlight) differed little (auditory: 27.19 ± 0.48% [M ± SD] with searchlight vs 26.72 ± 1.89% without searchlight; visual: 28.71 ± 1.55% with searchlight vs 27.71 ± 2.44% without searchlight, [averages across grid points and participants]; this equals an overall percent change in performance of -1.49% and -2.45% in the auditory and visual condition, respectively). A correlation between the full-brain maps with and without spatial dimension showed that 99.8% of grid points in the auditory, and 99.7% of grid points in the visual condition, were significantly correlated (Pearson correlation, at p &lt;.05, FDR-corrected). Finally, a direct group-level comparison revealed that the majority of grid points (66.9% in the auditory and 63.9% in the visual condition) showed evidence for no difference (i.e. BF<sub>10</sub> &lt; 1/3) between including or excluding the spatial dimension (see <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>, bottom panel). This shows that the spatial component contributes only modestly to the classification performance. This result is now reported in the manuscript as follows:</p><p>“We also probed the influence of the spatial searchlight by i) including each neighbouring spatial grid point into the searchlight, or ii) averaging across grid points, and iii) by not including a searchlight altogether. Ignoring the spatial pattern by averaging grid points led to a small drop in classification performance (individual vs average grid points: auditory, 27.19 ± 0.48 vs 26.72 ± 0.67; visual, 28.71 ± 1.55 vs 27.71 ± 1.25 [M ± SD]). Performance also dropped slightly when no searchlight was included (auditory, 26.77 ± 1.93; visual, 27.86 ± 2.50 [M ± SD]).”</p><fig id="sa2fig2"><label>Author response image 2.</label><caption><title>Word classification performance with and without a spatial searchlight.</title><p>Top panel: original results including a 1.2-cm searchlight (as in Figure 2 in the manuscript). Middle panel: classification results without searchlight. Bottom panel: Bayes factors of a group-level t-test comparing classification performance with and without searchlight. The majority of grid points (66.9% in the auditory and 63.9% in the visual condition) showed evidence for no difference (i.e. BF10 &lt; 1/3) between tests, while only a small fraction of grid points show evidence for a strong or substantial difference between test. There is no systematic improvement when including the searchlight.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-resp-fig2-v2.tif"/></fig><p>To address the question of how the temporal discretisation of MEG activity within this time window affects classification performance, we compared classifiers operating on data binned at 20, 40, and 60 ms bins. These results had already been reported in the previous reply letter and were included in the Materials and methods section. To give more emphasis to these results we have moved them to a separate section (<italic>“</italic>Selection of parameters and classifier procedures<italic>”</italic>).</p><p>In brief, we find that a temporal resolution of 20 ms is sufficient to recover most of the stimulus information contained in the data. Using larger windows would lead to a loss of information, while shorter windows did not seem to add any classification performance.</p><p>To address the concern that some form of non-specific temporal entrainment of brain activity may confound our results, we implemented a further analysis using shorter time windows. We divided the original 500-ms window into shorter time epochs, as suggested by the reviewer. The length of these (140 ms) was chosen to avoid contributions of rhythmic activity at the critical time scales of 2 – 4 Hz, which have prominently been implied in speech-to-brain entrainment (e.g. Ding and Simon, 2014, Luo, Liu and Poeppel, 2010, Molinaro and Lizarazu, 2017). We repeated the word classification (and the prediction of comprehension) in 7 partly overlapping (by 60 ms) epochs of 140 ms duration. We then subjected the epoch-specific results to the same full-brain cluster-based permutation statistics as used for the full 500-ms window. Finding significant word classification (or prediction of comprehension) in these shorter epochs would speak against the notion that some sort of entrainment critically contributes to (or confounds) our results.</p><p>Before presenting this result, we note that introducing 7 additional time epochs adds to the problem of correcting for multiple comparisons. The results of such a fine-grained analysis can either be considered at a much more stringent criterion (when correcting across all 7 time epochs) or a less stringent criterion (when not correcting, and hence accepting a sevenfold higher false positive rate) compared to the main analysis in the manuscript. We here chose to correct for multiple tests by using Bonferroni correction and adjusting the significance threshold by <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>ɑ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>0.05</mml:mn><mml:mn>7</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.0071</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p><xref ref-type="fig" rid="sa2fig3">Author response image 3</xref> presents a cumulative whole-brain histogram of the significant grid points across all 7 epochs and the epoch-specific number of grid points that are significant in each epoch.</p><fig id="sa2fig3"><label>Author response image 3.</label><caption><title>Classification performance and neurobehavioural prediction over time.</title><p>Top panels represent cumulative whole-brain histograms of the significant grid points across all 7 epochs and bottom panels represent the epoch-specific number of grid points that are significant in each epoch. Please note that these results are very conservative due to the Bonferroni-corrected threshold of ɑ = 0.0071. The maps resemble those of the full-window analysis presented in the manuscript.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56972-resp-fig3-v2.tif"/></fig><p>These results provide several important insights: First, they confirm that significant word classification (and prediction of comprehension) can be obtained in shorter epochs, arguing against temporally entrained brain activity presenting some critical confounding factor. Second, the results show that the significant grid points obtained (cumulatively) across epochs cover largely the same regions found using the full time window. Out of those grid points reported in Figures 2 and 3, the percentage of grid points that becomes significant in at least one time epoch is: <italic>forword classification</italic>: auditory 44.7% of grid points, visual 60.3%; for the <italic>neurobehavioural analysis</italic>: auditory 19.2% of grid points, visual 24.4%. Note that without the very strict Bonferroni-correction, a much larger frequency of original grid points is also found in the short time windows (<italic>classification</italic>: 77.3%/82.3% and <italic>neurobehavioural</italic>: 57.9%/77.2%; auditory/visual).</p><p>Third, the fraction of grid points being significant in at least one time epoch but not significant in the analysis of the full time window, and hence emerging <italic>only</italic> in the analysis of shorter time epochs, is small: 9.2%/5.2% for auditory/visual <italic>word classification</italic>, 4.6%/2.7% for auditory/visual <italic>neurobehaviouralprediction</italic>.</p><p>These results suggest that the use of the full 500-ms time window can be justified: first, this time window covers a large proportion of target words relevant for the behavioural task and is therefore directly motivated given the experimental design. Second, it is sufficiently separated from the motor response (c.f. Figure 1 and Materials and methods). Third, it is not confounded by temporally entrained activity at 2 – 4 Hz (<xref ref-type="fig" rid="sa2fig3">Author response image 3</xref>). And finally, the use of shorter time epochs reveals largely the same brain regions (<xref ref-type="fig" rid="sa2fig3">Author response image 3</xref>). However, in contrast to the full time window, the length of any shorter window will always remain arbitrary, hence necessitating one more arbitrary choice in the analysis pipeline.</p><disp-quote content-type="editor-comment"><p>Finally, it is somewhat unclear how spatial and temporal information are combined in the current classifier. Figure 1—figure supplement 2 creates the impression that the time-series for each vertex within a spotlight were simply concatenated. However, this would conflate within-vertex (temporal) and across-vertex (spatial) variance.</p></disp-quote><p>As is common in classification or RSA analyses based on spatio-temporal activity patterns, we indeed concatenated the time series obtained at the different grid points within each spatial neighbourhood. The same procedure is often used in studies using fMRI voxel based analysis and MEG/EEG source level analyses (Cao, et al., 2019, Giordano, et al., 2017, Kocagoncu, et al., 2017, Su, et al., 2012). While this indeed conflates spatial and temporal information, we note that the 1.2-cm radius of the spatial searchlight still retains a high level of spatial information in the overall source map. Most importantly, and as shown by the above analyses, only a small amount of extra information is contained in the spatial pattern, and hence this mixing of spatial and temporal dimensions did not influence our results heavily.</p><disp-quote content-type="editor-comment"><p>3) The concern that the classifier could conceivably index factors influencing &quot;accuracy&quot; rather than the perceived stimulus does not appear to be addressed sufficiently. Indeed, the classifier is referred to as identifying &quot;sensory representations&quot; throughout the manuscript, when it could just as well identify areas involved in any other functions (e.g., attention, motor function) that would contribute to accurate behavioral performance. This limitation should be acknowledged in the manuscript. The authors could consider using the timing of decoding accuracy in different areas to disambiguate these explanations.</p></disp-quote><p>We agree that other factors enhancing the cerebral processes involved in encoding the sensory input and translating this into a percept (e.g. attention) could affect behaviour. At the same time, we designed the paradigm to specifically avoid confounding factors, such as motor preparation, by placing the target words not at the end of the sentence (c.f. Figure 1; Materials and methods). In the revised manuscript we discuss this as follows:</p><p>“While we found strongest word classification performance in sensory areas, significant classification also extended into central, frontal and parietal regions. This suggests that the stimulus-domain classifier used here may also capture processes potentially related to attention or motor preparation. While we cannot rule out that levels of attention differed between conditions, we ensured by experimental design that comprehension performance did not differ between modalities. In addition, the relevant target words were placed not at the end of the sentence to prevent motor planning and preparation during their presentation (see Stimuli).”</p><p>The idea to investigate the time courses within specific brain areas is interesting, but opens a very large number of additional degrees of freedom. The above presented analysis of small time-windows did this to some extent. We would like to refrain from a complete analysis of decoding over time in different brain areas, as this is a different research question than the one the study was designed to answer.</p><disp-quote content-type="editor-comment"><p>The authors state in their response that classifying based on the participant's reported stimulus (rather than response accuracy) could &quot;possibly capture representations not related to speech encoding but relevant for behaviour only (e.g. pre-motor activity). These could be e.g. brain activity that leads to perceptual errors based on intrinsic fluctuations in neural activity in sensory pathways, noise in the decision process favouring one alternative response among four choices, or even noise in the motor system that leads to a wrong button press without having any relation to sensory representations at all.&quot;</p><p>But, it seems that all of these issues would also effect the accuracy-based classifier as well. Moreover, it seems that intrinsic fluctuations in sensory pathways, or possibly noise in the decision process, are part of what the authors are after. If noise in a sensory pathway can be used to predict particular inaccurate responses, isn't that strong evidence that it encodes behaviorally-relevant sensory representations? For example, intrinsic noise in V1 has been found to predict responses in a simple visual task in non-human primates, with false alarm trials exhibiting noise patterns that are similar to target responses (Seidemann and Geisler 2018). Showing accurate trial-by-trial decoding of participants' incorrect responses could similarly provide stronger evidence that a certain area contributes to behavior.</p></disp-quote><p>Unfortunately, we are not sure what “accuracy-based classifier” here refers to. The classifier used in the present study classifies the word “identity”, and operates in the stimulus domain, not in the domain of participants' response or accuracy. The evidence contained in this stimulus-domain classifier is then used, in a regression model, as a predictor of participants' response accuracy. In the response to the previous comments, we contrasted this approach with a putative analysis based on a classifier directly trained on participant’s choice; such an analysis had been mentioned in the previous set of reviewer comments (“It seems the most natural way to perform such an analysis would be to train a classifier to predict participants' trial-wise responses” to quote from the previous set of comments). We have tried to make our approach clearer early in the manuscript:</p><p>“Using multivariate classification, we quantified how well the single-trial identity of the target words (18 target words, each repeated 10 times) could be correctly predicted from source-localised brain activity (“stimulus classifier”).”</p><p>As we argued previously, we believe that our analysis has several strengths in contrast to this suggestion of a choice-based classifier. In particular, the argument that “these issues” (quoted from this reviewer’s point above) affect our analysis as well, does not seem clear to us. For example, a classifier trained on choice would contain information about activity patterns that drive behaviour (erroneously) on trials where participants missed the stimulus. In this case, their sensory cortices would not encode these and overt behaviour would be driven by noise somewhere in the decision or motor circuits. In contrast, an analysis capitalising first on the encoding of the relevant sensory information (by a stimulus-domain classifier), and then using a signature of how well this is encoded in cerebral activity, avoids being confounded by decision or motor noise. The reasoning behind our approach is very much in line with recent suggestions made by various groups for how to best elucidate the sensory representations that drive perception and comprehension, based on an analysis that in a first stage chiefly capitalises on the encoding of sensory information (Grootswagers, Cichy, and Carlson, 2018, Panzeri et al., 2017) and then relates this to behavioural performance. We refined the manuscript to ensure this is very clearly spelled out in different places, for example:</p><p>“This approach directly follows the idea to capture processes related to the encoding of external (stimulus-driven) information and to then ask whether these representations correlate over trials with the behavioural outcome or report.”</p><p>Of course, no technical approach is perfect, and the statement about noise in a sensory pathway touches upon an interesting issue (“If noise in a sensory pathway can be used to predict particular inaccurate responses, isn't that strong evidence that it encodes behaviourally-relevant sensory representations?” quoted from the present set of comments). Indeed, for our approach it does not matter where precisely variations in noise in the encoding process emerge, as long these affect either the cerebral reflection of sensory information or how this relates on a trial-by-trial basis to behaviour. However, by design of our paradigm (4 alternative response choices) and the type of classifier used (c.f. Materials and methods), the present analysis is capitalising on the correct encoding of the sensory information, while we cannot specifically link the incorrect encoding of a stimulus with behaviour. This arises because evidence against the correct stimulus is not directly evidence in favour of one specific other stimulus; this is in contrast to studies using a two-alternative forced choice design, where evidence against one stimulus / response option directly translates into evidence in favour of the other. We have revised the text to directly reflect this limitation of the present approach:</p><p>“In addition, by design of our experiment (4 response options) and data analysis, the neurobehavioral analysis was primary driven by trials in which the respective brain activity encoded the sensory stimulus correctly. We cannot specifically link the incorrect encoding of a stimulus with behaviour. This is in contrast to studies using only two stimulus or response options, where evidence for one option directly provides evidence against the other (Frühholz et al., 2016, Petro et al., 2013).”</p><p><bold>References</bold></p><p>Cao, Y., Summerfield, C., Park, H., Giordano, B. L., and Kayser, C. (2019). Causal inference in the multisensory brain. Neuron, 102(5), 1076-1087. e1078.</p><p>Cheung, C., Hamilton, L. S., Johnson, K., and Chang, E. F. (2016). The auditory representation of speech sounds in human motor cortex. eLife, 5, e12577.</p><p>Cichy, R. M., and Pantazis, D. (2017). Multivariate pattern analysis of MEG and EEG: A comparison of representational structure in time and space. NeuroImage, 158, 441-454.</p><p>Ding, N., and Simon, J. Z. (2014). Cortical entrainment to continuous speech: functional roles and interpretations. Front Hum Neurosci, 8, 311. doi: 10.3389/fnhum.2014.00311</p><p>Frühholz, S., Van Der Zwaag, W., Saenz, M., Belin, P., Schobert, A.-K., Vuilleumier, P., and Grandjean, D. (2016). Neural decoding of discriminative auditory object features depends on their socio-affective valence. Social cognitive and affective neuroscience, 11(10), 1638-1649.</p><p>Giordano, B. L., Ince, R. A. A., Gross, J., Schyns, P. G., Panzeri, S., and Kayser, C. (2017). Contributions of local speech encoding and functional connectivity to audio-visual speech perception. elife, 6. doi: 10.7554/eLife.24763</p><p>Grootswagers, T., Cichy, R. M., and Carlson, T. A. (2018). Finding decodable information that can be read out in behaviour. Neuroimage, 179, 252-262.</p><p>Guggenmos, M., Sterzer, P., and Cichy, R. M. (2018). Multivariate pattern analysis for MEG: a comparison of dissimilarity measures. Neuroimage, 173, 434-447.</p><p>Guitard, D., and Cowan, N. (2020). Do we use visual codes when information is not presented visually? Memory and Cognition.</p><p>Haxby, J. V., Connolly, A. C., and Guntupalli, J. S. (2014). Decoding neural representational spaces using multivariate pattern analysis. Annual review of neuroscience, 37, 435-456.</p><p>Kaiser, D., Azzalini, D. C., and Peelen, M. V. (2016). Shape-independent object category responses revealed by MEG and fMRI decoding. Journal of neurophysiology, 115(4), 2246-2250.</p><p>Kamitani, Y., and Tong, F. (2005). Decoding the visual and subjective contents of the human brain. Nature neuroscience, 8(5), 679-685.</p><p>Karas, P. J., Magnotti, J. F., Metzger, B. A., Zhu, L. L., Smith, K. B., Yoshor, D., and Beauchamp, M. S. (2019). The visual speech head start improves perception and reduces superior temporal cortex responses to auditory speech. eLife, 8.</p><p>Kimel, E., Ahissar, M., and Lieder, I. (2020). Capacity of short-term memory in dyslexia is reduced due to less efficient utilization of items' long-term frequency. bioRxiv.</p><p>Klimovich-Gray, A., Tyler, L. K., Randall, B., Kocagoncu, E., Devereux, B., and Marslen-Wilson, W. D. (2019). Balancing prediction and sensory input in speech comprehension: The spatiotemporal dynamics of word recognition in context. Journal of Neuroscience, 39(3), 519-527.</p><p>Kocagoncu, E., Clarke, A., Devereux, B. J., and Tyler, L. K. (2017). Decoding the cortical dynamics of sound-meaning mapping. Journal of Neuroscience, 37(5), 1312-1319.</p><p>Luo, H., Liu, Z., and Poeppel, D. (2010). Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation. PLoS Biol, 8(8), e1000445. doi: 10.1371/journal.pbio.1000445</p><p>Marslen-Wilson, W. D., and Welsh, A. (1978). Processing interactions and lexical access during word recognition in continuous speech. Cognitive psychology, 10(1), 29-63.</p><p>Mazor, M., Friston, K. J., and Fleming, S. M. (2020). Distinct neural contributions to metacognition for detecting, but not discriminating visual stimuli. eLife, 9, e53900.</p><p>Mihai, P. G., Moerel, M., de Martino, F., Trampel, R., Kiebel, S., and von Kriegstein, K. (2019). Modulation of tonotopic ventral medial geniculate body is behaviorally relevant for speech recognition. eLife, 8.</p><p>Molinaro, N., and Lizarazu, M. (2017). Delta (but not theta)-band cortical entrainment involves speech-specific processing. European Journal of Neuroscience, 48(7), 2642-2650. doi: doi:10.1111/ejn.13811</p><p>Ozker, M., Yoshor, D., and Beauchamp, M. S. (2018). Frontal cortex selects representations of the talker’s mouth to aid in speech perception. eLife, 7, e30387.</p><p>Panzeri, S., Harvey, C. D., Piasini, E., Latham, P. E., and Fellin, T. (2017). Cracking the neural code for sensory perception by combining statistics, intervention, and behavior. Neuron, 93(3), 491-507.</p><p>Petro, L. S., Smith, F. W., Schyns, P. G., and Muckli, L. (2013). Decoding face categories in diagnostic subregions of primary visual cortex. European Journal of Neuroscience, 37(7), 1130-1139.</p><p>Puvvada, K. C., and Simon, J. Z. (2017). Cortical representations of speech in a multitalker auditory scene. Journal of Neuroscience, 37(38), 9189-9196.</p><p>Ritchie, J. B., Kaplan, D. M., and Klein, C. (2019). Decoding the brain: Neural representation and the limits of multivariate pattern analysis in cognitive neuroscience. The British Journal for the Philosophy of Science, 70(2), 581-607.</p><p>Ritchie, J. B., Tovar, D. A., and Carlson, T. A. (2015). Emerging object representations in the visual system predict reaction times for categorization. PLoS computational biology, 11(6), e1004316.</p><p>Rouder, J. N., Morey, R. D., Speckman, P. L., and Province, J. M. (2012). Default Bayes factors for ANOVA designs. Journal of Mathematical Psychology, 56(5), 356-374.</p><p>Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., and Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. Psychonomic bulletin and review, 16(2), 225-237.</p><p>Schönbrodt, F. D., and Wagenmakers, E.-J. (2018). Bayes factor design analysis: Planning for compelling evidence. Psychonomic bulletin and review, 25(1), 128-142.</p><p>Su, L., Fonteneau, E., Marslen-Wilson, W., and Kriegeskorte, N. (2012). Spatiotemporal searchlight representational similarity analysis in EMEG source space. Paper presented at the 2012 Second International Workshop on Pattern Recognition in NeuroImaging.</p></body></sub-article></article>