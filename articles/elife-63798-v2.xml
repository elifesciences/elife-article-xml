<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63798</article-id><article-id pub-id-type="doi">10.7554/eLife.63798</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Clustered functional domains for curves and corners in cortical area V4</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-210282"><name><surname>Jiang</surname><given-names>Rundong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9217-0749</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-112399"><name><surname>Andolina</surname><given-names>Ian Max</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9985-3414</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-105553"><name><surname>Li</surname><given-names>Ming</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5173-1602</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-100693"><name><surname>Tang</surname><given-names>Shiming</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0294-3259</contrib-id><email>tangshm@pku.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Peking University School of Life Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Peking-Tsinghua Center for Life Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution>IDG/McGovern Institute for Brain Research at Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution>Key Laboratory of Machine Perception (Ministry of Education), Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution>The Center for Excellence in Brain Science and Intelligence Technology, State Key Laboratory of Neuroscience, Key Laboratory of Primate Neurobiology, Institute of Neuroscience, Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution>Beijing Normal University Faculty of Psychology</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Vinck</surname><given-names>Martin</given-names></name><role>Reviewing Editor</role><aff><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with Max Planck Society</institution><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>05</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e63798</elocation-id><history><date date-type="received" iso-8601-date="2020-10-07"><day>07</day><month>10</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-05-16"><day>16</day><month>05</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Jiang et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Jiang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63798-v2.pdf"/><abstract><p>The ventral visual pathway is crucially involved in integrating low-level visual features into complex representations for objects and scenes. At an intermediate stage of the ventral visual pathway, V4 plays a crucial role in supporting this transformation. Many V4 neurons are selective for shape segments like curves and corners; however, it remains unclear whether these neurons are organized into clustered functional domains, a structural motif common across other visual cortices. Using two-photon calcium imaging in awake macaques, we confirmed and localized cortical domains selective for curves or corners in V4. Single-cell resolution imaging confirmed that curve- or corner-selective neurons were spatially clustered into such domains. When tested with hexagonal-segment stimuli, we find that stimulus smoothness is the cardinal difference between curve and corner selectivity in V4. Combining cortical population responses with single-neuron analysis, our results reveal that curves and corners are encoded by neurons clustered into functional domains in V4. This functionally specific population architecture bridges the gap between the early and late cortices of the ventral pathway and may serve to facilitate complex object recognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>curve/corner domain</kwd><kwd>V4</kwd><kwd>ventral visual pathway</kwd><kwd>macaque</kwd><kwd>two-photon imaging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31730109</award-id><principal-award-recipient><name><surname>Tang</surname><given-names>Shiming</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012166</institution-id><institution>National Basic Research Program of China</institution></institution-wrap></funding-source><award-id>2017YFA0105201</award-id><principal-award-recipient><name><surname>Tang</surname><given-names>Shiming</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>U1909205</award-id><principal-award-recipient><name><surname>Tang</surname><given-names>Shiming</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Beijing Municipal Commission of Science and Technology</institution></institution-wrap></funding-source><award-id>Z181100001518002</award-id><principal-award-recipient><name><surname>Tang</surname><given-names>Shiming</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Peking-Tsinghua Center for Life Sciences</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Tang</surname><given-names>Shiming</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Curves and corners are separately encoded by neurons clustered into functional domains in macaque V4.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The visual system faces the daunting task of combining highly ambiguous local patterns of contrast into robust, coherent, and spatially extensive complex object representations (<xref ref-type="bibr" rid="bib9">Connor et al., 2007</xref>; <xref ref-type="bibr" rid="bib21">Haxby et al., 1991</xref>; <xref ref-type="bibr" rid="bib38">Mishkin et al., 1983</xref>). Such information is predominantly processed along the ventral visual pathway (areas V1, V2, V4, and inferotemporal cortex [IT]). At early stages of this cortical pathway, neurons are tuned to a local single orientation (<xref ref-type="bibr" rid="bib24">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib25">Hubel and Wiesel, 1968</xref>) or a combination of orientations (<xref ref-type="bibr" rid="bib1">Anzai et al., 2007</xref>; <xref ref-type="bibr" rid="bib26">Ito and Komatsu, 2004</xref>). Orientation responses are functionally organized into iso-orientation domains that form pinwheel structures in V1 (<xref ref-type="bibr" rid="bib58">Ts'o et al., 1990</xref>). At later stages like IT, neurons are selective for complex objects, predominantly organized categorically (<xref ref-type="bibr" rid="bib12">Desimone et al., 1984</xref>; <xref ref-type="bibr" rid="bib15">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib16">Fujita et al., 1992</xref>; <xref ref-type="bibr" rid="bib31">Kobatake and Tanaka, 1994</xref>; <xref ref-type="bibr" rid="bib59">Tsao et al., 2003</xref>; <xref ref-type="bibr" rid="bib60">Tsao et al., 2006</xref>). Such complex object organization is embodied using combinations of structurally separated feature columns (<xref ref-type="bibr" rid="bib16">Fujita et al., 1992</xref>; <xref ref-type="bibr" rid="bib49">Rajalingham and DiCarlo, 2019</xref>; <xref ref-type="bibr" rid="bib54">Tanaka, 2003</xref>; <xref ref-type="bibr" rid="bib61">Tsunoda et al., 2001</xref>; <xref ref-type="bibr" rid="bib63">Wang et al., 1996</xref>). Positioned in-between the local orientation architecture of V1 and the global object architecture of IT lies cortical area V4, exhibiting visual selectivity that demonstrates integration of simple-towards-complex information (<xref ref-type="bibr" rid="bib45">Pasupathy et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Roe et al., 2012</xref>; <xref ref-type="bibr" rid="bib68">Yue et al., 2014</xref>), and extensive anatomical connectivity across the visual hierarchy (<xref ref-type="bibr" rid="bib20">Gattass et al., 1990</xref>; <xref ref-type="bibr" rid="bib62">Ungerleider et al., 2008</xref>).</p><p>Functional organization within V4 has previously been visualized by intrinsic signal optical imaging (ISOI), and cortical representations of low-level features for orientation, color, and spatial frequency have been systematically demonstrated (<xref ref-type="bibr" rid="bib10">Conway et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Li et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Li et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Lu et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Tanigawa et al., 2010</xref>). Such functional clustering suggests that the intracortical organizational motifs in V4 bear some similarity to V1. It remains unknown how more complex feature-selective neurons in V4 are spatially organized, and whether feature-like columns found in IT also exist in V4. Because intrinsic imaging is both spatially and temporally limited, it is unable to measure selective responses of single neurons. Using electrophysiology, early studies in V4 using bar and grating stimuli found that V4 neurons are tuned for orientation, size, and spatial frequency (<xref ref-type="bibr" rid="bib13">Desimone and Schein, 1987</xref>). Subsequent studies revealed V4 selectivity for complex gratings and shapes in natural scenes (<xref ref-type="bibr" rid="bib11">David et al., 2006</xref>; <xref ref-type="bibr" rid="bib17">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="bib31">Kobatake and Tanaka, 1994</xref>). In particular, Gallant and colleagues discovered V4 neurons with significant preferences for concentric, radial, and hyperbolic gratings (<xref ref-type="bibr" rid="bib17">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="bib18">Gallant et al., 1996</xref>). Neurons with similar preferences were spatially clustered when reconstructing the electrophysiological electrode penetrations (<xref ref-type="bibr" rid="bib18">Gallant et al., 1996</xref>). These results were extended by later studies confirming the systematic tuning of V4 neurons for shape segments such as curves and corners as well as combination of these segments using parametric stimulus sets consisting of complex shape features (<xref ref-type="bibr" rid="bib6">Cadieu et al., 2007</xref>; <xref ref-type="bibr" rid="bib8">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Oleskiw et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Pasupathy and Connor, 1999</xref>; <xref ref-type="bibr" rid="bib47">Pasupathy and Connor, 2001</xref>; <xref ref-type="bibr" rid="bib48">Pasupathy and Connor, 2002</xref>). Temporally varying heterogeneous fine-scale tuning within the spatial-temporal receptive field has also been observed (<xref ref-type="bibr" rid="bib40">Nandy et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Nandy et al., 2013</xref>; <xref ref-type="bibr" rid="bib66">Yau et al., 2013</xref>). More recently, artificial neural networks were used to generate complex stimuli that characterize the selectivity of V4 neurons (<xref ref-type="bibr" rid="bib3">Bashivan et al., 2019</xref>). However, whether such complex feature-selective neurons are spatially organized in V4 remains poorly understood.</p><p>In this study, we aimed to confirm the presence of functional domains in V4 encoding complex features such as curves and corners. We utilized two-photon (2P) calcium imaging in awake macaque V4, which provides visualization of the spatial distribution and clustering within the cortical population alongside substantially enhanced spatial resolution for functional characterization at the single-cell level (<xref ref-type="bibr" rid="bib19">Garg et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib41">Nauhaus et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Ohki et al., 2005</xref>; <xref ref-type="bibr" rid="bib53">Seidemann et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Tang et al., 2018</xref>). We scanned a large cortical area in dorsal V4 using a low-power objective lens to search for patches selectively activated by curves or corners. We subsequently imaged these patches using a high-power objective lens to record single neurons’ responses in order to examine whether spatially clustered curve or corner-selective neurons could be found. If such neural clusters were found, we further aimed to understand how different curves and corners are encoded and differentiated in greater detail.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We injected AAV1-hSyn-GCaMP into dorsal V4 (V4d) of two rhesus macaques — GCaMP6f for monkey A and GCaMP5G for monkey B. An imaging window and head posts were implanted 1–2 months after viral injection (see Materials and methods). Subjects were trained to initiate and maintain fixation within a 1° circular window for 2 s: the first second contained the fixation spot alone, and then stimuli appeared for 1 s on a LCD monitor positioned 45 cm away (17 inch, 1280 × 960 pixel, 30 pixel/°). Neuronal responses were recorded using 2P calcium imaging, with differential images generated using ΔF = F – F0, where F0 is the average fluorescence 0.5–0 s before stimulus onset, and F is the average response 0.5–1.25 s after stimulus onset.</p><sec id="s2-1"><title>Cortical mapping of curve-biased and corner-biased patches in V4</title><p>We first identified the retinal eccentricity using drifting gratings for our sites and found they were positioned with an eccentricity of ~0.7° from the fovea in monkey A and ~0.9° in monkey B. We next used a low-power (4×) objective lens to identify and localize any cortical subregions selectively activated by curves or corners. Using a large range of contour feature stimuli including bars, curves, and corners (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), we scanned a large area (3.4 × 3.4 mm) in V4d (<xref ref-type="fig" rid="fig1">Figure 1B, C</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) between the lunate sulcus (LS) and the terminal portion of the inferior occipital sulcus (IOS). We obtained global activation maps by Gaussian smoothing (standard deviation σ = 10 pixels, 67 μm) the ΔF/F0 maps. We observed that orientation is organized in linear iso-orientation domains or pinwheel-like patterns, as previously reported (<xref ref-type="bibr" rid="bib50">Roe et al., 2012</xref>), using ISOI in V4 (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Cortical mapping of curve-biased and corner-biased patches in V4 using a 4× objective lens.</title><p>(<bold>A</bold>) The stimulus set used for initial cortical mapping consisting of bars, corners, and smooth curves. (<bold>B</bold>) Vascular map. LS: lunate sulcus; IOS: inferior occipital sulcus. The black box indicates the imaging site in each subject. (<bold>C</bold>) Two-photon fluorescence images of the two monkeys. Scale bar = 400 μm. (<bold>D</bold>) Left: subtraction map showing curve-selective activation in monkey A, derived by the average response (ΔF/F0) to all curves minus the average response to all other stimuli (corners and bars). Right: subtraction map showing corner-selective activation in monkey A. (<bold>E</bold>) The equivalent of (<bold>D</bold>) for monkey B. (<bold>F</bold>) Left: significant curve patches in monkey A. For each pixel, independent t-tests were performed to compare the responses to all curves against all corners and against all bars. Benjamini-Hochberg procedure was used to compute the pixel FDR (false discovery rate, see Materials and methods). Threshold q = 0.01. The white box indicates the imaging site selected for 16× objective single-cell mapping. Right: significant corner patches in monkey A. (<bold>G</bold>) The equivalent of (<bold>F</bold>) for monkey B.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Two-photon fluorescence images.</title><p>The numbers indicate the average fluorescence of the areas marked by the red boxes (normalized).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Pseudo-color orientation map obtained by 4× imaging.</title><p>Scale bar = 400 μm. For each pixel, the preferred orientation was derived by the vector summation of its responses (Gaussian smoothed ΔF/F0) to eight orientations. Hues in the map indicate the orientations of the sum vectors (preferred orientation), and lightness indicates the length (orientation selectivity). Orientation is organized in linear bands and pinwheels in V4.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Maps of uncorrected p-values (p&lt;0.01), FDR q-values (q &lt; 0.01), and Bonferroni-corrected p-values (p&lt;0.01), before cluster permutation tests.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig1-figsupp3-v2.tif"/></fig></fig-group><p>We then examined the response to curve and corner stimuli. Using map subtraction, we computed the curve-selective activation as the average response (ΔF/F0) to all curves minus all corners and bars, and corner-selective activation as the average response to all corners minus all curves and bars (<xref ref-type="fig" rid="fig1">Figure 1D, E</xref>). The subtraction maps we obtained clearly revealed several possible candidates for curve- or corner-selective patches. To statistically detect and locate the curve and corner patches, we performed pixel-level FDR tests to examine the curve or corner preference. For each pixel, we performed independent t-tests to compare the responses to all curves, all corners, and all bars, obtaining the p-value maps for curve and corner selectivity (see Materials and methods and <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). We then computed the FDR (false dicovery rate) using Benjamini-Hochberg procedure (<xref ref-type="bibr" rid="bib4">Benjamini and Hochberg, 1995</xref>), with the threshold level q = 0.01 to locate the significant patches. Cluster permutation tests were also performed to exclude patches with not enough significant pixels (<xref ref-type="bibr" rid="bib42">Nichols and Holmes, 2002</xref>). We found several patches significantly selective to curves or corners in dorsal V4 (<xref ref-type="fig" rid="fig1">Figure 1F, G</xref>). These curve- or corner-selective patches were considered candidates for functional domains encoding shape segments in V4.</p></sec><sec id="s2-2"><title>Single-cell mapping of curve- and corner-selective neurons reveals they are spatially clustered</title><p>To confirm that neurons within these patches were indeed curve or corner selective, we next performed single-cell resolution imaging with a high-power objective lens (16×) to record neuronal responses (ΔF/F0) as well as their spatial organization (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The imaging sites (850 × 850 μm) in both subjects were chosen to include both curve- and corner-selective domains found by our 4× imaging (<xref ref-type="fig" rid="fig1">Figure 1F, G</xref>). 535 visually responsive neurons (292 from monkey A and 243 from monkey B) were recorded in total. Each stimulus was repeated 10 times and averaged to derive neuronal responses (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). To characterize neurons’ curve and corner selectivity, we calculated a curve selectivity index (CVSI) and corner selectivity index (CNSI). A positive CVSI value indicates a neuron’s maximal response to curves is stronger than its maximal response to other stimuli: a CVSI = 0.33 signifies a response twice as strong, and a CVSI = 0.2 is 1.5 times as strong. The same definition applies to CNSI. 70.5% (74 out of 105) neurons with CVSI &gt; 0.2 significantly (one-way ANOVA, p&lt;0.05) preferred curves over corners and bars, and 76.9% (120 out of 156) for CNSI (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3A, B</xref>). We found neurons with high CVSI or CNSI were spatially clustered (<xref ref-type="fig" rid="fig2">Figure 2A–D</xref>), and these neurons were also selective to the orientation of the integral curves or corners (<xref ref-type="fig" rid="fig2">Figure 2E–H</xref>; 91.6% of the neurons are significantly tuned to the orientation of curves or corners; one-way ANOVA, p&lt;0.05). Their overall spatial distribution was consistent with the spatial distribution of curve and corner domains revealed by 4× imaging (<xref ref-type="fig" rid="fig2">Figure 2A–D</xref> vs. <xref ref-type="fig" rid="fig1">Figure 1E, F</xref>), especially considering the possible loss of detailed spatial information during Gaussian smoothing of 4× images. This parsimoniously suggests that the observed cortical activation was evoked by responsive neuronal clusters.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Single-cell mapping of curve- and corner-selective neurons using a 16× objective lens.</title><p>(<bold>A</bold>) Cell map of curve selectivity index (CVSI). Responsive neurons are labeled at their spatial location and colored according to their CVSI. Neurons with high positive CVSI (high curve preference) were clustered in the upper part of the imaging area. The white line indicates the curve-biased patches derived by 4× imaging (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Scale bar = 100 μm. (<bold>B</bold>) Cell map of corner selectivity index (CNSI). Neurons with high positive CNSI (high corner preference) were clustered in the lower part of the imaging area. (<bold>C, D</bold>) Equivalent maps for monkey B. (<bold>E–H</bold>) Responses of four example neurons preferring curves or corners, their locations labeled in (<bold>A–D</bold>), respectively. (<bold>I</bold>) Neuronal pairwise tuning correlation (mean ± SE, averaging all neurons every 100 μm) plotted against spatial distances. The average correlation between different repeats of same neuron is 0.71 (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). The dash curve indicates the average of neurons when shuffled. Significance levels were determined by permutation test. (<bold>J</bold>) Absolute CVSI value differences (mean ± SE) plotted against distances. (<bold>K</bold>) Absolute CNSI value differences (mean ± SE) plotted against distances.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Single-cell resolution fluorecence imge.</title><p>(<bold>A</bold>) Fluorescence image using a 16× objective lens. (<bold>B</bold>) Raw fluorescence traces of eight neurons (colored circles in <bold>A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Single-neuron responses.</title><p>(<bold>A</bold>) GCaMP timecourses (ΔF/F0, mean ± SE) of the neurons in <xref ref-type="fig" rid="fig2">Figure 2A–D</xref>, each under its optimal curves, corners, and bars. Trials were synchronized so that 0 s indicates the time of stimulus onset. (<bold>B</bold>) Scatterplots showing average neuronal responses to the stimuli (bars, curves, and corners) in the odd repeats (1, 3, 5, 7, 9) and even repeats (2, 4, 6, 8, 10). The red dash line indicates the y = x line. Pearson correlation R &gt; 0.5 for 474 out of total 535 neurons (88.6%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Curve selectivity index (CVSI) and corner selectivity index (CNSI).</title><p>(<bold>A-B</bold>) CVSI and CNSI were tested, comparing the maximum responses to curves against the maximum responses to corners/bars (CVSI), or bars against curves/bars (CNSI). One-way ANOVA, p&lt;0.05, n = 10. (<bold>C-D</bold>) Average CVSI and CNSI (mean ± SE) plotted against the distances to the CV or CN domain boundary.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig2-figsupp3-v2.tif"/></fig></fig-group><p>We next assessed this clustering quantitatively by examining how neuronal responses correlate with spatial distance. For each neuronal pair recorded from the same subject, we computed the pairwise tuning correlation and absolute value differences for CVSI and CNSI plotted against the neuronal pairwise distances. We found that neurons close to each other (&lt;300 μm approximately) often had more correlated tuning (<xref ref-type="fig" rid="fig2">Figure 2I</xref>) and generally exhibited more similar CVSI and CNSI values (<xref ref-type="fig" rid="fig2">Figure 2J, K</xref>). These results indicate curve-selective and corner-selective neurons are spatially clustered, which could potentially form curve domains and corner domains in V4, which could therefore be detected when imaged at a larger scale.</p><p>Out of all 535 neurons recorded from two animals, the majority (346 neurons, 64.7%) significantly preferred curve and corner stimuli over single bars, and only 1.5% (eight neurons) significantly preferred bars over curves and corners (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>), indicating that neurons in these areas were indeed much more likely to encode more complex shape features compared to simple orientation. Therefore, we made a combined cell map to depict curve and corner selectivity (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), neglecting bar responses, by calculating curve/corner index (CVCNI). Similar to CVSI and CNSI, positive CVCNI values indicate a neuron’s maximum response to curves is stronger than its maximum response to corners, and vice versa. As expected, neurons with similar CVCNI values were spatially clustered. Neurons that fell into the 4×-defined curve domains generally had positive CVCNI values (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and those in the 4× corner domains generally had negative CVCNI values (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). We also performed a one-way ANOVA comparing neurons’ maximum curve and corner responses. We found that neurons with CVCNI &gt; 0.2 or &lt;−0.2 (which means 1.5 times as strong) predominantly showed significant preferences (p&lt;0.05) to curves or corners over the other kind (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The curve- or corner-selective neurons (red and blue neurons in <xref ref-type="fig" rid="fig3">Figure 3D</xref>) have very diverse curve or corner tuning, and could be either selective or invariant to the radius and radian of curves or bar length and separation angle of corners (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B–D</xref>), which potentially enables the encoding of multiple shape segments. More interestingly, these neurons that were heavily biased to curves or corners over the other tended to respond very weakly to single bars (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), implying that they might be detecting more complex and integral shape features instead of local orientation. These results suggest that curves and corners are encoded by different neuronal clusters organized in curve and corner domains, and these domains are distinct from those representing single orientations.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Combined maps of curve/corner preference.</title><p>(<bold>A</bold>) Cell map of curve/corner index (CVCNI). Positive CVCNI indicates preference for curves over corners and vice versa. Curve-selective neurons and corner-selective neurons are spatially clustered. Scale bar = 100 μm. (<bold>B</bold>) Histogram of CVCNI for neurons located within the curve-biased domains. Mean = 0.15 ± 0.03 S.E. (<bold>C</bold>) Histogram of CVCNI for neurons located within the corner-biased domains. Mean = −0.20 ± 0.02 S.E. (<bold>D</bold>) Scatterplot of maximum responses to bars (normalized to 0–1 by the maximum responses to all contour features) against CVCNI. Red dots indicate neurons showing significant preference for curves (ANOVA p&lt;0.05, n = 10) and blue for corners. The majority of neurons (74.5%) with CVCNI &lt; −0.2 or &gt;0.2 were significantly selective. Neurons that highly preferred curves over corners or corners over curves did not respond strongly to single-orientated bars. (<bold>E</bold>) Neurons’ maximum bar responses were negatively correlated with the absolute values of CVCNI. The red line represents the linear regression line.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Neurons’ tuning to curves and corners.</title><p>(<bold>A</bold>) Scatterplot of neurons’ maximum responses to curves and corners against bars. 65.7% of 535 neurons recoded in the imaging area showed significantly stronger responses to curves and corners over bars (n = 10, one-way ANOVA, p&lt;0.05). Only 1.5% showed significantly stronger responses to bars over curves and corners. (<bold>B</bold>) Size and separation angle selectivity of corner-selective neurons (147 out of 535 neurons that significantly preferred corners over curves). 62, 52, and 33 neurons preferred 45°, 90°, and 135° corners, respectively. 70 neurons significantly preferred big corners (bar length = 20 pixels, 0.67°), and 20 significantly preferred small corners (bar length = 10 pixels, 0.33°). 57 neurons were size invariant. (<bold>C</bold>) Scatterplot of maximal responses to large (R = 20 pixels, 0.67°) against small curves (R = 10 pixels, 0.33°). 84 out of 535 neurons that significantly preferred curves over corners are included, 22 of which significantly preferred big curves and 12 preferred small curves (n = 10, one-way ANOVA, p&lt;0.05). 50 neurons were size invariant. (<bold>D</bold>) Scatterplot of neurons’ maximum responses to long curves (rad = 180°) against short curves (rad = 120°). Neurons included are the same as (<bold>B</bold>). 35 neurons significantly preferred long curves and 18 preferred small curves. 31 neurons were radian invariant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig3-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Curve-preferring neurons are selective for smoothness</title><p>Curves and corners are both different from single bars in that they potentially contain multiple different local orientations, yet we found them to be encoded by different neuronal clusters in V4. This suggests that V4 neurons are not recognizing shapes with more than one local orientation, but computing a more fundamental feature difference. To investigate what distinguishes curves from corners in V4, we tested hexagonal segments (Π-shape stimuli; <xref ref-type="fig" rid="fig4">Figure 4A</xref>) that highly resemble curves except for a lack of smoothness (<xref ref-type="bibr" rid="bib39">Nandy et al., 2013</xref>). We found that neurons that were very selective to smooth curves did not respond strongly to Π-shape stimuli (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), suggesting that they were selective to smoothness, rather than multiple orientations. In the same way as CVCNI, we calculated curve/Π-shape index (CVPII), which characterizes a neuron’s preference to smooth curves over the Π-shape stimuli. We found that neurons’ CVPII were highly consistent with CVCNI (R = 0.72, p&lt;0.001, <xref ref-type="fig" rid="fig4">Figure 4B</xref>), which means neurons preferring smooth curves over corners would also prefer smooth curves over Π-shape stimuli. As a result, the maps of CVPII were also consistent to CVCNI maps (<xref ref-type="fig" rid="fig4">Figure 4C</xref> vs. <xref ref-type="fig" rid="fig3">Figure 3A</xref>). K-means clustering analysis of population responses also showed that smooth curves are encoded differently from rectilinear shapes including Π-shapes and corners (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Therefore, smoothness is important to the distinct encoding of curves and corners in the specific curve domains and corner domains in V4.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Curve-preferring neurons are selective for smoothness.</title><p>(<bold>A</bold>) Left: responses of an example curve preferring neurons to bars, corners, smooth curves, and Π-shape stimuli, indicated by the white circle in (<bold>C</bold>). The neurons responded strongly to smooth curves but not to Π-shape, which highly resemble curves despite lack of smoothness. Right: an example neuron responding to rectilinear corners and Π-shapes, indicated by the white square in (<bold>C</bold>). (<bold>B</bold>) Scatterplot of curve/corner index (CVCNI) against curve/Π-shape index (CVPII), which characterizes neuronal preference for smooth curves over Π-shape stimuli. The red dash line represents the linear regression line. The two values were highly correlated, indicating that neurons preferring curves over corners also preferred curves over Π-shape stimuli. (<bold>C</bold>) Cell map of CVPII. Scale bar = 100 μm. Neurons are clustered similarly to CVCNI (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>K-means clustering analysis.</title><p>535 neurons’ responses to 20 stimulus forms (two bars, eight curves, six corners, and four Π-shapes, each at eight orientations) were used. (<bold>A</bold>) We clustered the stimulus forms using population response vector (see Materials and methods). Cluster number = 2 according to Calinski–Harabasz criterion. (<bold>B</bold>) The two clusters were visualized using multi-dimensional scaling (MDS). (<bold>C</bold>) The normalized stress of MDS. Stress = 0.104 when dimensionality = 2. (<bold>D</bold>) We clustered the neurons using neuron response vector (see Materials and methods). Cluster number = 2 according to Calinski–Harabasz criterion. (<bold>E</bold>) The two neural clusters are also spatially clustered and are consistent with curve/corner index (CVCNI) maps (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig4-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Curve and corner selectivity is related to concentric and radial grating preference</title><p>Early studies in V4 demonstrated that many V4 neurons are selective for non-Cartesian gratings (<xref ref-type="bibr" rid="bib11">David et al., 2006</xref>; <xref ref-type="bibr" rid="bib17">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="bib18">Gallant et al., 1996</xref>). While concentric gratings highly resemble curves and radial gratings resemble corners, this result highly implied the curve/corner preference. Therefore, we wondered whether these two types of gratings are also separately encoded by neurons in curve and corner domains. So in addition to contour feature stimuli, we also tested concentric, radial, and Cartesian gratings (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>). The resultant selectivity maps were consistent with the contour feature maps as predicted. 48.4% of the neurons recorded in the imaging areas significantly preferred concentric or radial gratings over Cartesian gratings, while only 2.2% significantly preferred Cartesian gratings (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>). In addition, many of them were heavily biased to one over the other. Similar to CVCNI, we computed concentric/radial index (CRI) to characterize this bias. CRI and CVCNI values were found to be correlated (R = 0.38, p&lt;0.001; <xref ref-type="fig" rid="fig5">Figure 5B,</xref> <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), and naturally their cell maps were also consistent (<xref ref-type="fig" rid="fig5">Figure 5C</xref> vs. <xref ref-type="fig" rid="fig3">Figure 3A</xref>), suggesting that classical polar grating selectivity is closely related to curve and corner selectivity. Meanwhile, to assess whether the observed selectivity is related to different spatial frequencies, we examined the CRI map at 1, 2, and 4 cycle/°. The CRI values of all neurons at three spatial frequencies are highly correlated (Pearson correlation, all R &gt; 0.5, p&lt;0.001), and the map structures were found to remain consistent across three spatial frequencies (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), implying such selectivity is not directly related to spatial frequency.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Concentric and radial gratings preference.</title><p>(<bold>A</bold>) Cell map of concentric/radial index (CRI). Positive CRI indicates preference for concentric over radial gratings and vice versa. Concentric grating-selective neurons and radial grating-selective neurons are spatially clustered, and the overall distribution was consistent with curve/corner selectivity (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Scale bar = 100 μm. (<bold>B</bold>) Scatterplot of curve/corner index (CVCNI) against CRI, which were positively correlated. The red dash line represents the linear regression line. (<bold>C</bold>) CRI cell maps at spatial frequencies of 1, 2, and 4 cycles/° (cpd). The map structure remained consistent.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Responses to Cartesian, concentric, and radial gratings.</title><p>(<bold>A</bold>) Responses (ΔF/F0, mean ± SE) of three example neurons to grating stimuli. (<bold>B</bold>) Scatterplot of neurons’ maximum responses to concentric and radial gratings against Cartesian gratings. 48.4% of 535 neurons recoded in the imaging area showed significantly stronger responses to concentric or radial gratings (red, n = 11 for monkey A, and n = 8 for monkey B, one-way ANOVA, p&lt;0.05). Only 2.2% showed significantly stronger responses Cartesian gratings (blue). Black indicates no significant preference (p≥0.05). (<bold>C</bold>) Histogram of neurons’ optimal Cartesian gratings spatial frequencies. Neuronal responses to its optimal Cartesian gratings at spatial frequencies of 1, 2, and 4 cycle/° were compared using one-way ANOVA (p&lt;0.05).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Concentric/radial index (CRI).</title><p>(<bold>A-B</bold>) CRI were tested, comparing the maximum responses to concentric gratings against the maximum responses to radial gratings. One-way ANOVA, p&lt;0.05, n = 10. (<bold>C-D</bold>) Average CRI and -CRI (mean ± SE) plotted against the distances to the CV or CN domain boundary.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-fig5-figsupp2-v2.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using 2P calcium imaging, we identified cortical patches in macaque V4d selective for curves or corners (<xref ref-type="fig" rid="fig1">Figure 1E, F</xref>), with individual curve- and corner-selective neurons consistently clustered spatially (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). These neurons exhibited diverse curve or corner selectivity (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B–D</xref>) and could potentially be involved in the encoding and processing of a large variety of curves and corners. These results demonstrate the existence of functionally specific curve and corner domains in V4d.</p><p>Functional organization for low-order orientation and spatial frequency representations in macaque V4 had previously been visualized using ISOI (<xref ref-type="bibr" rid="bib37">Lu et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Roe et al., 2012</xref>). For more complex shape features, very few studies have been carried out in V4 to characterize its functional organization, let alone at single-cell resolution. We report here the existence of cortical micro-domains consisting almost entirely of neurons selective for curves. This finding at the single-cell level is consistent with an fMRI study that reported curvature-biased patches in macaque V4 (<xref ref-type="bibr" rid="bib68">Yue et al., 2014</xref>). The patches we found were smaller in size (about 300 μm) than those observed using fMRI; we suspect due to the improved spatial resolution afforded by 2P imaging. Additionally, we also found cortical domains in V4d selective for corners. Apart from this fMRI study, one of the reasons why exactly the curve/corner contrast was used to study functional domains in V4 was that in our recent study using natural images we found smooth curves and rectilinear corners to be one of the dominant features encoded by many V4 neurons (<xref ref-type="bibr" rid="bib27">Jiang et al., 2019</xref>). Here, we directly demonstrated and visualized the combined functional organization of smooth curves and rectilinear corners in V4 at both cortical and single-cell level.</p><p>Two recent papers have also reported curvature domains in anesthetized macaque V4. <xref ref-type="bibr" rid="bib23">Hu et al., 2020</xref> used ISOI, finding functional domains that prefer curved over straight gratings. <xref ref-type="bibr" rid="bib56">Tang et al., 2020</xref> used both ISOI and 2P imaging, finding functional domains that prefer circles over rectilinear triangles. In general, these two imaging studies alongside our own provide clear replication of the core importance of curvature as an organizing principle in the functional architecture of V4. Compared to ISOI, 2P imaging holds the advantage of higher spatial resolution, and therefore makes it possible to characterize the transition between domains more precisely than Gaussian smoothed ISOI. We found the transition taking place within around 300 µm, remaining relatively elevated thereafter (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Comparing the different stimulus set (curves vs. corners, concentric vs. radial), the transition of CRI maps of monkey A in <xref ref-type="fig" rid="fig5">Figure 5A</xref> looked sharper probably because too many neurons had negative CRI values. CRI and CVCNI were correlated but not identical. Since concentric gratings only have 360° full circles but some neurons might prefer short arches (small radian, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), it is possible that they do not respond strongly to concentric gratings and tend to have negative CRI.</p><p>A number of electrophysiology studies have reported that some neurons in V4d are selective for more complex features (<xref ref-type="bibr" rid="bib17">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="bib22">Hegdé and Van Essen, 2007</xref>; <xref ref-type="bibr" rid="bib31">Kobatake and Tanaka, 1994</xref>; <xref ref-type="bibr" rid="bib46">Pasupathy and Connor, 1999</xref>). Our results, consistent with these works, identified many curve- or corner-selective neurons. In addition, given the ability of 2P imaging to quantify the spatial relationships between neurons, we confirm that they are spatially clustered. We also observed some deviations of our results from earlier studies. First, the percentage of complex feature-selective neurons we found in our study is higher than previously observed (<xref ref-type="bibr" rid="bib17">Gallant et al., 1993</xref>; <xref ref-type="bibr" rid="bib46">Pasupathy and Connor, 1999</xref>); in our hands, the vast majority of neurons preferred curves and corners over bars and concentric and radial gratings over Cartesian gratings. Second, although Π-shape stimuli were sometimes regarded also as curved contours (<xref ref-type="bibr" rid="bib39">Nandy et al., 2013</xref>), we found V4 neurons responding to them differently. We think these two deviations are primarily due to sampling neurons within or close to curve and corner domains (which is difficult to detect with classical electrophysiology). We do not wish to infer that curve and corner stimuli are only encoded by neurons in the curve and corner domains while other neurons are not involved. But we have demonstrated that neurons in the curve and corner domains are tuned to more complex and integral features rather than local orientation, spatial frequency, or multiple orientations, alone, supporting the encoding of shape segments with intermediate complexity in V4 (<xref ref-type="bibr" rid="bib5">Bushnell and Pasupathy, 2012</xref>; <xref ref-type="bibr" rid="bib14">El-Shamayleh and Pasupathy, 2016</xref>; <xref ref-type="bibr" rid="bib44">Oleskiw et al., 2014</xref>; <xref ref-type="bibr" rid="bib51">Rust and DiCarlo, 2010</xref>).</p><p>Complexity increases as visual shape information is processed along the ventral visual pathway. Neurons in V1 are tuned to low-order orientation and spatial frequency and organized in iso-orientation domains and orientation pinwheels (<xref ref-type="bibr" rid="bib41">Nauhaus et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">Ts'o et al., 1990</xref>). Neurons in IT are selective for complex features and objects and organized in feature columns and face patches (<xref ref-type="bibr" rid="bib54">Tanaka, 2003</xref>; <xref ref-type="bibr" rid="bib60">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib61">Tsunoda et al., 2001</xref>). The simple-to-complex transformation and integration take place in the intermediate stages between V1 and IT. Researchers have reported that some V2 neurons are selective to combination of multiple local orientations, from which corner selectivity might emerge (<xref ref-type="bibr" rid="bib1">Anzai et al., 2007</xref>; <xref ref-type="bibr" rid="bib26">Ito and Komatsu, 2004</xref>). Our results in V4d showed that intermediate shape segments like curves and corners are separately encoded by neurons in specific functional domains, and the curve- and corner-selective neurons are tuned to the integral features instead of local orientation or combination of orientations. It is possible that these complex feature-selective neurons receive inputs or modulation from nearby neurons or downstream areas to form a recurrent network, which might underlie previous findings that the response profiles of V4 neurons were temporally heterogeneous (<xref ref-type="bibr" rid="bib40">Nandy et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Yau et al., 2013</xref>). Such evidence is also recently accumulating for IT cortex (<xref ref-type="bibr" rid="bib30">Kar and DiCarlo, 2021</xref>). Unfortunately, this question is difficult to address given the temporal resolution of the existing calcium imaging technique. One possible solution is to use genetically encoded voltage indicators (<xref ref-type="bibr" rid="bib64">Xu et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Yang and St-Pierre, 2016</xref>), which once successfully applied in macaques could help to reveal the simple-to-complex integration of neurons.</p><p>Given that we recorded neurons whose stimulation was not isolated to the ‘optimal’ spatial location in the receptive fields (i.e., the RF locations of some neurons might deviate for the population RF), the nature of the domains may also be modulated by stimulus translation variance, and future studies addressing positional variance and stimulus encoding are warranted. Our sample of V4d was also near-foveal in terms of eccentricity. It is well established that the ventral pathway connectivity to IT favors central rather than peripheral visual space (<xref ref-type="bibr" rid="bib62">Ungerleider et al., 2008</xref>), but the relationship of visual eccentricity to these functional domains remains unknown. The existence of curve and corner domains for neuronal encoding in V4d provides significant support for integration of shape information in the intermediate stages of the visual hierarchy. These findings provide a more comprehensive understanding of the functional architecture of V4 feature selectivity.</p><p>Finally, our results may also help to explore the later stage of the visual hierarchy. The data suggests that higher-order pattern domains may emerge gradually along the ventral pathway. The specificity of clustered patches/domains in the cortex has been proposed as an important organizing principle for some, though not all, domains of cortical processing (<xref ref-type="bibr" rid="bib29">Kanwisher, 2010</xref>). A recent study has suggested that, at least for faces and color processing, such functional domains are causally specific for human visual recognition (<xref ref-type="bibr" rid="bib52">Schalk et al., 2017</xref>). The curve and corner domain responses in V4 could possibly form the basis for more complex feature columns, object domains, and face patches in IT. This is consistent with a growing body of evidence from the ventral stream (<xref ref-type="bibr" rid="bib2">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Rajalingham and DiCarlo, 2019</xref>; <xref ref-type="bibr" rid="bib68">Yue et al., 2014</xref>). Recent explorations of neuronal response fields in artificial neural networks have likewise found a prevalence of curve detectors with increasing complexity along the processing hierarchy (<xref ref-type="bibr" rid="bib7">Cammarata et al., 2020</xref>). Studying such functional cross-areal connectivity (both bottom-up and top-down) remains a critical goal for future studies of the visual system. It is also interesting to try to identify why smooth curves and rectilinear corners are separated as early as V4. One possible explanation is that smooth curves are more prevalent in living animals or foods that are of particular interest to primates, while corners are often found in the background environment of stones or branches. Such differences may underlie the statistical regularities in natural images of objects (<xref ref-type="bibr" rid="bib36">Long et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Levin et al., 2001</xref>; <xref ref-type="bibr" rid="bib67">Yetter et al., 2020</xref>; <xref ref-type="bibr" rid="bib69">Zachariou et al., 2018</xref>). Such comparisons will provide a basis for future investigations comparing the statistical feature relationships for natural images between V4 and IT functional domains.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th valign="top">Reagent type <break/>(species) or <break/>resource</th><th valign="top">Designation</th><th valign="top">Source or <break/>reference</th><th valign="top">Identifiers</th><th valign="top">Additional <break/>information</th></tr></thead><tbody><tr><td valign="top">Strain, strain background (<italic>Macaca mulatta</italic>)</td><td valign="top"><italic>Macaca mulatta</italic></td><td valign="top">Beijing Prima Biotech Inc</td><td valign="top"/><td valign="top"><ext-link ext-link-type="uri" xlink:href="http://www.primasbio.com/en/Home">http://www.primasbio.com/en/Home</ext-link></td></tr><tr><td valign="top">Recombinant DNA reagent</td><td valign="top">AAV9.Syn.GCaMP6f.WPRE.SV40</td><td valign="top">Penn Vector Core</td><td valign="top">CS1001</td><td valign="top"/></tr><tr><td valign="top">Recombinant DNA reagent</td><td valign="top">AAV1.Syn.GCaMP5G.WPRE.SV40</td><td valign="top">Penn Vector Core</td><td valign="top">V4102MI-R</td><td valign="top"/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">MATLAB R2018b</td><td valign="top">MathWorks</td><td valign="top"/><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link></td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Code for data analysis</td><td valign="top">This paper</td><td valign="top"/><td valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/RJiang1994/macaque-v4-2P">https://github.com/RJiang1994/macaque-v4-2P</ext-link> (<xref ref-type="bibr" rid="bib28">Jiang, 2021</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0918938d55c574704ae3761018223debc0149c27;origin=https://github.com/RJiang1994/macaque-v4-2P;visit=swh:1:snp:c344907d3ef7f22770d47ff483729de6fdc0a1e4;anchor=swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c">swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c</ext-link>)</td></tr></tbody></table></table-wrap><p>All procedures involving animals were in accordance with the Guide of Institutional Animal Care and Use Committee (IACUC) of Peking University Laboratory Animal Center and approved by the Peking University Animal Care and Use Committee (LSC-TangSM-5).</p><sec id="s4-1"><title>Animal preparation</title><p>The subjects used in this study were two adult male rhesus monkeys (<italic>Macaca mulatta</italic>, 4 and 5 years of age, respectively), purchased from Beijing Prima Biotech Inc and housed at Peking University Laboratory Animal Center. Two sequential surgeries were performed on each animal under general anesthesia. In the first surgery, we performed a craniotomy over V4 and opened the dura. We injected 200 nl of AAV9.Syn.GCaMP6f.WPRE.SV40 (CS1001, titer 7.748e13 [GC/ml], Penn Vector Core) or AAV1.Syn.GCaMP5G.WPRE.SV40 (V4102MI-R, titer 2.37e13 [GC/ml], Penn Vector Core) at a depth of about 350 μm and speed of 5–10 nl/s. Injection and surgical protocols followed our previous study (<xref ref-type="bibr" rid="bib35">Li et al., 2017</xref>). After injections, we sutured the dura, replaced the skull cap with titanium screws, and closed the scalp. The animal was then returned for recovery and received Ceftriaxone sodium antibiotic (Youcare Pharmaceutical Group Co. Ltd., China) for 1 week. 45 days later, we performed the second surgery to implant the imaging window and head posts. The dura was removed and a glass coverslip was put directly above the cortex without any artificial dura and glued to a titanium ring. We then glued the titanium ring to the skull using dental acrylic. The detailed design of the chamber and head posts can be found in our previous study (<xref ref-type="bibr" rid="bib35">Li et al., 2017</xref>). Monkeys can be ready for recording about 1 week after the second surgery.</p></sec><sec id="s4-2"><title>Behavioral task</title><p>Monkeys were trained to maintain fixation on a small white spot (0.1°) while seated in a primate chair with head restraint to obtain a juice reward. Eye positions were monitored by an ISCAN ETL-200 infrared eye-tracking system (ISCAN Inc, Woburn, MA) at a 120 Hz sampling rate. Trials in which the eye position deviated 1° or more from the fixation point were terminated and the same condition was repeated immediately. Only data from the successful trials was used.</p></sec><sec id="s4-3"><title>Visual stimuli</title><p>The visual stimuli were displayed on an LCD monitor 45 cm from the animal’s eyes (Acer v173Db, 17 inch, 1280 × 960 pixel, 30 pixel/°, 80 Hz refresh rate). After acquiring fixation, only the gray background (32 cd/m<sup>2</sup>) was presented for the first 1 s to obtain the fluorescence baseline, and then the visual stimuli were displayed for further 1 s. No inter-trial interval was used. Stimuli were presented in pseudo-random order. We used square-wave drifting gratings (0.4° diameter circular patch, full contrast, 4 cycle/°, 3 cycle/s) generated and presented by the ViSaGe system (Cambridge Research Systems, Rochester, UK) to measure the retinal eccentricity, which was about 1° bottom left to the fovea for both monkeys.</p><p>Contour feature stimuli were generated using MATLAB (The MathWorks, Natick, MA) and presented using the ViSaGe system (Cambridge Research Systems). The contour feature stimuli were two pixels wide. The lengths of the bars and corner edges were 10 and 20 pixels (30 pixel/°, 0.33° and 0.67°), and the radius of curve stimuli were also 10 and 20 pixels. For each of the two sizes, the curve stimuli varied in radians (120°, 180° for 4× imaging and 60°, 90°, 120°, 180° for 16× imaging). The corner stimuli also varied in three separation angles (45° and 90° and 135°). All contour feature stimuli were rotated to eight orientations (0°, 45°, 90°, 135°, 180°, 225°, 270°, 315° for curves and corners, and 0°, 22.5°, 45°, 67.5°, 90°, 112.5°, 135°, 157.5° for bars).</p><p>The Cartesian (eight orientations, 0°, 45°, 90°, 135°, 180°, 225°, 270°, 315°), concentric, and radial grating stimuli were full contrast sinusoidal gratings (edge blurred), which were 90 pixels (3°) in diameter, with spatial frequencies (SF) of 1, 2, and 4 cycle/°. The concentric gratings were generated as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">CG</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"/><mml:mi mathvariant="italic">sin</mml:mi><mml:mo>(</mml:mo></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="normal">*</mml:mi><mml:msqrt><mml:msup><mml:mrow><mml:mi mathvariant="normal"/><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal"/><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>The radial gratings were generated as<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">RG</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"/><mml:mi mathvariant="italic">sin</mml:mi><mml:mo>(</mml:mo></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal"/><mml:mfrac><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="normal"/><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>The data for contour feature stimuli was recorded on one day, and the data for gratings on another day.</p></sec><sec id="s4-4"><title>2P imaging</title><p>2P imaging was performed using a Prairie Ultima IV 2P laser scanning microscope (Bruker Corporation, Billerica, MA) during experiments. 1000 nm mode-lock laser (Spectra-Physics, Santa Clara, CA) was used for excitation of GCaMPs, and resonant galvo scanning (512 × 512 pixel, 32 frame/s) was used to record the fluorescence images (8 fps, averaging every four frames). A 4× objective (Nikon Corporation, Tokyo, Japan) was used for sub-cortical-level recording (3.4 × 3.4 mm, 6.7 μm/pixel), and a 16× objective (Nikon Corporation) for neural population recording at single-cellular resolution (850 × 850 μm, 1.7 μm/pixel). We used a Neural Signal Processor (Cerebus system, Blackrock Microsystem, Salt Lake City, UT) to record the time stamp of each frame of the 2P images as well as the time stamps of visual stimuli onset for synchronization.</p></sec><sec id="s4-5"><title>Image data processing</title><p>Image data was processed by MATLAB. The 2P images were first aligned to a template image by a 2D cross-correlation algorithm (<xref ref-type="bibr" rid="bib35">Li et al., 2017</xref>) to eliminate motion artifacts during recording sections. For all the successful trials, we found the corresponding 2P images by synchronizing the time stamps of stimulus onset recorded by the Neural Signal Processor (Cerebus system, Blackrock Microsystem). The differential fluorescence image was calculated as ΔF = F – F0, where the basal fluorescence image F0 was defined as the average image of 0–0.5 s before stimulus onset, and F as the average of 0.5–1.25 s after stimulus onset, both averaged across all repeats for each stimulus.</p><p>For 4× imaging, the ΔF/F0 maps were Gaussian smoothed using a low-pass Gaussian filter (σ = 10 pixels) to obtain the activation maps. For 16× imaging, to identify responding cell bodies (ROIs), the differential image (ΔF) for each stimuli went through a band-pass Gaussian filter (σ = 2 pixels and 5 pixels, respectively, only used for identifying ROIs) and were then binarized using a pixel value threshold of 3 SD. The connected components (&gt;25 pixels) were identified as candidates for active ROIs. An ROI was discarded if its maximum response (ΔF/F0) was below 0.3. The roundness of these ROIs was calculated as<disp-formula id="equ3"><mml:math id="m3"><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"/><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi mathvariant="normal">π</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>where P is the perimeter of the ROI and S is the area. Only ROIs with C &lt; 1.1 were identified as cell bodies. We also tested this criterion by ANOVA, comparing the fluorescence 0–0.5 s before and 0.5–1.25 s after stimulus onset (same definition as ΔF), all trials together. 533 out of the 535 neurons identified had p&lt;0.05.</p></sec><sec id="s4-6"><title>Curve and corner domains</title><p>All trials (stim number × repeat number) in 4× imaging were categorized first as curves (32 stim), corners (48 stim), or bars (16 stim). Curve patches: for each pixel, independent t-tests were performed to compare the responses to all curves against all corners and against all bars, respectively, and the larger one of the two p-values was chosen if the mean response to curves is stronger than corners and bars. FDR was computed following a Benjamini–Hochberg procedure, using the MATLAB command mafdr, in which q<sub>i</sub> = p<sub>i</sub> × 512×512/rank(p<sub>i</sub>). Corner patches followed the same procedure.</p><p>Cluster permutation tests were then performed to exclude patches with too few significant pixels. For each permutation, all trials (stim number × repeat number) were randomly relabeled as curves, corners, or bars, keeping the total trial number within each of the three groups unchanged. Independent t-tests as in <xref ref-type="fig" rid="fig1">Figure 1F</xref> were performed, with an uncorrected p=0.01 as threshold. The cluster (connected component) with the maximum pixel number was recorded. 60,000 random permutations were performed, resulting in 60,000 maximum cluster sizes as null distribution. The top 5% (3000) of the null distribution was used as the threshold, and the patches with pixels below this level were regarded as insignificant and excluded.</p></sec><sec id="s4-7"><title>Quantification and statistical analysis</title><p>Two tests were performed to determine whether a neuron was selective to the orientation of curves or corners. First, we performed ANOVA to compare the fluorescence 0–0.5 s before and 0.5–1.25 s after stimulus onset (same definition as ΔF) using all the trials for curve and corner stimuli. Then we find the optimal curve or corner stimuli of this neuron and used ANOVA to compare among the eight orientations of this optimal form. The p-value was then Bonferroni-corrected (14 comparisons, 6 corners, and 8 curves). Only neurons passing both ANOVA tests (p&lt;0.05) were deemed as tuned to the orientation of curves or corners.</p><p>CVSI is used to characterize a neuron’s preference to curves over other stimuli (bars and corners), defined as<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where MaxResp<sub>curve</sub> is the neuron’s maximum response to curve stimuli and <bold><italic>MaxResp</italic></bold><sub>other</sub> is the neuron’s maximum response to other stimuli (bars and corners). CVSI ranges from −1 to 1, and a positive CVSI value indicates a neuron’s response to its optimal curve stimuli is greater than its response to optimal bar or corner stimuli.</p><p>CNSI is defined as<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where MaxResp <sub>corner</sub> is the neuron’s maximum response to corner stimuli and <bold><italic>MaxResp</italic></bold><sub>other</sub> is the neuron’s maximum response to other stimuli (bars and curves).</p><p>CVCNI is defined as<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We also performed one-way ANOVA test comparing neuron’s maximum response to curve stimuli and maximum response to corner stimuli in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, with threshold value p=0.05, repeats n = 10. The same tests were also applied to CVSI and CNSI in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.</p><p>CVPII is defined as<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where MaxResp<sub>Π-shape</sub> is the neuron’s maximum response to Π-shape stimuli. The Pearson correlation of CVCNI and CVPII was calculated in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, and the regression line was derived by minimizing <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mrow><mml:mover><mml:mrow/><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mrow><mml:mover><mml:mrow/><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>CRI is defined as<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where MaxResp<sub>concentric</sub> is the neuron’s maximum response to concentric gratings and <bold><italic>MaxResp</italic></bold><sub>radial</sub> is the neuron’s maximum response to radial gratings. The Pearson correlation of CVCNI and CRI was also calculated in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, and the regression line was derived by minimizing <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mrow><mml:mover><mml:mrow/><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi/><mml:mrow><mml:mover><mml:mrow/><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Clustering analysis</title><p>We analyzed <inline-formula><mml:math id="inf3"><mml:mfenced separators="|"><mml:mrow><mml:mfrac linethickness="0pt"><mml:mrow><mml:mn>292</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula> neuron pairs from monkey A and <inline-formula><mml:math id="inf4"><mml:mfenced separators="|"><mml:mrow><mml:mfrac linethickness="0pt"><mml:mrow><mml:mn>243</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></inline-formula> neuron pairs from monkey B in <xref ref-type="fig" rid="fig2">Figure 2I–K</xref>. Pairwise tuning correlation was calculated as the Pearson correlation of the two neurons’ responses to all bar, curve, and corner stimuli, and were plotted against pairwise spatial distances (averaging all neurons every 100 μm).</p><p>Similarly, the differences in CVSI and CNSI were also plotted against pairwise spatial distances:<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where CVSI<italic><sub>i</sub></italic> is the CVSI of neuron <italic>i</italic> and CVSI<italic><sub>j</sub></italic> is the CVSI of neuron <italic>j</italic>.<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where CNSI<italic><sub>i</sub></italic> is the CVSI of neuron <italic>i</italic> and CNSI<italic><sub>j</sub></italic> is the CNSI of neuron <italic>j</italic>.</p><p>Permutation test was performed to evaluate the significance of each average |ΔCVSI| and |ΔCNSI|. |ΔCVSI| or |ΔCNSI| were randomly paired with distances for 100,000 times to build the null distribution and averaged. A point was considered significant if it is higher than the top 100 of the null distribution or lower than the bottom 100 (p&lt;0.001).</p></sec><sec id="s4-9"><title>K-means analysis</title><p>We performed K-means analysis to cluster the stimulus forms and the neurons. Responses of 535 neurons to 20 forms (two bars, eight curves, six corners, and four Π-shapes, each at eight orientations) are used to construct the responses matrix R as<disp-formula id="equ11"><mml:math id="m11"><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"/><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1,535</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>20,1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>20,535</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula>where <italic>r<sub>i,j</sub></italic> is the response of neuron <italic>j</italic> to stimulus form <italic>i</italic>. Only the maximum responses among eight orientations were used.</p><p>We used population response vectors (<italic>RP,</italic> rows of matrix R) to cluster the forms. For form <italic>i</italic>,<disp-formula id="equ12"><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal"/><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal"/><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>535</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>We used neuron response vectors (<italic>RN</italic>, columns of matrix R) to cluster the neurons. For neuron <italic>j</italic>,<disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal"/><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal"/><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal"/><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>The number of clusters was determined using Calinski–Harabasz criterion and squared Euclidean distance. Maximum literation time = 10,000. Clustering was repeated for 10,000 times with new initial cluster centroid, and the one with the lowest within-cluster sum was used.</p></sec><sec id="s4-10"><title>Multi-dimensional scaling</title><p>Classical multi-dimensional scaling (MDS) was performed to visualize the clustering of stimulus forms derived by K-means. The distance (dissimilarity matrix) was computed as<disp-formula id="equ14"><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula>where<italic> D<sub>i,j</sub></italic> is the distance between form <italic>i</italic> and <italic>j</italic>, <italic>corrcoef</italic> is the Pearson correlation, and <italic>RP<sub>i</sub></italic> is the population response vectors of form <italic>i</italic>. Classical MDS was performed using singular value decomposition (SVD) algorithm.</p><p>The normalized stress was computed as<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where D<sub>i,j</sub> is the distance in the original space and <italic>D<sup>’</sup><sub>i,j</sub></italic> is the distance in the new MDS space.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the Peking University Laboratory Animal Center for animal care. We acknowledge the Genetically Encoded Calcium Indicator (GECI) project at Janelia Farm Research Campus Howard Hughes Medical Institute. We thank Niall Mcloughlin and Cong Yu for their comments and suggestion on the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Software, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures involving animals were in accordance with the Guide of Institutional Animal Care and Use Committee (IACUC) of Peking University Laboratory Animal Center, and approved by the Peking University Animal Care and Use Committee (LSC-TangSM-5).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-63798-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data and MATLAB codes used in this study can be found in GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/RJiang1994/macaque-v4-2P">https://github.com/RJiang1994/macaque-v4-2P</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c">https://archive.softwareheritage.org/swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>R</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>macaque-v4-2P</data-title><source>GitHub</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://github.com/RJiang1994/macaque-v4-2P">RJiang1994/macaque-v4-2P</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anzai</surname> <given-names>A</given-names></name><name><surname>Peng</surname> <given-names>X</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neurons in monkey visual area V2 encode combinations of orientations</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1313</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1038/nn1975</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname> <given-names>P</given-names></name><name><surname>She</surname> <given-names>L</given-names></name><name><surname>McGill</surname> <given-names>M</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname> <given-names>P</given-names></name><name><surname>Kar</surname> <given-names>K</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><volume>364</volume><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bushnell</surname> <given-names>BN</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Shape encoding consistency across colors in primate V4</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>1299</fpage><lpage>1308</lpage><pub-id pub-id-type="doi">10.1152/jn.01063.2011</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname> <given-names>C</given-names></name><name><surname>Kouh</surname> <given-names>M</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A Model of V4 Shape Selectivity and Invariance</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1733</fpage><lpage>1750</lpage><pub-id pub-id-type="doi">10.1152/jn.01265.2006</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cammarata</surname> <given-names>N</given-names></name><name><surname>Goh</surname> <given-names>G</given-names></name><name><surname>Carter</surname> <given-names>S</given-names></name><name><surname>Schubert</surname> <given-names>L</given-names></name><name><surname>Petrov</surname> <given-names>M</given-names></name><name><surname>Olah</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curve detectors</article-title><source>Distill</source><volume>5</volume><elocation-id>003</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00024.003</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>ET</given-names></name><name><surname>Rasquinha</surname> <given-names>RJ</given-names></name><name><surname>Zhang</surname> <given-names>K</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A Sparse Object Coding Scheme in Area V4</article-title><source>Current Biology</source><volume>21</volume><fpage>288</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.01.013</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname> <given-names>CE</given-names></name><name><surname>Brincat</surname> <given-names>SL</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Transformation of shape information in the ventral pathway</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>140</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.03.002</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname> <given-names>BR</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Specialized Color Modules in Macaque Extrastriate Cortex</article-title><source>Neuron</source><volume>56</volume><fpage>560</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.008</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spectral Receptive Field Properties Explain Shape Selectivity in Area V4</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>3492</fpage><lpage>3505</lpage><pub-id pub-id-type="doi">10.1152/jn.00575.2006</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Albright</surname> <given-names>TD</given-names></name><name><surname>Gross</surname> <given-names>CG</given-names></name><name><surname>Bruce</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Stimulus-selective properties of inferior temporal neurons in the macaque</article-title><source>The Journal of Neuroscience</source><volume>4</volume><fpage>2051</fpage><lpage>2062</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.04-08-02051.1984</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Schein</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Visual properties of neurons in area V4 of the macaque: sensitivity to stimulus form</article-title><source>Journal of Neurophysiology</source><volume>57</volume><fpage>835</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1152/jn.1987.57.3.835</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Shamayleh</surname> <given-names>Y</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Contour Curvature As an Invariant Code for Objects in Visual Area V4</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>5532</fpage><lpage>5543</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4139-15.2016</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional Compartmentalization and Viewpoint Generalization Within the Macaque Face-Processing System</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujita</surname> <given-names>I</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Cheng</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Columns for visual features of objects in monkey inferotemporal cortex</article-title><source>Nature</source><volume>360</volume><fpage>343</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/360343a0</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallant</surname> <given-names>J</given-names></name><name><surname>Braun</surname> <given-names>J</given-names></name><name><surname>Van Essen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Selectivity for polar, hyperbolic, and Cartesian gratings in macaque visual cortex</article-title><source>Science</source><volume>259</volume><fpage>100</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1126/science.8418487</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallant</surname> <given-names>JL</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name><name><surname>Rakshit</surname> <given-names>S</given-names></name><name><surname>Lewis</surname> <given-names>JW</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Van</surname> <given-names>EDC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural responses to polar, hyperbolic, and Cartesian gratings in area V4 of the macaque monkey</article-title><source>Journal of Neurophysiology</source><volume>76</volume><fpage>2718</fpage><lpage>2739</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.76.4.2718</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garg</surname> <given-names>AK</given-names></name><name><surname>Li</surname> <given-names>P</given-names></name><name><surname>Rashid</surname> <given-names>MS</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Color and orientation are jointly coded and spatially organized in primate primary visual cortex</article-title><source>Science</source><volume>364</volume><fpage>1275</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1126/science.aaw5868</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gattass</surname> <given-names>R</given-names></name><name><surname>Rosa</surname> <given-names>MG</given-names></name><name><surname>Sousa</surname> <given-names>AP</given-names></name><name><surname>Piñon</surname> <given-names>MC</given-names></name><name><surname>Fiorani Júnior</surname> <given-names>M</given-names></name><name><surname>Neuenschwander</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Cortical streams of visual information processing in primates</article-title><source>Brazilian Journal of Medical and Biological Research = Revista Brasileira De Pesquisas Medicas E Biologicas</source><volume>23</volume><fpage>375</fpage><lpage>393</lpage><pub-id pub-id-type="pmid">1965642</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Grady</surname> <given-names>CL</given-names></name><name><surname>Horwitz</surname> <given-names>B</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Mishkin</surname> <given-names>M</given-names></name><name><surname>Carson</surname> <given-names>RE</given-names></name><name><surname>Herscovitch</surname> <given-names>P</given-names></name><name><surname>Schapiro</surname> <given-names>MB</given-names></name><name><surname>Rapoport</surname> <given-names>SI</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Dissociation of object and spatial visual processing pathways in human extrastriate cortex</article-title><source>PNAS</source><volume>88</volume><fpage>1621</fpage><lpage>1625</lpage><pub-id pub-id-type="doi">10.1073/pnas.88.5.1621</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegdé</surname> <given-names>J</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A Comparative Study of Shape Representation in Macaque Visual Areas V2 and V4</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>1100</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl020</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname> <given-names>JM</given-names></name><name><surname>Song</surname> <given-names>XM</given-names></name><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>Roe</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curvature domains in V4 of macaque monkey</article-title><source>eLife</source><volume>9</volume><elocation-id>e57261</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57261</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Segregation of form, color, and stereopsis in primate area 18</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>3378</fpage><lpage>3415</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-11-03378.1987</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields and functional architecture of monkey striate cortex</article-title><source>The Journal of Physiology</source><volume>195</volume><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname> <given-names>M</given-names></name><name><surname>Komatsu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Representation of angles embedded within contour stimuli in area v2 of macaque monkeys</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>3313</fpage><lpage>3324</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4364-03.2004</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>R</given-names></name><name><surname>Li</surname> <given-names>M</given-names></name><name><surname>Tang</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discrete neural clusters encode orientation, curvature and corners in macaque V4</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/808907</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>macaque-v4-2P</data-title><source>Software Heritage</source><version designator="swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c">swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0918938d55c574704ae3761018223debc0149c27;origin=https://github.com/RJiang1994/macaque-v4-2P;visit=swh:1:snp:c344907d3ef7f22770d47ff483729de6fdc0a1e4;anchor=swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c">https://archive.softwareheritage.org/swh:1:dir:0918938d55c574704ae3761018223debc0149c27;origin=https://github.com/RJiang1994/macaque-v4-2P;visit=swh:1:snp:c344907d3ef7f22770d47ff483729de6fdc0a1e4;anchor=swh:1:rev:57dfeac5e81b91c93ef0687f8cf04010d3f47f8c</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional specificity in the human brain: A window into the functional architecture of the mind</article-title><source>PNAS</source><volume>107</volume><fpage>11163</fpage><lpage>11170</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname> <given-names>K</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast recurrent processing via ventrolateral prefrontal cortex is needed by the primate ventral stream for robust core visual object recognition</article-title><source>Neuron</source><volume>109</volume><fpage>164</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.035</pub-id><pub-id pub-id-type="pmid">33080226</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobatake</surname> <given-names>E</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex</article-title><source>Journal of Neurophysiology</source><volume>71</volume><fpage>856</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.71.3.856</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>DT</given-names></name><name><surname>Takarae</surname> <given-names>Y</given-names></name><name><surname>Miner</surname> <given-names>AG</given-names></name><name><surname>Keil</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficient visual search by category: Specifying the features that mark the difference between artifacts and animals in preattentive vision</article-title><source>Perception &amp; Psychophysics</source><volume>63</volume><fpage>676</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.3758/BF03194429</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>P</given-names></name><name><surname>Zhu</surname> <given-names>S</given-names></name><name><surname>Chen</surname> <given-names>M</given-names></name><name><surname>Han</surname> <given-names>C</given-names></name><name><surname>Xu</surname> <given-names>H</given-names></name><name><surname>Hu</surname> <given-names>J</given-names></name><name><surname>Fang</surname> <given-names>Y</given-names></name><name><surname>Lu</surname> <given-names>HD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A motion direction preference map in monkey V4</article-title><source>Neuron</source><volume>78</volume><fpage>376</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.02.024</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>M</given-names></name><name><surname>Liu</surname> <given-names>F</given-names></name><name><surname>Juusola</surname> <given-names>M</given-names></name><name><surname>Tang</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Perceptual Color Map in Macaque Visual Area V4</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>202</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4549-12.2014</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>M</given-names></name><name><surname>Liu</surname> <given-names>F</given-names></name><name><surname>Jiang</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>TS</given-names></name><name><surname>Tang</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Long-Term Two-Photon imaging in awake macaque monkey</article-title><source>Neuron</source><volume>93</volume><fpage>1049</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.027</pub-id><pub-id pub-id-type="pmid">28215557</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>B</given-names></name><name><surname>Yu</surname> <given-names>CP</given-names></name><name><surname>Konkle</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>PNAS</source><volume>115</volume><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id><pub-id pub-id-type="pmid">30171168</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname> <given-names>Y</given-names></name><name><surname>Yin</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Gong</surname> <given-names>H</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Qian</surname> <given-names>L</given-names></name><name><surname>Li</surname> <given-names>X</given-names></name><name><surname>Liu</surname> <given-names>R</given-names></name><name><surname>Andolina</surname> <given-names>IM</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Revealing detail along the visual hierarchy: neural clustering preserves acuity from V1 to V4</article-title><source>Neuron</source><volume>98</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.009</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishkin</surname> <given-names>M</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Macko</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Object vision and spatial vision: two cortical pathways</article-title><source>Trends in Neurosciences</source><volume>6</volume><fpage>414</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(83)90190-X</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nandy</surname> <given-names>AS</given-names></name><name><surname>Sharpee</surname> <given-names>TO</given-names></name><name><surname>Reynolds</surname> <given-names>JH</given-names></name><name><surname>Mitchell</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The fine structure of shape tuning in area V4</article-title><source>Neuron</source><volume>78</volume><fpage>1102</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.016</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nandy</surname> <given-names>AS</given-names></name><name><surname>Mitchell</surname> <given-names>JF</given-names></name><name><surname>Jadi</surname> <given-names>MP</given-names></name><name><surname>Reynolds</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurons in Macaque Area V4 Are Tuned for Complex Spatio-Temporal Patterns</article-title><source>Neuron</source><volume>91</volume><fpage>920</fpage><lpage>930</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.026</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nauhaus</surname> <given-names>I</given-names></name><name><surname>Nielsen</surname> <given-names>KJ</given-names></name><name><surname>Disney</surname> <given-names>AA</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Orthogonal micro-organization of orientation and spatial frequency in primate primary visual cortex</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1683</fpage><lpage>1690</lpage><pub-id pub-id-type="doi">10.1038/nn.3255</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname> <given-names>TE</given-names></name><name><surname>Holmes</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohki</surname> <given-names>K</given-names></name><name><surname>Chung</surname> <given-names>S</given-names></name><name><surname>Ch'ng</surname> <given-names>YH</given-names></name><name><surname>Kara</surname> <given-names>P</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional imaging with cellular resolution reveals precise micro-architecture in visual cortex</article-title><source>Nature</source><volume>433</volume><fpage>597</fpage><lpage>603</lpage><pub-id pub-id-type="doi">10.1038/nature03274</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oleskiw</surname> <given-names>TD</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Bair</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spectral receptive fields do not explain tuning for boundary curvature in V4</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>2114</fpage><lpage>2122</lpage><pub-id pub-id-type="doi">10.1152/jn.00250.2014</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Kim</surname> <given-names>T</given-names></name><name><surname>Popovkina</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Object shape and surface properties are jointly encoded in mid-level ventral visual cortex</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>199</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.009</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Responses to Contour Features in Macaque Area V4</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>2490</fpage><lpage>2502</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.5.2490</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Shape Representation in Area V4: Position-Specific Tuning for Boundary Conformation</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>2505</fpage><lpage>2519</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.5.2505</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Population coding of shape in area V4</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1038/972</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname> <given-names>R</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reversible Inactivation of Different Millimeter-Scale Regions of Primate IT Results in Different Patterns of Core Object Recognition Deficits</article-title><source>Neuron</source><volume>102</volume><fpage>493</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.02.001</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roe</surname> <given-names>AW</given-names></name><name><surname>Chelazzi</surname> <given-names>L</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name><name><surname>Conway</surname> <given-names>BR</given-names></name><name><surname>Fujita</surname> <given-names>I</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name><name><surname>Vanduffel</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Toward a Unified Theory of Visual Area V4</article-title><source>Neuron</source><volume>74</volume><fpage>12</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.011</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname> <given-names>NC</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Selectivity and tolerance (&quot;invariance&quot;) both increase as visual information propagates from Cortical Area V4 to IT</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12978</fpage><lpage>12995</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0179-10.2010</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Kapeller</surname> <given-names>C</given-names></name><name><surname>Guger</surname> <given-names>C</given-names></name><name><surname>Ogawa</surname> <given-names>H</given-names></name><name><surname>Hiroshima</surname> <given-names>S</given-names></name><name><surname>Lafer-Sousa</surname> <given-names>R</given-names></name><name><surname>Saygin</surname> <given-names>ZM</given-names></name><name><surname>Kamada</surname> <given-names>K</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Facephenes and rainbows: Causal evidence for functional and anatomical specificity of face and color processing in the human brain</article-title><source>PNAS</source><volume>114</volume><fpage>12285</fpage><lpage>12290</lpage><pub-id pub-id-type="doi">10.1073/pnas.1713447114</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seidemann</surname> <given-names>E</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Bai</surname> <given-names>Y</given-names></name><name><surname>Chen</surname> <given-names>SC</given-names></name><name><surname>Mehta</surname> <given-names>P</given-names></name><name><surname>Kajs</surname> <given-names>BL</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name><name><surname>Zemelman</surname> <given-names>BV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Calcium imaging with genetically encoded indicators in behaving primates</article-title><source>eLife</source><volume>5</volume><elocation-id>e16178</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16178</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Columns for complex visual object features in the inferotemporal cortex: clustering of cells with similar but slightly different stimulus selectivities</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>90</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.1.90</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>TS</given-names></name><name><surname>Li</surname> <given-names>M</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Liu</surname> <given-names>F</given-names></name><name><surname>Teo</surname> <given-names>B</given-names></name><name><surname>Jiang</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Complex Pattern Selectivity in Macaque Primary Visual Cortex Revealed by Large-Scale Two-Photon Imaging</article-title><source>Current Biology</source><volume>28</volume><fpage>38</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.039</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname> <given-names>R</given-names></name><name><surname>Song</surname> <given-names>Q</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Zhang</surname> <given-names>R</given-names></name><name><surname>Cai</surname> <given-names>X</given-names></name><name><surname>Lu</surname> <given-names>HD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curvature-processing domains in primate V4</article-title><source>eLife</source><volume>9</volume><elocation-id>e57502</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57502</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanigawa</surname> <given-names>H</given-names></name><name><surname>Lu</surname> <given-names>HD</given-names></name><name><surname>Roe</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional organization for color and orientation in macaque V4</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1542</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1038/nn.2676</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ts'o</surname> <given-names>D</given-names></name><name><surname>Frostig</surname> <given-names>R</given-names></name><name><surname>Lieke</surname> <given-names>E</given-names></name><name><surname>Grinvald</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Functional organization of primate visual cortex revealed by high resolution optical imaging</article-title><source>Science</source><volume>249</volume><fpage>417</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1126/science.2165630</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Knutsen</surname> <given-names>TA</given-names></name><name><surname>Mandeville</surname> <given-names>JB</given-names></name><name><surname>Tootell</surname> <given-names>RBH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Faces and objects in macaque cerebral cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>989</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1038/nn1111</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>DY</given-names></name><name><surname>Freiwald</surname> <given-names>WA</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunoda</surname> <given-names>K</given-names></name><name><surname>Yamane</surname> <given-names>Y</given-names></name><name><surname>Nishizaki</surname> <given-names>M</given-names></name><name><surname>Tanifuji</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Complex objects are represented in macaque inferotemporal cortex by the combination of feature columns</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>832</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1038/90547</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Galkin</surname> <given-names>TW</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Gattass</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical connections of area V4 in the macaque</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>477</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm061</pub-id><pub-id pub-id-type="pmid">17548798</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>G</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Tanifuji</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Optical Imaging of Functional Organization in the Monkey Inferotemporal Cortex</article-title><source>Science</source><volume>272</volume><fpage>1665</fpage><lpage>1668</lpage><pub-id pub-id-type="doi">10.1126/science.272.5268.1665</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Zou</surname> <given-names>P</given-names></name><name><surname>Cohen</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Voltage imaging with genetically encoded indicators</article-title><source>Current Opinion in Chemical Biology</source><volume>39</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/j.cbpa.2017.04.005</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>HH</given-names></name><name><surname>St-Pierre</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Genetically Encoded Voltage Indicators: Opportunities and Challenges</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>9977</fpage><lpage>9989</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1095-16.2016</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yau</surname> <given-names>JM</given-names></name><name><surname>Pasupathy</surname> <given-names>A</given-names></name><name><surname>Brincat</surname> <given-names>SL</given-names></name><name><surname>Connor</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Curvature Processing Dynamics in Macaque Area V4</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>198</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs004</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yetter</surname> <given-names>M</given-names></name><name><surname>Robert</surname> <given-names>S</given-names></name><name><surname>Mammarella</surname> <given-names>G</given-names></name><name><surname>Richmond</surname> <given-names>B</given-names></name><name><surname>Eldridge</surname> <given-names>MAG</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Yue</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curvilinear features are important for animate/inanimate categorization in macaques</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.25.267393</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname> <given-names>X</given-names></name><name><surname>Pourladian</surname> <given-names>IS</given-names></name><name><surname>Tootell</surname> <given-names>RBH</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Curvature-processing network in macaque visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>E3467</fpage><lpage>E3475</lpage><pub-id pub-id-type="doi">10.1073/pnas.1412616111</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zachariou</surname> <given-names>V</given-names></name><name><surname>Del Giacco</surname> <given-names>AC</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Yue</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bottom-up processing of curvilinear visual features is sufficient for animate/inanimate object categorization</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>388</elocation-id><pub-id pub-id-type="doi">10.1167/18.12.3</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63798.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Vinck</surname><given-names>Martin</given-names></name><role>Reviewing Editor</role><aff><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with Max Planck Society</institution><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>van Kerkoerle</surname><given-names>Timo</given-names> </name><role>Reviewer</role><aff><institution/><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Connor</surname><given-names>Ed</given-names> </name><role>Reviewer</role><aff><institution>Johns Hopkins University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Gallant</surname><given-names>Jack L</given-names></name><role>Reviewer</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Two-photon imaging in area V4 of awake monkeys was used to characterize the organization of tuning for distinct shape elements (curves, corners, and bars). The authors use a combination of wide field/low resolution imaging, to visualize large scale organization, with smaller field/high resolution imaging, to measure tuning and organization of individual neurons underlying the wide field results. At both scales, they establish that most V4 neurons are more responsive to curves and corners than to bars, and they establish anatomical segregation between neurons tuned for curves and neurons tuned for bars. These findings advance our understanding of the topographic organization of neuronal feature selectivity in area V4 of the macaque monkey.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Clustered Functional Domains for Curves and Corners in Cortical Area V4&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 4 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Timo van Kerkoerle (Reviewer #2); Ed Connor (Reviewer #3); Jack L Gallant (Reviewer #4).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>A prominent aspect of the visual cortex is the topographic organization in feature dimensions such as orientation, color, motion etc. The present study uses 2-photon imaging in area V4 of awake monkeys, which is a novel application of 2-photon, to characterize the organization of tuning for established shape elements (curves, corners, and bars). The authors use a combination of wide field/low resolution imaging, to visualize large scale organization, with smaller field/high resolution imaging, to measure tuning and organization of individual neurons underlying the wide field results. At both scales, they establish that most V4 neurons are more responsive to curves and corners than to bars, and they establish anatomical segregation between neurons tuned for curves and neurons tuned for bars.</p><p>Overall the reviewers made positive comments about this study especially noting the technological advance and the application of a new high-resolution imaging modality to the question of topographic organisation in area V4, although reviewers also commented that the present study is largely a replication of previous work.</p><p>Nonetheless, because of the technology used here, the reviewers assess that the work is of significant interest. The main comments of the reviewers pertained to the statistical analyses in this manuscript, which will require extensive revisions and data analyses.</p><p>Essential revisions:</p><p>1. Statistics</p><p>Major improvements will be required on the level of statistical and data analyses. In light of these concerns, we require the authors publish the data and software underlying the figures so that the statistical analyses become transparent and can be verified by the reviewers.</p><p>Reviewers commented that statistical analysis is almost completely lacking and is potentially wrong where it is provided. Complex results such as the ones presented by the authors need to be accompanied by appropriate spatial statistics. This will likely require substantial revision to the data analysis and the text. If necessary, the authors should consult a statistical/data science specialist for advice on how to perform the statistical analyses. It remains unclear whether the main claims will survive after appropriate analysis.</p><p>More specifically, it is unclear whether the ANOVA tests for significance of curvature- and corner-selective patches has been performed correctly. It appears that the authors identified curvature-selective patches by subtraction, and then performed the ANOVA on these patches. It is unclear whether this procedure is correct and may amount to double-dipping because regions are pre-selected before statistics are run. This kind of analysis can dramatically increase the Type 1 error rate and lead to false conclusions. Therefore, the significance values that are reported here are likely far more extreme than they would be otherwise. Many tutorials regarding how to do these sorts of tests correctly can be found in the neuroimaging literature, where this sort of problem has been extensively discussed in the literature and where it is standard to address it appropriately. The authors should consult one of those tutorials and implement a strictly correct (probably FDR-based) procedure. For instance, here is a possible starting point (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3221040/).</p><p>Reviewers commented that the statistical analysis to determine the significance of clustering also appear to be problematic. In fact, it is not clear from the details of the paper what has been precisely done. It appears that the CVSI and CNSI were not evaluated statistically, but ANOVA was used to evaluate tuning in some way, which remains unclear. It furthermore appears that spatial clustering was not assessed statistically at all. The lack of statistics and unclarity about statistics does not meet the prevailing standards of the field. Single neuron tuning needs to be assessed with the correct statistical tests, as does spatial clustering. Given these data and the pre-selection methods that were used to identify targets for the high-resolution analysis, this could be tricky. The authors should consult a statistical/data science specialist for advice on how do to these analyses correctly.</p><p>In general the results with the 16x objective likely suffer from double dipping, as they are preselected, therefore the statistical claims about large-scale topographic organization remain unconvincing.</p><p>2. Limitations were noted about the fine-grained analyses with the 16x objective. A limitation of the present work is that there is only one pair of patches for each animal images at 16x. The analyses need to be also extended. The authors should further analyze the topographic organization at the local scale, is the transition sharp or gradual, what is the variability etc. It seems that there is a rather sharp boundary and that the tuning stays relatively flat, looking at the Figures 2A-D, 3A, 4C and seems particular clear in 5A and C (using concentric versus radial gratings). However, there is no real quantification of this. Figure 2I-K shows the tuning over distance, but this analysis seems to be performed without taking the shape of the domain into account. One possibility would be to show a similar plot, but where the axis is taken perpendicular to the boundary of the domain. It seems that the interpretation that the authors give of the data would predict that the selectivity shows a sharp transition at the boundary and stays elevated within the domain. Furthermore, it would be relevant to get an estimate of the averaged selectivity as well as the variability within the domain, separately for the two animals. Finally, it would be relevant to compare both the sharpness of the transition as well as the mean and variability with the domain between the different stimulus set (curves versus angles, and concentric versus radial gratings).</p><p>3. Data visualisation</p><p>The authors should show more raw data (high-resolution fluorescence images with the field of view used for the main analyses), as well as traces of fluorescence as a function of time as is standard with imaging to appreciate the quality of the fluorescence traces (over tens of seconds). In addition showing dF/F responses for single neurons to different stimuli would be important.</p><p>4. Bar tuning</p><p>It needs to be very clear if the small amount of bar tuning reported is only in the ROIs that are defined by subtracting bars (where this would be therefore expected) or overall, in the discussion it currently sounds like this is the case overall which was not clear from the results.</p><p>5. Choice of stimuli</p><p>The exact choice of stimulus needs to be discussed: why only black (other studies used only white stimuli), why only lines (not surfaces as in e.g. Pasupathy et al. study that is referred to), why no colors. Is it assumed that this will not matter for the results and why?</p><p>The bar length is matched to the radius of the curve stimuli, which implies to me that the overall number of black pixels is never matched for bars vs the other categories? The authors should discuss if this is a problem.</p><p>Do you expect more curve/corner functional domains if you use different color or luminance contrast, or do you expect the non-significantly curve/corner clustered parts of V4 to contain other functional domains?</p><p>6. Temporal dynamics</p><p>The imaging technique confines analyses to a late time window. If possible refer to literature demonstrating that response preferences remain similar across time for these stimuli, since tuning can be dynamic over time (e.g. Nandy et al. 2016, Issa and DiCarlo <italic>eLife</italic>).</p><p>7. Introduction and Discussion:</p><p>Intro and Discussion read quite well and a lot of the relevant literature is referred to. But Intro and Discussion could include further/more explicit clarification why exactly this contrast (curve/corner) was used to study functional domains (or is this just a starting point), what other functional domains there could be.</p><p>The paper needs to cite literature relating curve/corner to animate/inanimate contrasts you discuss (e.g. Zachariou et al. 2018, and other work from Yue lab). You may consider a brief discussion/mention the potential use or function of functional topographic clustering (e.g. Kanwisher, DiCarlo), which is proposed to be related to naturalistic experience that is also discussed here without references.</p><p>The history presented in the introductory section of this paper is very strange. The first paper that reported curvature tuning in V4 was the Gallant et al. 1993 paper that is cited ambiguously here. It is true that paper used gratings rather than curved lines, but a neuron that is selective for curved gratings is also likely selective for curved lines. A similar principle holds for the hyperbolic grating selectivity reported in Gallant et al. 1993. The authors should address this directly and acknowledge the relationship late in their paper.</p><p>Similarly, in their subsequent 1996 longer report Gallant et al. argued that neurons selective for curved and hyperbolic gratings were spatially clustered. The data presented in the paper under review is far better than the data that were available to Gallant et al. way back in 1996, but this result was anticipated by that earlier 1996 report, however this finding is not cited.</p><p>The authors should discuss the recent paper by Roe lab on curvature patches using intrinsic optical imaging has just been published in <italic>eLife</italic>: https://elifesciences.org/articles/57261. This paper is relevant for relevant the points above, as they claim that there is a smooth transition from rectilinear to low curvature to high curvature (figure 7).</p><p>The authors should furthermore discuss this recent <italic>eLife</italic> paper on curvature domains, using both intrinsic and 2-photon imaging: https://elifesciences.org/articles/57502</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Clustered Functional Domains for Curves and Corners in Cortical Area V4&quot; for further consideration by <italic>eLife</italic>. Your revised article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Tirin Moore as the Senior Editor, and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p><italic>Reviewer #2:</italic></p><p>The authors replied sufficiently to most of the comments.</p><p>One answer is not clear to me, in response to the comment:</p><p>&quot;Some more details about the expression levels would be useful. Most importantly, it is unclear from Figure 1C-F how homogenous the expression was in the selected region. Could you show a separate image where it is possible to judge the level of expression? Also, would it be possible to give an estimate of the general expression levels in terms of percentage of total neurons, as well as the percentage of neurons that were nucleus filled? Finally, it would be relevant to know injection speed in this regard.&quot;</p><p>First of all, they still do not provide the injection speed.</p><p>Also, they write: &quot;Most of the neurons that are clearly visible in an average image are not nucleus filled (Figure 1—figure supplement 2).&quot;</p><p>However, Figure 1—figure supplement 2 does not show any individual cells. Nor do any of the other supplementary figures provide an image where it is possible to judge the structure of the labelling in individual cells, so allowing to see whether they have a clear donut shape, or are nucleus filled. It would therefore still be relevant to see a large / high resolution image where this can be judged.</p><p><italic>Reviewer #4:</italic></p><p>The authors have put a lot of work into this revision and the paper is substantially improved over the initial submission. The paper is still largely replicative and confirmatory, but there is a place in the literature for such papers.</p><p>It is reported that the V4 receptive fields sampled here were very close to the fovea. That implies that the viewing window was very far lateral, much farther than most prior V4 studies. My intuition is that the ear would have had to be removed in order to access V4 at this location. If the authors recorded more medially then I suggest that they recheck their reported eccentricity to be sure that it is correct.</p><p>The indexes that are used here have a pretty unintuitive and unusual scaling range. (For example, an index of 0.2 indicates a 1.5 times difference.) The paper would probably be easier to understand if they had a more intuitive range/form. (For example, if 1.5 indicated a 1.5 times difference.) However, this is up to the authors' discretion.</p><p>Figure 2I &quot;significant&quot; is misspelled. There are also a few places throughout the manuscript where pronouns are missing. (I commend the authors on the English though, it is generally quite good!)</p><p>Also in Figure 2, please spell out what &quot;CVSI&quot; and &quot;CNSI&quot; mean in the caption. In this and other captions, it is best if the reader can generally understand the caption on its own, w/o having to wade through the text.</p><p>The use of hexagonal segments to try to understand differences in tuning for curves versus angles is a weak approach, because hexagonal shapes are a poor intermediate model for these feature classes. A much more powerful method for understanding these differences would be to use an explicit computational model. But that seems to be beyond the scope of this paper…</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63798.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. Statistics</p><p>Major improvements will be required on the level of statistical and data analyses. In light of these concerns, we require the authors publish the data and software underlying the figures so that the statistical analyses become transparent and can be verified by the reviewers.</p><p>Reviewers commented that statistical analysis is almost completely lacking and is potentially wrong where it is provided. Complex results such as the ones presented by the authors need to be accompanied by appropriate spatial statistics. This will likely require substantial revision to the data analysis and the text. If necessary, the authors should consult a statistical/data science specialist for advice on how to perform the statistical analyses. It remains unclear whether the main claims will survive after appropriate analysis.</p><p>More specifically, it is unclear whether the ANOVA tests for significance of curvature- and corner-selective patches has been performed correctly. It appears that the authors identified curvature-selective patches by subtraction, and then performed the ANOVA on these patches. It is unclear whether this procedure is correct and may amount to double-dipping because regions are pre-selected before statistics are run. This kind of analysis can dramatically increase the Type 1 error rate and lead to false conclusions. Therefore, the significance values that are reported here are likely far more extreme than they would be otherwise. Many tutorials regarding how to do these sorts of tests correctly can be found in the neuroimaging literature, where this sort of problem has been extensively discussed in the literature and where it is standard to address it appropriately. The authors should consult one of those tutorials and implement a strictly correct (probably FDR-based) procedure. For instance, here is a possible starting point (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3221040/).</p></disp-quote><p>We agree that the map subtraction used in the previous version of the manuscript is more appropriate as a visualization of the data trends but not a strict statistical analysis. In the revised manuscript, we now have performed independent t-test to each pixel using Benjamini–Hochberg FDR correction and cluster permutation testing following the recommendations in Nichols and Holmes 2002 (Hum. Brain Mapp.). The new FDR q-value, replacing the previous SD maps, can be found in Figure 1F-G. The details of the new analysis can be found in the Methods – Curve and corner domains section line 491-507. In brief, p-value was computed for each pixel comparing the responses to all curves, all corners and all bars, and corrected by BH FDR (Figure 1 — figure supplement 3). Cluster permutation tests were performed to exclude patches (q-value&lt;0.01) with too few pixels (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). We used uncorrected p-values instead of q-values in permutation to build the null distribution and compared to the real FDR clusters because otherwise the cluster sizes in the permutations would be too small. This would result in larger cluster sizes in the null distribution and therefore larger critical value, and make the Type 1 error rate even lower.</p><p>As the shape of the domains are slightly changed using the corrected procedures, the example neuron 2 in <italic><bold>Figure 2G</bold></italic> of the old version is now outside the previous curve domain, so we have changed it to another neuron.</p><fig id="sa2fig1"><label>Author response image 1.</label><caption><title>The null distribution of cluster permutation test (in descending rank order).</title><p>The 3000<sup>th</sup> (top 0.05) maximum cluster size (in pixels) is chosen as threshold.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Reviewers commented that the statistical analysis to determine the significance of clustering also appear to be problematic. In fact, it is not clear from the details of the paper what has been precisely done. It appears that the CVSI and CNSI were not evaluated statistically, but ANOVA was used to evaluate tuning in some way, which remains unclear. It furthermore appears that spatial clustering was not assessed statistically at all. The lack of statistics and unclarity about statistics does not meet the prevailing standards of the field. Single neuron tuning needs to be assessed with the correct statistical tests, as does spatial clustering. Given these data and the pre-selection methods that were used to identify targets for the high-resolution analysis, this could be tricky. The authors should consult a statistical/data science specialist for advice on how do to these analyses correctly.</p><p>In general the results with the 16x objective likely suffer from double dipping, as they are preselected, therefore the statistical claims about large-scale topographic organization remain unconvincing.</p></disp-quote><p>We agree that CVSI and CNSI should be statistically analyzed. In fact, we only performed ANOVA to compare the responses to the optimal curve against the optimal corner (<italic><bold>Figure 3D</bold></italic>) to evaluate CVCNI. (As we were comparing only 2 conditions, it’s equivalent to independent t-test). We only compared the optimal stimuli for each neuron instead of all the stimuli as in the 4x imaging because single neurons are often very selective, while 4x signals are Gaussian smoothed and are therefore contributed by many nearby cell bodies and neurites. We added the same analysis for CVSI and CNSI in <italic><bold>Figure 2 — figure supplement 3A-B.</bold></italic> For CVSI, we compare the maximum response to curves against the maximum response to corners and bars (one-way ANOVA, repeat = 10, p &lt; 0.05), and 70.5% (74 out of 105) neurons with CVSI &gt; 0.2 significantly preferred curves over corners and bars. For CNSI it was 76.9% (120 out of 156).</p><p>We also added permutation test to the clustering analysis in Figure 2I-K. For instance, in Figure 2I, tuning correlation and distances were randomly paired for 100,000 times to build the null distribution and average to derive the dash curve. A point was considered significant if it’s higher than the top 100 of the null distribution (p &lt; 0.001) or lower than the bottom 100 (Methods line 556-559). We said in the old version that the curve and corner domains are about 400 μm in size, now we changed it to 300.</p><disp-quote content-type="editor-comment"><p>2. Limitations were noted about the fine-grained analyses with the 16x objective. A limitation of the present work is that there is only one pair of patches for each animal images at 16x. The analyses need to be also extended.</p></disp-quote><p>The possible choice for 16x imaging is quite limited (Figure 1F-G and Figure 1 — figure supplement 3). Yue et al. 2014 reported only one significant curvature patch in dorsal V4, and we would not be surprised if these patches are not too many in V4d. We also have to avoid dense blood vessels in 16x imaging as it will heavily affect the visualization of clustering. In fact, we had another monkey but GCaMP6s was expressed in that monkey. GCaMP6s is easily saturated and hard to differentiate strong and weak responses, and therefore may be not extremely suitable for quantitative and semi-quantitative analysis, though its absolute response can be very strong.</p><disp-quote content-type="editor-comment"><p>The authors should further analyze the topographic organization at the local scale, is the transition sharp or gradual, what is the variability etc. It seems that there is a rather sharp boundary and that the tuning stays relatively flat, looking at the Figures 2A-D, 3A, 4C and seems particular clear in 5A and C (using concentric versus radial gratings). However, there is no real quantification of this. Figure 2I-K shows the tuning over distance, but this analysis seems to be performed without taking the shape of the domain into account. One possibility would be to show a similar plot, but where the axis is taken perpendicular to the boundary of the domain. It seems that the interpretation that the authors give of the data would predict that the selectivity shows a sharp transition at the boundary and stays elevated within the domain. Furthermore, it would be relevant to get an estimate of the averaged selectivity as well as the variability within the domain, separately for the two animals. Finally, it would be relevant to compare both the sharpness of the transition as well as the mean and variability with the domain between the different stimulus set (curves versus angles, and concentric versus radial gratings).</p></disp-quote><p>This is a very good point. We now show the plot over distances to the boundary in Figure 2 — figure supplement 3 and Figure 5 — figure supplement 2. To us the transition is rather gradual. Generally it took around 300 μm before getting elevated. The transition of CRI maps of monkey A in Figure 5A intuitively looked sharp probably because too many neurons had negative CRI values. CRI and CVCNI were correlated but not identical. Since concentric gratings only have 360° full circles but some neurons might prefer short arch (small radian, Figure 3 — figure supplement), it is possible that they don’t respond strongly to concentric gratings. We added these discussions in the revised manuscript (line 313-321).</p><disp-quote content-type="editor-comment"><p>3. Data visualisation</p><p>The authors should show more raw data (high-resolution fluorescence images with the field of view used for the main analyses), as well as traces of fluorescence as a function of time as is standard with imaging to appreciate the quality of the fluorescence traces (over tens of seconds). In addition showing dF/F responses for single neurons to different stimuli would be important.</p></disp-quote><p>Yes, we have added the raw fluorescence traces of neurons in Figure 2 — figure supplement 1. We show the responses to the optimal curves, corners and bars together in figure supplement 2.</p><disp-quote content-type="editor-comment"><p>4. Bar tuning</p><p>It needs to be very clear if the small amount of bar tuning reported is only in the ROIs that are defined by subtracting bars (where this would be therefore expected) or overall, in the discussion it currently sounds like this is the case overall which was not clear from the results.</p></disp-quote><p>It is only in the FOVs defined in <bold>Figure 1F-G</bold> white box. Sorry we didn’t make it clear. We wrote in the old version L297: “This was not the case for our data, which we infer is primarily due to sampling neurons within or close to curve and corner domains.” We considered it the reason for both the above two points but it unfortunately seemed ambiguous. We have rewritten this part the revised manuscript (line 327-335).</p><disp-quote content-type="editor-comment"><p>5. Choice of stimuli</p><p>The exact choice of stimulus needs to be discussed: why only black (other studies used only white stimuli), why only lines (not surfaces as in e.g. Pasupathy et al. study that is referred to), why no colors. Is it assumed that this will not matter for the results and why?</p></disp-quote><p>Bushnell and Pauspathy 2012 study demonstrated shape encoding are largely consistent across colors. So we think the curve/corner preference may be to some extent color invariant and the choice of color may not matter too much. Besides, from our natural image result we did not find dominant color dimension in the curve and corner domains, and the preferred natural images of curves can be of various color. Our V1 study used mostly black lines (Tang et al. 2018), so we just used black lines.</p><disp-quote content-type="editor-comment"><p>The bar length is matched to the radius of the curve stimuli, which implies to me that the overall number of black pixels is never matched for bars vs the other categories? The authors should discuss if this is a problem.</p></disp-quote><p>Yes, but the long bars are precisely matched to the small corners in pixel number, and very closely matched to 60° curves (length ratio = 1:π/3 = 1:1.05). We compare the maximum responses to 60° curves against bars for curve selective neurons in Figure 3D and Figure 3 — figure supplement 1C-D, and we found 52.4% significantly preferred 60° curves to bars, only 7.1% preferred bars to 60° curves (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>). An example of this is Neuron 3 in Figure 2G. Note that many neurons preferred longer curves and might not even respond to 60° curves (Figure 3 — figure supplement 1D). So we think even with similar pixel number many neurons still preferred curves over bars, and curve preference is not a mere artifact of more pixels.</p><fig id="sa2fig2"><label>Author response image 2.</label><caption><title>Scatterplot showing the maximum response to 60° curves against the maximum response to bars of the curve selective neurons.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63798-resp-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Do you expect more curve/corner functional domains if you use different color or luminance contrast, or do you expect the non-significantly curve/corner clustered parts of V4 to contain other functional domains?</p></disp-quote><p>We are not exactly sure if more curve/corner domains can be found, but Bushnell and Pauspathy 2012 reported shape tuning remain consistent across color, and from our natural images result the neurons in one curve or corner could respond to images containing curves or corners of various color.</p><p>There are certainly other functional domains in V4, such as orientation (Tanigawa et al. 2010), color (Conway et al. 2007), spatial frequency (Lu et al. 2018) and 3D vision (Srinath et al. 2020).</p><disp-quote content-type="editor-comment"><p>6. Temporal dynamics</p><p>The imaging technique confines analyses to a late time window. If possible refer to literature demonstrating that response preferences remain similar across time for these stimuli, since tuning can be dynamic over time (e.g. Nandy et al. 2016, Issa and DiCarlo eLife).</p></disp-quote><p>Yes, the time it takes for Calcium influx and accumulation limits the temporal resolution so that we only record late responses. In fact, it’s known that the early and late responses of V4 neurons could be quite different (Yau et al. 2013). The early responses are considered feed-forward signals and are therefore more tuned to local orientation. Complex pattern preference emerges gradually and is more likely to follow a recurrent model. We are currently undertaking some loose patch recordings in V4 and we find that the early and late responses can be different. The limitation of temporal resolution cannot at present be overcome using Calcium based fluorescence. Hopefully the newly developed neurotransmitter or voltage sensors may help to achieve high temporal resolution imaging. We have added some of this to the discussion in the revised manuscript (line 353-362).</p><disp-quote content-type="editor-comment"><p>7. Introduction and Discussion:</p><p>Intro and Discussion read quite well and a lot of the relevant literature is referred to. But Intro and Discussion could include further/more explicit clarification why exactly this contrast (curve/corner) was used to study functional domains (or is this just a starting point), what other functional domains there could be.</p></disp-quote><p>We used curve/corner based on the early version of this manuscript posted on bioRxiv in 2019: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/808907v2.full">https://www.biorxiv.org/content/10.1101/808907v2.full</ext-link> (The recent 2 <italic>eLife</italic> papers from Roe and Lu both cited this manuscript). In brief, we recorded V4 neurons’ responses to thousands of natural images, and purely data-driven dimensional reduction on population responses without any apriori assumption. Apart from feature dimensions encoding simple orientation, we also found a dimension encoding curves and corners (on the opposite directions of the axis). The curve and corner selective neurons were separately spatially clustered. However, there are very limited curve and corner domains, and may other functional domains like orientation (Tanigawa et al. 2010), color (Conway et al. 2007) and spatial frequency (Lu et al. 2018) could be found in V4. If the result is to be repeated, one will have to first localize the curve/corner domain and then recording natural images responses (otherwise other feature dimensions might be more dominant), which is against the idea of “purely data-driven without apriori assumption”. So we removed the natural image part and simplify the manuscript to only focus on curve and corner domains in V4. We added some of these to the discussion in the revised manuscript (line 299-302).</p><disp-quote content-type="editor-comment"><p>The paper needs to cite literature relating curve/corner to animate/inanimate contrasts you discuss (e.g. Zachariou et al. 2018, and other work from Yue lab). You may consider a brief discussion/mention the potential use or function of functional topographic clustering (e.g. Kanwisher, DiCarlo), which is proposed to be related to naturalistic experience that is also discussed here without references.</p></disp-quote><p>We have restructured the final paragraph of the discussion to encompass the thread of ideas a little bit more clearly, and we have added references related to the concepts of functional specialization, including its specificity in terms of cognition.</p><disp-quote content-type="editor-comment"><p>The history presented in the introductory section of this paper is very strange. The first paper that reported curvature tuning in V4 was the Gallant et al. 1993 paper that is cited ambiguously here. It is true that paper used gratings rather than curved lines, but a neuron that is selective for curved gratings is also likely selective for curved lines. A similar principle holds for the hyperbolic grating selectivity reported in Gallant et al. 1993. The authors should address this directly and acknowledge the relationship late in their paper.</p><p>Similarly, in their subsequent 1996 longer report Gallant et al. argued that neurons selective for curved and hyperbolic gratings were spatially clustered. The data presented in the paper under review is far better than the data that were available to Gallant et al. way back in 1996, but this result was anticipated by that earlier 1996 report, however this finding is not cited.</p></disp-quote><p>Yes, we should have directly acknowledged these findings. These studies are now referred to on line 50-53 and line 250-253 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>The authors should discuss the recent paper by Roe lab on curvature patches using intrinsic optical imaging has just been published in eLife: https://elifesciences.org/articles/57261. This paper is relevant for relevant the points above, as they claim that there is a smooth transition from rectilinear to low curvature to high curvature (figure 7).</p><p>The authors should furthermore discuss this recent eLife paper on curvature domains, using both intrinsic and 2-photon imaging: https://elifesciences.org/articles/57502</p></disp-quote><p>These papers are now referred to on line 305-311 (both papers cite our 2019 preprint, and were not yet published when we submitted our manuscript to <italic>eLife</italic>). Roe’s paper used curved gratings vs straight gratings, while in our case it’s mainly curve vs corner. The 90° corners and the Π-shapes we used were of comparable level of curvature as smooth curves, but were encoded more in the corner domains. So we feel that our data may not be suitable to address the low curvature to high curvature transition problem.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The authors replied sufficiently to most of the comments.</p><p>One answer is not clear to me, in response to the comment:</p><p>&quot;Some more details about the expression levels would be useful. Most importantly, it is unclear from Figure 1C-F how homogenous the expression was in the selected region. Could you show a separate image where it is possible to judge the level of expression? Also, would it be possible to give an estimate of the general expression levels in terms of percentage of total neurons, as well as the percentage of neurons that were nucleus filled? Finally, it would be relevant to know injection speed in this regard.&quot;</p><p>First of all, they still do not provide the injection speed.</p></disp-quote><p>We are sorry for missing this in the previous revision. The injection speed is 5-10 nl/s. We have added this to Materials and methods line 408 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Also, they write: &quot;Most of the neurons that are clearly visible in an average image are not nucleus filled (Figure 1—figure supplement 2).&quot;</p><p>However, Figure 1—figure supplement 2 does not show any individual cells. Nor do any of the other supplementary figures provide an image where it is possible to judge the structure of the labelling in individual cells, so allowing to see whether they have a clear donut shape, or are nucleus filled. It would therefore still be relevant to see a large / high resolution image where this can be judged.</p></disp-quote><p>We firstly apologize for our mistaken figure labelling. Figure 1—figure supplement 2 here should be Figure 2—figure supplement 1A. We have now rearranged the layout of Figure 2—figure supplement 1 in the manuscript to make the image larger and have uploaded the source TIF file in the system. We hope that this revised figure satisfies your concern.</p><disp-quote content-type="editor-comment"><p>Reviewer #4:</p><p>The authors have put a lot of work into this revision and the paper is substantially improved over the initial submission. The paper is still largely replicative and confirmatory, but there is a place in the literature for such papers.</p><p>It is reported that the V4 receptive fields sampled here were very close to the fovea. That implies that the viewing window was very far lateral, much farther than most prior V4 studies. My intuition is that the ear would have had to be removed in order to access V4 at this location. If the authors recorded more medially then I suggest that they recheck their reported eccentricity to be sure that it is correct.</p></disp-quote><p>Yes, the optical window is indeed quite lateral. As can be seen in <italic><bold>Figure 1B</bold></italic>, a large part of the IOS and PIT is included in the 10 mm diameter window, and we were imaging in the lower half. Nevertheless, since the imaging window is smaller than those used for ISOI we did not remove any of the auricle; though the edge of the window was very close to it. In fact, we can also implant such windows to image PIT without removing the auricle.</p><disp-quote content-type="editor-comment"><p>The indexes that are used here have a pretty unintuitive and unusual scaling range. (For example, an index of 0.2 indicates a 1.5 times difference.) The paper would probably be easier to understand if they had a more intuitive range/form. (For example, if 1.5 indicated a 1.5 times difference.) However, this is up to the authors' discretion.</p></disp-quote><p>We agree that the selectivity indexes can be unintuitive. But such a definition is often used to characterize orientation selectivity (OSI, orientation selectivity index), and is also commonly used in some recent imaging studies such as those by Wilson et al. 2018 and Garg et al. 2019. We therefore think this precedent justifies the use such indexes.</p><disp-quote content-type="editor-comment"><p>Figure 2I &quot;significant&quot; is misspelled. There are also a few places throughout the manuscript where pronouns are missing. (I commend the authors on the English though, it is generally quite good!)</p></disp-quote><p>We have corrected the misspelling in Figure 2I, and thank you for your commendation.</p><disp-quote content-type="editor-comment"><p>Also in Figure 2, please spell out what &quot;CVSI&quot; and &quot;CNSI&quot; mean in the caption. In this and other captions, it is best if the reader can generally understand the caption on its own, w/o having to wade through the text.</p></disp-quote><p>We agree this is helpful to the reader. We now spell them out in the captions of Figure 2A-B, as well as Figure 3A and Figure 5A.</p><disp-quote content-type="editor-comment"><p>The use of hexagonal segments to try to understand differences in tuning for curves versus angles is a weak approach, because hexagonal shapes are a poor intermediate model for these feature classes. A much more powerful method for understanding these differences would be to use an explicit computational model. But that seems to be beyond the scope of this paper…</p></disp-quote><p>This is a very valid criticism. We are still working on a computational model to understand how neurons encode curves and corners differently and also to interpret our natural images data. But, as you said, it’s beyond the current scope of this paper. We do hope that we can better address this in our future work.</p></body></sub-article></article>