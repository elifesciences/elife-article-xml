<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93589</article-id><article-id pub-id-type="doi">10.7554/eLife.93589</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93589.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The spatial frequency representation predicts category coding in the inferior temporal cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Toosi</surname><given-names>Ramin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7099-9353</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Karami</surname><given-names>Behnam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6594-6516</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Koushki</surname><given-names>Roxana</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Shakerian</surname><given-names>Farideh</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-8426-5879</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Noroozi</surname><given-names>Jalaledin</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rezayat</surname><given-names>Ehsan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3808-1283</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Vahabie</surname><given-names>Abdol-Hossein</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1603-8866</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Akhaee</surname><given-names>Mohammad Ali</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3753-5618</contrib-id><email>akhaee@ut.ac.ir</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Dehaqani</surname><given-names>Mohammad-Reza A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4365-4365</contrib-id><email>dehaqani@ut.ac.ir</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vf56z40</institution-id><institution>School of Electrical and Computer Engineering, College of Engineering, University of Tehran</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xreqs31</institution-id><institution>School of Cognitive Sciences, Institute for Research in Fundamental Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Donders Centre for Cognitive Neuroimaging, Donders Institute for Brain, Cognition, and Behaviour, Radboud University</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0126z4b94</institution-id><institution>Department of Brain and Cognitive Sciences, Cell Science Research Center, Royan Institute for Stem Cell Biology and Technology, ACECR</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vf56z40</institution-id><institution>Department of Cognitive Sciences, Faculty of Psychology and Education, University of Tehran</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>09</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP93589</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-11-02"><day>02</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-11-07"><day>07</day><month>11</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.07.566068"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-17"><day>17</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93589.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-28"><day>28</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93589.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-02-27"><day>27</day><month>02</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93589.3"/></event></pub-history><permissions><copyright-statement>© 2024, Toosi et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Toosi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93589-v1.pdf"/><abstract><p>Understanding the neural representation of spatial frequency (SF) in the primate cortex is vital for unraveling visual processing in object recognition. While many studies focus on SF in the primary visual cortex, the characteristics and interaction of SF with category representation remain unclear. To explore SF representation in the inferior temporal (IT) cortex of macaques, we conducted extracellular recordings with complex stimuli systematically filtered by SF. Our findings reveal explicit SF coding at both single-neuron and population levels. Temporal dynamics analysis of SF representation reveals that low SF (LSF) is decoded faster than high SF (HSF), and the SF preference dynamically shifts from LSF to HSF over time. Additionally, the SF representation for each neuron forms a profile that predicts category selectivity at the population level. IT neurons cluster into four groups based on SF preference, each with distinct category coding behaviors. Notably, HSF-preferring neurons show the highest category decoding for faces. Despite the existing connection between SF and category coding, we have identified uncorrelated representations of SF and category. Unlike category coding, SF is more sparsely represented and depends more on individual neurons. These findings dissociate SF and category representations, underscoring SF’s pivotal role in object recognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>spatial frequency</kwd><kwd>inferior temporal cortex</kwd><kwd>object recognition</kwd><kwd>coarse-to-fine processing</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Spatial frequency is explicitly and dynamically represented in the primate inferior temporal cortex, revealing distinct neural populations that differentially support object recognition, specially emphasizing high-frequency contributions to face processing.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Spatial frequency (SF) constitutes a pivotal component of visual stimuli encoding in the primate visual system, encompassing the number of grating cycles within a specific visual angle. Higher SF (HSF) corresponds to intricate details, while lower SF (LSF) captures broader information. Previous psychophysical studies have compellingly demonstrated the profound influence of SF manipulation on object recognition and categorization processes (<xref ref-type="bibr" rid="bib29">Joubert et al., 2007</xref>; <xref ref-type="bibr" rid="bib39">Schyns and Oliva, 1994</xref>; <xref ref-type="bibr" rid="bib3">Ashtiani et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Caplette et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Cheung and Bar, 2014</xref>; <xref ref-type="bibr" rid="bib15">Craddock et al., 2013</xref>). <xref ref-type="bibr" rid="bib27">Jahfari et al., 2013</xref> and <xref ref-type="bibr" rid="bib38">Saneyoshi and Michimata, 2015</xref> have highlighted the significance of HSF and LSF for categorical/coordinate processing and in object recognition and decision-making, respectively. The sequence in which SF content is presented also affects the categorization performance, with coarse-to-fine presentation leading to faster categorizations (<xref ref-type="bibr" rid="bib30">Kauffmann et al., 2015</xref>). Considering face as a particular object, several studies showed that middle and higher SFs are more critical for face recognition (<xref ref-type="bibr" rid="bib12">Cheung et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Costen et al., 1996</xref>; <xref ref-type="bibr" rid="bib21">Fiorentini et al., 1983</xref>; <xref ref-type="bibr" rid="bib24">Hayes et al., 1986</xref>). Another vital theory suggested by psychophysical studies is the coarse-to-fine perception of visual stimuli, which states that LSF or global contents are processed faster than HSF or local contents (<xref ref-type="bibr" rid="bib22">Gao and Bentin, 2011</xref>; <xref ref-type="bibr" rid="bib30">Kauffmann et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Rokszin et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Rotshtein et al., 2010</xref>; <xref ref-type="bibr" rid="bib39">Schyns and Oliva, 1994</xref>; <xref ref-type="bibr" rid="bib43">Yardley et al., 2012</xref>). Despite the extensive reliance on psychophysical studies to examine the influence of SF on categorization tasks, our understanding of SF representation within primate visual systems, particularly in higher visual areas like the inferior temporal (IT) cortex, remains constrained due to the limited research in this specific domain.</p><p>One of the seminal studies investigating the neural correlates of SF processing and its significance in object recognition was conducted by <xref ref-type="bibr" rid="bib6">Bar, 2003</xref>. Their research proposes a top–down mechanism driven by the rapid processing of LSF content, facilitating object recognition (<xref ref-type="bibr" rid="bib6">Bar, 2003</xref>; <xref ref-type="bibr" rid="bib19">Fenske et al., 2006</xref>). The exploration of SF representation has revealed the engagement of distinct brain regions in processing various SF contents (<xref ref-type="bibr" rid="bib7">Bastin et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Bermudez et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Chaumon et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Cheung and Bar, 2014</xref>; <xref ref-type="bibr" rid="bib20">Fintzi and Mahon, 2014</xref>; <xref ref-type="bibr" rid="bib23">Gaska et al., 1988</xref>; <xref ref-type="bibr" rid="bib26">Iidaka et al., 2004</xref>; <xref ref-type="bibr" rid="bib33">Oram and Perrett, 1994</xref>; <xref ref-type="bibr" rid="bib34">Peyrin et al., 2010</xref>). More specifically, the orbitofrontal cortex has been identified as accessing global (LSF) and local (identity; HSF) information in the right and left hemispheres, respectively (<xref ref-type="bibr" rid="bib20">Fintzi and Mahon, 2014</xref>). The V3A area exhibits low-pass tuning curves (<xref ref-type="bibr" rid="bib23">Gaska et al., 1988</xref>), while HSF processing activates the left fusiform gyrus (<xref ref-type="bibr" rid="bib26">Iidaka et al., 2004</xref>). Neural responses in the IT cortex, which play a pivotal role in object recognition and face perception, demonstrate correlations with the SF components of complex stimuli (<xref ref-type="bibr" rid="bib8">Bermudez et al., 2009</xref>). Despite the acknowledged importance of SF as a critical characteristic influencing object recognition, a more comprehensive understanding of its representation is warranted. By unraveling the neural mechanisms underlying SF representation in the IT cortex, we can enrich our comprehension of the processing and categorization of visual information.</p><p>To address this issue, we investigate the SF representation in the IT cortex of two passive-viewing macaque monkeys. We studied the neural responses of the IT cortex to intact, SF-filtered (five ranges), and phase-scrambled stimuli. SF decoding is observed in both population- and single-level representations. Investigating the decoding pattern of individual SF bands reveals a course-to-fine manner in recall performance where LSF is decoded more accurately than HSF. Temporal dynamics analysis shows that SF coding exhibits a coarse-to-fine pattern, emphasizing faster processing of lower frequencies. Moreover, SF representation forms an average LSF-preferred tuning across neuron responses at 70–170 ms after stimulus onset. Then, the average preferred SF shifts monotonically to HSF in time after the early phase of the response, with its peak at 220 ms after the stimulus onset. The LSF-preferred tuning turns into an HSF-preferred one in the late neuron response phase.</p><p>Next, we examined the relationship between SF and category coding. We found a strong positive correlation between SF and category coding performances in sub-populations of neurons. SF coding capability of individual neurons is highly correlated with the category coding capacity of the sub-population. Moreover, clustering neurons based on their SF responses indicates a relationship between SF representation and category coding. Employing the neuron responses to five SF ranges considering only the scrambled stimuli, an SF profile was identified for each neuron that predicts the categorization performance of that neuron in a population of the neurons sharing the same profile. Neurons whose response increases with increasing SF encode faces better than other neuron populations with other profiles.</p><p>Given the co-existence of SF and category coding within the IT cortex and the prediction capability of SF for category selectively, we examined the neural mechanisms underlying SF and category representation. In single level, we found no correlation between SF and category coding capability of single neurons. At the population level, we found that the contribution of neurons to SF coding did not correlate with their contribution to category coding. Delving into the characteristics of SF coding, we found that individual neurons carry more independent SF-related information compared to the encoding of categories (face vs. non-face). Analyzing the temporal dynamics of each neuron’s contribution to population-level SF coding reveals a shift in sparsity during different phases of the response. In the early phase (70–170 ms), the contribution is more sparse than category coding. However, this behavior is reversed in the late phase (170–270 ms), with SF coding showing a less sparse contribution.</p><p>Finally, we compared the representation of SF in the IT cortex with several popular convolutional neural networks (CNNs). We found that CNNs exhibited robust SF coding capabilities with significantly higher accuracies than the IT cortex. Like the IT cortex, LSF content showed higher decoding performance than the HSF content. However, while there were similarities in SF representation, CNNs did not replicate the SF-based profiles predicting neuron category selectivity observed in the IT cortex. We posit that our findings establish neural correlates pertinent to behavioral investigations into SF’s role in object recognition. Additionally, our results shed light on how the IT cortex represents and utilizes SF during the object recognition process.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>SF coding in the IT cortex</title><p>To study the SF representation in the IT cortex, we designed a passive stimulus presentation task (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, see Materials and methods). The task comprises two phases: the selectivity and the main. During the selectivity phase, 155 stimuli, organized into two super-ordinate and four ordinate categories, were presented (with a 50-ms stimulus presentation followed by a 500-ms blank period, see Materials and methods). Next, the six most responsive stimuli are selected along with nine fixed stimuli (six faces and three non-face objects, <xref ref-type="fig" rid="fig1">Figure 1b</xref>) to be presented during the main phase (33-ms stimulus presentation followed by a 465-ms blank, see Materials and methods). Each stimulus is phase scrambled, and then the intact and scrambled versions are filtered in five SF ranges (R1–R5, with R5 representing the highest frequency band, <xref ref-type="fig" rid="fig1">Figure 1b</xref>), resulting in a total of 180 unique stimuli presented in each session (see Materials and methods). Each session consists of 15 blocks, with each stimulus presented once per block in a random order. The IT neurons of passive-viewing monkeys are recorded where the cells cover all areas of the IT area uniformly (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). We only considered the responsive neurons (see Materials and methods), totaling 266 (157 M1 and 109 M2). A sample neuron (neuron #155, M1) peristimulus time histogram is illustrated in <xref ref-type="fig" rid="fig1">Figure 1c</xref> in response to the scrambled stimuli for R1, R3, and R5. R1 exhibits the most pronounced firing rate, indicating the highest neural activity level. In contrast, R5 displays the lowest firing rate, suggesting an LSF-preferred trend in the neuron’s response. To explore the SF representation and coding capability of IT neurons, each stimulus in each session block is represented by an <italic>N</italic>-element vector where the <italic>i</italic>th element is the average response of the <italic>i</italic>th neuron to that stimulus within a 50-ms time window (see Materials and methods).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and spatial frequency (SF) coding.</title><p>(<bold>a</bold>) <italic>Experimental design</italic>. The design of the experiment involved the collection of responses from inferior temporal (IT) neurons to 15 stimuli (including six faces, three non-faces, and six selective stimuli, see Materials and methods) in six SF bands (intact and R1–R5, see Materials and methods), and two versions (scrambled and unscrambled) using a passive presentation task. The presentation of blocks starts if the monkey preserves fixation for 200 ms. Each block consisted of a 33-ms stimulus presentation followed by a blank screen with a fixation point of 465 ms, and each stimulus was presented 15 times. The recorded signals were sorted, and visually responsive neurons were selected (<italic>N</italic> = 266, see Materials and methods). (<bold>b</bold>) <italic>A sample of the fixed stimulus set</italic>. This panel shows three (out of six) faces, three non-faces, and one scrambled sample stimulus. Each row corresponds to an SF range starting with intact, followed by R1–R5 (low to high SF). (<bold>c</bold>) <italic>A sample neuron</italic>. The peristimulus time histogram (PSTH) of a sample neuron (<italic>N</italic> = 151, M1) for scrambled stimuli is depicted. To generate a response vector for a given stimulus or trial, the responses of each neuron were averaged in a 50-ms time window centered around the relevant time point. The PSTH was smoothed using a Gaussian kernel with a standard deviation of 20 ms. The responses of three SF bands (R1, R3, and R5) are shown for better illustration. (<bold>d</bold>) <italic>SF coding exists in the IT cortex</italic>. The decoding performance of SF ranges using scrambled stimuli is shown over time. Single- and population-level representations were fed into a linear discriminant analysis (LDA) algorithm to predict the SF range of the scrambled stimuli. Shadows illustrate the SEM and STD for single and population levels, respectively. This figure highlights the presence of SF coding in both individual and population neural activity. (<bold>e</bold>) <italic>Low SF (LSF)-preferred nature of SF coding</italic>. The population recall of each SF band in response to scrambled stimuli, determined using the LDA method, is presented. The error bars indicate the STD. The results demonstrate a decreasing trend as SF moves toward higher frequencies, suggesting a coarse-to-fine decoding preference.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Source data for neuronal firing rates, SF decoding accuracy at single-unit and population levels, and recall of SF decoding per SF.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-93589-fig1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-fig1-v1.tif"/></fig><p>To assess whether individual neurons encode SF-related information, we utilized the linear discriminant analysis (LDA) method to predict the SF range of the scrambled stimuli based on neuron responses (see Materials and methods). <xref ref-type="fig" rid="fig1">Figure 1d</xref> displays the average time course of SF discrimination accuracy across neurons. The accuracy value is normalized by subtracting the chance level (0.2). At single level, the accuracy surpasses the chance level by an average of 4.02% at 120 ms after stimulus onset. We only considered neurons demonstrating at least three consecutive time windows with accuracy significantly greater than the chance level, resulting in a subset of 105 neurons. The maximum accuracy of a single neuron was 19.08% higher than the chance level (unnormalized accuracy is 39.08%, neuron #193, M2). Subsequently, the SF decoding performance of the IT population is investigated (R1–R5 and scrambled stimuli only, see Materials and methods). <xref ref-type="fig" rid="fig1">Figure 1d</xref> also illustrates the SF classification accuracy across time in population-level representations. The peak accuracy is 24.68% higher than the chance level at 115 ms after the stimulus onset. These observations indicate the explicit presence of SF coding in the IT cortex. The strength of SF selectivity, considering the trial-to-trial variability, is provided in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, by ranking the SF bands for each neuron based on half of the trials and then plotting the average responses for the obtained ranks for the other half of the trials. To determine the discrimination of each SF range, <xref ref-type="fig" rid="fig1">Figure 1e</xref> shows the recall of each SF content for the time window of 70–170 ms after stimulus onset. This observation reveals an LSF-preferred decoding behavior across the IT population (recall, R1 = 0.47 ± 0.04, R2 = 0.36 ± 0.03, R3 = 0.30 ± 0.03, R4 = 0.32 ± 0.04, R5 = 0.30 ± 0.03, and R1 &gt; R5, p-value &lt;0.001).</p></sec><sec id="s2-2"><title>Temporal dynamics of SF representation</title><p>The sample neuron and recall values in <xref ref-type="fig" rid="fig1">Figure 1</xref> indicate an LSF-preferred neuron response. To explore this behavior over time, we analyzed the temporal dynamics of SF representation. <xref ref-type="fig" rid="fig2">Figure 2a</xref> illustrates the onset of SF recalls, revealing a coarse-to-fine trend where R1 is decoded faster than R5 (onset times in milliseconds after stimulus onset, R1 = 84.5 ± 3.02, R2 = 86.0 ± 4.4, R3 = 88.9 ± 4.9, R4 = 86.5 ± 4.1, R5 = 97.15 ± 4.9, R1 &lt; R5, p-value &lt;0.001). <xref ref-type="fig" rid="fig2">Figure 2b</xref> illustrates the time course of the average preferred SF across the neurons. To calculate the preferred SF for each neuron, we multiplied the firing rate by the SF range and normalized the values (see Materials and methods). <xref ref-type="fig" rid="fig2">Figure 2b</xref> demonstrates that following the early phase of the response (70–170 ms), the average preferred SF shifts toward HSF, reaching its peak at 215 ms after stimulus onset (preferred SF, 0.54 ± 0.15). Furthermore, a second peak emerges at 320 ms after stimulus onset (preferred SF, 0.22 ± 0.16), indicating a shift in the average preferred SF in the IT cortex toward higher frequencies. To analyze this shift, we divided the time course into two intervals of 70–170 ms, where the response peak of the neurons happens, and 170–270 ms, where the first peak of SF preference occurs. We calculated the percentage of the neurons that significantly responded to a specific SF range higher than others (one-way ANOVA with a significance level of 0.05, see Materials and methods) for the two time intervals. <xref ref-type="fig" rid="fig2">Figure 2c, d</xref> shows the percentage of the neurons in each SF range for the two time steps. In the early phase of the response (T1, 70–170 ms), the highest percentage of the neurons belongs to R1, 40.19%, and a decreasing trend is observed as we move toward higher frequencies (R1 = 40.19%, R2 = 19.60%, R3 = 13.72%, R4 = 10.78%, R5 = 15.68%). Moving to T2, the percentage of neurons responding to R1 higher than the others remains stable, dropping to 38.46%. The number of neurons in R2 also drops to under 5% from 19.60% observed in T1. On the other hand, the percentage of the neurons in R5 reaches 46.66% in T2 compared to 15.68% in T1 (higher than R1 in T1). This observation indicates that the increase in preferred SF is due to a substantial increase in the selective neurons to HSF, while the response of the neurons to R1 is roughly unchanged. To further understand the population response to various SF ranges, the average response across neurons for R1–R5 is depicted in <xref ref-type="fig" rid="fig2">Figure 2c, d</xref> (bottom panels). In the first interval, T1, an average LSF-preferred tuning is observed where the average neuron response decreases as the SF increases (normalized firing rate for R1 = 1.09 ± 0.01, R2 = 1.05 ± 0.01, R3 = 1.03 ± 0.01, R4 = 1.03 ± 0.02, R5 = 1.00 ± 0.01, Bonferroni corrected p-value for R2 &lt; R5, 0.006). Considering the strength of responses to scrambled stimuli, the average firing rate in response to scrambled stimuli is 26.3 Hz, which is significantly higher than the response observed between −50 and 50 ms, where it is 23.4 Hz (p-value = 3 × 10<sup>−5</sup>). In comparison, the mean response to intact face stimuli is 30.5 Hz, while non-face stimuli elicit an average response of 28.8 Hz. The distribution of neuron responses for scrambled, face, and non-face in T1 is illustrated in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. During the second time interval, excluding R1, the decreasing pattern transformed to an increasing one, with the response to R5 surpassing that of R1 (normalized firing rate for R1 = 0.80 ± 0.02, R2 = 0.73 ± 0.02, R3 = 0.76 ± 0.02, R4 = 0.81 ± 0.02, R5 = 0.84 ± 0.01, Bonferroni corrected p-value for R2 &lt; R4, 0.022, R2 &lt; R5, 0.0003, and R3 &lt; R5, 0.03). Moreover, the average firing rates of scrambled, face, and non-face stimuli are 19.5, 19.4, and 22.4 Hz, respectively. The distribution of neuron responses is illustrated in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. These observations illustrate an LSF-preferred tuning in the early phase of the response, shifting toward HSF-preferred tuning in the late response phase.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The temporal dynamics of spatial frequency (SF) representation.</title><p>(<bold>a</bold>) <italic>Course-to-fine nature of SF coding</italic>. The onset time of the recall of each SF range in scrambled stimuli is illustrated, with error bars indicating the STD. The results suggest that the onset time of decoding increases as SF increases. (<bold>b</bold>) <italic>SF preference shifts toward higher frequencies over time</italic>. The time course of the average preferred SF (see Materials and methods) across neurons is illustrated. The average preferred SF of inferior temporal (IT) neurons moves toward higher frequencies from 170 ms after stimulus onset, reaching its highest value at 220 ms. A second peak emerges at 320 ms following the stimulus onset. The SF preference shows a monotonic increase followed by a decrease in time. The shadow shows SEM. (<bold>c, d</bold>) <italic>Shift in neural response toward high SF (HSF)</italic>. The average response of all neurons within the two time intervals (T1 and T2 in panel b) is shown, with error bars indicating the SEM. (<bold>c</bold>) In T1, from 70 to 170 ms after stimulus onset, a decreasing response of the neurons is observed as the SF content shifts toward higher frequencies. The relative percentage of neurons showing stronger responses to SF ranges (R1–R5) in T1 is depicted in the inner top panel. R1 is the most responsive SF for roughly 40% of the neurons. (<bold>d</bold>) In the following interval (T2, 170–270 ms), an increasing tuning is observed from R2 to R5, where R5 elicits the highest firing rates. Furthermore, in T2, there is a roughly threefold increase in the percentage of neurons exhibiting stronger responses to R5 compared to T1, indicating a shift in the neurons’ responses toward HSF (top panel).</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source data for temporal dynamics of SF coding, including onset of recall, preferred SF, and neuronal responses across time intervals.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-93589-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-fig2-v1.tif"/></fig></sec><sec id="s2-3"><title>SF profile predicts category coding</title><p>Our findings indicate explicit SF coding in the IT cortex. Given the co-existence of SF and category coding in this region, we examine the relationship between SF and category codings. As depicted in <xref ref-type="fig" rid="fig2">Figure 2</xref>, while the average preferred SF across the neurons shifts to HSF, the most responsive SF range varies across individual neurons. To investigate the relation between SF representation and category coding, we identified an SF profile by fitting a quadratic curve to the neuron responses across SF ranges (R1–R5, phase-scrambled stimuli only). Then, according to the fitted curve, an SF profile is determined for each neuron (see Materials and methods). Five distinct profiles were identified based on the tuning curves (<xref ref-type="fig" rid="fig3">Figure 3a</xref>): (1) flat, where the neuron has no preferred SF (not included in the results), (2) LSF preferred (LP), where the neuron response decreases as SF increases, (3) HSF preferred (HP), where neuron response increases as the SF shifts toward higher SFs, (4) U-shaped where the neuron response to middle SF is lower than that of HSF or LSF, and (5) inverse U-shaped (IU), where the neuron response to middle SF is higher than that of LSF and HSF. The U-shaped and HSF-preferred profiles represent the largest and smallest populations, respectively. To check the robustness of the profiles, considering the trial-to-trial variability, the strength of SF selectivity in each profile is provided in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>, by forming the profile of each neuron based on half of the trials and then plotting the average SF responses with the other half. Following profile identification, the object coding capability of each profile population is assessed. Here, instead of LDA, we employ the separability index (SI) introduced by <xref ref-type="bibr" rid="bib17">Dehaqani et al., 2016</xref>, because of the LDA limitation in fully capturing the information differences between groups as it categorizes samples as correctly classified or misclassified.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Spatial frequency (SF) profile predicts category coding.</title><p>(<bold>a, b</bold>) <italic>SF profile predicts category selectivity</italic>. (<bold>a</bold>) The responses of each neuron were standardized by subtracting the mean and dividing by the standard deviation of the baseline time. Neurons were then categorized into four groups based on the fitting of a quadratic function to their responses (see Materials and methods). Each panel presents the average neuron responses within each category for SF ranges R1–R5, with error bars indicating the SEM of the response values. The percentage of the neurons in each category is displayed at the top of each panel. The ‘flat’ category, where the response to no SF was higher than others, was excluded from this analysis. (<bold>b</bold>) Separability index (SI) of face/non-face vs. scrambled stimuli is illustrated (see Materials and methods). The error bar shows STD. The SI value and SF profile are determined within the time window of 70–170 ms after stimulus onset. The high SF (HSF)-preferred population exhibited significantly higher face SI compared to the other groups. The low SF (LSF)-preferred population displayed a significant difference in face and non-face SI. On the other hand, the IU profile indicates a significantly higher SI value for the non-face compared to the face. The U-shaped profile did not show any significant differences between the face and the non-face. These results suggest that the neuron's response to various SF bands can predict its decoding capability. (<bold>c, d</bold>) <italic>The relation between SF and category coding in sub-populations</italic>. Initially, the linear discriminant analysis (LDA) method was employed to calculate the individual neuron's performance in the single-level category and SF coding. Next, a sorting procedure based on SF (panel c) and category (panel d) coding performances was conducted to create sub-populations of neurons exhibiting similar capabilities (see Materials and methods). The scatter plot of the category and SF coding accuracy of these sub-populations demonstrated a notably high degree of positive correlation between SF and category accuracies in the inferior temporal (IT) cortex.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Source data for neuronal responses, SF profiles, and category selectivity, including separability indices and decoding accuracy for sub-populations of inferior temporal neurons.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-93589-fig3-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-fig3-v1.tif"/></fig><p>To examine the face and non-face information separately, SI is calculated for face vs. scrambled and non-face vs. scrambled. <xref ref-type="fig" rid="fig3">Figure 3a</xref> displays the identified profiles, and <xref ref-type="fig" rid="fig3">Figure 3b</xref> indicates the average SI value during 70–170 ms after the stimulus onset. The HP profile shows significantly higher face information compared to other profiles (face SI for LP = 0.58 ± 0.03, HP = 0.89 ± 0.05, U = 0.07 ± 0.01, IU = 0.07 ± 0.01, HP &gt; LP, U, IU with p-value &lt;0.001) and than non-face information in all other profiles (non-face SI for LP = 0.04 ± 0.01, HP = 0.02 ± 0.01, U = 0.19 ± 0.03, IU = 0.08 ± 0.02, and face SI in HP is greater than non-face SI in all profiles with p-value &lt;0.001). This observation underscores the importance of middle and higher frequencies for face representation. The LSF-preferred profile also exhibits significantly higher face SI than non-face objects (p-value &lt;0.001). On the other hand, in the IU profile, non-face information surpasses face SI (p-value &lt;0.001), indicating the importance of middle frequency for the non-face objects. Finally, in the U profile, there is no significant difference between the face and non-face objects (face vs. non-face p-value = 0.36).</p><p>To assess whether the SF profiles distinguish category selectivity or merely evaluate the neuron’s responsiveness, we quantified the number of face/non-face-selective neurons in the 70- to 170-ms time window. Our analysis shows a total of 43 face-selective neurons and 36 non-face-selective neurons (FDR-corrected p-value &lt;0.05). The results indicate a higher proportion of face-selective neurons in LP and HP, while a greater number of non-face-selective neurons is observed in the IU category (number of face/non-face-selective neurons: LP = 13/3, HP = 6/2, IU = 3/9). The U category exhibits a roughly equal distribution of face and non-face-selective neurons (U = 14/13). This finding reinforces the connection between category selectivity and the identified profiles. We then analyzed the average neuron response to faces and non-faces within each profile. The difference between the firing rates for faces and non-faces in none of the profiles is significant (face/non-face average firing rate (Hz): LP = 36.72/28.77, HP = 28.55/25.52, IU = 21.55/27.25, U = 38.48/36.28, rank-sum with significance level of 0.05). Although the observed differences are not statistically significant, they provide support for the association between profiles and categories rather than mere responsiveness.</p><p>Next, to examine the relation between the SF (category) coding capacity of the single neurons and the category (SF) coding capability of the population level, we calculated the correlation between coding performance at the population level and the coding performance of single neurons within that population (<xref ref-type="fig" rid="fig3">Figure 3a</xref> and <xref ref-type="fig" rid="fig3">Figure 3b</xref>). In other words, we investigated the relation between single and population levels of coding capabilities between SF and category. The SF (or category) coding performance of a sub-population of 20 neurons that have roughly the same single-level coding capability of the category (or SF) is examined. Neurons were sorted based on their SF or category performances, resulting in two separate groups of ranks—one for SF and another for category. Subsequently, we selected sub-populations of neurons with similar ranks according to SF or category (see Materials and methods). Each sub-population comprises 20 neurons with approximately similar SF (or category) performance levels. Then, the SF and category decoding accuracy is calculated for each sub-population. The scatterplot of individual vs. sub-population accuracy demonstrated a significant positive correlation between the sub-population performance and the accuracy of individual neurons within those populations. Specifically, the correlation value for SF- and category-sorted groups is 0.66 (p-value = 10<sup>−31</sup>) and 0.39 (p-value = 10<sup>−10</sup>), respectively. This observation illustrates that SF coding capacity at single-level representations significantly predicts category coding capacity at the population level.</p></sec><sec id="s2-4"><title>Uncorrelated mechanisms for SF and category coding</title><p>As both SF and category coding exist in the IT cortex at both the single-neuron and population levels, we investigated their underlying coding mechanisms (for single and population levels separately). <xref ref-type="fig" rid="fig4">Figure 4a</xref> displays the scatter plot of SF and category coding capacity for individual neurons. The correlation between SF and category accuracy across individual neurons shows no significant relationship (correlation: 0.024 and p-value: 0.53), suggesting two uncorrelated mechanisms for SF and category coding. To explore the population-level coding, we considered neuron weights in the LDA classifier as indicators of each neuron’s contribution to population coding. <xref ref-type="fig" rid="fig4">Figure 4b</xref> indicates the scatter plot of the neuron’s weights in SF and category decoding. The LDA weights reveal no correlation between the patterns of neuron contribution in population decoding of SF and category (correlation = 0.002 and p-value = 0.39). These observations indicate uncorrelated coding mechanisms for SF and category in both single and population-level representations in the IT cortex. <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref> examine different aspects of the relationship between SF and category coding. <xref ref-type="fig" rid="fig3">Figure 3</xref> highlights a relationship between SF coding at the single-neuron level and category coding at the population level. Conversely, <xref ref-type="fig" rid="fig4">Figure 4</xref> demonstrates the uncorrelated mechanisms underlying SF and category coding, showing that a neuron’s ability to decode SF is not predictive of its ability to decode object categories. This distinction underscores that while SF and category coding are related at broader levels, their underlying mechanisms are independent, emphasizing the distinct processes driving each form of coding.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Uncorrelated mechanisms for spatial frequency (SF) and category coding.</title><p>(<bold>a</bold>) <italic>Uncorrelated SF and category coding in the single level</italic>. The scatter plot indicates the category–SF accuracies and does not reveal a significant correlation between SF and category coding capabilities within the inferior temporal (IT) cortex at the single-neuron level. The error bars show the STD for SF and category decoding accuracies. (<bold>b</bold>) <italic>Uncorrelated neuron contribution in SF and category coding in population</italic>. The linear discriminant analysis (LDA) weight of each neuron is considered as the neuron contribution in the population coding of SF or category (see Materials and methods). The scatter plot of the neuron weights in SF shows a near-zero correlation with the neuron weights in category coding.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Source data for single-neuron and population-level contributions to SF and category coding.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-93589-fig4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-fig4-v1.tif"/></fig><p>Next, to investigate SF and category coding characteristics, we systematically removed individual neurons from the population and measured the resulting drop in LDA classifier accuracy as a metric for the neuron’s impact, termed single-neuron contribution (SNC). <xref ref-type="fig" rid="fig5">Figure 5a</xref> illustrates the SNC score for SF (two labels, LSF (R1 and R2) vs. HSF (R4 and R5)) and category (face vs. non-face) decoding within 70–170 ms after the stimulus onset. The SNC in SF is significantly higher than for category (average SNC for SF = 0.51% ± 0.02 and category = 0.1% ± 0.04, SF &gt; category with p-value = 1.6 × 1.6<sup>−13</sup>). Therefore, SF representation relies more on individual neuron representations, suggesting a sparse mechanism of SF coding where single-level neuron information is less redundant. In contrast, single-level representations of category appear to be more redundant and robust against information loss or noise at the level of individual neurons. We utilized conditional mutual information (CMI) between pairs of neurons conditioned on the label, SF (LSF (R1 and R2) vs. HSF (R4 and R5)) or category, to assess the information redundancy across the neurons. CMI quantifies the shared information between the population of two neurons regarding SF or category coding. <xref ref-type="fig" rid="fig5">Figure 5b</xref> indicates a significantly lower CMI for SF (average CMI for SF = 0.66 ± 0.0009 and category = 0.69 ± 0.0007, SF &lt; category with p-value ≈ 0), indicating that neurons carry more independent SF-related information than category-related information.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Sparse spatial frequency (SF) coding compared to category coding.</title><p>(<bold>a, b</bold>) <italic>Sparse mechanism for SF coding</italic>. (<bold>a</bold>) The contribution of each neuron in SF and category (face vs. non-face) decoding is evaluated by removing it from the feature set fed to the linear discriminant analysis (LDA) within the time window of 70–170 ms after stimulus onset. The histogram of the single-neuron contribution (SNC) value (see Materials and methods) is presented, indicating the amount of accuracy loss when a neuron is removed. The bar plot displays the average SNC values for SF and category, with error bars representing the SEM. The SNC value for SF is significantly higher than for the category. (<bold>b</bold>) Furthermore, the conditional mutual information (CMI) of each neuron pair, conditioned to the label (category or SF), is illustrated. CMI reflects the information redundancy between neuron pairs during SF or category decoding. A lower CMI value for SF indicates that individual neurons carry more independent SF-related information compared to category information. (<bold>c</bold>) <italic>Sparse neuron contribution in SF coding at the early phase of the response</italic>. To investigate the contribution of the neurons in population decoding, the sparseness of the LDA weights assigned to each neuron is calculated. Higher sparseness indicates a greater contribution of a smaller group of neurons to the decoding process. The time course of weight sparseness is depicted for SF and category (face vs. non-face) decoding, with shadows representing the STD. During the early phase of the response, the sparseness of SF-related weights is higher than that of the category, while this relationship is reversed during the late phase of the response.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Source data for single-neuron and population contributions to SF and category decoding, including single-neuron contribution values, conditional mutual information, and temporal sparseness of LDA weights.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-93589-fig5-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-fig5-v1.tif"/></fig><p>To investigate each neuron’s contribution to the decoding procedure (LDA decision), we computed the sparseness of the LDA weights corresponding to each neuron (see Materials and methods). For SF, we trained the LDA on R1, R2, R4, and R5 with two labels (one for R1 and R2 and the alternative for R4 and R5). A second LDA was trained to discriminate between faces and non-faces. Subsequently, we calculated the sparseness of the weights associated with each neuron in SF and category decoding. <xref ref-type="fig" rid="fig5">Figure 5c</xref> illustrates the time course of the weight sparseness for SF and category. The category reflects a bimodal curve with the first peak at 110 ms and the second at 210 ms after stimulus onset. The second peak is significantly larger than the first one (category first peak, 0.016 ± 0.007, second peak, 0.051 ± 0.013, and p-value &lt;0.001). In SF decoding, neurons’ weights exhibit a trimodal curve with peaks at 100, 215, and 320 ms after the stimulus onset. The first peak is significantly higher than the other two (SF first peak, 0.038 ± 0.005, second peak, 0.018 ± 0.003, third peak, 0.028 ± 0.003, first peak &gt; second peak with p-value &lt;0.001, and first peak &gt; third peak with p-value = 0.014). Comparing SF and category, during the early phase of the response (70–170 ms), SF sparseness is higher, while in 170–270 ms, the sparseness value of the category is higher (p-value &lt;0.001 for both time intervals). This suggests that, initially, most neurons contribute to category representation, but later, the majority of neurons are involved in SF coding. These findings support distinct mechanisms governing SF and category coding in the IT cortex.</p></sec><sec id="s2-5"><title>SF representation in the artificial neural networks</title><p>We conducted a thorough analysis to compare our findings with CNNs. To assess the SF coding capabilities and trends of CNNs, we utilized popular architectures, including ResNet18, ResNet34, VGG11, VGG16, InceptionV3, EfficientNetb0, CORNet-S, CORTNet-RT, and CORNet-z, with both pre-trained on ImageNet and randomly initialized weights (see Materials and methods). Employing feature maps from the four last layers of each CNN, we trained an LDA model to classify the SF content of input images. <xref ref-type="fig" rid="fig6">Figure 6a</xref> shows the SF decoding accuracy of the CNNs on our dataset (SF decoding accuracy with random (R) and pre-trained (P) weights, ResNet18: P = 0.96 ± 0.01/R = 0.94 ± 0.01, ResNet34: P = 0.95 ± 0.01/R = 0.86 ± 0.01, VGG11: P = 0.94 ± 0.01/R = 0.93 ± 0.01, VGG16: P = 0.92 ± 0.02/R = 0.90 ± 0.02, InceptionV3: P = 0.89 ± 0.01/R = 0.67 ± 0.03, EfficientNetb0: P = 0.94 ± 0.01/R = 0.30 ± 0.01, CORNet-S: P = 0.77 ± 0.02/R = 0.36 ± 0.02, CORTNet-RT: P = 0.31 ± 0.02/R = 0.33 ± 0.02, and CORNet-z: P = 0.94±0.01/R = 0.97 ± 0.01). Except for CORNet-z, object recognition training increases the network’s capacity for SF coding, with an improvement as significant as 64% in EfficientNetb0. Furthermore, except for the CORNet family, LSF content exhibits higher recall values than HSF content, as observed in the IT cortex (p-value with random (R) and pre-trained (P) weights, ResNet18: P = 0.39/R = 0.06, ResNet34: P = 0.01/R = 0.01, VGG11: P = 0.13/R = 0.07, VGG16: P = 0.03/R = 0.05, InceptionV3: P = &lt;0.001/R = 0.05, EfficientNetb0: P = 0.07/R = 0.01). The recall values of CORNet-Z and ResNet18 are illustrated in <xref ref-type="fig" rid="fig6">Figure 6b</xref>. However, while the CNNs exhibited some similarities in SF representation with the IT cortex, they did not replicate the SF-based profiles that predict neuron category selectivity. As depicted in <xref ref-type="fig" rid="fig6">Figure 6c</xref>, although neurons formed similar profiles, these profiles were not associated with the category decoding performances of the neurons sharing the same profile.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Spatial frequency (SF) representation in convolutional neural networks (CNNs).</title><p>(<bold>a</bold>) <italic>SF coding capabilities</italic>. We assessed the SF coding capabilities of popular CNN architectures (ResNet18, ResNet34, VGG11, VGG16, InceptionV3, EfficientNetb0, CORNet-S, CORTNet-RT, and CORNet-z) using both randomly initialized (R) and pre-trained (P) weights on ImageNet. A linear discriminant analysis (LDA) model was trained using feature maps from the four last layers of each CNN to classify the SF content of input images. The SF decoding accuracy for each CNN on our dataset is presented with error bars indicating the STD. (<bold>b</bold>) <italic>Low SF (LSF)-preferred recall performance</italic>. The recall performance of two sample networks (CORNET-z and ResNet18) is presented. STD values are illustrated with error bars. The recall values for LSF content were higher than high SF (HSF) content in most CNNs, resembling the trends observed in the inferior temporal (IT) cortex. (<bold>c</bold>) The profiles (left) and face/non-face separability index (SI) value (right) of a sample network (ResNet18). Error bars show STD. Profiles are calculated similarly to the IT cortex. CNNs did not replicate the SF-based profiles observed in the IT cortex.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Source data for SF decoding and recall performance in multiple CNN architectures, SF preference profiles and face/non-face separability indices.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-93589-fig6-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-fig6-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Utilizing neural responses from the IT cortex of passive-viewing monkeys, we conducted a study on SF representation within this pure visual high-level area. Numerous psychophysical studies have underscored the significant impact of SF on object recognition, highlighting the importance of its representation. To the best of our knowledge, this study presents the first attempt to systematically examine the SF representation in a high-level area, that is, the IT cortex, using extracellular recording. Understanding SF representation is crucial, as it can elucidate the object recognition procedure in the IT cortex.</p><p>Our findings demonstrate explicit SF coding at both the single-neuron and population levels, with LSF being decoded faster and more accurately than HSF. During the early phase of the response, we observe a preference for LSF, which shifts toward a preference for HSF during the late phase. Next, we made profiles based on SF-only (phase-scrambled stimuli) responses for each neuron to predict its category selectivity. Our results show a direct relationship between the population’s category coding capability and the SF coding capability of individual neurons. While we observed a relation between SF and category coding, we have found uncorrelated representations. Unlike category coding, SF relies more on sparse, individual neuron representations. Finally, when comparing the responses of IT with those of CNNs, it is evident that while SF coding exists in CNNs, the SF profile observed in the IT cortex is notably absent. Our results are based on grouping the neurons of the two monkeys; however, the results remain consistent when looking at the data from individual monkeys as illustrated in Appendix 2. However, for neurons preferring LSF, we observed some inconsistencies across monkeys, which may reflect individual differences or sampling variability. These findings highlight the complexity of SF processing in the IT cortex and suggest the need for further research to explore these variations.</p><p>The influence of SF on object recognition has been extensively investigated through psychophysical studies (<xref ref-type="bibr" rid="bib3">Ashtiani et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Caplette et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Cheung and Bar, 2014</xref>; <xref ref-type="bibr" rid="bib15">Craddock et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Joubert et al., 2007</xref>; <xref ref-type="bibr" rid="bib39">Schyns and Oliva, 1994</xref>). One frequently explored theory is the coarse-to-fine nature of SF processing in object recognition (<xref ref-type="bibr" rid="bib22">Gao and Bentin, 2011</xref>; <xref ref-type="bibr" rid="bib30">Kauffmann et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Rokszin et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Rotshtein et al., 2010</xref>; <xref ref-type="bibr" rid="bib39">Schyns and Oliva, 1994</xref>; <xref ref-type="bibr" rid="bib43">Yardley et al., 2012</xref>). This aligns with our observation that the onset of LSF is significantly lower than HSF. Different SF bands carry distinct information, progressively conveying coarse-to-fine shape details as we transition from LSF to HSF. Psychophysical studies have indicated the utilization of various SF bands for distinct categorization tasks (<xref ref-type="bibr" rid="bib37">Rotshtein et al., 2010</xref>). Considering the face as a behaviorally demanded object, psychophysical studies have observed the influence of various SF bands on face recognition. These studies consistently show that enhanced face recognition performance is achieved in the middle and higher SF bands compared to LSF (<xref ref-type="bibr" rid="bib5">Awasthi et al., 2012</xref>; <xref ref-type="bibr" rid="bib12">Cheung et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Costen et al., 1996</xref>; <xref ref-type="bibr" rid="bib21">Fiorentini et al., 1983</xref>; <xref ref-type="bibr" rid="bib24">Hayes et al., 1986</xref>; <xref ref-type="bibr" rid="bib28">Jeantet et al., 2019</xref>). These observations resonate with the identified SF profiles in our study. Neurons that exhibit heightened responses as SF shifts toward HSF demonstrate superior coding of faces compared to other neuronal groups.</p><p>Unlike psychophysical studies, imaging studies in this area have been relatively limited. <xref ref-type="bibr" rid="bib23">Gaska et al., 1988</xref> observed low-pass tuning curves in the V3A area, and <xref ref-type="bibr" rid="bib11">Chen et al., 2018</xref> reported an average low-pass tuning curve in the superior colliculus (SC). <xref ref-type="bibr" rid="bib35">Purushothaman et al., 2014</xref> identified two distinct types of neurons in V1 based on their response to SF. The majority of neurons in the first group exhibited a monotonically shifting preference toward HSF over time. In contrast, the second group showed an initial increase in preferred SF followed by a decrease. Our findings align with these observations, showing a rise in preferred SF starting at 170 ms after stimulus onset, followed by a decline at 220 ms after stimulus onset. Additionally, <xref ref-type="bibr" rid="bib44">Zhang et al., 2023</xref> found that LSF is the preferred band for over 40% of V4 neurons. This finding is also consistent with our observations, where approximately 40% of neurons consistently exhibited the highest firing rates in response to LSF throughout all response phases. Collectively, these results suggest that the average LP tuning curve observed in the IT cortex could be a characteristic inherited from the lower areas in the visual hierarchy. Moreover, examining the course-to-fine theory of SF processing, <xref ref-type="bibr" rid="bib11">Chen et al., 2018</xref> and <xref ref-type="bibr" rid="bib35">Purushothaman et al., 2014</xref> observed a faster response to LSF compared to HSF in SC and V1, which resonates with our course-to-fine observation in SF decoding. When analyzing the relationship between the SF content of complex stimuli and IT responses, <xref ref-type="bibr" rid="bib8">Bermudez et al., 2009</xref> observed a correlation between neural responses in the IT cortex and the SF content of the stimuli. This finding is in line with our observations, as decoding results directly from the distinct patterns exhibited by various SF bands in neural responses.</p><p>To rule out the degraded contrast sensitivity of the visual system to medium and high SF information because of the brief exposure time, we repeated the analysis with 200 ms exposure time as illustrated in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> which indicates the same LSF-preferred results. Furthermore, according to <xref ref-type="fig" rid="fig2">Figure 2</xref>, the average firing rate of IT neurons for HSF could be higher than LSF in the late response phase. It indicates that the amount of HSF input received by the IT neurons in the later phase is as much as LSF; however, its impact on the IT response is observable in the later phase of the response. Thus, the LSF preference is because of the temporal advantage of the LSF processing rather than contrast sensitivity. Next, according to <xref ref-type="fig" rid="fig3">Figure 3a</xref>, 6% of the neurons are HSF-preferred and their firing rate in HSF is comparable to the LSF firing rate in the LSF-preferred group. This analysis is carried out in the early phase of the response (70–170 ms). While most of the neurons prefer LSF, this observation shows that there is an HSF input that excites a small group of neurons. Importantly, findings involving small neuronal populations can still be meaningful, as studies like <xref ref-type="bibr" rid="bib16">Dalgleish et al., 2020</xref> have demonstrated that perception can arise from the activity of as few as 14 neurons in the mouse cortex, emphasizing the robustness of sparse coding. Additionally, the highest SI belongs to the HSF-preferred profile in the early phase of the response, which supports the impact of the HSF part of the input. Similar LSF-preferred responses are also reported by <xref ref-type="bibr" rid="bib11">Chen et al., 2018</xref> (50 ms for SC) and (<xref ref-type="bibr" rid="bib44">Zhang et al., 2023</xref>) (3.5–4 s for V2 and V4). Therefore, our results show that the LSF-preferred nature of the IT responses in terms of firing rate and recall is not due to the weakness or lack of input source (or information) for HSF but rather to the processing nature of the SF in the IT cortex.</p><p><xref ref-type="bibr" rid="bib25">Hong et al., 2016</xref> suggested that the neural mechanisms responsible for developing tolerance to identity-preserving transform also contribute to explicitly representing these category–orthogonal transforms, such as rotation. Extending this perspective to SF, our results similarly suggest an explicit representation of SF within the IT population. However, unlike transforms such as rotation, the neural mechanisms in IT leverage various SF bands for various categorization tasks. Furthermore, our analysis introduced a novel SF-only profile for the first time predicting category selectivity.</p><p>These findings prompt the question of why the IT cortex explicitly represents and codes the SF content of the input stimuli. In our perspective, the explicit representation and coding of SF contents in the IT cortex facilitates object recognition. The population of the neurons in the IT cortex becomes selective for complex object features, combining SFs to transform simple visual features into more complex object representations. However, the specific mechanism underlying this combination is yet to be known. The diverse SF contents present in each image carry valuable information that may contribute to generating expectations in predictive coding during the early phase, thereby facilitating information processing in subsequent phases. This top–down mechanism is suggested by the works of <xref ref-type="bibr" rid="bib6">Bar, 2003</xref> and <xref ref-type="bibr" rid="bib19">Fenske et al., 2006</xref>.</p><p>Moreover, each object has a unique ‘characteristic SF signature’, representing its specific arrangement of SFs. ‘Characteristic SF signatures’ refer to the unique patterns or profiles of SFs associated with different objects or categories of objects. When we look at visual stimuli, such as objects or scenes, they contain specific arrangements of different SFs. Imagine a scenario where we have two objects, such as a cat and a car. These objects will have different textures and shapes, which correspond to different distributions of SFs. The cat, for instance, might have a higher concentration of mid-range SFs related to its fur texture, while the car might have more pronounced LSFs that represent its overall shape and structure. The IT cortex encodes these signatures, facilitating robust discrimination and recognition of objects based on their distinctive SF patterns.</p><p>The concept of ‘characteristic SF signatures’ is also related to the ‘SF tuning’ observed in our results. Neurons in the visual cortex, including the IT cortex, have specific tuning preferences for different SFs. Some neurons are more sensitive to HSF, while others respond better to LSF. This distribution of sensitivity allows the visual system to analyze and interpret different information related to different SF components of visual stimuli concurrently. Moreover, the IT cortex’s coding of SF can contribute to object invariance and generalization. By representing objects in terms of their SF content, the IT cortex becomes less sensitive to variations in size, position, or orientation, ensuring consistent recognition across different conditions. SF information also aids the IT cortex in categorizing objects into meaningful groups at various levels of abstraction. Neurons can selectively respond to shared SF characteristics among different object categories (assuming that objects in the same category share a level of SF characteristics), facilitating decision-making about visual stimuli. Overall, we posit that SF’s explicit representation and coding in the IT cortex enhance its proficiency in object recognition. By capturing essential details and characteristics of objects, the IT cortex creates a rich representation of the visual world, enabling us to perceive, recognize, and interact with objects in our environment.</p><p>Finally, we compared SF’s representation trends and findings within the IT cortex and the current state of the art networks in deep neural networks. CNNs stand as one of the most promising models for comprehending visual processing within the primate ventral visual processing stream (<xref ref-type="bibr" rid="bib32">Kubilius, 2019</xref>; <xref ref-type="bibr" rid="bib31">Kubilius et al., 2018</xref>). Examining the higher layers of CNN models (most similar to IT), we found that randomly initialized and pre-trained CNNs can code for SF. This is consistent with our previous work on the CIFAR dataset (<xref ref-type="bibr" rid="bib41">Toosi et al., 2022</xref>). Nevertheless, they do not exhibit the SF profile we observed in the IT cortex. This emphasizes the uniqueness of SF coding in the IT cortex and suggests that artificial neural networks might not fully capture the complete complexity of biological visual processing mechanisms, even when they encompass certain aspects of SF representation. Our results intimate that the IT cortex uses a different mechanism for SF coding compared to contemporary deep neural networks, highlighting the potential for innovating new approaches to consider the role of SF in the ventral stream models.</p><p>Our results are not affected by several potential confounding factors. First, each stimulus in the set also has a corresponding phase-scrambled variant. These phase-scrambled stimuli maintain the same SF characteristics as their respective face or non-face counterparts but lack shape information. This approach allows us to investigate SF representation in the IT cortex without the confounding influence of shape information. Second, our results, obtained through a passive-viewing task, remain unaffected by attention mechanisms. Third, all stimuli (intact, SF filtered, and phase scrambled) are corrected for illumination and contrast to remove the attribution of the category–orthogonal basic characteristics of stimuli into the results (see Materials and methods). Fourth, while our dataset does not exhibit a balance in samples per category, it is imperative to acknowledge that this imbalance does not exert an impact on our observed outcomes. We have equalized the number of samples per category when training our classification models by random sampling from the stimulus set (see Materials and methods). One limitation of our study is the relatively low number of objects in the stimulus set. However, the decoding performance of category classification (face vs. non-face) in intact stimuli is 94.2%. The recall value for objects vs. scrambled is 90.45%, and for faces vs. scrambled is 92.45 (p-value = 0.44), which indicates the high level of generalizability and validity characterizing our results. Finally, since our experiment maintains a fixed SF content in terms of both cycles per degree and cycles per image, further experiments are needed to discern whether our observations reflect sensitivity to cycles per degree or cycles per image.</p><p>In summary, we studied the SF representation within the IT cortex. Our findings reveal the existence of a sparse mechanism responsible for encoding SF in the IT cortex. Moreover, we studied the relationship between SF representation and object recognition by identifying an SF profile that predicts object recognition performance. These findings establish neural correlates of the psychophysical studies on the role of SF in object recognition and shed light on how IT represents and utilizes SF for the purpose of object recognition.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Biological sample (<italic>Macaca mulatta</italic>, male)</td><td align="left" valign="bottom">IT cortex neurons; Neurons; Monkey</td><td align="left" valign="bottom">This paper</td><td align="left" valign="bottom"/><td align="left" valign="bottom">From two adult male macaques (10 and 11 kg); see Materials and methods</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">MATLAB</td><td align="left" valign="bottom">MathWorks</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Used for stimulus presentation, control, and analysis</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">MonkeyLogic toolbox</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://monkeylogic.nimh.nih.gov/">MonkeyLogic website</ext-link></td><td align="left" valign="bottom"/><td align="left" valign="bottom">For experimental control in MATLAB</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Python</td><td align="left" valign="bottom">Python Software Foundation</td><td align="left" valign="bottom">3.10</td><td align="left" valign="bottom">Used for data analysis and machine learning workflows</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">PyTorch</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">pytorch.org</ext-link></td><td align="left" valign="bottom">2.0</td><td align="left" valign="bottom">Deep learning framework used for neural modeling</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Animals and recording</title><p>The activity of neurons in the IT cortex of two male macaque monkeys weighing 10 and 11 kg, respectively, was analyzed following the National Institutes of Health Guide for the Care and Use of Laboratory Animals and the Society for Neuroscience Guidelines and Policies. The experimental procedures were approved by the Institute of Fundamental Science committee (Approval Code: 99/60/1/160/1). Before implanting a recording chamber in a subsequent surgery, magnetic resonance imaging and computed tomography scans were performed to locate the prelunate gyrus and arcuate sulcus. The surgical procedures were carried out under sterile conditions and isoflurane anesthesia. Each monkey was fitted with a custom-made stainless-steel chamber, secured to the skull using titanium screws and dental acrylics. A craniotomy was performed within the 30 × 70 mm chamber for both monkeys, with dimensions ranging from 5 to 30 mm A/P and 0 to 23 mm M/L.</p><p>During the experiment, the monkeys were seated in custom-made primate chairs, and their heads were restrained while a tube delivered juice rewards to their mouths. The system was mounted in front of the monkey, and eye movements were captured at 2 kHz using the EyeLink PM-910 Illuminator Module and EyeLink 1000 Plus Camera (SR Research Ltd, Ottawa, CA). Stimulus presentation and juice delivery were controlled using custom software written in MATLAB with the MonkeyLogic toolbox (see Supplementary File – <italic>PreTestCode.m</italic>). Visual stimuli were presented on a 24-inch LED-lit monitor (AsusVG248QE, 1920 × 1080, 144 Hz) positioned 65.5 cm away from the monkeys’ eyes. The actual time the stimulus appeared on the monitor was recorded using a photodiode (OSRAM Opto Semiconductors, Sunnyvale, CA).</p><p>One electrode was affixed to a recording chamber and positioned within the craniotomy area using the Narishige two-axis platform, allowing for continuous electrode positioning adjustment. To make contact with or slightly penetrate the dura, a 28-gauge guide tube was inserted using a manual oil hydraulic micromanipulator from Narishige, Tokyo, Japan. For recording neural activity extracellularly in both monkeys, varnish-coated tungsten microelectrodes (FHC, Bowdoinham, ME) with a shank diameter of 200–250 μm and impedance of 0.2–1 Mω (measured at 1 kHz) were inserted into the brain. A pre-amplifier and amplifier (Resana, Tehran, Iran) were employed for single-electrode recordings, with filtering set between 300 Hz and 5 kHz for spikes and 0.1 Hz and 9 kHz for local field potentials. Spike waveforms and continuous data were digitized and stored at a sampling rate of 40 kHz for offline spike sorting and subsequent data analysis. Area IT was identified based on its stereotaxic location, position relative to nearby sulci, patterns of gray and white matter, and response properties of encountered units.</p></sec><sec id="s4-2"><title>Stimulus and task paradigm</title><p>The experimental task comprised two distinct phases: selectivity and main phases, each involving different stimuli. During the selectivity phase, the objective was to identify a responsive neuron for recording purposes. If an appropriate neuron was detected, the main phase was initiated. However, if a responsive neuron was not observed, the recording location was adjusted, and the selectivity phase was repeated. First, we will outline the procedure for stimulus construction, followed by an explanation of the task paradigm.</p><sec id="s4-2-1"><title>The stimulus set</title><p>The size of each image was 500 × 500 pixels. Images were displayed on a 120-Hz monitor with a resolution of 1920 × 1080 pixels. The monitor’s response time (changing the color of pixels in gray space) was one millisecond. The monkey’s eyes were located at a distance of 65 cm from the monitor. Each stimulus occupied a space of 5 × 5 degrees. All images were displayed in the center of the monitor. During the selectivity phase, a total number of 155 images were used as stimuli. Regarding SF, the stimuli were divided into unfiltered and filtered categories. Unfiltered images included 74 separate grayscale images in the categories of animal face, animal body, human face, human body, man-made, and natural. To create the stimulus, these images were placed on a gray background with a value of 0.5. The filtered images included 27 images in the same categories as the previous images, which were filtered in two frequency ranges (along with the intact form): low (1–10 cycles per image) and high (18–75 cycles per image), totaling 81 images. In the main phase of the test, nine images, including three non-face images and six face images, were considered. These images were displayed in the center of the monitor. During the selectivity phase, a total number of 155 images were used as stimuli. Regarding SF, the stimuli were divided into unfiltered and filtered categories. Unfiltered images included 74 separate grayscale images in the categories of animal face, animal body, human face, human body, man-made, and natural. To create the stimulus, these images were placed on a gray background with a value of 0.5. The filtered images included 27 images in the same categories as the previous images, which were filtered in two frequency ranges (along with the intact form): low (1–10 cycles per image) and high (18–75 cycles per image), totaling 81 images. In the main phase of the test, nine images, including three non-face images and six face images, were considered. These images were displayed in <xref ref-type="fig" rid="fig1">Figure 1c</xref>. For the main phase, the images were filtered in five frequency ranges. These intervals were 1–5, 5–10, 10–18, 18–45, and 45–75 cycles per image. For each image in each frequency range, a scrambled version had been obtained by scrambling the image phase in the Fourier transform domain. Therefore, each image in the main phase contained one unfiltered version (intact), five filtered versions (R1–R5), and six scrambled versions (i.e., 12 versions in total).</p></sec><sec id="s4-2-2"><title>SF filtering</title><p>Butterworth filters were used to filter the images in this study. A low-pass Butterworth filter is defined as follows:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  B_{lp}(r, f_{c}) = \frac{1}{1 + (r/f_{c})^{2n}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$B_{lp}$\end{document}</tex-math></alternatives></inline-formula> is the absolute value of the filter, <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> is the distance of the pixel from the center of the image, <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$f_{c}$\end{document}</tex-math></alternatives></inline-formula> is the filter’s cutoff frequency in terms of cycles per image, and <inline-formula><alternatives><mml:math id="inf4"><mml:mi>n</mml:mi></mml:math><tex-math id="inft4">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> is the order of the filter. Similarly, the high-pass filter is defined as follows:<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  B_{hp}(r, f_{c}) = \frac{1}{1 + (f_{c}/r)^{2n}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>To create a band-pass filter with a pass frequency of <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$f_{1}$\end{document}</tex-math></alternatives></inline-formula> and a cutoff frequency of <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$f_{2}$\end{document}</tex-math></alternatives></inline-formula>, a multiplication of a high-pass and a low-pass filter was performed <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$(B_{bp}(r, f_{1}, f_{2}) = B_{lp}(r; f_{1}) \times B_{hp}(r; f_{2}))$\end{document}</tex-math></alternatives></inline-formula>. To apply the filter, the image underwent a two-dimensional Fourier transform, followed by multiplication with the appropriate filter. Subsequently, the inverse Fourier transform was employed to obtain the filtered image. Afterward, a linear transformation was applied to adjust the brightness and contrast of the images. Brightness was determined by the average pixel value of the image, while contrast was represented by its standard deviation (STD). To achieve specific brightness (L) and contrast (C) levels, the following equation was employed to correct the images:<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>L</mml:mi></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  I_{norm}= C \times \left(\frac{I - \mu}{\sigma}\right) + L$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf8"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft8">\begin{document}$\sigma$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf9"><mml:mi>μ</mml:mi></mml:math><tex-math id="inft9">\begin{document}$\mu$\end{document}</tex-math></alternatives></inline-formula> are the STD and mean of the image. In this research, specific values for <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> were chosen as 0.5 (corresponding to 128 on a scale of 255) and 0.0314 (equivalent to 8 on a scale of 255), respectively. ANOVA indicated no significant difference in brightness and contrast among various groups, with p-values of 0.62 for brightness and 0.25 for contrast. Finally, we equalized the stimulus power in all SF bands (intact, R–R5). The SF power among all conditions (all SF bands, face vs. non-face and unscrambled vs. scrambled) does not vary significantly (ANOVA, p-value &gt;0.1). SF power is calculated as the sum of the square value of the image coefficients in the Fourier domain. To create scrambled images, the original image underwent Fourier transformation, after which its phase was scrambled. Subsequently, the inverse Fourier transform was applied. Since the resulting signal may not be real, its real part was extracted. The resulting image then underwent processing through the specified filters in the primary phase.</p></sec><sec id="s4-2-3"><title>Task paradigm</title><p>The task was divided into two distinct phases: the selectivity phase and the main phase. Each phase comprised multiple blocks, each containing two components: the presentation of a fixation point and a stimulus. The monkey was required to maintain fixation within a window of ±1.5 degrees around the center of the monitor throughout the entire task. During the selectivity phase, there were five blocks, and stimuli were presented randomly within each block. The duration of stimulus presentation was 50 ms, while the fixation point was presented for 500 ms. The selectivity phase consisted of a total of 775 trials. A neuron was considered responsive if it exhibited a significant increase in response during the time window of 70 to 170 ms after stimulus onset, compared to a baseline window of −50 to 50 ms. This significance was determined using the Wilcoxon signed-rank test with a significance level of 0.05. Once a neuron was identified as responsive, the main phase began. In the main phase, there were 15 blocks. The main phase involved a combination of the six most responsive stimuli, selected from the selectivity phase, along with nine fixed stimuli. There was, on average, 7.54 face stimuli in each session. In each block, all stimuli were presented once in random order. The stimulus duration in the main phase was 33 ms, and the fixation point was presented for 465 ms. For the purpose of analysis, our focus was primarily on the main phase of the task.</p></sec></sec><sec id="s4-3"><title>Neural representation</title><p>All analyses were conducted using custom code developed in Matlab (MathWorks). In total, 266 neurons (157 M1 and 109 M2) were considered for the analysis. The recorded locations, along with their SF and category selectivity are illustrated in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>. Neurons were sorted using the ROSS toolbox (<xref ref-type="bibr" rid="bib40">Toosi et al., 2021</xref>). In our analysis, we utilized both well-isolated single units and multi-unit activities (which represent neural activities that could not be further sorted into single units), ensuring a comprehensive representation of neural responses across the recorded population. Each stimulus in each time step was represented by a vector of <italic>N</italic> elements where the <italic>i</italic>th element was the average response of the <italic>i</italic>th neuron for that stimulus in a time window of 50 ms around the given time step. We used both single- and population-level analysis. Numerous studies had examined the benefits of population representation (<xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>; <xref ref-type="bibr" rid="bib2">Adibi et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Dehaqani et al., 2018</xref>). These studies have demonstrated that enhancing signal correlation within the neural data population leads to improved decoding performance for object discrimination. To maintain consistency across trials, responses were normalized using the <italic>z</italic>-score procedure. All time courses were based on a 50-ms sliding window with a 5-ms time step. We utilized a time window from 70 to 170 ms after stimulus onset for our analysis (except for temporal analysis). This time window was selected because the average firing rate across neurons was significantly higher than the baseline window of −50 to 50 ms (Wilcoxon signed-rank test, p-value &lt;0.05).</p></sec><sec id="s4-4"><title>Statistical analysis</title><p>All statistical analyses were conducted as outlined in this section unless otherwise specified. In the single-level analysis, where each run involves a single neuron, pair comparisons were performed using the Wilcoxon signed-rank test, and unpaired comparisons utilized the Wilcoxon rank-sum test, both at a significance level of 0.05. The results and their SEM were reported. For population analysis, we used an empirical method, and the results were reported with their STD. To compare two paired sets of <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> (<inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> could represent the chance level), we calculated the statistic <inline-formula><alternatives><mml:math id="inf15"><mml:mi>r</mml:mi></mml:math><tex-math id="inft15">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> as the number of pairs where <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$x-y \lt 0$\end{document}</tex-math></alternatives></inline-formula>. The p-value was computed as r divided by the total number of runs, <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$r/M$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> is the total number of runs. When <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$r = 0$\end{document}</tex-math></alternatives></inline-formula>, we used the notation of p-value &lt;<inline-formula><alternatives><mml:math id="inf20"><mml:mstyle><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$1/M$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-5"><title>Classification</title><p>All classifications were carried out employing the LDA method, both in population and single level. As described before, each stimulus in each block was shown by an <italic>N</italic>-element vector to be fed to the classifier. For face (non-face) vs. scrambled classification, only the face (non-face) and scrambled intact stimuli were used. For face vs. non-face (category) classification, only unscrambled intact stimuli were utilized. Finally, only the scrambled stimuli were fed to the classifier for the SF classification, and the labels were SF bands (R1, R2, ..., R5, multi-label classifier). In population-level analysis, averages and standard deviations were computed using a leave-p-out method, where 30% of the samples were kept as test samples in each run. All analyses were based on 1000 leave-p-out runs. To determine the onset time, one STD was added to the average accuracy value in the interval of 50 ms before to 50 ms after stimulus onset. Then, the onset time was identified as the point where the accuracy was significantly greater than this value for five consecutive time windows.</p></sec><sec id="s4-6"><title>Preferred SF</title><p>Preferred SF for a given neuron was calculated as follows:<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle  PSF = \sum_{i}(f_{Ri}\times c_{Ri}) / \sum_{i}f_{Ri}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$PSF$\end{document}</tex-math></alternatives></inline-formula> is the preferred SF, <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$f_{Ri}$\end{document}</tex-math></alternatives></inline-formula> is the average firing rate of the neuron for <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$Ri$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf24"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft24">\begin{document}$c_{Ri}$\end{document}</tex-math></alternatives></inline-formula> is −2 for R1, −1 for R2, …, 2 for R5. When <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$PSF\gt0$\end{document}</tex-math></alternatives></inline-formula>, the neuron exhibits higher firing rates for higher SF ranges on average and vice versa. To identify the number of neurons responding to a specific SF range higher than others, we performed an ANOVA analysis with a significance level of 0.05. Then, we picked the SF range with the highest firing rate for that neuron.</p></sec><sec id="s4-7"><title>SF profile</title><p>To form the SF profiles, a quadratic curve was fitted to the neuron response from R1 to R5, using exclusively scrambled stimuli. Each trial was treated as an individual sample. Neurons were categorized into three groups based on the extremum point of the fitted curve: (1) extremum is lower than R2, (2) between R2 and R4, and (3) greater than R4. Within the first group, if the neuron’s response in R1 and R2 significantly exceeded (or fell below) R4 and R5, the SF profile was classified as LP (or HP). The same procedure went for the third group. Considering the second group, if the neuron response in R2 was significantly (Wilcoxon signed-rank) higher (or lower) than the response of R1 and R5, the neuron profile identified as U (or IU). Neurons not meeting any of these criteria were grouped under the flat category.</p><p>To establish sub-populations of SF/category-sorted neurons, we initially sorted the neurons according to their accuracy to decode the SF/category. Subsequently, a sliding window of size 20 was employed to select adjacent neurons in the SF- or category-sorted list. Consequently, the first sub-population comprised the initial 20 neurons exhibiting the lowest individual accuracy in decoding the SF/category. In comparison, the last sub-population encompassed the top 20 neurons with the highest accuracy in decoding SF/category.</p></sec><sec id="s4-8"><title>Separability index</title><p>The discrimination of two or more categories, as represented by the responses of the IT population, can be characterized through the utilization of the scatter matrix of category members. The scatter matrix serves as an approximate measure of covariance within a high-dimensional space. The discernibility of these categories is influenced by two key components: the scatter within a category and the scatter between categories. SI is defined as the ratio of between- to within-category scatter. The computation of SI involves three sequential steps. Initially, the center of mass for each category, referred to as <inline-formula><alternatives><mml:math id="inf26"><mml:mi>μ</mml:mi></mml:math><tex-math id="inft26">\begin{document}$\mu$\end{document}</tex-math></alternatives></inline-formula> and the overall mean across all categories, termed the total mean, <inline-formula><alternatives><mml:math id="inf27"><mml:mi>m</mml:mi></mml:math><tex-math id="inft27">\begin{document}$m$\end{document}</tex-math></alternatives></inline-formula>, was calculated. Second, we calculated the between- and within-category scatters,<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∑</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  S_{i}&amp;=  \sum_{j,k \in C_i}(r_{j}- \mu_{j})(r_{k}- \mu_{k})\nonumber\\ S_{w}&amp;=  \sum S_{i}\nonumber\\ S_{B}&amp;=  \sum_{j,k \in C_i}n_{i}\times (\mu_{i}- m)(\mu_{i}- m)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$S_{i}$\end{document}</tex-math></alternatives></inline-formula> is the scatter matrix of the <italic>i</italic>th category, <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> is the stimulus response, <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$S_{w}$\end{document}</tex-math></alternatives></inline-formula> is within-category scatter, <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$S_{B}$\end{document}</tex-math></alternatives></inline-formula> is the between-category scatter, and <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$n_{i}$\end{document}</tex-math></alternatives></inline-formula> is the number of samples in the <italic>i</italic>th category. Finally, SI was computed as<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  SI = \frac{\lVert S_{B}\rVert}{\lVert S_{W}\rVert}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>S</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$\lVert S\rVert$\end{document}</tex-math></alternatives></inline-formula> indicates the norm of <italic>S</italic>. For additional information, please refer to the study conducted by <xref ref-type="bibr" rid="bib17">Dehaqani et al., 2016</xref>.</p></sec><sec id="s4-9"><title>SNC and CMI</title><p>To examine the influence of individual neurons on population-level decoding, we introduced the concept of the SNC. It measures the reduction in decoding performance when a single neuron is removed from the population. We systematically removed each neuron from the population one at a time and measured the corresponding drop in accuracy compared to the case where all neurons were present.</p><p>To quantify the CMI between pairs of neurons, we discretized their response patterns using 10 levels of uniformly spaced bins. The CMI is calculated using the following formula:<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  CMI(n_{i}, n_{j}| c) = \sum_{n_i \in N_i, n_j \in N_j, c \in C}P(n_{i}, n_{j}, c) \times log_{2}\frac{P(n_{i}, n_{j}| c)}{P(n_{i}| c) \times P(n_{j}| c)}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf34"><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="inft34">\begin{document}$n_{i}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$n_{j}$\end{document}</tex-math></alternatives></inline-formula> represent the discretized responses of the two neurons, and <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> represents the conditioned variable, which can be the category (face/non-face) or the SF range (LSF (R1 and R2) and HSF (R4 and R5)). We normalized the CMI by subtracting the CMI obtained from randomly shuffled responses and added the average CMI of SF and category. CMI calculation enables us to assess the degree of information shared or exchanged between pairs of neurons, conditioned on the category or SF while accounting for the underlying probability distributions.</p></sec><sec id="s4-10"><title>Sparseness analysis</title><p>The sparseness analysis was conducted on the LDA weights, regarded as a measure of task relevance. To calculate the sparseness of the LDA weights, the neuron responses were first normalized using the <italic>z</italic>-score method. Then, the sparseness of the weights associated with the neurons in the LDA classifier was computed. The sparseness is computed using the following formula:<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  S = 1 - \frac{E(|w|)^{2}}{E(w^{2})}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$w$\end{document}</tex-math></alternatives></inline-formula> is the neuron weight in LDA and <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$E(w^{2})$\end{document}</tex-math></alternatives></inline-formula> represents the mean of the squared weights of the neurons. The maximum sparseness occurs when only one neuron is active, whereas the minimum sparseness occurs when all neurons are equally active.</p></sec><sec id="s4-11"><title>Deep neural network analysis</title><p>To compare our findings with those derived from deep neural networks, we commenced by curating a diverse assortment of CNN architectures. This selection encompassed ResNet18, ResNet34, VGG11, VGG16, InceptionV3, EfficientNetb0, CORNet-S, CORTNet-RT, and CORNet-z, strategically chosen to offer a comprehensive overview of SF processing capabilities within deep learning models. Our experimentation spanned the utilization of both randomly initialized weights and pre-trained weights sourced from the ImageNet dataset. This dual approach allowed us to assess the influence of prior knowledge embedded in pre-trained weights on SF decoding. In the process of extracting feature maps, we fed our stimulus set to the models, capturing the feature maps from the last four layers, excluding the classifier layer. Our results were primarily rooted in the final layer (preceding classification), yet they demonstrated consistency across all layers under examination. For classification and SF profiling, our methodology mirrored the procedures employed in our neural response analysis.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Data curation, Methodology</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Data curation, Methodology</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Investigation</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Validation, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Data curation, Supervision, Validation, Investigation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The activity of neurons in the IT cortex of two male macaque monkeys weighing 10 and 11 kg, respectively, was analyzed following the National Institutes of Health Guide for the Care and Use of Laboratory Animals and the Society for Neuroscience Guidelines and Policies. The experimental procedures were approved by the Institute of Fundamental Science committee.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93589-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Source code for controlling stimulus presentation and juice delivery using MATLAB and the MonkeyLogic toolbox.</title></caption><media xlink:href="elife-93589-code1-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files; source data files have been provided for Figures 1–6. A repository containing sample data and the code necessary to reproduce the main results is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ramintoosi/spatial-frequency-selectivity">https://github.com/ramintoosi/spatial-frequency-selectivity</ext-link> (copy archived at <xref ref-type="bibr" rid="bib42">Toosi, 2025</xref>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of correlated variability on the accuracy of a population code</article-title><source>Neural Computation</source><volume>11</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1162/089976699300016827</pub-id><pub-id pub-id-type="pmid">9950724</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adibi</surname><given-names>M</given-names></name><name><surname>McDonald</surname><given-names>JS</given-names></name><name><surname>Clifford</surname><given-names>CWG</given-names></name><name><surname>Arabzadeh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Population decoding in rat barrel cortex: optimizing the linear readout of correlated population responses</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003415</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003415</pub-id><pub-id pub-id-type="pmid">24391487</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashtiani</surname><given-names>MN</given-names></name><name><surname>Kheradpisheh</surname><given-names>SR</given-names></name><name><surname>Masquelier</surname><given-names>T</given-names></name><name><surname>Ganjtabesh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Object categorization in finer levels relies more on higher spatial frequencies and takes longer</article-title><source>Frontiers in Psychology</source><volume>8</volume><elocation-id>1261</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.01261</pub-id><pub-id pub-id-type="pmid">28790954</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Awasthi</surname><given-names>B</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reach trajectories reveal delayed processing of low spatial frequency faces in developmental prosopagnosia</article-title><source>Cognitive Neuroscience</source><volume>3</volume><fpage>120</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1080/17588928.2012.673482</pub-id><pub-id pub-id-type="pmid">24168693</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A cortical mechanism for triggering top-down facilitation in visual object recognition</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>600</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1162/089892903321662976</pub-id><pub-id pub-id-type="pmid">12803970</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastin</surname><given-names>J</given-names></name><name><surname>Vidal</surname><given-names>JR</given-names></name><name><surname>Bouvier</surname><given-names>S</given-names></name><name><surname>Perrone-Bertolotti</surname><given-names>M</given-names></name><name><surname>Bénis</surname><given-names>D</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>David</surname><given-names>O</given-names></name><name><surname>Lachaux</surname><given-names>JP</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Temporal components in the parahippocampal place area revealed by human intracerebral recordings</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>10123</fpage><lpage>10131</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4646-12.2013</pub-id><pub-id pub-id-type="pmid">23761907</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez</surname><given-names>MA</given-names></name><name><surname>Vicente</surname><given-names>AF</given-names></name><name><surname>Romero</surname><given-names>MC</given-names></name><name><surname>Perez</surname><given-names>R</given-names></name><name><surname>Gonzalez</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spatial frequency components influence cell activity in the inferotemporal cortex</article-title><source>Visual Neuroscience</source><volume>26</volume><fpage>421</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1017/S0952523809990198</pub-id><pub-id pub-id-type="pmid">19804657</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caplette</surname><given-names>L</given-names></name><name><surname>West</surname><given-names>G</given-names></name><name><surname>Gomot</surname><given-names>M</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Wicker</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Affective and contextual values modulate spatial frequency use in object recognition</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>512</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00512</pub-id><pub-id pub-id-type="pmid">24904514</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaumon</surname><given-names>M</given-names></name><name><surname>Kveraga</surname><given-names>K</given-names></name><name><surname>Barrett</surname><given-names>LF</given-names></name><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual predictions in the orbitofrontal cortex rely on associative content</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>2899</fpage><lpage>2907</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht146</pub-id><pub-id pub-id-type="pmid">23771980</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CY</given-names></name><name><surname>Sonnenberg</surname><given-names>L</given-names></name><name><surname>Weller</surname><given-names>S</given-names></name><name><surname>Witschel</surname><given-names>T</given-names></name><name><surname>Hafed</surname><given-names>ZM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial frequency sensitivity in macaque midbrain</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2852</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05302-5</pub-id><pub-id pub-id-type="pmid">30030440</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname><given-names>OS</given-names></name><name><surname>Richler</surname><given-names>JJ</given-names></name><name><surname>Palmeri</surname><given-names>TJ</given-names></name><name><surname>Gauthier</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Revisiting the role of spatial frequencies in the holistic processing of faces</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>34</volume><fpage>1327</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1037/a0011752</pub-id><pub-id pub-id-type="pmid">19045978</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname><given-names>OS</given-names></name><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The resilience of object predictions: Early recognition across viewpoints and exemplars</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>682</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0546-5</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costen</surname><given-names>NP</given-names></name><name><surname>Parker</surname><given-names>DM</given-names></name><name><surname>Craw</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Effects of high-pass and low-pass spatial filtering on face identification</article-title><source>Perception &amp; Psychophysics</source><volume>58</volume><fpage>602</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.3758/BF03213093</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craddock</surname><given-names>M</given-names></name><name><surname>Martinovic</surname><given-names>J</given-names></name><name><surname>Müller</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Task and spatial frequency modulations of object processing: an EEG study</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e70293</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0070293</pub-id><pub-id pub-id-type="pmid">23936181</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalgleish</surname><given-names>HW</given-names></name><name><surname>Russell</surname><given-names>LE</given-names></name><name><surname>Packer</surname><given-names>AM</given-names></name><name><surname>Roth</surname><given-names>A</given-names></name><name><surname>Gauld</surname><given-names>OM</given-names></name><name><surname>Greenstreet</surname><given-names>F</given-names></name><name><surname>Thompson</surname><given-names>EJ</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How many neurons are sufficient for perception of cortical activity?</article-title><source>eLife</source><volume>9</volume><elocation-id>e58889</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58889</pub-id><pub-id pub-id-type="pmid">33103656</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaqani</surname><given-names>MRA</given-names></name><name><surname>Vahabie</surname><given-names>AH</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Ahmadabadi</surname><given-names>MN</given-names></name><name><surname>Araabi</surname><given-names>BN</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Temporal dynamics of visual category representation in the macaque inferior temporal cortex</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>587</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1152/jn.00018.2016</pub-id><pub-id pub-id-type="pmid">27169503</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaqani</surname><given-names>MRA</given-names></name><name><surname>Vahabie</surname><given-names>AH</given-names></name><name><surname>Parsa</surname><given-names>M</given-names></name><name><surname>Noudoost</surname><given-names>B</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Selective changes in noise correlations contribute to an enhanced representation of saccadic targets in prefrontal neuronal ensembles</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>3046</fpage><lpage>3063</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy141</pub-id><pub-id pub-id-type="pmid">29893800</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenske</surname><given-names>MJ</given-names></name><name><surname>Aminoff</surname><given-names>E</given-names></name><name><surname>Gronau</surname><given-names>N</given-names></name><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Top-down facilitation of visual object recognition: object-based and context-based contributions</article-title><source>Progress in Brain Research</source><volume>155</volume><fpage>3</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)55001-0</pub-id><pub-id pub-id-type="pmid">17027376</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fintzi</surname><given-names>AR</given-names></name><name><surname>Mahon</surname><given-names>BZ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A bimodal tuning curve for spatial frequency across left and right human orbital frontal cortex during object recognition</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1311</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs419</pub-id><pub-id pub-id-type="pmid">23307636</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorentini</surname><given-names>A</given-names></name><name><surname>Maffei</surname><given-names>L</given-names></name><name><surname>Sandini</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The role of high spatial frequencies in face perception</article-title><source>Perception</source><volume>12</volume><fpage>195</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1068/p120195</pub-id><pub-id pub-id-type="pmid">6657426</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Bentin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Coarse-to-fine encoding of spatial frequency information into visual short-term memory for faces but impartial decay</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>37</volume><fpage>1051</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.1037/a0023091</pub-id><pub-id pub-id-type="pmid">21500938</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaska</surname><given-names>JP</given-names></name><name><surname>Jacobson</surname><given-names>LD</given-names></name><name><surname>Pollen</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Spatial and temporal frequency selectivity of neurons in visual cortical area V3A of the macaque monkey</article-title><source>Vision Research</source><volume>28</volume><fpage>1179</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(88)90035-1</pub-id><pub-id pub-id-type="pmid">3253990</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>T</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Recognition of positive and negative bandpass-filtered images</article-title><source>Perception</source><volume>15</volume><fpage>595</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1068/p150595</pub-id><pub-id pub-id-type="pmid">3588219</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>613</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1038/nn.4247</pub-id><pub-id pub-id-type="pmid">26900926</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iidaka</surname><given-names>T</given-names></name><name><surname>Yamashita</surname><given-names>K</given-names></name><name><surname>Kashikura</surname><given-names>K</given-names></name><name><surname>Yonekura</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Spatial frequency of visual image modulates neural responses in the temporo-occipital lobe. An investigation with event-related fMRI</article-title><source>Brain Research. Cognitive Brain Research</source><volume>18</volume><fpage>196</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2003.10.005</pub-id><pub-id pub-id-type="pmid">14736578</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jahfari</surname><given-names>S</given-names></name><name><surname>Ridderinkhof</surname><given-names>KR</given-names></name><name><surname>Scholte</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spatial frequency information modulates response inhibition and decision-making processes</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e76467</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0076467</pub-id><pub-id pub-id-type="pmid">24204630</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeantet</surname><given-names>C</given-names></name><name><surname>Laprevote</surname><given-names>V</given-names></name><name><surname>Schwan</surname><given-names>R</given-names></name><name><surname>Schwitzer</surname><given-names>T</given-names></name><name><surname>Maillard</surname><given-names>L</given-names></name><name><surname>Lighezzolo-Alnot</surname><given-names>J</given-names></name><name><surname>Caharel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Time course of spatial frequency integration in face perception: An ERP study</article-title><source>International Journal of Psychophysiology</source><volume>143</volume><fpage>105</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2019.07.001</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joubert</surname><given-names>OR</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Processing scene context: fast categorization and object interference</article-title><source>Vision Research</source><volume>47</volume><fpage>3286</fpage><lpage>3297</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.09.013</pub-id><pub-id pub-id-type="pmid">17967472</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kauffmann</surname><given-names>L</given-names></name><name><surname>Chauvin</surname><given-names>A</given-names></name><name><surname>Guyader</surname><given-names>N</given-names></name><name><surname>Peyrin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rapid scene categorization: role of spatial frequency order, accumulation mode and luminance contrast</article-title><source>Vision Research</source><volume>107</volume><fpage>49</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2014.11.013</pub-id><pub-id pub-id-type="pmid">25499838</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cornet: modeling the neural mechanisms of core object recognition</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/408385</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain-like object recognition with high-performing shallow recurrent anns</article-title><conf-name>Proceedings of the 33rd International Conference on Neural Information Processing Systems</conf-name><fpage>12805</fpage><lpage>12816</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oram</surname><given-names>MW</given-names></name><name><surname>Perrett</surname><given-names>DI</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Modeling visual recognition from neurobiological constraints</article-title><source>Neural Networks</source><volume>7</volume><fpage>945</fpage><lpage>972</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(05)80153-4</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peyrin</surname><given-names>C</given-names></name><name><surname>Michel</surname><given-names>CM</given-names></name><name><surname>Schwartz</surname><given-names>S</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Seghier</surname><given-names>M</given-names></name><name><surname>Landis</surname><given-names>T</given-names></name><name><surname>Marendaz</surname><given-names>C</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural substrates and timing of top-down processes during coarse-to-fine categorization of visual scenes: a combined fMRI and ERP study</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>2768</fpage><lpage>2780</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21424</pub-id><pub-id pub-id-type="pmid">20044901</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purushothaman</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Yampolsky</surname><given-names>D</given-names></name><name><surname>Casagrande</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of coarse-to-fine discrimination in the visual cortex</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>2822</fpage><lpage>2833</lpage><pub-id pub-id-type="doi">10.1152/jn.00612.2013</pub-id><pub-id pub-id-type="pmid">25210162</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokszin</surname><given-names>AA</given-names></name><name><surname>Győri-Dani</surname><given-names>D</given-names></name><name><surname>Nyúl</surname><given-names>LG</given-names></name><name><surname>Csifcsák</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Electrophysiological correlates of top-down effects facilitating natural image categorization are disrupted by the attenuation of low spatial frequency information</article-title><source>International Journal of Psychophysiology</source><volume>100</volume><fpage>19</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2015.12.006</pub-id><pub-id pub-id-type="pmid">26707649</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rotshtein</surname><given-names>P</given-names></name><name><surname>Schofield</surname><given-names>A</given-names></name><name><surname>Funes</surname><given-names>MJ</given-names></name><name><surname>Humphreys</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Effects of spatial frequency bands on perceptual decision: it is not the stimuli but the comparison</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.1167/10.10.25</pub-id><pub-id pub-id-type="pmid">20884490</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saneyoshi</surname><given-names>A</given-names></name><name><surname>Michimata</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Categorical and coordinate processing in object recognition depends on different spatial frequencies</article-title><source>Cognitive Processing</source><volume>16</volume><fpage>27</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1007/s10339-014-0635-z</pub-id><pub-id pub-id-type="pmid">25236965</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>From blobs to boundary edges: evidence for time- and spatial-scale-dependent scene recognition</article-title><source>Psychological Science</source><volume>5</volume><fpage>195</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1994.tb00500.x</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toosi</surname><given-names>R</given-names></name><name><surname>Akhaee</surname><given-names>MA</given-names></name><name><surname>Dehaqani</surname><given-names>MRA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An automatic spike sorting algorithm based on adaptive spike detection and a mixture of skew-t distributions</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>13925</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-93088-w</pub-id><pub-id pub-id-type="pmid">34230517</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Toosi</surname><given-names>R</given-names></name><name><surname>Akhaee</surname><given-names>MA</given-names></name><name><surname>Dehaqani</surname><given-names>MRA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Brain-inspired feedback for spatial frequency aware artificial networks</article-title><conf-name>2022 56th Asilomar Conference on Signals, Systems, and Computers</conf-name><conf-loc>Pacific Grove, CA, USA</conf-loc><fpage>806</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1109/IEEECONF56349.2022.10052093</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Toosi</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Spatial frequency decoding in the inferior temporal cortex</data-title><version designator="swh:1:rev:657a2084a9ce0937aa371a4a7ae497e82290e590">swh:1:rev:657a2084a9ce0937aa371a4a7ae497e82290e590</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:73c2cfdd0fca90b85ad355872d7ca017baeaf62f;origin=https://github.com/ramintoosi/spatial-frequency-selectivity;visit=swh:1:snp:aed8f5b830e2d61caeb5c2bba2faea93346dfe89;anchor=swh:1:rev:657a2084a9ce0937aa371a4a7ae497e82290e590">https://archive.softwareheritage.org/swh:1:dir:73c2cfdd0fca90b85ad355872d7ca017baeaf62f;origin=https://github.com/ramintoosi/spatial-frequency-selectivity;visit=swh:1:snp:aed8f5b830e2d61caeb5c2bba2faea93346dfe89;anchor=swh:1:rev:657a2084a9ce0937aa371a4a7ae497e82290e590</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yardley</surname><given-names>H</given-names></name><name><surname>Perlovsky</surname><given-names>L</given-names></name><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Predictions and incongruency in object recognition: a cognitive neuroscience perspective</chapter-title><person-group person-group-type="editor"><name><surname>Yardley</surname><given-names>H</given-names></name></person-group><source>Detection and Identification of Rare Audiovisual Cues</source><publisher-name>Springer Nature</publisher-name><fpage>139</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-24034-8_12</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Schriver</surname><given-names>KE</given-names></name><name><surname>Hu</surname><given-names>JM</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spatial frequency representation in V2 and V4 of macaque monkey</article-title><source>eLife</source><volume>12</volume><elocation-id>e81794</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.81794</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Strength of SF selectivity</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Strength of SF selectivity.</title><p>To assess the strength of SF selectivity in IT responses, we first ranked the SF content based on the firing rate in each neuron employing half of the trials. Then, the other half is used to calculate the firing rate of each rank. Results show that the firing rate of rank 5 is significantly higher than rank 1 (p-value = 4 × 10<sup>−4</sup>). The error bars show STD.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>SF response distribution.</title><p>To check the SF response strength, the histogram of IT neuron responses to scrambled, face, and non-face stimuli is illustrated in this figure. A Gamma distribution is also fitted to each histogram. To calculate the histogram, the neuron response to each unique stimulus is calculated for each neuron in spike/seconds (Hz). In the early phase, T1, the average firing rate to scrambled stimuli is 26.3 Hz which is significantly higher than the response in −50 to 50 ms which is 23.4 Hz. In comparison, the mean response to intact face stimuli is 30.5 Hz, while non-face stimuli elicit an average response of 28.8 Hz. The average net responses to the scrambled, face, and non-face stimuli are 2.9, 7.1, and 5.4 Hz, respectively. Moving to the late phase, T2, the responses to scrambled, face, and object stimuli are 19.5, 19.4, and 22.4 Hz, respectively. The corresponding average net responses are 3.9, 4.0, and 1.0 Hz below the baseline response. While the firing rates and net responses to scrambled stimuli were modest (e.g., 2.9 Hz in T1), the differences across spatial frequency (SF) bands were statistically significant (p ≈ 1e−5) and led to a classification accuracy 24.68% above chance. This demonstrates the robustness of SF modulation in IT neurons despite low firing rates. The modest responses align with expectations for noise-like stimuli, which are less effective in driving IT neurons, yet the observed SF selectivity highlights a fundamental property of IT encoding.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-app1-fig2-v1.tif"/></fig></sec><sec sec-type="appendix" id="s9"><title>Robustness of SF profiles</title><p>To investigate the robustness of the SF profiles, considering the trial-to-trial variability, we calculated the neuron’s profile using half of the trials. Then, the neuron’s response to R1, R2, ..., R5 is calculated with the remaining trials. <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref> illustrates the average response of each profile for SF bands in each profile.</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>SF profile robustness.</title><p>Profiles are calculated using half of the trials. Then, the average of the neuron responses in each profile is calculated with the remaining half. STD is illustrated with error bars.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-app1-fig3-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10"><title>Extended stimulus duration supports LSF-preferred tuning</title><p>Our recorded data in the main phase contains the 200 ms version of stimulus duration for all neurons. In this experiment, we investigate the impact of stimulus duration on LSF-preferred recall and course-to-fine nature of SF decoding. As illustrated in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, the LSF-preferred nature of SF decoding (recall R1 = 0.60 ± 0.02, R2 = 0.44 ± 0.03, R3 = 0.32 ± 0.03, R4 = 0.35 ± 0.03, R5 = 0.36 ± 0.02, and R1 &gt; R5, p-value &lt;0.001) and course-to-fine nature of SF processing (onset times in milliseconds after stimulus onset, R1 = 87.0 ± 2.9, R2 = 86.0 ± 4.0, R3 = 93.8 ± 3.5, R4 = 96.1 ± 3.9, R5 = 96.0 ± 3.9, R1 &lt; R5, p-value &lt;0.001) is observed in extended stimulus duration. For the 200-ms stimulus duration, the firing rates were 27.7, 30.7, and 30.4 Hz for scrambled, face, and object stimuli in T1, respectively, and 26.2, 29.1, and 33.9 Hz in T2. The corresponding net responses were 4.3, 7.3, and 7.0 Hz in T1, and 2.8, 5.7, and 10.5 Hz in T2. While the longer stimulus duration did not substantially increase firing rates in T1, its impact was more pronounced in T2.</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>LSF-preferred responses with extended stimulus duration.</title><p>We conducted the experiments in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1c</xref> and <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2a</xref> with 200 ms of stimulus duration with the same method, in 70–170 ms after stimulus onset. (<bold>a</bold>) The recall of each SF band in the population, as elicited by scrambled stimuli and determined through the LDA method, is presented. The error bars denote the STD. The findings support the LSF-preferred nature of SF decoding observed with 33 ms of stimulus duration. (<bold>b</bold>) The onset time of recall for each spatial SF band in response to scrambled stimuli is depicted, with error bars representing the STD. The results imply an increasing onset time of decoding as SF values rise, as we observed in 33-ms stimulus duration.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-app1-fig4-v1.tif"/></fig></sec><sec sec-type="appendix" id="s11"><title>SF and category selectivity based on the neuron’s location</title><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>The SF and category selectivity of the recorded locations.</title><p>The accuracy of single neurons for SF prediction (<bold>a</bold>) and category prediction (<bold>b</bold>) is illustrated for each recorded location. The <italic>x</italic>- and <italic>y</italic>-axes show anterior–posterior (A/P) or medial–lateral (M/L) hole location and the depth of the electrode in milliliters. A/P ranges from 5 mm (hole number 1) to 30 mm (hole number 18) and M/L ranges from 0 mm (hole number 1) to 23 mm (hole number 18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-app1-fig5-v1.tif"/></fig></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s12"><title>Main results for each monkey</title><p>In this section, we provide a summary of the main results for each monkey. <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> illustrates the key findings separately for M1 (157 neurons) and M2 (109 neurons). Regarding recall, both monkeys exhibit a decrease in recall values as the shift toward higher frequencies occurs (recall value for <bold>M1:</bold> R1 = 0.32 ± 0.03, R2 = 0.30 ± 0.02, R3 = 0.25 ± 0.03, R4 = 0.24 ± 0.03, and R5 = 0.24 ± 0.03. <bold>M2:</bold> R1 = 0.60 ± 0.03, R2 = 0.38 ± 0.03, R3 = 0.29 ± 0.03, R4 = 0.35 ± 0.03, and R5 = 0.35 ± 0.03). In both monkeys, the recall value of R1 is significantly lower than R5 (for both M1 and M2, p-value &lt;0.001). In terms of onset, we observed a coarse-to-fine behavior in both monkeys (onset value in ms, <bold>M1:</bold> R1 = 84.7 ± 5.5, R2 = 82.1 ± 4.5, R3 = 90.0 ± 4.3, R4 = 86.8 ± 7.0, R5 = 103.3 ± 5.2. <bold>M2:</bold> R1 = 76.6 ± 1.3, R2 = 76.0 ± 1.2, R3 = 90.0 ± 4.3, R4 = 86.8 ± 2.2, R5 = 89.0 ± 1.9). Next, we examined the SF-based profiles (<xref ref-type="fig" rid="fig3">Figure 3</xref>) in M1 and M2. As depicted in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1c</xref>, both monkeys exhibit similar decoding capabilities in the SF-based profiles, consistent with what we observed in <xref ref-type="fig" rid="fig3">Figure 3</xref>. In both M1 and M2, face decoding in HP significantly surpasses face/non-face decoding in all other profiles (<bold>M1:</bold> face SI: LP = 0.23 ± 0.05, HP = 0.91 ± 0.16, IU = 0.06 ± 0.03, U = 0.14 ± 0.02/non-face, and HP &gt; LP, U, IU with p-value &lt;0.001. Non-face SI: LP = 0.13 ± 0.07, HP = 0.08 ± 0.05, IU = 0.16 ± 0.09, U = 0.19 ± 0.10, and face SI in HP is greater than non-face SI in all profiles with p-value &lt;0.001. <bold>M2:</bold> face SI: LP = 0.07 ± 0.03, HP = 0.38 ± 0.18, IU = 0.06 ± 0.03, U = 0.07 ± 0.05/non-face, and HP &gt; LP, U, IU with p-value &lt;0.001. Non-face SI: LP = 0.08 ± 0.06, HP = 0.03 ± 0.03, IU = 0.17 ± 0.04, U = 0.07 ± 0.05, and face SI in HP is greater than non-face SI in all profiles with p-value &lt;0.001). Furthermore, in both monkeys, the non-face decoding capability in IU is significantly higher than face decoding (p-value &lt;0.001). For neurons preferring LSF, LP profile, it is important to note the lack of consistency in responses across monkeys. This variability may reflect individual differences in neural processing or variations in sampling between subjects.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Main results for the two monkeys.</title><p>Their call (<bold>a</bold>), on set of recall (<bold>b</bold>) and SI of each profile (<bold>c</bold>) is illustrated for M1 and M2, respectively. The results are consistent with our observations in the Results section. The error bars indicate STD.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93589-app2-fig1-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93589.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Otto-von-Guericke University Magdeburg</institution><country>Germany</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Incomplete</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Useful</kwd></kwd-group></front-stub><body><p>This <bold>useful</bold> study aimed to examine the relationship of spatial frequency selectivity of single macaque inferotemporal (IT) neurons to category selectivity. Interesting findings in this report suggest a shift in preferred spatial frequency during the response, from low to high spatial frequencies. This agrees with a coarse-to-fine processing strategy, which is in line with multiple studies in the early visual cortex. Some of the findings were difficult to evaluate because the methods are <bold>incomplete</bold>. The conclusion that single-unit spatial frequency selectivity can predict object coding requires further evidence to confirm.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93589.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This study reports that spatial frequency representation can predict category coding in the inferior temporal cortex. The original conclusion was based on likely problematic stimulus timing (33 ms which was too brief). Now the authors claim that they also have a different set of data on the basis of longer stimulus duration (200 ms).</p><p>One big issue in the original report was that the experiments used a stimulus duration that was too brief and could have weakened the effects of high spatial frequencies and confounded the conclusions. Now the authors provided a new set of data on the basis of a longer stimulus duration and made the claim that the conclusions are unchanged. These new data and the data in the original report were collected at the same time as the authors report.</p><p>The authors may provide an explanation why they performed the same experiments using two stimulus durations and only reported one data set with the brief duration. They may also explain why they opted not to mention in the original report the existence of another data set with a different stimulus duration, which would otherwise have certainly strengthened their main conclusions.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93589.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper aimed to examine the spatial frequency selectivity of macaque inferotemporal (IT) neurons and its relation to category selectivity. The authors suggest in the present study that some IT neurons show a sensitivity for the spatial frequency of scrambled images. Their report suggests a shift in preferred spatial frequency during the response, from low to high spatial frequencies. This agrees with a coarse-to-fine processing strategy, which is in line with multiple studies in the early visual cortex. In addition, they report that the selectivity for faces and objects, relative to scrambled stimuli, depends on the spatial frequency tuning of the neurons.</p><p>Strengths:</p><p>Previous studies using human fMRI and psychophysics studied the contribution of different spatial frequency bands to object recognition, but as pointed out by the authors little is known about the spatial frequency selectivity of single IT neurons. This study addresses this gap and shows spatial frequency selectivity in IT for scrambled stimuli that drive the neurons poorly. They related this weak spatial frequency selectivity to category selectivity, but these findings are premature given the low number of stimuli they employed to assess category selectivity.</p><p>The authors revised their manuscript and provided some clarifications regarding their experimental design and data analysis. They responded to most of my comments but I find that some issues were not fully or poorly addressed. The new data they provided confirmed my concern about low responses to their scrambled stimuli. Thus, this paper shows spatial frequency selectivity in IT for scrambled stimuli that drive the neurons poorly (see main comments below). They related this (weak) spatial frequency selectivity to category selectivity, but these findings are premature given the low number of stimuli to assess category selectivity.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93589.4.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Toosi</surname><given-names>Ramin</given-names></name><role specific-use="author">Author</role><aff><institution>University of Tehran</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Karami</surname><given-names>Behnam</given-names></name><role specific-use="author">Author</role><aff><institution>German Primate Center</institution><addr-line><named-content content-type="city">Gottingen</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Koushki</surname><given-names>Roxana</given-names></name><role specific-use="author">Author</role><aff><institution>Institute for Research in Fundamental Sciences</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Shakerian</surname><given-names>Farideh</given-names></name><role specific-use="author">Author</role><aff><institution>Royan Institute</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Noroozi</surname><given-names>Jalaledin</given-names></name><role specific-use="author">Author</role><aff><institution>Institute for Research in Fundamental Sciences</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Rezayat</surname><given-names>Ehsan</given-names></name><role specific-use="author">Author</role><aff><institution>Department of Cognitive Sciences, Faculty of Psychology and Education, University of Tehran, Tehran, Iran</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Vahabie</surname><given-names>Abdol-Hossein</given-names></name><role specific-use="author">Author</role><aff><institution>Department of Cognitive Sciences, Faculty of Psychology and Education, University of Tehran, Tehran, Iran</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Akhaee</surname><given-names>Mohammad Ali</given-names></name><role specific-use="author">Author</role><aff><institution>University of Tehran</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Dehaqani</surname><given-names>Mohammad-Reza A</given-names></name><role specific-use="author">Author</role><aff><institution>University of Tehran</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>This study reports that spatial frequency representation can predict category coding in the inferior temporal cortex.</p></disp-quote><p>Thank you for taking the time to review our manuscript. We greatly appreciate your valuable feedback and constructive comments, which have been instrumental in improving the quality and clarity of our work.</p><disp-quote content-type="editor-comment"><p>The original conclusion was based on likely problematic stimulus timing (33 ms which was too brief). Now the authors claim that they also have a different set of data on the basis of longer stimulus duration (200 ms).</p><p>One big issue in the original report was that the experiments used a stimulus duration that was too brief and could have weakened the effects of high spatial frequencies and confounded the conclusions. Now the authors provided a new set of data on the basis of a longer stimulus duration and made the claim that the conclusions are unchanged. These new data and the data in the original report were collected at the same time as the authors report.</p><p>The authors may provide an explanation why they performed the same experiments using two stimulus durations and only reported one data set with the brief duration. They may also explain why they opted not to mention in the original report the existence of another data set with a different stimulus duration, which would otherwise have certainly strengthened their main conclusions.</p></disp-quote><p>Thank you for your comments regarding the stimulus duration used in our experiments. We appreciate the opportunity to clarify and provide further details on our methodology and decisions.</p><p>In our original report, we focused on the early phase of the neuronal response, which is less affected by the duration of the stimulus. Observations from our data showed that certain neurons exhibited high firing rates even with the brief 33 ms stimulus duration, and the results we obtained were consistent across different durations. To avoid redundancy, we initially chose not to include the results from the 200 ms stimulus duration, as they reiterated the findings of the 33 ms duration.</p><p>However, we acknowledge that the brief stimulus duration could raise concerns regarding the robustness of our conclusions, particularly concerning the effects of high spatial frequencies. Upon reflecting on the reviewer’s comments during the first revision, we recognized the importance of addressing these potential concerns directly. Therefore, we have included the data from the 200 ms stimulus duration in our revised manuscript.</p><p>Furthermore, Our team is actively investigating the differences between fast (33 ms) and slow (200 ms) presentations in terms of SF processing. Our preliminary observations suggest similar processing of HSF in the early phase of the response for both fast and slow presentations, but different processing of HSF in the late phase. This was another reason we initially opted to publish the results from the brief stimulus duration separately, as we intended to explore the different aspects of SF processing in fast and slow presentations in subsequent studies.</p><disp-quote content-type="editor-comment"><p>I suggest the authors upload both data sets and analyzing codes, so that the claim could be easily examined by interested readers.</p></disp-quote><p>Thank you for your suggestion to make both data sets and the analyzing codes available for examination by interested readers.</p><p>We have created a repository that includes a sample of the dataset along with the necessary codes to output the main results. While we cannot provide the entire dataset at this time due to ongoing investigations by our team, we are committed to ensuring transparency and reproducibility. The data and code samples we have provided should enable interested readers to verify our claims and understand our analysis process.</p><p>Repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/ramintoosi/spatial-frequency-selectivity">https://github.com/ramintoosi/spatial-frequency-selectivity</ext-link></p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This paper aimed to examine the spatial frequency selectivity of macaque inferotemporal (IT) neurons and its relation to category selectivity. The authors suggest in the present study that some IT neurons show a sensitivity for the spatial frequency of scrambled images. Their report suggests a shift in preferred spatial frequency during the response, from low to high spatial frequencies. This agrees with a coarse-to-fine processing strategy, which is in line with multiple studies in the early visual cortex. In addition, they report that the selectivity for faces and objects, relative to scrambled stimuli, depends on the spatial frequency tuning of the neurons.</p><p>Strengths:</p><p>Previous studies using human fMRI and psychophysics studied the contribution of different spatial frequency bands to object recognition, but as pointed out by the authors little is known about the spatial frequency selectivity of single IT neurons. This study addresses this gap and shows spatial frequency selectivity in IT for scrambled stimuli that drive the neurons poorly. They related this weak spatial frequency selectivity to category selectivity, but these findings are premature given the low number of stimuli they employed to assess category selectivity.</p></disp-quote><p>Thank you for your thorough review and insightful feedback on our manuscript. We greatly appreciate your time and effort in providing valuable comments and suggestions, which have significantly contributed to enhancing the quality of our work.</p><disp-quote content-type="editor-comment"><p>The authors revised their manuscript and provided some clarifications regarding their experimental design and data analysis. They responded to most of my comments but I find that some issues were not fully or poorly addressed. The new data they provided confirmed my concern about low responses to their scrambled stimuli. Thus, this paper shows spatial frequency selectivity in IT for scrambled stimuli that drive the neurons poorly (see main comments below). They related this (weak) spatial frequency selectivity to category selectivity, but these findings are premature given the low number of stimuli to assess category selectivity.</p></disp-quote><p>While we acknowledge that the number of instances per condition is relatively low, the overall dataset is substantial. Specifically, our study includes a total of 180 stimuli (6 spatial frequencies × 2 scrambled/non-scrambled conditions × 15 instances, including 9 fixed and 6 non-fixed) and 5400 trials (180 stimuli × 2 durations × 15 repetitions). Conducting these trials requires approximately one hour of experimental time per session.</p><p>Extending the number of stimuli, while potentially addressing this limitation, would significantly compromise the quality of the experiment by increasing the duration and introducing potential fatigue effects in the subjects. Despite this limitation, our findings lay important groundwork by offering novel insights into object recognition through the lens of spatial frequency. We believe this work can serve as a foundation for future experiments designed to further explore and validate these theories with expanded stimulus sets.</p><disp-quote content-type="editor-comment"><p>Main points.</p><p>(1) They have provided now the responses of their neurons in spikes/s and present a distribution of the raw responses in a new Figure. These data suggest that their scrambled stimuli were driving the neurons rather poorly and thus it is unclear how well their findings will generalize to more effective stimuli. Indeed, the mean net firing rate to their scrambled stimuli was very low: about 3 spikes/s. How much can one conclude when the stimuli are driving the recorded neurons that poorly? Also, the new Figure 2- Appendix 1 shows that the mean modulation by spatial frequency is about 2 spikes/s, which is a rather small modulation. Thus, the spatial frequency selectivity the authors describe in this paper is rather small compared to the stimulus selectivity one typically observes in IT (stimulus-driven modulations can be at least 20 spikes/s).</p></disp-quote><p>To address the concerns regarding the firing rates and the modulation of neuronal responses by spatial frequency (SF), we emphasize several key points:</p><p>(1) Significance of Firing Rate Differences: While it is true that the mean net firing rate to our scrambled stimuli was relatively low, the firing rate differences observed were statistically significant, with p-values approximately at 1e-5. This indicates that despite the low firing rates, the observed differences are reliable and unlikely to have occurred by chance.</p><p>(2) Classification Rate and Modulation by SF: Our analysis showed that the difference between various SF responses led to a classification rate of 44.68%, which is 24.68% higher than the chance level. This substantial increase above the chance level demonstrates that SF significantly modulates IT responses, even if the overall firing rates are modest.</p><p>(3) Effect Size and SF Modulation: While the effect size in terms of firing rate differences may be small, it is significant. The significant modulation of IT responses by SF, as evidenced by our statistical analyses and classification rate, supports our conclusions regarding the role of SF in driving IT responses.</p><p>(4) Expectations for Noise-like Pure SF Stimuli: We acknowledge that IT responses are typically higher for various object stimuli. Given the nature of our pure SF stimuli, which resemble noise-like patterns, we did not anticipate high responses in terms of spikes per second. The low firing rates are consistent with the expectation for such stimuli and do not undermine the significance of the observed modulation by SF.</p><p>We believe that these points collectively support the validity of our findings and the significance of SF modulation in IT responses, despite the low firing rates. We appreciate your insights and hope this clarifies our stance on the data and its implications.</p><p>We added the following description to the Appendix 1 - “Strength of SF selectivity” section:</p><p>“While the firing rates and net responses to scrambled stimuli were modest (e.g., 2.9 Hz in T1), the differences across spatial frequency (SF) bands were statistically significant (p ≈ 1e-5) and led to a classification accuracy 24.68\% above chance. This demonstrates the robustness of SF modulation in IT neurons despite low firing rates. The modest responses align with expectations for noise-like stimuli, which are less effective in driving IT neurons, yet the observed SF selectivity highlights a fundamental property of IT encoding.”</p><disp-quote content-type="editor-comment"><p>(2) Their new Figure 2-Appendix 1 does not show net firing rates (baseline-subtracted; as I requested) and thus is not very informative. Please provide distributions of net responses so that the readers can evaluate the responses to the stimuli of the recorded neurons.</p></disp-quote><p>We understand the reviewer’s concern about the presentation of net firing rates. In T2 (the late time interval), the average response rate falls below the baseline, resulting in negative net firing rates, which might confuse readers. To address this, we have added the net responses to the text for clarity. Additionally, we have included the average baseline response in the figure to provide a more comprehensive view of the data.</p><p>“To check the SF response strength, the histogram of IT neuron responses to scrambled, face, and non-face stimuli is illustrated in this figure. A Gamma distribution is also fitted to each histogram. To calculate the histogram, the neuron response to each unique stimulus is calculated for each neuron in spike/seconds (Hz). In the early phase, T1, the average firing rate to scrambled stimuli is 26.3 Hz which is significantly higher than the response in -50 to 50ms which is 23.4 Hz. In comparison, the mean response to intact face stimuli is 30.5 Hz, while non-face stimuli elicit an average response of 28.8 Hz. The average net responses to the scrambled, face, and non-face stimuli are 2.9 Hz, 7.1 Hz, and 5.4 Hz, respectively. Moving to the late phase, T2, the responses to scrambled, face, and object stimuli are 19.5 Hz, 19.4 Hz, and 22.4 Hz, respectively. The corresponding average net responses are 3.9 Hz, 4.0 Hz, and 1.0 Hz below the baseline response.”</p><disp-quote content-type="editor-comment"><p>(3) The poor responses might be due to the short stimulus duration. The authors report now new data using a 200 ms duration which supported their classification and latency data obtained with their brief duration. It would be very informative if the authors could also provide the mean net responses for the 200 ms durations to their stimuli. Were these responses as low as those for the brief duration? If so, the concern of generalization to effective stimuli that drive IT neurons well remains.</p></disp-quote><p>The firing rates for the 200 ms stimulus duration are as follows: 27.7 Hz, 30.7 Hz, and 30.4 Hz for scrambled, face, and object stimuli in T1, respectively; and 26.2 Hz, 29.1 Hz, and 33.9 Hz in T2. The average baseline firing rate (−50 to 50 ms) is 23.4 Hz. Therefore, the net responses are 4.3 Hz, 7.3 Hz, and 7.0 Hz for T1; and 2.8 Hz, 5.7 Hz, and 10.5 Hz for T2 for scrambled, face, and object stimuli, respectively.</p><p>Notably, the impact of stimulus duration is more pronounced in T2, which is consistent with the time interval of the T2 compared to T1. However, the firing rates in T1 do not show substantial changes with the longer duration. As we discussed in our response to the first comment, it is important to note that high net responses are not typically expected for scrambled or noise-like stimuli in IT neurons. Instead, the key findings of this study lie in the statistical significance of these responses and their meaningful relationship to category selectivity. These results highlight the broader implications for understanding the role of spatial frequency in object recognition.</p><p>We added the firing rates to the, Appendix 1, “Extended stimulus duration supports LSF-preferred tuning” part as follows.</p><p>“For the 200 ms stimulus duration, the firing rates were 27.7 Hz, 30.7 Hz, and 30.4 Hz for scrambled, face, and object stimuli in T1, respectively, and 26.2 Hz, 29.1 Hz, and 33.9 Hz in T2. The corresponding net responses were 4.3 Hz, 7.3 Hz, and 7.0 Hz in T1, and 2.8 Hz, 5.7 Hz, and 10.5 Hz in T2. While the longer stimulus duration did not substantially increase firing rates in T1, its impact was more pronounced in T2.”</p><disp-quote content-type="editor-comment"><p>(4) I still do not understand why the analyses of Figures 3 and 4 provide different outcomes on the relationship between spatial frequency and category selectivity. I believe they refer to this finding in the Discussion: &quot;Our results show a direct relationship between the population's category coding capability and the SF coding capability of individual neurons. While we observed a relation between SF and category coding, we have found uncorrelated representations. Unlike category coding, SF relies more on sparse, individual neuron representations.&quot;. I believe more clarification is necessary regarding the analyses of Figures 3 and 4, and why they can show different outcomes.</p></disp-quote><p>Figure 3 explores the relationship between SF coding and category coding at both the single-neuron and population levels.</p><p>● Figures 3(a) and 3(b) examine the relationship between a single neuron’s response pattern and object decoding in the population.</p><p>● Figure 3(c) investigates the relationship between a single neuron’s SF decoding capabilities and object decoding in the population.</p><p>● Figure 3(d) assesses the relationship between a single neuron’s object decoding capabilities and SF decoding in the population.</p><p>In summary, Figure 3 demonstrates a relation between SF coding/response pattern at the single level and category coding at the population level.</p><p>Figure 4, on the other hand, addresses the uncorrelated nature of SF and category coding.</p><p>● Figure 4(a) shows the uncorrelated relation between a single neuron’s SF decoding capability and its object decoding capability. This suggests that a neuron's ability to decode SF does not predict its ability to decode object categories.</p><p>● Figure 4(b) illustrates that the contribution of a neuron to the population decoding of SF is uncorrelated with its contribution to the population decoding of object categories. This further supports the idea that the mechanisms behind SF coding and object coding are uncorrelated.</p><p>In summary, Figure 4 suggests that while there is a relation between SF coding and category coding as illustrated in Figure 3, the mechanisms underlying SF coding and object coding operate independently (in terms of correlation), highlighting the distinct nature of these processes.</p><p>We hope this explanation clarifies why the analyses in Figures 3 and 4 present different outcomes. Figure 3 provides insight into the relationship between SF and category coding, while Figure 4 emphasizes the uncorrelated nature of these processes. We also added the following explanation in the “Uncorrelated mechanisms for SF and category coding” section.</p><p>Based on your command, to clarify the presentation of the work, we added the following description to the “Uncorrelated mechanisms for SF and category coding” section:</p><p>“Figures 3 and 4 examine different aspects of the relationship between SF and category coding. Figure 3 highlights a relationship between SF coding at the single-neuron level and category coding at the population level. Conversely, Figure 4 demonstrates the uncorrelated mechanisms underlying SF and category coding, showing that a neuron’s ability to decode SF is not predictive of its ability to decode object categories. This distinction underscores that while SF and category coding are related at broader levels, their underlying mechanisms are independent, emphasizing the distinct processes driving each form of coding.”</p><disp-quote content-type="editor-comment"><p>(5) The authors found a higher separability for faces (versus scrambled patterns) for neurons preferring high spatial frequencies. This is consistent for the two monkeys but we are dealing here with a small amount of neurons. Only 6% of their neurons (16 neurons) belonged to this high spatial frequency group when pooling the two monkeys. Thus, although both monkeys show this effect I wonder how robust it is given the small number of neurons per monkey that belong to this spatial frequency profile. Furthermore, the higher separability for faces for the low-frequency profiles is not consistent across monkeys which should be pointed out.</p></disp-quote><p>We appreciate the reviewer’s concern regarding the relatively small number of neurons in the high spatial frequency group (16 neurons, 6% of the total sample across the two monkeys) and the consistency of the results. While we acknowledge this limitation, it is important to note that findings involving sparse subsets of neurons can still be meaningful. For example, Dalgleish et al. (2020) demonstrated that perception can arise from the activity of as few as ~14 neurons in the mouse cortex, supporting the sparse coding hypothesis. This underscores the potential robustness of results derived from small neuronal populations when the activity is statistically significant and functionally relevant.</p><p>Regarding the higher separability for faces among neurons preferring high spatial frequencies, the consistency of this finding across both monkeys suggests that this effect is robust within this subgroup. For neurons preferring low spatial frequencies, we agree that the lack of consistency across monkeys should be explicitly noted. These differences may reflect individual variability or differences in sampling across subjects and merit further investigation in future studies.</p><p>To address this concern, we have updated the text to explicitly discuss the small size of the high spatial frequency group, its implications, and the observed inconsistency in the low spatial frequency profiles between monkeys. We have added the following description to the discussion.</p><p>“Next, according to Figure 3(a), 6% of the neurons are HSF-preferred and their firing rate in HSF is comparable to the LSF firing rate in the LSF-preferred group. This analysis is carried out in the early phase of the response (70-170ms). While most of the neurons prefer LSF, this observation shows that there is an HSF input that excites a small group of neurons. Importantly, findings involving small neuronal populations can still be meaningful, as studies like Dalgleish et al. (2020) have demonstrated that perception can arise from the activity of as few as ~14 neurons in the mouse cortex, emphasizing the robustness of sparse coding.”</p><p>Regarding the separability of faces for the low-frequency profiles, we added the following to the appendix section,</p><p>“For neurons preferring LSF, LP profile, it is important to note the lack of consistency in responses across monkeys. This variability may reflect individual differences in neural processing or variations in sampling between subjects.”</p><p>And in the discussion:</p><p>“Our results are based on grouping the neurons of the two monkeys; however, the results remain consistent when looking at the data from individual monkeys as illustrated in Appendix 2. However, for neurons preferring LSF, we observed inconsistency across monkeys, which may reflect individual differences or sampling variability. These findings highlight the complexity of SF processing in the IT cortex and suggest the need for further research to explore these variations.”</p><p>* Henry WP Dalgleish, Lloyd E Russel, lAdam M Packer, Arnd Roth, Oliver M Gauld, Francesca Greenstreet, Emmett J Thompson, Michael Häusser (2020) How many neurons are sufficient for perception of cortical activity? eLife 9:e58889.</p><disp-quote content-type="editor-comment"><p>(6) I agree that CNNs are useful models for ventral stream processing but that is not relevant to the point I was making before regarding the comparison of the classification scores between neurons and the model. Because the number of features and trial-to-trial variability differs between neural nets and neurons, the classification scores are difficult to compare. One can compare the trends but not the raw classification scores between CNN and neurons without equating these variables.</p></disp-quote><p>We appreciate the reviewer’s follow-up comment and agree that differences in the number of features and trial-to-trial variability between IT neurons and CNN units make direct comparisons of raw classification scores challenging. As the reviewer suggests, it is more appropriate to focus on comparing trends rather than absolute scores when analyzing the similarities and differences between these systems. In light of this, we have revised the text to clarify that our intention was not to equate raw classification scores but to highlight the qualitative patterns and trends observed in spatial frequency encoding between IT and CNN units.</p><p>“SF representation in the artificial neural networks</p><p>We conducted a thorough analysis to compare our findings with CNNs. To assess the SF coding capabilities and trends of CNNs, we utilized popular architectures, including ResNet18, ResNet34, VGG11, VGG16, InceptionV3, EfficientNetb0, CORNet-S, CORTNet-RT, and CORNet-z, with both pre-trained on ImageNet and randomly initialized weights. Employing feature maps from the four last layers of each CNN, we trained an LDA model to classify the SF content of input images. Figure 5(a) shows the SF decoding accuracy of the CNNs on our dataset (SF decoding accuracy with random (R) and pre-trained (P) weights, ResNet18: P=0.96±0.01 / R=0.94±0.01, ResNet34 P=0.95±0.01 / R=0.86±0.01, VGG11: P=0.94±0.01 / R=0.93±0.01, VGG16: P=0.92±0.02 / R=0.90±0.02, InceptionV3: P=0.89±0.01 / R=0.67±0.03, EfficientNetb0: P=0.94±0.01 / R=0.30±0.01, CORNet-S: P=0.77±0.02 / R=0.36±0.02, CORTNet-RT: P=0.31±0.02 / R=0.33±0.02, and CORNet-z: P=0.94±0.01 / R=0.97±0.01). Except for CORNet-z, object recognition training increases the network's capacity for SF coding, with an improvement as significant as 64\% in EfficientNetb0. Furthermore, except for the CORNet family, LSF content exhibits higher recall values than HSF content, as observed in the IT cortex (p-value with random (R) and pre-trained (P) weights, ResNet18: P=0.39 / R=0.06, ResNet34 P=0.01 / R=0.01, VGG11: P=0.13 / R=0.07, VGG16: P=0.03 / R=0.05, InceptionV3: P=&lt;0.001 / R=0.05, EfficientNetb0: P=0.07 / R=0.01). The recall values of CORNet-Z and ResNet18 are illustrated in Figure 5(b). However, while the CNNs exhibited some similarities in SF representation with the IT cortex, they did not replicate the SF-based profiles that predict neuron category selectivity. As depicted in Figure 5(c) although neurons formed similar profiles, these profiles were not associated with the category decoding performances of the neurons sharing the same profile.”</p><p>Discussion:</p><p>“Finally, we compared SF's representation trends and findings within the IT cortex and the current state-of-the-art networks in deep neural networks.”</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>The mean baseline firing rate of their neurons (23.4 Hz) was rather high for single IT neurons (typically around 10 spikes/s or lower). Were these well-isolated units or mainly multiunit activity?</p></disp-quote><p>We confirm that the recordings in our study were from both well-isolated single units and multi-unit activities (remaining after isolation neurons) sorted based on our spike sorting toolbox. The higher baseline firing rate is likely due to the experimental design, particularly the inclusion of the responsive neurons from the selectivity phase. We added the following statement to the methods section.</p><p>“In our analysis, we utilized both well-isolated single units and multi-unit activities (which represent neural activities that could not be further sorted into single units), ensuring a comprehensive representation of neural responses across the recorded population.”</p></body></sub-article></article>