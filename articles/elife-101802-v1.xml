<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">101802</article-id><article-id pub-id-type="doi">10.7554/eLife.101802</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101802.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Statistical learning beyond words in human neonates</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Fló</surname><given-names>Ana</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3260-0559</contrib-id><email>ana.flo@unipd.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Benjamin</surname><given-names>Lucas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9578-6039</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Palu</surname><given-names>Marie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xjwb503</institution-id><institution>Cognitive Neuroimaging Unit, CNRS ERL 9003, INSERM U992, CEA, Université Paris Saclay, NeuroSpin center</institution></institution-wrap><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00240q980</institution-id><institution>Department of Developmental Psychology and Socialisation and Department of Neuroscience, University of Padova</institution></institution-wrap><addr-line><named-content content-type="city">Padova</named-content></addr-line><country>Italy</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>Departement d’étude Cognitives, École Normale Supérieure</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>Aix Marseille Univ, INSERM, INS, Inst Neurosci syst</institution></institution-wrap><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Herrmann</surname><given-names>Björn</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03gp5b411</institution-id><institution>Baycrest Hospital</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>17</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP101802</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-08-19"><day>19</day><month>08</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-07-26"><day>26</day><month>07</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.26.605295"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-29"><day>29</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101802.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-14"><day>14</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101802.2"/></event></pub-history><permissions><copyright-statement>© 2024, Fló et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Fló et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-101802-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-101802-figures-v1.pdf"/><abstract><p>Interest in statistical learning in developmental studies stems from the observation that 8-month-olds were able to extract words from a monotone speech stream solely using the transition probabilities (TP) between syllables (Saffran et al., 1996). A simple mechanism was thus part of the human infant’s toolbox for discovering regularities in language. Since this seminal study, observations on statistical learning capabilities have multiplied across domains and species, challenging the hypothesis of a dedicated mechanism for language acquisition. Here, we leverage the two dimensions conveyed by speech –speaker identity and phonemes– to examine (1) whether neonates can compute TPs on one dimension despite irrelevant variation on the other and (2) whether the linguistic dimension enjoys an advantage over the voice dimension. In two experiments, we exposed neonates to artificial speech streams constructed by concatenating syllables while recording EEG. The sequence had a statistical structure based either on the phonetic content, while the voices varied randomly (Experiment 1) or on voices with random phonetic content (Experiment 2). After familiarisation, neonates heard isolated duplets adhering, or not, to the structure they were familiarised with. In both experiments, we observed neural entrainment at the frequency of the regularity and distinct Event-Related Potentials (ERP) to correct and incorrect duplets, highlighting the universality of statistical learning mechanisms and suggesting it operates on virtually any dimension the input is factorised. However, only linguistic duplets elicited a specific ERP component, potentially an N400 precursor, suggesting a lexical stage triggered by phonetic regularities already at birth. These results show that, from birth, multiple input regularities can be processed in parallel and feed different higher-order networks.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Imagine listening to a language you don't know. When does one word end, and another begin? Human infants face a similar challenge, yet remarkably, they grasp the structure of their mother tongue naturally without receiving any explicit indications. By six months, they recognize some common nouns, and by one year, they start saying their first words. This learning begins from birth, with newborns already sensitive to speech patterns.</p><p>Previous studies have shown that the likelihood of certain syllables appearing after others allows infants to detect regularity and separate speech into chunks. This is because some syllables are more predictive of what comes next than others. For example, in English, many different syllables can follow ‘the’. However, it is highly likely that ‘brocco’ will be followed by ‘li’. The ability to detect these regularities is known as statistical learning. However, whether this relies on a general mechanism or is restricted to a specific speech component, such as the sequence of syllables, remained unknown.</p><p>To investigate, Fló et al. measured brain electrical activity of newborns up to 4 days old in response to speech specifically designed to contain certain patterns of syllables or voices. In one experiment, the speech had regular patterns in the syllables, while in a second experiment, the pattern was in the voices, and each voice could utter each syllable. Unlike tracking syllable variation, which can help with learning words, voice changes within a word are unnatural and predicting them is not relevant to real-life speech processing. Therefore, if statistical learning in speech is shaped to promote language acquisition, learning should be restricted to syllable patterns. Instead, if statistical learning is a general mechanism, newborns should also detect the patterns in voice.</p><p>Analysis revealed that newborns were equally capable of discerning regular patterns in syllables despite voice changes and in voices disregarding the syllable that was pronounced. This suggests that statistical learning is a general learning mechanism that can operate across multiple features. Additionally, pseudo-words (those which resemble a real world but don’t exist in the language) were presented to the newborns after they had been familiarised with speech containing either similar syllable or voice patterns. The researchers observed a specific neural response to the pseudowords only when related to syllable patterns. This neural component suggests that only syllabic structures are considered word candidates and processed by a dedicated neural network from birth.</p><p>Taken together, the findings of Fló et al. reveal insights into how humans process speech when experience with language is minimal, suggesting that statistical learning may have a broader role in early language acquisition that previously thought.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>neonates</kwd><kwd>statistical learning</kwd><kwd>neural entrainment</kwd><kwd>language</kwd><kwd>speech</kwd><kwd>ERP</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>695710</award-id><principal-award-recipient><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Human neonates process regularities in speech's phonetic and voice content in parallel, but only phonetic regularities evoke a specific ERP component in a post-learning phase.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Since speech is a continuous signal, one of the infants' first challenges during language acquisition is to break it down into smaller units, notably to be able to extract words. Parsing has been shown to rely on prosodic cues (e.g. pitch and duration changes) but also on identifying regular patterns across perceptual units. Nearly 20 years ago, <xref ref-type="bibr" rid="bib54">Saffran et al., 1996</xref> demonstrated that infants are sensitive to local regularities, specifically Transitional Probabilities (TP) between syllables—that is, the probability that two particular syllables follow each other in a given sequence, TP = P(S<sub>i+1</sub>|S<sub>i</sub>). In their study, 8-month-old infants were exposed to a continuous, monotonous stream of syllables composed of four randomly concatenated tri-syllabic pseudo-words. Within each word, the sequence of syllables was fixed, resulting in a TP of 1 for each syllable pair within a word. By contrast, because each word could be followed by any of the three other words, the TP for syllable pairs spanning word boundaries dropped to 1/3. The authors found that, after only 2 min of exposure to this stream, infants could distinguish between the original ‘words’ and ‘part-words’—syllable triplets, including the between-word TP drop. Since this seminal study, statistical learning has been regarded as an essential mechanism for language acquisition because it allows for the extraction of regular patterns without prior knowledge.</p><p>During the last two decades, many studies have extended this finding by demonstrating sensitivity to statistical regularities in sequences across domains and species. For example, segmentation capacities analogous to those observed for a syllable stream are observed throughout life in the auditory modality for tones (<xref ref-type="bibr" rid="bib41">Kudo et al., 2011</xref>; <xref ref-type="bibr" rid="bib55">Saffran et al., 1999</xref>) and in the visual domain for shapes (<xref ref-type="bibr" rid="bib13">Bulf et al., 2011</xref>; <xref ref-type="bibr" rid="bib24">Fiser and Aslin, 2002</xref>; <xref ref-type="bibr" rid="bib40">Kirkham et al., 2002</xref>) and actions (<xref ref-type="bibr" rid="bib3">Baldwin et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Monroy et al., 2017</xref>). Non-human animals, including cotton-top tamarins (<xref ref-type="bibr" rid="bib34">Hauser et al., 2001</xref>), rats (<xref ref-type="bibr" rid="bib63">Toro and Trobalón, 2005</xref>), dogs (<xref ref-type="bibr" rid="bib11">Boros et al., 2021</xref>), and chicks (<xref ref-type="bibr" rid="bib58">Santolin et al., 2016</xref>), have also been shown to be sensitive to TPs between successive events. While the level of complexity that each species can track might differ, statistical learning appears as a general learning mechanism for auditory and visual sequence processing (for a review of statistical learning capacities across species, see <xref ref-type="bibr" rid="bib59">Santolin and Saffran, 2018</xref>).</p><p>Using near-infra-red spectroscopy (NIRS) and electroencephalography (EEG), we have shown that statistical learning is observed in sleeping neonates (<xref ref-type="bibr" rid="bib26">Fló et al., 2022a</xref>; <xref ref-type="bibr" rid="bib25">Fló et al., 2019</xref>), highlighting the automaticity of this mechanism. We also discovered that tracking statistical probabilities might not lead to stream segmentation in the case of quadrisyllabic words in both neonates and adults, revealing an unsuspected limitation of this mechanism (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2023</xref>). Here, we aimed to further characterise this mechanism to shed light on its role in the early stages of language acquisition. Specifically, we addressed two questions: First, we investigated whether statistical learning in human neonates is a general learning mechanism applicable to any speech feature or whether there is a bias in favour of computations on linguistic content to extract words. Second, we explored the level at which newborns compute transitions between syllables, at a low auditory level (i.e. between the presented events) or at a later phonetic level, after normalisation through irrelevant dimensions such as voices.</p><p>To test this, we have taken advantage of the fact that syllables convey two important pieces of information for humans: what is being said and who is speaking, that is linguistic content and speaker’s identity. While statistical learning can be helpful to word extraction, a statistical relationship between successive voices is not of obvious use and could even hinder word extraction if instances of a word uttered by a different speaker are considered independently. However, as auditory processing is organised along several hierarchical and parallel pathways integrating different spectro-temporal dimensions (<xref ref-type="bibr" rid="bib4">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="bib20">DeWitt and Rauschecker, 2012</xref>; <xref ref-type="bibr" rid="bib48">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Zatorre and Belin, 2001</xref>), statistical learning might be computed on one dimension independently of the variation of the other along the linguistic and the voice pathways in parallel. Numerous behavioural and brain imaging studies have revealed phonetic normalisation across speakers in infants (<xref ref-type="bibr" rid="bib16">Dehaene-Lambertz and Pena, 2001</xref>; <xref ref-type="bibr" rid="bib31">Gennari et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Kuhl and Miller, 1982</xref>). By the second half of the first year, statistical learning has also been shown to occur even when different voices are used (<xref ref-type="bibr" rid="bib23">Estes and Lew-Williams, 2015</xref>) or when natural speech is presented, where syllable production can vary from instance to instance (<xref ref-type="bibr" rid="bib35">Hay et al., 2011</xref>; <xref ref-type="bibr" rid="bib51">Pelucchi et al., 2009</xref>). Therefore, we hypothesised that neonates would compute TPs between syllables even when each syllable is produced by a different speaker, relying on a normalisation process at the syllable or phonetic level. However, our predictions regarding TPs learning across different voices were more open-ended. Either statistical learning is universal and can be similarly computed over any feature comprising voices, or listening to a speech stream favours the processing of phonetic regularities over other non-linguistic dimensions of speech and thus hinders the possibility of computing regularities over voices.</p><p>To study these possibilities, we constructed artificial streams using six consonant-vowel (CV) syllables produced by six voices (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>, <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>), resulting in 36 possible tokens (6 syllables ×6 voices). To form the streams, tokens were combined either by imposing structure to their phonetic content (Experiment 1: Structure over Phonemes) or their voice content (Experiment 2: Structure over Voices), while the second dimension varied randomly (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The structure consisted of the random concatenation of three duplets (i.e. two-syllable units) defined only by one of the two dimensions. For example, in Experiment 1, one duplet could be <italic>petu</italic> with each syllable uttered by a random voice each time they appear in the stream (e.g. <italic>pe</italic> is produced by voice<sup>1</sup> and <italic>tu</italic> by voice<sup>6</sup> in one instance and in another instance <italic>pe</italic> is produced by voice<sup>3</sup> and <italic>tu</italic> by voice<sup>2</sup>). In contrast, in Experiment 2, one duplet could be the combination [voice<sup>1</sup>- voice<sup>6</sup>], each uttering randomly any of the syllables.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental protocol.</title><p>The experiments started with a Random stream (120 s) in which both syllables and voices changed randomly, followed by a long-Structured stream (120 s). Then, 10 short familiarisation streams (30 s), each followed by test blocks comprising 18 isolated duplets (SOA 2–2.3 s) were presented. Example streams are presented to illustrate the construction of the streams, with different colours representing different voices. In Experiment 1, the Structured stream had a statistical structure based on phonemes (TPs alternated between 1 and 0.5), while the voices were randomly changing (uniform TPs of 0.2). For example, the two syllables of the word ‘<italic>petu’</italic> were produced by different voices, which randomly changed at each presentation of the word (e.g. ‘<italic>yellow’</italic> voice and ‘<italic>green’</italic> voice for the first instance, ‘<italic>blue’</italic> and ‘<italic>purple’</italic> voice for the second instance, etc..). In Experiment 2, the statistical structure was based on voices (TPs alternated between 1 and 0.5), while the syllables changed randomly (uniform TPs of 0.2). For example, the ‘<italic>green’</italic> voice was always followed by the ‘<italic>red’</italic> voice, but they were randomly saying different syllables ‘<italic>boda’</italic> in the first instance, ‘<italic>tupe’</italic> in the second instance, etc... The test duplets in the recognition test phase were either Words (TP = 1) or Partwords (TP = 0.5). Words and Partwords were defined in terms of phonetic content for Experiment 1 and voice content for Experiment 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig1-v1.tif"/></fig><p>If infants at birth compute regularities based on a neural representation of the syllable as a whole, that is comprising both phonetic and voice content, this would require computing a 36×36 TPs matrix relating each token. Under this computation, TPs in the structured stream would alternate between 1/6 within words and 1/12 between words. We predicted infants would fail the task in both experiments as previous studies showing successful segmentation in infants commonly use higher within-word TPs (usually 1) and far fewer tokens (typically 4–12) (<xref ref-type="bibr" rid="bib56">Saffran and Kirkham, 2018</xref>). By contrast, if speech input is processed along the two studied dimensions in distinct pathways, it enables the calculation of two independent TP matrices of 6×6 between the six voices in a voice pathway and between the six syllables in a phonetic pathway. These computations would result in TPs alternating between 1 and 1/2 for the informative feature while remining uniform at 1/5 for the uninformative feature, leading to stream segmentation based on the informative dimension.</p><p>As in our previous experiments (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2023</xref>; <xref ref-type="bibr" rid="bib27">Fló et al., 2022b</xref>), we used high-density EEG (128 electrodes) to study speech segmentation abilities. Using artificial language with syllables with a fixed duration elicits Steady State Evoked Potentials (SSEP) at the syllable rate. Crucially, if the artificial language presents a regular structure (i.e. regular drops in TPs marking word boundaries) and if the structure is perceived, then the neural responses reflect the slower frequency of the word as well (<xref ref-type="bibr" rid="bib12">Buiatti et al., 2009</xref>). In other words, the brain activity becomes phase-locked to the regular input, increasing the Inter Trial Coherence (ITC) and power at the input regularity frequencies. Under these circumstances, the analysis in the frequency domain is advantageous since fast and periodic responses can be easily investigated by looking at the target frequencies without considering their specific timing (<xref ref-type="bibr" rid="bib39">Kabdebon et al., 2022</xref>). The phenomenon is also named frequency tagging or neural entrainment in the literature. Here, we will refer to it indistinctively as SSEP or neural entrainment since we do not aim to make any hypothesis on the origin of the response (i.e. pure evoked response or phase reset of endogenous oscillations, <xref ref-type="bibr" rid="bib32">Giraud and Poeppel, 2012</xref>).</p><p>Our study used an orthogonal design across two groups of 1- to 4-day-old neonates. In Experiment 1 (34 infants), the regularities in the speech stream were based on the phonetic content, while the voices varied randomly (Phoneme group). Conversely, in Experiment 2 (33 infants), regularities were based on voices, while the phonemes changed randomly (Voice group). Both experiments started with a control stream in which both features varied randomly (i.e. Random stream, 120 s). Next, neonates were exposed to the Structured stream (120 s) with statistical structure over one or the other feature. The experiments ended with ten sets of 18 test duplets presented in isolation, preceded by short Structured streams (30 s) to maintain learning (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Half of the test duplets corresponded to familiar regularities (Words, TP = 1), and the other half were duplets present in the stream but which straddled a drop in TPs (Partwords, TP = 0.5).</p><p>To investigate online learning, we quantified the ITC as a measure of neural entrainment at the syllable (4 Hz) and word rate (2 Hz) during the presentation of the continuous streams. For the recognition process, we compared ERPs to Word and Part-Word duplets. We also tested 57 adult participants in a comparable behavioural experiment to investigate adults’ segmentation capacities under the same conditions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Neural entrainment during the familiarisation phase</title><p>To measure neural entrainment, we quantified the ITC in non-overlapping epochs of 7.5 s. We compared the studied frequency (syllabic rate 4 Hz or duplet rate 2 Hz) with the 12 adjacent frequency bins following the same methodology as in our previous studies.</p><p>For the Random streams, we observed significant entrainment at syllable rate (4 Hz) over a broad set of electrodes in both experiments (p&lt;0.05, FDR corrected) and no enhanced activity at the duplet rate for any electrode (p&gt;0.05, FDR corrected). Concerning the Structured streams, ITC increased at both the syllable and duplet rate (p&lt;0.05, FDR corrected) in both experiments (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). The duplet effect was localised over occipital and central-left electrodes in the Phoneme group and over occipital and temporal-right electrodes in the Voice group.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural entrainment during the random and structured streams.</title><p>(<bold>A</bold>) SNR for the ITC during the Random and Structured streams of Experiment 1 (structure on phonetic content) (n=32). The topographies represent the entrainment in the electrode space at the syllabic (4 Hz) and duplet rates (2 Hz). Crosses indicate the electrodes showing enhanced neural entrainment (cross: p&lt;0.05, one-sided paired permutation test, FDR corrected by the number of electrodes; dot: p&lt;0.05, without FDR correction). Colour scale limits [–1.8, 1.8]. The entrainment for each electrode is shown in light grey. The thick black line shows the mean over the electrodes with significant entrainment relative to the adjacent frequency bins at the syllabic rate (4 Hz) (p&lt;0.05 FDR corrected). The thick green line shows the mean over the electrodes showing significant entrainment relative to the adjacent frequency bins at the duplet rate (2 Hz) (p&lt;0.05 FDR corrected). The asterisks indicate frequency bins with entrainment significantly higher than on adjacent frequency bins for the average across electrodes (p&lt;0.05, one-sided permutation test, FDR corrected for the number of frequency bins). (<bold>B</bold>) Analog to A for Experiment 2 (structure on voice content) (n=32). (<bold>C</bold>) The first two rows show the topographies for the difference in entrainment during the Structured and Random streams at 4 Hz and 2 Hz for both experiments. Crosses indicate the electrodes showing stronger entrainment during the Structured stream (cross: p&lt;0.05, one-sided paired permutation test, FDR corrected by the number of electrodes; dot: p&lt;0.05, without FDR correction). The bottom row shows the interaction effect by comparing the difference in entrainment during the Structured and Random streams between Experiments 1 and 2. Crosses indicate significant differences (cross: p&lt;0.05, two-sided unpaired permutation test, FDR corrected by the number of electrodes; dot: p&lt;0.05, without FDR correction). (<bold>D</bold>) Time course of the neural entrainment at 4 Hz for the average over electrodes showing significant entrainment during the Random stream and at 2 Hz for the average over electrodes showing significant entrainment during the Structured stream (Phoneme: green line, Voice blue line). The shaded area represents standard errors. The horizontal lines on the bottom indicate when the entrainment was larger than 0 (p&lt;0.05, one-sided t-test, corrected by FDR by the number of time points).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig2-v1.tif"/></fig><p>We also directly compared the ITC at both frequencies of interest between the Random and Structured conditions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We found electrodes with significantly higher ITC at the duplet rate during Structured streams than Random streams in both experiments (p&lt;0.05, FDR corrected). We also found electrodes with higher entrainment at syllable rate during the Structured than Random streams in both experiments (p&lt;0.05, FDR corrected). This effect might result from stronger or more phase-locked responses to syllables when the input is structured. Since the first harmonic of the duplet rate (2x2 Hz) coincides with the syllable rate (4 Hz), word entrainment during the structured streams could also contribute to this effect. However, this contribution is unlikely since the electrodes showing higher 4 Hz entrainment during Structured than Random streams differ from those showing duplet-rate activity at 2 Hz.</p><p>Finally, we looked for an interaction effect between groups and conditions (Structured vs. Random streams; <xref ref-type="fig" rid="fig2">Figure 2C</xref>). A few electrodes show differential responses between groups, reflecting the topographical differences observed in the previous analysis, notably the trend for stronger ITC at 2 Hz over the left central electrodes for the Phoneme group compared to the Voice group, but none survive multiple comparison corrections.</p><sec id="s2-1-1"><title>Learning time-course</title><p>To investigate the time course of the learning, we computed neural entrainment at the duplet rate in sliding time windows of 2 min with a 1 s step across both random and structured streams (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Notice that because the integration window was 2 min long, the entrainment during the first minute of the Structured stream included data from the random stream. To test whether ITC at 2 Hz increased during long Structured familiarisation (120 s), we fitted a Linear Mixed Model (LMM) with a fixed effect of time and random slopes and interceptions for individual subjects: <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∼</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the Phoneme group, we found a significant time effect (<italic>β</italic>=4.16 × 10<sup>–3</sup>, 95% CI=[2.06×10<sup>–3</sup>, 6.29×10<sup>–3</sup>], SE = 1.05 × 10<sup>–3</sup>, p=4 × 10<sup>–4</sup>), as well as in the Voice group (<italic>β</italic>=2.46 × 10<sup>–3</sup>, 95% CI=[2.6×10<sup>–4</sup>, 4.66×10<sup>–3</sup>], SE = 1.09 × 10<sup>–3</sup>, p=0.03). To test for differences in the time effect between groups, we included all data in a single LMM: <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∼</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>∗</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The model showed a significant fixed effect of time for the Phoneme group consistent with the previous results (<italic>β</italic>=4.22 × 10<sup>–3</sup>, 95% CI=[1.07×10<sup>–3</sup>, 7.37×10<sup>–3</sup>], SE = 1.58 × 10<sup>–3</sup>, p=0.0096), while the fixed effect estimating the difference between the Phoneme and Voice groups was not significant (<italic>β</italic>=−1.84 × 10<sup>–3</sup>, 95% CI=[–6.29×10<sup>–3</sup>, 2.61×10<sup>–3</sup>], SE = 2.24 × 10<sup>–3</sup>, p=0.4).</p></sec></sec><sec id="s2-2"><title>ERPs during the test phase</title><p>To test the recognition process, we also measured ERP to isolated duplets afterwards. The average ERP to all conditions merged is shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. We investigated (1) the main effect of test duplets (Word vs. Part-word) across both experiments, (2) the main effect of familiarisation structure (Phoneme group vs. Voice group), and finally (3) the interaction between these two factors. We used non-parametric cluster-based permutation analyses (i.e. without a priori ROIs; <xref ref-type="bibr" rid="bib49">Oostenveld et al., 2011</xref>).</p><p>The difference between Word and Part-word consisted of a dipole with a median positivity and a left temporal negativity ranging from 400 to 1500ms, with a maximum around 800–900ms (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Cluster-based permutations recovered two significant clusters around 500–1500ms: a frontal-right positive cluster (p=0.019) and a left temporal negative cluster (p=0.0056). A difference between groups was observed consisting of a dipole that started with a right temporal positivity left temporo-occipital negativity around 300ms and rotated anti-clockwise to bring the positivity over the frontal electrodes and the negativity at the back of the head (500–800ms; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Cluster-based permutations on the Phoneme group vs. Voice group recovered a posterior cluster (p=0.018) around 500ms; with no positive cluster reaching significance (p&gt;0.10). A cluster-based permutation analysis on the interaction effect, that is comparing Words - Part-Words between both experiments, showed no significant clusters (p&gt;0.1).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cluster-based permutation analysis of ERPs to isolated duplets during recognition The topographies show the difference between the two conditions corresponding to each main effect.</title><p>Results obtained from the cluster-based permutation analyses are shown at the bottom of each panel. Thick lines correspond to the grand averages for the two main tested conditions. Shaded areas correspond to the standard error across participants. Thin lines show the ERPs separated by duplet type and familiarisation type. The shaded areas between the thick lines show the time extension of the cluster. The topographies correspond to the difference between conditions during the time extension of the cluster. The electrodes belonging to the cluster are marked with a cross. Significant clusters are indicated with an asterisk. Color scale limits [–0.07, 0.07] a.u. (A) Main effect of Test-duplets (Words - Part-words) over a frontal-right positive cluster (p=0.019) and a left temporal negative cluster (p=0.0056) (n = 67 Words, n = 67 Part-words). (B) Main effect of familiarisation (Phonemes - Voices) over a posterior negative cluster (p=0.018) (n = 68 Phonemes, n = 66 Voices). The frontal positive cluster did not reach significance (p=0.12). Results are highly comparable to the ROIs-based analysis (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Topographies for the grand average ERP ERP across all participants for both Experiments.</title><p>The three main topographies observed during the response are plotted on the bottom. The markers show the electrodes belonging to the 7 defined ROIs: for the first topography, central electrodes; for the second topography, frontal left, frontal right and occipital electrodes; and for the third topography, temporal left, temporal right and pre-frontal electrodes. Color scale limits [–0.07, 0.07] a.u.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Topographies for the ERPs to test words during recognition Color scale limits [–0.07, 0.07] a.u.</title><p>(<bold>A</bold>) Topographies for the Test-word effect. (<bold>B</bold>) Topographies for the Famliarisation effect. (<bold>C</bold>) Topographies for the Test-word effect during Experiment 1 (structured over Phonemes). (<bold>D</bold>) Topographies for the Test-word effect during Experiment 2 (structured over Voices).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Result for the ROI analysis of the ERPs to test words during recognition.</title><p>ROIs showing significant differences for the ERPs during the recognition phase in A Words vs Partwords comparison and B Voice vs Phoneme Comparison: the thick lines show the grand averages for the two conditions showing the main effect. Shaded areas around the lines correspond to the standard error across participants (n = 67 Words, n = 67 Part-words; n = 68 Phonemes, n = 66 Voices). Thin lines show the ERPs separated by duplet type and Familiarization type. The shaded areas between the thick lines indicate time windows where significant differences were found after correcting by multiple comparisons (p&lt;0.05, FDR corrected by the number of ROIs and times points). The topographies represent the electrodes belonging to the ROI. We run an ANOVA for the average activity in each ROI and significant time window, including test duplet and familiarisation as factors. We did not observe significant interactions in any case. Voi = Voice; Pho = Phoneme; W=Words, <italic>P</italic>=Part-Words.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig3-figsupp3-v1.tif"/></fig></fig-group><p>As cluster-based statistics are not very sensitive, we also analysed the ERPs over seven ROIS defined on the grand average ERP of all merged conditions (see Methods). Results replicated what we observed with the cluster-based permutation analysis with similar differences between Words and Part-words for the effect of familiarisation and no significant interactions. Results are presented in SI. The temporal progression of voltage topographies for all ERPs is presented in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. To verify that the effects were not driven by one group per duplet type condition, we ran a mixed two-way ANOVA for the average activity in each ROI and significant time window, with duplet type (Word/Part-word) as within-subjects factor and familiarisation as between-subjects factor. We did not observe significant interactions in any case. Future studies should consider a within-subject design to gain sensitivity to possible interaction effects.</p></sec><sec id="s2-3"><title>Adult’s behavioural performance in the same task</title><p>Adult participants heard a Structure Learning stream lasting 120 s and then ten sets of 18 test duplets preceded by Short Structure streams (30 s). For each test duplet, they had to rate its familiarity on a scale from 1 to 6. For the group familiarised with the Phoneme structure, there was a significant difference between the scores attributed to Words and Part-words (<italic>t(26)=2.92, p=0.007, Cohen’s d=0.562</italic>). The difference was marginally significant for the group familiarised with the Voice structure (<italic>t(29)=2.0443, p=0.050, Cohen’s d=0.373</italic>; <xref ref-type="fig" rid="fig4">Figure 4</xref>). A 2-way ANOVA with test-duplets and familiarisation as factors revealed a main effect of Word (<italic>F(1,55)=12.52, p=0.0008, η<sub>g</sub><sup>2</sup>=0.039</italic>), no effect of familiarisation (<italic>F(1,55) &lt;1</italic>), and a significant interaction Word ×Familiarisation (<italic>F(1,55) = 5.28, p=0.025, η<sub>g</sub><sup>2</sup>=0.017</italic>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Adults’ behavioural experiment.</title><p>Each subject’s average score attributed to the Words (blue) and Partwords (orange) is represented. On the right, for the group familiarised with the Phoneme structure (n=27) and on the left, for the group familiarised with the Voice structure (n=30). The difference between test duplets was significant for the Phoneme group (p<italic>=0.007</italic>) and only marginally significant for the Voice group (p<italic>=0.050</italic>). There was also a significant interaction group ×duplet type (p=0.025).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-fig4-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Statistical learning is a general learning mechanism</title><p>In two experiments, we compared auditory statistical learning over a linguistic and a non-linguistic dimension in sleeping neonates. We took advantage of the possibility of constructing streams based on the same 36 tokens (6 syllables ×6 voices), the only difference between the experiments being the arrangement of the tokens in the streams. We showed that neonates were sensitive to regularities based either on the phonetic or the voice dimensions of speech, even in the presence of a non-informative feature that must be disregarded.</p><p>Parsing based on statistical information was revealed by steady-state evoked potentials at the duplet rate observed around 2 min after the onset of the familiarisation stream and by different ERPs to Words and Part-words presented during a recognition phase in both experiments. Despite variations in the other dimension, statistical learning was possible, showing that this mechanism operates at a stage when these dimensions have already been separated along different processing pathways. Our results, thus, revealed that linguistic content and voice identity are calculated independently and in parallel. This result confirms that, even in newborns, the syllable is not a holistic unit (<xref ref-type="bibr" rid="bib31">Gennari et al., 2021</xref>) but that the rich temporo-frequential spectrum of speech is processed in parallel along different networks, probably using different integration factors (<xref ref-type="bibr" rid="bib10">Boemio et al., 2005</xref>; <xref ref-type="bibr" rid="bib46">Moerel et al., 2012</xref>; <xref ref-type="bibr" rid="bib64">Zatorre and Belin, 2001</xref>). While statistical learning has already been described in many domains in neonates and young infants (<xref ref-type="bibr" rid="bib25">Fló et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Fló et al., 2022b</xref>; <xref ref-type="bibr" rid="bib40">Kirkham et al., 2002</xref>; <xref ref-type="bibr" rid="bib41">Kudo et al., 2011</xref>), we add here that even sleeping neonates distinctly applied it to potentially all dimensions in which speech is factorised in the auditory cortex (<xref ref-type="bibr" rid="bib31">Gennari et al., 2021</xref>; <xref ref-type="bibr" rid="bib33">Gwilliams et al., 2022</xref>). Our result contribute to a growing body of evidence supporting the universality of statistical learning —albeit with potential quantitative differences (<xref ref-type="bibr" rid="bib30">Frost et al., 2015</xref>; <xref ref-type="bibr" rid="bib53">Ren and Wang, 2023</xref>; <xref ref-type="bibr" rid="bib59">Santolin and Saffran, 2018</xref>). This mechanism might be rooted in associative learning processes relying on the co-existence of event representations driven by slow activation decays (<xref ref-type="bibr" rid="bib7">Benjamin et al., 2024</xref>).</p><p>We observed no clear processing advantage for the linguistic dimension over the voice dimension in neonates. The ability to track regularities in parallel for different speech features provides newborns with a powerful tool to create associations between recurring events and uncover structure. Future work could explore whether they can simultaneously track multiple regularities.</p></sec><sec id="s3-2"><title>Differences between statistical learning over voices and over phonemes</title><p>While the main pattern of results between experiments was comparable, we did observe some differences. The word-rate steady-state response (2 Hz) for the group of infants exposed to structure over phonemes was over posterior electrodes and left lateralised over central electrodes, while the group of infants hearing structure over voices showed mostly entrainment over posterior and right temporal electrodes. Auditory ERPs, after reference-averaged, typically consist of a central positivity and posterior negativity. These results are consistent with statistical learning in distinct lateralised neural networks to process speech’s phonetic and voice content. Recent brain imaging studies in infants do indeed show hemispheric biases in precursors of later networks (<xref ref-type="bibr" rid="bib9">Blasi et al., 2011</xref>; <xref ref-type="bibr" rid="bib18">Dehaene-Lambertz et al., 2010</xref>; <xref ref-type="bibr" rid="bib44">Mahmoudzadeh et al., 2013</xref>), even if specialisation increases during development (<xref ref-type="bibr" rid="bib61">Shultz et al., 2014</xref>; <xref ref-type="bibr" rid="bib62">Sylvester et al., 2023</xref>). However, the hemispheric differences reported here should be considered cautiously since the group comparison did not survive multiple comparison corrections. Future work investigating the neural networks involved should implement a within-subject design to gain statistical power.</p><p>The time course of entrainment at the duplet rate revealed that it emerged at a similar time for both statistical structures. While the duplet rate response seemed more stable in the Phoneme group (evidenced by a sustained ITC at the word rate above zero and a steeper slope of increase), no significant difference was observed between the groups. Furthermore, since both groups exhibited differences in the Words vs Part-words comparison during the recognition phase, it is unlikely that the differences observed during exposure to the stream were due to poorer computation of statistical transitions for voice regularities compared to phoneme regularities. Another explanation may lie not in the statistical calculation but in the stability of the voice representation itself, as voice processing challenges have been reported in both children and adults (<xref ref-type="bibr" rid="bib36">Johnson et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Mahmoudzadeh et al., 2016</xref>). This could disrupt stable chunking during the stream even while preserving word recognition during the test. In a previous study, we observed that entrainment failed to emerge under certain conditions despite neonates successfully computing TPs—likely due to the absence of chunking (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2023</xref>).</p></sec><sec id="s3-3"><title>Phoneme regularities might trigger a lexical search</title><p>In the test phase using isolated duplets, we observed a significant difference between groups: A dipole, consisting of a posterior negative pole and a frontal positivity, was recorded around 500ms following linguistic duplets but not voice duplets. Since the acoustic properties of the duplets were identical in both experiments, the difference can only be attributed to an endogenous process modulating duplet processing driven by the type of regularity to which neonates were exposed during the familiarisation phase.</p><p>Given its topography, latency and the phonetic context in which it appears, this component might be a precursor of the N400, a negative deflection at 200–600ms over central-parietal electrodes elicited by lexico-semantic manipulations in adults (<xref ref-type="bibr" rid="bib43">Kutas and Federmeier, 2011</xref>). Previous studies have already postulated components analogous to the N400 in infants. For example, a posterior negativity has been reported in infants as young as five months when hearing their own name compared to a stranger’s name (<xref ref-type="bibr" rid="bib50">Parise et al., 2010</xref>), when a pseudo-word was consistently vs inconsistently associated with an object (<xref ref-type="bibr" rid="bib29">Friedrich and Friederici, 2011</xref>), and when they saw unexpected vs expected actions (<xref ref-type="bibr" rid="bib52">Reid et al., 2009</xref>). These results suggest that such negativity might be related to semantic processing. As is often the case in infants, the latency of the component was delayed, and its topography was more posterior compared to older developmental stages (<xref ref-type="bibr" rid="bib28">Friedrich and Friederici, 2005</xref>; <xref ref-type="bibr" rid="bib37">Junge et al., 2021</xref>).</p><p>Furthermore, even in the absence of clear semantic content, an N400 has been reported in adults listening to artificial languages. For instance, <xref ref-type="bibr" rid="bib57">Sanders et al., 2002</xref> observed an N400 in adults listening to an artificial language after prior exposure to isolated pseudo-words. Other studies have demonstrated larger N400 amplitudes in adults when listening to structured streams compared to random sequences, whether these sequences consisted of syllables (<xref ref-type="bibr" rid="bib14">Cunillera et al., 2006</xref>; <xref ref-type="bibr" rid="bib15">Cunillera et al., 2009</xref>), tones (<xref ref-type="bibr" rid="bib1">Abla et al., 2008</xref>), or shapes (<xref ref-type="bibr" rid="bib2">Abla and Okanoya, 2009</xref>). Comparing ERPs from the recognition phase with those elicited by duplets during the familiarisation stream was not feasible due to the weaker signal-to-noise ratio in continuous stream recordings versus isolated stimuli and baseline challenges specific to infant background EEG (<xref ref-type="bibr" rid="bib22">Eisermann et al., 2013</xref>). However, some inferences can still be drawn regarding the differences between the phoneme and voice experiments. Neural responses in both the learning and recognition phases could reflect a top-down effect: Phonetic regularities, but not voice regularities, would induce a lexical search during the presentation of the isolated duplets or at least activate a proto-lexical network.</p><p>Previous evidence suggests that infants extract and store possible word forms even before associating them with a clear meaning (<xref ref-type="bibr" rid="bib38">Jusczyk and Hohne, 1997</xref>). For example, stronger fMRI activation for forward speech than backward speech in the left angular gyrus in 3-month-old infants has been linked to the activations of possible word forms in a proto-lexicon for native language sentences (<xref ref-type="bibr" rid="bib17">Dehaene-Lambertz et al., 2002</xref>). Similarly, <xref ref-type="bibr" rid="bib60">Shukla et al., 2011</xref> demonstrated that chunks extracted from the speech stream serve as candidate words to which meanings can be attached. In their study, 6-month-olds spontaneously associated a pseudo-word extracted from natural sentences with a visual object. Although their experiment relied on TP and prosodic cues to extract the word, while our study only used statistical cues, this spontaneous bias to treat possible word forms as referring to a meaning (see also <xref ref-type="bibr" rid="bib8">Bergelson and Aslin, 2017</xref>) might trigger activation along a lexicon pathway, explaining the difference between the two experiments: Only speech chunks based on phonetic regularities, and not voice regularities, are viable word candidates.</p><p>A lexical entry might also explain the more sustained activity during the familiarisation stream in the phonemes group, as the chunk might be encoded as a putative word in this admittedly rudimentary but present lexical store. In this hypothesis, the neural entrainment may reflect not only TPs but also the recovery of the ‘lexical’ item.</p></sec><sec id="s3-4"><title>Adults also learn voice regularities</title><p>Finally, we would like to emphasise that it is highly unnatural for a word not to be produced by the same speaker, nor for speakers to exhibit the kind of statistical relationships used here. Despite several years of exposure to speech, adults still demonstrated some learning of voice duplets, supporting the hypothesis of a general and automatic statistical learning ability. However, they also clearly displayed an advantage for phonetic regularities over voice regularities, revealed by the significant interaction Familiarisation ×Word, not observed in neonates.</p><p>This difference might have several not mutually exclusive explanations. First, it may stem from the behavioural test being a more explicit measure of word recognition than the implicit task allowed by EEG recordings. Second, adults may perform better with phoneme structure due to a more effective auditory normalisation process or the additional use of a writing code for phonemes, which does not exist for voices. Finally, neonates, having little experience and therefore fewer expectations or constraints, might serve as better revealers of the possibilities afforded by statistical learning compared to older participants.</p></sec><sec id="s3-5"><title>Conclusion</title><p>Altogether, our results show that statistical learning works similarly on different speech features in human neonates with no clear advantage for computing linguistically relevant regularities in speech. This supports the idea that statistical learning is a general learning mechanism, probably operating on common computational principles across neural networks (<xref ref-type="bibr" rid="bib7">Benjamin et al., 2024</xref>) but within different neural networks with a different chain of operations: phonetic regularities induce a supplementary component –not seen in the case of voice regularities—that we related to a lexical N400. Understanding how statistical learning computations over linguistically relevant dimensions, such as the phonetic content of speech, are extracted and passed on to subsequent processing stages might be fundamental to uncovering how the infant brain acquires language. Further research is needed to understand how extracting regularities over different features articulates the language network.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Participants were healthy-full-term neonates with normal pregnancy and birth (GA &gt;38 weeks, Apgar scores ≥7/8 at 1/5 min, birthweight &gt;2.5 Kg, cranial perimeter ≥33.0 cm), tested at the Port Royal Maternity (AP-HP), in Paris, France. Parents provided informed consent. The regional ethical committee for biomedical research (Comité de Protection des Personnes Region Centre Ouest 1, EudraCT/ID RCB: 2017-A00513-50) approved the protocol, and the study was carried out according to relevant guidelines and regulations. 67 participants (34 in Experiment 1 and 33 in Experiment 2) who provided enough data without motion artefacts were included (Experiment 1: 19 females; 1–4 days old; mean GA: 39.3 weeks; mean weight: 3387 g; Experiment 2: 15 females; 1–4 days old; mean GA: 39.0 weeks; mean weight: 3363 g). 12 other infants were excluded from the analyses (11 due to fussiness; 1 due to bad data quality).</p></sec><sec id="s4-2"><title>Stimuli</title><p>The stimuli were synthesised using the MBROLA diphone database (<xref ref-type="bibr" rid="bib21">Dutoit et al., 1996</xref>). Syllables had a consonant-vowel structure and lasted 250ms (consonants 90ms, vowels 160ms). Six different syllables (<italic>ki</italic>, <italic>da</italic>, <italic>pe</italic>, <italic>tu</italic>, <italic>bo</italic>, <italic>gɛ</italic>) and six different voices were used (<italic>fr3</italic>, <italic>fr1</italic>, <italic>fr7</italic>, <italic>fr2</italic>, <italic>it4</italic>, <italic>fr4</italic>), resulting in a total of 36 syllable-voice combinations, from now on, tokens. The voices could be female or male and have three different pitch levels (low, middle, and high) (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>). We performed post-hoc tests to ensure that the results were not driven by a perception of two voices: female and male (see Appendix). The 36 tokens were synthesised independently in MBROLA, their intensity was normalised, and the first and last 5ms were ramped to zero to avoid ‘clicks’. The streams were synthesised by concatenating the tokens’ audio files, and they were ramped up and down during the first and last 5 s to avoid the start and end of the stream serving as perceptual anchors.</p><p>The Structured streams were created by concatenating the tokens in such a way that they resulted in a semi-random concatenation of the duplets (i.e. pseudo-words) formed by one of the features (syllable/voice) while the other feature (voice/syllable) vary semi-randomly. In other words, in Experiment 1, the order of the tokens was such that Transitional Probabilities (TPs) between syllables alternated between 1 (within duplets) and 0.5 (between duplets), while between voices, TPs were uniformly 0.2. The design was orthogonal for the Structured streams of Experiment 2 (i.e. TPs between voices alternated between 1 and 0.5, while between syllables were evenly 0.2). The random streams were created by semi-randomly concatenating the 36 tokens to achieve uniform TPs equal to 0.2 over both features. The semi-random concatenation implied that the same element could not appear twice in a row, and the same two elements could not repeatedly alternate more than two times (i.e. the sequence <italic>X<sub>k</sub>X<sub>j</sub>X<sub>k</sub>X<sub>j</sub></italic>, where <italic>X<sub>k</sub></italic> and <italic>X<sub>j</sub></italic> are two elements, was forbidden). Notice that with an element, we refer to a duplet when it concerns the choice of the structured feature and to the identity of the second feature when it involves the other feature. The same statistical structures were used for both Experiments, only changing over which dimension the structure was applied. The learning stream lasted 120 s, with each duplet appearing 80 times. The 10 short structured streams lasted 30 s each, each duplet appearing a total of 200 times (10×20). The same random stream was used for both Experiments, and it lasted 120 s.</p><p>In Experiment 1, the duplets were created to prevent specific phonetic features from facilitating stream segmentation. In each experiment, two different structured streams (lists A and B) were used by modifying how the syllables/voices were combined to form the duplets (<xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>). Crucially, the Words/duplets of list A are the Part-words of list B and vice versa any difference between those two conditions can thus not be caused by acoustical differences. Participants were randomly assigned and balanced between lists and Experiments.</p><p>The test words were duplets formed by the concatenation of two tokens, such that they formed a Word or a Part-word according to the structured feature.</p></sec><sec id="s4-3"><title>Procedure and data acquisition</title><p>Scalp electrophysiological activity was recorded using a 128-electrode net (Electrical Geodesics, Inc) referred to the vertex with a sampling frequency of 250 Hz. Neonates were tested in a soundproof booth while sleeping or during quiet rest. The study involved: (1) 120 s of a random stream, (2) 120 s of a structured stream, (3) 10 series of 30 s of structured streams followed by 18 test sequences (SOA 2–2.3 s).</p></sec><sec id="s4-4"><title>Data pre-processing</title><p>Data were band-pass filtered 0.1–40 Hz and pre-processed using custom MATLAB scripts based on the EEGLAB toolbox 2021.0 (<xref ref-type="bibr" rid="bib19">Delorme and Makeig, 2004</xref>) according to the APICE pre-processing pipeline to recover as much free-artifacts data as possible (<xref ref-type="bibr" rid="bib27">Fló et al., 2022b</xref>).</p></sec><sec id="s4-5"><title>Neural entrainment</title><p>The pre-processed data were further high-pass filtered at 0.2 Hz. Then, data was segmented from the beginning of each phase into 0.5 s long segments (240 duplets for the Random, 240 duplets for the long Structured, and 600 duplets for the short Structured). Segments containing samples with artefacts defined as bad data in more than 30% of the channels were rejected, and the remaining channels with artefacts were spatially interpolated.</p><sec id="s4-5-1"><title>Neural entrainment per condition</title><p>The 0.5 s epochs belonging to the same condition were reshaped into non-overlapping epochs (<xref ref-type="bibr" rid="bib5">Benjamin et al., 2021</xref>) of 7.5 s (15 duplets, 30 syllables), retaining the chronological order; thus, the timing of the steady-state response. Subjects who did not provide at least 50% artifact-free epochs for each condition (at least 8 long epochs during Random and 28 during Structured) were excluded from the entrainment analysis (32 included subjects in Experiment 1, and 32 included subjects in Experiment 2). The retained subjects for Experiment 1, on average provided 13.59 epochs for the Random condition (SD 2.07, range [8, 16]) and 48.16 for the Structured conditions (SD 5.89, range [33, 55]). The retained subjects for Experiment 2, on average provided 13.78 epochs for the Random condition (SD 1.93, range [8, 16]) and 46.88 for the Structured conditions (SD 5.62, range [36, 55]). After data rejection, data were referenced to the average and normalized by dividing by the standard deviation within an epoch across electrodes and time. Next, data were converted to the frequency domain using the Fast Fourier Transform (FFT) algorithm, and the ITC was estimated for each electrode during each condition (Random, Structured) as <inline-formula><mml:math id="inf3"><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>φ</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula>, where N is the number of trials and φ(f,i) is the phase at frequency f and trial i. The ITC ranges from 0 to 1 (i.e. completely desynchronized activity to perfectly phased locked activity). Since we aim to detect an increase in signal synchronization at specific frequencies, the SNR was computed relative to the twelve adjacent frequency bins (six of each side corresponding to 0.8 Hz; <xref ref-type="bibr" rid="bib39">Kabdebon et al., 2022</xref>). This procedure also enables correcting differences in the ITC due to a different number of trials. Specifically, the SNR was <italic>SNR(f) = (ITC(f)-mean(ITC<sub>noise</sub>(f)))/std(ITC<sub>noise</sub>(f))</italic>, where <italic>ITC<sub>noise</sub>(f)</italic> is the ITC over the adjacent frequency bins. For statistical analysis, we compared the SNR at syllable rate (4 Hz) and duplet rate (2 Hz) against the average SNR over the 12 adjacent frequency bins using a one-tail paired permutation test (5000 permutations). We also directly compared the entrainment during the two conditions to individuate the electrodes showing a greater entrainment during the Structured than Random streams. We evaluated the interaction between Stream type (Random and Structured) and Familiarization type (Structured over Phonemes or Voices) by comparing the difference in entrainment between Structured and Random during the two experiments using a two sides unpaired permutation test (5000 permutations). All p-values were corrected across electrodes by FDR.</p></sec><sec id="s4-5-2"><title>Neural entrainment time course</title><p>The 0.5 s epochs were concatenated chronologically (2 mins of Random, 2 min of long Structured stream, and 5 min of short Structured blocks). The same analysis as above was performed in sliding time windows of 2 min with a 1 s step. A time window was considered valid if at least 8 out of the 16 epochs were free of motion artefacts. Missing values due to the presence of motion artifacts where linearly interpolated. Then, the entrainment time course at the syllable rate was computed as the average over the electrodes showing significant entrainment at 4 Hz during the Random condition, and at the duplet rate, as the average over the electrodes showing significant entrainment at 2 Hz during the Structured condition. Finally, data was smooth over a time window of 30 s.</p><p>To investigate the increase in the neural activity locked to the regularity during the long familiarisation, we fitted a LMM for each group of subjects. We included time as a fixed effect and random slopes and interceptions for individual subjects: <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∼</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We then compare the time effect between groups by including all data in a single LMM with time and group as fixed effects: <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>C</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∼</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>∗</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec id="s4-6"><title>ERPs to test words</title><p>The pre-processed data were filtered between 0.2 and 20 Hz, and epoched between [–0.2, 2.0] s from the onset of the duplets. Epochs containing samples identified as artifacts by APICE procedure were rejected. Subjects who did not provide at least half of the trials (45 trials) per condition were excluded (34 subjects kept for Experiment 1, and 33 for Experiment 2). No subject was excluded based on this criterion in the Phoneme groups, and one subject was excluded in the Voice groups. For Experiment 1, we retained on average 77.47 trials (SD 9.98, range [52, 89]) for the Word condition and 77.12 trials (SD 10.04, range [56, 89]) for the Part-word condition. For Experiment 2, we retained on average 73.73 trials (SD 10.57, range [47, 90]) for the Word condition and 74.18 trials (SD 11.15, range [46, 90]) for the Part-word condition. Data were reference averaged and normalised within each epoch by dividing by the standard deviation across electrodes and time.</p><p>Since the grand average response across both groups and conditions returned to the pre-stimulus level at around 1500ms, we defined [0, 1500] ms as time windows of analysis. We first analysed the data using non-parametric cluster-based permutation analysis (<xref ref-type="bibr" rid="bib49">Oostenveld et al., 2011</xref>) in the time window [0, 1500] ms (alpha threshold for clustering 0.10, neighbour distance ≤2.5 cm, clusters minimum size 3 and 5000 permutations).</p><p>We also analysed the data in seven ROIs to ensure that no other effects were present that were not caught by the cluster-based permutation analysis. By inspecting the grand average ERP across both experiments and conditions, we identified three characteristic topographies: (a) positivity over central electrodes, (b) positivity over frontal electrodes and negativity over occipital electrodes, and (c) positivity over prefrontal electrodes and negativity over temporal electrodes (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Then, we defined seven symmetric ROIs: Central, Frontal Left, Frontal Right, Occipital, Prefrontal, Temporal Left, Temporal Right. We evaluated the main effect of Test-word type by comparing the EPRs between Words and Partwords (paired t-test) and the main effect of Familiarization type by comparing ERPs between Experiment 1 (structured over Phonemes) and Experiment 2 (structure over Voices; unpaired t-test). All p-values were FDR corrected by the number of time points (n=376) and ROIs (n=7). To test for possible interaction effects, we compared the difference between Words and Partwords between the two groups. To verify that the main effects were not driven by one condition or group, we computed the average on each of the time windows where a main effect was identified considering both the Test-word type and Familiarization type factors, and we ran a two ways-ANOVA (Test-word type x Familiarization type). Results are presented in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>.</p></sec><sec id="s4-7"><title>Adults’ behavioural experiment</title><p>57 French-speaking adults were tested in an online experiment analogous to the infant study through the Prolific platform. All participants provided informed consent and received monetary compensation for their participation. The study was approved by the Ethical Research Committee of Paris Saclay University under the reference CER-Paris-Saclay-2019–063. The same stimuli as in the infants’ experiment were used. Participants first heard 2 min of familiarisation with the Structured stream. Then, they completed ten sessions of re-familiarisation and testing. Each re-familiarization lasted 30 s, and in each test session, all 18 test words were presented. The structure could be either over the phonetic or the voice content, and two lists were used (see <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>). Participants were randomly assigned to one of the groups and to one list. The Phoneme group included 27 participants, and the Voice group 30 participants. Before starting the experiment, subjects were instructed to pay attention to an invented language because later, they would have to answer if different sequences adhered to the structure of the language. During the test phase, subjects were asked to scale their familiarity with each test word by clicking with a cursor on a scale from 1 to 6.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Project administration</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Neonates were tested at the Port Royal Maternity (AP-HP), in Paris, France. Parents provided informed consent. The regional ethical committee for biomedical research (Comité de Protection des Personnes Region Centre Ouest 1, EudraCT/ID RCB: 2017-A00513-50) approved the protocol, and the study was carried out according to relevant guidelines and regulations.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-101802-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>This study involves sensitive data collected from human infants, which can only be used for scientific research and not for commercial applications. Parents have given consent for the data to be used strictly for scientific research. Consequently, individual infant data cannot be made publicly available but can be accessed on request. Researchers who wish to request access to the infant data must be affiliated with a public research institution and clearly outline the scientific objectives of their request, with replication being an acceptable goal. Requests should be submitted to Ghislaine Dehaene-Lambertz (gdehaene@gmail.com), and the ethical officer of the institution (CEA) will evaluate them. All the analysis tools for the infants' data and grand average results are publicly available. The adult data and analysis tools are publicly available. All openly available material can be found at <ext-link ext-link-type="uri" xlink:href="https://osf.io/an4jk/">https://osf.io/an4jk/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Palu</surname><given-names>M</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Flo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Statistical learning beyond words in human neonates</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/an4jk/">an4jk</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We want to thank all the families who participated in the study. This research has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement No. 695710).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abla</surname><given-names>D</given-names></name><name><surname>Katahira</surname><given-names>K</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>On-line assessment of statistical learning by event-related potentials</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>952</fpage><lpage>964</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20058</pub-id><pub-id pub-id-type="pmid">18211232</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abla</surname><given-names>D</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual statistical learning of shape sequences: an ERP study</article-title><source>Neuroscience Research</source><volume>64</volume><fpage>185</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2009.02.013</pub-id><pub-id pub-id-type="pmid">19428699</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldwin</surname><given-names>D</given-names></name><name><surname>Andersson</surname><given-names>A</given-names></name><name><surname>Saffran</surname><given-names>J</given-names></name><name><surname>Meyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Segmenting dynamic human action via statistical structure</article-title><source>Cognition</source><volume>106</volume><fpage>1382</fpage><lpage>1407</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2007.07.005</pub-id><pub-id pub-id-type="pmid">18035346</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1038/35002078</pub-id><pub-id pub-id-type="pmid">10659849</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Remarks on the analysis of steady-state responses: Spurious artifacts introduced by overlapping epochs</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>142</volume><fpage>370</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2021.05.023</pub-id><pub-id pub-id-type="pmid">34311971</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Palu</surname><given-names>M</given-names></name><name><surname>Naik</surname><given-names>S</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Tracking transitional probabilities and segmenting auditory sequences are dissociable processes in adults and neonates</article-title><source>Developmental Science</source><volume>26</volume><elocation-id>e13300</elocation-id><pub-id pub-id-type="doi">10.1111/desc.13300</pub-id><pub-id pub-id-type="pmid">35772033</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Al Roumi</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Long-horizon associative learning explains human sensitivity to statistical and network structures in auditory sequences</article-title><source>The Journal of Neuroscience</source><volume>44</volume><elocation-id>e1369232024</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1369-23.2024</pub-id><pub-id pub-id-type="pmid">38408873</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergelson</surname><given-names>E</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Nature and origins of the lexicon in 6-mo-olds</article-title><source>PNAS</source><volume>114</volume><fpage>12916</fpage><lpage>12921</lpage><pub-id pub-id-type="doi">10.1073/pnas.1712966114</pub-id><pub-id pub-id-type="pmid">29158399</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blasi</surname><given-names>A</given-names></name><name><surname>Mercure</surname><given-names>E</given-names></name><name><surname>Lloyd-Fox</surname><given-names>S</given-names></name><name><surname>Thomson</surname><given-names>A</given-names></name><name><surname>Brammer</surname><given-names>M</given-names></name><name><surname>Sauter</surname><given-names>D</given-names></name><name><surname>Deeley</surname><given-names>Q</given-names></name><name><surname>Barker</surname><given-names>GJ</given-names></name><name><surname>Renvall</surname><given-names>V</given-names></name><name><surname>Deoni</surname><given-names>S</given-names></name><name><surname>Gasston</surname><given-names>D</given-names></name><name><surname>Williams</surname><given-names>SCR</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Simmons</surname><given-names>A</given-names></name><name><surname>Murphy</surname><given-names>DGM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Early specialization for voice and emotion processing in the infant brain</article-title><source>Current Biology</source><volume>21</volume><fpage>1220</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.06.009</pub-id><pub-id pub-id-type="pmid">21723130</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boemio</surname><given-names>A</given-names></name><name><surname>Fromm</surname><given-names>S</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>389</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1038/nn1409</pub-id><pub-id pub-id-type="pmid">15723061</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boros</surname><given-names>M</given-names></name><name><surname>Magyari</surname><given-names>L</given-names></name><name><surname>Török</surname><given-names>D</given-names></name><name><surname>Bozsik</surname><given-names>A</given-names></name><name><surname>Deme</surname><given-names>A</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural processes underlying statistical learning for speech segmentation in dogs</article-title><source>Current Biology</source><volume>31</volume><fpage>5512</fpage><lpage>5521</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.10.017</pub-id><pub-id pub-id-type="pmid">34717832</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buiatti</surname><given-names>M</given-names></name><name><surname>Peña</surname><given-names>M</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Investigating the neural correlates of continuous speech computation with frequency-tagged neuroelectric responses</article-title><source>NeuroImage</source><volume>44</volume><fpage>509</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.015</pub-id><pub-id pub-id-type="pmid">18929668</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bulf</surname><given-names>H</given-names></name><name><surname>Johnson</surname><given-names>SP</given-names></name><name><surname>Valenza</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual statistical learning in the newborn infant</article-title><source>Cognition</source><volume>121</volume><fpage>127</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2011.06.010</pub-id><pub-id pub-id-type="pmid">21745660</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunillera</surname><given-names>T</given-names></name><name><surname>Toro</surname><given-names>JM</given-names></name><name><surname>Sebastián-Gallés</surname><given-names>N</given-names></name><name><surname>Rodríguez-Fornells</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The effects of stress and statistical cues on continuous speech segmentation: an event-related brain potential study</article-title><source>Brain Research</source><volume>1123</volume><fpage>168</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.09.046</pub-id><pub-id pub-id-type="pmid">17064672</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunillera</surname><given-names>T</given-names></name><name><surname>Càmara</surname><given-names>E</given-names></name><name><surname>Toro</surname><given-names>JM</given-names></name><name><surname>Marco-Pallares</surname><given-names>J</given-names></name><name><surname>Sebastián-Galles</surname><given-names>N</given-names></name><name><surname>Ortiz</surname><given-names>H</given-names></name><name><surname>Pujol</surname><given-names>J</given-names></name><name><surname>Rodríguez-Fornells</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Time course and functional neuroanatomy of speech segmentation in adults</article-title><source>NeuroImage</source><volume>48</volume><fpage>541</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.069</pub-id><pub-id pub-id-type="pmid">19580874</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Pena</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Electrophysiological evidence for automatic phonetic processing in neonates</article-title><source>Neuroreport</source><volume>12</volume><fpage>3155</fpage><lpage>3158</lpage><pub-id pub-id-type="doi">10.1097/00001756-200110080-00034</pub-id><pub-id pub-id-type="pmid">11568655</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Functional neuroimaging of speech perception in infants</article-title><source>Science</source><volume>298</volume><fpage>2013</fpage><lpage>2015</lpage><pub-id pub-id-type="doi">10.1126/science.1077066</pub-id><pub-id pub-id-type="pmid">12471265</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Montavont</surname><given-names>A</given-names></name><name><surname>Jobert</surname><given-names>A</given-names></name><name><surname>Allirol</surname><given-names>L</given-names></name><name><surname>Dubois</surname><given-names>J</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Language or music, mother or Mozart? Structural and environmental influences on infants’ language networks</article-title><source>Brain and Language</source><volume>114</volume><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2009.09.003</pub-id><pub-id pub-id-type="pmid">19864015</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeWitt</surname><given-names>I</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Phoneme and word recognition in the auditory ventral stream</article-title><source>PNAS</source><volume>109</volume><fpage>E505</fpage><lpage>E514</lpage><pub-id pub-id-type="doi">10.1073/pnas.1113427109</pub-id><pub-id pub-id-type="pmid">22308358</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dutoit</surname><given-names>T</given-names></name><name><surname>Pagel</surname><given-names>V</given-names></name><name><surname>Pierret</surname><given-names>N</given-names></name><name><surname>Bataille</surname><given-names>F</given-names></name><name><surname>van der Vrecken</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The MBROLA project: towards a set of high quality speech synthesizers free of use for non commercial purposes</article-title><conf-name>Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP ’96</conf-name><conf-loc>Philadelphia, PA, USA</conf-loc><fpage>1393</fpage><lpage>1396</lpage><pub-id pub-id-type="doi">10.1109/ICSLP.1996.607874</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eisermann</surname><given-names>M</given-names></name><name><surname>Kaminska</surname><given-names>A</given-names></name><name><surname>Moutard</surname><given-names>ML</given-names></name><name><surname>Soufflet</surname><given-names>C</given-names></name><name><surname>Plouin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Normal EEG in childhood: from neonates to adolescents</article-title><source>Neurophysiologie Clinique = Clinical Neurophysiology</source><volume>43</volume><fpage>35</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.neucli.2012.09.091</pub-id><pub-id pub-id-type="pmid">23290174</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Estes</surname><given-names>KG</given-names></name><name><surname>Lew-Williams</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Listening through voices: infant statistical word segmentation across multiple speakers</article-title><source>Developmental Psychology</source><volume>51</volume><fpage>1517</fpage><lpage>1528</lpage><pub-id pub-id-type="doi">10.1037/a0039725</pub-id><pub-id pub-id-type="pmid">26389607</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Statistical learning of new visual feature combinations by infants</article-title><source>PNAS</source><volume>99</volume><fpage>15822</fpage><lpage>15826</lpage><pub-id pub-id-type="doi">10.1073/pnas.232472899</pub-id><pub-id pub-id-type="pmid">12429858</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Brusini</surname><given-names>P</given-names></name><name><surname>Macagno</surname><given-names>F</given-names></name><name><surname>Nespor</surname><given-names>M</given-names></name><name><surname>Mehler</surname><given-names>J</given-names></name><name><surname>Ferry</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Newborns are sensitive to multiple cues for word segmentation in continuous speech</article-title><source>Developmental Science</source><volume>22</volume><elocation-id>e12802</elocation-id><pub-id pub-id-type="doi">10.1111/desc.12802</pub-id><pub-id pub-id-type="pmid">30681763</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Palu</surname><given-names>M</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Sleeping neonates track transitional probabilities in speech but only retain the first syllable of words</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>4391</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-08411-w</pub-id><pub-id pub-id-type="pmid">35292694</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Gennari</surname><given-names>G</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Dehane-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022b</year><source>Automated Pipeline for Infants Continuous EEG (APICE): A Flexible Pipeline for Developmental Cognitive Studies</source><publisher-name>Elsevier Enhanced Reader</publisher-name><pub-id pub-id-type="doi">10.1016/j.dcn.2022.101077</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>M</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Semantic sentence processing reflected in the event-related potentials of one- and two-year-old children</article-title><source>Neuroreport</source><volume>16</volume><fpage>1801</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1097/01.wnr.0000185013.98821.62</pub-id><pub-id pub-id-type="pmid">16237330</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>M</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Word learning in 6-month-olds: fast encoding-weak retention</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>3228</fpage><lpage>3240</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00002</pub-id><pub-id pub-id-type="pmid">21391764</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frost</surname><given-names>R</given-names></name><name><surname>Armstrong</surname><given-names>BC</given-names></name><name><surname>Siegelman</surname><given-names>N</given-names></name><name><surname>Christiansen</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Domain generality versus modality specificity: the paradox of statistical learning</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.010</pub-id><pub-id pub-id-type="pmid">25631249</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gennari</surname><given-names>G</given-names></name><name><surname>Marti</surname><given-names>S</given-names></name><name><surname>Palu</surname><given-names>M</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Orthogonal neural codes for speech in the infant brain</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2020410118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2020410118</pub-id><pub-id pub-id-type="pmid">34326247</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural dynamics of phoneme sequences reveal position-invariant code for content and order</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>6606</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-34326-1</pub-id><pub-id pub-id-type="pmid">36329058</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname><given-names>MD</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of the speech stream in a non-human primate: statistical learning in cotton-top tamarins</article-title><source>Cognition</source><volume>78</volume><fpage>B53</fpage><lpage>B64</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(00)00132-3</pub-id><pub-id pub-id-type="pmid">11124355</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hay</surname><given-names>JF</given-names></name><name><surname>Pelucchi</surname><given-names>B</given-names></name><name><surname>Graf Estes</surname><given-names>K</given-names></name><name><surname>Saffran</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Linking sounds to meanings: infant statistical learning in a natural language</article-title><source>Cognitive Psychology</source><volume>63</volume><fpage>93</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2011.06.002</pub-id><pub-id pub-id-type="pmid">21762650</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>EK</given-names></name><name><surname>Westrek</surname><given-names>E</given-names></name><name><surname>Nazzi</surname><given-names>T</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Infant ability to tell voices apart rests on language experience</article-title><source>Developmental Science</source><volume>14</volume><fpage>1002</fpage><lpage>1011</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2011.01052.x</pub-id><pub-id pub-id-type="pmid">21884316</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Junge</surname><given-names>C</given-names></name><name><surname>Boumeester</surname><given-names>M</given-names></name><name><surname>Mills</surname><given-names>DL</given-names></name><name><surname>Paul</surname><given-names>M</given-names></name><name><surname>Cosper</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Development of the n400 for word learning in the first 2 years of life: a systematic review</article-title><source>Frontiers in Psychology</source><volume>12</volume><elocation-id>689534</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2021.689534</pub-id><pub-id pub-id-type="pmid">34276518</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jusczyk</surname><given-names>PW</given-names></name><name><surname>Hohne</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Infants’ memory for spoken words</article-title><source>Science</source><volume>277</volume><fpage>1984</fpage><lpage>1986</lpage><pub-id pub-id-type="doi">10.1126/science.277.5334.1984</pub-id><pub-id pub-id-type="pmid">9302291</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabdebon</surname><given-names>C</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>de Heering</surname><given-names>A</given-names></name><name><surname>Aslin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The power of rhythms: how steady-state evoked responses reveal early neurocognitive development</article-title><source>NeuroImage</source><volume>254</volume><elocation-id>119150</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119150</pub-id><pub-id pub-id-type="pmid">35351649</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkham</surname><given-names>NZ</given-names></name><name><surname>Slemmer</surname><given-names>JA</given-names></name><name><surname>Johnson</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual statistical learning in infancy: evidence for a domain general learning mechanism</article-title><source>Cognition</source><volume>83</volume><fpage>B35</fpage><lpage>B42</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(02)00004-5</pub-id><pub-id pub-id-type="pmid">11869728</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kudo</surname><given-names>N</given-names></name><name><surname>Nonaka</surname><given-names>Y</given-names></name><name><surname>Mizuno</surname><given-names>N</given-names></name><name><surname>Mizuno</surname><given-names>K</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>On-line statistical segmentation of a non-speech auditory stream in neonates as demonstrated by event-related brain potentials</article-title><source>Developmental Science</source><volume>14</volume><fpage>1100</fpage><lpage>1106</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2011.01056.x</pub-id><pub-id pub-id-type="pmid">21884325</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Miller</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Discrimination of auditory target dimensions in the presence or absence of variation in a second dimension by infants</article-title><source>Perception &amp; Psychophysics</source><volume>31</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.3758/BF03202536</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmoudzadeh</surname><given-names>M</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Fournier</surname><given-names>M</given-names></name><name><surname>Kongolo</surname><given-names>G</given-names></name><name><surname>Goudjil</surname><given-names>S</given-names></name><name><surname>Dubois</surname><given-names>J</given-names></name><name><surname>Grebe</surname><given-names>R</given-names></name><name><surname>Wallois</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Syllabic discrimination in premature human infants prior to complete formation of cortical layers</article-title><source>PNAS</source><volume>110</volume><fpage>4846</fpage><lpage>4851</lpage><pub-id pub-id-type="doi">10.1073/pnas.1212220110</pub-id><pub-id pub-id-type="pmid">23440196</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmoudzadeh</surname><given-names>M</given-names></name><name><surname>Wallois</surname><given-names>F</given-names></name><name><surname>Kongolo</surname><given-names>G</given-names></name><name><surname>Goudjil</surname><given-names>S</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional maps at the onset of auditory inputs in very early preterm human neonates</article-title><source>Cerebral Cortex</source><volume>27</volume><elocation-id>bhw103</elocation-id><pub-id pub-id-type="doi">10.1093/cercor/bhw103</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname><given-names>M</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Processing of natural sounds in human auditory cortex: tonotopy, spectral tuning, and relation to voice sensitivity</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>14205</fpage><lpage>14216</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1388-12.2012</pub-id><pub-id pub-id-type="pmid">23055490</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monroy</surname><given-names>CD</given-names></name><name><surname>Gerson</surname><given-names>SA</given-names></name><name><surname>Hunnius</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Toddlers’ action prediction: statistical learning of continuous action sequences</article-title><source>Journal of Experimental Child Psychology</source><volume>157</volume><fpage>14</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.jecp.2016.12.004</pub-id><pub-id pub-id-type="pmid">28103496</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical pathways for music and speech revealed by hypothesis-free voxel decomposition</article-title><source>Neuron</source><volume>88</volume><fpage>1281</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id><pub-id pub-id-type="pmid">26687225</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>E</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Striano</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>“Did you call me?” 5-month-old infants own name guides their attention</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e14208</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0014208</pub-id><pub-id pub-id-type="pmid">21151971</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelucchi</surname><given-names>B</given-names></name><name><surname>Hay</surname><given-names>JF</given-names></name><name><surname>Saffran</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Statistical learning in a natural language by 8-month-old infants</article-title><source>Child Development</source><volume>80</volume><fpage>674</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8624.2009.01290.x</pub-id><pub-id pub-id-type="pmid">19489896</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reid</surname><given-names>VM</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Grigutsch</surname><given-names>M</given-names></name><name><surname>Groendahl</surname><given-names>A</given-names></name><name><surname>Parise</surname><given-names>E</given-names></name><name><surname>Striano</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The neural correlates of infant and adult goal prediction: evidence for semantic processing systems</article-title><source>Developmental Psychology</source><volume>45</volume><fpage>620</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1037/a0015209</pub-id><pub-id pub-id-type="pmid">19413420</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Development of statistical learning ability across modalities, domains, and languages</article-title><source>Journal of Experimental Child Psychology</source><volume>226</volume><elocation-id>105570</elocation-id><pub-id pub-id-type="doi">10.1016/j.jecp.2022.105570</pub-id><pub-id pub-id-type="pmid">36332433</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Saffran</surname><given-names>JR</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Statistical Learning by 8-Month-Old Infants</source><publisher-name>ScienceAdviser</publisher-name><pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname><given-names>JR</given-names></name><name><surname>Johnson</surname><given-names>EK</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Statistical learning of tone sequences by human infants and adults</article-title><source>Cognition</source><volume>70</volume><fpage>27</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(98)00075-4</pub-id><pub-id pub-id-type="pmid">10193055</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname><given-names>JR</given-names></name><name><surname>Kirkham</surname><given-names>NZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Infant Statistical Learning</article-title><source>Annual Review of Psychology</source><volume>69</volume><fpage>181</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122216-011805</pub-id><pub-id pub-id-type="pmid">28793812</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>LD</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name><name><surname>Neville</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Segmenting nonsense: an event-related potential index of perceived onsets in continuous speech</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>700</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1038/nn873</pub-id><pub-id pub-id-type="pmid">12068301</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santolin</surname><given-names>C</given-names></name><name><surname>Rosa-Salva</surname><given-names>O</given-names></name><name><surname>Vallortigara</surname><given-names>G</given-names></name><name><surname>Regolin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Unsupervised statistical learning in newly hatched chicks</article-title><source>Current Biology</source><volume>26</volume><fpage>R1218</fpage><lpage>R1220</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.10.011</pub-id><pub-id pub-id-type="pmid">27923125</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santolin</surname><given-names>C</given-names></name><name><surname>Saffran</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Constraints on statistical learning across species</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>52</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.10.003</pub-id><pub-id pub-id-type="pmid">29150414</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shukla</surname><given-names>M</given-names></name><name><surname>White</surname><given-names>KS</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants</article-title><source>PNAS</source><volume>108</volume><fpage>6038</fpage><lpage>6043</lpage><pub-id pub-id-type="doi">10.1073/pnas.1017617108</pub-id><pub-id pub-id-type="pmid">21444800</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shultz</surname><given-names>S</given-names></name><name><surname>Vouloumanos</surname><given-names>A</given-names></name><name><surname>Bennett</surname><given-names>RH</given-names></name><name><surname>Pelphrey</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural specialization for speech in the first months of life</article-title><source>Developmental Science</source><volume>17</volume><fpage>766</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1111/desc.12151</pub-id><pub-id pub-id-type="pmid">24576182</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sylvester</surname><given-names>CM</given-names></name><name><surname>Kaplan</surname><given-names>S</given-names></name><name><surname>Myers</surname><given-names>MJ</given-names></name><name><surname>Gordon</surname><given-names>EM</given-names></name><name><surname>Schwarzlose</surname><given-names>RF</given-names></name><name><surname>Alexopoulos</surname><given-names>D</given-names></name><name><surname>Nielsen</surname><given-names>AN</given-names></name><name><surname>Kenley</surname><given-names>JK</given-names></name><name><surname>Meyer</surname><given-names>D</given-names></name><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Graham</surname><given-names>AM</given-names></name><name><surname>Fair</surname><given-names>DA</given-names></name><name><surname>Warner</surname><given-names>BB</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Rogers</surname><given-names>CE</given-names></name><name><surname>Luby</surname><given-names>JL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Smyser</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Network-specific selectivity of functional connections in the neonatal brain</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>2200</fpage><lpage>2214</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhac202</pub-id><pub-id pub-id-type="pmid">35595540</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toro</surname><given-names>JM</given-names></name><name><surname>Trobalón</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Statistical computations over a speech stream in a rodent</article-title><source>Perception &amp; Psychophysics</source><volume>67</volume><fpage>867</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.3758/BF03193539</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Spectral and Temporal Processing in Human Auditory Cortex</source><publisher-name>Cerebral Cortex</publisher-name></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Post-hoc test: investigating female and male voice perception effects</title><p>Given that gender may be an important factor in infants' speech perception (newborns, for instance, prefer female voices at birth), we conducted tests to assess whether this dimension could have influenced the results observed in Experiment 2.</p><sec sec-type="appendix" id="s8-1"><title>Computation of TPs</title><p>We first quantified the transitional probabilities matrices during the structured stream of Experiment 2, considering that there were only two types of voices: Female and Male.</p><p>For List A, all transition probabilities were equal to 0.5 (P(M|F), P(F|M), P(M|M), P(F|F)), resulting in flat TPs throughout the stream (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, top). Therefore, we would not expect neural entrainment at the word rate (2 Hz) nor anticipate ERP differences between the presented duplets in the test phase.</p><p>For List B, P(M|F)=P(F|M)=0.66 while P(M|M)=P(F|F)=0.33, without a regular pattern of TP drops throughout the stream (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> -, bottom). Although this pattern is unlikely to induce strong neural entrainment at 2 Hz, some degree of entrainment might have occasionally occurred due to some drops occurring at a 2 Hz frequency. Regarding the test phase, all three Words and only one Part-word presented alternating patterns (TP=0.6). Therefore, we cannot rule out that gender alternation might have affected the difference in ERPs between Words and Partwords in List B.</p><p>While it seems unlikely that gender alternation alone explains the entire pattern of results, as the effect was inconsistent and appeared in only one of the lists, we separately analysed the entrainment and ERP effects in each list to rule out this possibility</p></sec><sec sec-type="appendix" id="s8-2"><title>Neural entrainment effect</title><p>We computed the average entrainment over the electrodes, which showed significant entrainment at 2 Hz (word rate). A comparison of the 2 Hz entrainment between participants who completed List A and List B showed no significant differences (t(30) = -0.27, p = 0.79). A test against zero for each list indicated significant entrainment in both cases (List A: t(17) = 4.44, p = 0.00036; List B: t(13) = 3.16, p = 0.0075). See <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>.</p></sec><sec sec-type="appendix" id="s8-3"><title>ERP effect</title><p>We computed the mean activation within the time windows and electrodes of interest and compared the effects of word type and list using a two-way ANOVA. For the difference between 47 Words and Part-words over the positive cluster, we observed a main effect of word type (F(1,31) = 5.902, p = 0.021), with no effects of list or interactions (ps &gt; 0.1). Over the negative cluster, we again observed a main effect of word type (F(1,31) = 10.916, p = 0.0016), with no effects of list or interactions (ps &gt; 0.1). See <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Transition probabilities (TPs) across the structured stream in Experiment 2, considering voices processed by gender (Female or Male).</title><p>Top: List A. Bottom: List B.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Neural entrainment at word rate (2Hz) during the structured stream of Experiment 2 for Lists A and B.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Difference in ERP voltage (Words – Part-words) for the twPo lists (A and B); W=Words; P=Part-Words.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-app1-fig3-v1.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Voice stimuli.</title><p>properties of the six voices used to construct the stimuli. The Voice column indicates the name of the voice from the MBROLA diphone database voice used.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Voice</th><th align="left" valign="bottom">Gender</th><th align="left" valign="bottom">Pitch (Hz)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Ma</td><td align="left" valign="bottom">fr3</td><td align="left" valign="bottom">Male</td><td align="left" valign="bottom">75</td></tr><tr><td align="left" valign="bottom">Mb</td><td align="left" valign="bottom">fr1</td><td align="left" valign="bottom">Male</td><td align="left" valign="bottom">108</td></tr><tr><td align="left" valign="bottom">Mc</td><td align="left" valign="bottom">fr7</td><td align="left" valign="bottom">Male</td><td align="left" valign="bottom">140</td></tr><tr><td align="left" valign="bottom">Fa</td><td align="left" valign="bottom">fr2</td><td align="left" valign="bottom">Female</td><td align="left" valign="bottom">133</td></tr><tr><td align="left" valign="bottom">Fb</td><td align="left" valign="bottom">it4</td><td align="left" valign="bottom">Female</td><td align="left" valign="bottom">190</td></tr><tr><td align="left" valign="bottom">Fc</td><td align="left" valign="bottom">fr4</td><td align="left" valign="bottom">Female</td><td align="left" valign="bottom">247</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Stimuli.</title><p>Words and Part-words for each experiment and list.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" colspan="4">Experiment 1</th><th align="left" valign="bottom" colspan="4">Experiment 2</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="2">List A</td><td align="left" valign="bottom" colspan="2">List B</td><td align="left" valign="bottom" colspan="2">List A</td><td align="left" valign="bottom" colspan="2">List B</td></tr><tr><td align="left" valign="bottom">Word</td><td align="left" valign="bottom">Part-word</td><td align="left" valign="bottom">Word</td><td align="left" valign="bottom">Part-word</td><td align="left" valign="bottom">Word</td><td align="left" valign="bottom">Part-word</td><td align="left" valign="bottom">Word</td><td align="left" valign="bottom">Part-word</td></tr><tr><td align="left" valign="bottom">kida</td><td align="left" valign="bottom">dape</td><td align="left" valign="bottom">dape</td><td align="left" valign="bottom">kida</td><td align="left" valign="bottom">FbMb</td><td align="left" valign="bottom">MbFc</td><td align="left" valign="bottom">MbFc</td><td align="left" valign="bottom">FbMb</td></tr><tr><td align="left" valign="bottom">petu</td><td align="left" valign="bottom">tubo</td><td align="left" valign="bottom">tubo</td><td align="left" valign="bottom">petu</td><td align="left" valign="bottom">FcFa</td><td align="left" valign="bottom">FaMa</td><td align="left" valign="bottom">FaMa</td><td align="left" valign="bottom">FcFa</td></tr><tr><td align="left" valign="bottom">bogɛ</td><td align="left" valign="bottom">gɛki</td><td align="left" valign="bottom">gɛki</td><td align="left" valign="bottom">bogɛ</td><td align="left" valign="bottom">MaMc</td><td align="left" valign="bottom">McFb</td><td align="left" valign="bottom">McFb</td><td align="left" valign="bottom">MaMc</td></tr></tbody></table></table-wrap></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101802.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Herrmann</surname><given-names>Björn</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Baycrest Hospital</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The manuscript provides <bold>important</bold> new insights into the mechanisms of statistical learning in early human development, showing that statistical learning in neonates occurs robustly and is not limited to linguistic features but occurs across different domains. The evidence is <bold>convincing</bold> and the findings are highly relevant for researchers working in several domains, including developmental cognitive neuroscience, developmental psychology, linguistics, and speech pathology.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101802.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Parsing speech into meaningful linguistic units is a fundamental yet challenging task that infants face while acquiring the native language. Computing transitional probabilities (TPs) between syllables is a segmentation cue well-attested since birth. In this research, the authors examine whether newborns compute TPs over any available speech feature (linguistic and non-linguistic), or whether by contrast newborns favor computation of TPs over linguistic content over non-linguistic speech features such as speaker voice. Using EEG and the artificial language learning paradigm, they record the neural responses of two groups of newborns presented with speech streams in which either phonetic content or speaker voice are structured to provide TPs informative of word boundaries, while the other dimension provides uninformative information. They compare newborns' neural responses to these structured streams to their processing of a stream in which both dimensions vary randomly. After the random and structured familiarization streams, the newborns are presented with (pseudo)words as defined by their informative TPs, as well as partwords (that is, sequences that straddle a word boundary), extracted from the same streams. Analysis of the neural responses show that while newborns neural activity entrained to the syllabic rate (2 Hz) when listening to the random and structured streams, it additionally entrained at the word rate (4 Hz) only when listening to the structured streams, finding no differential response between the streams structured around voice or phonetic information. Newborns showed also different neural activity in response to the words and part words. In sum, the study reveals that newborns compute TPs over linguistic and non-linguistic features of speech, these are calculated independently, and linguistic features do not lead to a processing advantage.</p><p>Strengths:</p><p>This interesting research furthers our knowledge of the scope of the statistical learning mechanism, which is confirmed to be a general-purpose powerful tool that allows humans to extract patterns of co-occurring events while revealing no apparent preferential processing for linguistic features. To answer its question, the study combines a highly replicated and well-established paradigm, i.e. the use of an artificial language in which pseudowords are concatenated to yield informative TPs to word boundaries, with a state-of-the-art EEG analysis, i.e. neural entrainment. The sample size of the groups is sufficient to ensure power, and the design and analysis are solid and have been successfully employed before.</p><p>Weaknesses:</p><p>There are no significant weaknesses to signal in the manuscript. However, in order to fully conclude that there is no obvious advantage for the linguistic dimension in neonates, future studies should pit both dimensions against each other, to determine whether statistical learning weighs linguistic and non-linguistic features equally, or whether phonetic content is preferentially processed.</p><p>To sum up, the authors achieved their central aim of determining whether TPs are computed over both linguistic and non-linguistic features, and their conclusions are supported by the results. This research is important for researchers working on language and cognitive development, and language processing, as well as for those working on cross-species comparative approaches.</p><p>Comments on revisions:</p><p>The authors have addressed my suggestions. I have no further comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101802.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The manuscript investigates to what degree neonates show evidence for statistical learning from regularities in streams of syllables, either with respect to phonemes or with respect to speaker identity. Using EEG, the authors found evidence for both, stronger entrainment to regularities as well as ERP differences in response to violations of previously introduced regularities. In addition, violations of phoneme regularities elicited an ERP pattern which the authors argue might index a precursor of the N400 response in older children and adults.</p><p>Strengths:</p><p>All in all, this is a very convincing paper, which uses a clever manipulation of syllable streams to target the processing of different features. The combination of neural entrainment and ERP analysis allows for the assessment of different processing stages, and implementing this paradigm in a comparably large sample of neonates is impressive.</p><p>Weaknesses</p><p>The authors addressed all the concerns I previously raised well and I have no further comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101802.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study is focused on testing whether statistical learning (a mechanism for parsing the speech signal into smaller chunks) preferentially operates over certain features of the speech at birth in humans. The features under investigation are phonetic content and speaker identity. Newborns are tested in an EEG paradigm in which they are exposed to a long stream of syllables. In Experiment 1, newborns are familiarized with a sound stream that comprises regularities (transitional probabilities) over syllables (e.g., &quot;pe&quot; followed by &quot;tu&quot; in &quot;petu&quot; with 1.0 probability) while the voices uttering the syllables remain random. In Experiment 2, newborns are familiarized with the same sound stream but, this time, the regularities are built over voices (e.g., &quot;green voice&quot; followed by &quot;red voice&quot; with 1.0 probability) while the concatenation of syllables stays random. At the test, all newborns listened to duplets (individual chunks) that either matched or violated the structure of the familiarization. In both experiments, newborns showed neural entrainment to the regularities implemented in the stream, but only the duplets defined by transitional probabilities over syllables (aka word forms) elicited a N400 ERP component. These results suggest that statistical learning operates in parallel and independently on different dimensions of the speech already at birth and that there seems to be an advantage for processing statistics defining word forms rather than voice patterns.</p><p>Strengths:</p><p>This paper presents an original experimental design that combines two types of statistical regularities in a speech input. The design is robust and appropriate for EEG with newborns. I appreciated the clarity of the Methods section. There is also a behavioral experiment with adults that acts like a control study for newborns. The research question is interesting, and the results add new information about how statistical learning works at the beginning of postnatal life, and on which features of the speech. The figures are clear and helpful in understanding the methods, especially the stimuli and how the regularities were implemented.</p><p>Weaknesses:</p><p>I appreciated how the authors addressed my previous comments and concerns. I am satisfied with the changes made by the authors. I believe the paper reads much better. Also, the adjustment to the theoretical framework suits well.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101802.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Flo</surname><given-names>Ana</given-names></name><role specific-use="author">Author</role><aff><institution>University of Padova</institution><addr-line><named-content content-type="city">Padova</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>Benjamin</surname><given-names>Lucas</given-names></name><role specific-use="author">Author</role><aff><institution>CNRS ERL 9003, INSERM U992</institution><addr-line><named-content content-type="city">Paris-Saclay</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Palu</surname><given-names>Marie</given-names></name><role specific-use="author">Author</role><aff><institution>INSERM U992</institution><addr-line><named-content content-type="city">Paris-Saclay</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name><role specific-use="author">Author</role><aff><institution>Université Paris- Saclay</institution><addr-line><named-content content-type="city">Paris-Saclay</named-content></addr-line><country>France</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>We thank the three reviewers for their positive comments and useful suggestions. We have implemented most of the reviewers’ recommendations and hope the manuscript is clearer now.</p><p>The main modifications are:</p><p>- A revision of the introduction to better explain what Transitional Probabilities are and clarify the rationale of the experimental design</p><p>- A revision of the discussion</p><p>- To tune down and better explain the interpretation of the different responses between duplets after a stream with phonetic or voice regularities (possibly an N400).</p><p>- To better clarify the framing of statistical learning as a universal learning mechanism that might share computational principles across features (or domains).</p><p>Below, we provide detailed answers to each reviewer's point.</p><disp-quote content-type="editor-comment"><p><bold>Response to Reviewer 1:</bold></p><p>There are no significant weaknesses to signal in the manuscript. However, in order to fully conclude that there is no obvious advantage for the linguistic dimension in neonates, it would have been most useful to test a third condition in which the two dimensions were pitted against each other, that is, in which they provide conflicting information as to the boundaries of the words comprised in the artificial language.</p><p>This last condition would have allowed us to determine whether statistical learning weighs linguistic and non-linguistic features equally, or whether phonetic content is preferentially processed.</p></disp-quote><p>We appreciate the reviewers' suggestion that a stream with conflicting information would provide valuable insights. In the present study, we started with a simpler case involving two orthogonal features (i.e., phonemes and voices), with one feature being informative and the other uninformative, and we found similar learning capacities for both. Future work should explore whether infants—and humans more broadly—can simultaneously track regularities in multiple speech features. However, creating a stream with two conflicting statistical structures is challenging. To use neural entrainment, the two features must lead to segmentation at different chunk sizes so that their effects lead to changes in power/PLV at different frequencies—for instance, using duplets for the voice dimension and triplets for the linguistic dimension (or vice versa). Consequently, the two dimensions would not be directly comparable within the same participant in terms of the number of distinguishable syllables/voices, memory demand, or SNR given the 1/F decrease in amplitude of background EEG activity. This would involve comparisons between two distinct groups counter-balancing chunk size and linguistic non-linguistic dimension. Considering the test phase, words for one dimension would have been part-words for the other dimension. As we are measuring differences and not preferences, interpreting the results would also have been difficult. Additionally, it may be difficult to find a sufficient number of clearly discriminable voices for such a design (triplets imply 12 voices). Therefore, an entirely different experimental paradigm would need to be developed.</p><p>If such a design were tested, one possibility is that the regularities for the two dimensions are calculated in parallel, in line with the idea that the calculation of statistical regularities is a ubiquitous implicit mechanism (see Benjamin et al., 2024, for a proposed neural mechanism). Yet, similar to our present study, possibly only phonetic features would be used as word candidates. Another possibility is that only one informative feature would be explicitly processed at a time due to the serial nature of perceptual awareness, which may prioritise one feature over the other.</p><p>We added one sentence in the discussion stating that more research is needed to understand whether infants can track both regularities simultaneously (p.13, l.270 “Future work could explore whether they can simultaneously track multiple regularities.”).</p><p>Note: The reviewer’s summary contains a typo: syllabic rate (4 Hz) –not 2 Hz, and word rate (2 Hz) –not 4 Hz.</p><disp-quote content-type="editor-comment"><p><bold>Response to Reviewer 2:</bold></p><p>N400: I am skeptical regarding the interpretation of the phoneme-specific ERP effect as a precursor of the N400 and would suggest toning it down. While the authors are correct in that infant ERP components are typically slower and more posterior compared to adult components, and the observed pattern is hence consistent with an adult N400, at the same time, it could also be a lot of other things. On a functional level, I can't follow the author's argument as to why a violation in phoneme regularity should elicit an N400, since there is no evidence for any semantic processing involved. In sum, I think there is just not enough evidence from the present paradigm to confidently call it an N400.</p></disp-quote><p>The reviewer is correct that we cannot definitively determine the type of processing reflected by the ERP component that appears when neonates hear a duplet after exposure to a stream with phonetic regularities. We interpreted this component as a precursor to the N400, based on prior findings in speech segmentation tasks without semantic content, where a ~400 ms component emerged when adult participants recognised pseudowords (Sander et al., 2002) or during structured streams of syllables (Cunillera et al., 2006, 2009). Additionally, the component we observed had a similar topography and timing to those labelled as N400 in infant studies, where semantic processing was involved (Parise et al., 2010; Friedrich &amp; Friederici, 2011).</p><p>Given our experimental design, the difference we observed must be related to the type of regularity during familiarisation (either phonemes or voices). Thus, we interpreted this component as reflecting lexical search— a process which could be triggered by a linguistic structure but which would not be relevant to a non-linguistic regularity such as voices. However, we are open to alternative interpretations. In any case, this difference between the two streams reveals that computing regularities based on phonemes versus voices does not lead to the same processes.</p><p>We revised the abstract (p.2, l.33) and the discussion of this result (p.15, l.299), toning them down. We hope the rationale of the interpretation is clearer now, as is the fact that it is just one possible interpretation of the results.</p><disp-quote content-type="editor-comment"><p>Female and male voices: Why did the authors choose to include male and female voices? While using both female and male stimuli of course leads to a higher generalizability, it also introduces a second dimension for one feature that is not present for this other (i.e., phoneme for Experiment 1 and voice identity plus gender for Experiment 2). Hence, couldn't it also be that the infants extracted the regularity with which one gender voice followed the other? For instance, in List B, in the words, one gender is always followed by the other (M-F or F-M), while in 2/3 of the part-words, the gender is repeated (F-F and M-M). Wouldn't you expect the same pattern of results if infants learned regularities based on gender rather than identity?</p></disp-quote><p>We used three female and three male voices to maximise acoustic variability. The streams were synthesised using MBROLA, which provides a limited set of artificial voices. Indeed, there were not enough French voices of acceptable quality, so we also used two Italian voices (the phonemes used existed in both Italian and French).</p><p>Voices differ in timbre, and female voices tend to be higher pitched. However, it is sometimes difficult to categorise low-pitched female voices and high-pitched male voices. Given that gender may be an important factor in infants' speech perception (newborns, for instance, prefer female voices at birth), we conducted tests to assess whether this dimension could have influenced our results.</p><p>We report these analyses in SI and referred to them in the methods section (p.25, l.468 “We performed post-hoc tests to ensure that the results were not driven by a perception of two voices: female and male (see SI).”).</p><p>We first quantified the transitional probabilities matrices during the structured stream of Experiment 2, considering that there are only two types of voices: Female and Male.</p><p>For List A, all transition probabilities are equal to 0.5 (P(M|F), P(F|M), P(M|M), P(F|F)), resulting in flat TPs throughout the stream (see Author response image 1, top). Therefore, we would not expect neural entrainment at the word rate (2 Hz), nor would we anticipate ERP differences between the presented duplets in the test phase.</p><p>For List B, P(M|F)=P(F|M)=0.66 while P(M|M)=P(F|F)=0.33. However, this does not produce a regular pattern of TP drops throughout the stream (see Author response image 1, bottom). As a result, strong neural entrainment at 2 Hz was unlikely, although some degree of entrainment might have occasionally occurred due to some drops occurring at a 2 Hz frequency. Regarding the test phase, all three Words and only one Part-word presented alternating patterns (TP=0.6). Therefore, the difference in the ERPs between Words and Part- words in List B might be attributed to gender alternation.</p><p>However, it seems unlikely that gender alternation alone explains the entire pattern of results, as the effect is inconsistent and appears in only one of the lists. To rule out this possibility, we analysed the effects in each list separately.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><caption><title>Transition probabilities (TPs) across the structured stream in Experiment 2, considering voices processed by gender (Female or Male).</title><p>Top: List A. Bottom: List B.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-sa4-fig1-v1.tif"/></fig><p>We computed the mean activation within the time windows and electrodes of interest and compared the effects of word type and list using a two-way ANOVA. For the difference between Words and Part-words over the positive cluster, we observed a main effect of word type (F(1,31) = 5.902, p = 0.021), with no effects of list or interactions (p &gt; 0.1). Over the negative cluster, we again observed a main effect of word type (F(1,31) = 10.916, p = 0.0016), with no effects of list or interactions (p &gt; 0.1). See Author response image 2.</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><caption><title>Difference in ERP voltage (Words – Part-words) for the two lists (A and B); W=Words; P=Part-Words.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-sa4-fig2-v1.tif"/></fig><p>We conducted a similar analysis for neural entrainment during the structured stream on voices. A comparison of entrainment at 2 Hz between participants who completed List A and List B showed no significant differences (t(30) = -0.27, p = 0.79). A test against zero for each list indicated significant entrainment in both cases (List A: t(17) = 4.44, p = 0.00036; List B: t(13) = 3.16, p = 0.0075). See Author response image 3.</p><fig id="sa4fig3" position="float"><label>Author response image 3.</label><caption><title>Neural entrainment at 2Hz during the structured stream of Experiment 2 for Lists A and B.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-sa4-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>Words entrainment over occipital electrodes: Do you have any idea why the duplet entrainment effect occurs over the electrodes it does, in particular over the occipital electrodes (which seems a bit unintuitive given that this is a purely auditory experiment with sleeping neonates).</p></disp-quote><p>Neural entrainment might be considered as a succession of evoked response induced by the stream. After applying an average reference in high-density EEG recordings, the auditory ERP in neonates typically consists of a central positivity and a posterior negativity with a source located at the electrical zero in a single-dipole model i.e. approximately in the superior temporal region (Dehaene-Lambertz &amp; Dehaene, 1994). In adults, because of the average reference (i.e. the sum of voltages is equal to zero at each time point) and because the electrodes cannot capture the negative pole of the auditory response, the negativity is distributed around the head. In infants, however, the brain is higher within the skull, allowing for a more accurate recording of the negative pole of the auditory ERP (see Figure 4 for the location of electrodes in an infant head model).</p><p>Besides the posterior electrodes, we can see some entrainment on more anterior electrodes that probably corresponds to the positive pole of the auditory ERP.</p><p>We added a phrase in the discussion to explain why we can expect phase-locked activity in posterior electrodes (p.14, l.277: “Auditory ERPs, after reference-averaged, typically consist of a central positivity and posterior negativity”).</p><fig id="sa4fig4" position="float"><label>Author response image 4.</label><caption><title>International 10–20 sensors' location on the skull of an infant template, with the underlying 3-D reconstruction of the grey-white matter interface and projection of each electrode to the cortex.</title><p>Computed across 16 infants (from Kabdebon et al, Neuroimage, 2014). The O1, O2, T5, and T6 electrodes project lower than in adults.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101802-sa4-fig4-v1.tif"/></fig><disp-quote content-type="editor-comment"><p><bold>Response to Reviewer 3:</bold></p><p>(1) While it's true that voice is not essential for language (i.e., sign languages are implemented over gestures; the use of voices to produce non-linguistic sounds, like laughter), it is a feature of spoken languages. Thus I'm not sure if we can really consider this study as a comparison between linguistic and non-linguistic dimensions. In turn, I'm not sure that these results show that statistical learning at birth operates on non-linguistic features, being voices a linguistic dimension at least in spoken languages. I'd like to hear the authors' opinions on this.</p></disp-quote><p>On one hand, it has been shown that statistical learning (SL) operates across multiple modalities and domains in human adults and animals. On the other hand, SL is considered essential for infants to begin parsing speech. Therefore, we aimed to investigate whether SL capacities at birth are more effective on linguistic dimensions of speech, potentially as a way to promote language learning.</p><p>We agree with the reviewer that voices play an important role in communication (e.g., for identifying who is speaking); however, they do not contribute to language structure or meaning, and listeners are expected to normalize across voices to accurately perceive phonemes and words. Thus, voices are speech features but not linguistic features. Additionally, in natural speech, there are no abrupt voice changes within a word as in our experiment; instead, voice changes typically occur on a longer timescale and involve only a limited number of voices, such as in a dialogue. Therefore, computing regularities based on voice changes would not be useful in real-life language learning. We considered that contrasting syllables and voices was an elegant way to test SL beyond its linguistic dimension, as the experimental paradigm is identical in both experiments.</p><p>We have rephrased the introduction to make this point clearer. See p.5, l.88-92: “To test this, we have taken advantage of the fact that syllables convey two important pieces of information for humans: what is being said and who is speaking, i.e. linguistic content and speaker’s identity. While statistical learning…”.</p><disp-quote content-type="editor-comment"><p>Along the same line, in the Discussion section, the present results are interpreted within a theoretical framework showing statistical learning in auditory non-linguistic (string of tones, music) and visual domains as well as visual and other animal species. I'm not sure if that theoretical framework is the right fit for the present results.</p><p>(2) I'm not sure whether the fact that we see parallel and independent tracking of statistics in the two dimensions of speech at birth indicates that newborns would be able to do so in all the other dimensions of the speech. If so, what other dimensions are the authors referring to?</p></disp-quote><p>The reviewer is correct that demonstrating the universality of SL requires testing additional modalities and acoustic dimensions. However, we postulate that SL is grounded in a basic mechanism of long-term associative learning, as proposed in Benjamin et al. (2024), which relies on a slow decay in the representation of a given event. This simple mechanism, capable of operating on any representational output, accounts for many types of sequence learning reported in the literature (Benjamin et al., in preparation).</p><p>We have revised the discussion to clarify this theoretical framework.</p><p>In p.13, l.264: “This mechanism might be rooted in associative learning processes relying on the co- existence of event representations driven by slow activation decays (Benjamin et al., 2024). ”</p><p>In p., l. 364: “Altogether, our results show that statistical learning works similarly on different speech features in human neonates with no clear advantage for computing linguistically relevant regularities in speech. This supports the idea that statistical learning is a general learning mechanism, probably operating on common computational principles across neural networks (Benjamin et al., 2024)…”.</p><disp-quote content-type="editor-comment"><p>(3) Lines 341-345: Statistical learning is an evolutionary ancient learning mechanism but I do not think that the present results are showing it. This is a study on human neonates and adults, there are no other animal species involved therefore I do not see a connection with the evolutionary history of statistical learning. It would be much more interesting to make claims on the ontogeny (rather than philogeny) of statistical learning, and what regularities newborns are able to detect right after birth. I believe that this is one of the strengths of this work.</p></disp-quote><p>We did not intend to make claims about the phylogeny of SL. Since SL appears to be a learning mechanism shared across species, we use it as a framework to suggest that SL may arise from general operational principles applicable to diverse neural networks. Thus, while it is highly useful for language acquisition, it is not specific to it.</p><p>We have removed the sentence “Statistical learning is an evolutionary ancient learning mechanism.”, and replaced it by (p.18, l.364) “Altogether, our results show that statistical learning works similarly on different speech features in human neonates with no clear advantage for computing linguistically relevant regularities in speech.” We now emphasise in the discussion that infants compute regularities on both features and propose that SL might be a universal learning mechanism sharing computational principles (Benjamin et al., 2024) (see point 2).</p><disp-quote content-type="editor-comment"><p>(4) The description of the stimuli in Lines 110-113 is a bit confusing. In Experiment 1, e.g., &quot;pe&quot; and &quot;tu&quot; are both uttered by the same voice, correct? (&quot;random voice each time&quot; is confusing). Whereas in Experiment 2, e.g., &quot;pe&quot; and &quot;tu&quot; are uttered by different voices, for example, &quot;pe&quot; by yellow voice and &quot;tu&quot; by red voice. If this is correct, then I recommend the authors to rephrase this section to make it more clear.</p></disp-quote><p>To clarify, in Experiment 1, the voices were randomly assigned to each syllable, with the constraint that no voice was repeated consecutively. This means that syllables within the same word were spoken by different voices, and each syllable was heard with various voices throughout the stream. As a result, neonates had to retrieve the words based solely on syllabic patterns, without relying on consistent voice associations or specific voice relationships.</p><p>In Experiment 2, the design was orthogonal: while the syllables were presented in a random order, the voices followed a structured pattern. Similar to Experiment 1, each syllable (e.g., “pe” and “tu”) was spoken by different voices. The key difference is that in Experiment 2, the structured regularities were applied to the voices rather than the syllables. In other words, the “green” voice was always followed by the “red” voice for example but uttered different syllables.</p><p>We have revised the description of the stimuli and the legend of Figure 1 to clarify these important points.</p><p>See p.6, l. 113: “The structure consisted of the random concatenation of three duplets (i.e., two-syllable units) defined only by one of the two dimensions. For example, in Experiment 1, one duplet could be petu with each syllable uttered by a random voice each time they appear in the stream (e.g pe is produced by voice1 and tu by voice6 in one instance and in another instance pe is produced by voice3 and tu by voice2). In contrast, in Experiment 2, one duplet could be the combination [voice1- voice6], each uttering randomly any of the syllables.”</p><p>p.20, l. 390 (Figure 1 legend): “For example, the two syllables of the word “petu” were produced by different voices, which randomly changed at each presentation of the word (e.g. “yellow” voice and “green” voice for the first instance, “blue” and “purple” voice for the second instance, etc..). In Experiment 2, the statistical structure was based on voices (TPs alternated between 1 and 0.5), while the syllables changed randomly (uniform TPs of 0.2). For example, the “green” voice was always followed by the “red” voice, but they were randomly saying different syllables “boda” in the first instance, “tupe” in the second instance, etc... “</p><disp-quote content-type="editor-comment"><p>(5) Line 114: the sentence &quot;they should compute a 36 x 36 TPs matrix relating each acoustic signal, with TPs alternating between 1/6 within words and 1/12 between words&quot; is confusing as it seems like there are different acoustic signals. Can the authors clarify this point?</p></disp-quote><p>Thank you for highlighting this point. To clarify, our suggestion is that neonates might not track regularities between phonemes and voices as separate features. Instead, they may treat each syllable-voice combination as a distinct item—for example, &quot;pe&quot; spoken by the &quot;yellow&quot; voice is one item, while &quot;pe&quot; spoken by the &quot;red&quot; voice is another. Under this scenario, there would be a total of 36 unique items (6 syllables × 6 voices), and infants would need to track regularities between these 36 combinations.</p><p>We have modified this sentence in the manuscript to make it clearer.</p><p>See p.7, l. 120: “If infants at birth compute regularities based on a neural representation of the syllable as a whole, i.e. comprising both phonetic and voice content, this would require computing a 36 × 36 TPs matrix relating each token.”</p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><disp-quote content-type="editor-comment"><p>(1) The acronym TP should be spelled out, and a brief description of the fact that dips in TPs signal boundaries while high TPs signal a cohesive unit could be useful for non-specialist readers.</p></disp-quote><p>We have added it at the beginning of the introduction (lines 52-60)</p><disp-quote content-type="editor-comment"><p>(2) p.5, l.76: &quot;Here, we aimed to further characterise the characteristics of this mechanism...&quot;. I suggest this is rephrased as &quot;to further characterise this mechanism&quot;.</p></disp-quote><p>We have changed it as suggested by the reviewer (now p.5, l.81)</p><disp-quote content-type="editor-comment"><p>(3) p.9, l.172: &quot;[...] this contribution is unlikely since the electrodes differ from the electrodes, showing enhanced word-rate activity at 2 Hz.&quot;</p><p>It is unclear which electrodes differ from which electrodes. I figure that the authors mean that the electrodes showing stronger activity at 2 Hz differ from those showing it at 4 Hz, but the sentence could use rephrasing.</p></disp-quote><p>This part has been rephrased (p.9, l.177-181)</p><disp-quote content-type="editor-comment"><p>(4) p.10, l.182: &quot;[...] the entrainment during the first minute of the structure stream […]&quot;.</p><p>Structured stream.</p></disp-quote><p>It has been corrected (p.10, l.190)</p><disp-quote content-type="editor-comment"><p>(5) p.12, l.234: &quot;we compared STATISTICAL LEARNING&quot;</p><p>Why the use of capitals?</p></disp-quote><p>This was an error and it was corrected (p.12, l.242).</p><disp-quote content-type="editor-comment"><p>(6) p.15, l.298: &quot;[...] suggesting that such negativity might be related to semantic.&quot;</p><p>The sentence feels incomplete. To semantics? To the processing of semantic information?</p></disp-quote><p>The phrase has been corrected (p.15, l.314). Additionally, the discussion of the posterior negativity observed for duplets after familiarisation with a stream with regularities over phonemes has been rephrased (p.15, l.)</p><disp-quote content-type="editor-comment"><p>(7) Same page, l.301: &quot;3-mo-olds&quot; 3-month-olds.</p></disp-quote><p>It has been corrected (now in p.16, l.333)</p><disp-quote content-type="editor-comment"><p>(8) Same page, l.307: &quot;see also (Bergelson and Aslin, 2017)&quot; (see also Bergelson and Aslin, 2017).</p></disp-quote><p>It has been corrected (now in p.17, l.340)</p><disp-quote content-type="editor-comment"><p>(9) Same page, l.310: &quot;[...] would be considered as possible candidate&quot; As possible candidates.</p></disp-quote><p>This has been rephrased and corrected (now in p.17, l.343)</p><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><disp-quote content-type="editor-comment"><p>(1) Figure 2: The authors mention a &quot;thick orange line&quot;, which I think should be a &quot;thick black line&quot;.</p></disp-quote><p>We are sorry for this. It has been corrected.</p><disp-quote content-type="editor-comment"><p>(2) Ln 166: Should be Figure 2C rather than 3C.</p></disp-quote><p>It has been corrected (now in p.9, l.173)</p><disp-quote content-type="editor-comment"><p>(3) Figure 4 is not referenced in the manuscript.</p></disp-quote><p>We referred to it now on p. 12, l.236</p></body></sub-article></article>