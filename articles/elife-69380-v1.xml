<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">69380</article-id><article-id pub-id-type="doi">10.7554/eLife.69380</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Developmental Biology</subject></subj-group></article-categories><title-group><article-title>Tracking cell lineages in 3D by incremental deep learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-236296"><name><surname>Sugawara</surname><given-names>Ko</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1392-9340</contrib-id><email>ko.sugawara@ens-lyon.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-236297"><name><surname>Çevrim</surname><given-names>Çağrı</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4720-7944</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-8058"><name><surname>Averof</surname><given-names>Michalis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6803-7251</contrib-id><email>michalis.averof@ens-lyon.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Institut de Génomique Fonctionnelle de Lyon (IGFL), École Normale Supérieure de Lyon</institution><addr-line><named-content content-type="city">Lyon</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution>Centre National de la Recherche Scientifique (CNRS)</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution>École Normale Supérieure</institution><country>France</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>01</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e69380</elocation-id><history><date date-type="received" iso-8601-date="2021-04-13"><day>13</day><month>04</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-12-07"><day>07</day><month>12</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-02-26"><day>26</day><month>02</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.02.26.432552"/></event></pub-history><permissions><copyright-statement>© 2022, Sugawara et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Sugawara et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-69380-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-69380-figures-v1.pdf"/><abstract><p>Deep learning is emerging as a powerful approach for bioimage analysis. Its use in cell tracking is limited by the scarcity of annotated data for the training of deep-learning models. Moreover, annotation, training, prediction, and proofreading currently lack a unified user interface. We present ELEPHANT, an interactive platform for 3D cell tracking that addresses these challenges by taking an incremental approach to deep learning. ELEPHANT provides an interface that seamlessly integrates cell track annotation, deep learning, prediction, and proofreading. This enables users to implement cycles of incremental learning starting from a few annotated nuclei. Successive prediction-validation cycles enrich the training data, leading to rapid improvements in tracking performance. We test the software’s performance against state-of-the-art methods and track lineages spanning the entire course of leg regeneration in a crustacean over 1 week (504 timepoints). ELEPHANT yields accurate, fully-validated cell lineages with a modest investment in time and effort.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cell tracking</kwd><kwd>cell lineage</kwd><kwd>deep learning</kwd><kwd>regeneration</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2015-AdG #694918</award-id><principal-award-recipient><name><surname>Sugawara</surname><given-names>Ko</given-names></name><name><surname>Averof</surname><given-names>Michalis</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001645</institution-id><institution>Boehringer Ingelheim Fonds</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Çevrim</surname><given-names>Çağrı</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Incremental deep learning enables accurate tracking of cell lineages in 3D with a modest investment in time and effort.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Recent progress in deep learning has led to significant advances in bioimage analysis (<xref ref-type="bibr" rid="bib24">Moen et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Ouyang et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Weigert et al., 2018</xref>). As deep learning is data-driven, it is adaptable to a variety of datasets once an appropriate model architecture is selected and trained with adequate data (<xref ref-type="bibr" rid="bib24">Moen et al., 2019</xref>). In spite of its powerful performance, deep learning remains challenging for non-experts to utilize, for three reasons. First, pre-trained models can be inadequate for new tasks and the preparation of new training data is laborious. Because the quality and quantity of the training data are crucial for the performance of deep learning, users must invest significant time and effort in annotation at the start of the project (<xref ref-type="bibr" rid="bib24">Moen et al., 2019</xref>). Second, an interactive user interface for deep learning, especially in the context of cell tracking, is lacking (<xref ref-type="bibr" rid="bib16">Kok et al., 2020</xref>; <xref ref-type="bibr" rid="bib45">Wen et al., 2021</xref>). Third, deep learning applications are often limited by accessibility to computing power (high-end GPU).</p><p>We have addressed these challenges by establishing ELEPHANT (<underline>E</underline>fficient <underline>le</underline>arning using s<underline>p</underline>arse <underline>h</underline>uman <underline>a</underline>nnotations for <underline>n</underline>uclear <underline>t</underline>racking), an interactive web-friendly platform for cell tracking, which seamlessly integrates manual annotation with deep learning and proofreading of the results. ELEPHANT implements two algorithms optimized for incremental deep learning using sparse annotations, one for detecting nuclei in 3D and a second for linking these nuclei across timepoints in 4D image datasets. Incremental learning allows models to be trained in a stepwise fashion on a given dataset, starting from sparse annotations that are incrementally enriched by human proofreading, leading to a rapid increase in performance (<xref ref-type="fig" rid="fig1">Figure 1</xref>). ELEPHANT is implemented as an extension of Mastodon (<ext-link ext-link-type="uri" xlink:href="https://github.com/mastodon-sc/mastodon">https://github.com/mastodon-sc/mastodon</ext-link>; <xref ref-type="bibr" rid="bib22">Mastodon Science, 2021</xref>), an open-source framework for large-scale tracking deployed in Fiji (<xref ref-type="bibr" rid="bib33">Schindelin et al., 2012</xref>). It implements a client-server architecture, in which the server provides a deep learning environment equipped with sufficient GPU (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Conventional and incremental deep learning workflows for cell tracking.</title><p>(<bold>A</bold>) Schematic illustration of a typical deep learning workflow, starting with the annotation of imaging data to generate training datasets, training of deep learning models, prediction by deep learning and proofreading. (<bold>B</bold>) Schematic illustration of incremental learning with ELEPHANT. Imaging data are fed into a cycle of annotation, training, prediction, and proofreading to generate cell lineages. At each iteration, model parameters are updated and saved. This workflow applies to both detection and linking phases (see <xref ref-type="fig" rid="fig2">Figures 2A</xref> and <xref ref-type="fig" rid="fig4">4A</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>ELEPHANT client-server architecture.</title><p>The client provides an interactive user interface for annotation, proofreading, and visualization. The server performs training and prediction with deep learning. The client and server communicate using HTTP and JSON.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Block diagram of ELEPHANT tracking workflow.</title><p>A block diagram showing how the entire tracking workflow can be performed using ELEPHANT.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig1-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2" sec-type="results|discussion"><title>Results and discussion</title><p>ELEPHANT employs the tracking-by-detection paradigm (<xref ref-type="bibr" rid="bib21">Maška et al., 2014</xref>), which involves initially the <italic>detection</italic> of nuclei in 3D and subsequently their <italic>linking</italic> over successive timepoints to generate tracks. In both steps, the nuclei are represented as ellipsoids, using the data model of Mastodon (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and Figure 4A). We use ellipsoids for annotation because ellipsoids allow rapid and efficient training and prediction, compared with more complex shapes. This is essential for interactive deep learning. In the detection phase, voxels are labelled as <italic>background</italic>, <italic>nucleus center</italic> or <italic>nucleus periphery</italic>, or left unlabelled (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, top right). The <italic>nucleus center</italic> and <italic>nucleus periphery</italic> labels are generated by the annotation of nuclei, and the <italic>background</italic> can be annotated either manually or by intensity thresholding. Sparse annotations (e.g. of a few nuclei in a single timepoint) are sufficient to start training. A U-Net convolutional neural network (U-Net CNN; <xref ref-type="bibr" rid="bib4">Cicek et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Ronneberger et al., 2015</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) is then trained on these labels (ignoring the unlabelled voxels) to generate voxel-wise probability maps for <italic>background</italic>, <italic>nucleus center</italic>, or <italic>nucleus periphery</italic>, across the entire image dataset (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom right). Post-processing on these probability maps yields predictions of nuclei which are available for visual inspection and proofreading (validation or rejection of each predicted nucleus) by the user (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom left). Human-computer interaction is facilitated by color coding of the annotated nuclei as predicted (green), accepted (cyan), or rejected (magenta) (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), based on the proofreading. The cycles of training and prediction are rapid because only a small amount of training data are added each time (in the order of seconds, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). As a result, users can enrich the annotations by proofreading the output almost simultaneously, enabling incremental training of the model in an efficient manner.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>ELEPHANT detection workflow.</title><p>(<bold>A</bold>) Detection workflow, illustrated with orthogonal views on the CE1 dataset. Top left: The user annotates nuclei with ellipsoids in 3D; newly generated annotations are colored in cyan. Top right: The detection model is trained with the labels generated from the sparse annotations of nuclei and from the annotation of <italic>background</italic> (in this case by intensity thresholding); <italic>background</italic>, <italic>nucleus center</italic>, <italic>nucleus periphery</italic> and unlabelled voxels are indicated in magenta, blue, green, and black, respectively. Bottom right: The trained model generates voxel-wise probability maps for <italic>background</italic> (magenta), <italic>nucleus center</italic> (blue), or <italic>nucleus periphery</italic> (green). Bottom left: The user validates or rejects the predictions; predicted nuclei are shown in green, predicted and validated nuclei in cyan. (<bold>B</bold>) Comparison of the speed of detection and validation of nuclei on successive timepoints in the CE1 dataset, by manual annotation (magenta), semi-automated detection without a pre-trained model (orange) and semi-automated detection using a pre-trained model (blue) using ELEPHANT.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>3D U-Net architecture for detection.</title><p>Schematic illustration of the 3D U-Net architecture for detection, using an input image with a size of 384 × 384 x 12 and a ratio of lateral-to-axial resolution of 8 as an example. Rectangles show the input/intermediate/output layers, with the sizes shown on the left of each row and the number of channels shown above each rectangle. Block arrows represent different operations as described in the figure. The resolution of the z dimension is maintained until the image becomes nearly isotropic (ratio of lateral-to-axial resolution of 1, in the bottom layers in this example).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Proofreading in detection.</title><p>The ellipses show the sections of nuclear annotations in the xy plane and the dots represent the projections of the center position of the annotated nuclei, drawn in distinct colors; color code explained on the right. Nuclei that are out of focus in this view appear only as dots.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We evaluated the detection performance of ELEPHANT on diverse image datasets capturing the embryonic development of <italic>Caenorhabditis elegans</italic> (CE1), leg regeneration in the crustacean <italic>Parhyale hawaiensis</italic> (PH), human intestinal organoids (ORG1 and ORG2) and human breast carcinoma cells (MDA231) by confocal or light sheet microscopy (<xref ref-type="fig" rid="fig3">Figure 3A–E</xref>). First, we tested the performance of a generic model that had been pre-trained with various annotated image datasets (<xref ref-type="fig" rid="fig3">Figure 3A–E</xref> top). We then annotated 3–10 additional nuclei or cells on each test dataset (<xref ref-type="fig" rid="fig3">Figure 3A–E</xref> middle) and re-trained the model. This resulted in greatly improved detection performance (<xref ref-type="fig" rid="fig3">Figure 3A–E</xref>), showing that a very modest amount of additional training on a given dataset can yield rapid improvements in performance. We find that sparsely trained ELEPHANT detection models have a comparable performance to state-of-the-art software (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) and fully trained ELEPHANT models outperform most tracking software (<xref ref-type="table" rid="table1">Table 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>ELEPHANT detection with sparse annotations.</title><p>Detection results obtained using ELEPHANT with sparse annotations on five image datasets recording the embryonic development of <italic>C.</italic> <italic>elegans</italic> (CE1 dataset, <bold>A</bold>), leg regeneration in the crustacean <italic>P. hawaiensis</italic> (PH dataset, <bold>B</bold>), human intestinal organoids (ORG1 and ORG2, <bold>C and D</bold>), and human breast carcinoma cells (MDA231, <bold>E</bold>). CE1, PH, ORG2, and MDA231 were captured by confocal microscopy; ORG1 was captured by light sheet microscopy. Top: Detection results using models that were pre-trained on diverse annotated datasets, excluding the test dataset (see <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Precision and Recall scores are shown at the bottom of each panel, with the number of true positive (TP), false positive (FP), and false negative (FN) predicted nuclei. Middle: Addition of sparse manual annotations for each dataset. n: number of sparse annotations. Scale bars: 10 µm. Bottom: Detection results with an updated model that used the sparse annotations to update the pre-trained model. Precision, Recall, TP, FP, and FN values are shown as in the top panels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Comparing detection predictions of ELEPHANT and StarDist3D.</title><p>ELEPHANT and StarDist3D models were trained with the CE1 dataset (timepoint = 100) and tested on the CE2 dataset (timepoint = 100). The ELEPHANT model was trained from scratch with sparse annotations (10 nuclei, as shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), whereas the StarDist3D model was trained with full annotations (93 nuclei). The cell instances segmented by StarDist3D are color-coded in Fiji. The DET scores were 0.975 (ELEPHANT) and 0.999 (StarDist3D), respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Evaluation of overfitting in detection using ELEPHANT.</title><p>An ELEPHANT detection model was trained with 10 sparse annotations for 500 training epochs starting from scratch. The top left panel shows a slice of image volume used for training, on which the sparse annotations are overlaid in cyan. The bottom left plot shows the loss curves in training and validation datasets. The top right and bottom right panels show the detection results on the CE1 (top) and CE2 (bottom) datasets, obtained using the model trained for 500 epochs, where the annotations and predictions are indicated with outlines in cyan and green colors. Precision and Recall scores are shown at the bottom of each panel, with the number of true positive (TP), false positive (FP), and false negative (FN) predicted nuclei.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig3-figsupp2-v1.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Performance of ELEPHANT on the Cell Tracking Challenge dataset.</title><p>Performance of ELEPHANT compared with two state-of-the-art algorithms, using the metrics of the Cell Tracking Challenge on unseen CE datasets. ELEPHANT outperforms the other methods in detection and linking accuracy (DET and TRA metrics); it performs less well in segmentation accuracy (SEG).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">ELEPHANT</th><th align="left" valign="top">KTH-SE</th><th align="left" valign="top">KIT-Sch-GE</th></tr></thead><tbody><tr><td align="left" valign="top"/><td align="char" char="hyphen" valign="top">(IGFL-FR)</td><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">SEG</td><td align="char" char="." valign="top">0.631</td><td align="char" char="." valign="top">0.662</td><td align="char" char="." valign="top">0.729</td></tr><tr><td align="left" valign="top">TRA</td><td align="char" char="." valign="top">0.975</td><td align="char" char="." valign="top">0.945</td><td align="char" char="." valign="top">0.886</td></tr><tr><td align="left" valign="top">DET</td><td align="char" char="." valign="top">0.979</td><td align="char" char="." valign="top">0.959</td><td align="char" char="." valign="top">0.930</td></tr></tbody></table></table-wrap><p>We also investigated whether training with sparse annotations could cause overfitting of the data in the detection model, by training the detection model using sparse annotations in dataset CE1 and calculating the loss values using a second, unseen but similar dataset (see Materials and methods). The training and validation learning curves did not show any signs of overfitting even after a large amount of training (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The trained model could detect nuclei with high precision and recall both on partially seen data (CE1) and unseen data (CE2). A detection model that has been pre-trained with diverse image datasets is available to users as a starting point for tracking on new image data (see Materials and methods).</p><p>In the linking phase, we found that nearest neighbor approaches for tracking nuclei over time (<xref ref-type="bibr" rid="bib5">Crocker and Grier, 1996</xref>) perform poorly in challenging datasets when the cells are dividing; hence we turned to optical flow modeling to improve linking (<xref ref-type="bibr" rid="bib2">Amat et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Horn and Schunck, 1981</xref>; <xref ref-type="bibr" rid="bib17">Lucas and Kanade, 1981</xref>). A second U-Net CNN, optimized for optical flow estimation (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), is trained on manually generated/validated links between nuclei in successive timepoints (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top left). Unlabelled voxels are ignored, hence training can be performed on sparse linking annotations. The flow model is used to generate voxel-wise 3D flow maps, representing predicted x, y and z displacements over time (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom right), which are then combined with nearest neighbor linking to predict links between the detected nuclei (see Materials and methods). Users proofread the linking results to finalize the tracks and to update the labels for the next iteration of training (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom left).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>ELEPHANT linking workflow.</title><p>(<bold>A</bold>) Linking workflow, illustrated on the CE1 dataset. Top left: The user annotates links by connecting detected nuclei in successive timepoints; annotated/validated nuclei and links are shown in cyan, non-validated ones in green. Top right: The flow model is trained with optical flow labels coming from annotated nuclei with links (voxels indicated in the label mask), which consist of displacements in X, Y, and Z; greyscale values indicate displacements along a given axis, annotated nuclei with link labels are outlined in red. Bottom right: The trained model generates voxel-wise flow maps for each axis; greyscale values indicate displacements, annotated nuclei are outlined in red. Bottom left: The user validates or rejects the predictions; predicted links are shown in green, predicted and validated links in cyan. (<bold>B</bold>) Tracking results obtained with ELEPHANT. Left panels: Tracked nuclei in the CE1 and CE2 datasets at timepoints 194 and 189, respectively. Representative optical sections are shown with tracked nuclei shown in green; out of focus nuclei are shown as green spots. Right panels: Corresponding lineage trees. (<bold>C</bold>) Comparison of tracking results obtained on the PH dataset, using the nearest neighbor algorithm (NN) with and without optical flow prediction (left panels); linking errors are highlighted in red on the correct lineage tree. The panels on the right focus on the nuclear division that is marked by a dashed line rectangle. Without optical flow prediction, the dividing nuclei (in magenta) are linked incorrectly.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>3D U-Net architecture for flow.</title><p>Schematic illustration of the 3D U-Net architecture for the flow model, depicted as in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The structure of ResBlock is shown on the bottom.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig4-figsupp1-v1.tif"/></fig><media id="fig4video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-69380-fig4-video1.mp4"><label>Figure 4—video 1.</label><caption><title>ELEPHANT flow predictions in 3D.</title><p>The PH dataset is shown in parallel with the corresponding flow predictions of the ELEPHANT optical flow model (in three dimensions), over the entire duration of the recording. Gray values for flow predictions represent displacements between timepoints as introduced in <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</p></caption></media></fig-group><p>We evaluated the linking performance of ELEPHANT using two types of 4D confocal microscopy datasets in which nuclei were visualized by fluorescent markers: the first type of dataset captures the embryonic development of <italic>Caenorhabditis elegans</italic> (CE datasets), which has been used in previous studies to benchmark tracking methods (<xref ref-type="bibr" rid="bib26">Murray et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Ulman et al., 2017</xref>), and the second type captures limb regeneration in <italic>Parhyale hawaiensis</italic> (PH dataset, imaging adapted from <xref ref-type="bibr" rid="bib1">Alwes et al., 2016</xref>), which presents greater challenges for image analysis (see below, <xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>). For both types of dataset, we find that fewer than 10 annotated nuclei are sufficient to initiate a virtuous cycle of training, prediction, and proofreading, which efficiently yields cell tracks and validated cell lineages in highly dynamic tissues. Interactive cycles of manual annotation, deep learning, and proofreading on ELEPHANT reduce the time required to detect and validate nuclei (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). On the CE1 dataset, a complete cell lineage was built over 195 timepoints, from scratch, using ELEPHANT’s semi-automated workflow (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The detection model was trained incrementally starting from sparse annotations (four nuclei) on the first timepoint. On this dataset, linking could be performed using the nearest neighbor algorithm (without flow modeling) and manual proofreading. In this way, we were able to annotate in less than 8 hr a total of 23,829 nuclei (across 195 timepoints), of which ~ 2% were manually annotated (483 nuclei) and the remaining nuclei were collected by validating predictions of the deep-learning model.</p><p>Although ELEPHANT works efficiently without prior training, cell tracking can be accelerated by starting from models trained on image data with similar characteristics. To illustrate this, we used nuclear annotations in a separate dataset, CE2, to train a model for detection, which was then applied to CE1. This pre-trained model allowed us to detect nuclei in CE1 much more rapidly and effortlessly than with an untrained model (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, blue versus orange curves). For benchmarking, the detection and linkage models trained with the annotations from the CE1 and CE2 lineage trees were then tested on unseen datasets with similar characteristics (without proofreading), as part of the Cell Tracking Challenge (<xref ref-type="bibr" rid="bib21">Maška et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Ulman et al., 2017</xref>). In this test, our models with assistance of flow-based interpolation (see Materials and methods) outperformed state-of-the-art tracking algorithms (<xref ref-type="bibr" rid="bib19">Magnusson et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Scherr et al., 2020</xref>) in detection (DET) and tracking (TRA) metrics (<xref ref-type="table" rid="table1">Table 1</xref>). ELEPHANT performs less well in segmentation (SEG), probably due to the use of ellipsoids to approximate nuclear shapes.</p><p>The PH dataset presents greater challenges for image analysis, such as larger variations in the shape, intensity, and distribution of nuclei, lower temporal resolution, and more noise (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). ELEPHANT has allowed us to grapple with these issues by supporting the continued training of the models through visual feedback from the user (annotation of missed nuclei, validation and rejection of predictions). Using ELEPHANT, we annotated and validated over 260,000 nuclei in this dataset, across 504 timepoints spanning 168 hr of imaging.</p><p>We observed that the conventional nearest neighbor approach was inadequate for linking in the PH dataset, resulting in many errors in the lineage trees (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). This is likely due to the lower temporal resolution in this dataset (20 min in PH, versus 1–2 min in CE) and the fact that daughter nuclei often show large displacements at the end of mitosis. We trained optical flow using 1,162 validated links collected from 10 timepoints (including 18 links for 9 cell divisions). These sparse annotations were sufficient to generate 3D optical flow predictions for the entire dataset (<xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>), which significantly improved the linking performance (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>): the number of false positive and false negative links decreased by ~57% (from 2093 to 905) and ~32% (from 1991 to 1349), respectively, among a total of 259,071 links.</p><p>By applying ELEPHANT’s human-in-the-loop semi-automated workflow, we succeeded in reconstructing 109 complete and fully validated cell lineage trees encompassing the duration of leg regeneration in <italic>Parhyale</italic>, each lineage spanning a period of ~1 week (504 timepoints, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Using analysis and visualization modules implemented in Mastodon and ELEPHANT, we could capture the distribution of cell divisions across time and space (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and produce a fate map of the regenerating leg of <italic>Parhyale</italic> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). This analysis, which would have required several months of manual annotation, was achieved in ~1 month of interactive cell tracking in ELEPHANT, without prior training. Applying the best performing models to new data could improve tracking efficiency even further.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Cell lineages tracked during the time course of leg regeneration.</title><p>(<bold>A</bold>) Spatial and temporal distribution of dividing nuclei in the regenerating leg of <italic>Parhyale</italic> tracked over a 1-week time course (PH dataset), showing that cell proliferation is concentrated at the distal part of the regenerating leg stump and peaks after a period of proliferative quiescence, as described in <xref ref-type="bibr" rid="bib1">Alwes et al., 2016</xref>. Top: Nuclei in lineages that contain at least one division are colored in magenta, nuclei in non-dividing lineages are in cyan, and nuclei in which the division status is undetermined are blank (see Materials and methods). Bottom: Heat map of the temporal distribution of nuclear divisions; hpa, hours post amputation. The number of divisions per 20-min time interval ranges from 0 (purple) to 9 (yellow). (<bold>B</bold>) Fate map of the regenerating leg of <italic>Parhyale</italic>, encompassing 109 fully tracked lineage trees (202 cells at 167 hpa). Each clone is assigned a unique color and contains 1–9 cells at 167 hpa. Partly tracked nuclei are blank. In both panels, the amputation plane (distal end of the limb) is located on the left.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Image quality issues in the PH dataset.</title><p>Snapshots represent the image characteristics of the PH dataset that render cell tracking more challenging: fluorescence from cellular debris, low signal, variations in nuclear fluorescence intensity and nuclear shape, and variations in image quality across the imaged sample. The top panels show parts of a field of view indicated with red squares; the bottom panel shows an entire xy plane.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Complete cell lineage trees in a regenerating leg of <italic>Parhyale</italic>.</title><p>The displayed trees contain 109 complete and fully validated cell lineages in a regenerating leg of <italic>Parhyale</italic> (PH dataset), corresponding to <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69380-fig5-figsupp2-v1.tif"/></fig><media id="fig5video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-69380-fig5-video1.mp4"><label>Figure 5—video 1.</label><caption><title>Live imaging of <italic>Parhyale</italic> leg regeneration (PH dataset).</title><p>A maximum intensity projection of the PH dataset captures the regeneration of a <italic>Parhyale</italic> T4 leg amputated at the distal end of the carpus, over a period of 1 week. hpa, hours post amputation.</p></caption></media></fig-group></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Image datasets</title><p>The PH dataset (dataset li13) was obtained by imaging a regenerating T4 leg of the crustacean <italic>Parhyale hawaiensis</italic>, based on the method described by <xref ref-type="bibr" rid="bib1">Alwes et al., 2016</xref>; <xref ref-type="video" rid="fig5video1">Figure 5—video 1</xref>. The imaging was carried out on a transgenic animal carrying the <italic>Mi(3xP3&gt; DsRed; PhHS&gt; H2B-mRFPRuby</italic>) construct (<xref ref-type="bibr" rid="bib46">Wolff et al., 2018</xref>), in which nuclear-localised mRFPRuby fluorescent protein is expressed in all cells following heat-shock. The leg was amputated at the distal end of the carpus. Following the amputation, continuous live imaging over a period of 1 week was performed on a Zeiss LSM 800 confocal microscope equipped with a Plan-Apochromat 20 x/0.8 M27 objective (Zeiss 420650-9901-000), in a temperature control chamber set to 26 °C. Heat-shocks (45 minutes at 37 °C) were applied 24 hr prior to the amputation, and 65 and 138 hr post-amputation. Every 20 min we recorded a stack of 11 optical sections, with a z step of 2.48 microns. Voxel size (in xyz) was 0.31 × 0.31 x 2.48 microns.</p><p>The CE1 and CE2 datasets (<xref ref-type="bibr" rid="bib26">Murray et al., 2008</xref>) and the MDA231 dataset were obtained via the Cell Tracking Challenge (<xref ref-type="bibr" rid="bib39">Ulman et al., 2017</xref>) (datasets Fluo-N3DH-CE and Fluo-C3DL-MDA231). The ORG1 and ORG2 datasets were obtained from <xref ref-type="bibr" rid="bib6">de Medeiros, 2021</xref> and <xref ref-type="bibr" rid="bib16">Kok et al., 2020</xref>, respectively. Additional datasets used to train the generic models (see <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>) were obtained from the Cell Tracking Challenge (<xref ref-type="bibr" rid="bib39">Ulman et al., 2017</xref>).</p></sec><sec id="s3-2"><title>ELEPHANT platform architecture</title><p>ELEPHANT implements a client-server architecture (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), which can be set up on the same computer or on multiple connected computers. This architecture brings flexibility: allowing the client to run Mastodon (implemented in Java) while the deep learning module is implemented separately using Python, and releasing the client computer from the requirements of high GPU needed to implement deep learning. The client side is implemented by extending Mastodon, a framework for cell tracking built upon the SciJava ecosystem (<ext-link ext-link-type="uri" xlink:href="https://scijava.org/">https://scijava.org/</ext-link>) and is available as a Fiji (<xref ref-type="bibr" rid="bib33">Schindelin et al., 2012</xref>) plugin. Combining the BigDataViewer (<xref ref-type="bibr" rid="bib30">Pietzsch et al., 2015</xref>) with an efficient memory access strategy (<ext-link ext-link-type="uri" xlink:href="https://github.com/mastodon-sc/mastodon/blob/master/doc/trackmate-graph.pdf">https://github.com/mastodon-sc/mastodon/blob/master/doc/trackmate-graph.pdf</ext-link>), Mastodon enables fast and responsive user interaction even for very large datasets. ELEPHANT leverages the functionalities provided by Mastodon, including the functions for manual annotation of nuclei, and extends them by implementing modules for deep learning-based algorithms.</p><p>The server side is built using an integrated system of a deep learning library (PyTorch <xref ref-type="bibr" rid="bib29">Paszke et al., 2019</xref>), tools for tensor computing and image processing (NumPy <xref ref-type="bibr" rid="bib8">Harris et al., 2020</xref>), SciPy (<xref ref-type="bibr" rid="bib41">Virtanen et al., 2020</xref>), Scikit Image (<xref ref-type="bibr" rid="bib40">van der Walt et al., 2014</xref>), and web technologies (Nginx, uWSGI, Flask). The client and the server communicate by Hypertext Transfer Protocol (HTTP) and JavaScript Object Notation (JSON). To reduce the amount of data exchanged between the client and the server, the image data is duplicated and stored in an appropriate format on each side. An in-memory data structure (Redis) is used to organize the priorities of the HTTP requests sent by the client. A message queue (RabbitMQ) is used to notify the client that the model is updated during training. The client software is available as an extension on Fiji (<ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/elephant-client">https://github.com/elephant-track/elephant-client</ext-link>). The server environment is provided as a Docker container to ensure easy and reproducible deployment (<ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/elephant-server">https://github.com/elephant-track/elephant-server</ext-link>). The server can also be set up with Google Colab in case the user does not have access to a computer that satisfies the system requirements.</p></sec><sec id="s3-3"><title>Computer setup and specifications</title><p>In this study, we set up the client and the server on the same desktop computer (Dell Alienware Aurora R6) with the following specifications: Intel Core i7-8700K CPU @3.70 GHz, Ubuntu 18.04, 4 × 16 GB DDR4 2,666 MHz RAM, NVIDIA GeForce GTX 1080 Ti 11 GB GDDR5X (used for deep learning), NVIDIA GeForce GTX 1650 4 GB GDDR5, 256 GB SSD and 2 TB HDD. System requirements for the client and the server are summarized in the user manual (<ext-link ext-link-type="uri" xlink:href="https://elephant-track.github.io/">https://elephant-track.github.io/</ext-link>).</p></sec><sec id="s3-4"><title>Dataset preparation</title><p>Images were loaded in the BigDataViewer (BDV, <xref ref-type="bibr" rid="bib30">Pietzsch et al., 2015</xref>) format on the client software. The CE1, CE2, ORG1, ORG2, and MDA231 datasets were converted to the BDV format using the BigDataViewer Fiji plugin (<ext-link ext-link-type="uri" xlink:href="https://imagej.net/BigDataViewer">https://imagej.net/BigDataViewer</ext-link>) without any preprocessing. Because the PH dataset showed non-negligible variations in intensity during long-term imaging, the original 16-bit images were intensity normalized per timepoint before conversion to the BDV format, for better visualization on Mastodon. In this normalization, the intensity values were re-scaled so that the minimum and maximum values at each timepoint become 0 and 65535, respectively. The PH dataset also showed 3D drifts due to heat-shocks. The xy drifts were corrected using an extended version of image alignment tool (<xref ref-type="bibr" rid="bib38">Tseng et al., 2011</xref>) working as an ImageJ (<xref ref-type="bibr" rid="bib34">Schneider et al., 2012</xref>) plugin, where the maximum intensity projection images were used to estimate the xy displacements, subsequently applied to the whole image stack (<ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/align-slices3d">https://github.com/elephant-track/align-slices3d</ext-link>). The z drifts were corrected manually by visual inspection using Fiji.</p><p>On the server, images, annotation labels and outputs were stored in the Zarr format, allowing fast read/write access to subsets of image data using chunk arrays (<xref ref-type="bibr" rid="bib25">Moore et al., 2021</xref>). At the beginning of the analysis, these data were prepared using a custom Python script that converts the original image data from HDF5 to Zarr and creates empty Zarr files for storing annotation labels and outputs (<ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/elephant-server">https://github.com/elephant-track/elephant-server</ext-link>). This conversion can also be performed from the client application. Generally, HDF5 is slower in writing data than Zarr, especially in parallelization, while they show comparable reading speeds (<xref ref-type="bibr" rid="bib25">Moore et al., 2021</xref>).</p><p>On the server, the image data are stored in unsigned 8-bit or unsigned 16-bit format, keeping the original image format. At the beginning of processing on the server, the image data are automatically converted to a 32-bit float and their intensity is normalized at each timepoint such that the minimum and maximum values become 0 and 1.</p></sec><sec id="s3-5"><title>Algorithm for detection</title><p>Detection of nuclei relies on three components: (i) a U-Net CNN that outputs probability maps for <italic>nucleus center</italic>, <italic>nucleus periphery</italic>, and <italic>background</italic>, (ii) a post-processing workflow that extracts <italic>nucleus center</italic> voxels from the probability maps, (iii) a module that reconstructs nuclei instances as ellipsoids. We designed a variation of 3D U-Net (<xref ref-type="bibr" rid="bib4">Cicek et al., 2016</xref>) as illustrated in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. In both encoder and decoder paths, repeated sets of 3D convolution, ReLU activation (<xref ref-type="bibr" rid="bib27">Nair and Hinton, 2010</xref>) and Group Normalization (<xref ref-type="bibr" rid="bib47">Wu and He, 2020</xref>) are employed. Max pooling in 3D is used for successive downsampling in the encoder path, in each step reducing the size to half the input size (in case of anisotropy, maintaining the z dimension until the image becomes nearly isotropic). Conversely, in the decoder path, upsampling with nearest-neighbor interpolation is applied to make the dimensions the same as in the corresponding intermediate layers in the encoder path. As a result, we built a CNN with 5,887,011 trainable parameters. The weights are initialized with the Kaiming fan-in algorithm (<xref ref-type="bibr" rid="bib11">He et al., 2015a</xref>) and the biases are initialized to zero for each convolution layer. For each group normalization layer, the number of groups is set as the smallest value between 32 and the number of output channels, and the weights and biases are respectively initialized to one and zero. When starting to train from scratch, the CNN is trained using the cropped out 3D volumes from the original image prior to training with annotations. In this prior training phase, a loss function <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is used that penalizes the addition of the following two mean absolute differences (MADs): (i) <italic>nucleus center</italic> probabilities <italic>c</italic><sub><italic>i</italic></sub> and the [0, 1] normalized intensity of the original image <italic>y</italic><sub><italic>i</italic></sub>, (ii) <italic>background</italic> probabilities <italic>b</italic><sub><italic>i</italic></sub> and the [0, 1] normalized intensity of the intensity-inverted image <inline-formula><mml:math id="inf2"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where i stands for the voxel index of an input volume with <inline-formula><mml:math id="inf3"><mml:mi>n</mml:mi></mml:math></inline-formula> voxels <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The prior training is performed on three cropped out 3D volumes generated from the 4D datasets, where the timepoints are randomly picked, and the volumes are randomly cropped with random scaling in the range (0.8, 1.2). The training is iterated for three epochs with decreasing learning rates (0.01, 0.001, and 0.0001, in this order) with the Adam optimizer (<xref ref-type="bibr" rid="bib15">Kingma and Ba, 2014</xref>). The prior training can be completed in ~20 s for each dataset.</p><p>Training with sparse annotations is performed in the following steps. First, the client application extracts the timepoint, 3D coordinates and covariances representing ellipsoids of all the annotated nuclei in the specified time range. Subsequently, these data, combined with user-specified parameters for training, are embedded in JSON and sent to the server in an HTTP request. On the server side, training labels are generated from the received information by rendering <italic>nucleus center</italic>, <italic>nucleus periphery</italic>, <italic>background</italic> and unlabelled voxels with distinct values. The <italic>background</italic> labels are generated either by explicit manual annotation or intensity thresholding, where the threshold value is specified by the user, resulting in the label images as shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. To render ellipsoids in the anisotropic dimension, we extended the draw module in the scikit-image library (<xref ref-type="bibr" rid="bib40">van der Walt et al., 2014</xref>) (<ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/elephant-server">https://github.com/elephant-track/elephant-server</ext-link>). Training of the CNN is performed using the image volumes as input and the generated labels as target with a loss function <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that consists of three terms: (i) a class-weighted negative log-likelihood (NLL) loss, (ii) a term computed as one minus the dice coefficient for the <italic>nucleus center</italic> voxels, and (iii) a term that penalizes the roughness of the <italic>nucleus center</italic> areas. We used the empirically-defined class weights <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the NLL loss: <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf9"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>; the unlabelled voxels are ignored. The first two terms accept different weights for the true annotations <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> (i.e. true positive and true negative) and the false annotations <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> (i.e. false positive and false negative). The third term is defined as the MAD between the voxel-wise probabilities for <italic>nucleus center</italic> and its smoothed representations, which are calculated by the Gaussian filter with downsampling (<inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>) and upsampling (<inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>). Let i stand for the voxel index of an input volume with <inline-formula><mml:math id="inf14"><mml:mi>n</mml:mi></mml:math></inline-formula> voxels <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic>x</italic><sub><italic>i</italic></sub> for the input voxel value, <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for the output from the CNN before the last activation layer for the three classes, <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the voxel class label (1: <italic>nucleus center</italic>, 2: <italic>nucleus periphery</italic>, 3: <italic>background</italic>, respectively), and <inline-formula><mml:math id="inf18"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo rspace="4.2pt" stretchy="false">{</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>d</mml:mi></mml:mpadded></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the voxel annotation label. We define the following subsets: the voxel index with true labels <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo rspace="4.2pt" stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>e</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with false labels <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo rspace="4.2pt" stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>e</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the <italic>nucleus center</italic> <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo rspace="4.2pt" stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>∣</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mpadded width="+1.7pt"><mml:mn>1</mml:mn></mml:mpadded></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the calculation of the <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, a constant <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.000001</mml:mn></mml:mrow></mml:math></inline-formula> is used to prevent zero division. Using these components and the empirically-defined weights for each loss term (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), we defined the <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as below.<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the analyses shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> and its supplements 1 and 2, the following loss functions are updated to make them more robust using normalization, which are employed in the current version of software.<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Training of the CNN is performed on the image volumes generated from the 4D datasets, where the volumes are randomly cropped with/without random scaling, random contrast, random flip and random rotation, which are specified at runtime. There are two modes for training: (i) an interactive mode that trains a model incrementally, as the annotations are updated, and (ii) a batch mode that trains a model with a fixed set of annotations. In the interactive training mode, sparse annotations in a given timepoint are used to generate crops of image and label volumes, with which training is performed using the Adam optimizer with a learning rate specified by the user. In the batch training mode, a set of crops of image and label volumes per timepoint is generated each iteration, with which training is performed for a number of epochs specified by the user (ranging from 1 to 1000) using the Adam optimizer with the specified learning rates. In the prediction phase, the input volume can be cropped into several blocks with smaller size than the original size to make the volume can be cropped into several blocks with smaller size than the original size to make the input data compatible with available GPU memory. To stitch the output blocks together, the overlapping regions are seamlessly blended by weighted linear blending.</p><p>In post-processing for the CNN output, voxel-wise probabilities for <italic>nucleus center</italic> class are denoised by subtracting edges of <italic>background</italic> class that are calculated with the Gaussian filter and the Prewitt operation for each z-slice. After denoising, the voxels with <italic>nucleus center</italic> probabilities greater than a user defined value are thresholded and extracted as connected components, which are then represented as ellipsoids (from their central moments). These ellipsoids representing the <italic>nucleus center</italic> regions are enlarged so that they cover the original nucleus size (without excluding its periphery). The ellipsoids with radii smaller than <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are removed and the radii are clamped to <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> specified by the user, generating a list of center positions and covariances that can be used to reconstruct the nuclei. On the client application, the detection results are converted to Mastodon spots and rendered on the BDV view, where the existing and predicted nuclei are tagged based on their status: labelled as ‘true positive’ (positive and predicted), ‘false negative’ (positive and not predicted), ‘true negative’ (negative and not predicted), ‘false positive’ (negative and predicted), and ‘non-validated’ (newly predicted). These labels can be visualized when running ELEPHANT in the advanced color mode (in basic color mode true positives and false negatives are visualized as ‘accepted’ and false positives and true negatives as ‘rejected’). If more than one nucleus is predicted within a user-specified threshold <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the one with human annotation is given priority, followed by the one with the largest volume.</p></sec><sec id="s3-6"><title>Algorithm for linking</title><p>Linking of nuclei relies on two components: (i) estimation of the positions of nuclei at the previous timepoint by optical flow estimation using deep learning, which is skipped in the case of the nearest neighbor algorithm without flow support, (ii) association of nuclei based on the nearest neighbor algorithm. We designed a variation of 3D U-Net for flow estimation as illustrated in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. In the encoder path, the residual blocks (<xref ref-type="bibr" rid="bib12">He et al., 2015b</xref>) with 3D convolution and LeakyReLU (<xref ref-type="bibr" rid="bib18">Maas et al., 2013</xref>) activation are applied, in which the outputs are divided by two after the sum operation to keep the consistency of the scale of values. In the decoder path, repeated sets of 3D convolution and LeakyReLU activation are employed. Downsampling and upsampling are applied as described for the detection model. Tanh activation is used as a final activation layer. As a result, we built a CNN with 5,928,051 trainable parameters. The weights and biases for convolution layers are initialized as described for the detection model. Training of the flow model with sparse annotations is performed in a similar way as for the detection model. First, on the client application, for each annotated link, which connects the source and target nuclei, the following information gets extracted: the timepoint, the backward displacements in each of the three dimensions, and the properties of the target nucleus (3D coordinates and covariances). Subsequently, these data, combined with parameters for training, are embedded in JSON and sent to the server in an HTTP request. On the server side, flow labels are generated from the received information by rendering backward displacements for each target nucleus in each of three dimensions, where the displacements are scaled to fit the range (–1, 1). In this study, we used fixed scaling factors (1/80, 1/80, 1/10) for each dimension, but they can be customized to the target dataset. Foreground masks are generated at the same time to ignore unlabelled voxels during loss calculation. Ellipsoid rendering is performed as described for the detection training. Training of the CNN for flow estimation is performed using the two consecutive image volumes <inline-formula><mml:math id="inf29"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> as input, and the generated label as target. A loss function <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined with the following three terms; (i) a dimension-weighted MAD between the CNN outputs and the flow labels, (ii) a term computed as one minus the structural similarity (SSIM) (<xref ref-type="bibr" rid="bib42">Wang et al., 2004</xref>) of <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, where the estimated flow is applied to <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib14">Ilg et al., 2017</xref>), (iii) a term penalizing the roughness of the CNN outputs. Let i stand for the voxel index of an input volume with <inline-formula><mml:math id="inf34"><mml:mi>n</mml:mi></mml:math></inline-formula> voxels <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic>x</italic><sub><italic>i</italic></sub> for the input voxel value, <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the output of the CNN, <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the flow label, <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mi>M</mml:mi><mml:mo>⊂</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> for the index of the annotated voxels, <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the dimension index for three dimensions and <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for the dimension weights. In the SSIM calculation, we defined a function <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> as a 3D Gaussian filter with the window size 7 × 7 x 3 and standard deviation of 1.5. Using these components and the empirically defined weights for each loss term <inline-formula><mml:math id="inf42"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, we defined the <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as below.<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>3</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>, where <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0009</mml:mn></mml:mrow></mml:math></inline-formula>. In the current version of software, the following loss functions are updated to make them more robust using normalization.<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The training is performed on the image volumes generated from the 4D datasets, where the sets of two consecutive images and corresponding flow labels are randomly cropped with/without random scaling and random rotation, which are specified at runtime. The training is performed for a fixed number of epochs using the Adam optimizer and with learning rates specified by the user, generating a set of images and labels for each timepoint in each epoch. The CNN outputs are rescaled to the original physical scale and used to calculate the estimated coordinate of each nucleus center at the previous timepoint. Let <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⊂</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula> stands for a subset of voxel index of a nucleus and <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for its center coordinate. Using the output of the CNN <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the scaling factor <inline-formula><mml:math id="inf49"><mml:mi>s</mml:mi></mml:math></inline-formula>, the estimated coordinate at the previous timepoint <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is calculated.<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">p</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>These estimated coordinates are subsequently used to find the parent of the nucleus at the previous timepoint by the nearest neighbor algorithm (a similar concept was introduced for 2D phase contrast microscopy data; <xref ref-type="bibr" rid="bib9">Hayashida and Bise, 2019</xref>; <xref ref-type="bibr" rid="bib10">Hayashida et al., 2020</xref>). The pairs with a distance smaller than <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are considered as link candidates, where the closer the Euclidean distance between the two points, the higher their priority of being the correct link. Each nucleus accepts either one or two links, determined by the estimated displacements and actual distances. Briefly, given that a single nucleus has two possible links, it can accept both if at least one of the estimated displacements is larger than the threshold <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or both distances are smaller than the threshold <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. In this study, we used ad hoc thresholds <inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula>. If there are competing links beyond the allowed maximum of two links, the links with smaller <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are adopted and the remaining nucleus looks for the next closest nucleus up to <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> neighbors. The links are generated by repeating the above procedure until all the nuclei get linked or the iteration count reaches to five. We optionally implement an interpolation algorithm, in which each orphan nucleus tries to find its source up to <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> timepoints back and is linked with a nucleus at the estimated coordinate based on the flow prediction, interpolating the points in between.</p></sec><sec id="s3-7"><title>Preparation of generic pre-trained models and fine-tuning with sparse annotations (Figure 3)</title><p>The generic pre-trained models for each dataset were trained on the datasets summarized in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>. Training of the detection models was performed with volumes of 384 × 384 x 16 voxels or smaller, which were generated by preprocessing with random flip in each dimension, random scaling in the range (0.5, 2), random cropping and random contrast in the range (0.5, 1). In the label generation step, the center ratio was set to 0.4 and the background threshold was set to 1 (i.e. all voxels without manual annotations are background) for the Cell Tracking Challenge datasets (Fluo-C3DH-A549, Fluo-C3DH-H157, Fluo-C3DL-MDA231, Fluo-N3DH-CE and Fluo-N3DH-CHO), and to 0.03 for the PH dataset. The labels for the Cell Tracking Challenge datasets were automatically generated from the silver-standard corpus (silver truth). We trained the models for up to 200 epochs starting from scratch using the Adam optimizer with the learning rate of 5 × 10<sup>–3</sup>, where each epoch contained randomly selected 10 pre-processed volumes from each dataset. Validation was performed after each epoch using randomly selected five timepoints from each dataset. In the validation phase, image volumes were fed into the model using blocks with size 512 × 512 x 24 or smaller, and the outputs were stitched together to reconstruct the whole volume. For each condition, the model with the highest score in the validation data was finally adopted. At the start of each epoch, the model parameters were set to the ones that had previously produced the highest scores on the validation data. The parameters for training and validation are summarized in <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>. Fine-tuning of the model was performed as follows: (i) 3–10 sparse annotations were added at the points where the pre-trained model failed in detection (<xref ref-type="fig" rid="fig3">Figure 3</xref> middle), (ii) we trained the models for 10 epochs starting from the pre-trained model parameters with volumes of 384 × 384 x 16 voxels or smaller. These volumes were generated by preprocessing with random flip in each dimension, random scaling in the range (0.5, 2), random cropping and random contrast in the range (0.5, 1), using the Adam optimizer with the learning rate of 0.01 or 0.001, where each epoch contained five randomly cropped volumes. The pre-trained model and the fine-tuned model were applied to each dataset with parameters summarized in <xref ref-type="supplementary-material" rid="supp6">Supplementary file 6</xref>. The evaluation scores were calculated based on the detection criterion of ELEPHANT, which recognizes that a prediction is correct if the distance between the prediction and the manual annotation is less than <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s3-8"><title>Comparison between ELEPHANT and StarDist3D (Figure 3 – Supplement 1)</title><p>The ELEPHANT detection model is the same as the one used in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. For training of the StarDist3D (<xref ref-type="bibr" rid="bib44">Weigert et al., 2020</xref>) segmentation model, a single volume of the CE1 dataset (timepoint = 100) with the fully labelled instance segmentation annotations (93 nuclei) was used for training, and another volume of the CE1 dataset (timepoint = 101) with the fully labelled instance segmentation annotations (95 nuclei) was used for validation during training. The instance segmentation annotations were taken from the silver-standard corpus (silver truth) in the Cell Tracking Challenge. Training of the StarDist3D model was performed with the parameters summarized in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>. The model with the best performance in the validation data was selected for comparison. The trained ELEPHANT and StarDist3D models were applied to a single volume (timepoint = 100) of the CE2 dataset to generate the output for comparison (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The DET scores were calculated by the evaluation software provided by the Cell Tracking Challenge using the gold-standard corpus (gold truth).</p></sec><sec id="s3-9"><title>Evaluation of overfitting of the detection model (Figure 3 – Supplement 2)</title><p>A single volume of the CE1 dataset (timepoint = 100) with sparse annotations (10 nuclei) was used for training, and a single volume of the fully labelled CE2 dataset (timepoint = 100) was used for validation. Training of the detection model was performed using 384 × 384 x 16 cropped-out image volumes generated by preprocessing with random flip in each dimension, random scaling in the range (0.5, 2), random cropping and random contrast in the range (0.5, 1). In the label generation step, the center ratio was set to 0.4 and the background threshold was set to 0 (i.e. all voxels without manual annotations are ignored). We trained a model for 500 epochs starting from scratch using the Adam optimizer with the learning rate of 5 × 10<sup>–4</sup>, where each epoch contained five pre-processed volumes. Training and validation losses were recorded at the end of each epoch (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, bottom left). The detection model trained for 500 epochs was tested on the CE1 dataset (partially-seen data; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, top right) and the CE2 dataset (unseen data; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, bottom right). In the prediction phase, the input volumes were cropped into 2 × 2 x 2 blocks with size 544 × 384 x 28 for CE1 or 544 × 384 x 24 for CE2, and stitched together to reconstruct the whole image of 708 × 512 x 35 for CE1 or 712 × 512 x 31 for CE2. In the postprocessing of the prediction for detection, a threshold for the nucleus center probabilities were set to 0.5, and <inline-formula><mml:math id="inf60"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were set to 0.5 µm, 3 µm, and 2 µm respectively. The evaluation scores were calculated in the same way as described in the previous section.</p></sec><sec id="s3-10"><title>Detection and tracking in the CE datasets (Figures 2 and 4)</title><p>On the CE1 and CE2 datasets, training of detection and flow models was performed with volumes of 384 × 384 x 16 voxels that were generated by preprocessing with random scaling in the range (0.5, 2) and random cropping. For training of a detection model, preprocessing with random contrast in the range (0.5, 1) was also applied. In the label generation step, the center ratio was set to 0.3 and the background threshold was set to 0.1 and 1 (i.e. all voxels without manual annotations are background). In the interactive training of detection models, 10 labelled cropped out volumes were generated per iteration, with which training was performed using the Adam optimizer with a learning rate between 5 × 10<sup>–5</sup> and 5 × 10<sup>–6</sup>. In the batch training of detection models, training was performed for 100 epochs using the Adam optimizer with learning rates of 5 × 10<sup>–5</sup>. In the training of a flow model, training was performed for 100 epochs using the Adam optimizer with learning rates of 5 × 10<sup>–5</sup> for the first 50 epochs and 5 × 10<sup>–6</sup> for the last 50 epochs. <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> were set to 1 and 5, respectively, and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was set to (1/3, 1/3, 1/3). In the prediction phase, the input volumes were cropped into 2 × 2 x 2 blocks with size 544 × 384 x 28 for CE1 or 544 × 384 x 24 for CE2, and stitched together to reconstruct the whole image of 708 × 512 x 35 for CE1 or 712 × 512 x 31 for CE2. In the preprocessing of the prediction for detection, we corrected the uneven background levels across the z-slices by shifting the slice-wise median value to the volume-wise median value. In the postprocessing of the prediction for detection, a threshold for the <italic>nucleus center</italic> probabilities were set to 0.3, and <inline-formula><mml:math id="inf66"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf68"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were set to 1 µm, 3 µm and 1 µm, respectively. In the nearest-neighbor linking with/without flow prediction, <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 5 µm and <inline-formula><mml:math id="inf70"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 3. In the results submitted to the Cell Tracking Challenge (<xref ref-type="table" rid="table1">Table 1</xref>), the suppression of detections with <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was not applied, and the linking was performed by the nearest-neighbor linking with flow support and an optional interpolation module, where <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 5.</p></sec><sec id="s3-11"><title>Detection and tracking in the PH dataset</title><p>On the PH dataset, training of detection and flow models was performed with volumes of 384 × 384 x 12 voxels generated by preprocessing with random rotation in the range of ±180 degrees and random cropping. For training a detection model, preprocessing with random contrast in the range (0.5, 1) was also applied. In the label generation step, the center ratio was set to 0.3, and the background threshold was set to 0.03. In the interactive training of a detection model, 10 crops of image and label volumes were generated per iteration, with which training was performed using the Adam optimizer with a learning rate between 5 × 10<sup>–5</sup> and 5 × 10<sup>–6</sup>. In the batch training of a detection model, training was performed for 100 epochs using the Adam optimizer with learning rates of 5 × 10<sup>–5</sup>. In the training of a flow model, training was performed for 100 epochs using the Adam optimizer with learning rates of 5 × 10<sup>–5</sup> for the first 50 epochs and 5 × 10<sup>–6</sup> for the last 50 epochs. <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> were set to 1 and 3, respectively, and <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was set to (1, 1, 8). In the prediction phase, the input volumes were fed into the CNNs without cropping or further preprocessing. In the postprocessing of the prediction for detection, a threshold for the <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> probabilities were set to 0.3, and <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were set to 1 µm, 3 µm and 5 µm respectively. In the nearest-neighbor linking with/without flow prediction, <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 5 µm and <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was set to 3.</p></sec><sec id="s3-12"><title>Analysis of CE and PH datasets</title><p>On the CE1 and CE2 datasets, the detection and link annotations were made starting from timepoint 0 and proceeding forward until timepoints 194 (CE1) and 189 (CE2), respectively. In the CE1 dataset, the detection was made from scratch, based on manual annotation and incremental training, and the linking was performed by the nearest neighbor algorithm without flow prediction. After completing annotation from timepoint 0 to 194 on the CE1 dataset, the detection and flow models were trained by the batch mode with the fully labelled annotations. In the CE2 dataset, the detection was performed in a similar way as for CE1, by extending the model trained with CE1, and the linking was performed by the nearest neighbor algorithm with flow support using the pre-trained model followed by proofreading. Incremental training of the detection model was performed when there were annotations from nuclei that were not properly predicted.</p><p>On the PH dataset, the annotations were made by iterating the semi-automated workflow. In general, the nuclei with high signal-to-noise ratio (SNR) were annotated early, while the nuclei with low SNR were annotated in a later phase. The detection model was updated frequently to fit the characteristics of each region and timepoint being annotated, while the flow model was updated less frequently. The CE1 dataset was used to evaluate the speed of detection and validation (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). All workflows started at timepoint 0 and proceeded forward in time, adding and/or validating all the nuclei found in each timepoint. To evaluate the manual workflow, we annotated nuclei using hotkeys that facilitate the annotation of a given nucleus at successive timepoints. To evaluate the ELEPHANT from scratch workflow, we performed prediction with the latest model, followed by proofreading, including add, modify, or delete operations, and incremental training. At each timepoint, the model was updated with the new annotations added manually or by proofreading. To evaluate the ELEPHANT pre-trained workflow, we performed predictions with a model trained on the CE2 dataset, followed by proofreading without additional training. The numbers of validated nuclei associated with time were counted from the log data. We measured the counts over 30 min after the start of each workflow and plotted them in <xref ref-type="fig" rid="fig2">Figure 2B</xref>.</p><p>To compare the linking performances (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), we trained the flow model with 1,162 validated links, including 18 links corresponding to 9 cell divisions, from 108 lineage trees collected between timepoints 150 and 159. It took around 30 hr to train the flow model from scratch using these links. Starting from a pre-trained model, the training time can be decreased to a few minutes, providing a major increase in speed compared with training from scratch (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>The results shown in A and B<xref ref-type="fig" rid="fig5">Figure 5A, B</xref> were generated based on the tracking results with 260,600 validated nuclei and 259,071 validated links. In the analysis for <xref ref-type="fig" rid="fig5">Figure 5A</xref>, nuclei were categorised as dividing or non-dividing depending on whether the lineages to which they belong contain at least one cell division or not during the period of cell proliferation (timepoints 100–350). Nuclei that did not meet these criteria were left undetermined. For <xref ref-type="fig" rid="fig5">Figure 5B</xref>, the complete lineages of 109 nuclei were tracked through the entire duration of the recording, from 0 to 167 hr post-amputation, with no missing links.</p></sec><sec id="s3-13"><title>Evaluation of cell tracking performance</title><p>We submitted our results and executable software to the Cell Tracking Challenge organizers, who evaluated our algorithm’s performance, validated its reproducibility using the executable software that we submitted, and provided us with the scores. The details of the detection accuracy (DET), tracking accuracy (TRA), and segmentation accuracy (SEG) metrics can be found in the original paper (<xref ref-type="bibr" rid="bib23">Matula et al., 2015</xref>) and the website (<ext-link ext-link-type="uri" xlink:href="http://celltrackingchallenge.net/evaluation-methodology/">http://celltrackingchallenge.net/evaluation-methodology/</ext-link>). Briefly, the DET score evaluates how many <italic>split</italic>, <italic>delete,</italic> and <italic>add</italic> operations are required to achieve the ground truth starting from the predicted nuclei, reflecting the accuracy of detection; the TRA score evaluates how many <italic>split</italic>, <italic>delete,</italic> and <italic>add</italic> operations for nuclei, and <italic>delete</italic>, <italic>add,</italic> and <italic>alter the semantics</italic> operations for links are required to reconstruct the ground truth lineage trees from the predicted lineage trees, reflecting the accuracy of linking; the SEG score evaluates the overlap of the detected ellipsoids with fully segmented nuclei, reflecting the precision of nucleus segmentation. All three scores range from 0 (poorest) to 1 (best).</p></sec><sec id="s3-14"><title>Data availability</title><p>The CE1, CE2, and MDA231 datasets are available from the Cell Tracking Challenge website: <ext-link ext-link-type="uri" xlink:href="http://celltrackingchallenge.net/3d-datasets/">http://celltrackingchallenge.net/3d-datasets/</ext-link>. The ORG1 and ORG2 datasets were obtained from <xref ref-type="bibr" rid="bib6">de Medeiros, 2021</xref> and <xref ref-type="bibr" rid="bib16">Kok et al., 2020</xref>, respectively. The following files are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4630933">https://doi.org/10.5281/zenodo.4630933</ext-link>: (i) the tracking results shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, (ii) the PH dataset and its tracking results, and (iii) deep-learning model parameters for the CE and PH datasets.</p></sec><sec id="s3-15"><title>Code availability</title><p>The source code for the ELEPHANT client is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/elephant-server">https://github.com/elephant-track/elephant-client</ext-link> (<xref ref-type="bibr" rid="bib37">Sugawara, 2021c</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e69da53d731182d6c6ffcb97588396e59a472e4f;origin=https://github.com/elephant-track/elephant-client;visit=swh:1:snp:f7f13f47ba9af8edaef97291b89bc4825a63a1b9;anchor=swh:1:rev:449f9ff8ad17ce75f355e18f815653ec0aa4bbb8">swh:1:rev:449f9ff8ad17ce75f355e18f815653ec0aa4bbb8</ext-link>), for the ELEPHANT server at <ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/elephant-server">https://github.com/elephant-track/elephant-server</ext-link> (<xref ref-type="bibr" rid="bib35">Sugawara, 2021a</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a3028f2a4adb71c0cc6249963f0777c6198d8602;origin=https://github.com/elephant-track/elephant-server;visit=swh:1:snp:2efc080405dc4ba11998f598bb4e9e785f39d314;anchor=swh:1:rev:8935febdbcb2e2d6ba2220ca139e765db44e6458">swh:1:rev:8935febdbcb2e2d6ba2220ca139e765db44e6458</ext-link>), and for the Align Slices 3D + t extension ImageJ plugin at <ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track/align-slices3d">https://github.com/elephant-track/align-slices3d</ext-link> (<xref ref-type="bibr" rid="bib36">Sugawara, 2021b</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:663a99923602d153e97af69164cd6762ed80f51d;origin=https://github.com/elephant-track/align-slices3d;visit=swh:1:snp:d18a8bf98eee86f6fe757f2087dcca11b051f897;anchor=swh:1:rev:36c6cb6ccb7e308f9349ec26294d408c35be1ed7">swh:1:rev:36c6cb6ccb7e308f9349ec26294d408c35be1ed7</ext-link>). The user manual for ELEPHANT is available at <ext-link ext-link-type="uri" xlink:href="https://elephant-trackgithub.io/">https://elephant-trackgithub.io/</ext-link>.</p></sec></sec></body><back><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>KS is employed part-time by LPIXEL Inc</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Investigation, Methodology, Software, Validation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Acquired and annotated imaging data, Validation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Processing speed of the detection model Processing speed of the deep learning model for the detection of nuclei, applied to three datasets.</title><p>The training speed is affected by the distribution of annotations because the algorithm contains a try-and-error process for cropping, in which the <italic>nucleus periphery</italic> labels are forced to appear with the <italic>nucleus center</italic> labels.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69380-supp1-v1.xlsx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Comparison of linking performances Linking performances on the PH dataset, on a total number of 259,071 links (including 688 links on cell divisions).</title><p>Incremental training was performed by transferring the training parameters from the model pre-trained with the CE datasets. Linking performance on dividing cells is scored separately.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69380-supp2-v1.xlsx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Datasets used for training generic detection models Datasets used in training of the detection models used in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Columns correspond to the datasets analysed in <xref ref-type="fig" rid="fig3">Figure 3</xref> and rows indicate the image datasets included in the training. In each case, the test image data were excluded from training. The Fluo-C3DH-A549 (<xref ref-type="bibr" rid="bib3">Castilla et al., 2019</xref>), Fluo-C3DH-H157 (<xref ref-type="bibr" rid="bib20">Maška et al., 2013</xref>), Fluo-N3DH-CHO (<xref ref-type="bibr" rid="bib7">Dzyubachyk et al., 2010</xref>), Fluo-N3DH-CE (<xref ref-type="bibr" rid="bib26">Murray et al., 2008</xref>) datasets are from the Cell Tracking Challenge (<xref ref-type="bibr" rid="bib21">Maška et al., 2014</xref>).</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69380-supp3-v1.xlsx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Parameters used for training and prediction using StarDist3D Parameters used for training and prediction in the StarDist3D model used in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</title><p>These parameters were extracted from “config.json” and “thresholds.json” generated by the software.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69380-supp4-v1.xlsx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Parameters used for training and validation of generic models Parameters used for training and validation of the generic models used in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Size and scale are represented in the format [X]x[Y]x[Z].</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69380-supp5-v1.xlsx"/></supplementary-material><supplementary-material id="supp6"><label>Supplementary file 6.</label><caption><title>Parameters used for fine-tuning and prediction using generic models Parameters used for fine-tuning of the generic models and prediction used in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Size and scale are represented in the format [X]x[Y]x[Z].</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-69380-supp6-v1.xlsx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-69380-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>The imaging datasets are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4630933">https://doi.org/10.5281/zenodo.4630933</ext-link>. The source code for the ELEPHANT software is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/elephant-track">https://github.com/elephant-track</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Sugawara, Çevrim and Averof</collab></person-group><year iso-8601-date="2021">2021</year><data-title>PH_li13 Zenodo</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.4630933</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Waterston Lab</collab></person-group><year iso-8601-date="2008">2008</year><data-title>C. elegans developing embryo</data-title><source>Cell Tracking Challenge</source><pub-id pub-id-type="accession" xlink:href="http://data.celltrackingchallenge.net/training-datasets/Fluo-N3DH-CE.zip">Fluo-N3DH-CE</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Kamm</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>MDA231 human breast carcinoma cells</data-title><source>Cell Tracking Challenge</source><pub-id pub-id-type="accession" xlink:href="http://data.celltrackingchallenge.net/training-datasets/Fluo-C3DL-MDA231.zip">Fluo-C3DL-MDA231</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Anna Kreshuk and Constantin Pape for training in machine learning, to Jean-Yves Tinevez (Image Analysis Hub, Institut Pasteur) and Tobias Pietzsch for support in developing ELEPHANT as a Mastodon plugin, to the NEUBIAS community for feedback on the software, to Martin Maška, Michal Kozubek and Carlos Ortiz de Solórzano for support in our submission to the Cell Tracking Challenge, and to Christian Tischer and Sebastian Tosi for extensive feedback on the software and manuscript. We thank Carlos Ortiz de Solórzano, Bob Waterston, Jeroen van Zon, Gustavo de Madeiros and Prisca Liberali for sharing image and cell tracking data used to test ELEPHANT. We also thank Jan Funke, Carsten Wolff, Martin Weigert, Jean-Yves Tinevez, Philipp Keller, Irepan Salvador-Martínez, Severine Urdy, and Mathilde Paris for comments on the manuscript. This research was supported by the European Research Council, under the European Union Horizon 2020 programme, grant ERC-2015-AdG #694918; ÇÇ was supported by a doctoral fellowship from Boehringer Ingelheim Fonds.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alwes</surname><given-names>F</given-names></name><name><surname>Enjolras</surname><given-names>C</given-names></name><name><surname>Averof</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Live imaging reveals the progenitors and cell dynamics of limb regeneration</article-title><source>eLife</source><volume>5</volume><elocation-id>e19766</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19766</pub-id><pub-id pub-id-type="pmid">27776632</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amat</surname><given-names>F</given-names></name><name><surname>Myers</surname><given-names>EW</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast and robust optical flow for time-lapse microscopy using super-voxels</article-title><source>Bioinformatics</source><volume>29</volume><fpage>373</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts706</pub-id><pub-id pub-id-type="pmid">23242263</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castilla</surname><given-names>C</given-names></name><name><surname>Maska</surname><given-names>M</given-names></name><name><surname>Sorokin</surname><given-names>DV</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name><name><surname>Ortiz-de-Solorzano</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>3-D Quantification of Filopodia in Motile Cancer Cells</article-title><source>IEEE Transactions on Medical Imaging</source><volume>38</volume><fpage>862</fpage><lpage>872</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2873842</pub-id><pub-id pub-id-type="pmid">30296215</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cicek</surname><given-names>O</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Lienkamp</surname><given-names>SS</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</chapter-title><person-group person-group-type="editor"><name><surname>Ourselin</surname><given-names>S</given-names></name><name><surname>Joskowicz</surname><given-names>L</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Unal</surname><given-names>G</given-names></name><name><surname>Wells</surname><given-names>W</given-names></name></person-group><source>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016</source><publisher-name>Springer International Publishing</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-46723-8</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crocker</surname><given-names>JC</given-names></name><name><surname>Grier</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Methods of Digital Video Microscopy for Colloidal Studies</article-title><source>Journal of Colloid and Interface Science</source><volume>179</volume><fpage>298</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1006/jcis.1996.0217</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>de Medeiros</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><conf-name>Multiscale light-sheet organoid imaging framework</conf-name><article-title>European Light Microscopy Initiative 2021</article-title><pub-id pub-id-type="doi">10.22443/rms.elmi2021.90</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dzyubachyk</surname><given-names>O</given-names></name><name><surname>van Cappellen</surname><given-names>WA</given-names></name><name><surname>Essers</surname><given-names>J</given-names></name><name><surname>Niessen</surname><given-names>WJ</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Advanced level-set-based cell tracking in time-lapse fluorescence microscopy</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>852</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1109/TMI.2009.2038693</pub-id><pub-id pub-id-type="pmid">20199920</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hayashida</surname><given-names>J</given-names></name><name><surname>Bise</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Cell Tracking with Deep Learning for Cell Detection and Motion Estimation in Low-Frame-Rate</chapter-title><person-group person-group-type="editor"><name><surname>Shen</surname><given-names>D</given-names></name></person-group><source>Cal Image Computing and Computer Assisted Intervention – MICCAI 2019</source><publisher-name>Springer International Publishing</publisher-name><fpage>397</fpage><lpage>405</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hayashida</surname><given-names>J</given-names></name><name><surname>Nishimura</surname><given-names>K</given-names></name><name><surname>Bise</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><conf-name>MPM: Joint Representation of Motion and Position Map for Cell Tracking</conf-name><article-title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</article-title><fpage>3823</fpage><lpage>3832</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00388</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015a</year><conf-name>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</conf-name><article-title>IEEE International Conference on Computer Vision</article-title><fpage>1026</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Deep Residual Learning for Image Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horn</surname><given-names>BKP</given-names></name><name><surname>Schunck</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Determining optical flow</article-title><source>Artificial Intelligence</source><volume>17</volume><fpage>185</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(81)90024-2</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ilg</surname><given-names>E</given-names></name><name><surname>Mayer</surname><given-names>N</given-names></name><name><surname>Saikia</surname><given-names>T</given-names></name><name><surname>Keuper</surname><given-names>M</given-names></name><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><conf-name>FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</conf-name><article-title>2017 IEEE Conference on Computer Vision and Pattern Recognition</article-title><fpage>1647</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.179</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>RNU</given-names></name><name><surname>Hebert</surname><given-names>L</given-names></name><name><surname>Huelsz-Prince</surname><given-names>G</given-names></name><name><surname>Goos</surname><given-names>YJ</given-names></name><name><surname>Zheng</surname><given-names>X</given-names></name><name><surname>Bozek</surname><given-names>K</given-names></name><name><surname>Stephens</surname><given-names>GJ</given-names></name><name><surname>Tans</surname><given-names>SJ</given-names></name><name><surname>van Zon</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>OrganoidTracker: Efficient cell tracking using machine learning and manual error correction</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0240802</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0240802</pub-id><pub-id pub-id-type="pmid">33091031</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>BD</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1981">1981</year><conf-name>An iterative image registration technique with an application to stereo vision</conf-name><article-title>Proceedings of the 7th international joint conference on Artificial intelligence - Volume 2 IJC</article-title><fpage>674</fpage><lpage>679</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maas</surname><given-names>AL</given-names></name><name><surname>Hannun</surname><given-names>AY</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2013">2013</year><conf-name>Rectifier nonlinearities improve neural network acoustic models</conf-name><article-title>CML Workshop on Deep Learning for Audio, Speech and Language Processing</article-title></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnusson</surname><given-names>KEG</given-names></name><name><surname>Jalden</surname><given-names>J</given-names></name><name><surname>Gilbert</surname><given-names>PM</given-names></name><name><surname>Blau</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Global linking of cell tracks using the Viterbi algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>34</volume><fpage>911</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1109/TMI.2014.2370951</pub-id><pub-id pub-id-type="pmid">25415983</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maška</surname><given-names>M</given-names></name><name><surname>Daněk</surname><given-names>O</given-names></name><name><surname>Garasa</surname><given-names>S</given-names></name><name><surname>Rouzaut</surname><given-names>A</given-names></name><name><surname>Muñoz-Barrutia</surname><given-names>A</given-names></name><name><surname>Ortiz-de-Solorzano</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Segmentation and shape tracking of whole fluorescent cells based on the Chan-Vese model</article-title><source>IEEE Transactions on Medical Imaging</source><volume>32</volume><fpage>995</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1109/TMI.2013.2243463</pub-id><pub-id pub-id-type="pmid">23372077</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maška</surname><given-names>M</given-names></name><name><surname>Ulman</surname><given-names>V</given-names></name><name><surname>Svoboda</surname><given-names>D</given-names></name><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Ederra</surname><given-names>C</given-names></name><name><surname>Urbiola</surname><given-names>A</given-names></name><name><surname>España</surname><given-names>T</given-names></name><name><surname>Venkatesan</surname><given-names>S</given-names></name><name><surname>Balak</surname><given-names>DMW</given-names></name><name><surname>Karas</surname><given-names>P</given-names></name><name><surname>Bolcková</surname><given-names>T</given-names></name><name><surname>Streitová</surname><given-names>M</given-names></name><name><surname>Carthel</surname><given-names>C</given-names></name><name><surname>Coraluppi</surname><given-names>S</given-names></name><name><surname>Harder</surname><given-names>N</given-names></name><name><surname>Rohr</surname><given-names>K</given-names></name><name><surname>Magnusson</surname><given-names>KEG</given-names></name><name><surname>Jaldén</surname><given-names>J</given-names></name><name><surname>Blau</surname><given-names>HM</given-names></name><name><surname>Dzyubachyk</surname><given-names>O</given-names></name><name><surname>Křížek</surname><given-names>P</given-names></name><name><surname>Hagen</surname><given-names>GM</given-names></name><name><surname>Pastor-Escuredo</surname><given-names>D</given-names></name><name><surname>Jimenez-Carretero</surname><given-names>D</given-names></name><name><surname>Ledesma-Carbayo</surname><given-names>MJ</given-names></name><name><surname>Muñoz-Barrutia</surname><given-names>A</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name><name><surname>Kozubek</surname><given-names>M</given-names></name><name><surname>Ortiz-de-Solorzano</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A benchmark for comparison of cell tracking algorithms</article-title><source>Bioinformatics</source><volume>30</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu080</pub-id><pub-id pub-id-type="pmid">24526711</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Mastodon Science</collab></person-group><year iso-8601-date="2021">2021</year><data-title>mastodon</data-title><version designator="2f1572c">2f1572c</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mastodon-sc/mastodon">https://github.com/mastodon-sc/mastodon</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Maška</surname><given-names>M</given-names></name><name><surname>Sorokin</surname><given-names>DV</given-names></name><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Ortiz-de-Solórzano</surname><given-names>C</given-names></name><name><surname>Kozubek</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cell Tracking Accuracy Measurement Based on Comparison of Acyclic Oriented Graphs</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0144959</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0144959</pub-id><pub-id pub-id-type="pmid">26683608</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moen</surname><given-names>E</given-names></name><name><surname>Bannon</surname><given-names>D</given-names></name><name><surname>Kudo</surname><given-names>T</given-names></name><name><surname>Graf</surname><given-names>W</given-names></name><name><surname>Covert</surname><given-names>M</given-names></name><name><surname>Van Valen</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning for cellular image analysis</article-title><source>Nature Methods</source><volume>16</volume><fpage>1233</fpage><lpage>1246</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id><pub-id pub-id-type="pmid">31133758</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>J</given-names></name><name><surname>Allan</surname><given-names>C</given-names></name><name><surname>Besson</surname><given-names>S</given-names></name><name><surname>Burel</surname><given-names>JM</given-names></name><name><surname>Diel</surname><given-names>E</given-names></name><name><surname>Gault</surname><given-names>D</given-names></name><name><surname>Kozlowski</surname><given-names>K</given-names></name><name><surname>Lindner</surname><given-names>D</given-names></name><name><surname>Linkert</surname><given-names>M</given-names></name><name><surname>Manz</surname><given-names>T</given-names></name><name><surname>Moore</surname><given-names>W</given-names></name><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Tischer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>OME-NGFF: a next-generation file format for expanding bioimaging data-access strategies</article-title><source>Nature Methods</source><volume>18</volume><fpage>1496</fpage><lpage>1498</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01326-w</pub-id><pub-id pub-id-type="pmid">34845388</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JI</given-names></name><name><surname>Bao</surname><given-names>Z</given-names></name><name><surname>Boyle</surname><given-names>TJ</given-names></name><name><surname>Boeck</surname><given-names>ME</given-names></name><name><surname>Mericle</surname><given-names>BL</given-names></name><name><surname>Nicholas</surname><given-names>TJ</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Sandel</surname><given-names>MJ</given-names></name><name><surname>Waterston</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Automated analysis of embryonic gene expression with cellular resolution in <italic>C. elegans</italic></article-title><source>Nature Methods</source><volume>5</volume><fpage>703</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1228</pub-id><pub-id pub-id-type="pmid">18587405</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>V</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2010">2010</year><conf-name>Rectified linear units improve Restricted Boltzmann machines</conf-name><article-title>ICML 2010 - Proceedings, 27th International Conference on Machine Learning</article-title></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>W</given-names></name><name><surname>Aristov</surname><given-names>A</given-names></name><name><surname>Lelek</surname><given-names>M</given-names></name><name><surname>Hao</surname><given-names>X</given-names></name><name><surname>Zimmer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning massively accelerates super-resolution localization microscopy</article-title><source>Nature Biotechnology</source><volume>36</volume><fpage>460</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1038/nbt.4106</pub-id><pub-id pub-id-type="pmid">29658943</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Köpf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>BigDataViewer: visualization and processing for large image data sets</article-title><source>Nature Methods</source><volume>12</volume><fpage>481</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3392</pub-id><pub-id pub-id-type="pmid">26020499</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Ronneberger</surname><given-names>O</given-names></name></person-group><source>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</source><publisher-name>Springer</publisher-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scherr</surname><given-names>T</given-names></name><name><surname>Löffler</surname><given-names>K</given-names></name><name><surname>Böhland</surname><given-names>M</given-names></name><name><surname>Mikut</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0243219</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0243219</pub-id><pub-id pub-id-type="pmid">33290432</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Frise</surname><given-names>E</given-names></name><name><surname>Kaynig</surname><given-names>V</given-names></name><name><surname>Longair</surname><given-names>M</given-names></name><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Rueden</surname><given-names>C</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Schmid</surname><given-names>B</given-names></name><name><surname>Tinevez</surname><given-names>JY</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Eliceiri</surname><given-names>K</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fiji: an open-source platform for biological-image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><pub-id pub-id-type="pmid">22743772</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>CA</given-names></name><name><surname>Rasband</surname><given-names>WS</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>NIH Image to ImageJ: 25 years of image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>671</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2089</pub-id><pub-id pub-id-type="pmid">22930834</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sugawara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021a</year><data-title>elephant server</data-title><version designator="swh:1:rev:8935febdbcb2e2d6ba2220ca139e765db44e6458">swh:1:rev:8935febdbcb2e2d6ba2220ca139e765db44e6458</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a3028f2a4adb71c0cc6249963f0777c6198d8602;origin=https://github.com/elephant-track/elephant-server;visit=swh:1:snp:2efc080405dc4ba11998f598bb4e9e785f39d314;anchor=swh:1:rev:8935febdbcb2e2d6ba2220ca139e765db44e6458">https://archive.softwareheritage.org/swh:1:dir:a3028f2a4adb71c0cc6249963f0777c6198d8602;origin=https://github.com/elephant-track/elephant-server;visit=swh:1:snp:2efc080405dc4ba11998f598bb4e9e785f39d314;anchor=swh:1:rev:8935febdbcb2e2d6ba2220ca139e765db44e6458</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sugawara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>Align Slices 3D+t extension</data-title><version designator="swh:1:rev:36c6cb6ccb7e308f9349ec26294d408c35be1ed7">swh:1:rev:36c6cb6ccb7e308f9349ec26294d408c35be1ed7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:663a99923602d153e97af69164cd6762ed80f51d;origin=https://github.com/elephant-track/align-slices3d;visit=swh:1:snp:d18a8bf98eee86f6fe757f2087dcca11b051f897;anchor=swh:1:rev:36c6cb6ccb7e308f9349ec26294d408c35be1ed7">https://archive.softwareheritage.org/swh:1:dir:663a99923602d153e97af69164cd6762ed80f51d;origin=https://github.com/elephant-track/align-slices3d;visit=swh:1:snp:d18a8bf98eee86f6fe757f2087dcca11b051f897;anchor=swh:1:rev:36c6cb6ccb7e308f9349ec26294d408c35be1ed7</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sugawara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021c</year><data-title>ELEPHANT: Tracking cell lineages in 3D by incremental deep learning</data-title><version designator="swh:1:rev:449f9ff8ad17ce75f355e18f815653ec0aa4bbb8">swh:1:rev:449f9ff8ad17ce75f355e18f815653ec0aa4bbb8</version><source>SoftwareHeritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e69da53d731182d6c6ffcb97588396e59a472e4f;origin=https://github.com/elephant-track/elephant-client;visit=swh:1:snp:f7f13f47ba9af8edaef97291b89bc4825a63a1b9;anchor=swh:1:rev:449f9ff8ad17ce75f355e18f815653ec0aa4bbb8">https://archive.softwareheritage.org/swh:1:dir:e69da53d731182d6c6ffcb97588396e59a472e4f;origin=https://github.com/elephant-track/elephant-client;visit=swh:1:snp:f7f13f47ba9af8edaef97291b89bc4825a63a1b9;anchor=swh:1:rev:449f9ff8ad17ce75f355e18f815653ec0aa4bbb8</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tseng</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>I</given-names></name><name><surname>Duchemin-Pelletier</surname><given-names>E</given-names></name><name><surname>Azioune</surname><given-names>A</given-names></name><name><surname>Carpi</surname><given-names>N</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Filhol</surname><given-names>O</given-names></name><name><surname>Piel</surname><given-names>M</given-names></name><name><surname>Théry</surname><given-names>M</given-names></name><name><surname>Balland</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A new micropatterning method of soft substrates reveals that different tumorigenic signals can promote or reduce cell contraction levels</article-title><source>Lab on a Chip</source><volume>11</volume><fpage>2231</fpage><lpage>2240</lpage><pub-id pub-id-type="doi">10.1039/c0lc00641f</pub-id><pub-id pub-id-type="pmid">21523273</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulman</surname><given-names>V</given-names></name><name><surname>Maška</surname><given-names>M</given-names></name><name><surname>Magnusson</surname><given-names>KEG</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Haubold</surname><given-names>C</given-names></name><name><surname>Harder</surname><given-names>N</given-names></name><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Matula</surname><given-names>P</given-names></name><name><surname>Svoboda</surname><given-names>D</given-names></name><name><surname>Radojevic</surname><given-names>M</given-names></name><name><surname>Smal</surname><given-names>I</given-names></name><name><surname>Rohr</surname><given-names>K</given-names></name><name><surname>Jaldén</surname><given-names>J</given-names></name><name><surname>Blau</surname><given-names>HM</given-names></name><name><surname>Dzyubachyk</surname><given-names>O</given-names></name><name><surname>Lelieveldt</surname><given-names>B</given-names></name><name><surname>Xiao</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Cho</surname><given-names>S-Y</given-names></name><name><surname>Dufour</surname><given-names>AC</given-names></name><name><surname>Olivo-Marin</surname><given-names>J-C</given-names></name><name><surname>Reyes-Aldasoro</surname><given-names>CC</given-names></name><name><surname>Solis-Lemus</surname><given-names>JA</given-names></name><name><surname>Bensch</surname><given-names>R</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Stegmaier</surname><given-names>J</given-names></name><name><surname>Mikut</surname><given-names>R</given-names></name><name><surname>Wolf</surname><given-names>S</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name><name><surname>Esteves</surname><given-names>T</given-names></name><name><surname>Quelhas</surname><given-names>P</given-names></name><name><surname>Demirel</surname><given-names>Ö</given-names></name><name><surname>Malmström</surname><given-names>L</given-names></name><name><surname>Jug</surname><given-names>F</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name><name><surname>Muñoz-Barrutia</surname><given-names>A</given-names></name><name><surname>Kozubek</surname><given-names>M</given-names></name><name><surname>Ortiz-de-Solorzano</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An objective comparison of cell-tracking algorithms</article-title><source>Nature Methods</source><volume>14</volume><fpage>1141</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4473</pub-id><pub-id pub-id-type="pmid">29083403</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Sheikh</surname><given-names>HR</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Image quality assessment: from error visibility to structural similarity</article-title><source>IEEE Transactions on Image Processing</source><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/tip.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Schmidt</surname><given-names>U</given-names></name><name><surname>Boothe</surname><given-names>T</given-names></name><name><surname>Müller</surname><given-names>A</given-names></name><name><surname>Dibrov</surname><given-names>A</given-names></name><name><surname>Jain</surname><given-names>A</given-names></name><name><surname>Wilhelm</surname><given-names>B</given-names></name><name><surname>Schmidt</surname><given-names>D</given-names></name><name><surname>Broaddus</surname><given-names>C</given-names></name><name><surname>Culley</surname><given-names>S</given-names></name><name><surname>Rocha-Martins</surname><given-names>M</given-names></name><name><surname>Segovia-Miranda</surname><given-names>F</given-names></name><name><surname>Norden</surname><given-names>C</given-names></name><name><surname>Henriques</surname><given-names>R</given-names></name><name><surname>Zerial</surname><given-names>M</given-names></name><name><surname>Solimena</surname><given-names>M</given-names></name><name><surname>Rink</surname><given-names>J</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Royer</surname><given-names>L</given-names></name><name><surname>Jug</surname><given-names>F</given-names></name><name><surname>Myers</surname><given-names>EW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title><source>Nature Methods</source><volume>15</volume><fpage>1090</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id><pub-id pub-id-type="pmid">30478326</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Schmidt</surname><given-names>U</given-names></name><name><surname>Haase</surname><given-names>R</given-names></name><name><surname>Sugawara</surname><given-names>K</given-names></name><name><surname>Myers</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><conf-name>Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy</conf-name><article-title>2020 IEEE Winter Conference on Applications of Computer Vision</article-title><fpage>3655</fpage><lpage>3662</lpage><pub-id pub-id-type="doi">10.1109/WACV45572.2020.9093435</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>C</given-names></name><name><surname>Miura</surname><given-names>T</given-names></name><name><surname>Voleti</surname><given-names>V</given-names></name><name><surname>Yamaguchi</surname><given-names>K</given-names></name><name><surname>Tsutsumi</surname><given-names>M</given-names></name><name><surname>Yamamoto</surname><given-names>K</given-names></name><name><surname>Otomo</surname><given-names>K</given-names></name><name><surname>Fujie</surname><given-names>Y</given-names></name><name><surname>Teramoto</surname><given-names>T</given-names></name><name><surname>Ishihara</surname><given-names>T</given-names></name><name><surname>Aoki</surname><given-names>K</given-names></name><name><surname>Nemoto</surname><given-names>T</given-names></name><name><surname>Hillman</surname><given-names>EMC</given-names></name><name><surname>Kimura</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>3DeeCellTracker, a deep learning-based pipeline for segmenting and tracking cells in 3D time lapse images</article-title><source>eLife</source><volume>10</volume><elocation-id>e59187</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59187</pub-id><pub-id pub-id-type="pmid">33781383</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>C</given-names></name><name><surname>Tinevez</surname><given-names>JY</given-names></name><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Stamataki</surname><given-names>E</given-names></name><name><surname>Harich</surname><given-names>B</given-names></name><name><surname>Guignard</surname><given-names>L</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Shorte</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Pavlopoulos</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multi-view light-sheet imaging and tracking with the MaMuT software reveals the cell lineage of a direct developing arthropod limb</article-title><source>eLife</source><volume>7</volume><elocation-id>e34410</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34410</pub-id><pub-id pub-id-type="pmid">29595475</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Group Normalization</article-title><source>International Journal of Computer Vision</source><volume>128</volume><fpage>742</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1007/s11263-019-01198-w</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69380.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Iqbal</surname><given-names>Asim</given-names></name><role>Reviewer</role><aff><institution>EPFL</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Tischer</surname><given-names>Christian</given-names></name><role>Reviewer</role><aff><institution>EMBL</institution><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.02.26.432552">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.02.26.432552v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Tracking cell lineages in 3D by incremental deep learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Aleksandra Walczak as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Asim Iqbal (Reviewer #1); Christian Tischer (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Please address the usability of the code (i.e., please see all three reviewers comments and try to address them).</p><p>2) Please address reviewer comments regarding if a 3DUNet is sufficient, and truly state of the art; please provide results comparing to other architectures (such as StarDist, as reviewer 2 points out, or other architectures, see reviewer 1). Also, please consider adding limitations around network options to the discussion.</p><p>3) Multiple reviewers have questions around the ellipse tracker – could you show this is best / alternatives for cells with other shapes, and please consider a limitation regarding this (i.e., I assume a neuron with a long axon would not be well fit by an ellipse).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>– Consider moving figure 1 in supplementary and make figure 2 as figure 1.</p><p>– Add a block diagram to show how to use ELEPHANT step by step through an example.</p><p>– It would be nice to show training, validation curves of 3D U-Nets in the supplementary figures to confirm if there is no over-fitting in the models.</p><p>– Demonstrate the usage and performance of the framework on diverse examples in the main figures (e.g. Figure 2).</p><p>– Expand the Result section in the study.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>More details about running the code: The specific link to the documentation was broken on github. I tried to go through the instructions in the readme, discovered that Mastodon installation requires java sdk (these instructions should be included for linux, windows and mac). Once java was installed the user needs to convert tiffs, like those from celltrackingchallenge.net, into bigdataviewer format. This procedure should be documented at least for the format provided on celltrackingchallenge. For instance, I opened a folder of tiffs in imageJ, converted to a stack, then saved as an xml. If there is a way to do this for instance from the command line it would be great for it to be documented. Next I opened the xml from inside mastodon (the window that pops up from &quot;java -jar elephant-0.1.0-client.jar&quot;). I was able to open bdv but when I tried to do anything in the elephant plugin it said connection refused. I was running the code on Ubuntu 18.04.</p><p>It would be helpful to include more discussion about the amount of data needed, and the amount of manual input. This tool has increased practical value if ~1 month of interactive tracking (as described in the paper) is not needed for each dataset. It is excellent that the networks in the paper are provided as pth files. Can you have the networks as options in the mastodon plugin so that users can easily access them?</p><p>Can you comment on the use of ellipses to approximate nuclei instead of more complex shapes? Is the advantage of this representation that it is easy to use in the case of sparse labels? Or do you see it as advantageous to allow overlapping masks? Similarly, for the optical flow model, the output of the detection model is used to compute optical flow, so ellipses instead of precise cell boundaries. Have you considered how having precise cell boundaries might help the optical flow model perform better?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Overall I think this is fantastic work and I would be very happy to review a revised version of the software.</p><p>30: I would just write &quot;..for 3D cell tracking..&quot;</p><p>37: &quot;in a crustacean (1 week)&quot; It is not clear to me what the &quot;1 week&quot; refers to. Maybe the number of time points and cells would be more informative in this technical context?</p><p>63: It is not really &quot;based on Fiji&quot;, maybe write &quot;deployed in Fiji&quot;?</p><p>194: &quot;To reduce the amount of…&quot; Does one also need to duplicate the data when running client and server on the same computer? For big image data it would be very nice to avoid this.</p><p>214: Insert space between &quot;without &quot;</p><p>215: &quot;showed non-negligible variations in intensity&quot; Is this a problem for the deep learning detection model? If so, this should be elaborated on and a section &quot;Image data preparation&quot; where this is explained should be added to the documentation.</p><p>224: &quot;On the server, images, annotation labels and outputs were stored in the Zarr format,&quot; I am curious: Why is it necessary to store the image in Zarr format rather than HDF5?</p><p>226: &quot;these data were prepared using a custom Python script&quot; Running a python script within a Docker container could be quite a hurdle for non-computational end-users. Any chance that could be simplified?</p><p>247: &quot;<italic>L</italic>prioris&quot; There is a space missing.</p><p>ELEPHANT/Mastodon software and documentation</p><p>Mastodon</p><p>The author's software should be compatible with the latest version of Mastodon, which includes a few bug fixes that avoid hanging of the software during the annotation process.</p><p>Example demo data set</p><p>To get started, the authors provide an example data set, which is great. However, for me training the detection and linkage on the current example data set takes too much time to be done during a review of the publication. I would appreciate if the authors provided a much simpler demo dataset where everything (detection + linkage) could be done within maximally 30 minutes of work. I think for reviewing the software and also for beginner users such a toy data set would be extremly useful.</p><p>Server connection</p><p>I think adding something to the user interface that makes the connection to the server more explicit would be very nice.</p><p>For example: Plugins &gt; ELEPHANT &gt; Connect to Server</p><p>Then one could put functionality there that would, e.g., allow the user to check whether the connection is working and maybe some feedback about which server one is connecting to.</p><p>In fact, for connecting to the Google Colab sever one should explore whether it is possible to create a UI in Mastodon where the user could just copy and paste these two lines:</p><p>SSH command: ssh -p10739 root@8.tcp.ngrok.io</p><p>Root password: qXzK8cOwvkWxdAcGZwM0</p><p>And then the Java code would parse those two lines create system calls to establish the server connection via the two SSH commands. This would be much more convenient than the current workflow where one needs to open a terminal and modify tedious SSH command line calls (also, many less IT savvy users could be put off by the command line calls).</p><p>Maybe for the other server modes similar ideas could be explored (personally I only looked into the Colab based solution).</p><p>It would be great if there was more feedback within the client on what is happening right now on the server side. I added specific suggestions in few places (see below). One could even consider mirroring all the text output that is generated server side in the Elephant client log window.</p><p>Training of detection</p><p>While I think I get the point now, it is a bit though to understand all the different tags (TP,FP,…).</p><p>What I understood now is that probably it is OK to simply add spots manually and they would be used as training data (being tagged as TP by default). If that is true I would suggest to split the annotation workflow in the documentation in a basic and advanced version, where in the basic version one maybe does not need to explicitly provide manual tags at all?!</p><p>https://elephant-track.github.io/#/v0.1/?id=_2-shortcuts</p><p>Current text: If you cannot find the ~/.mastodon/keymaps/ directory, please run [File &gt; Preferences…] first to create it with the ~/.mastodon/keymaps/keymaps.yaml.</p><p>Suggested text: If you cannot find the ~/.mastodon/keymaps/ directory, please run [File &gt; Preferences…] and click [OK] to create it. Please restart Mastodon for the Elephant keymap to become active.</p><p>In addition, it would really be great if setting up the keymap.yaml file was easier.</p><p>One could for example provide the already edited keymap.yaml file for download and tell the user to replace the current one. Since you are shipping a stand-alone version of Mastodon anyway, even better would be if that was somehow included in (or taken care of by) the elephant.jar. Could you somehow ship this information inside the jar?</p><p>https://elephant-track.github.io/#/v0.1/?id=detection-workflow</p><p>I would recommend adding a sentence here that first the connection to the server needs to be established.</p><p>https://elephant-track.github.io/#/v0.1/?id=_5-establish-connections-from-your-computer-to-the-server-on-colab</p><p>It would be nice to add an explanation why one needs to establish two connections (rather than only one).</p><p>https://elephant-track.github.io/#/v0.1/?id=_2-initialize-a-model</p><p>It would be very good if there was more feedback within the Mastodon UI about whether and when the model initialization has finished successfully.</p><p>Also feedback about the training progress, e.g. the decrease of the loss, the current cycle, a progress bar, would be great such that one can judge how well the training worked and whether the current number of training cycles is adequat.</p><p>Typo in Mastodon: &quot;Detection &gt; Reset *a* Seg Model&quot;. I suggest removing the &quot;a&quot;.</p><p>&quot;Predicted spots and manually added spots are tagged by default as unlabeled and fn, respectively.&quot;</p><p>I wonder whether manually added spots should be tagged as tp by default? At least I often forgot clicking &quot;4&quot; to mark them as tp. In fact, I am confused now, because maybe the manually added spots are tagged as tp by default?</p><p>https://elephant-track.github.io/#/v0.1/?id=_6-importing-and-extending-a-pretrained-model</p><p>Importing a pretrained model is simple. Just specify the model parameter file located at the workspace/models in the settings.</p><p>I could not figure out where to specify the model parameter file. On the client or on the server? And how to do it exactly?</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Tracking cell lineages in 3D by incremental deep learning&quot; for further consideration by <italic>eLife</italic>. Your revised article has been reviewed by 3 peer reviewers and the evaluation has been overseen by Aleksandra Walczak as the Senior Editor, and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Please be sure to not use the term &quot;state of the art&quot; (SOTA) unless you demonstrate truly best performance (which you do not) – it is not a requirement to be SOTA to be published. Moreover, please address reviewer #2's request, and consider reviewer #3, i.e., providing local GPU instructions (vs only COLAB).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Thanks to the authors for submitting the revised manuscript and providing the response to the reviewers' comments. The manuscript, as well as the codebase, are significantly updated after taking the feedback from the reviewers into account, in particular, figure 3 is a useful addition in the manuscript and it showcases the performance of ELEPHANT on diverse datasets. A systematic comparison between ELEPHANT and StarDist 3D is also useful to evaluate the performance comparison.</p><p>However, the limited performance of ELEPHANT on segmentation tasks is expected since the method is limited to detect ellipsoid shape-based objects but since the method is focused on only detection and tracking so it would be useful to state it clearly in the abstract and manuscript. This will help the users to get a better idea about the strengths and limitations of the toolbox in advance. Overall the study seems to be in much better shape now.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Thank you for the really great improvements to usability. I was able to easily install Elephant and Mastodon through Fiji. The google colab server setup took around 30 minutes to get started – I'm not sure if there's any way to make it faster, but wanted to point it out. After that I tried to &quot;add port forward&quot; and received a &quot;Connection refused&quot; error, there was no pop up to input my password. Is there another step with rabbitMQ permissions perhaps that I'm missing?</p><p>Thanks for also running StarDist on one of the frames. Can you please add quantitative metrics to Supplementary Figure 8? Maybe they are somewhere but I missed them and apologies if I did. Given StarDist does not have temporal information, it is likely that Elephant outperforms StarDist, but it would be good to include the quantitative results for the reader to be able to decide whether to use StarDist or Elephant. Thanks for the information about how stardist+trackmate are only in 2D.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>First of all we would like to congratulate the authors for doing a great job in addressing the issues that we have raised in the previous review. As a result the software is in our view now much more user friendly; for example connecting from the Fiji user interface to the deep learning server is a great improvement as compared to the previous command line based way.</p><p>However, in practice we still struggled to reliably work with the Google Colab server and we feel that this might be a source of frustration for the potential users of the software. In the previous version of the software the authors also presented another solution (i.e. a local server), given that the users would have a computer with an appropriate GPU. Maybe one could reconsider those ideas?</p><p>We are also wondering, given the advances in running deep learning models in Java (DeepImageJ and CSDBDeep) whether a fully Java based (i.e. one Fiji plugin) solution would be feasible to make this great tool more user friendly and stable? We know that this would not solve the issue of providing the GPU resources, but maybe users would then simply need to have a computer with a GPU (which we think could be &quot;fair enough&quot;).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69380.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Please address the usability of the code (i.e., please see all three reviewers comments and try to address them).</p></disp-quote><p>We have implemented many changes that improved the usability of the code, following the reviewers' suggestions (see our responses to individual reviewers' comments, below).</p><disp-quote content-type="editor-comment"><p>2) Please address reviewer comments regarding if a 3DUNet is sufficient, and truly state of the art; please provide results comparing to other architectures (such as StarDist, as reviewer 2 points out, or other architectures, see reviewer 1). Also, please consider adding limitations around network options to the discussion.</p></disp-quote><p>ELEPHANT is state of the art because it makes deep learning available for cell tracking/lineaging in the absence of extensive annotated datasets for training. To achieve this, the network architecture must support training with sparse annotation. 3DUNet is adapted to this strategy. Existing deep learning applications which employ other architectures, such as StarDist, do not currently fulfil this purpose.</p><p>We now show that ELEPHANT models trained with sparse annotations perform similarly well to trained StarDist3D models for nuclear detection in single 3D stacks (see Figure 3—figure supplement 1). For cell tracking over time, StarDist and Trackmate have so far only been implemented in 2D and could therefore not be used on our 3D datasets.</p><p>As we describe in the paper, using the 3DUNet architecture, ELEPHANT outperformed a large number of other tracking applications in the Cell Tracking Challenge (http://celltrackingchallenge.net/latest-ctb-results/) in both cell detection and linking (tracking). This comparison includes methods that employ deep-learning.</p><p>ELEPHANT is less performant than other algorithms in segmentation, which is not the software's main purpose. We explain this point in the revised manuscript, page 7.</p><disp-quote content-type="editor-comment"><p>3) Multiple reviewers have questions around the ellipse tracker – could you show this is best / alternatives for cells with other shapes, and please consider a limitation regarding this (i.e., I assume a neuron with a long axon would not be well fit by an ellipse).</p></disp-quote><p>We use ellipsoids for annotation because they are essential for rapid and efficient training and predictions, which are the backbone of interactive deep learning (see detailed response to reviewer 2). This is in fact acknowledged as one of the strengths of our method by reviewer 3. We now explain this in page 4 of the revised manuscript.</p><p>We provide additional data showing that using ellipsoids is sufficient for detection of elongated and irregularly-shaped cells using ELEPHANT (Figure 3E). Post-processing can be appended to our workflow if a user needs to extract the precise morphology of the tracked nuclei or cells.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>– Consider moving figure 1 in supplementary and make figure 2 as figure 1.</p></disp-quote><p>Incremental training is the key concept of our method and we want to present this in figure 1.</p><disp-quote content-type="editor-comment"><p>– Add a block diagram to show how to use ELEPHANT step by step through an example.</p></disp-quote><p>We thank the reviewer for this suggestion. We have added a block diagram as Figure 1—figure supplement 2. We also provide a demo dataset and we are preparing a step by step video tutorial, which will be embedded in the user manual (also see response to reviewer 3).</p><disp-quote content-type="editor-comment"><p>– It would be nice to show training, validation curves of 3D U-Nets in the supplementary figures to confirm if there is no over-fitting in the models.</p></disp-quote><p>We have included training and validation curves for a pair of CE datasets, which confirm there is no significant over-fitting in the model (Figure 3—figure supplement 2). We present these results in the revised manuscript, page 5.</p><disp-quote content-type="editor-comment"><p>– Demonstrate the usage and performance of the framework on diverse examples in the main figures (e.g. Figure 2).</p><p>– Expand the Result section in the study.</p></disp-quote><p>We have analysed three additional datasets with different characteristics, on which we provide the tracking performance of a pre-trained model as well as the dramatic improvement in performance obtained by the addition of sparse annotations. We present the results in the main text (page 5) and in new Figure 3.</p><p>The new datasets we used are:</p><p>– Intestinal organoids acquired by confocal microscopy (Kok et al. 2020 PLoS One doi:10.1101/2020.03.18.996421)</p><p>– Intestinal organoids acquired by light-sheet microscopy (de Madeiros et al. 2021 bioRxiv doi:10.1101/2021.05.12.443427)</p><p>– MDA231 human breast carcinoma cells, including elongated and irregularly-shaped cells (Fluo-C3DL-MDA231 dataset from Cell Tracking Challenge)</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>More details about running the code: The specific link to the documentation was broken on github. I tried to go through the instructions in the readme, discovered that Mastodon installation requires java sdk (these instructions should be included for linux, windows and mac). Once java was installed the user needs to convert tiffs, like those from celltrackingchallenge.net, into bigdataviewer format. This procedure should be documented at least for the format provided on celltrackingchallenge. For instance, I opened a folder of tiffs in imageJ, converted to a stack, then saved as an xml. If there is a way to do this for instance from the command line it would be great for it to be documented. Next I opened the xml from inside mastodon (the window that pops up from &quot;java -jar elephant-0.1.0-client.jar&quot;). I was able to open bdv but when I tried to do anything in the elephant plugin it said connection refused. I was running the code on Ubuntu 18.04.</p></disp-quote><p>We thank the reviewer for raising these practical issues that matter to the users. We have addressed these issues as follows:</p><p>– The practical issue with the broken link has been fixed.</p><p>– ELEPHANT is now available as an extension on Fiji. The Fiji environment automatically installs all the components needed to run ELEPHANT/Mastodon, with no need for the users to install additional packages. We agree that this will greatly facilitate the use of ELEPHANT by non-expert users.</p><p>– We prepared a command line tool for converting Cell Tracking Challenge data into datasets usable on ELEPHANT/Mastodon. Fiji also provides a user-friendly GUI tool for this purpose.</p><p>– In addition, we have introduced numerous changes to facilitate user interactions, particularly through a control panel that allows users to set up the server and to monitor its status.</p><p>– We have added a demo dataset and pre-trained models that users can use to test ELEPHANT (explained in the updated user manual).</p><p>– We are in the process of producing a video tutorial, which will be embedded in the user manual and available on YouTube.</p><disp-quote content-type="editor-comment"><p>It would be helpful to include more discussion about the amount of data needed, and the amount of manual input. This tool has increased practical value if ~1 month of interactive tracking (as described in the paper) is not needed for each dataset. It is excellent that the networks in the paper are provided as pth files. Can you have the networks as options in the mastodon plugin so that users can easily access them?</p></disp-quote><p>We now show that a generic pre-trained model can be re-trained with a very modest amount of new annotations – less than 10 nuclei, requiring an annotation time of a few minutes – to achieve a very high level of precision and recall in most datasets (Figure 3). This is shown with diverse image datasets (<italic>C. elegans</italic> embryo, <italic>Parhyale</italic> regenerating limbs, human intestinal organoids, breast carcinoma cells, captured either by confocal or light sheet microscopy).</p><p>Within ELEPHANT, we now provide a pre-trained model that has been trained on a wide range of image datasets. Users can start their tracking using this pre-trained model, which provides a basic level of performance.</p><p>All trained models are saved and can be re-used for subsequent tracking by selecting the required model in the Preferences panel (the generic pre-trained model is used as a default). Users can also share and access trained models through URL links (explained in the updated user manual).</p><disp-quote content-type="editor-comment"><p>Can you comment on the use of ellipses to approximate nuclei instead of more complex shapes? Is the advantage of this representation that it is easy to use in the case of sparse labels? Or do you see it as advantageous to allow overlapping masks? Similarly, for the optical flow model, the output of the detection model is used to compute optical flow, so ellipses instead of precise cell boundaries. Have you considered how having precise cell boundaries might help the optical flow model perform better?</p></disp-quote><p>As we now explain in the manuscript (page 4), we use ellipsoids for annotation because they are essential for rapid and efficient training and predictions, compared with complex shapes, which require a larger number of parameters to describe them. This is essential for interactive (real-time) training. In practice, using ellipsoids also reduces the amount of work required for annotating the data, compared with precise drawing of cell outlines. Ellipsoids also take less memory space than more complex annotations in the tracked data files.</p><p>Ellipsoids indeed allow overlapping masks, which could be an advantage when dealing with complex objects under low spatial resolution.</p><p>To clarify the process followed in ELEPHANT: the optical flow maps are generated directly from the images, with no input from the detected ellipsoids; the annotated nuclei are linked subsequently by associating the flow maps with the detected nuclei (see Methods).</p><p>ELEPHANT has currently been built as an extension of Mastodon, which uses ellipsoids for cell annotation. In this framework we are not able to test and compare the potential impact of using precise cell annotations, but this might be relevant to test in a different framework.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Overall I think this is fantastic work and I would be very happy to review a revised version of the software.</p><p>30: I would just write &quot;..for 3D cell tracking..&quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>37: &quot;in a crustacean (1 week)&quot; It is not clear to me what the &quot;1 week&quot; refers to. Maybe the number of time points and cells would be more informative in this technical context?</p></disp-quote><p>We have added the number of timepoints.</p><disp-quote content-type="editor-comment"><p>63: It is not really &quot;based on Fiji&quot;, maybe write &quot;deployed in Fiji&quot;?</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>194: &quot;To reduce the amount of…&quot; Does one also need to duplicate the data when running client and server on the same computer? For big image data it would be very nice to avoid this.</p></disp-quote><p>Duplicating the data is currently required because Mastodon only supports the HDF5/XML format while the ELEPHANT server supports the Zarr format. This point will be addressed by future updates of the software.</p><disp-quote content-type="editor-comment"><p>214: Insert space between &quot;without &quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>215: &quot;showed non-negligible variations in intensity&quot; Is this a problem for the deep learning detection model? If so, this should be elaborated on and a section &quot;Image data preparation&quot; where this is explained should be added to the documentation.</p></disp-quote><p>On the server, the image data are stored in unsigned 8-bit or unsigned 16-bit format, keeping the original image format. At the beginning of processing on the server, the image data are automatically converted to a 32-bit float and their intensity is normalized at each timepoint in a way that the minimum and maximum values become 0 and 1. The text in this part of the manuscript refers to normalization on the client, which is helpful for a better visualization on Mastodon. This is now explained in the revised manuscript in the section 'Dataset preparation'.</p><disp-quote content-type="editor-comment"><p>224: &quot;On the server, images, annotation labels and outputs were stored in the Zarr format,&quot; I am curious: Why is it necessary to store the image in Zarr format rather than HDF5?</p></disp-quote><p>We chose this format because we expected that Zarr would be more rapid than HDF5 (see Moore et al. 2021, bioRxiv 2021.03.31.437929; Kang et al. 2019, Proc Int Conf High Performance Computing, Networking, Storage and Analysis), which is preferable when the ELEPHANT server is deployed on the cloud environment. However we have not compared the speed of these formats directly.</p><disp-quote content-type="editor-comment"><p>226: &quot;these data were prepared using a custom Python script&quot; Running a python script within a Docker container could be quite a hurdle for non-computational end-users. Any chance that could be simplified?</p></disp-quote><p>In the latest version of ELEPHANT, this conversion can also be done from the client application. This is explained in the revised manuscript, page 11.</p><disp-quote content-type="editor-comment"><p>247: &quot;<italic>L</italic>prioris&quot; There is a space missing.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>ELEPHANT/Mastodon software and documentation</p><p>Mastodon</p><p>The author's software should be compatible with the latest version of Mastodon, which includes a few bug fixes that avoid hanging of the softare during the annotation process.</p></disp-quote><p>After introducing some updates on both Mastodon and ELEPHANT, the software is now implemented on the latest version of Mastodon. ELEPHANT is also now deployed in Fiji.</p><disp-quote content-type="editor-comment"><p>Example demo data set</p><p>To get started, the authors provide an example data set, which is great. However, for me training the detection and linkage on the current example data set takes too much time to be done during a review of the publication. I would appreciate if the authors provided a much simpler demo dataset where everything (detection + linkage) could be done within maximally 30 minutes of work. I think for reviewing the software and also for beginner users such a toy data set would be extremly useful.</p></disp-quote><p>We now provide a new demo dataset that is a small subset of the CE1 dataset (see revised user manual).</p><disp-quote content-type="editor-comment"><p>Server connection</p><p>I think adding something to the user interface that makes the connection to the server more explicit would be very nice.</p><p>For example: Plugins &gt; ELEPHANT &gt; Connect to Server</p><p>Then one could put functionality there that would, e.g., allow the user to check whether the connection is working and maybe some feedback about which server one is connecting to.</p></disp-quote><p>To facilitate the setting up of the ELEPHANT server, we have now implemented a control panel that allows users to monitor the process and provides links to the relevant section of the user manual and to Google Colab (see revised user manual).</p><disp-quote content-type="editor-comment"><p>In fact, for connecting to the Google Colab sever one should explore whether it is possible to create a UI in Mastodon where the user could just copy and paste these two lines:</p><p>SSH command: ssh -p10739 root@8.tcp.ngrok.io</p><p>Root password: qXzK8cOwvkWxdAcGZwM0</p><p>And then the Java code would parse those two lines create system calls to establish the server connection via the two SSH commands. This would be much more convenient than the current workflow where one needs to open a terminal and modify tedious SSH command line calls (also, many less IT savy users could be put off by the command line calls).</p><p>Maybe for the other server modes similar ideas could be explored (personally I only looked into the Colab based solution).</p></disp-quote><p>We have now implemented an SSH client that runs on ELEPHANT using the JSch library. Users can establish a connection to the server via SSH from the ELEPHANT Control Panel.</p><disp-quote content-type="editor-comment"><p>It would be great if there was more feedback within the client on what is happening right now on the server side. I added specific suggestions in few places (see below). One could even consider mirroring all the text output that is generated server side in the Elephant client log window.</p></disp-quote><p>In addition to the client log window, we now implement a server log window that duplicates the server-side text output to the client side.</p><disp-quote content-type="editor-comment"><p>Training of detection</p><p>While I think I get the point now, it is a bit though to understand all the different tags (TP,FP,…).</p><p>What I understood now is that probably it is OK to simply add spots manually and they would be used as training data (being tagged as TP by default). If that is true I would suggest to split the annotation workflow in the documentation in a basic and advanced version, where in the basic version one maybe does not need to explicitly provide manual tags at all?!</p></disp-quote><p>We recognize that the colour tags would be overly complicated for most users, but we think that they can be useful for advanced users. As the reviewer suggests, we have therefore established two colour modes, which users can select depending on their needs (see user manual): a basic colour mode (which is the default) and an advanced colour mode. In the basic mode, only three colour tags are used ('accepted' in cyan, 'rejected' in magenta, 'predicted' in green). The original colors are kept for advanced usage (e.g. to visually inspect prediction results using True/False information). In both modes, False/True information is used in the deep learning to facilitate the training process by giving a higher weight to False annotations.</p><disp-quote content-type="editor-comment"><p>https://elephant-track.github.io/#/v0.1/?id=_2-shortcuts</p><p>Current text: If you cannot find the ~/.mastodon/keymaps/ directory, please run [File &gt; Preferences…] first to create it with the ~/.mastodon/keymaps/keymaps.yaml.</p><p>Suggested text: If you cannot find the ~/.mastodon/keymaps/ directory, please run [File &gt; Preferences…] and click [OK] to create it. Please restart Mastodon for the Elephant keymap to become active.</p><p>In addition, it would really be great if setting up the keymap.yaml file was easier.</p><p>One could for example provide the already edited keymap.yaml file for download and tell the user to replace the current one. Since you are shipping a stand-alone version of Mastodon anyway, even better would be if that was somehow included in (or taken care of by) the elephant.jar. Could you somehow ship this information inside the jar?</p></disp-quote><p>In the latest version of ELEPHANT, the default keymap settings are installed automatically. Users can switch the keymap settings via Mastodon’s Preferences dialog. The user manual has been updated accordingly.</p><disp-quote content-type="editor-comment"><p>https://elephant-track.github.io/#/v0.1/?id=detection-workflow</p><p>I would recommend adding a sentence here that first the connection to the server needs to be established.</p></disp-quote><p>When the connection to the server fails, the latest version of ELEPHANT shows an error dialog with the message &quot;The ELEPHANT server is unavailable. Please set it up first.&quot;. The availability of the server can be checked in the Control Panel.</p><disp-quote content-type="editor-comment"><p>https://elephant-track.github.io/#/v0.1/?id=_5-establish-connections-from-your-computer-to-the-server-on-colab</p><p>It would be nice to add an explanation why one needs to establish two connections (rather than only one).</p></disp-quote><p>The ELEPHANT server provides main functionalities (e.g. detection, linking), while the RabbitMQ server is used to send messages to the client (e.g. progress, completion). The updated version of the user manual explains this.</p><disp-quote content-type="editor-comment"><p>https://elephant-track.github.io/#/v0.1/?id=_2-initialize-a-model</p><p>It would be very good if there was more feedback within the Mastodon UI about whether and when the model initialization has finished successfully.</p></disp-quote><p>The updated version of ELEPHANT shows the progress messages in the server log window.</p><disp-quote content-type="editor-comment"><p>Also feedback about the training progress, e.g. the decrease of the loss, the current cycle, a progress bar, would be great such that one can judge how well the training worked and whether the current number of training cycles is adequat.</p></disp-quote><p>Information on training progress is now available in the server log window. Advanced users can now get access to loss information through TensorBoard (instructions on how to do this will be given in a future update of the user manual).</p><disp-quote content-type="editor-comment"><p>Typo in Mastodon: &quot;Detection &gt; Reset *a* Seg Model&quot;. I suggest removing the &quot;a&quot;.</p></disp-quote><p>The menu titles have been updated.</p><disp-quote content-type="editor-comment"><p>&quot;Predicted spots and manually added spots are tagged by default as unlabeled and fn, respectively.&quot;</p><p>I wonder whether manually added spots should be tagged as tp by default? At least I often forgot clicking &quot;4&quot; to mark them as tp. In fact, I am confused now, because maybe the manually added spots are tagged as tp by default?</p></disp-quote><p>Following the reviewer's suggestion, in the basic color scheme tp and fn are now both highlighted with the same colour (cyan).</p><p>Nuclei that are missed by prediction but subsequently annotated by the user are tagged as fn, which will turn into tp if they are predicted correctly in the next iteration. The distinction between tp and fn is still useful when examining the performance of the detection model. Users can access this information in the advanced mode.</p><disp-quote content-type="editor-comment"><p>https://elephant-track.github.io/#/v0.1/?id=_6-importing-and-extending-a-pretrained-model</p><p>Importing a pretrained model is simple. Just specify the model parameter file located at the workspace/models in the settings.</p><p>I could not figure out where to specify the model parameter file. On the client or on the server? And how to do it exactly?</p></disp-quote><p>The model parameter file can be specified in the Preferences dialog on the client, where the file path is relative to “/workspace/models/” on the server. In the updated version of ELEPHANT, there are two ways to import the pre-trained model parameters:</p><p>1. Upload the pre-trained parameters file to the website that provides a public download URL (e.g. GitHub, Google Drive, Dropbox). Run [Plugins &gt; ELEPHANT &gt; Detection &gt; Reset Detection Model] and select the “From URL” option with the download URL.</p><p>2. Directly place/replace the file at the specified file path on the server.</p><p>The updated version of the user manual includes this explanation.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Please be sure to not use the term &quot;state of the art&quot; (SOTA) unless you demonstrate truly best performance (which you do not) – it is not a requirement to be SOTA to be published. Moreover, please address reviewer #2's request, and consider reviewer #3, i.e., providing local GPU instructions (vs only COLAB).</p></disp-quote><p>We do not apply this term to ELEPHANT, as requested. We apply the term only to benchmarked, best-performing software that we compared with ELEPHANT in the Cell Tracking Challenge.</p><p>We have addressed the relevant comments of reviewers #2 and #3, as requested.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Thanks to the authors for submitting the revised manuscript and providing the response to the reviewers' comments. The manuscript, as well as the codebase, are significantly updated after taking the feedback from the reviewers into account, in particular, figure 3 is a useful addition in the manuscript and it showcases the performance of ELEPHANT on diverse datasets. A systematic comparison between ELEPHANT and StarDist 3D is also useful to evaluate the performance comparison.</p><p>However, the limited performance of ELEPHANT on segmentation tasks is expected since the method is limited to detect ellipsoid shape-based objects but since the method is focused on only detection and tracking so it would be useful to state it clearly in the abstract and manuscript. This will help the users to get a better idea about the strengths and limitations of the toolbox in advance. Overall the study seems to be in much better shape now.</p></disp-quote><p>The title and abstract of the paper state clearly that ELEPHANT's objective is to enable or to facilitate cell tracking. We make no claims about cell or nuclei segmentation.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Thank you for the really great improvements to usability. I was able to easily install Elephant and Mastodon through Fiji. The google colab server setup took around 30 minutes to get started – I'm not sure if there's any way to make it faster, but wanted to point it out. After that I tried to &quot;add port forward&quot; and received a &quot;Connection refused&quot; error, there was no pop up to input my password. Is there another step with rabbitMQ permissions perhaps that I'm missing?</p></disp-quote><p>We have included a video tutorial in the user manual, which shows how to set up ELEPHANT with the Google Collab option. We also provide some online support and advice on troubleshooting to users through the image.sc forum.</p><p>We have reduced the time required for setup to around 15 minutes. The Google Collab option is sometimes restricted/failing due to GPU quota and connection timeout. We now provide tips on how to resolve these issues in the user manual.</p><disp-quote content-type="editor-comment"><p>Thanks for also running StarDist on one of the frames. Can you please add quantitative metrics to Supplementary Figure 8? Maybe they are somewhere but I missed them and apologies if I did. Given StarDist does not have temporal information, it is likely that Elephant outperforms StarDist, but it would be good to include the quantitative results for the reader to be able to decide whether to use StarDist or Elephant. Thanks for the information about how stardist+trackmate are only in 2D.</p></disp-quote><p>We provide the relevant metrics in Figure 3—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>First of all we would like to congratulate the authors for doing a great job in addressing the issues that we have raised in the previous review. As a result the software is in our view now much more user friendly; for example connecting from the Fiji user interface to the deep learning server is a great improvement as compared to the previous command line based way.</p><p>However, in practice we still struggled to reliably work with the Google Colab server and we feel that this might be a source of frustration for the potential users of the software. In the previous version of the software the authors also presented another solution (i.e. a local server), given that the users would have a computer with an appropriate GPU. Maybe one could reconsider those ideas?</p></disp-quote><p>We still provide the option with a local server. The instruction can be found in the user manual at the section 'Advanced options for the ELEPHANT Server' (https://elephant-track.github.io/#/v0.3/?id=advanced-options-for-the-elephant-server)</p><p>The corresponding system requirements can be also found in the user manual (https://elephant-track.github.io/#/v0.3/?id=system-requirements).</p><p>We have kept the following description in the manuscript: &quot;The server environment is provided as a Docker container to ensure easy and reproducible deployment (https://github.com/elephant-track/elephant-server). The server can also be set up with Google Colab in case the user does not have access to a computer that satisfies the system requirements.&quot;</p><disp-quote content-type="editor-comment"><p>We are also wondering, given the advances in running deep learning models in Java (DeepImageJ and CSDBDeep) whether a fully Java based (i.e. one Fiji plugin) solution would be feasible to make this great tool more user friendly and stable? We know that this would not solve the issue of providing the GPU resources, but maybe users would then simply need to have a computer with a GPU (which we think could be &quot;fair enough&quot;).</p></disp-quote><p>We agree that it would be interesting to explore a fully Java based solution. This would require extensive re-writing of the software. We are considering this for future updates.</p></body></sub-article></article>