<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64876</article-id><article-id pub-id-type="doi">10.7554/eLife.64876</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Unsupervised learning of haptic material properties</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-216519"><name><surname>Metzger</surname><given-names>Anna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5704-2821</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-214853"><name><surname>Toscani</surname><given-names>Matteo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1884-5533</contrib-id><email>mtoscani@bournemouth.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05wwcw481</institution-id><institution>Department of Psychology, Bournemouth University</institution></institution-wrap><addr-line><named-content content-type="city">Bournemouth</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Department of Psychology, Justus-Liebig University</institution></institution-wrap><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>23</day><month>02</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e64876</elocation-id><history><date date-type="received" iso-8601-date="2020-11-13"><day>13</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-12-09"><day>09</day><month>12</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-02-26"><day>26</day><month>02</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.02.25.432896"/></event></pub-history><permissions><copyright-statement>© 2022, Metzger and Toscani</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Metzger and Toscani</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64876-v1.pdf"/><abstract><p>When touching the surface of an object, its spatial structure translates into a vibration on the skin. The perceptual system evolved to translate this pattern into a representation that allows to distinguish between different materials. Here, we show that perceptual haptic representation of materials emerges from efficient encoding of vibratory patterns elicited by the interaction with materials. We trained a deep neural network with unsupervised learning (Autoencoder) to reconstruct vibratory patterns elicited by human haptic exploration of different materials. The learned compressed representation (i.e., latent space) allows for classification of material categories (i.e., plastic, stone, wood, fabric, leather/wool, paper, and metal). More importantly, classification performance is higher with perceptual category labels as compared to ground truth ones, and distances between categories in the latent space resemble perceptual distances, suggesting a similar coding. Crucially, the classification performance and the similarity between the perceptual and the latent space decrease with decreasing compression level. We could further show that the temporal tuning of the emergent latent dimensions is similar to properties of human tactile receptors.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>touch</kwd><kwd>natural textures</kwd><kwd>materials</kwd><kwd>unsupervised deep learning</kwd><kwd>haptic perception</kwd><kwd>efficient coding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>222641018 - SFB/TRR 135</award-id><principal-award-recipient><name><surname>Toscani</surname><given-names>Matteo</given-names></name><name><surname>Metzger</surname><given-names>Anna</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Perceptual haptic representation of materials emerges from unsupervised learning as a consequence of efficient encoding of the physical signals at the input of tactile sensory system.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>With our sense of touch, we are able to discriminate a vast number of materials. We usually slide the hand over the material’s surface to perceive its texture (<xref ref-type="bibr" rid="bib23">Lederman and Klatzky, 1987</xref>). Motion between the hand and the material’s surface elicits vibrations on the skin, which are the sensorial input mediating texture perception (<xref ref-type="bibr" rid="bib45">Weber et al., 2013</xref>).</p><p>It was proposed that perceptual representations emerge from learning to efficiently encode sensorial input (<xref ref-type="bibr" rid="bib3">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib31">Olshausen and Field, 1996</xref>; see for a review <xref ref-type="bibr" rid="bib38">Simoncelli and Olshausen, 2001</xref>). For example, in color vision, the excitations of long- and middle-wavelength-sensitive cones are highly correlated. At the second stage of processing, still in the retina, a transformation into two-color opponent and a luminance channel achieves an efficient and decorrelated representation, akin to a principal components analysis (PCA of the input signals <xref ref-type="bibr" rid="bib9">Buchsbaum and Gottschalk, 1983</xref>; <xref ref-type="bibr" rid="bib46">Zaidi, 1997</xref>; <xref ref-type="bibr" rid="bib18">Gegenfurtner, 2003</xref>). Receptive field properties in the early visual pathway (<xref ref-type="bibr" rid="bib2">Atick and Redlich, 1992</xref>; <xref ref-type="bibr" rid="bib31">Olshausen and Field, 1996</xref>) as well as the tuning properties of auditory nerve fibers (<xref ref-type="bibr" rid="bib25">Lewicki, 2002</xref>; <xref ref-type="bibr" rid="bib39">Smith and Lewicki, 2006</xref>) can emerge by efficiently encoding natural images or sounds, respectively. Recently, it was shown that efficient coding could also explain the simultaneous development of vergence and accommodation as a result of maximizing coding efficiency of the retinal signals (<xref ref-type="bibr" rid="bib15">Eckmann et al., 2020</xref>). There is currently a lot of interest whether higher-level representations can also be learned by efficient encoding of the retinal images (<xref ref-type="bibr" rid="bib16">Fleming and Storrs, 2019</xref>).</p><p>Here, we explore whether the perceptual representations of our haptic world can be learned by encoding vibratory signals elicited by the interaction with textures. We used a Deep Convolutional Autoencoder, a framework for unsupervised learning, to reconstruct acceleration recordings from free explorations of 81 everyday materials (<xref ref-type="fig" rid="fig1">Figure 1A and C</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Materials and vibratory signals.</title><p>(<bold>A</bold>) Examples of materials. One example per category is shown. (<bold>B</bold>) Accelerometer mounted on a 3D-printed pen with a steel tip used for acquisition of vibratory signals. (<bold>C</bold>) Original and reconstructed acceleration traces for one example material per category. The black lines represent the original amplitude signals (y-axis) over time (x-axis), and the red line represents the corresponding reconstructed signals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64876-fig1-v1.tif"/></fig><p>We trained the Deep Convolutional Autoencoder to reconstruct the vibratory signals after compressing them into a 16-features latent representation. Then, we related the latent representation to haptic material perception: we show that, based on the emerging latent representation, it is possible to classify the vibratory signals into the different material categories assigned by human participants. We computed the centroids of each category within the latent space and showed that the distances between these categories resemble perceptual distances, measured with a rating experiment. These results suggest that the latent representation produced by unsupervised learning is similar to the information coding of the haptic perceptual system.</p><p>To interpret the dimensions of the latent space, we mimicked a physiology experiment. We generated a large number of sinusoids systematically varying in frequency and computed the corresponding representation within the latent space. We observed that the temporal tuning of the latent dimensions is similar to properties of human tactile receptors responsible for perception of haptic textures (i.e., Pacinian [PC] and rapidly adapting [RA] afferents).</p><p>As a control, we repeated all analyses after reducing the compression level by increasing the number of latent dimensions. Crucially, the similarity between the latent representation and perceptual representation increases with compression. This suggests that perceptual representations emerge by efficient encoding of the sensory input signals.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavior</title><p>Participants were instructed to slide the steel tip of the pen containing the accelerometer over the surface of the material sample (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Visual and audio signals were excluded. After a 10-s-long exploration, participants rated each material based on seven descriptors of its haptic properties (roughness, orderliness, hardness, temperature, elasticity, friction, and texture). We used the same materials and similar descriptors as previously used by <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>, who investigated haptic and visual representations of materials. Crucially, in their study participants could touch the stimuli with their barehands; therefore, temperature and softness information was available, whereas in our study participants had mostly to rely on vibratory signals for rating the materials.</p><p>To visualize the perceptual representation emerging from the ratings, we performed a PCA on the ratings averaged across participants and plotted each material in the space defined by the first two principal components (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). This is helpful because ratings based on different descriptors might be highly correlated; thus, the main principal components offer a compact representation of the responses. In fact, the first two principal components explain 65.9% of the total variance, very close to the 70.8% found by <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Perceptual ratings.</title><p>(<bold>A</bold>) Perceptual representation within the first (x-axis) and the second (y-axis) principal components space. Principal components analysis (PCA) was performed on the z-transformed ratings pooled over participants. Each point represents one material, and different colors represent different categories. (<bold>B</bold>) Same representation as in (<bold>A</bold>) based on barehand explorations (adapted from Figure 5 of <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>). (<bold>C</bold>) Distance matrix based on the ratings from our experiment and (<bold>D</bold>) based on <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref> data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64876-fig2-v1.tif"/></fig><p>The emerging representation is remarkably similar to the one discovered by <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>, despite no temperature and limited softness information (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). To quantitatively asses this similarity, we computed distance matrices from the ratings (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>). To do so, we correlated the ratings for each pair of materials, then averaged the obtained correlations, resulting in a correlation value for each pair of categories, which is a measure of how close the two categories were perceived. Finally, correlations were transformed into perceptual distances with the following equation:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>The distance matrix we obtained is highly correlated with the one from <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>: <italic>r</italic> = 0.69, <italic>p</italic>&lt;0.05, indicating that perceptual representations are very similar. The missing temperature and limited softness information did not much affect the perceptual representation we inferred from our rating experiment, which could therefore be thought as representative for natural haptic perception.</p></sec><sec id="s2-2"><title>Unsupervised learning</title><p>The Autoencoder learned a compressed latent representation to reconstruct the signals provided as input (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We evaluated reconstruction performance by correlating the input with the corresponding output signals and computing R<sup>2</sup>. Performance is computed for the train set (95% of the data) and the test set (remaining 5%). The reconstructed signals could explain 25% of the variance of the original signals for the train set and 23% for the test set. The learned latent representation is redundant: 95% of the variance of the latent representation of the full data set can be explained with 10 principal components.</p><p>We used the t-distributed stochastic neighbor embedding (t-SNE) method (<xref ref-type="bibr" rid="bib19">Hinton, 2008</xref>), a nonlinear dimensionality reduction technique for visualization, to represent the 10D <italic>latent PCs space</italic> in a 2D space (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). This seems a faithful representation of the <italic>10D latent PCs space</italic> as the distances between the category averages in the 2D t-SNE space are highly correlated with the ones computed in the 10D space (<italic>r</italic> = 0.994). Some categories were easy to discriminate, some more difficult. This is visible from the category averages. For instance, <italic>metal</italic> is far apart from <italic>animal</italic> but close to <italic>wood</italic>.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Latent space representation of the Autoencoder.</title><p>(<bold>A</bold>) Centers of categories in the t-distributed stochastic neighbor embedding (t-SNE). Each color represents a different category, as indicated in the legend. Dots represent material categories averaged across material samples, with the two-dimensional errors represented by error ellipses (axes length given by 1/2 of the eigenvalues of the covariance matrix; axes directions correspond to the eigenvectors). (<bold>B</bold>) Example of material category classification in the two first principal components of the latent PCs space, based on the ground truth material category labels. The black data points are the 2D representation of plastic; wood is in orange. The icons represent examples of material samples. The dashed line is the classification line learned by a linear classifier, which could tell apart the two categories with 75% accuracy. (<bold>C</bold>) Same as in (<bold>B</bold>), but based on perceptual labels. Black and orange data points indicate the perceptual categories plastic and wood, respectively. The data points surrounded by a colored rim represent material samples that were perceived as wood or plastic but belong to a different ground truth category (indicated by the color of the rim with the same color code as in <bold>A</bold>). The dashed line is the classification line learned by a linear classifier, which could tell apart the two categories with 87% accuracy. Note that the classification results reported in the main text are based on 10 principal components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64876-fig3-v1.tif"/></fig><p>Linear classification within the 10D <italic>latent PCs space</italic> could identify the correct ground truth category better than chance (i.e., 36%; empirical chance level given by a bootstrap analysis = 14%, with [11–22%] 95% confidence interval). However, samples of one material might be systematically perceived as being made of a different material. Hence, we also asked participants after the exploration of each material how much it ‘felt like’ each of the seven categories. This way we could assign perceived categories to each material sample based on the participants’ ratings. For each participant, we assigned for every material sample the category label corresponding to the highest rated category, and then we assigned for each material sample the most frequent label among participants (i.e., majority vote). Crucially, classification performance improved when the linear classifier was used to predict the labels given by human participants (40%, empirical chance level given by a bootstrap analysis = 14%, with [11–22%] 95% confidence interval). This classification performance is very close to the between-participants agreement level (44%), indicating that the information present in the <italic>latent space</italic> is almost enough to explain human category judgments.</p><p>Classification based on the raw vibratory signals could be achieved only by extracting known features related to the psychophysical dimensions of tactile perception such as hardness or friction (<xref ref-type="bibr" rid="bib42">Strese et al., 2017</xref>).</p><p>We used the distances between centers in the latent PCs space to compute a distance matrix (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), which is remarkably similar to the one obtained from the behavioral results. Crucially, the distances between centers of categories in the <italic>latent PCs space</italic> correlate with perceptual similarity (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, black data points; N = 21, <italic>r</italic> = 0.74, <italic>p</italic>&lt;0.0005), indicating that the representation of material properties within the compressed space learned by the Autoencoder from vibratory signals resembles the haptic perceptual representation.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Similarity between the perceptual and the latent representation of the Autoencoder.</title><p>(<bold>A</bold>) Distance matrix based on the distances between the category centers in the (10D) latent PCs space. (<bold>B</bold>) Relationship between perceptual distances (y-axis) and distances between categories in the latent PCs space (x-axis). The black dashed line indicates the linear regression line. (<bold>C</bold>) Same as (<bold>B</bold>), but based on the rating experiments from <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>, for the touch-only (black data points) and vision-only (gray data points) conditions. Again, the dashed lines represent regression lines, and in black and gray for the touch-only and the vision-only conditions, respectively. (<bold>D</bold>) Effect of compression. Correlation between distances in the latent PCs space and perceptual distances (black y-axis), reconstruction performance (blue y-axis), and classification performance (orange y-axis), as a function of compression rate (x-axis). Black line and data points indicate Pearson’s r correlation coefficient, and orange line and data points indicate classification accuracy (%). The blue lines represent the reconstruction performance computed as R<sup>2</sup> and expressed in percentage of explained variance; the dashed blue line represents the reconstruction performance for the training data set, and the continuous line for the validation data set. The horizontal orange dashed line represents the between-participants agreement expressed as classification accuracy. The gray shaded area indicates the compression level corresponding to the Autoencoder used for the analyses depicted in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4A–C</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64876-fig4-v1.tif"/></fig><p><xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref> found similar perceptual representations of materials with purely visual and purely haptic stimulations. We repeated the correlation analysis based on the distance matrix computed from the rating data from <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref> for the vision-only and the touch-only conditions. Again, perceptual distances based on the ratings in the touch-only condition correlate with the distances between centers of categories in the <italic>latent PCs space</italic> (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, black data points; N = 21, <italic>r</italic> = 0.72, p&lt;0.05). This is consistent with the similarity between the distance matrices from our and their rating experiment. However, the same analysis repeated based on the ratings from the vision-only condition yields similar correlation results (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, gray data points; N = 21, <italic>r</italic> = 0.66, p&lt;0.05). This is intriguing because visual and tactile signals originating from the same material are fundamentally different. However, similar physical properties are likely to create similar patterns of covariation in the sensory inputs. The similarity between the visual and the haptic material representations, and the representation emerging from unsupervised learning corroborates the idea that by learning to efficiently represent the sensory input perceptual systems discover the latent variables responsible for the sensorial input (<xref ref-type="bibr" rid="bib16">Fleming and Storrs, 2019</xref>), that is, the systematic differences between materials.</p><p>Our results showed a remarkable similarity between perceptual representations and how material categories are represented in the latent space learned by the Autoencoder. This is consistent with the theory that perceptual representations emerge from efficient encoding of the sensory signals. Specifically, we believe that by compressing information the Autoencoder has learned the regularities due to the physical properties of the different materials and discounted the variability due to the specific properties of each sample, e.g., it has learned to represent the general properties of wood rather than the specific properties of each wooden sample. We argue that such generalization process resembles how we build perceptual representations, that is, compression is responsible for the similarity between perceptual representations and representations within the latent space. We tested this prediction by gradually decreasing the compression rate of the Autoencoder until no compression is applied, that is, the dimensionality of the latent space is the same as the one of the input. Higher compression rate, defined as the ratio between the dimensionality of the latent space and the one of the input (in percentage), implies lower compression and results in higher reconstruction performance (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, blue continuous and dashed lines, for the validation and the training data set, respectively). The least the compression (higher compression rate), the lower the correlation between perceptual and latent space distance matrices (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, black), and the lower the classification performance (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, orange). These results show that compression is crucial for the Autoencoder to learn an internal representation that resembles perceptual representations, that is, perceptual representations emerge by efficiently encoding the sensory input. In fact, because of the bottleneck, Autoencoders must learn a compressed representation from which the input signals can still be reconstructed. To do so, they tend to discover latent variables that are good at capturing the pattern of covariation in the input generated by distal causes (e.g., material properties of the surfaces, <xref ref-type="bibr" rid="bib16">Fleming and Storrs, 2019</xref>; <xref ref-type="bibr" rid="bib40">Storrs et al., 2021</xref>).</p><p>Haptic textures are mainly sensed by the PC and RA afferents, which are highly sensitive to skin vibrations (<xref ref-type="bibr" rid="bib45">Weber et al., 2013</xref>) and have known temporal tuning properties (<xref ref-type="bibr" rid="bib22">Kandel et al., 2000</xref>; <xref ref-type="bibr" rid="bib29">Mountcastle et al., 1972</xref>). We probed the model’s temporal tuning by mimicking a physiology experiment: we presented the network with a large number of sinusoids systematically varying in frequency within the perceptually relevant range (<xref ref-type="bibr" rid="bib45">Weber et al., 2013</xref>). We defined the temporal tuning of the <italic>latent PCs space</italic> as the responses to sinusoidal signals of different frequencies along each of its dimensions (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Frequency tuning.</title><p>(<bold>A</bold>) Comparison between temporal tuning of the dimensions of the 10D latent PCs space and the temporal tuning of the Pacinian (PC) and rapidly adapting (RA) afferents (green and red dashed lines, respectively). Tuning is given by sensitivity (y-axis) as a function of temporal frequency (x-axis). RA is optimally tuned at 40–60 Hz, and the PC is optimally tuned at 250–300 Hz. The first two principal components of the latent PCs space are represented by thick lines; thinner lines represent the other components. For a group of components (in red), sensitivity peaks around the RA afferents peak, the other group (in green) reaches the maximum sensitivity in proximity of the PC peak. (<bold>B</bold>) same as (<bold>A</bold>), but for the Autoencoder with no compression (100% compression rate). Here, only the first two principal components of the latent PCs space are shown as the tuning profiles tend to differ from each other. (<bold>C</bold>) Frequency tuning for the 16 features (6.25% compression rate) and the no compression (100% compression rate) Autoencoder. Tuning is again defined as average frequency weighted by sensitivity, and colors are assigned by the <italic>k</italic>-means clustering algorithm.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64876-fig5-v1.tif"/></fig><p>We defined a simple measure of frequency tuning as the average frequency weighted by sensitivity. We used the <italic>k</italic>-means clustering algorithm to divide the components into two groups (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, lower tuning to lower frequencies in red, and to higher in green). The tuning curves of the principal components of the <italic>latent PCs space</italic> are remarkably similar to the ones of the PC and RA afferents (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, dashed green and red lines, respectively). When we repeated this analysis on the Autoencoder without compression, the tuning profiles were different from the ones of the PC and RA afferents (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) and from each other (<xref ref-type="fig" rid="fig5">Figure 5C</xref>); frequency tuning did not cluster as much as for the more compressive 16-feature Autoencoder (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Taken together, these results suggest that our sensors for texture (and material) properties have evolved to efficiently encode the statistics of natural textures as they are sensed through vibrations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We used unsupervised learning to reconstruct the vibratory signals that constitute the very input of the haptic system based on a highly compressed representation. Such representation shares similarities with the perceptual representation of natural materials: it allows for classification of material categories, as provided by human judgments, and crucially, distances between material categories in the latent space resemble perceptual distances. Furthermore, the temporal tuning of the dimensions of the learned representation is similar to properties of human tactile receptors. These similarities suggest that the computations involved in touch perception evolved to efficiently encode the input, yielding perceptual representations that allow to distinguish between different materials. This may be because a way to efficiently represent the vibratory patterns is to discover latent variables that create systematic differences between them, that is, the systematic physical differences between materials.</p><p>A similar idea has been proposed for visual perception of material properties (<xref ref-type="bibr" rid="bib16">Fleming and Storrs, 2019</xref>). The challenge for vision is that the information in the retinal image (proximal stimulus) is insufficient to recover the properties of the world (distal stimulus) that caused it (<xref ref-type="bibr" rid="bib1">Anderson, 2011</xref>). In fact, multiple causes are usually responsible for a proximal stimulus, for example, illumination and reflectance spectra are confused within the reflected light. To solve this ambiguity, it is proposed that we learn to represent the dimensions of variation in the proximal stimuli, which arise from the systematic effects of distal stimuli (<xref ref-type="bibr" rid="bib16">Fleming and Storrs, 2019</xref>), rather than learning to directly estimate the distal properties of the world, as predicted by the <italic>inverse optic</italic>s approach (<xref ref-type="bibr" rid="bib28">Marr, 1982</xref>; <xref ref-type="bibr" rid="bib34">Poggio and Koch, 1985</xref>; <xref ref-type="bibr" rid="bib33">Pizlo, 2001</xref>). This approach could successfully be used to predict perception and misperception of gloss (<xref ref-type="bibr" rid="bib40">Storrs et al., 2021</xref>). Our results support the hypothesis that efficiently encoding the proximal stimuli is the way sensory systems develop perceptual representations.</p><p>Similar to the ambiguities in the visual input, a challenge for touch perception is that the temporal frequency content of the input vibratory signals depends both on the surface properties and the exploration speed. Nevertheless, we can recognize different materials regardless of the speed of the hand movements used to explore them. This ability of the haptic system can be referred to as speed invariance (<xref ref-type="bibr" rid="bib8">Boundy-Singer et al., 2017</xref>). Our classification analysis could discriminate the material categories even though exploration speed was not controlled. This is consistent with the fact that human roughness judgments can be predicted from nonspeed invariant responses of the PC and RA afferents (<xref ref-type="bibr" rid="bib45">Weber et al., 2013</xref>), presumably because of limited variability in exploration speed. In fact, responses of tactile nerve fibers are highly non-speed invariant, whereas populations of neurons in the somatosensory cortex exhibit different degrees of speed invariance (<xref ref-type="bibr" rid="bib26">Lieber and Bensmaia, 2020</xref>). This may be due to the increased separability of information about texture and speed, which could be implemented by the temporal and the spatial differentiation mechanisms active in the somatosensory cortex (e.g., <xref ref-type="bibr" rid="bib6">Bensmaia et al., 2008</xref>; <xref ref-type="bibr" rid="bib36">Saal et al., 2015</xref>). Speed invariance could also be implemented by normalizing neural responses by movement speed, yielding a representation in spatial coordinates. For this, the haptic system would need a precise and robust measure of the movement speed not only during active but also passive exploration as perceptual speed invariance is found even when textures are passively scanned (<xref ref-type="bibr" rid="bib5">BensmaIa and Hollins, 2003</xref>). There is evidence that an estimate of scanning speed is available (<xref ref-type="bibr" rid="bib14">Dépeault et al., 2008</xref>). An alternative mechanism for speed invariance could be similar to the one mediating (auditory) timbre constancy: while changes in exploration speed affect the frequency composition of the receptors’ spiking responses, the harmonic structure remains relatively constant (<xref ref-type="bibr" rid="bib45">Weber et al., 2013</xref>).</p><p>It is also possible that observers adjusted their exploration movements to the best speed for texture recognition, yielding a certain level of speed invariance. It is known that exploratory behavior is optimized to maximize information gain (<xref ref-type="bibr" rid="bib23">Lederman and Klatzky, 1987</xref>; <xref ref-type="bibr" rid="bib30">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib21">Kaim and Drewing, 2011</xref>; <xref ref-type="bibr" rid="bib43">Toscani et al., 2013</xref>). Specific to exploration speed, it is shown that when participants were asked to haptically discriminate spatial frequencies of gratings, low-performing participants could improve their performance by adjusting the scanning velocity to the one used by better-performing participants (<xref ref-type="bibr" rid="bib17">Gamzu and Ahissar, 2001</xref>). In fact, the statistics of the sensory input are shaped by exploratory behavior, which therefore may contribute to efficient encoding, consistent with the recently proposed ‘active efficient coding’ theory (<xref ref-type="bibr" rid="bib15">Eckmann et al., 2020</xref>). However, it seems that exploratory strategies depend on the task but not on texture properties. The changes in the properties of exploratory movements (e.g., exploration speed) depending on texture properties probably arise purely from the biophysical interaction with surfaces rather than any changes in motor behavior (<xref ref-type="bibr" rid="bib10">Callier et al., 2015</xref>).</p><p>In our study, participants could not access temperature and had limited access to softness information; nevertheless, the perceptual representation inferred from the rating experiment is very similar to the one reported by <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>, even though in their study participants could explore materials with their barehands. This suggests that human perception of materials is largely based on vibrations they elicit on the skin. <xref ref-type="bibr" rid="bib7">Bensmaïa and Hollins, 2005</xref> showed that 82% of variance in human dissimilarity judgments between pairs of materials could be explained by the differences in vibrations they elicited on the skin.</p><p>Here, we show that efficient encoding of the raw vibratory signals (i.e., measured at the tool with which the surface texture is explored) produces a representation similar to the signals recorded from the afferents responsible for texture perception (i.e., PC and RA). This does not imply that the Autoencoder we tested only mimics the function of the Pacini and Meissner corpuscles innervated by the PC and RA afferents because, before the receptors, the signal might already be compressed by the biomechanic properties of the hand (<xref ref-type="bibr" rid="bib37">Shao et al., 2020</xref>). The mechanosensory representation does not keep all the vibratory information present at the input level: responses of the PC and RA afferents follow a specific tuning profile, which implies selection of vibratory information in the time-frequency domain. Such selection of information can be achieved by learning a compressed representation that gives more weight to some temporal frequency than others. We speculate that the tuning profiles learned by the Autoencoder, as well as the ones exhibited by the PC and RA afferents, are tuned to the statistical properties of vibratory signals elicited by natural material textures during active human exploration.</p><p>Crucially, the similarity between perceptual representations and the representations learned by the Autoencoder increases with the level of compression, suggesting a crucial role of efficient encoding of the sensory signals in learning perceptual representations. Because of constraints on perceptual systems, like the number of neurons and the metabolic cost of neural activity (<xref ref-type="bibr" rid="bib24">Lennie, 2003</xref>), receptor input is likely to be compressed and encoded as efficiently as possible to maximize neural resources. Our results suggest that efficient coding forces the brain to discover perceptual representations that relate to the physical factors responsible for variations between signals.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Eleven students (seven females) participated in the experiment; all were volunteers, naïve to the purpose of the experiment, and were reimbursed for their participation (8 euros/hr). All participants were right-handed and did not report any sensory or motor impairment at the right hand. The study was approved by the local ethics committee LEK FB06 at Giessen University and was in line with the Declaration of Helsinki from 2008. Written informed consent was obtained from each participant.</p></sec><sec id="s4-2"><title>Vibratory signals</title><p>Participants freely explored 81 out of 84 material samples used by <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref> with a steel tool tip. Three samples had to be excluded because they were not well preserved. The tip was mounted on a 3D-printed pen and connected to an accelerometer (ADXL345) so that the vibrations elicited by the interaction between the tip and the material samples could be recorded. For each material, 10 s of recording were acquired at 3200 Hz temporal resolution. Then, each recording was parsed into 125 vibratory signals of 80 ms each. This yields a total of 111,375 vibratory signals. To prevent signals from getting affected by the onset and offset of contact between the tool and the material surface, we started the recording after 2 s of exploration and stopped it 2 s before the exploration was terminated. Frequencies below 10 Hz were ascribed to exploratory hand movements and therefore were filtered out (<xref ref-type="bibr" rid="bib41">Strese et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Strese et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Romano and Kuchenbecker, 2014</xref>; <xref ref-type="bibr" rid="bib13">Culbertson et al., 2014</xref>). If not removed, vibrations due to hand movements constitute a potential confound because participants could move their hand differently depending on the material, presumably because of the biophysical interaction between their hand and the surfaces (<xref ref-type="bibr" rid="bib10">Callier et al., 2015</xref>). We also removed frequencies above 800 Hz as they are not relevant for perception of texture properties of materials (<xref ref-type="bibr" rid="bib20">Hollins et al., 2001</xref>; <xref ref-type="bibr" rid="bib5">BensmaIa and Hollins, 2003</xref>; <xref ref-type="bibr" rid="bib7">Bensmaïa and Hollins, 2005</xref>; <xref ref-type="bibr" rid="bib45">Weber et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Manfredi et al., 2014</xref>) and may be caused by measuring noise.</p></sec><sec id="s4-3"><title>Rating experiment</title><p>Participants sat at a table looking at a monitor on which instructions and rating scales were displayed. Material samples were positioned by the experimenter in front of them. Vision of the samples was prevented by a blind, and sound resulting from the exploration of the materials was covered by earplugs and white noise displayed over headphones. Prior to every trial, participants were informed to grab the pen with the accelerometer and prepare for the exploration. The onset of the white noise signaled that they could begin with the exploration. They were instructed to slide the pen over the material’s surface. 2 s after the onset of the white noise, the vibratory signal was recorded for 10 s. After this, participants were informed to stop the exploration. White noise was played for two more seconds. Subsequently, participants reported first how much the explored material felt like each of the seven material categories (paper, fabric, animal, stone, plastic, wood, metal) by clicking for each on a scale ranging from complete dissimilarity to complete similarity with this category (e.g. ‘no wood’–‘wood’). Then, they rated the material according to seven antagonist adjective pairs again by clicking for each on a scale with the outermost positions corresponding to the two adjectives. We used the same descriptors as used by <xref ref-type="bibr" rid="bib4">Baumgartner et al., 2013</xref>: rough vs. smooth, hard vs. soft, orderly vs. chaotic, warm vs. cold, elastic vs. not elastic, high friction vs. slippery, textured/patterned vs. homogeneous/uniform, omitting descriptors related to the visual appearance (glossiness, colorfulness, three-dimensionality). The experiment was carried out using PsychoPy (<xref ref-type="bibr" rid="bib32">Peirce et al., 2019</xref>). Before all analyses described in the results, we z-transformed the ratings separately for each descriptor and each participant to convert them into a common scale and reduce the impact of subjective criteria.</p></sec><sec id="s4-4"><title>Deep neural network</title><p>The network used for the analyses described in the article is a relatively simple Deep Convolutional Autoencoder. The <italic>encoder</italic> encodes the 256 time samples of each vibratory pattern into a latent representation consisting of one point in a 16-feature space (i.e., <italic>code</italic>). This is done by means of four 1D convolutional layers, each of them with kernel size equal to five time samples.</p><p>The size of the representation in time is progressively reduced by means of a max pooling operation, so that the kernel size is relatively increased for deeper layers. Specifically, after the input layer, the first convolutional layer takes as input signals of 256 time samples and outputs 256 time samples for each of the 64 features. The max pooling operation reduces the time samples to 64. The second convolutional layer takes as input 64 time samples per 64 features and outputs 64 time samples per 32 features. Again, max pooling reduces the time samples; this time to 16. The third convolutional layer takes as input 16 time samples per 32 features and outputs 16 time samples per 16 features. Max pooling reduced the time samples to 4. The fourth convolutional layer takes as input four time samples per 16 features and outputs the same size representation. Finally, max pooling reduces the time samples to one single point for each of the 16 features. This output is the most compressed representation within the network, that is, the <italic>code</italic>, and constitutes the bottleneck of the Autoencoder.</p><p>The <italic>code</italic> is decoded by the <italic>decoder</italic> to reconstruct the input signals. The first convolutional layer of the decoder takes as input and outputs one time sample per 16 features. The upsampling operation increases the number of time samples from 1 to 4. This is done by repeating each temporal step four times along the time axis. The second convolutional layer takes as input and outputs four time samples for 16 features. Upsampling increases the time samples from 4 to 16. The third convolutional layer takes as input 16 time samples per 16 features and outputs 16 time samples per 32 features. Upsampling increases the time samples from 16 to 64. The fourth convolutional layer takes as input 64 time samples for 32 features and outputs 64 time samples for 64 features. Upsampling increases the time samples from 64 to the original size (i.e., 256). The fifth and last convolutional layer takes as input 256 time samples per 64 features and outputs 256 time samples. Thus, input and output have the same size and can be directly compared within the loss function.</p><p>The activation function (∏) of all convolutional layers is the rectified linear unit (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), with the exception of the last layer of the decoder, for which it is a sigmoid function (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>).<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Training consisted of 50 epochs to minimize the <italic>mean absolute error (MAE</italic>) loss function:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>With <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the <italic>i</italic> vibratory signal of the training set (consisting of N signals), and <inline-formula><mml:math id="inf2"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> the corresponding reconstructed signal.</p><p>The network was adapted from <xref ref-type="bibr" rid="bib12">Chollet, 2016</xref> to work with one-dimensional data rather than images. The implementation and training of the network were done through the Keras deep learning framework (<xref ref-type="bibr" rid="bib11">Chollet, 2015</xref>) via its Python interface.</p></sec><sec id="s4-5"><title>Networks with different compression rates</title><p>We started from the architecture described above and changed the bottleneck to reduce the level of compression. To do so, we increased the number of features in the latent space (i.e., from 16 to 32, 64, 192, 256; with compression rate from 6.25% to 25, 50, 75, and 100%). To make sure that none of the intermediate layers had a lower dimensionality than the latent space, we imposed their number of features to be not less than the dimensionality of the latent space.</p></sec><sec id="s4-6"><title>Linear classification</title><p>Classification was done based on the <italic>latent PCs space</italic> dimensions by iteratively leaving out one material per category, training the classifier on the remaining signals, and computing performance on the leftout samples. Before classification, the latent representations of all signals of each material sample were averaged, so that there was one point for each material sample. As a conservative method for hypothesis testing, we used bootstrap analysis to assess the significance of all classification results. For that, we repeated the classification analysis 5000 times under the null hypothesis of no relationship between the points in the <italic>latent PC space</italic> and the material categories, that is, we shuffled the category labels. This produced the accuracy distribution under the null hypothesis of chance-level classification; the 95% confidence interval was computed by reading out the 2.5th and 97.5th percentiles of the distribution. The empirical chance level corresponded to the mean of that distribution. The labels used for training and testing were either the ground truth material categories (example in <xref ref-type="fig" rid="fig3">Figure 3B</xref>) or the ones assigned by participants (example in <xref ref-type="fig" rid="fig3">Figure 3C</xref>). To assign category labels to different material samples, first we assigned to each material the highest rated material category by each participant, then we chose the most frequent category, that is, the one chosen by the majority of participants. The between-participants agreement level is computed based on each participant’s category rating. Again, for each sample, we assigned a category label as the highest rated category for each participant. Then, we considered each participant for testing, iteratively, and computed the predicted category as the one chosen by the majority of the other participants. The agreement level is the classification accuracy computed by comparing the predicted and the testing category labels.</p></sec><sec id="s4-7"><title>Temporal tuning</title><p>We fed the network with 830 sinusoids with frequencies ranging from 0 to 800 Hz and computed their representation in the <italic>latent PCs space</italic>. Temporal tuning is given by the normalized (range is forced between 0 and 1) model responses as a function of temporal frequency. For the tactile afferents, sensitivity is computed as normalized inverse of the discrimination thresholds from <xref ref-type="bibr" rid="bib22">Kandel et al., 2000</xref>; <xref ref-type="bibr" rid="bib29">Mountcastle et al., 1972</xref>.</p></sec><sec id="s4-8"><title>Data availability</title><p>The analysis code is publicly available. The code for processing the vibratory signals and the Python code for training the DNNs and saving the reconstructed signals and the latent representations is available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties">here</ext-link>; <xref ref-type="bibr" rid="bib44">Toscani, 2021</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c8ee34b2064e7ad44f9ca49126fcd6cdae34ff87;origin=https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties;visit=swh:1:snp:8bddda2b3ea018061a8ed6928fa7abfdf324a557;anchor=swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2">swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2</ext-link>).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study was approved by the local ethics committee LEK FB06 at Giessen University and was in line with the declaration of Helsinki from 2008. Written informed consent was obtained from each participant.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-64876-transrepform1-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data are publicly available online (<ext-link ext-link-type="uri" xlink:href="https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties">https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c8ee34b2064e7ad44f9ca49126fcd6cdae34ff87;origin=https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties;visit=swh:1:snp:8bddda2b3ea018061a8ed6928fa7abfdf324a557;anchor=swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2">swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Deutsche Forschungsgemeinschaf (DFG, German Research Foundation) – project number 222641018 – SFB/TRR 135. We are grateful to Karl Gegenfurtner and Knut Drewing for helpful discussions and comments.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual perception of materials and surfaces</article-title><source>Current Biology</source><volume>21</volume><fpage>R978</fpage><lpage>R983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.022</pub-id><pub-id pub-id-type="pmid">22192826</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atick</surname><given-names>JJ</given-names></name><name><surname>Redlich</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>What Does the Retina Know about Natural Scenes?</article-title><source>Neural Computation</source><volume>4</volume><fpage>196</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.2.196</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB.</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory communication</source><volume>1</volume><fpage>217</fpage><lpage>234</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumgartner</surname><given-names>E</given-names></name><name><surname>Wiebel</surname><given-names>CB</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual and haptic representations of material properties</article-title><source>Multisensory Research</source><volume>26</volume><fpage>429</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1163/22134808-00002429</pub-id><pub-id pub-id-type="pmid">24649528</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>BensmaIa</surname><given-names>SJ</given-names></name><name><surname>Hollins</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The vibrations of texture</article-title><source>Somatosensory &amp; Motor Research</source><volume>20</volume><fpage>33</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1080/0899022031000083825</pub-id><pub-id pub-id-type="pmid">12745443</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bensmaia</surname><given-names>SJ</given-names></name><name><surname>Denchev</surname><given-names>PV</given-names></name><name><surname>Dammann</surname><given-names>JF</given-names></name><name><surname>Craig</surname><given-names>JC</given-names></name><name><surname>Hsiao</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The representation of stimulus orientation in the early stages of somatosensory processing</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>776</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4162-07.2008</pub-id><pub-id pub-id-type="pmid">18199777</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bensmaïa</surname><given-names>S</given-names></name><name><surname>Hollins</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Pacinian representations of fine surface texture</article-title><source>Perception &amp; Psychophysics</source><volume>67</volume><fpage>842</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.3758/bf03193537</pub-id><pub-id pub-id-type="pmid">16334056</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boundy-Singer</surname><given-names>ZM</given-names></name><name><surname>Saal</surname><given-names>HP</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Speed invariance of tactile texture perception</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>2371</fpage><lpage>2377</lpage><pub-id pub-id-type="doi">10.1152/jn.00161.2017</pub-id><pub-id pub-id-type="pmid">28724777</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchsbaum</surname><given-names>G</given-names></name><name><surname>Gottschalk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Trichromacy, opponent colours coding and optimum colour information transmission in the retina</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>220</volume><fpage>89</fpage><lpage>113</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callier</surname><given-names>T</given-names></name><name><surname>Saal</surname><given-names>HP</given-names></name><name><surname>Davis-Berg</surname><given-names>EC</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Kinematics of unconstrained tactile texture exploration</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>3013</fpage><lpage>3020</lpage><pub-id pub-id-type="doi">10.1152/jn.00703.2014</pub-id><pub-id pub-id-type="pmid">25744883</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Keras: The python deep learning library</data-title><source>Keras</source><ext-link ext-link-type="uri" xlink:href="https://Keras.io">https://Keras.io</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F.</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Building autoencoders in keras</source><publisher-name>The Keras Blog</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Culbertson</surname><given-names>H</given-names></name><name><surname>Lopez Delgado</surname><given-names>JJ</given-names></name><name><surname>Kuchenbecker</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><conf-name>One hundred data-driven haptic texture models and open-source methods for rendering on 3D objects</conf-name><article-title>2014 IEEE Haptics Symposium</article-title><fpage>319</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1109/HAPTICS.2014.6775475</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dépeault</surname><given-names>A</given-names></name><name><surname>Meftah</surname><given-names>EM</given-names></name><name><surname>Chapman</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Tactile speed scaling: contributions of time and space</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>1422</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.1152/jn.01209.2007</pub-id><pub-id pub-id-type="pmid">18199814</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckmann</surname><given-names>S</given-names></name><name><surname>Klimmasch</surname><given-names>L</given-names></name><name><surname>Shi</surname><given-names>BE</given-names></name><name><surname>Triesch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Active efficient coding explains the development of binocular vision and its failure in amblyopia</article-title><source>PNAS</source><volume>117</volume><fpage>6156</fpage><lpage>6162</lpage><pub-id pub-id-type="doi">10.1073/pnas.1908100117</pub-id><pub-id pub-id-type="pmid">32123102</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name><name><surname>Storrs</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning to see stuff</article-title><source>Current Opinion in Behavioral Sciences</source><volume>30</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.07.004</pub-id><pub-id pub-id-type="pmid">31886321</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gamzu</surname><given-names>E</given-names></name><name><surname>Ahissar</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Importance of temporal cues for tactile spatial- frequency discrimination</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>7416</fpage><lpage>7427</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-18-07416.2001</pub-id><pub-id pub-id-type="pmid">11549752</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical mechanisms of colour vision</article-title><source>Nature Reviews. Neuroscience</source><volume>4</volume><fpage>563</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1038/nrn1138</pub-id><pub-id pub-id-type="pmid">12838331</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing data using t-SNE</article-title><source>Journal of Machine Learning Research</source><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollins</surname><given-names>M</given-names></name><name><surname>Bensmaïa</surname><given-names>SJ</given-names></name><name><surname>Washburn</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Vibrotactile adaptation impairs discrimination of fine, but not coarse, textures</article-title><source>Somatosensory &amp; Motor Research</source><volume>18</volume><fpage>253</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1080/01421590120089640</pub-id><pub-id pub-id-type="pmid">11794728</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaim</surname><given-names>L</given-names></name><name><surname>Drewing</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Exploratory Strategies in Haptic Softness Discrimination Are Tuned to Achieve High Levels of Task Performance</article-title><source>IEEE Transactions on Haptics</source><volume>4</volume><fpage>242</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1109/TOH.2011.19</pub-id><pub-id pub-id-type="pmid">26963653</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kandel</surname><given-names>ER</given-names></name><name><surname>Schwartz</surname><given-names>JH</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Siegelbaum</surname><given-names>SA</given-names></name><name><surname>Hudspeth</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Principles of Neural Science</source><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lederman</surname><given-names>SJ</given-names></name><name><surname>Klatzky</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hand movements: a window into haptic object recognition</article-title><source>Cognitive Psychology</source><volume>19</volume><fpage>342</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(87)90008-9</pub-id><pub-id pub-id-type="pmid">3608405</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The cost of cortical computation</article-title><source>Current Biology</source><volume>13</volume><fpage>493</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(03)00135-0</pub-id><pub-id pub-id-type="pmid">12646132</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Efficient coding of natural sounds</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>356</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nn831</pub-id><pub-id pub-id-type="pmid">11896400</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieber</surname><given-names>JD</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Emergence of an Invariant Representation of Texture in Primate Somatosensory Cortex</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>3228</fpage><lpage>3239</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz305</pub-id><pub-id pub-id-type="pmid">31813989</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manfredi</surname><given-names>LR</given-names></name><name><surname>Saal</surname><given-names>HP</given-names></name><name><surname>Brown</surname><given-names>KJ</given-names></name><name><surname>Zielinski</surname><given-names>MC</given-names></name><name><surname>Dammann</surname><given-names>JF</given-names></name><name><surname>Polashock</surname><given-names>VS</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Natural scenes in tactile texture</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>1792</fpage><lpage>1802</lpage><pub-id pub-id-type="doi">10.1152/jn.00680.2013</pub-id><pub-id pub-id-type="pmid">24523522</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision</source><publisher-name>Freeman</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name><name><surname>LaMotte</surname><given-names>RH</given-names></name><name><surname>Carli</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Detection thresholds for stimuli in humans and monkeys: comparison with threshold events in mechanoreceptive afferent nerve fibers innervating the monkey hand</article-title><source>Journal of Neurophysiology</source><volume>35</volume><fpage>122</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1152/jn.1972.35.1.122</pub-id><pub-id pub-id-type="pmid">4621505</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najemnik</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Optimal eye movement strategies in visual search</article-title><source>Nature</source><volume>434</volume><fpage>387</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/nature03390</pub-id><pub-id pub-id-type="pmid">15772663</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id><pub-id pub-id-type="pmid">30734206</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pizlo</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Perception viewed as an inverse problem</article-title><source>Vision Research</source><volume>41</volume><fpage>3145</fpage><lpage>3161</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00173-0</pub-id><pub-id pub-id-type="pmid">11711140</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>III-Posed problems early vision: from computational theory to analogue networks</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>226</volume><fpage>303</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1098/rspb.1985.0097</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Romano</surname><given-names>JM</given-names></name><name><surname>Kuchenbecker</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><conf-name>2014 IEEE Haptics Symposium</conf-name><article-title>Methods for robotic tool-mediated haptic surface recognition</article-title><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1109/HAPTICS.2014.6775432</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saal</surname><given-names>HP</given-names></name><name><surname>Harvey</surname><given-names>MA</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rate and timing of cortical responses driven by separate sensory channels</article-title><source>eLife</source><volume>4</volume><elocation-id>e10450</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10450</pub-id><pub-id pub-id-type="pmid">26650354</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>Y</given-names></name><name><surname>Hayward</surname><given-names>V</given-names></name><name><surname>Visell</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Compression of dynamic tactile information in the human hand</article-title><source>Science Advances</source><volume>6</volume><elocation-id>eaaz1158</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aaz1158</pub-id><pub-id pub-id-type="pmid">32494610</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>EC</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient auditory coding</article-title><source>Nature</source><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Anderson</surname><given-names>BL</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised learning predicts human perception and misperception of gloss</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>1402</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01097-6</pub-id><pub-id pub-id-type="pmid">33958744</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Strese</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>JY</given-names></name><name><surname>Schuwerk</surname><given-names>C</given-names></name><name><surname>Han</surname><given-names>Q</given-names></name><name><surname>Kim</surname><given-names>HG</given-names></name><name><surname>Steinbach</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><conf-name>A haptic texture database for tool-mediated texture recognition and classification</conf-name><article-title>2014 IEEE International Symposium on Haptic, Audio and Visual Environments and Games</article-title><fpage>118</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1109/HAVE.2014.6954342</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Strese</surname><given-names>M</given-names></name><name><surname>Boeck</surname><given-names>Y</given-names></name><name><surname>Steinbach</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><conf-name>Content-based surface material retrieval</conf-name><article-title>2017 IEEE World Haptics Conference</article-title><fpage>352</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1109/WHC.2017.7989927</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Valsecchi</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Optimal sampling of visual information for lightness judgments</article-title><source>PNAS</source><volume>110</volume><fpage>11163</fpage><lpage>11168</lpage><pub-id pub-id-type="doi">10.1073/pnas.1216954110</pub-id><pub-id pub-id-type="pmid">23776251</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Toscani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Unsupervised-learning-of-haptic-material-properties</data-title><version designator="swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2">swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c8ee34b2064e7ad44f9ca49126fcd6cdae34ff87;origin=https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties;visit=swh:1:snp:8bddda2b3ea018061a8ed6928fa7abfdf324a557;anchor=swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2">https://archive.softwareheritage.org/swh:1:dir:c8ee34b2064e7ad44f9ca49126fcd6cdae34ff87;origin=https://github.com/matteo-toscani-24-01-1985/Unsupervised-learning-of-haptic-material-properties;visit=swh:1:snp:8bddda2b3ea018061a8ed6928fa7abfdf324a557;anchor=swh:1:rev:53b1d7407307c00f08543cad096f983217a53ef2</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>AI</given-names></name><name><surname>Saal</surname><given-names>HP</given-names></name><name><surname>Lieber</surname><given-names>JD</given-names></name><name><surname>Cheng</surname><given-names>JW</given-names></name><name><surname>Manfredi</surname><given-names>LR</given-names></name><name><surname>Dammann</surname><given-names>JF</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spatial and temporal codes mediate the tactile perception of natural textures</article-title><source>PNAS</source><volume>110</volume><fpage>17107</fpage><lpage>17112</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305509110</pub-id><pub-id pub-id-type="pmid">24082087</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidi</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Decorrelation of L- and M-cone signals</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>14</volume><fpage>3430</fpage><lpage>3431</lpage><pub-id pub-id-type="doi">10.1364/josaa.14.003430</pub-id><pub-id pub-id-type="pmid">9392903</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64876.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The authors apply machine learning techniques to empirical-based simulations of the vibrotactile patterns to find their compressed representations. These compressed representations turn out to be similar to those deduced from human responses and also correspond to those expected based on efficiency arguments.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64876.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Ahissar</surname><given-names>Ehud</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0316ej306</institution-id><institution>Weizmann Institute of Science</institution></institution-wrap><country>Israel</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Bensmaia</surname><given-names>Sliman J</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Unsupervised learning of haptic material properties&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Chris Baker as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Ehud Ahissar (Reviewer #1); Sliman J Bensmaia (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. The main concern is about classification performance. It should be demonstrated that the present approach provides substantial improvements compared to more standard and simpler methods. However, the reviewers have expressed doubt that this is the case during the discussion stage. However, we would like to give you an opportunity to address this and other comments and therefore are inviting a revised manuscript. If you do not think that this can be addressed, then it is unlikely that the manuscript will ultimately be accepted for publication.<italic>Reviewer #1:</italic></p><p>The paper describes a very interesting attempt to understand texture coding by touch using autoencoding (AE) of empirical-based simulations of the vibrotactile patterns that would dominate receptor activation during tactile scanning of various surfaces. By showing that the compressed representation generated by the AE allows surface classification, with distances that are correlated to some degree with those exhibited by human perceivers, the authors suggest that the latent representations underlying human perception might be similar to that observed in the simulations. The authors further claim that the simulated latent representation is efficient, and that it matches the characteristics of RA and PC mechanoreceptors. These observations lead the authors to suggest that haptic representation is dictated by efficient coding.</p><p>1. There is a significant gap between the simulated data used here and the empirical data of material perception by touch. The vibratory signals were taken from recordings of surface exploration using a tool tip (Strese et al., 2017) whereas the ratings of the different materials are taken from an experiment in which participants used bare hand touch (Baumgartner et al., 2013). The difference is significant especially when material classification, and not only texture classification, is required. It is not at all clear how vibratory signals could code hardness, warmth, elasticity, friction, 3D, etc (see Baumgartner et al., 2013). The authors must provide a serious discussion about this gap and convince the reader that their simulations can indeed provide an access to the internal representations of natural haptic touch. In the same spirit, they should explain why, and demonstrate that, the pre-processing of the vibrotactile data (cutting and filtering) makes sense for natural haptic touch.</p><p>2. The authors should provide good reasons to convince the readers that the compressed representation they found is indeed a good candidate for the biological representation. First, the nature of the AE algorithm is that it will converge to some representation in the minimal encoder dimension. Why is that a good encoding representation, and why is it a good model for the biological one? Second, the differences between the results obtained with the AE and those obtained with humans (Baumgartner et al., 2013) seem to outnumber the partial similarities found. The authors should list both differences and similarities and discuss, based on these comparisons, the probability that the coding found here is similar to the coding guiding human behavior.</p><p>3. Notion of efficiency and compression. It was not demonstrated that the main result (figure 3A) is due to compression and efficiency of the AE. What will happen if no AE is used and the distance is measured in the raw input domain (e.g. between Fourier coefficients or principal components)? One could expect figure 4 to account for that, and also show that for a very wide AE there is some deterioration of the main result. Otherwise, the main result about correlation to perceptual data cannot be attributed to the compressive property of the AE.</p><p>4. Biological correlates of the latent representation. On the one hand the authors claim that the AE latent representation aims to mimic a latent representation of the haptic space, which they assume to be compressed and efficient. On the other hand, they claim that the AE representation is similar to mechano-sensory representation, which is a first biological representation before any compression can take place (when hand movements are ignored, as done here). This needs to be clarified.</p><p>5. Validity of the latent representation. The reconstruction error of the AE is large and systematic: only ~50% of the variance are explained and its high frequencies are systematically ignored. The resulting latent representation is such that classes are poorly separable (~29%) and it seems to be by far worse than the human level (around 70% in Baumgartner et al., 2013). It will be therefore interesting to see if the key result, i.e. relation between AE latent space and the perceptual distance, remains valid for a more advanced AE.</p><p><italic>Reviewer #2:</italic></p><p>The goal of using a deep neural network to understand how neuronal representations of tactile texture are constructed is exciting and potentially promising. My enthusiasm for this paper is diminished by the poor performance of the classification and the weak relationship between latent space and perceptual ratings. Indeed, the output of the autoencoder preserves texture information only at a very coarse granularity, resulting in poor classification and poor perceptual predictions.</p><p>The main problem is that the latent factors yield poor classification performance and are only weakly related to perceptual judgments. Indeed, analogous analyses (without fancy machine learning algorithms) tend to perform better on both fronts (classification and perceptual prediction).</p><p>Another issue is that the paper lacks focus: the modest results are cast in the context of a long and somewhat pedantic discussion of optimality.</p><p>The discussion of texture invariance omits two important threads. First, exploratory procedures have been shown not to be optimized on a texture by texture basis (Callier et al., JNP, 2015), as is suggested could be the case. Second, the neural mechanisms that mediate invariance have been discussed (Lieber et al., Cerebral Cortex, 2020) beyond speculations about timbre (Yau et al., Communicative and Integrative Biology, 2009).</p><p>The comparison of the performance of different machine learning approaches does not seem to yield any additional insight and is probably better relegated to supplemental materials.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64876.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The paper describes a very interesting attempt to understand texture coding by touch using autoencoding (AE) of empirical-based simulations of the vibrotactile patterns that would dominate receptor activation during tactile scanning of various surfaces. By showing that the compressed representation generated by the AE allows surface classification, with distances that are correlated to some degree with those exhibited by human perceivers, the authors suggest that the latent representations underlying human perception might be similar to that observed in the simulations. The authors further claim that the simulated latent representation is efficient, and that it matches the characteristics of RA and PC mechanoreceptors. These observations lead the authors to suggest that haptic representation is dictated by efficient coding.</p><p>1. There is a significant gap between the simulated data used here and the empirical data of material perception by touch. The vibratory signals were taken from recordings of surface exploration using a tool tip (Strese et al., 2017 ) whereas the ratings of the different materials are taken from an experiment in which participants used bare hand touch (Baumgartner et al., 2013). The difference is significant especially when material classification, and not only texture classification, is required. It is not at all clear how vibratory signals could code hardness, warmth, elasticity, friction, 3D, etc (see Baumgartner et al., 2013). The authors must provide a serious discussion about this gap and convince the reader that their simulations can indeed provide an access to the internal representations of natural haptic touch.</p></disp-quote><p>We agree with the reviewer that perception of materials is not exclusively limited to vibrations. Indeed, a review of studies investigating the psychophysical dimensions in texture perception revealed five potential dimensions: macro and fine roughness, warmness/coldness, hardness/softness, and friction (Okamoto, Nagano and Yamada, 2013).</p><p>We collected new behavioral data, based on explorations with a tool tip, in which participants had no or very limited access to temperature or softness information and mainly had to rely on the sensed vibratory signals. We compared the perceptual representation inferred from our data with the one from the Baumgartner at al. study (2013) and found that these representations are remarkably similar. Thus we are confident that the missing temperature and softness information did not much affect the perceptual representations we inferred from the rating experiments, which could therefore be thought as representative for natural haptic touch.</p><p>This is explained in the “Behavior” paragraph we added to the Results, and discussed in the discussion of the revised version:</p><p>“In our study participants could not access temperature and had limited access to softness information, nevertheless the perceptual representation inferred from the rating experiment are very similar to the one reported by Baumgartner and colleagues (2013), even though in their study participants could explore materials with their bare hands. This suggests that human perception of materials is largely based on vibrations they elicit on the skin. Bensmaïa and Hollins (2005) showed that 82% of variance in human dissimilarity judgments between pairs of materials could be explained by the differences in vibrations they elicited on the skin.”</p><p>Our behavioral experiment (similar to the one form Baumgartner and colleagues) is explained in the “Rating experiment” section in the methods of the revised version.</p><disp-quote content-type="editor-comment"><p>In the same spirit, they should explain why, and demonstrate that, the pre-processing of the vibrotactile data (cutting and filtering) makes sense for natural haptic touch.</p></disp-quote><p>The reason why we cut the beginning of samples was that we wanted to exclude the vibrations elicited by the contact of the tool tip with the surfaces. For the new measures we started to record after two seconds each trial begun and stopped two seconds before each trial was over. Thus cutting was no longer necessary. This is explained in the methods:</p><p>“To prevent that signals are affected by the onset and offset of contact between the tool and the material surface we started the recording after 2 seconds of exploration and stopped it 2 seconds before the exploration was terminated”</p><p>We filtered our data to focus on the relevant frequency range, thus reducing complexity and potentially also reducing noise and potential confounds. Filtering out frequencies due to exploratory hand movements is a common approach (e.g. Strese et al., 2014, Strese et al., 2017; Romano and Kuchenbecker, 2014; Culbertson, Delgado and Kuchenbecker, 2014) and it controls for a possible confound, i.e. that people explored the textures with a different dynamics depending on the materials, so that the frequencies due to exploratory hand movements would be diagnostic for material differences. For this, we followed Strese et al., (2014) and removed frequencies below 10 Hz. We write this in the methods:</p><p>“Frequencies below 10Hz were ascribed to exploratory hand movements and therefore were filtered out (e.g. Strese et al., 2014, Strese et al., 2017; Romano and Kuchenbecker, 2014; Culbertson, Delgado and Kuchenbecker, 2014). If not removed, vibrations due to hand movements constitute a potential confound because participants could move their hand differently depending on the material, presumably because of the biophysical interaction between their hand and the surfaces (Callier et al., 2015).”</p><p>We also removed frequencies above 800Hz because they are scarcely relevant for perception of texture, as they exceed the limit of the sensitivity of the Pacinian receptors. We comment on this in the methods:</p><p>“We also removed frequencies above 800Hz, as they are not relevant for perception of texture properties of materials (Hollins, Bensmaïa and Washburn, 2001; Bensmaïa and Hollins, 2003; Bensmaïa and Hollins, 2005; Weber et al., 2013; Manfredi et al., 2014) and may be caused by measuring noise.”</p><disp-quote content-type="editor-comment"><p>2. The authors should provide good reasons to convince the readers that the compressed representation they found is indeed a good candidate for the biological representation. First, the nature of the AE algorithm is that it will converge to some representation in the minimal encoder dimension. Why is that a good encoding representation, and why is it a good model for the biological one?</p></disp-quote><p>We do not claim that the AE algorithm is the best model for efficient encoding. We argue that the AE is a plausible model for it because it needs to learn a compressed representation of the sensory input and by doing so it may discover the latent variables caused by reoccurring physical properties of the stimuli, e.g. general properties of wood, metal, plastic, etc. We write this in the discussion of the revised version.</p><p>“Because of the bottleneck Autoencoders must learn a compressed representation from which the input signals can still be reconstructed. To do so, they tend to discover latent variables that are good at capturing the pattern of covariations in the input generated by distal causes (e.g. material properties of the surfaces, Fleming and Storrs, 2019; Storrs and Fleming, 2021).</p><p>Our point is that by compressing the sensory input we learn general patterns of covariation which may reflect the physical properties of the world. In fact, with the new analysis we present in the revised version, we show that the similarity between perceptual representations and the representation learned by the AE depends on the compression level:</p><p>“Our results showed a remarkable similarity between perceptual representations and how material categories are represented in the latent space learned by the Autoencoder. This is consistent with the theory that perceptual representations emerge from efficient encoding of the sensory signals. Thus, when encoding is less efficient, the similarity between learned and perceptual representations should be reduced. We tested this prediction by gradually decreasing the compression rate of the Autoencoder until no compression is applied, i.e. the dimensionality of the latent space is the same as the one of the input. Higher compression rate, defined as the ration between the dimensionality of the latent space and the one of the input (in percentage), implies lower compression and results in higher reconstruction performance (Figure 4D, blue continuous and dashed lines, for the validation and the training data-set, respectively). The least compression (higher compression rate) the lower the correlation between perceptual and latent space distance matrices (Figure 4D – black) and the lower the classification performance (Figure 4D – orange). These results show that compression is crucial for the Autoencoder to learn an internal representation that resembles perceptual representations.”</p><p>We discuss the general theory in the discussion:</p><p>“A similar idea has been proposed for visual perception of material properties (Fleming and Storrs, 2019). The challenge for vision is that the information in the retinal image (proximal stimulus) is insufficient to recover the properties of the world (distal stimulus) that caused it (Anderson, 2011). In fact, multiple causes are usually responsible for a proximal stimulus, e.g. illumination and reflectance spectra are confused within the reflected light. To solve this ambiguity, it is proposed that we learn to represent the dimensions of variation in the proximal stimuli, which arise from the systematic effects of distal stimuli (Fleming and Storrs, 2019), rather than learning to directly estimate the distal properties of the world, as predicted by the inverse optics approach (Marr, 1982; Poggio and Koch, 1985; Pizlo, 2001). This approach could successfully be used to predict perception and misperception of gloss (Storrs, Anderson and Fleming, 2021). Our results support the hypothesis that efficiently encoding the proximal stimuli is the way sensory systems develop perceptual representations.”</p><disp-quote content-type="editor-comment"><p>Second, the differences between the results obtained with the AE and those obtained with humans (Baumgartner et al., 2013) seem to outnumber the partial similarities found. The authors should list both differences and similarities and discuss, based on these comparisons, the probability that the coding found here is similar to the coding guiding human behavior.</p></disp-quote><p>We believe that the major reason for the differences we found between the results we obtained with the AE and those obtained with humans is that we tested the AE on vibrations elicited by a different set of stimuli than the ones used for the behavioral data in Baumgartner et al., (2013). Given this, we were actually surprised to find similarities at all, and we found our results striking. However, ideally one would collect rating judgments and vibratory signals with the same material samples and exploratory movements, and under the same conditions (i.e. no temperature and limited softness signals available). We did this for the revised version: we build our measuring setup and borrowed the stimuli used by Baumgartner and colleagues. This also allowed a direct comparison with their data, which revealed that perceptual representations inferred from exploration with the tool tip are very similar to the ones from bare hand exploration, i.e. our perceptual data are likely to be ecologically valid.</p><p>Analysis on the new data show remarkable similarities between results we obtained with the AE and those obtained with humans. The correlation between the distance matrix (indicating the structure of perceptual representations) between the category averages in the latent space of the Autoencoder with the one obtained from human data is remarkably high and much higher than in the original version (r=0.74 vs r=0.55). Classification performance improved when using labels assigned by human participants, and it is overall higher than in the previous version, and more importantly, close to the between-observers agreement level. This suggests that the model is able to capture nearly all the information available for the perceptual judgments expressed by our participants.</p><disp-quote content-type="editor-comment"><p>3. Notion of efficiency and compression. It was not demonstrated that the main result (figure 3A) is due to compression and efficiency of the AE. What will happen if no AE is used and the distance is measured in the raw input domain (e.g. between Fourier coefficients or principal components)?</p></disp-quote><p>We thank the reviewer for this important insight. In the original version we missed the opportunity to directly test our theory that perceptual representations emerge from efficient encoding (which implies compression). However, the test is straightforward, as the compression level can be systematically varied by changing the bottleneck of the Autoencoder. We repeated all analysis at different compression levels and found that the similarity between perceptual representations and the representation in the latent space increases with the compression level.</p><p>These new results are presented in the “results” section:</p><p>“Our results showed a remarkable similarity between perceptual representations and how material categories are represented in the latent space learned by the Autoencoder. […] To do so, they tend to discover latent variables that are good at capturing the pattern of covariation in the input generated by distal causes (e.g. material properties of the surfaces, Fleming and Storrs, 2019; Storrs and Fleming, 2021).”</p><p>We also followed the suggestion of repeating our analysis with a simple linear compression algorithm (i.e. PCA) rather than the AE. With decreasing compression level, i.e. increasing dimensionality of the bottleneck (16, 64, 128, 192 and 256 Principal Components) classification performance decreases, like for the AE, but it is generally much lower than the one obtained with the AE and at chance level for lower compression levels (24%, 25%, 14%, 14% and 14%, for the bottleneck dimensionalities listed above). The correlation between the perceptual between-category distances and the between-category distances in the latent space is rather independent of the compression level, and never statistically different from zero (rs = 0.3455, 0.3490, 0.3490, 0.3488, 0.3488). The AE seems to be a better model of efficient encoding, as suggested by Fleming and Storrs (2019), although probably other algorithms may learn perceptual representations by compressing the input signals. Note that the mile stone paper by Buchsbaum and Gottschalk (1983) on efficient encoding of color information is based on a principal component transformation.</p><disp-quote content-type="editor-comment"><p>One could expect figure 4 to account for that, and also show that for a very wide AE there is some deterioration of the main result. Otherwise, the main result about correlation to perceptual data cannot be attributed to the compressive property of the AE.</p></disp-quote><p>We thank again the review for pointing this out as we failed to directly test the effect of compression on our results. In Figure 4 of the original version the maximum compression rate we tested was 12%. In the new analysis we go from 6.25% to 100%. By inspecting our results in Figure 4 of the revised version it appears that no much deterioration is to expect around 12% as things start to change with higher compression rates.</p><disp-quote content-type="editor-comment"><p>4. Biological correlates of the latent representation. On the one hand the authors claim that the AE latent representation aims to mimic a latent representation of the haptic space, which they assume to be compressed and efficient. On the other hand, they claim that the AE representation is similar to mechano-sensory representation, which is a first biological representation before any compression can take place (when hand movements are ignored, as done here). This needs to be clarified.</p></disp-quote><p>We are not sure we understand the criticism here. The mechano-sensory representation does not keep all the vibratory information as it can be roughly approximated by a filter in the frequency domain. Such selection of information can be achieved by learning a compressed representation that keeps information at some temporal frequency and throws others out. Similarly, chromatic signals are decorrelated by the retinal circuitry to efficiently transmit them to the brain. Such efficient encoding happens at the very input stage of the visual system and implies color-opponency, which is one of the primary properties of how color is represented perceptually.</p><p>We try to spell this out in the discussion of the revised version.</p><p>“The mechano-sensory representation does not keep all the vibratory information present at the input level: responses of the PC and RA afferents follow a specific tuning profile which implies selection of vibratory information in the time frequency domain. Such selection of information can be achieved by learning a compressed representation that gives more weight to some temporal frequency than others. We speculate that the tuning profiles learned by the Autoencoder, as well as the ones exhibited by the PC and RA afferents, are tuned to the statistical properties of vibratory signals elicited by natural material textures during active human exploration.”</p><disp-quote content-type="editor-comment"><p>5. Validity of the latent representation. The reconstruction error of the AE is large and systematic: only ~50% of the variance are explained and its high frequencies are systematically ignored. The resulting latent representation is such that classes are poorly separable (~29%) and it seems to be by far worse than the human level (around 70% in Baumgartner et al., 2013). It will be therefore interesting to see if the key result, i.e. relation between AE latent space and the perceptual distance, remains valid for a more advanced AE.</p></disp-quote><p>Our idea is that the AE learns what is perceptually relevant and discards the rest, because it represents a plausible model of how information is efficiently encoded by biological systems. So, we are not worried about the proportion of explained variance. Actually, higher compression (thus less explained variance) implies higher similarities with perceptual representations.</p><p>With respect of the poor classification accuracy, the new analysis revealed that our AE model performs very close to the between-participants agreement which gives a reasonable measure of upper performance limit. The correlation between AE latent space and the perceptual distance is also high (r=0.74) indicating that representations are remarkably similar and there is no need for a more complex AE.</p><p>We believe that the mismatch between perceptual representations and the representations learned by the AE reported in the original version of the manuscript mainly arose because for deep learning we used vibratory signals elicited by different stimuli than the ones used for the behavior experiments. After solving this problem, we think that our manuscript has significantly improved.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The goal of using a deep neural network to understand how neuronal representations of tactile texture are constructed is exciting and potentially promising. My enthusiasm for this paper is diminished by the poor performance of the classification and the weak relationship between latent space and perceptual ratings. Indeed, the output of the autoencoder preserves texture information only at a very coarse granularity, resulting in poor classification and poor perceptual predictions.</p></disp-quote><p>We believe that the major reason for the poor classification performance and the weak relationship between the latent space and the perceptual ratings is that we tested the AE on vibrations elicited by a different set of stimuli than the ones used for the behavioral data in Baumgartner et al., (2013). Our idea is that by efficient encoding the AE and biological systems discover the latent variables which caused the signal and by this e.g. typical vibratory patterns for different material categories. So in principle, perceptual and latent spaces should be similar independent of the specific material samples used for training the AE and in the behavioral experiment. However, this requires a huge number of samples. For instance, Storrs and Fleming (2021, Nature) used 10 000 images to train the AE. Available databases of vibratory recordings are 100 times smaller. Given this, we were actually surprised to find similarities between the latent and the perceptual space based on different material samples at all, and we found our results striking. However, ideally, one would collect rating judgments and vibratory signals with the same material samples and exploratory movements, and under the same conditions (i.e. no temperature and limited softness signals available). We did this for the revised version: we build our measuring setup and borrowed the stimuli used by Baumgartner and colleagues. This also allowed a direct comparison with their data, which revealed that perceptual representations inferred from exploration with the tool tip are very similar to the ones from bare hand exploration, i.e. our perceptual data are likely to be ecologically valid.</p><p>Analysis on the new data show remarkable similarities between results we obtained with the AE and those obtained with humans. The correlation between the distance matrix (indicating the structure of perceptual representations) between the category averages in the latent space of the AE with the one obtained from human data is remarkably high and much higher than in the original version (r=0.74 vs. r=0.55). The classification performance also improved as compared to the original version (36% vs 29%)and it is even higher when using labels assigned by human participants (40%). More importantly it is close to the between-observers agreement level (44%). This suggests that the model is able to capture nearly all the information available for the perceptual judgments expressed by our participants.</p><p>We hope our new data an analysis could restore the reviewer‘s enthusiasm as much as it boosted ours.</p><disp-quote content-type="editor-comment"><p>The main problem is that the latent factors yield poor classification performance and are only weakly related to perceptual judgments. Indeed, analogous analyses (without fancy machine learning algorithms) tend to perform better on both fronts (classification and perceptual prediction).</p></disp-quote><p>With the new data and analysis we show that the latent factors yield classification performance nearly as high as between-participants agreement and that they are highly correlated with perceptual judgments. However, we thank the reviewer for the opportunity to explain ourselves better with respect of the choice of the algorithm. We wanted to test the hypothesis that perceptual representations emerge by compressing sensory information to learn to reconstruct the ambiguous sensory input, with no access to the physical ground-truth. This is what autoencoders do, that‘s why we followed the theory of Fleming and Storrs (2019) and used an autoencoder. Note that our autoencoder is much simpler than the one used by Storrs and Fleming (Nature, 2021) to learn visual representation of complex material properties (like gloss). In fact, by systematically varying compression level in the new anaylsis, we discover that compression determines the similarity between perceptual representations and the representations emerged in the latent space. This is in line with the theory that a compressive bottleneck is necessary to discover latent variables that are good at capturing the pattern of covariation in the input generated by distal causes (e.g. material properties of the surfaces) (Storrs and Fleming, 2019; 2021).</p><p>We comment on this while presenting the new results about the compression level in the revised version:</p><p>“Our results showed a remarkable similarity between perceptual representations and how material categories are represented in the latent space learned by the Autoencoder. […] To do so, they tend to discover latent variables that are good at capturing the pattern of covariation in the input generated by distal causes (e.g. material properties of the surfaces, Fleming and Storrs, 2019; Storrs and Fleming, 2021).”</p><p>The theory is also presented and related to the sense of vision, for which it was proposed, in the discussion</p><p>A similar idea has been proposed for visual perception of material properties (Fleming and Storrs, 2019). The challenge for vision is that the information in the retinal image (proximal stimulus) is insufficient to recover the properties of the world (distal stimulus) that caused it (Anderson, 2011). In fact, multiple causes are usually responsible for a proximal stimulus, e.g. illumination and reflectance spectra are confused within the reflected light. To solve this ambiguity, it is proposed that we learn to represent the dimensions of variation in the proximal stimuli, which arise from the systematic effects of distal stimuli (Fleming and Storrs, 2019), rather than learning to directly estimate the distal properties of the world, as predicted by the inverse optics approach (Marr, 1982; Poggio and Koch, 1985; Pizlo, 2001). This approach could successfully be used to predict perception and misperception of gloss (Storrs, Anderson and Fleming, 2021). Our results support the hypothesis that efficiently encoding the proximal stimuli is the way sensory systems develop perceptual representations.</p><disp-quote content-type="editor-comment"><p>Another issue is that the paper lacks focus: the modest results are cast in the context of a long and somewhat pedantic discussion of optimality.</p></disp-quote><p>We thank the reviewer for giving us the chance to clarify: rather than a general theory of optimal encoding of information, we focus on compression and unsupervised learning, starting from the theory of Fleming and Storrs (2019). We hope this is clearer in the revised version.</p><disp-quote content-type="editor-comment"><p>The discussion of texture invariance omits two important threads. First, exploratory procedures have been shown not to be optimized on a texture by texture basis (Callier et al., JNP, 2015), as is suggested could be the case.</p></disp-quote><p>We thank the reviewer for pointing out their results. We included this in the discussion in the revised version:</p><p>“although exploration movements may be optimized for information sampling depending on the task, it seems that they are not optimized on a texture by texture basis”</p><disp-quote content-type="editor-comment"><p>Second, the neural mechanisms that mediate invariance have been discussed (Lieber et al., Cerebral Cortex, 2020) beyond speculations about timbre (Yau et al., Communicative and Integrative Biology, 2009).</p></disp-quote><p>We agree with the reviewer that our discussion may be partial and misleading. Speed-invariance is of course an important topic, but out of the focus of our study. We only wanted to state that we do not claim that the representation learned by the autoencoder is speed-invariant, as we think that it roughly corresponds to the output of the PC and the RA afferents. Nevertheless, the representation emerged in the latent space allows for classification of material categories (as well as the non-speed invariant responses of the PC and RA afferents), presumably because the variability caused by differences in the exploration speed is less than the variability between categories.</p><p>Although speed-invariance wasn’t our focus, we agree we addressed the topic superficially and thank the reviewer for their criticism.</p><p>We changed the paragraphs about optimal exploration and speed-invariance to be clearer and more exhaustive:</p><p>“Similar to the ambiguities in the visual input, a challenge for touch perception is that the temporal frequency content of the input vibratory signals depends both on the surface properties and the exploration speed. […] The changes in the properties of exploratory movements (e.g. exploration speed) depending on texture properties probably arise purely from the biophysical interaction with surfaces rather than any changes in motor behaviour (Callier, Saal, Davis-Berg and Bensmaia, 2015).”</p><disp-quote content-type="editor-comment"><p>The comparison of the performance of different machine learning approaches does not seem to yield any additional insight and is probably better relegated to supplemental materials.</p></disp-quote><p>We agree with the reviewer that a non-systematic exploration of different machine learning approaches does not yield additional insights. By following the suggestions received during the reviewing process we think we understood that the compression level imposed by the bottleneck is the crucial factor to determine the similarity between the latent representation and perceptual representations. We removed the comparisons analysis from the revised version and only report the effects of different bottlenecks.</p></body></sub-article></article>