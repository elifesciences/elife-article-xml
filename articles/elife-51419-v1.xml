<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">51419</article-id><article-id pub-id-type="doi">10.7554/eLife.51419</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Bottom-up and top-down neural signatures of disordered multi-talker speech perception in adults with normal hearing</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-154894"><name><surname>Parthasarathy</surname><given-names>Aravindakshan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4573-8004</contrib-id><email>Aravindakshan_Parthasarathy@meei.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-155842"><name><surname>Hancock</surname><given-names>Kenneth E</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-155843"><name><surname>Bennett</surname><given-names>Kara</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-155844"><name><surname>DeGruttola</surname><given-names>Victor</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-19578"><name><surname>Polley</surname><given-names>Daniel B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5120-2409</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Eaton-Peabody Laboratories</institution><institution>Massachusetts Eye and Ear Infirmary</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Otolaryngology – Head and Neck Surgery</institution><institution>Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Bennett Statistical Consulting Inc</institution><addr-line><named-content content-type="city">Ballston</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Biostatistics</institution><institution>Harvard TH Chan School of Public Health</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>01</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e51419</elocation-id><history><date date-type="received" iso-8601-date="2019-08-28"><day>28</day><month>08</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-12-15"><day>15</day><month>12</month><year>2019</year></date></history><permissions><copyright-statement>© 2020, Parthasarathy et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Parthasarathy et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-51419-v1.pdf"/><abstract><p>In social settings, speech waveforms from nearby speakers mix together in our ear canals. Normally, the brain unmixes the attended speech stream from the chorus of background speakers using a combination of fast temporal processing and cognitive active listening mechanisms. Of &gt;100,000 patient records,~10% of adults visited our clinic because of reduced hearing, only to learn that their hearing was clinically normal and should not cause communication difficulties. We found that multi-talker speech intelligibility thresholds varied widely in normal hearing adults, but could be predicted from neural phase-locking to frequency modulation (FM) cues measured with ear canal EEG recordings. Combining neural temporal fine structure processing, pupil-indexed listening effort, and behavioral FM thresholds accounted for 78% of the variability in multi-talker speech intelligibility. The disordered bottom-up and top-down markers of poor multi-talker speech perception identified here could inform the design of next-generation clinical tests for hidden hearing disorders.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Our ears were not designed for the society our brains created. The World Health Organization estimates that a billion young adults are at risk for hearing problems due to prolonged exposure to high levels of noise. For many people, the first symptoms of hearing loss consist in an inability to follow a single speaker in crowded places such as restaurants.</p><p>However, when Parthasarathy et al. examined over 100,000 records from the Massachusetts Eye and Ear audiology database, they found that around 10% of patients who complained about hearing difficulties were sent home with a clean bill of hearing health. This is because existing tests do not detect common problems related to understanding speech in complex, real-world environments: new tests are needed to spot these hidden hearing disorders. Parthasarathy et al. therefore focused on identifying biological measures that would reflect these issues.</p><p>Normally, the brain can ‘unmix’ different speakers and focus on one person, but even in the context of normal hearing, some people are better at this than others. Parthasarathy et al pinpointed several behavioral and biological markers which, when combined, could predict most of this variability. This involved, for example, measuring the diameter of the pupil while people are listening to speech in the presence of several distracting voices (which mirrors how intensively they have to focus on the task) or measuring the participants’ ability to detect subtle changes in frequency (which reflects how fast-changing sound elements are encoded early on in the hearing system). The findings show that an over-reliance on high-level cognitive processes, such as increased listening effort, coupled with problems in the early processing of certain sound traits, was associated with problems in following a speaker in a busy environment.</p><p>The biological and behavioral markers highlighted by Parthasarathy et al do not require specialized equipment or marathon sessions to be recorded. In theory, these tests could be implemented into most hospital hearing clinics to give patients and health providers objective data to understand, treat and monitor these hearing difficulties.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fine structure</kwd><kwd>hidden hearing loss</kwd><kwd>cochlear synaptopathy</kwd><kwd>FFR</kwd><kwd>pupillometry</kwd><kwd>effortful listening</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P50-DC015857</award-id><principal-award-recipient><name><surname>Polley</surname><given-names>Daniel B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Ear canal EEG and pupillometry reveal disordered temporal processing in adults with normal hearing who struggle to understand conversations in noisy backgrounds.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Slow fluctuations in the sound pressure envelope are sufficient for accurate speech perception in quiet backgrounds (<xref ref-type="bibr" rid="bib99">Shannon et al., 1995</xref>). Envelope cues are less useful when speech is embedded in fluctuant backgrounds comprised of multiple talkers, environmental noise or reverberation (<xref ref-type="bibr" rid="bib120">Zeng et al., 2005</xref>). Under these conditions, segregating a target speech stream from background noise requires accurate encoding of low-frequency spectral and binaural cues contained in the stimulus temporal fine structure (sTFS) (<xref ref-type="bibr" rid="bib37">Hopkins and Moore, 2009</xref>; <xref ref-type="bibr" rid="bib57">Lorenzi et al., 2006</xref>). Monaural sTFS cues convey acoustic signatures of target speaker identity based on the arrangement of peaks in the sound spectrum (e.g., formant frequencies of target speech), while binaural sTFS cues can support spatial separation of target and competing speakers via interaural phase differences (<xref ref-type="bibr" rid="bib71">Moore, 2014</xref>). With aging and hearing loss, monaural and binaural sTFS cues become less perceptually available, even when audibility thresholds for low-frequency signals that convey sTFS cues are normal (<xref ref-type="bibr" rid="bib9">Buss et al., 2004</xref>; <xref ref-type="bibr" rid="bib15">DiNino et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Füllgrabe et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Léger et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">Lorenzi et al., 2009</xref>; <xref ref-type="bibr" rid="bib64">Mehraei et al., 2014</xref>; <xref ref-type="bibr" rid="bib71">Moore, 2014</xref>; <xref ref-type="bibr" rid="bib103">Strelcyk and Dau, 2009</xref>). The biological underpinnings for poor sTFS processing with aging or hearing impairment are unknown, but may reflect the loss of auditory nerve afferent fibers, which degenerate at the rate of approximately 1000 per decade, such that only half survive by the time a typical adult has reached 40 years of age (<xref ref-type="bibr" rid="bib61">Makary et al., 2011</xref>; <xref ref-type="bibr" rid="bib115">Wu et al., 2019</xref>). A selective loss of cochlear afferent fibers would not likely affect audibility thresholds, but could adversely affect the ability of the auditory system to fully exploit suprathreshold monaural and binaural sTFS cues that are critical for multi-talker speech intelligibility (<xref ref-type="bibr" rid="bib12">Deroche et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Hopkins et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Jin and Nelson, 2010</xref>; <xref ref-type="bibr" rid="bib56">Lopez-Poveda and Barrios, 2013</xref>; <xref ref-type="bibr" rid="bib73">Moore and Glasberg, 1987</xref>; <xref ref-type="bibr" rid="bib91">Qin and Oxenham, 2003</xref>, for review see - <xref ref-type="bibr" rid="bib71">Moore, 2014</xref>).</p><p>Accurate processing of a target speaker in a multi-talker background reflects a harmony between high-fidelity encoding of bottom-up acoustic features such as sTFS alongside cognitive signatures of active listening including attention, listening effort, memory, multisensory integration and prediction (<xref ref-type="bibr" rid="bib5">Best et al., 2009</xref>; <xref ref-type="bibr" rid="bib29">Gordon-Salant and Cole, 2016</xref>; <xref ref-type="bibr" rid="bib78">Narayan et al., 2007</xref>; <xref ref-type="bibr" rid="bib89">Pichora-Fuller et al., 2016</xref>; <xref ref-type="bibr" rid="bib113">Wild et al., 2012</xref>; <xref ref-type="bibr" rid="bib114">Winn et al., 2015</xref>). These top-down assets can be leveraged to compensate for poorly resolved bottom-up sensory cues, suggesting that listeners with clinically normal hearing that struggle to process speech in noise might be identified by an over-reliance on top-down active listening mechanisms to de-noise a corrupted afferent speech input (<xref ref-type="bibr" rid="bib4">Besser et al., 2015</xref>; <xref ref-type="bibr" rid="bib81">Ohlenforst et al., 2017</xref>; <xref ref-type="bibr" rid="bib114">Winn et al., 2015</xref>). Here, we apply parallel psychophysical and neurophysiological tests of sTFS processing in combination with physiological measures of effortful listening to converge on a set of neural biomarkers that identify poor multi-talker speech intelligibility in adults with clinically normal hearing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Many individuals seek medical care for poor hearing but have no evidence of hearing loss</title><p>We identified the first visit records of English-speaking adult patients from the Massachusetts Eye and Ear audiology database over a 16 year period, with complete bilateral audiometric records at six octave frequencies from 250 Hz to 8000 Hz according to the inclusion criteria in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Of the 106,787 patient records that met these criteria, we found that approximately one out of every five patients had no clinical evidence of hearing loss, defined as thresholds &gt; 20 dB HL at test frequencies up to 8 KHz (19,952, 19%, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The majority of these individuals were between 20–50 years old (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and had no conductive hearing impairment, nor focal threshold shifts or ‘notches’ in their audiograms greater than 10 dB (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). The thresholds between their left and right ears were also symmetrical within 10 dB for &gt;95% of these patients (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Despite their clean bill of hearing health, 45% of these individuals primarily complained of decreased hearing or hearing loss (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Absent any objective measure of hearing difficulty, these patients are typically informed that their hearing is ‘normal’ and that they are not expected to experience communication problems.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>A normal audiogram does not guarantee robust speech intelligibility in everyday listening conditions.</title><p>(<bold>A</bold>) Screening criteria for eligible audiology patient records from our hospital collected between 2000 and 2016. (<bold>B</bold>) Bilateral normal audiograms, defined as thresholds better than 20 dB HL (gray dashed line) were identified in 19% of the total patient population. Average audiograms from the left (blue) and right (red) ears are shown with individual data points in gray open circles. (<bold>C</bold>) Normalized age distribution of patients with bilateral normal audiograms shows a larger percentage of younger and middle-aged patients between 20–50 years of age. Black square indicates median age of 39 years. (<bold>D</bold>) Top five primary complaints that resulted in the visit to the clinic for these patients, including perceived hearing loss or decreased hearing presenting in 45% of these patients. (<bold>E</bold>) Schematic of a multi-talker digit recognition task. Subjects (N = 23) were familiarized with a target male speaker (red) producing four digits between 1 and 9 (excluding the bi-syllabic ‘7’), while two spatially co-localized distractors, one male and one female, with F<sub>0</sub> frequencies above and below the target speaker simultaneously spoke four digits at varying signal-to-noise ratios (SNRs). (<bold>F</bold>) Accuracy decreased as a function of SNR at variable rates and to variable degrees. Correct trials required correctly reporting all four digits. (<bold>G</bold>) Variability in individual speech reception thresholds, defined as the SNR that produced a 70.7% success rate. Value at right represents sample mean ± SEM. (<bold>H</bold>) Auditory brainstem responses measured using ear canal tiptrodes yielded robust wave one amplitudes, a marker for auditory nerve integrity. Data reflect mean ± SEM. (<bold>I</bold>) Wave one values from individual subjects (left) and mean ± SEM of the sample (right). (<bold>J</bold>) No significant associations were observed between the ABR wave one amplitudes and speech reception threshold on the multi-talker digit task. r = Pearson’s correlation, and shaded area indicates 95% confidence intervals of the regression line (black) in <xref ref-type="fig" rid="fig1">Figures 1</xref>–<xref ref-type="fig" rid="fig4">4</xref>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Digits comprehension thresholds and ABR wave one amplitudes.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Audiometric characteristics of patients with normal audiograms that present at the Massachusetts Eye and Ear audiology clinic with complaints of poor hearing.</title><p>(<bold>A</bold>) Distribution of air-bone gaps, that is differences in pure tone thresholds measured via air conduction and bone conduction suggests the absence of any conductive hearing loss in these patients with normal audiograms at any test frequency (<italic>left</italic>). These patients also did not exhibit large focal threshold shifts (notches) in the audiograms, which are indicative of significant noise damage (<italic>right</italic>) (<xref ref-type="bibr" rid="bib18">Fausti et al., 1981</xref>; <xref ref-type="bibr" rid="bib65">Mehrparvar et al., 2011</xref>; <xref ref-type="bibr" rid="bib48">Le Prell et al., 2013</xref>) (<bold>B</bold>) Normalized distribution plots of the difference in hearing thresholds between the right and left ears at the various test frequencies indicate the lack of significant between-ear asymmetries. Hence by all clinical measures of hearing, these patients coming in to the clinic with hearing difficulties are considered to be audiometrically ‘normal’.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Audiometric profiles and markers of noise exposure in study participants.</title><p>(<bold>A</bold>) Audiograms from the right ear of the 23 participants in this study, indicating normal hearing thresholds for test frequencies up to 8 kHz. Similar thresholds were present in the left ear (data not shown). Individual audiograms are in gray, and the mean audiogram in black. Unshaded region represents the range of normal thresholds. (<bold>B</bold>) High frequency audiograms, considered an early marker for noise damage, showed wide variability in these individuals with normal thresholds in the lower frequencies. (<bold>C</bold>) This came as something of a surprise, as listeners reported lifetime levels of noise exposure that are deemed safe by the EPA, and well below unsafe levels recommended by OSHA and NIOSH. These data suggested that subjective self-reports of noise damage may underestimate the degree of noise damage present in these listeners and that extended high-frequency audibility may be one source of explanation for poor speech processing in noise. (<bold>D</bold>) However, the correlation between the high frequency thresholds and performance on the digits comprehension task was not statistically significant. r = Pearson’s correlation, and shaded area indicates 95% confidence intervals of the regression line (black).</p><p><supplementary-material id="fig1s2sdata1"><label>Figure 1—figure supplement 2—source data 1.</label><caption><title>High-frequency audiometry and noise exposure questionnaire values.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig1-figsupp2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Experimental study design.</title><p>Of the 27 subjects who provided informed consent to participate in the study, 23 were found to be eligible, based on initial screening for English proficiency, use of listening devices, executive function (Montreal Cognitive Assessment, MOCA), depression (Beck’s depression index), tinnitus (Tinnitus Reaction Questionnaire, TRQ) and pure tone audiometry. Eligible participants completed a set of behavioral and physiological test in the clinic. They were then sent home with tablets and calibrated head phones to perform additional testing for 8 days. Subjects returned to the clinic with the tablet for a final day of electrophysiological testing.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Digits comprehension task captures aspects of self-reported difficulties in real-world multi-talker listening conditions experienced by the participants.</title><p>(<bold>A</bold>) Questions related to hearing in multiple-speaker situations questions from the speech, spatial and qualities of hearing scale (SSQ) were among the top five answers that showed the maximum variability in responses in our participants. Participants answered on a sliding scale with 100 meaning ‘perfectly’ and 0 meaning ‘not at all’. (<bold>B</bold>) Mean scores on these five questions in the SSQ correlated with the participants’ performance on the digits comprehension task, indicating that the task captures self-reported difficulties of these participants in real world listening scenarios. r = Pearson’s correlation, and shaded area indicates 95% confidence intervals of the regression line (black).</p><p><supplementary-material id="fig1s4sdata1"><label>Figure 1—figure supplement 4—source data 1.</label><caption><title>Mean values from the SSQ questionnaire.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig1-figsupp4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig1-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Speech-in-noise intelligibility varies widely in individuals with clinically normal hearing</title><p>Our database analysis suggested that approximately one in ten adults arrived to our clinic seeking care for reduced hearing, only to be told that their hearing was fine. This was not entirely surprising, as most clinical tests are not designed to capture difficulties with ‘real world’ speech communication problems that likely prompted their visit to the clinic. To better understand the nature of their suprathreshold hearing problems, we recruited 23 young or middle-aged listeners (mean age: 28.3 ± 0.9 years) that matched the clinically normal hearing from the database profile (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). Our subjects agreed to participate in a multi-stage research study consisting of self-reported questionnaires, behavioral measures of hearing, and EEG measures of auditory processing (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</p><p>Like the patients from the clinical database, the audiograms from these subjects were clinically normal, yet many reported difficulties with speech intelligibility, particularly in listening conditions with multiple overlapping speakers (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4A</xref>). We directly measured speech-in-noise intelligibility with a digits comprehension task, which simulates the acoustic challenge of focused listening in multi-talker environments, while eliminating linguistic and contextual speech cues. Subjects attended to a familiar male speaker (F<sub>0</sub> = 115 Hz) producing a stream of four digits in the presence of one male and one female distracting speakers (F<sub>0</sub> = 90 Hz and 175 Hz, respectively). The distracting speakers produced digits simultaneously at variable signal-to-noise ratios (SNRs) (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Performance on the digits task decreased as a function of SNR (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). Speech reception thresholds, defined as the 70.7% correct point on the response curve, varied widely across a 0–7 dB SNR range with a mean of 2.42 dB (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). We found that speech intelligibility thresholds were significantly correlated with the subjects’ self-reported difficulties in multi-speaker conditions, suggesting that the digits comprehension task captures aspects of their real-world communication difficulties (r = 0.46, p=0.02, <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4B</xref>).</p></sec><sec id="s2-3"><title>Peripheral markers of noise damage do not explain performance on the speech-in-noise task</title><p>We first determined whether simple adaptations of existing clinical tests could identify deficits in multi-talker speech intelligibility. We measured hearing thresholds at extended high frequencies, a marker for early noise damage (<xref ref-type="bibr" rid="bib18">Fausti et al., 1981</xref>; <xref ref-type="bibr" rid="bib48">Le Prell et al., 2013</xref>; <xref ref-type="bibr" rid="bib65">Mehrparvar et al., 2011</xref>). Subjects exhibited substantial variability in their extended high frequency thresholds (&gt;8 kHz) despite having clinically normal audibility at lower frequencies (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>). We also measured the amplitude of auditory brainstem response (ABR) wave 1, which can reveal age- or trauma-related changes in auditory nerve health (<xref ref-type="fig" rid="fig1">Figure 1H–I</xref>) (<xref ref-type="bibr" rid="bib19">Fernandez et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Liberman et al., 2016</xref>; <xref ref-type="bibr" rid="bib86">Parthasarathy and Kujawa, 2018</xref>). ABR wave one amplitude and extended high frequency thresholds both showed substantial variability in subjects with clinically normal audiograms, but neither could account for performance on the competing digits task (r = 0.10 p=0.64, <xref ref-type="fig" rid="fig1">Figure 1J</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref>).</p></sec><sec id="s2-4"><title>Encoding of sTFS cues predicts speech-in-noise intelligibility</title><p>Poor processing of sTFS cues has long been associated with elevated speech recognition thresholds, especially in patients with hearing loss (<xref ref-type="bibr" rid="bib57">Lorenzi et al., 2006</xref>). In a classic test of sTFS processing, subjects were asked to detect a slow, subtle FM imposed on a low-frequency carrier (<xref ref-type="bibr" rid="bib43">King et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Moore and Sek, 1996</xref>; <xref ref-type="bibr" rid="bib74">Moore and Sek, 1995</xref>; <xref ref-type="bibr" rid="bib76">Moore and Skrodzka, 2002</xref>; <xref ref-type="bibr" rid="bib96">Sek and Moore, 1995</xref>; <xref ref-type="bibr" rid="bib108">Wallaert et al., 2018</xref>). We tested our subjects with this psychophysical task, which uses an adaptive two-interval forced choice procedure to converge on the threshold for detecting FM of a 500 Hz tone (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). FM detection thresholds varied widely between subjects (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and were strongly correlated with performance on the competing digits task (r = 0.85, p&lt;0.001, <xref ref-type="fig" rid="fig2">Figure 2C</xref>). We were struck that detection thresholds for such a simple stimulus could accurately predict performance in a much more complex task. On the one hand, sensitivity to FM could reflect superior low-level encoding of sTFS cues that are critical for segregating a target speech stream from distractors. Alternatively, perceptual thresholds for FM could reflect a superior abstracted representation of stimulus features at any downstream stage of neural processing, and not the high fidelity representation of sTFS cues, per se. Taking this line of argument a step further, a correlation between competing talker thresholds and FM thresholds may not reflect the stimulus representation at all, but instead could reflect subjects’ general aptitude for utilizing cognitive resources such as attention and effort to perform a wide range of listening tasks (<xref ref-type="bibr" rid="bib89">Pichora-Fuller et al., 2016</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Perceptual and neural processing of sTFS cues predict speech in noise intelligibility.</title><p>(<bold>A</bold>) Design of a psychophysical task to measure frequency modulation (FM) detection threshold. Participants indicated FM in a 500 Hz tone in an adaptive (2-down 1-up) two alternative forced choice task. (<bold>B</bold>) Individual (left) and average ± SEM (right) FM detection thresholds. Top and bottom five performers (~20<sup>th</sup> percentile) on the behavioral FM detection task are shown in blue and orange respectively, in panels B-C and F-I (<bold>C</bold>) FM detection thresholds were strongly predictive of speech in noise recognition threshold defined with the multi-talker digit comprehension task. (<bold>D</bold>) An objective neurophysiological measure of monaural sTFS processing was obtained using ear canal (-) and scalp (+) electrodes, and a 500 Hz tone presented with various FM deviations in alternating polarity. The averaged response was analyzed at 1000 Hz (2F) in order to minimize contributions by the cochlear microphonic and emphasize neural generators. The magnitude of the FM following response (FMFR) was computed using a heterodyne. (<bold>E</bold>) The FMFR magnitude increased as a function of FM deviation up to ~8 Hz. <italic>Inset:</italic> The FMFR magnitude was normalized by the pure tone phase-locking amplitude of each subject to minimize variability due to head size and recording conditions. (<bold>F–G</bold>) A sigmoidal fit to the normalized FMFR growth curve was used to calculate an FMFR measure of slope for each subject, by dividing the overall dynamic range of the response by the halfway point to the maximum (illustrated in (<bold>F</bold>) for the top and bottom five performers of the behavioral task). Blue and orange bars indicate the X and Y axes intercepts of the halfway point of the fit. (<bold>H–I</bold>) The neurophysiological FMFR was strongly predictive of FM detection thresholds determined with the psychophysical task (<bold>H</bold>) as well as performance on the digits comprehension task (<bold>I</bold>).</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>FM detection thresholds and FMFR slope values.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Determination of optimal electrode montages for obtaining electrophysiological responses.</title><p>(<bold>A</bold>) Grand-averaged ABRs elicited by 3 kHz tones at 105 dB SPL, and recorded simultaneously with an electrode on the ear lobe (yellow), and a gold-foil coated ‘tiptrode’ inserted in the ear canal (blue), shows greater amplitudes using tiptrodes for wave 1, with generators in the auditory nerve, but not wave 5, with midbrain generators. (<bold>B</bold>) Simultaneous multi-channel recordings reveal differential contributions from auditory generators for each electrode montage. Grand-averaged ABR waveforms (left) for three electrode montages show the differential contributions of the ABR waves for each montage, reflecting emphasis on peripheral vs. central generators. The relative amplitudes of waves 1 and 5 is characterized by the w1-w5 asymmetry index (right) with values &gt; 0 having a larger wave 1, and values &lt; 0 having a larger wave 5. Gray bars represent the mean asymmetry index, and error bars represent SEM.</p><p><supplementary-material id="fig2s1sdata1"><label>Figure 2—figure supplement 1—source data 1.</label><caption><title>ABR wave 1 – wave 5 indices for various electrode montages.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig2-figsupp1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Cortical event-related potentials (ERPs) are modulated by FM stimuli, but not related to behavioral performance.</title><p>(<bold>A</bold>) Grand averaged ERPs recorded to FM tones from the Fz – tiptrode electrode montage show clear P1-N1-P2 peaks with associated latencies. (<bold>B</bold>) Peak amplitudes are modulated by FM modulation depth, with N1 amplitudes and N1-P2 amplitudes showing a decrease with larger FM deviations. (<bold>C–D</bold>) Absolute amplitudes for N1, and the N1-P2 elicited to pure tones (FM deviation of 0 Hz) were not correlated with behavioral performance on the FM detection task. (<bold>E–F</bold>) Slopes calculated from amplitude curves expressed as a fraction of the pure-tone phase-locking amplitude, in a manner similar to FMFRs (<xref ref-type="fig" rid="fig2">Figure 2F</xref>) were also not significantly correlated to behavioral performance on the FM task. (<bold>G–H</bold>) Neither the N1 amplitude nor the N1P2 slope were significantly correlated with performance on the digits task. These data demonstrate that whereas phase-locking to sTFS cues was significantly related to multi-talker speech perception, evoked potentials elicited by the same stimuli generated by downstream stages of auditory processing had no such relationship. However, these results should be interpreted with the caveat that the arousal states of the subjects were monitored but not controlled, and changes in arousal states can have significant effects on cortical ERP amplitudes.</p><p><supplementary-material id="fig2s2sdata1"><label>Figure 2—figure supplement 2—source data 1.</label><caption><title>ERP measures.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig2-figsupp2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig2-figsupp2-v1.tif"/></fig></fig-group><p>To test the hypothesis that early neural processing of sTFS cues in the FM tone is associated with superior speech-in-noise processing, we developed a non-invasive physiological measure of neural sTFS phase-locking at early stages of auditory processing. We first determined that an ear canal to Fz electrode montage was sensitive to evoked potentials generated by the auditory nerve (<xref ref-type="fig" rid="fig1">Figure 1I</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A–B</xref>), but we also wanted to exclude any pre-neural contributions, such as the cochlear microphonic, that are generated by hair cells (<xref ref-type="bibr" rid="bib20">Fettiplace, 2017</xref>). Because pre-neural responses are nearly sinusoidal for low-frequency tones, but auditory nerve fiber discharges are partially half-wave-rectified, we could isolate the neural component by alternating the polarity of FM tones, averaging the responses, and analyzing the phase-locking at twice the carrier frequency (<xref ref-type="bibr" rid="bib52">Lichtenhan et al., 2014</xref>). We observed robust phase-locked following response to the FM stimulus (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, termed the FM following response or FMFR). We used a heterodyne method to extract the FMFR for FM depths up to 10 Hz, or ~0.02 octaves (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). To factor out variability in phase-locking due to head size and overall electrode SNR we calculated the amplitude of the carrier frequency following response to a tone with 0 Hz FM and then expressed the FMFR magnitude as a fraction of this value (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, inset). Sigmoidal fits to the FMFR growth function (illustrated for the top and bottom five performers on the behavioral task in <xref ref-type="fig" rid="fig2">Figure 2F</xref>) were further reduced to a single value per subject by dividing the maximum dynamic range for each subject (min-max) by the halfway point to get a measure of slope (halfmax; <xref ref-type="fig" rid="fig2">Figure 2G</xref>). With this approach, subjects with a wide dynamic range for encoding FM depth have more robust FM encoding and therefore smaller min-max/halfmax ratio values.</p><p>We found that robust low-level encoding of FM cues was highly predictive of an individual’s performance on the FM psychophysical detection task (r = 0.66 p=0.001; <xref ref-type="fig" rid="fig2">Figure 2H</xref>), suggesting that the FMFR can provide an objective neurophysiological measure of an individual’s ability to encode sTFS cues. Importantly, the FMFR was also significantly correlated with thresholds on the competing digits task (r = 0.49, p=0.02, <xref ref-type="fig" rid="fig2">Figure 2I</xref>). Whereas phase-locking to the sTFS cues in the FM tone was related to psychophysical performance and speech recognition thresholds, the cortical evoked potentials recorded simultaneously from the same stimuli were not correlated with either measure (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). These data suggest that the strong association between psychophysical tests for FM detection and speech-in-noise intelligibility can be attributed, at least in part, to encoding of FM cues at early stages of auditory processing.</p></sec><sec id="s2-5"><title>Encoding of unrelated sTFS cues do not predict speech-in-noise intelligibility</title><p>We reasoned that the correlation between low-level FM encoding and speech intelligibility might just reflect a correlation between any measure of fast temporal processing fidelity and speech intelligibility. This could be addressed by measuring temporal processing fidelity on an unrelated stimulus and noting whether it had any correlation with speech-in-noise thresholds. Interaural timing cues can improve speech processing in noise, but would not be expected to have any association with the competing digits task used here, where the identical waveform was presented to both ears. To test whether poor encoding of binaural sTFS cues would also predict poor performance in the competing digits task, we performed parallel psychophysical and electrophysiological measurements of sensitivity to interaural phase differences (<xref ref-type="bibr" rid="bib32">Haywood et al., 2015</xref>; <xref ref-type="bibr" rid="bib63">McAlpine et al., 2016</xref>; <xref ref-type="bibr" rid="bib92">Ross et al., 2007</xref>; <xref ref-type="bibr" rid="bib105">Undurraga et al., 2016</xref>).</p><p>In this task, the phase of a 520 Hz tone presented to each ear was shifted by a variable amount (up to 180°), creating the percept of a tone that moved from the center to the sides of the head. To eliminate phase transition artifacts, an amplitude modulation of ~41 Hz was imposed on the tone, such that the instantaneous phase shift always coincided with the null of the amplitude envelope (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) (<xref ref-type="bibr" rid="bib32">Haywood et al., 2015</xref>; <xref ref-type="bibr" rid="bib105">Undurraga et al., 2016</xref>). To test psychophysical thresholds for binaural sTFS cues, subjects indicated the presence of an interaural phase difference (IPD) in one of two tokens presented in a 2-interval forced choice task. IPD thresholds were variable, ranging between 5 and 25 degrees (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). To quantify electrophysiological encoding of IPD, recordings were made with electrodes in a vertical Fz-C7 montage to emphasize binaural generators (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). The IPD was alternated at 6.8 Hz, inducing a following response to the IPD (IPDFR) as well as a phase-insensitive envelope following response at the 41 Hz amplitude modulation rate (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). As expected, the amplitude of the IPDFR at 6.8 Hz increases monotonically with larger interaural time differences, whereas the amplitude of the envelope following response remains constant (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). As above, we minimized variability due to head size and electrode SNR by expressing the IPDFR amplitude as a fraction of the envelope following response. Sigmoidal fits to the normalized growth curves were then used to calculate min-max/halfmax values, similar to the FMFRs (shown for the top and bottom five performers on the behavioral task in <xref ref-type="fig" rid="fig3">Figure 3E</xref>). Like the FMFR above, we noted a strong association between an individual’s psychophysical threshold for IPD and the growth of the electrophysiological IPDFR (r = −0.65 p=0.001, <xref ref-type="fig" rid="fig3">Figure 3F</xref>). Unlike the FMFR, subjects that were most sensitive to IPD showed a large, rapid increase in IPDFR amplitude across the testing range, resulting in a large min-max and a small half-max (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). As a result, the correlation between psychophysical threshold and IPDFR is negative (<xref ref-type="fig" rid="fig3">Figure 3F</xref>) whereas the correlation between FM threshold and the FMFR amplitude is positive (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). This can be attributed to inherent differences in the FMFR (a measure of sTFS phase-locking) versus the IPDFR (a measure of sTFS envelope processing; see Discussion). More to the point, neither the IPD psychophysical threshold, nor the IPDFR amplitude had statistically significant correlations with the digits in noise threshold, confirming that task performance was specifically linked to encoding of task-relevant FM cues and not a general sensitivity to unrelated sTFS cues (IPD threshold and speech, r = 0.21 p=0.34; IPDFR and speech, r = 0.20 p=0.38, <xref ref-type="fig" rid="fig2">Figure 2G</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neural and perceptual processing of rapid temporal cues unrelated to the speech task do not predict speech recognition thresholds.</title><p>(<bold>A</bold>) Design of interaural phase difference (IPD) detection task. The phase of a 520 Hz tone instantaneously shifted from diotic (aligned interaural phase) to dichotic (variable interaural phase). Amplitude modulation (AM) at 40.8 Hz was aligned to the interaural phase shift such that the amplitude minimum coincided with the phase transition. (<bold>B</bold>) Minimal IPD detection threshold was measured in a 2-alternative forced choice task. IPD thresholds varied between 5 and 25 degrees across individual subjects (left), mean ± SEM shown at right. Top and bottom five performers on the behavioral FM detection task are shown in blue and orange respectively. (<bold>C–D</bold>) In EEG recordings, the IPD alternated between diotic and dichotic at a rate of 6.8 Hz. Fast Fourier transforms of scalp-recorded evoked responses revealed a phase-dependent IPD following response (IPDFR) at 6.8 Hz and a 40.8 Hz AM envelope following response. (<bold>E</bold>) The IPDFR magnitude was expressed as a fraction of the envelope following response for each subject to minimize variability due to head size and recording conditions. Min-max and half-max values were computed from sigmoidal fits to the normalized IPDFR growth function (Illustrated here for the top and bottom five performers on the behavioral task) (<bold>F–G</bold>) The IPDFR was strongly predictive of IPD detection thresholds (<bold>F</bold>), but not performance on the digits comprehension task (<bold>G</bold>).</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>IPD detection thresholds and IPDFR slope values.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig3-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig3-v1.tif"/></fig></sec><sec id="s2-6"><title>Pupil-indexed effortful listening predicts speech intelligibility</title><p>Speech recognition is a whole brain phenomenon that is intimately linked to cortical processing as well as cognitive resource allocation such as listening effort, spatial attention, working memory, and prediction (<xref ref-type="bibr" rid="bib14">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib68">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="bib80">O'Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="bib88">Peelle, 2018</xref>; <xref ref-type="bibr" rid="bib93">Ruggles et al., 2011</xref>; <xref ref-type="bibr" rid="bib101">Shinn-Cunningham et al., 2017</xref>; <xref ref-type="bibr" rid="bib102">Song et al., 2014</xref>). In this sense, encoding of bottom-up sTFS cues can provide critical building blocks for downstream speech processing but ultimately provide an incomplete basis for predicting performance on cognitively demanding listening tasks. To capture variability in speech processing that was not accounted for by sTFS cues, we measured task-evoked changes in pupil diameter, while subjects performed the digits comprehension task. Under isoluminous conditions, pupil diameter can provide an objective index of the sensory and cognitive challenge of processing a target speech stream in the presence of distracting speakers (<xref ref-type="bibr" rid="bib45">Koelewijn et al., 2015</xref>; <xref ref-type="bibr" rid="bib109">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib114">Winn et al., 2015</xref>; <xref ref-type="bibr" rid="bib116">Zekveld et al., 2010</xref>). Prior work has shown that increased pupil dilation in low SNR listening conditions can reflect greater utilization of top-down cognitive resources to enhance attended targets, whereas smaller pupil changes have been associated with higher fidelity bottom-up inputs that do not demand additional listening effort to process accurately (<xref ref-type="bibr" rid="bib44">Koelewijn et al., 2012</xref>; <xref ref-type="bibr" rid="bib117">Zekveld et al., 2014</xref>; <xref ref-type="bibr" rid="bib119">Zekveld and Kramer, 2014</xref>) (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>A pupil-indexed measure of effortful listening predicts multi-talker speech recognition thresholds.</title><p>(<bold>A</bold>) Fractional change in pupil diameter was measured under isoluminous conditions before, during and after the 4-digit sequence at various SNRs. (<bold>B</bold>) The peak fractional change in pupil diameter was normalized to the light-induced pupil change for each SNR (<bold>C</bold>). The SNR-dependent change in pupil diameter was calculated as the min-max/halfmax. (<bold>D</bold>) Greater recruitment of pupil-indexed effortful listening across SNRs was significantly associated with the speech intelligibility threshold. Baseline changes in pupil across the testing session, taken as a measure of listening fatigue, showed no relationship with task performance (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Pupil diameter slope values.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Listening fatigue does not account for performance on the multi-talker digit task.</title><p>Changes in baseline pupil diameter have been used as an index of listening fatigue (<xref ref-type="bibr" rid="bib116">Zekveld et al., 2010</xref>; <xref ref-type="bibr" rid="bib118">Zekveld et al., 2018</xref>). (<bold>A</bold>) Baseline pupil diameter measured over the course of successive testing blocks in the 0.5 s before trial onset shows minimal changes for one subject and a steady decrease for another. Gray lines show linear fit for each subject. (<bold>B</bold>) Calculated slope of linear fits for all subjects in the study showing the distributions of changes to baseline pupil diameter. Negative values are suggestive of listening fatigue. (<bold>C</bold>) No correlations were observed between changes in baseline pupil diameter and performance on the digits comprehension task, suggesting that listening fatigue did not contribute to changes seen in task performance. r = Pearson’s correlation, and shaded area indicates 95% confidence intervals of the regression line (black).</p><p><supplementary-material id="fig4s1sdata1"><label>Figure 4—figure supplement 1—source data 1.</label><caption><title>Pupil slope measures as an index of listening fatigue.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig4-figsupp1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig4-figsupp1-v1.tif"/></fig></fig-group><p>We confirmed here that the fractional change in pupil diameter was linearly related to the SNR of the target speaker (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) in 16 subjects that provided measurable pupil signals (see Materials and methods for a statement on exclusion criteria). In the same spirit as removing variability related to head size and electrode SNR, we first factored out unrelated measurement noise by expressing the SNR-dependent change in pupil diameter as a fraction of light-induced pupil change in each subject (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). We confirmed that individuals with steeper pupil recruitment functions had more difficulty in the multi-talker speech task, leading to a significant correlation between min-max/halfmax pupil change and speech intelligibility threshold (r = 0.53, p=0.03, <xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p></sec><sec id="s2-7"><title>Predictors of speech intelligibility – shared and private variability</title><p>Our overall motivation was to develop objective physiological markers that might explain complaints of poor speech communication in individuals with clinically normal hearing. Here, we examined whether poor speech-in-noise intelligibility was associated with poor auditory nerve integrity (indexed here by ABR wave one amplitude), poor encoding of monaural sTFS cues (as indexed by the FMFR), generally poor fast temporal processing (indexed here by IPDFR) and increased utilization of cognitive resources related to effortful listening (indexed here by pupil change). Importantly, none of these indices were correlated with each other, suggesting that – in principle – each of these markers could account for statistically independent components of the total variance in speech performance (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right). In practice, only FMFR and pupil showed a significant independent correlation with speech intelligibility threshold (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A multiple variable linear model of bottom-up and top-down neural and behavioral markers best predicts speech intelligibility thresholds.</title><p>(<bold>A</bold>) The four neural markers studied here (ABR wave 1, FMFR, IPDFR and pupil-indexed effortful listening) were not correlated with each other. FMFR and pupil were both significantly correlated with the outcome measure, digits comprehension threshold. White asterisk indicates p&lt;0.05 with a univariate linear regression model. (<bold>B</bold>) A multivariate regression model measuring the adjusted R<sup>2</sup> (proportion of variance explained by predictors, adjusted for the number of predictors in the model) reveals incremental improvement in the prediction of the digits comprehension threshold when pupil, FMFR and ABR wave one amplitudes are added in succession. Adding additional neural markers to the model did not improve the total explained variance. A separate model which included behavioral FM detection thresholds improved the adjusted R<sup>2</sup> value to 0.78 (gray). (<bold>C</bold>) All two-variable combinations were analyzed to study order effects for introducing variables into the model. The combination of pupil diameter and FMFR was still the most optimal model for explaining variance on the speech-in-noise task. Numbers indicate adjusted R<sup>2</sup> values for each combination.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Digits comprehension thresholds, ABR wave one amplitudes, FMFR slope values, IPDFR slope values and pupil diameter slope values used for the model.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-51419-fig5-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-fig5-v1.tif"/></fig><p>To determine whether combining these independent metrics could account for an even greater fraction of the total variance, we used a multiple variable linear regression model, and computed the adjusted R<sup>2</sup> values, after adding each successive variable. Variables were added in decreasing order of individual R<sup>2</sup> values. The adjusted R<sup>2</sup> penalizes for model complexity incurred due to the addition of more variables (See Materials and methods). With this model, listening effort indexed by pupil diameter explained 24% of the variance in the digit comprehension task. Adding in monaural sTFS processing measured using the FMFR increased the adjusted R<sup>2</sup>, explaining 49% of the overall variance. Adding in the ABR wave one provided only a minimal additional increase in predictive power, raising the total explained variance to 52% (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Adding additional neural markers such as the IPDFR or extended high frequency thresholds did not provide any further increase in the overall variance explained. Among the neural measures studied here, the best linear model for speech intelligibility included a measure of bottom-up monaural fine structure processing and a measure of top-down listening effort. In order to account for order effects in the model, we also looked at the adjusted R<sup>2</sup> for all 2-variable combinations between the FMFR, pupil diameter and the ABR. The combination of FMFR and pupil diameter provided the best model in all order configurations (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Finally, even though the behavioral FM detection thresholds and the FMFR were correlated (<xref ref-type="fig" rid="fig2">Figure 2H</xref>), constructing a model with the psychophysical threshold along with FMFR and pupil diameter increased the variance explained to 78% (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, gray), suggesting that the behavioral FM detection task reflects additional aspects of auditory processing that is not captured by the combination of peripheral sTFS encoding and non-sensory measures of listening effort.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Neural and perceptual processing of temporal fine structure</title><p>The cochlea acts as a limited-resolution filter bank that breaks down the broadband speech waveform into a spatially organized array of narrowband signals. Each cochlear filter contains two types of information that are encoded and reconfigured by neurons within the auditory nerve and central auditory pathway: sTFS and stimulus temporal envelope. STFS cues consist of rapid oscillations near the center of each cochlear filter that are encoded by phase-locked auditory nerve action potentials (<xref ref-type="bibr" rid="bib34">Henry and Heinz, 2013</xref>). Envelope cues, by comparison, reflect slower changes in amplitude over time that can be encoded by the short-term firing rate statistics of auditory nerve fibers (<xref ref-type="bibr" rid="bib42">Joris and Yin, 1992</xref>). The ability to detect slow rates of FM (&lt;~6 Hz) at low carrier frequencies (&lt;~1500 Hz) has long been associated with sTFS processing (<xref ref-type="bibr" rid="bib74">Moore and Sek, 1995</xref>; <xref ref-type="bibr" rid="bib83">Paraouty et al., 2018</xref>; <xref ref-type="bibr" rid="bib96">Sek and Moore, 1995</xref>). Under these stimulus conditions, changes in FM are hypothesized to be conveyed by spike timing information within a cochlear filter (<xref ref-type="bibr" rid="bib74">Moore and Sek, 1995</xref>).</p><p>The strong correlation between psychophysical thresholds for detecting pure tone FM and multi-talker speech recognition thresholds is striking (r = 0.85, <xref ref-type="fig" rid="fig2">Figure 2C</xref>) and has now been documented by several independent groups using a variety of speech-on-speech masking paradigms, but not with non-speech maskers (<xref ref-type="bibr" rid="bib40">Johannesen et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Lopez-Poveda et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Strelcyk and Dau, 2009</xref>; <xref ref-type="bibr" rid="bib112">Whitton et al., 2017</xref>). The exact mechanism of FM coding by the auditory pathway is not entirely clear, with some studies suggesting that FM cues are converted to amplitude modulation cues at early stages of auditory processing, and hence that the perception of FM relies more on neural envelope cues (<xref ref-type="bibr" rid="bib25">Ghitza, 2001</xref>; <xref ref-type="bibr" rid="bib110">Whiteford et al., 2017</xref>; <xref ref-type="bibr" rid="bib111">Whiteford and Oxenham, 2015</xref>), while other studies emphasize neural phase-locking to sTFS cues (<xref ref-type="bibr" rid="bib72">Moore et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Paraouty et al., 2016</xref>; <xref ref-type="bibr" rid="bib108">Wallaert et al., 2018</xref>). The spatial distribution of neural generators for the FMFR also deserves additional study, as some off-channel higher frequency neurons may be combined with low-frequency tonotopically aligned neurons (<xref ref-type="bibr" rid="bib27">Gockel et al., 2015</xref>; <xref ref-type="bibr" rid="bib84">Parthasarathy et al., 2016</xref>).</p><p>Here, we characterized processing of FM tones using a combination of classic psychophysical tests and a newly developed ear canal EEG FMFR. Because we looked at changes in phase-locking to the carrier, and not the actual rate of FM, we were able to explicitly emphasize neural coding of these timing cues in the early auditory system, while minimizing contributions from the recovered envelope, which would be reflected as the 2 Hz FM rate. The FMFR was strongly correlated with behavioral FM detection, suggesting that this response reflects aspects of the behavioral FM detection task (<xref ref-type="fig" rid="fig2">Figure 2H</xref>), and was also correlated with performance on the digits task (<xref ref-type="fig" rid="fig2">Figure 2I</xref>), suggesting that the representation of these fine stimulus timing cues contributes to multi-talker speech intelligibility.</p><p>Subjects with the lowest FM detection thresholds exhibited a small increase in FMFR amplitudes across a broad range of shallow excursion depths before suddenly increasing at FM excursion depths that exceeded the limits of a single cochlear filter, perhaps indicating the transition from a timing to a place code (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). By contrast, the IPDFR transfer function in subjects with the lowest IPD thresholds increased steadily for all IPDs above zero (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). As a result, top psychophysical performers had a shallow transfer function for FM excursion depth but a steep transfer function for IPDFR, producing a positive correlation between FMFR and FM detection threshold (<xref ref-type="fig" rid="fig2">Figure 2H</xref>) and a negative correlation between the IPDFR and IPD detection threshold (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). As described above, the FMFR is calculated as the phase coherence to the FM carrier, whereas the IPDFR is calculated as the entrainment to the rate of IPD alternation. As these measures are in no way equivalent, there is no reason to expect the same relationship between each transfer function and the corresponding psychophysical detection threshold.</p></sec><sec id="s3-2"><title>Revealing the modes of biological failure underlying hidden hearing disorder</title><p>Many forms of cochlear dysfunction could give rise to poor sTFS processing (<xref ref-type="bibr" rid="bib33">Henry et al., 2019</xref>), though when audibility thresholds are normal, the most likely explanation involves a loss of cochlear afferent synapses onto inner hair cells. Auditory nerve fiber loss has been observed in cochlear regions with normal thresholds in many animal species as well as post-mortem analysis of human temporal bone specimens (<xref ref-type="bibr" rid="bib22">Furman et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Gleich et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Kujawa and Liberman, 2009</xref>; <xref ref-type="bibr" rid="bib70">Möhrle et al., 2016</xref>; <xref ref-type="bibr" rid="bib106">Valero et al., 2017</xref>; <xref ref-type="bibr" rid="bib107">Viana et al., 2015</xref>; <xref ref-type="bibr" rid="bib115">Wu et al., 2019</xref>). In humans, recent findings suggest that appreciable auditory nerve fiber loss begins in early adulthood, well before degeneration is noted in cochlear sensory cells or spiral ganglion cell bodies (<xref ref-type="bibr" rid="bib115">Wu et al., 2019</xref>). In animal models, a loss of cochlear afferent synapses disrupts temporal coding of amplitude modulation on a variety of time scales without permanently elevating pure tone thresholds, consistent with observations made in our subject cohort (<xref ref-type="bibr" rid="bib2">Bakay et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">Parthasarathy and Kujawa, 2018</xref>; <xref ref-type="bibr" rid="bib97">Shaheen et al., 2015</xref>). In humans, it is impossible to directly assess the status of cochlear afferent synapses in vivo, though indirect proxies for cochlear afferent innervation may be possible (for recent reviews see - <xref ref-type="bibr" rid="bib6">Bharadwaj et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Bramhall et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Guest et al., 2019</xref>). Prior work has emphasized the amplitudes of ABR waves and extended high frequency hearing thresholds as possible indirect markers of cochlear synapse loss (<xref ref-type="bibr" rid="bib6">Bharadwaj et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Garrett and Verhulst, 2019</xref>; <xref ref-type="bibr" rid="bib51">Liberman et al., 2016</xref>). We found considerable individual variability in both of these measures in subjects with clinically normal hearing, although neither measure had a statistically meaningful relationship with multi-talker speech recognition thresholds (<xref ref-type="fig" rid="fig1">Figure 1J</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref>).</p><p>Speech perception does not arise directly from the auditory nerve, but rather reflects the patterning of neural activity in the central auditory pathway. Therefore, one might expect a weak correlation between a well-designed proxy for cochlear synaptopathy and a behavioral measure of speech recognition accuracy, but the correlation would never be expected to be too high simply because the periphery is a distal – not proximal – basis for speech perception. Hearing loss profoundly affects gene expression, cellular morphology, neurotransmitter levels and physiological signal processing at every stage of the central pathway - from cochlear nucleus to cortex - and these central sequelae resulting from a peripheral insult would also be expected to affect the neural representation of speech in ways that cannot be accounted for purely by peripheral measures (<xref ref-type="bibr" rid="bib1">Auerbach et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Balaram et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Caspary et al., 2008</xref>; <xref ref-type="bibr" rid="bib11">Chambers et al., 2016</xref>; <xref ref-type="bibr" rid="bib70">Möhrle et al., 2016</xref>; <xref ref-type="bibr" rid="bib85">Parthasarathy et al., 2019</xref>; <xref ref-type="bibr" rid="bib95">Sarro et al., 2008</xref>). To this point, the psychophysical FM detection threshold was more highly correlated with speech recognition than the neural measure of low-level FM encoding, suggesting that the behavioral task captured additional aspects of FM detection not present in the FMFR. In a recent placebo-controlled auditory training study, we observed that thresholds could improve by ~1.5 dB SNR on the same digits task used here without any improvement in FM detection threshold, or any other marker of bottom-up processing, again pointing towards the critical involvement of top-down active listening mechanisms in multi-talker speech perception (<xref ref-type="bibr" rid="bib112">Whitton et al., 2017</xref>). Adding neural markers of higher-order stream segregation to the multivariate model (<xref ref-type="bibr" rid="bib16">Divenyi, 2014</xref>; <xref ref-type="bibr" rid="bib46">Krishnan et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Lu et al., 2017</xref>; <xref ref-type="bibr" rid="bib98">Shamma et al., 2011</xref>; <xref ref-type="bibr" rid="bib104">Teki et al., 2013</xref>) or direct neural speech decoding (<xref ref-type="bibr" rid="bib14">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib13">Ding and Simon, 2009</xref>; <xref ref-type="bibr" rid="bib60">Maddox and Lee, 2018</xref>; <xref ref-type="bibr" rid="bib67">Mesgarani et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Mesgarani et al., 2008</xref>; <xref ref-type="bibr" rid="bib87">Pasley et al., 2012</xref>; <xref ref-type="bibr" rid="bib90">Presacco et al., 2016</xref>) would very likely capture even more of the unexplained variability in multi-talker speech intelligibility, though they offer less insight into the particular mode of sensory failure than FMFR and are also considerably harder to implement in a clinical setting.</p><p>Our findings suggest that the individuals who struggle most to follow conversations in noisy, social settings might be identified both by poor bottom-up processing of rapid temporal cues in speech and also by an over-utilization of top-down active listening resources. The interplay between bottom-up and top-down processing is likely more complex than a simple tradeoff where poor sTFS processing is linked to strong pupil-indexed listening effort, or vice versa, as we observed no linear correlation between these variables and no pattern emerged on a subject by subject basis (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Understanding how organisms balance bottom-up and top-down processing strategies to encode communication signals is a question of utmost importance (<xref ref-type="bibr" rid="bib17">Enikolopov et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Mandelblat-Cerf et al., 2014</xref>; <xref ref-type="bibr" rid="bib77">Moore and Woolley, 2019</xref>). In the context of human speech perception, this question would be best tackled by an approach that more explicitly identified the implementation of cognitive active listening mechanisms and was statistically powered to address the question of individual differences (<xref ref-type="bibr" rid="bib38">Jasmin et al., 2019</xref>; <xref ref-type="bibr" rid="bib53">Lim et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Michalka et al., 2015</xref>).</p></sec><sec id="s3-3"><title>Towards clinical biomarkers for hidden hearing disorder</title><p>Patients with bilateral normal audiograms represented ~19% of the patient population at the Massachusetts Eye and Ear Infirmary, 45% of whom reported some form of perceived hearing loss as their primary complaint (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The combination of an increased lifespan and the increased use of in-ear listening devices will likely exacerbate the societal impact of hidden hearing disorder, leaving hundreds of millions of people straining to follow conversations in noisy, reverberant environments typically encountered in the workplace and social settings (<xref ref-type="bibr" rid="bib28">Goman and Lin, 2016</xref>; <xref ref-type="bibr" rid="bib35">Hind et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Lin et al., 2011</xref>; <xref ref-type="bibr" rid="bib94">Ruggles et al., 2012</xref>; <xref ref-type="bibr" rid="bib93">Ruggles et al., 2011</xref>). The standard of care at hearing health clinics include measures of pure tone thresholds, otoacoustic emissions, middle ear reflexes and recognition of individual words presented in silence. These tests are useful in diagnosing late-stage hearing loss commonly found with aging, or exposure to ototoxic drugs or intense noise, where there is pathology in the sound transduction machinery of the middle and inner ear. New diagnostic measures and interventions are needed for the silent majority, who struggle to follow conversations in noisy, social environments and avoid seeking clinical care for their hearing difficulties. Here, we present a simple battery of behavioral and physiological tests that can account for nearly 80% of the variability in a test of multi-talker speech intelligibility that does not involve linguistic cues. Low-channel EEG systems and pupillometry cameras are relatively low-cost and could – in theory – be put to use in clinical settings to provide patients with an objective measure for their perceptual difficulties and provide hearing health providers with an objective readout for their therapeutic drugs or devices.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>All procedures were approved by the institutional review board at the Massachusetts Eye and Ear Infirmary (Protocol #1006581) and Partners Healthcare (Protocol #2019P002423). Twenty seven subjects (13 male, 14 female) were recruited and provided informed consent to be tested as part of the study. Of these, 4 subjects were excluded for either failing to meet the inclusion criteria (1 male, 1 female, see below for inclusion criteria) or not completing more than 60% of the test battery (2 male). One subject (female) did not participate in the electrophysiology tests, but data from the other two sessions were included for relevant analyses. Subjects were compensated per hour for their participation in the study.</p></sec><sec id="s4-2"><title>Testing paradigm - Overview</title><p>Eligibility of the participants was determined on day of first visit by screening for cognitive skills (Montreal Cognitive Assessment, MOCA &gt; 25 for inclusion), depression (Beck’s depression Inventory, BDI &lt;21 for inclusion), tinnitus (Tinnitus reaction questionnaire, TRQ &lt;72 for inclusion), use of assistive listening devices (“Do you routinely use any of the following devices – cochlear implants, hearing aids, bone-anchored hearing aids or FM assistive listening devices’ - subjects were excluded if they answered yes to any of the above) and familiarity with English (‘Are you a native speakers of English’, and ‘If not, Are you fluent or functionally fluent in English?’ - subjects were excluded if they answered no for both questions). Eligible participants were then tested in a double walled acoustically isolated chamber with an audiometer (Interacoustics AC40, Headphones: TDH39) to confirm normal audiograms with thresholds <underline>&lt;</underline> 20 dB HL for frequencies up to 8 kHz. Participants then performed high frequency audiometry (Headphones: Sennheiser HDA200) and the digits comprehension task paired with pupillometry (described in detail below). Subjects were then sent home with tablet computers (Microsoft Surface Pro 2) and calibrated headphones (Bose AE2). Subjects were asked to complete additional suprathreshold testing (FM detection, IPD detection) and questionnaires - Noise exposure questionnaire (NEQ) (<xref ref-type="bibr" rid="bib41">Johnson et al., 2017</xref>) Speech, spatial and Qualities of hearing scale SSQ (<xref ref-type="bibr" rid="bib24">Gatehouse and Noble, 2014</xref>),Tinnitus handicap questionnaires (<xref ref-type="bibr" rid="bib79">Newman et al., 1996</xref>) in a quiet environment over the course of 8 days. The microphone on the tablet was used to measure ambient noise level throughout home-based testing. If noise levels exceeded 60 dB A, the participant was locked out of the software, provided with a warning about excessive noise levels in the test environment, and prompted to find a quieter location for testing. Subjects returned to the laboratory on Day 10 (±1 day) for electrophysiological testing.</p></sec><sec id="s4-3"><title>Speech intelligibility threshold</title><p>Subjects were introduced to the target male speaker (F<sub>0</sub> = 115 Hz) as he produced a string of four randomly selected digits (digits 1–9, excluding the bisyllabic ‘7’) with 0.68 s between the onset of each digit. Once familiarized, the task required subjects to attend to the target speech steam in the presence of two additional speakers (male, F<sub>0</sub> = 90 Hz; female, F<sub>0</sub> – 175 Hz) that produced randomly selected digits with matched target-onset times. The two competing speakers could not produce the same digit as the target speaker or each other, but otherwise digits were selected at random. The target speaker was presented at 65 dB SPL. The signal-to-noise ratio of the distractors ranged from 0 to 20 dB SNR. Subjects reported the target 4-digit sequence using a virtual keypad on the tablet screen 1 s following the presentation of the 4<sup>th</sup> digit. Subjects were initially provided with visual feedback on the accuracy of their report in four practice blocks comprised of 5 trials each and 4 SNRs (target only, 20, 9 and 3 dB SNR). Testing consisted of 40 blocks of 8 trials each, with SNRs of 9, 6, 3 and 0 dB presented in a randomized order for each cycle of four blocks. The first three trials of each block served as refreshers to familiarize the subject with the target speaker at 20 dB SNR before progressively decreasing to the test SNR presented in the last five trials of each block. Trials were scored as correct if all four digits entered into the keypad matched the target speaker sequence. The response curve was constructed using percent correct as a function of SNR, and the 70.7% correct point on the response curve was defined as the speech reception threshold. Subjects with thresholds better than 0 dB SNR (n = 4) were marked as 0.</p></sec><sec id="s4-4"><title>Frequency Modulation detection threshold</title><p>Subjects were introduced to the percept corresponding to frequency modulation (FM) through a virtual slider on the tablet computer that they manipulated to increase and decrease the FM excursion depth of a 500 Hz tone. High excursions were labeled ‘squiggly’ to allow the subjects to associate the sound with a label that could be used when completing the 2-interval 2-alternative forced choice detection task. After initial familiarization, two tones (carrier frequency = 500 Hz, duration = 1 s, level = 55 dB SL) were binaurally presented with the same starting phase to subjects, with an interstimulus interval of 0.5 s. Frequency modulation was applied at a rate of 2 Hz to one of the two tones (order selected at random) and the other tone had no FM. A quasi-sinusoidal amplitude modulation (Amplitude modulation rate randomized between 1–3 Hz, randomized starting phase, 6 dB modulation depth) was applied to both tones to reduce cochlear excitation pattern cues (<xref ref-type="bibr" rid="bib75">Moore and Sek, 1996</xref>). The subject reported whether the first or second tone was ‘squiggly’ (i.e., was the FM tone). A two-down one-up procedure converged on the FM excursion depth that subjects could identify with 70.7% accuracy (<xref ref-type="bibr" rid="bib50">Levitt, 1971</xref>). FM excursion depth was initially set to 75 Hz and was then changed by a factor of 1.5 for the first five reversals, decreasing to a factor of 1.2 for the last seven reversals. The geometric mean of the last six reversals was used to compute the run value. A minimum of 3 runs were collected. The coefficient of variation (standard deviation/mean) for the reversal values was computed during testing. If the coefficient of variation was &gt;0.2, additional runs were collected until this criterion was met or six runs had been collected, whichever came first. The median threshold value obtained across individual runs defined the participant’s FM detection threshold.</p></sec><sec id="s4-5"><title>Interaural Phase Difference detection threshold</title><p>Sensitivity to interaural phase difference was tested using a 2-interval 2-alternative forced choice task. Sound tokens consisted of tones presented simultaneously to both ears at the same carrier frequency (520 Hz), amplitude modulation rate (100% depth at 40.8 Hz), duration (1 s) and level (85 dB SPL, 50 ms raised cosine onset/offset ramps). Each token was separated by a 0.5 s silent interval. Both tokens started in phase. But for one of the two tokens, a phase shift was applied to the tone in each ear in opposing polarity, 0.5 s after tone onset. This produced a perceptual switch, where the sound ‘moved’ from a diotic to a dichotic percept. The subjects were asked which of two sound tokens ‘moved’ in the middle. Subjects were familiarized with the task in two practice blocks of ten trials each and provided visual feedback about their accuracy in identifying the tone that ‘moved’. A two-down-one-up procedure was used to converge on the phase shift that could be identified with 70.7% correct accuracy. The phase shift was initially set to 81 degrees and changed by a factor of 1.5 for the first four reversals, decreasing to a factor of 1.2 for the last six reversals. The geometric mean of the last six reversals was used to compute the run value. The criteria for determining the number of runs and the threshold matched the FM detection task above.</p></sec><sec id="s4-6"><title>Electrophysiology</title><p>EEG recordings were performed in an electrically shielded sound attenuating chamber. Subjects reclined in a chair and were instructed to minimize movements. Arousal state was monitored but not regulated. Most subjects reported sleeping through the recordings. The recording session lasted ~3 hr and subjects were given breaks as necessary. Recordings were done on a 16-channel EEG system (Processor: RZ6, preamplifier: RA16PA, differential low impedance amplifier: RA16-LID, TDT Systems) with two foil electrodes positioned in the ear canals (Etymotic) and six cup electrodes (Grass) placed at Fz, Pz, Oz, C7, and both ear lobes, all referenced to ground at the nape. Impedances were kept below 1 kΩ by prepping the skin (NuPrep, Weaver and Co.) and applying a layer of conductive gel between the electrode and the skin (EC2, Natus Medical). Stimuli were delivered using calibrated ER3A (Etymotic) insert earphones. Stimulus delivery (sampling rate: 100 kHz) and signal acquisition (sampling rate: 25 kHz) were coordinated using the TDT system and a presentation and acquisition software (LabView).</p><p>Auditory brainstem responses were measured in response to 3 kHz tone pips of 5 ms duration. Stimuli had 2.5 ms raised cosine ramps, and were presented at 11.1 repetitions per second. Presentation level was fixed at 105 dB SPL. Stimulus polarity was alternated across trials and 1000 repetitions per polarity were collected. ABRs from the Fz-tiptrode montage were filtered offline between 300 Hz to 3 kHz. Peaks and following troughs of ABR waves were visually marked by an experienced observer, and wave amplitudes were measured using a peak analysis software (<ext-link ext-link-type="uri" xlink:href="https://github.com/bburan/abr">https://github.com/bburan/abr</ext-link>; <xref ref-type="bibr" rid="bib8">Buran, 2019</xref>).</p><p>The FMFR was measured in response to sinusoidal FM stimuli with a carrier frequency of 500 Hz, a modulation rate of 2 Hz and at modulation depths of 0 (i.e. a pure tone), 2, 5, 8, and 10 Hz. The stimuli were 1 s in duration with 5 ms raised cosine ramps and presented once every 1.19 s. Stimulus polarity was alternated across trials and 200 samples were acquired for each polarity. The level was fixed at 90 dB SPL. FMFRs from the Fz-tiptrode montage were used for subsequent analyses. Cochlear neural responses to low frequency tones, including the carrier of our FM stimuli, are phase-sensitive in such a way that the summed response to alternating polarities is effectively rectified, and thus periodic at twice the stimulus frequency (<xref ref-type="bibr" rid="bib52">Lichtenhan et al., 2014</xref>). Therefore, we quantified the modulation of the EEG signals with respect to twice the FM carrier frequency, or 1000 Hz. FMFR amplitudes were calculated using a heterodyne method (<xref ref-type="bibr" rid="bib31">Guinan et al., 2003</xref>). Briefly, a discrete Fourier transform (DFT) was computed for each FMFR average. The negative frequency components were discarded to create an analytic signal. This analytic signal was then down-shifted in frequency so that the components around 1000 Hz became centered at 0 Hz. The frequency-shifted signal was filtered in the frequency domain using an exponential filter (<xref ref-type="bibr" rid="bib100">Shera and Zweig, 1993</xref>), and finally, the inverse DFT was computed. The phase of the inverse DFT is a time-varying signal whose amplitude can be compared directly to the modulation depth of the stimulus. A bootstrapping technique was used to reduce the variability of the calculated FMFR amplitude. An average FMFR was constructed from a subsample of the raw data by drawing 100 samples of each polarity randomly without replacement. This was repeated 1000 times, and the heterodyne analysis was performed on each average. The phase signals output from the heterodyne were averaged and used to compute the final FMFR amplitude. One subject did not yield measurable FMFRs above the noise floor and was excluded from subsequent analyses.</p><p>Interaural phase difference following responses were collected to a 520 Hz tone carrier whose amplitude was modulated at 40.8 Hz. The phase of the carrier was modulated to shift in and out of phase at a rate of 6.8 Hz. The amplitude modulation rate and the rate of inter-aural phase shifts were chosen such that the minima of the amplitude modulation coincided with the point of phase shift. The degree of shift per ear was 0<sup>o</sup> (no shift), 22.5<sup>o</sup>, 45<sup>o</sup> and 90<sup>o</sup>. Presentation level was fixed at 85 dB SPL. Each stimulus condition was presented continuously for 1.47 min, epoched at 294.3 ms to contain 300 epochs with two phase shifts each (one out of phase, and one back into phase) and averaged. IPDFR amplitudes and envelope following response amplitudes were calculated from an FFT performed on the averaged waveforms of the Fz-C7 electrode montage at a resolution equal to 1/epoch length (~3.1 Hz), at 6.8 Hz for the IPD response, and at 40.8 Hz for the AM response. Control recordings consisted of phase shifts of 90<sup>o</sup> in both ears, but in the same polarity to eliminate the binaural component, which showed no responses at the frequency of the interaural phase shift (6.8 Hz).</p></sec><sec id="s4-7"><title>Pupillometry</title><p>Task-related changes in pupil diameter were collected with a head mounted pupillometry system at a 30 Hz sampling rate (Argus Science ET-Mobile), while the subjects used a tablet computer to complete the digits comprehension task (Microsoft Surface Pro 2). The dynamic range in pupil diameter was initially characterized in each subject by presenting alternating bright and dark screens via the tablet computer. Ambient light level was then adjusted to obtain a baseline pupil diameter in the middle of the dynamic range. Pupil measurements were made while subjects were instructed to look at a fixation point on the screen during the presentation of the digits, as confirmed by the experimenter with real time gaze tracking. Pupil data were preprocessed to interpolate for blinks and missing periods using a cubic spline fit, outlier values were removed with a Hampel filter, and the signal was smoothed using a 5-point moving average window. Subjects were excluded if they had extended periods of missing data or if the ambient light could not be adjusted to account for excessive dilation. Reliable data was obtained from 16 subjects who were included for subsequent analyses. A single trial included a 2 s pre-trial baseline period, 2 s for presentation of the digit sequences, a 2 s wait period and the presentation of the virtual keypad to indicate their response. The pupil analysis described here comes from the 2 s digit presentation period. Pupil measurements were normalized to the average baseline pupil diameter for each block, collected in the last 500 ms of the baseline period. A single measure of pupil-indexed listening effort was operationally defined as the peak value of the average fractional change in pupil diameter function, calculated independently for each SNR block as ((post-stimulus time point – baseline)/baseline). The amplitude value for each SNR was then expressed as a ratio of the complete dynamic range for each subject to reduce variability due to recording conditions, arousal states and eye anatomy.</p></sec><sec id="s4-8"><title>Statistical analysis</title><p>The distributions of all of the variables were summarized and examined for outliers. Pairwise linear correlations were computed using Pearson’s correlations (r). To assess which sets of predictors best account for variability in our outcome measure (Digits comprehension threshold), all predictors were considered in univariable models and the R<sup>2</sup> calculated (SAS v9.4, SAS Institute). Data from the 15 subjects who had reliable pupil and FMFR measures were used in the model building, due to the requirement for a balanced dataset across all metrics. Each predictor was then added to the model from highest to lowest R<sup>2</sup>, and the adjusted R<sup>2</sup> calculated using the formula<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where R<sup>2</sup>=sample R-square, p=number of predictors, N = total sample size. The adjusted R<sup>2</sup> penalizes for increasing complexity by adding more predictors.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Authors wish to thank William Goedicke and the audiology department of Mass. Eye and Ear for maintaining and providing access to the audiology database. Thanks also to Dr. Jonathon Whitton for help with designing the psychophysical tasks, and Dr. Kelly Jahn for comments on the manuscript.</p><p>This study was funded by the National Institutes of Health (NIDCD P50-DC015857) to DBP.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Affiliated with Bennett Statistical Consulting Inc. The author has no financial interests to declare.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Investigation, Data curation, Methodology, Formal analysis, Visualization</p></fn><fn fn-type="con" id="con2"><p>Software, Methodology</p></fn><fn fn-type="con" id="con3"><p>Formal analysis</p></fn><fn fn-type="con" id="con4"><p>Formal analysis</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects provided informed consent to be tested as part of the study. All procedures were approved by the institutional review board at the Massachusetts Eye and Ear Infirmary (Protocol #1006581) and Partners Healthcare (Protocol #2019P002423).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-51419-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data used to plot Figures 1-5, and the supplementary figures are provided as source data files.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auerbach</surname> <given-names>BD</given-names></name><name><surname>Radziwon</surname> <given-names>K</given-names></name><name><surname>Salvi</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Testing the Central Gain Model: Loudness Growth Correlates with Central Auditory Gain Enhancement in a Rodent Model of Hyperacusis</article-title><source>Neuroscience</source><volume>407</volume><fpage>93</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.09.036</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakay</surname> <given-names>WMH</given-names></name><name><surname>Anderson</surname> <given-names>LA</given-names></name><name><surname>Garcia-Lazaro</surname> <given-names>JA</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name><name><surname>Schaette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hidden hearing loss selectively impairs neural adaptation to loud sound environments</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4298</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06777-y</pub-id><pub-id pub-id-type="pmid">30327471</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balaram</surname> <given-names>P</given-names></name><name><surname>Hackett</surname> <given-names>TA</given-names></name><name><surname>Polley</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Synergistic transcriptional changes in AMP<sub>A</sub> and GABAA receptor genes support compensatory plasticity following unilateral hearing loss</article-title><source>Neuroscience</source><volume>407</volume><fpage>108</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.08.023</pub-id><pub-id pub-id-type="pmid">30176318</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besser</surname> <given-names>J</given-names></name><name><surname>Festen</surname> <given-names>JM</given-names></name><name><surname>Goverts</surname> <given-names>ST</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name><name><surname>Pichora-Fuller</surname> <given-names>MK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Speech-in-speech listening on the LiSN-S test by older adults with good audiograms depends on cognition and hearing acuity at high frequencies</article-title><source>Ear and Hearing</source><volume>36</volume><fpage>24</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000096</pub-id><pub-id pub-id-type="pmid">25207850</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Best</surname> <given-names>V</given-names></name><name><surname>Marrone</surname> <given-names>N</given-names></name><name><surname>Mason</surname> <given-names>CR</given-names></name><name><surname>Kidd</surname> <given-names>G</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Effects of sensorineural hearing loss on visually guided attention in a multitalker environment</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>10</volume><fpage>142</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1007/s10162-008-0146-7</pub-id><pub-id pub-id-type="pmid">19009321</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharadwaj</surname> <given-names>HM</given-names></name><name><surname>Mai</surname> <given-names>AR</given-names></name><name><surname>Simpson</surname> <given-names>JM</given-names></name><name><surname>Choi</surname> <given-names>I</given-names></name><name><surname>Heinz</surname> <given-names>MG</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Non-Invasive assays of cochlear synaptopathy - Candidates and considerations</article-title><source>Neuroscience</source><volume>407</volume><fpage>53</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2019.02.031</pub-id><pub-id pub-id-type="pmid">30853540</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bramhall</surname> <given-names>N</given-names></name><name><surname>Beach</surname> <given-names>EF</given-names></name><name><surname>Epp</surname> <given-names>B</given-names></name><name><surname>Le Prell</surname> <given-names>CG</given-names></name><name><surname>Lopez-Poveda</surname> <given-names>EA</given-names></name><name><surname>Plack</surname> <given-names>CJ</given-names></name><name><surname>Schaette</surname> <given-names>R</given-names></name><name><surname>Verhulst</surname> <given-names>S</given-names></name><name><surname>Canlon</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The search for noise-induced cochlear synaptopathy in humans: mission impossible?</article-title><source>Hearing Research</source><volume>377</volume><fpage>88</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2019.02.016</pub-id><pub-id pub-id-type="pmid">30921644</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Buran</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>ABR Analysis</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/bburan/abr">https://github.com/bburan/abr</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buss</surname> <given-names>E</given-names></name><name><surname>Hall</surname> <given-names>JW</given-names></name><name><surname>Grose</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Temporal fine-structure cues to speech and pure tone modulation in observers with sensorineural hearing loss</article-title><source>Ear and Hearing</source><volume>25</volume><fpage>242</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1097/01.AUD.0000130796.73809.09</pub-id><pub-id pub-id-type="pmid">15179115</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspary</surname> <given-names>DM</given-names></name><name><surname>Ling</surname> <given-names>L</given-names></name><name><surname>Turner</surname> <given-names>JG</given-names></name><name><surname>Hughes</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inhibitory neurotransmission, plasticity and aging in the mammalian central auditory system</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1781</fpage><lpage>1791</lpage><pub-id pub-id-type="doi">10.1242/jeb.013581</pub-id><pub-id pub-id-type="pmid">18490394</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname> <given-names>AR</given-names></name><name><surname>Resnik</surname> <given-names>J</given-names></name><name><surname>Yuan</surname> <given-names>Y</given-names></name><name><surname>Whitton</surname> <given-names>JP</given-names></name><name><surname>Edge</surname> <given-names>AS</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name><name><surname>Polley</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Central gain restores auditory processing following Near-Complete cochlear denervation</article-title><source>Neuron</source><volume>89</volume><fpage>867</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.041</pub-id><pub-id pub-id-type="pmid">26833137</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deroche</surname> <given-names>ML</given-names></name><name><surname>Culling</surname> <given-names>JF</given-names></name><name><surname>Chatterjee</surname> <given-names>M</given-names></name><name><surname>Limb</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Speech recognition against harmonic and inharmonic complexes: spectral dips and periodicity</article-title><source>The Journal of the Acoustical Society of America</source><volume>135</volume><fpage>2873</fpage><lpage>2884</lpage><pub-id pub-id-type="doi">10.1121/1.4870056</pub-id><pub-id pub-id-type="pmid">24815268</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural representations of complex temporal modulations in the human auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>2731</fpage><lpage>2743</lpage><pub-id pub-id-type="doi">10.1152/jn.00523.2009</pub-id><pub-id pub-id-type="pmid">19692508</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiNino</surname> <given-names>M</given-names></name><name><surname>Irvine</surname> <given-names>A</given-names></name><name><surname>Nolan</surname> <given-names>TP</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>B</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Individuals with normal hearing thresholds differ in their use of fine timing cues to identify consonants in noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>146</volume><elocation-id>3048</elocation-id><pub-id pub-id-type="doi">10.1121/1.5137563</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Divenyi</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decreased ability in the segregation of dynamically changing vowel-analog streams: a factor in the age-related cocktail-party deficit?</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>144</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00144</pub-id><pub-id pub-id-type="pmid">24971047</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enikolopov</surname> <given-names>AG</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Sawtell</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Internally generated predictions enhance neural and behavioral detection of sensory stimuli in an electric fish</article-title><source>Neuron</source><volume>99</volume><fpage>135</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.006</pub-id><pub-id pub-id-type="pmid">30001507</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fausti</surname> <given-names>SA</given-names></name><name><surname>Erickson</surname> <given-names>DA</given-names></name><name><surname>Frey</surname> <given-names>RH</given-names></name><name><surname>Rappaport</surname> <given-names>BZ</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The effects of impulsive noise upon human hearing sensitivity (8 to 20 kHz)</article-title><source>Scandinavian Audiology</source><volume>10</volume><fpage>21</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.3109/01050398109076158</pub-id><pub-id pub-id-type="pmid">7209369</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez</surname> <given-names>KA</given-names></name><name><surname>Jeffers</surname> <given-names>PW</given-names></name><name><surname>Lall</surname> <given-names>K</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name><name><surname>Kujawa</surname> <given-names>SG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Aging after noise exposure: acceleration of cochlear synaptopathy in &quot;recovered&quot; ears</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>7509</fpage><lpage>7520</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5138-14.2015</pub-id><pub-id pub-id-type="pmid">25972177</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fettiplace</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hair cell transduction, tuning, and synaptic transmission in the mammalian cochlea</article-title><source>Comprehensive Physiology</source><volume>7</volume><fpage>1197</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.1002/cphy.c160049</pub-id><pub-id pub-id-type="pmid">28915323</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Füllgrabe</surname> <given-names>C</given-names></name><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Stone</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Age-group differences in speech identification despite matched audiometrically normal hearing: contributions from auditory temporal processing and cognition</article-title><source>Frontiers in Aging Neuroscience</source><volume>6</volume><elocation-id>347</elocation-id><pub-id pub-id-type="doi">10.3389/fnagi.2014.00347</pub-id><pub-id pub-id-type="pmid">25628563</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furman</surname> <given-names>AC</given-names></name><name><surname>Kujawa</surname> <given-names>SG</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Noise-induced cochlear neuropathy is selective for fibers with low spontaneous rates</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>577</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1152/jn.00164.2013</pub-id><pub-id pub-id-type="pmid">23596328</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname> <given-names>M</given-names></name><name><surname>Verhulst</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Applicability of subcortical EEG metrics of synaptopathy to older listeners with impaired audiograms</article-title><source>Hearing Research</source><volume>380</volume><fpage>150</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2019.07.001</pub-id><pub-id pub-id-type="pmid">31306930</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gatehouse</surname> <given-names>S</given-names></name><name><surname>Noble</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Speech, spatial, and qualities of hearing scale (SSQ): Normative data from young, normal-hearing listeners</article-title><source>International Journal of Audiology</source><volume>43</volume><fpage>85</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1121/2.0000018</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>On the upper cutoff frequency of the auditory critical-band envelope detectors in the context of speech perception</article-title><source>The Journal of the Acoustical Society of America</source><volume>110</volume><fpage>1628</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1121/1.1396325</pub-id><pub-id pub-id-type="pmid">11572372</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleich</surname> <given-names>O</given-names></name><name><surname>Semmler</surname> <given-names>P</given-names></name><name><surname>Strutz</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behavioral auditory thresholds and loss of ribbon synapses at inner hair cells in aged gerbils</article-title><source>Experimental Gerontology</source><volume>84</volume><fpage>61</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.exger.2016.08.011</pub-id><pub-id pub-id-type="pmid">27569111</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gockel</surname> <given-names>HE</given-names></name><name><surname>Krugliak</surname> <given-names>A</given-names></name><name><surname>Plack</surname> <given-names>CJ</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Specificity of the Human Frequency Following Response for Carrier and Modulation Frequency Assessed Using Adaptation</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>16</volume><fpage>747</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1007/s10162-015-0533-9</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goman</surname> <given-names>AM</given-names></name><name><surname>Lin</surname> <given-names>FR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prevalence of Hearing Loss by Severity in the United States</article-title><source>American Journal of Public Health</source><volume>106</volume><fpage>1820</fpage><lpage>1822</lpage><pub-id pub-id-type="doi">10.2105/AJPH.2016.303299</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon-Salant</surname> <given-names>S</given-names></name><name><surname>Cole</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Effects of Age and Working Memory Capacity on Speech Recognition Performance in Noise Among Listeners With Normal Hearing</article-title><source>Ear and Hearing</source><volume>37</volume><fpage>593</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000316</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guest</surname> <given-names>H</given-names></name><name><surname>Munro</surname> <given-names>KJ</given-names></name><name><surname>Prendergast</surname> <given-names>G</given-names></name><name><surname>Plack</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reliability and interrelations of seven proxy measures of cochlear synaptopathy</article-title><source>Hearing Research</source><volume>375</volume><fpage>34</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2019.01.018</pub-id><pub-id pub-id-type="pmid">30765219</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guinan</surname> <given-names>JJ</given-names></name><name><surname>Backus</surname> <given-names>BC</given-names></name><name><surname>Lilaonitkul</surname> <given-names>W</given-names></name><name><surname>Aharonson</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Medial olivocochlear efferent reflex in humans: otoacoustic emission (OAE) measurement issues and the advantages of stimulus frequency OAEs</article-title><source>JARO - Journal of the Association for Research in Otolaryngology</source><volume>4</volume><fpage>521</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1007/s10162-002-3037-3</pub-id><pub-id pub-id-type="pmid">12799992</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haywood</surname> <given-names>NR</given-names></name><name><surname>Undurraga</surname> <given-names>JA</given-names></name><name><surname>Marquardt</surname> <given-names>T</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A comparison of two objective measures of binaural processing: the interaural phase modulation following response and the binaural interaction component</article-title><source>Trends in Hearing</source><volume>19</volume><elocation-id>2331216515619039</elocation-id><pub-id pub-id-type="doi">10.1177/2331216515619039</pub-id><pub-id pub-id-type="pmid">26721925</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname> <given-names>KS</given-names></name><name><surname>Sayles</surname> <given-names>M</given-names></name><name><surname>Hickox</surname> <given-names>AE</given-names></name><name><surname>Heinz</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Divergent auditory nerve encoding deficits between two common etiologies of sensorineural hearing loss</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>6879</fpage><lpage>6887</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0038-19.2019</pub-id><pub-id pub-id-type="pmid">31285299</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname> <given-names>KS</given-names></name><name><surname>Heinz</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effects of sensorineural hearing loss on temporal coding of narrowband and broadband signals in the auditory periphery</article-title><source>Hearing Research</source><volume>303</volume><fpage>39</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.01.014</pub-id><pub-id pub-id-type="pmid">23376018</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hind</surname> <given-names>SE</given-names></name><name><surname>Haines-Bazrafshan</surname> <given-names>R</given-names></name><name><surname>Benton</surname> <given-names>CL</given-names></name><name><surname>Brassington</surname> <given-names>W</given-names></name><name><surname>Towle</surname> <given-names>B</given-names></name><name><surname>Moore</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prevalence of clinical referrals having hearing thresholds within normal limits</article-title><source>International Journal of Audiology</source><volume>50</volume><fpage>708</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.3109/14992027.2011.582049</pub-id><pub-id pub-id-type="pmid">21714709</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopkins</surname> <given-names>K</given-names></name><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Stone</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of moderate cochlear hearing loss on the ability to benefit from temporal fine structure information in speech</article-title><source>The Journal of the Acoustical Society of America</source><volume>123</volume><fpage>1140</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1121/1.2824018</pub-id><pub-id pub-id-type="pmid">18247914</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopkins</surname> <given-names>K</given-names></name><name><surname>Moore</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The contribution of temporal fine structure to the intelligibility of speech in steady and modulated noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>125</volume><fpage>442</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1121/1.3037233</pub-id><pub-id pub-id-type="pmid">19173429</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jasmin</surname> <given-names>K</given-names></name><name><surname>Dick</surname> <given-names>F</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name><name><surname>Tierney</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Tailored perception: individuals’ speech and music perception strategies fit their perceptual abilities</article-title><source>Journal of Experimental Psychology: General</source><pub-id pub-id-type="doi">10.1037/xge0000688</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname> <given-names>SH</given-names></name><name><surname>Nelson</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Interrupted speech perception: the effects of hearing sensitivity and frequency resolution</article-title><source>The Journal of the Acoustical Society of America</source><volume>128</volume><fpage>881</fpage><lpage>889</lpage><pub-id pub-id-type="doi">10.1121/1.3458851</pub-id><pub-id pub-id-type="pmid">20707457</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johannesen</surname> <given-names>PT</given-names></name><name><surname>Pérez-González</surname> <given-names>P</given-names></name><name><surname>Kalluri</surname> <given-names>S</given-names></name><name><surname>Blanco</surname> <given-names>JL</given-names></name><name><surname>Lopez-Poveda</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The influence of cochlear mechanical dysfunction, temporal processing deficits, and age on the intelligibility of audible speech in noise for Hearing-Impaired listeners</article-title><source>Trends in Hearing</source><volume>20</volume><elocation-id>233121651664105</elocation-id><pub-id pub-id-type="doi">10.1177/2331216516641055</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>TA</given-names></name><name><surname>Cooper</surname> <given-names>S</given-names></name><name><surname>Stamper</surname> <given-names>GC</given-names></name><name><surname>Chertoff</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Noise exposure questionnaire: a tool for quantifying annual noise exposure</article-title><source>Journal of the American Academy of Audiology</source><volume>28</volume><fpage>14</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.3766/jaaa.15070</pub-id><pub-id pub-id-type="pmid">28054909</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joris</surname> <given-names>PX</given-names></name><name><surname>Yin</surname> <given-names>TC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Responses to amplitude-modulated tones in the auditory nerve of the cat</article-title><source>The Journal of the Acoustical Society of America</source><volume>91</volume><fpage>215</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1121/1.402757</pub-id><pub-id pub-id-type="pmid">1737873</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>A</given-names></name><name><surname>Varnet</surname> <given-names>L</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Accounting for masking of frequency modulation by amplitude modulation with the modulation filter-bank concept</article-title><source>The Journal of the Acoustical Society of America</source><volume>145</volume><fpage>2277</fpage><lpage>2293</lpage><pub-id pub-id-type="doi">10.1121/1.5094344</pub-id><pub-id pub-id-type="pmid">31046322</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelewijn</surname> <given-names>T</given-names></name><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Festen</surname> <given-names>JM</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Pupil dilation uncovers extra listening effort in the presence of a Single-Talker masker</article-title><source>Ear and Hearing</source><volume>33</volume><fpage>291</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3182310019</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelewijn</surname> <given-names>T</given-names></name><name><surname>de Kluiver</surname> <given-names>H</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The pupil response reveals increased listening effort when it is difficult to focus attention</article-title><source>Hearing Research</source><volume>323</volume><fpage>81</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2015.02.004</pub-id><pub-id pub-id-type="pmid">25732724</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Krishnan</surname> <given-names>L</given-names></name><name><surname>Shamma</surname> <given-names>S</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>Segregating complex sound sources through temporal coherence</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003985.s001</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kujawa</surname> <given-names>SG</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Adding insult to injury: cochlear nerve degeneration after &quot;Temporary&quot; Noise-Induced Hearing Loss</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>14077</fpage><lpage>14085</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2845-09.2009</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Prell</surname> <given-names>CG</given-names></name><name><surname>Spankovich</surname> <given-names>C</given-names></name><name><surname>Lobariñas</surname> <given-names>E</given-names></name><name><surname>Griffiths</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Extended High-Frequency thresholds in college students: effects of music player use and other recreational noise</article-title><source>Journal of the American Academy of Audiology</source><volume>24</volume><fpage>725</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.3766/jaaa.24.8.9</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Léger</surname> <given-names>AC</given-names></name><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Abnormal speech processing in frequency regions where absolute thresholds are normal for listeners with high-frequency hearing loss</article-title><source>Hearing Research</source><volume>294</volume><fpage>95</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2012.10.002</pub-id><pub-id pub-id-type="pmid">23104012</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Transformed up‐down methods in psychoacoustics</article-title><source>The Journal of the Acoustical Society of America</source><volume>49</volume><fpage>467</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1121/1.1912375</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname> <given-names>MC</given-names></name><name><surname>Epstein</surname> <given-names>MJ</given-names></name><name><surname>Cleveland</surname> <given-names>SS</given-names></name><name><surname>Wang</surname> <given-names>H</given-names></name><name><surname>Maison</surname> <given-names>SF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Toward a differential diagnosis of hidden hearing loss in humans</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0162726</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0162726</pub-id><pub-id pub-id-type="pmid">27618300</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lichtenhan</surname> <given-names>JT</given-names></name><name><surname>Hartsock</surname> <given-names>JJ</given-names></name><name><surname>Gill</surname> <given-names>RM</given-names></name><name><surname>Guinan</surname> <given-names>JJ</given-names></name><name><surname>Salt</surname> <given-names>AN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The auditory nerve overlapped waveform (ANOW) originates in the cochlear apex</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>15</volume><fpage>395</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1007/s10162-014-0447-y</pub-id><pub-id pub-id-type="pmid">24515339</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname> <given-names>S-J</given-names></name><name><surname>Fiez</surname> <given-names>JA</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Role of the striatum in incidental learning of sound categories</article-title><source>PNAS</source><volume>116</volume><fpage>4671</fpage><lpage>4680</lpage><pub-id pub-id-type="doi">10.1073/pnas.1811992116</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname> <given-names>FR</given-names></name><name><surname>Thorpe</surname> <given-names>R</given-names></name><name><surname>Gordon-Salant</surname> <given-names>S</given-names></name><name><surname>Ferrucci</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hearing loss prevalence and risk factors among older adults in the united states</article-title><source>The Journals of Gerontology Series A: Biological Sciences and Medical Sciences</source><volume>66A</volume><fpage>582</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1093/gerona/glr002</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Poveda</surname> <given-names>EA</given-names></name><name><surname>Johannesen</surname> <given-names>PT</given-names></name><name><surname>Pérez-González</surname> <given-names>P</given-names></name><name><surname>Blanco</surname> <given-names>JL</given-names></name><name><surname>Kalluri</surname> <given-names>S</given-names></name><name><surname>Edwards</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Predictors of Hearing-Aid outcomes</article-title><source>Trends in Hearing</source><volume>21</volume><elocation-id>233121651773052</elocation-id><pub-id pub-id-type="doi">10.1177/2331216517730526</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Poveda</surname> <given-names>EA</given-names></name><name><surname>Barrios</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Perception of stochastically undersampled sound waveforms: a model of auditory deafferentation</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00124</pub-id><pub-id pub-id-type="pmid">23882176</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenzi</surname> <given-names>C</given-names></name><name><surname>Gilbert</surname> <given-names>G</given-names></name><name><surname>Carn</surname> <given-names>H</given-names></name><name><surname>Garnier</surname> <given-names>S</given-names></name><name><surname>Moore</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Speech perception problems of the hearing impaired reflect inability to use temporal fine structure</article-title><source>PNAS</source><volume>103</volume><fpage>18866</fpage><lpage>18869</lpage><pub-id pub-id-type="doi">10.1073/pnas.0607364103</pub-id><pub-id pub-id-type="pmid">17116863</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenzi</surname> <given-names>C</given-names></name><name><surname>Debruille</surname> <given-names>L</given-names></name><name><surname>Garnier</surname> <given-names>S</given-names></name><name><surname>Fleuriot</surname> <given-names>P</given-names></name><name><surname>Moore</surname> <given-names>BCJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Abnormal processing of temporal fine structure in speech for frequencies where absolute thresholds are normal</article-title><source>The Journal of the Acoustical Society of America</source><volume>125</volume><fpage>27</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1121/1.2939125</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname> <given-names>K</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Yin</surname> <given-names>P</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal coherence structure rapidly shapes neuronal interactions</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13900</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13900</pub-id><pub-id pub-id-type="pmid">28054545</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname> <given-names>RK</given-names></name><name><surname>Lee</surname> <given-names>AKC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Auditory brainstem responses to continuous natural speech in human listeners</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0441-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0441-17.2018</pub-id><pub-id pub-id-type="pmid">29435487</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makary</surname> <given-names>CA</given-names></name><name><surname>Shin</surname> <given-names>J</given-names></name><name><surname>Kujawa</surname> <given-names>SG</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name><name><surname>Merchant</surname> <given-names>SN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Age-related primary cochlear neuronal degeneration in human temporal bones</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>12</volume><fpage>711</fpage><lpage>717</lpage><pub-id pub-id-type="doi">10.1007/s10162-011-0283-2</pub-id><pub-id pub-id-type="pmid">21748533</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandelblat-Cerf</surname> <given-names>Y</given-names></name><name><surname>Las</surname> <given-names>L</given-names></name><name><surname>Denisenko</surname> <given-names>N</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A role for descending auditory cortical projections in songbird vocal learning</article-title><source>eLife</source><volume>3</volume><elocation-id>e02152</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.02152</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McAlpine</surname> <given-names>D</given-names></name><name><surname>Haywood</surname> <given-names>N</given-names></name><name><surname>Undurraga</surname> <given-names>J</given-names></name><name><surname>Marquardt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Objective Measures of Neural Processing of Interaural Time Differences</chapter-title><person-group person-group-type="editor"><name><surname>VanDijk</surname> <given-names>P</given-names></name><name><surname>Baskent</surname> <given-names>D</given-names></name><name><surname>Gaudrain</surname> <given-names>E</given-names></name><name><surname>DeKleine</surname> <given-names>E</given-names></name><name><surname>Wagner</surname> <given-names>A</given-names></name><name><surname>Lanting</surname> <given-names>C</given-names></name></person-group><source>Psychoacoustics and Cognition in Normal and Impaired Hearing, Advances in Experimental Medicine and Biology Physiology</source><publisher-name>Springer</publisher-name><fpage>197</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-25474-6_21</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehraei</surname> <given-names>G</given-names></name><name><surname>Gallun</surname> <given-names>FJ</given-names></name><name><surname>Leek</surname> <given-names>MR</given-names></name><name><surname>Bernstein</surname> <given-names>JGW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spectrotemporal modulation sensitivity for hearing-impaired listeners: dependence on carrier center frequency and the relationship to speech intelligibility</article-title><source>The Journal of the Acoustical Society of America</source><volume>136</volume><fpage>301</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1121/1.4881918</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrparvar</surname> <given-names>A</given-names></name><name><surname>Ghoreyshi</surname> <given-names>A</given-names></name><name><surname>Loukzadeh</surname> <given-names>Z</given-names></name><name><surname>Mirmohammadi</surname> <given-names>S</given-names></name><name><surname>Mollasadeghi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>High-frequency audiometry: A means for early diagnosis of noise-induced hearing loss</article-title><source>Noise and Health</source><volume>13</volume><fpage>402</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.4103/1463-1741.90295</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Phoneme representation and classification in primary auditory cortex</article-title><source>The Journal of the Acoustical Society of America</source><volume>123</volume><fpage>899</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.1121/1.2816572</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic Feature Encoding in Human Superior Temporal Gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title><source>Nature</source><volume>485</volume><fpage>233</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature11020</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalka</surname> <given-names>SW</given-names></name><name><surname>Kong</surname> <given-names>L</given-names></name><name><surname>Rosen</surname> <given-names>ML</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Somers</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Short-Term Memory for Space and Time Flexibly Recruit Complementary Sensory-Biased Frontal Lobe Attention Networks</article-title><source>Neuron</source><volume>87</volume><fpage>882</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.028</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Möhrle</surname> <given-names>D</given-names></name><name><surname>Ni</surname> <given-names>K</given-names></name><name><surname>Varakina</surname> <given-names>K</given-names></name><name><surname>Bing</surname> <given-names>D</given-names></name><name><surname>Lee</surname> <given-names>SC</given-names></name><name><surname>Zimmermann</surname> <given-names>U</given-names></name><name><surname>Knipper</surname> <given-names>M</given-names></name><name><surname>Rüttiger</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Loss of auditory sensitivity from inner hair cell synaptopathy can be centrally compensated in the young but not old brain</article-title><source>Neurobiology of Aging</source><volume>44</volume><fpage>173</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2016.05.001</pub-id><pub-id pub-id-type="pmid">27318145</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BCJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Auditory Processing of Temporal Fine Structure: Effects of Age and Hearing Loss</source><publisher-name>World Scientific</publisher-name><pub-id pub-id-type="doi">10.1142/9064</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BCJ</given-names></name><name><surname>Mariathasan</surname> <given-names>S</given-names></name><name><surname>Sęk</surname> <given-names>AP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effects of age and hearing loss on the discrimination of amplitude and frequency modulation for 2- and 10-Hz rates</article-title><source>Trends in Hearing</source><volume>23</volume><elocation-id>233121651985396</elocation-id><pub-id pub-id-type="doi">10.1177/2331216519853963</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Glasberg</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Factors affecting thresholds for sinusoidal signals in narrow-band maskers with fluctuating envelopes</article-title><source>The Journal of the Acoustical Society of America</source><volume>82</volume><fpage>69</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1121/1.395439</pub-id><pub-id pub-id-type="pmid">3624643</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Sek</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Effects of carrier frequency, modulation rate, and modulation waveform on the detection of modulation and the discrimination of modulation type (amplitude modulation versus frequency modulation)</article-title><source>The Journal of the Acoustical Society of America</source><volume>97</volume><fpage>2468</fpage><lpage>2478</lpage><pub-id pub-id-type="doi">10.1121/1.411967</pub-id><pub-id pub-id-type="pmid">7714263</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Sek</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Detection of frequency modulation at low modulation rates: evidence for a mechanism based on phase locking</article-title><source>The Journal of the Acoustical Society of America</source><volume>100</volume><fpage>2320</fpage><lpage>2331</lpage><pub-id pub-id-type="doi">10.1121/1.417941</pub-id><pub-id pub-id-type="pmid">8865639</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>BC</given-names></name><name><surname>Skrodzka</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Detection of frequency modulation by hearing-impaired listeners: effects of carrier frequency, modulation rate, and added amplitude modulation</article-title><source>The Journal of the Acoustical Society of America</source><volume>111</volume><fpage>327</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1121/1.1424871</pub-id><pub-id pub-id-type="pmid">11833538</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>JM</given-names></name><name><surname>Woolley</surname> <given-names>SMN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emergent tuning for learned vocalizations in auditory cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1469</fpage><lpage>1476</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0458-4</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narayan</surname> <given-names>R</given-names></name><name><surname>Best</surname> <given-names>V</given-names></name><name><surname>Ozmeral</surname> <given-names>E</given-names></name><name><surname>McClaine</surname> <given-names>E</given-names></name><name><surname>Dent</surname> <given-names>M</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>B</given-names></name><name><surname>Sen</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cortical interference effects in the cocktail party problem</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1601</fpage><lpage>1607</lpage><pub-id pub-id-type="doi">10.1038/nn2009</pub-id><pub-id pub-id-type="pmid">17994016</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newman</surname> <given-names>CW</given-names></name><name><surname>Jacobson</surname> <given-names>GP</given-names></name><name><surname>Spitzer</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Development of the tinnitus handicap inventory</article-title><source>Archives of Otolaryngology - Head and Neck Surgery</source><volume>122</volume><fpage>143</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1001/archotol.1996.01890140029007</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Power</surname> <given-names>AJ</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Rajaram</surname> <given-names>S</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name><name><surname>Slaney</surname> <given-names>M</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional selection in a cocktail party environment can be decoded from Single-Trial EEG</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht355</pub-id><pub-id pub-id-type="pmid">24429136</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohlenforst</surname> <given-names>B</given-names></name><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Jansma</surname> <given-names>EP</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Naylor</surname> <given-names>G</given-names></name><name><surname>Lorens</surname> <given-names>A</given-names></name><name><surname>Lunner</surname> <given-names>T</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Effects of hearing impairment and hearing aid amplification on listening effort: a systematic review</article-title><source>Ear and Hearing</source><volume>38</volume><fpage>267</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000396</pub-id><pub-id pub-id-type="pmid">28234670</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paraouty</surname> <given-names>N</given-names></name><name><surname>Ewert</surname> <given-names>SD</given-names></name><name><surname>Wallaert</surname> <given-names>N</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Interactions between amplitude modulation and frequency modulation processing: Effects of age and hearing loss</article-title><source>The Journal of the Acoustical Society of America</source><volume>140</volume><fpage>121</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1121/1.4955078</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paraouty</surname> <given-names>N</given-names></name><name><surname>Stasiak</surname> <given-names>A</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name><name><surname>Varnet</surname> <given-names>L</given-names></name><name><surname>Winter</surname> <given-names>IM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dual Coding of Frequency Modulation in the Ventral Cochlear Nucleus</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>4123</fpage><lpage>4137</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2107-17.2018</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname> <given-names>A</given-names></name><name><surname>Lai</surname> <given-names>J</given-names></name><name><surname>Bartlett</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Age-Related Changes in Processing Simultaneous Amplitude Modulated Sounds Assessed Using Envelope Following Responses</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>17</volume><fpage>119</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1007/s10162-016-0554-z</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname> <given-names>A</given-names></name><name><surname>Herrmann</surname> <given-names>B</given-names></name><name><surname>Bartlett</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Aging alters envelope representations of speech-like sounds in the inferior colliculus</article-title><source>Neurobiology of Aging</source><volume>73</volume><fpage>30</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2018.08.023</pub-id><pub-id pub-id-type="pmid">30316050</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname> <given-names>A</given-names></name><name><surname>Kujawa</surname> <given-names>SG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synaptopathy in the aging cochlea: characterizing Early-Neural deficits in auditory temporal envelope processing</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7108</fpage><lpage>7119</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3240-17.2018</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasley</surname> <given-names>BN</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Flinker</surname> <given-names>A</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reconstructing speech from human auditory cortex</article-title><source>PLOS Biology</source><volume>10</volume><elocation-id>e1001251</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001251</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Listening effort: how the cognitive consequences of acoustic challenge are reflected in brain and behavior</article-title><source>Ear and Hearing</source><volume>39</volume><fpage>204</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000494</pub-id><pub-id pub-id-type="pmid">28938250</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname> <given-names>MK</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name><name><surname>Eckert</surname> <given-names>MA</given-names></name><name><surname>Edwards</surname> <given-names>B</given-names></name><name><surname>Hornsby</surname> <given-names>BW</given-names></name><name><surname>Humes</surname> <given-names>LE</given-names></name><name><surname>Lemke</surname> <given-names>U</given-names></name><name><surname>Lunner</surname> <given-names>T</given-names></name><name><surname>Matthen</surname> <given-names>M</given-names></name><name><surname>Mackersie</surname> <given-names>CL</given-names></name><name><surname>Naylor</surname> <given-names>G</given-names></name><name><surname>Phillips</surname> <given-names>NA</given-names></name><name><surname>Richter</surname> <given-names>M</given-names></name><name><surname>Rudner</surname> <given-names>M</given-names></name><name><surname>Sommers</surname> <given-names>MS</given-names></name><name><surname>Tremblay</surname> <given-names>KL</given-names></name><name><surname>Wingfield</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hearing impairment and cognitive energy: the framework for understanding effortful listening (FUEL)</article-title><source>Ear and Hearing</source><volume>37 Suppl 1</volume><fpage>5S</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000312</pub-id><pub-id pub-id-type="pmid">27355771</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Presacco</surname> <given-names>A</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Anderson</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Effect of informational content of noise on speech representation in the aging midbrain and cortex</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>2356</fpage><lpage>2367</lpage><pub-id pub-id-type="doi">10.1152/jn.00373.2016</pub-id><pub-id pub-id-type="pmid">27605531</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname> <given-names>MK</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Effects of simulated cochlear-implant processing on speech reception in fluctuating maskers</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>446</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1121/1.1579009</pub-id><pub-id pub-id-type="pmid">12880055</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname> <given-names>B</given-names></name><name><surname>Fujioka</surname> <given-names>T</given-names></name><name><surname>Tremblay</surname> <given-names>KL</given-names></name><name><surname>Picton</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Aging in binaural hearing begins in mid-life: evidence from cortical auditory-evoked responses to changes in interaural phase</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>11172</fpage><lpage>11178</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1813-07.2007</pub-id><pub-id pub-id-type="pmid">17942712</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruggles</surname> <given-names>D</given-names></name><name><surname>Bharadwaj</surname> <given-names>H</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normal hearing is not enough to guarantee robust encoding of suprathreshold features important in everyday communication</article-title><source>PNAS</source><volume>108</volume><fpage>15516</fpage><lpage>15521</lpage><pub-id pub-id-type="doi">10.1073/pnas.1108912108</pub-id><pub-id pub-id-type="pmid">21844339</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruggles</surname> <given-names>D</given-names></name><name><surname>Bharadwaj</surname> <given-names>H</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Why Middle-Aged listeners have trouble hearing in Everyday Settings</article-title><source>Current Biology</source><volume>22</volume><fpage>1417</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.05.025</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarro</surname> <given-names>EC</given-names></name><name><surname>Kotak</surname> <given-names>VC</given-names></name><name><surname>Sanes</surname> <given-names>DH</given-names></name><name><surname>Aoki</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Hearing Loss Alters the Subcellular Distribution of Presynaptic GAD and Postsynaptic GABAA Receptors in the Auditory Cortex</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>2855</fpage><lpage>2867</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn044</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sek</surname> <given-names>A</given-names></name><name><surname>Moore</surname> <given-names>BCJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Frequency discrimination as a function of frequency, measured in several ways</article-title><source>The Journal of the Acoustical Society of America</source><volume>97</volume><fpage>2479</fpage><lpage>2486</lpage><pub-id pub-id-type="doi">10.1121/1.411968</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaheen</surname> <given-names>LA</given-names></name><name><surname>Valero</surname> <given-names>MD</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Towards a diagnosis of cochlear neuropathy with envelope following responses</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>16</volume><fpage>727</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1007/s10162-015-0539-3</pub-id><pub-id pub-id-type="pmid">26323349</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname> <given-names>SA</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name><name><surname>Micheyl</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Temporal coherence and attention in auditory scene analysis</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>114</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.11.002</pub-id><pub-id pub-id-type="pmid">21196054</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>RV</given-names></name><name><surname>Zeng</surname> <given-names>FG</given-names></name><name><surname>Kamath</surname> <given-names>V</given-names></name><name><surname>Wygonski</surname> <given-names>J</given-names></name><name><surname>Ekelid</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shera</surname> <given-names>CA</given-names></name><name><surname>Zweig</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Noninvasive measurement of the cochlear traveling-wave ratio</article-title><source>The Journal of the Acoustical Society of America</source><volume>93</volume><fpage>3333</fpage><lpage>3352</lpage><pub-id pub-id-type="doi">10.1121/1.405717</pub-id><pub-id pub-id-type="pmid">8326061</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname> <given-names>B</given-names></name><name><surname>Best</surname> <given-names>V</given-names></name><name><surname>Akc</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Auditory Object Formation and Selection</chapter-title><person-group person-group-type="editor"><name><surname>Middlebrooks</surname> <given-names>JC</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Popper</surname> <given-names>AN</given-names></name><name><surname>Fay</surname> <given-names>RR</given-names></name></person-group><source>Auditory System at the Cocktail Party</source><publisher-name>Springer Handbook of Auditory Research</publisher-name><fpage>7</fpage><lpage>40</lpage></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname> <given-names>K</given-names></name><name><surname>Meng</surname> <given-names>M</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Zhou</surname> <given-names>K</given-names></name><name><surname>Luo</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Behavioral oscillations in attention: rhythmic α pulses mediated through θ band</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4837</fpage><lpage>4844</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4856-13.2014</pub-id><pub-id pub-id-type="pmid">24695703</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strelcyk</surname> <given-names>O</given-names></name><name><surname>Dau</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Relations between frequency selectivity, temporal fine-structure processing, and speech reception in impaired hearing</article-title><source>The Journal of the Acoustical Society of America</source><volume>125</volume><fpage>3328</fpage><lpage>3345</lpage><pub-id pub-id-type="doi">10.1121/1.3097469</pub-id><pub-id pub-id-type="pmid">19425674</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teki</surname> <given-names>S</given-names></name><name><surname>Chait</surname> <given-names>M</given-names></name><name><surname>Kumar</surname> <given-names>S</given-names></name><name><surname>Shamma</surname> <given-names>S</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Segregation of complex acoustic scenes based on temporal coherence</article-title><source>eLife</source><volume>2</volume><elocation-id>e00699</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00699</pub-id><pub-id pub-id-type="pmid">23898398</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Undurraga</surname> <given-names>JA</given-names></name><name><surname>Haywood</surname> <given-names>NR</given-names></name><name><surname>Marquardt</surname> <given-names>T</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural representation of interaural time differences in Humans-an objective measure that matches behavioural performance</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>17</volume><fpage>591</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1007/s10162-016-0584-6</pub-id><pub-id pub-id-type="pmid">27628539</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valero</surname> <given-names>MD</given-names></name><name><surname>Burton</surname> <given-names>JA</given-names></name><name><surname>Hauser</surname> <given-names>SN</given-names></name><name><surname>Hackett</surname> <given-names>TA</given-names></name><name><surname>Ramachandran</surname> <given-names>R</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Noise-induced cochlear synaptopathy in rhesus monkeys (Macaca mulatta)</article-title><source>Hearing Research</source><volume>353</volume><fpage>213</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.07.003</pub-id><pub-id pub-id-type="pmid">28712672</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viana</surname> <given-names>LM</given-names></name><name><surname>O'Malley</surname> <given-names>JT</given-names></name><name><surname>Burgess</surname> <given-names>BJ</given-names></name><name><surname>Jones</surname> <given-names>DD</given-names></name><name><surname>Oliveira</surname> <given-names>CACP</given-names></name><name><surname>Santos</surname> <given-names>F</given-names></name><name><surname>Merchant</surname> <given-names>SN</given-names></name><name><surname>Liberman</surname> <given-names>LD</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cochlear neuropathy in human presbycusis: Confocal analysis of hidden hearing loss in post-mortem tissue</article-title><source>Hearing Research</source><volume>327</volume><fpage>78</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2015.04.014</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallaert</surname> <given-names>N</given-names></name><name><surname>Varnet</surname> <given-names>L</given-names></name><name><surname>Moore</surname> <given-names>BCJ</given-names></name><name><surname>Lorenzi</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sensorineural hearing loss impairs sensitivity but spares temporal integration for detection of frequency modulation</article-title><source>The Journal of the Acoustical Society of America</source><volume>144</volume><fpage>720</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1121/1.5049364</pub-id><pub-id pub-id-type="pmid">30180712</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Naylor</surname> <given-names>G</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Wendt</surname> <given-names>D</given-names></name><name><surname>Ohlenforst</surname> <given-names>B</given-names></name><name><surname>Lunner</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Relations Between Self-Reported Daily-Life Fatigue, Hearing Status, and Pupil Dilation During a Speech Perception in Noise Task</article-title><source>Ear and Hearing</source><volume>39</volume><fpage>573</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000512</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiteford</surname> <given-names>KL</given-names></name><name><surname>Kreft</surname> <given-names>HA</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Assessing the role of place and timing cues in coding frequency and amplitude modulation as a function of age</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>18</volume><fpage>619</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1007/s10162-017-0624-x</pub-id><pub-id pub-id-type="pmid">28429126</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiteford</surname> <given-names>KL</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Using individual differences to test the role of temporal and place cues in coding frequency modulation</article-title><source>The Journal of the Acoustical Society of America</source><volume>138</volume><fpage>3093</fpage><lpage>3104</lpage><pub-id pub-id-type="doi">10.1121/1.4935018</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitton</surname> <given-names>JP</given-names></name><name><surname>Hancock</surname> <given-names>KE</given-names></name><name><surname>Shannon</surname> <given-names>JM</given-names></name><name><surname>Polley</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Audiomotor perceptual training enhances speech intelligibility in background noise</article-title><source>Current Biology</source><volume>27</volume><fpage>3237</fpage><lpage>3247</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.09.014</pub-id><pub-id pub-id-type="pmid">29056453</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wild</surname> <given-names>CJ</given-names></name><name><surname>Yusuf</surname> <given-names>A</given-names></name><name><surname>Wilson</surname> <given-names>DE</given-names></name><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effortful listening: the processing of degraded speech depends critically on attention</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>14010</fpage><lpage>14021</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1528-12.2012</pub-id><pub-id pub-id-type="pmid">23035108</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winn</surname> <given-names>MB</given-names></name><name><surname>Edwards</surname> <given-names>JR</given-names></name><name><surname>Litovsky</surname> <given-names>RY</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The impact of auditory spectral resolution on listening effort revealed by pupil dilation</article-title><source>Ear and Hearing</source><volume>36</volume><fpage>e153</fpage><lpage>e165</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000145</pub-id><pub-id pub-id-type="pmid">25654299</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>PZ</given-names></name><name><surname>Liberman</surname> <given-names>LD</given-names></name><name><surname>Bennett</surname> <given-names>K</given-names></name><name><surname>de Gruttola</surname> <given-names>V</given-names></name><name><surname>O'Malley</surname> <given-names>JT</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Primary neural degeneration in the human cochlea: evidence for hidden hearing loss in the aging ear</article-title><source>Neuroscience</source><volume>407</volume><fpage>8</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.07.053</pub-id><pub-id pub-id-type="pmid">30099118</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name><name><surname>Festen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Pupil response as an indication of effortful listening: the influence of sentence intelligibility</article-title><source>Ear and Hearing</source><volume>31</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181d4f251</pub-id><pub-id pub-id-type="pmid">20588118</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Heslenfeld</surname> <given-names>DJ</given-names></name><name><surname>Johnsrude</surname> <given-names>IS</given-names></name><name><surname>Versfeld</surname> <given-names>NJ</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The eye as a window to the listening brain: neural correlates of pupil size as a measure of cognitive listening load</article-title><source>NeuroImage</source><volume>101</volume><fpage>76</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.06.069</pub-id><pub-id pub-id-type="pmid">24999040</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Koelewijn</surname> <given-names>T</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The pupil dilation response to auditory stimuli: current state of knowledge</article-title><source>Trends in Hearing</source><volume>22</volume><elocation-id>233121651877717</elocation-id><pub-id pub-id-type="doi">10.1177/2331216518777174</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname> <given-names>AA</given-names></name><name><surname>Kramer</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cognitive processing load across a wide range of listening conditions: insights from pupillometry</article-title><source>Psychophysiology</source><volume>51</volume><fpage>277</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1111/psyp.12151</pub-id><pub-id pub-id-type="pmid">24506437</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname> <given-names>FG</given-names></name><name><surname>Nie</surname> <given-names>K</given-names></name><name><surname>Stickney</surname> <given-names>GS</given-names></name><name><surname>Kong</surname> <given-names>YY</given-names></name><name><surname>Vongphoe</surname> <given-names>M</given-names></name><name><surname>Bhargave</surname> <given-names>A</given-names></name><name><surname>Wei</surname> <given-names>C</given-names></name><name><surname>Cao</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Speech recognition with amplitude and frequency modulations</article-title><source>PNAS</source><volume>102</volume><fpage>2293</fpage><lpage>2298</lpage><pub-id pub-id-type="doi">10.1073/pnas.0406460102</pub-id><pub-id pub-id-type="pmid">15677723</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51419.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution>Peking University</institution><country>China</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Zeng</surname><given-names>Fan-Gang</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Lorenzi</surname><given-names>Christian</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This work tackles an important and interesting issue, that is, why subjects diagnosed with normal hearing show difficulty in speech recognition, by systematically assessing the behavioral and physiological measures of both low- (FM tracking) and high-level (pupil response) auditory processing. The study identifies an efficient and robust protocol that could account for large variability (to around 85%) in speech-in-noise performance and thus provides a promising mechanistic interpretation for the &quot;hidden hearing loss&quot; group. Importantly, based on pupil dynamics and ear canal EEG to non-speech FM sounds, they could largely predict the somewhat complex multi-talker speech tracking ability in human subjects. The findings will be of major interest to broad readership, including auditory neuroscientists, clinicians, hearing-aid and cochlear-implant manufactures, and computer scientists who might use AI to diagnose hidden hearing loss.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neural signatures of disordered multi-talker speech perception in adults with normal hearing&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Fan-Gang Zeng (Reviewer #2); Christian Lorenzi (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This work addresses an important and timely issue, that is, why subjects diagnosed with normal hearing show difficulty in speech recognition(&quot;hidden hearing loss&quot;). The study systematically examined peripheral and central performance and identified a combination of behavioral and physiological measures of low- and high-level auditory processing that could largely account for speech-in-noise performance in the &quot;hidden hearing loss&quot; group. All the reviewers agree that it is a timely and carefully performed study and would also have great impacts in a wide range of audience.</p><p>Essential revisions:</p><p>1) An important implication of the study is to possibly separate the bottom-up and top-down factors that contribute to speech recognition difficulty on a subject-by-subject basis. Could the authors examine the relative ratio of the two factors, e.g., some subjects show more bottom-up contribution whereas other show more top-down limitations?</p><p>2) Missing link between the 19% of the 106,787 patients (subsection “Many individuals seek medical care for poor hearing but have no evidence of hearing loss”) and the 23 subjects (subsection “Speech-in-noise intelligibility varies widely in individuals with clinically normal hearing”, first paragraph) who participated in the present study. At first, the reviewers thought these 23 subjects were part of those who complained about hearing difficulty but had normal audiograms, but nowhere can the reviewers find any info to confirm this connection and in fact, the 23 subjects seemed to be independently recruited to meet the audiogram, age, and other criteria. If it was true that those 23 subjects didn't seek any medical intervention because of hearing difficulty, then this fact needs to be explicitly spelled out.</p><p>3) The reviewers would strongly encourage the authors to discuss their FMFR data and methods in relationship with a previous study by Gockel et al., 2015, suggesting that FFR for low-frequency pure tones at medium to high levels mainly originates from neurons tuned to higher frequencies.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.51419.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) An important implication of the study is to possibly separate the bottom-up and top-down factors that contribute to speech recognition difficulty on a subject-by-subject basis. Could the authors examine the relative ratio of the two factors, e.g., some subjects show more bottom-up contribution whereas other show more top-down limitations?</p></disp-quote><p>We completely agree; it would be important to understand how bottom-up and top-down resources are leveraged to process low SNR speech, whether their benefits are interchangeable, or whether instead their benefits are complementary and are fully realized when they are both applied together. While this is a question of great importance, it wasn’t the question we set out to answer so one major caveat is that our study design was not powered to fully explore individual strategies. As such, the conclusions we offer below might be better considered as suggestions that could be addressed in a separate study designed with this purpose in mind.</p><p>To the reviewers’ question, no, it does not seem that our proxies for bottom-up (FMFR) and top-down (pupil) are interchangeable, such that a subject with strong bottom-up would have weak top-down, or vice versa. This conclusion is based on the following observations:</p><p>– No significant correlations are observed between the FMFR and the pupil measures (Figure 5A). They both make contributions towards explained variance in the multivariate model, but the relationship is more complex than a simple tradeoff, where one is strong while the other is weak.</p><p>– To address the relationship of each measure with multi-talker speech recognition, we plotted the FMFR and the pupil diameter for each subject and color coded the symbol to represent their speech intelligibility, with darker colors indicated better performance on the digits task. No systematic differences were noted, though the better performers tended to cluster to the bottom left, indicating lesser effort and better FMFRs (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>, left).</p><p>– To label individual subjects according to how they weighted top-down processing relative to bottom-up processing, we sorted the FMFR and pupil values for each subject by ordinal position. We then calculated an ordinal position index defined as (OP<sub>pupil</sub> – OP<sub>FMFR</sub>)/(OP<sub>pupil</sub> + OP<sub>FMFR</sub>). An OP index &gt;0 would identifies subjects that rank higher in their utilization of top-down than bottom-up measures, an OP index &lt;0 would indicate a relatively greater emphasis on bottom-up resources, and an index of 0 would indicate a balanced relative utilization of each. We then plotted this OP index as a function of the performance on the digits task but did not find any significant linear correlations. We did find that the subjects with the best multi-talker speech recognition thresholds tended to have stronger bias – in either direction – for utilization of either top-down or bottom-up processing. Subjects with poor speech recognition thresholds all had OP indices closer to 0. (<xref ref-type="fig" rid="respfig1">Author response image 1</xref>, right).</p><p>– Finally, we looked at the multivariate model for the physiological predictors of speech, and plotted the residuals of the model fit. While the residuals cannot make a determination on an individual basis of reliance on bottom up or top down factors, it can identify outliers to the model, i.e. individuals who have an over-reliance on one cue or the other. Only one such outlier was found, and removing this outlier increased the adjusted R2 value of the multivariate model from 0.52 to 0.71. These results suggest that both bottom up and top down factors were equally important in determining the model fit.</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Relationship between bottom-up and top-down measures and their contributions to multi-talker speech intelligibility.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-resp-fig1-v1.tif"/></fig><p>We have modified the Discussion to address these points:</p><p>“Our findings suggest that the individuals who struggle most to follow conversations in noisy, social settings might be identified both by poor bottom-up processing of rapid temporal cues in speech and also by an over-utilization of top-down active listening resources. […] In the context of human speech perception, this question would be best tackled by an approach that more explicitly identified the implementation of cognitive active listening mechanisms and was statistically powered to address the question of individual differences (Jasmin et al., 2019; Lim et al., 2019; Michalka et al., 2015).”</p><disp-quote content-type="editor-comment"><p>2) Missing link between the 19% of the 106,787 patients (subsection “Many individuals seek medical care for poor hearing but have no evidence of hearing loss”) and the 23 subjects (subsection “Speech-in-noise intelligibility varies widely in individuals with clinically normal hearing”, first paragraph) who participated in the present study. At first, the reviewers thought these 23 subjects were part of those who complained about hearing difficulty but had normal audiograms, but nowhere can the reviewers find any info to confirm this connection and in fact, the 23 subjects seemed to be independently recruited to meet the audiogram, age, and other criteria. If it was true that those 23 subjects didn't seek any medical intervention because of hearing difficulty, then this fact needs to be explicitly spelled out.</p></disp-quote><p>We thank the reviewers for identifying something in our original manuscript that may have caused confusion among our readers. We recruited subjects that matched the hearing profiles of subjects in the database, but we did not contact the former patients to be in our study. The clinical audiological assessment of these 23 subjects matched the distribution of our clinical sample and the SSQ questionnaire also identified communication difficulties, particularly in related to multi-talker speech intelligibility showed the greatest variability. For these reasons, the 23 subjects we tested were representative of the sample described from the audiology database. To make this explicit, the revised manuscript has been modified as follows:</p><p>“To better understand the nature of their suprathreshold hearing problems, we recruited 23 young or middle-aged listeners (mean age: 28.3+0.9 years) that matched the clinically normal hearing from the database profile (Figure 1—figure supplement 2A). […] Like the patients from the clinical database, the audiograms from these subjects were clinically normal, yet many reported difficulties with speech intelligibility, particularly in listening conditions with multiple overlapping speakers (Figure 1—figure supplement 4A).”</p><disp-quote content-type="editor-comment"><p>3) The reviewers would strongly encourage the authors to discuss their FMFR data and methods in relationship with a previous study by Gockel et al., 2015, suggesting that FFR for low-frequency pure tones at medium to high levels mainly originates from neurons tuned to higher frequencies.</p></disp-quote><p>We consider it unlikely that the FMFR signal arises mainly from neurons tuned to higher frequencies based on data shown in <xref ref-type="fig" rid="respfig2">Author response image 2</xref> from pilot experiments on 5 subjects, in which the 500 Hz FMFR was measured with simultaneous noise masking. Whereas the 500 Hz FMFR is totally eliminated with broadband noise (BB Noise) that includes the 500 Hz probe, high-pass (HP) noise ranging from either 640Hz to 25 kHz, 2560 Hz to 25 kHz or 10240 Hz to 25 kHz only moderately attenuated the strength of the following response. This finding suggests that some fraction of the generators may come from high-frequency areas, but that tonotopically aligned low-frequency generators still account for most of the measured signal.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Tonotopic contributions to the FMFR revealed using high-pass masking noise.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-51419-resp-fig2-v1.tif"/></fig><p>The Discussion has been modified to include these points:</p><p>“The spatial distribution of neural generators for the FMFR also deserves additional study, as some off-channel higher frequency neurons may be combined with low-frequency tonotopically aligned neurons (Gockel et al., 2015; Parthasarathy et al., 2016).”</p></body></sub-article></article>