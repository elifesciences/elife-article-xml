<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98351</article-id><article-id pub-id-type="doi">10.7554/eLife.98351</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98351.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>An image-computable model of speeded decision-making</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Jaffe</surname><given-names>Paul I</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0680-3923</contrib-id><email>pijaffe@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Santiago-Reyes</surname><given-names>Gustavo X</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Schafer</surname><given-names>Robert J</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Bissett</surname><given-names>Patrick G</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0854-9404</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Poldrack</surname><given-names>Russell A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Bioengineering, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gxkhe25</institution-id><institution>Lumos Labs</institution></institution-wrap><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP98351</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-11"><day>11</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-03-25"><day>25</day><month>03</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2403.16382"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-13"><day>13</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98351.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-29"><day>29</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98351.2"/></event></pub-history><permissions><copyright-statement>Â© 2024, Jaffe et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Jaffe et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98351-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98351-figures-v1.pdf"/><abstract><p>Evidence accumulation models (EAMs) are the dominant framework for modeling response time (RT) data from speeded decision-making tasks. While providing a good quantitative description of RT data in terms of abstract perceptual representations, EAMs do not explain how the visual system extracts these representations in the first place. To address this limitation, we introduce the visual accumulator model (VAM), in which convolutional neural network models of visual processing and traditional EAMs are jointly fitted to trial-level RTs and raw (pixel-space) visual stimuli from individual subjects in a unified Bayesian framework. Models fitted to large-scale cognitive training data from a stylized flanker task captured individual differences in congruency effects, RTs, and accuracy. We find evidence that the selection of task-relevant information occurs through the orthogonalization of relevant and irrelevant representations, demonstrating how our framework can be used to relate visual representations to behavioral outputs. Together, our work provides a probabilistic framework for both constraining neural network models of vision with behavioral data and studying how the visual system extracts representations that guide decisions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual processing</kwd><kwd>decision-making</kwd><kwd>neural network modelling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Combining biologically-plausible neural network models of vision with traditional decision-making models enables a detailed characterization of how the visual system extracts representations that guide decisions from raw sensory inputs.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Decision-making under time pressure is deeply embedded within the activities of daily life. To study the cognitive and neural processes underlying decision-making, psychologists fit computational models to response time (RT) data gathered from relatively simple cognitive tasks. Evidence accumulation models (EAMs), such as the diffusion-decision model (<xref ref-type="bibr" rid="bib69">Ratcliff and McKoon, 2008</xref>; <xref ref-type="bibr" rid="bib66">Ratcliff, 1978</xref>) and linear ballistic accumulator model (LBA) (<xref ref-type="bibr" rid="bib9">Brown and Heathcote, 2008</xref>), are the most successful and widely-used computational models of joint RT and choice data from decision-making tasks. In the EAM framework, sensory evidence is accumulated (possibly with noise) until reaching a threshold, at which point an overt response is generated. As such, decision-making is described in terms of a small number of interpretable parameters that capture basic cognitive processes. Empirically, EAMs have been shown to capture RT distributions from a variety of cognitive tasks at the individual subject level (<xref ref-type="bibr" rid="bib9">Brown and Heathcote, 2008</xref>; <xref ref-type="bibr" rid="bib66">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib91">Usher and McClelland, 2001</xref>; <xref ref-type="bibr" rid="bib67">Ratcliff and Rouder, 1998</xref>).</p><p>Despite this empirical success and theoretical merit, the simple characterization of decision-making encapsulated by EAMs results in somewhat restrictive applications (<xref ref-type="bibr" rid="bib20">Evans and Wagenmakers, 2020</xref>). In the context of visual tasks, EAMs do not typically provide a detailed specification of how raw visual stimuli are transformed into evidence and consequently do not explain the visual processing steps underlying decision-making. A number of recent modeling efforts have addressed this limitation by adapting convolutional neural network (CNN) models used in image classification tasks to the novel purpose of generating RTs or RT proxies (<xref ref-type="bibr" rid="bib81">Spoerer et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Taylor et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Kumbhar et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Rafiei et al., 2024</xref>; <xref ref-type="bibr" rid="bib25">Goetschalckx et al., 2023</xref>; <xref ref-type="bibr" rid="bib1">Annis et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Holmes et al., 2020</xref>; <xref ref-type="bibr" rid="bib88">Trueblood et al., 2021</xref>), a strategy we also pursue here. CNNs are useful in this regard since they are image-computableâthey accept arbitrary images as inputâand capture important characteristics of biological vision (<xref ref-type="bibr" rid="bib48">Lindsay, 2021</xref>; <xref ref-type="bibr" rid="bib97">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib44">Kriegeskorte, 2015</xref>). Early CNN layers exhibit spatially-organized units with local receptive fields, analogous to the retinotopic organization of the early mammalian visual pathway. The concatenation of multiple layers forms a hierarchy in which progressively more abstract features are extracted in deeper layers, in a way that is globally similar to the primate visual cortical hierarchy.</p><p>Here, we integrate CNN models for visual feature extraction with traditional EAMs of speeded decision-making tasks in a framework we call the visual accumulator model (VAM). As in the prior models referenced above, the VAM accepts raw (pixel-space) visual stimuli as inputs and generates RTs and choices as outputs. The key feature of the VAM that distinguishes it from prior models is that the CNN and EAM parameters are <italic>jointly fitted</italic> to the RT, choice, and visual stimulus data from individual participants in a unified Bayesian framework. Thus, both the visual representations learned by the CNN and the EAM parameters are directly constrained by behavioral data. In contrast, prior models first optimize the CNN to perform the behavioral task, then separately fit a minimal set of high-level CNN parameters (<xref ref-type="bibr" rid="bib63">Rafiei et al., 2024</xref>) and/or the EAM parameters to behavioral data (<xref ref-type="bibr" rid="bib1">Annis et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Holmes et al., 2020</xref>; <xref ref-type="bibr" rid="bib88">Trueblood et al., 2021</xref>). As we will show, fitting the CNN with human dataârather than optimizing the model to perform a taskâhas significant consequences for the representations learned by the model.</p><p>We leverage the VAM to explore how abstract, task-relevant information is extracted from raw sensory inputs, and we investigate how the behavioral phenomenon of congruency effects arises as a consequence of the representation geometry learned by the CNN. In doing so, our framework also addresses one of the main criticisms of deep neural network models of vision that are optimized to perform particular tasks (e.g. object identification): these models do not account for many results from psychology (<xref ref-type="bibr" rid="bib3">Baker et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Bowers et al., 2022</xref>; <xref ref-type="bibr" rid="bib21">Fel et al., 2022</xref>; <xref ref-type="bibr" rid="bib48">Lindsay, 2021</xref>; <xref ref-type="bibr" rid="bib51">Malhotra et al., 2022</xref>).</p><sec id="s1-1"><title>Modeling framework</title><p>We first describe the taskâLost in Migration (LIM), available as part of the Lumosity cognitive training platformâwhich will serve to ground the discussion of the model. LIM is a stylized version of the well-known arrows flanker task used in the study of cognitive control and visual attention (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib83">Stoffels and van der Molen, 1988</xref>). In LIM, participants are shown stimuli composed of several arrow-shaped birds. The task is to indicate the direction of the central bird (the target) using the arrow keys on the keyboard, ignoring the direction of the surrounding birds (the flankers). Participants engage with the task at home and are rewarded via a composite score that takes into account both speed and accuracy.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task and model.</title><p>(<bold>A</bold>) Top, Lost in Migration task. Bottom, the seven stimulus layouts (random target/flanker directions). (<bold>B</bold>) Visual accumulator model (VAM) schematic. The numbers after the convolutional neural network (CNN) layer names correspond to the number of channels used in that layer. See Methods for additional details.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig1-v1.tif"/></fig><p>The stimuli used in LIM vary along several dimensions that are not present in the standard flanker task, implying potentially complicated stimulus-behavior dependencies that are well-suited to the VAM. First, both targets and flankers can be independently oriented left, right, up, or down, such that there are four possible response directions (the standard arrow flanker task allows for only left and right responses). Second, the layout of the targets and the flankers can appear in one of seven configurations: left/right/up/down âV,â horizontal/vertical line, and cross (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Last, the target can be centered anywhere within the 640Ã480 pixel game window, subject to edge constraints.</p><p>As with other flanker tasks, a given trial is said to be congruent if targets and flankers are oriented in the same direction, and incongruent otherwise. A consistent observation from studies employing the flanker task and related conflict tasks are <italic>congruency effects</italic>: participants are slower and less accurate on incongruent trials (<xref ref-type="bibr" rid="bib19">Eriksen and Eriksen, 1974</xref>; <xref ref-type="bibr" rid="bib79">Simon, 1982</xref>; <xref ref-type="bibr" rid="bib84">Stroop, 1935</xref>). Congruency effects are considered to index a specific aspect of cognitive control: they indicate the extent to which an individual can selectively attend to task-relevant information and ignore irrelevant information.</p><p>The visual accumulator is an image-computable EAM, composed of a CNN and EAM chained together (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Minimally processed (pixel-space) visual stimuli are provided as inputs to the CNN. The outputs of the CNN correspond to the mean rates at which evidence is accumulated (the drift rates), one for each possible response (four in the case of LIM). The EAM then generates choices and RTs through a noisy evidence accumulation process. For each participant in the dataset, we fit one such model (CNN + EAM) jointly using that participantâs visual stimuli, RTs, and choices. Building on prior work (<xref ref-type="bibr" rid="bib12">Dao et al., 2024</xref>; <xref ref-type="bibr" rid="bib45">Kucukelbir et al., 2017</xref>), we developed an automatic differentiation variational inference (ADVI) algorithm that simultaneously optimizes the parameters of the CNN and learns the posterior distribution over the LBA parameters (Methods).</p><p>The particular EAM we adopt is the linear ballistic accumulator model (LBA <xref ref-type="bibr" rid="bib9">Brown and Heathcote, 2008</xref>; <xref ref-type="fig" rid="fig1">Figure 1B</xref>) since it can be applied to tasks with more than two possible responses, though the general VAM framework is compatible with other EAMs that have a closed-form or easily approximated likelihood. The LBA parameters fitted in the VAM are the decision threshold <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the non-decision time <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a parameter <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that controls the dispersion of the initial accumulator values (the drift rate means are fitted implicitly via the CNN). We used a seven layer CNN architecture (six convolutional layers, one fully-connected layer) in the VAM (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To speed up the training process, the first two convolutional layers were initialized with the parameters from a 16-layer VGG CNN trained to classify the ImageNet dataset (<xref ref-type="bibr" rid="bib14">Deng et al., 2009</xref>; <xref ref-type="bibr" rid="bib80">Simonyan and Zisserman, 2015</xref>).</p><p>All of the code (<ext-link ext-link-type="uri" xlink:href="https://github.com/pauljaffe/vam">https://github.com/pauljaffe/vam</ext-link>, copy archived at <xref ref-type="bibr" rid="bib35">Jaffe, 2025</xref>) and data (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.10775513">https://doi.org/10.5281/zenodo.10775513</ext-link>) used to train the VAMs and reproduce our results are publicly available.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Models capture human behavioral data</title><p>We fitted a separate VAM to LIM data (visual stimuli, RTs, choices) from each of 75 Lumosity users (participants). We selected participants who had practiced Lost in Migration extensively (â¥25,000 trials) and used data from a later stage of practice to minimize learning effects (â¥ each participantâs 50<sup>th</sup> gameplay). These participants varied in age (23â87 y) and in their behavior (mean RT, accuracy, congruency effects), allowing us to examine how well our framework captured individual differences.</p><p>To assess the model fits, we compared the mean RT, accuracy, RT congruency effect (incongruent trial mean RT minus congruent trial mean RT), and accuracy congruency effect (congruent trial accuracy minus incongruent trial accuracy) of each model/participant pair on a set of holdout stimuli, separate from those used to train the models (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). For each behavioral summary statistic, the responses of the fitted models were highly correlated with those of the participants (Pearsonâs <italic>r</italic> &gt; 0.75), with slopes close to unity (<xref ref-type="fig" rid="fig2">Figure 2BâE</xref>, statistics in figure legend). We found that the RT congruency effect could be attributed to a reduction in the mean target drift rate parameter on incongruent vs. congruent trials, while the accuracy congruency effect could be attributed to a higher mean flanker drift rate on incongruent trials relative to the non-target (other) mean drift rates on congruent trials (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). The latter observation follows from the fact that the drift rates on trial <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are sampled from <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the mean drift rates shown in <xref ref-type="fig" rid="fig2">Figure 2F</xref>. Since the flanker drift rates on incongruent trials are higher (less negative) than the non-target drift rates on congruent trials, more errors result on incongruent trials, giving rise to the accuracy congruency effect.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Comparison of model/participant behavior.</title><p>For panels <bold>BâE</bold>, each point is one model/participant (n=75), black line: unity, red line: linear best fit. (<bold>A</bold>) Example model/participant response time (RT) distributions. (<bold>B</bold>) Mean RT (Pearsonâs <italic>r</italic> = 0.99, bootstrap 95% CI = (0.99, 0.99), best fit slope = 1.07). (<bold>C</bold>) Accuracy (<italic>r</italic>=0.91, 95% CI = (0.87, 0.94), slope = 1.15). (<bold>D</bold>) RT congruency effect (<italic>r</italic>=0.77, 95% CI = (0.67, 0.86), slope = 1.01). (<bold>E</bold>) Accuracy congruency effect (<italic>r</italic>=0.92, 95% CI = (0.88, 0.94), slope = 1.20). (<bold>F</bold>) Drift rates averaged across all trials and models. (<bold>G</bold>) Mean RT vs. age averaged across models. (<bold>H</bold>) Example model/participant mean RT vs. stimulus layout (Pearsonâs <italic>r</italic> = 0.67). (<bold>I</bold>) Example model/participant mean RT vs. horizontal stimulus position (negative values: left of center; Pearsonâs <italic>r</italic> = 0.79). (<bold>J</bold>) Empirical CDF of Pearsonâs <italic>r</italic> between model/participant mean RTs across stimulus feature bins (only participants with significant RT modulation are shown; layout: <italic>n</italic> = 60 models/participants, x-position: <italic>n</italic> = 72, y-position: <italic>n</italic> = 69). Error bars in panels (<bold>F-I</bold>) are bootstrap 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Example model/participant response time (RT) distributions and dependence of RTs on stimulus features.</title><p>(<bold>A</bold>) Example model/participant RT distributions (all trials). (<bold>B</bold>) Examples of model/participant mean RT vs. stimulus layout. (<bold>C</bold>) Examples of model/participant mean RT vs. horizontal stimulus position (negative values: left of center). (<bold>D</bold>) Examples of model/participant mean RT vs. vertical stimulus position (negative values: above center). For all panels, error bars correspond to bootstrap 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>Age dependence of linear ballistic accumulator (LBA) parameters.</title><p>For all panels, we tested age-dependence with a one-way ANOVA and report Bonferroni-adjusted <italic>p</italic>-values, corrected for four comparisons (<italic>n</italic>=75 models). We also report adjusted <italic>p</italic>-values from a post-hoc comparison of the 20â29 vs. 70â89 age groups conducted with Tukeyâs HSD. Error bars correspond to bootstrap 95% confidence intervals. (<bold>A</bold>) Non-decision time parameter <italic>t</italic><sub><italic>0</italic></sub> (<italic>F</italic>(5, 69)=13.3, <italic>p</italic> &lt; 1e-7). Tukeyâs HSD for 20â29 vs. 70â89 age groups: <italic>p</italic> &lt; 1e-8. (<bold>B</bold>) Response caution (<inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>â</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>); <italic>F</italic>(5, 69) = 0.49, <italic>p</italic> = 1.0. (<bold>C</bold>) Mean target drift rate (<italic>F</italic>(5, 69) = 3.4, <italic>p</italic> = 0.026). Tukeyâs HSD for 20â29 vs. 70â89 age groups: <italic>p</italic> = 0.002. (<bold>D</bold>) Mean flanker drift rate (<italic>F</italic>(5, 69) = 0.72, <italic>p</italic> = 1.0).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 3.</label><caption><title>Dependence of response times (RTs) on stimulus layout and position.</title><p>For each participant/model, we calculated the mean RT in each stimulus feature bin, then subtracted the average of these mean RTs from each bin. The panels show the average of these centered RTs across all participants with significant modulation of RT for that particular stimulus feature. For all panels, we conducted a one-way ANOVA for both models/participants and report Bonferroni-adjusted <italic>p</italic>-values, corrected for three comparisons (<italic>n</italic> = 75 models). We also report results from post-hoc comparisons between select feature bins conducted with Tukeyâs HSD. Error bars correspond to bootstrap 95% confidence intervals. (<bold>A</bold>) RT vs. stimulus layout (models: <italic>F</italic>(6, 53) = 7.43, <italic>p</italic> &lt; 1e-6, RTs for the vertical line layout were significantly faster (Tukeyâs HSD adjusted <italic>p</italic>-value &lt; 0.05) than RTs from all other layouts except â&gt;â; participants: <italic>F</italic>(6, 53) = 23.1, <italic>p</italic> &lt; 1e-22; RTs for the vertical line layout were significantly faster than RTs from all other layouts). (<bold>B</bold>) RT vs. horizontal stimulus position (negative values: left of center; models: <italic>F</italic>(7, 64) = 16.8, <italic>p</italic> &lt; 1e-18, RTs for the leftmost and rightmost position bins were significantly slower than RTs from all intermediate position bins; participants: <italic>F</italic>(7, 64) = 72.6, &lt;<italic>p</italic>1e-73; RTs for the leftmost and rightmost position bins were significantly slower than RTs from all intermediate position bins). (<bold>C</bold>) RT vs. vertical stimulus position (negative values: above center; models: <italic>F</italic>(5, 66)=17.2,p &lt; 1e-14, RTs for the topmost and bottommost position bins were significantly slower than RTs from the two centermost position bins; participants: <italic>F</italic>(5, 66)=113.3,&lt;<italic>p</italic>1e-74; RTs for the topmost and bottommost position bins were significantly slower than RTs from the two centermost position bins).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 4.</label><caption><title>Response time (RT) delta plots and conditional accuracy functions.</title><p>(<bold>A</bold>) RT delta plots for participants and visual accumulator models (VAMs) (<italic>n</italic> = 75 models/participants). (<bold>B</bold>) Conditional accuracy functions for participants and VAMs. For all panels, error bars correspond to bootstrap 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig2-figsupp4-v1.tif"/></fig></fig-group><p>We also examined whether the fitted models captured demographic effects, focusing on the well-established slowing of RTs that occurs with age (<xref ref-type="bibr" rid="bib26">Gottsdanker, 1982</xref>; <xref ref-type="bibr" rid="bib57">Nettelbeck and Rabbitt, 1992</xref>; <xref ref-type="bibr" rid="bib66">Ratcliff, 1978</xref>). Mean RTs began increasing around age 50, effects captured by the fitted models (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). Consistent with prior work in a variety of decision-making tasks (<xref ref-type="bibr" rid="bib23">Forstmann et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Ratcliff et al., 2001</xref>; <xref ref-type="bibr" rid="bib77">Servant and Evans, 2020</xref>; <xref ref-type="bibr" rid="bib82">Steyvers et al., 2019</xref>), we found that an age-dependent increase in the non-decision time (<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) component of the response contributed to longer RTs in the models from older adults (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). In contrast to these prior studies, we did not observe increased response caution (measured as <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>â</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) in the models from older adults (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>). This discrepancy could be explained by differences in the task design or the fact that our participants had practiced the task substantially more than in typical studies (<xref ref-type="bibr" rid="bib82">Steyvers et al., 2019</xref>). We also observed an age-dependent reduction in the target drift rates and no age-dependence of the flanker drift rates (<xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2</xref>), findings that have received mixed support in the literature (<xref ref-type="bibr" rid="bib4">Ben-David et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Forstmann et al., 2011</xref>; <xref ref-type="bibr" rid="bib66">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib77">Servant and Evans, 2020</xref>; <xref ref-type="bibr" rid="bib82">Steyvers et al., 2019</xref>).</p><p>One virtue of the VAM is that the model implicitly learns which stimulus properties influence behavior. As a simple demonstration of this, we investigated how two high-level visual features influenced participant behaviorâstimulus layout and both horizontal/vertical stimulus positionâand whether the fitted models captured these effects. We focused on RT effects, since no participants exhibited a significant effect of layout or horizontal/vertical position on accuracy (<italic>p</italic>&gt;0.05 for all stimulus features, chi-squared test), though we note that ceiling effects resulting from the high accuracy of the participants may have hindered our ability to detect these accuracy effects.</p><p>The majority of participants exhibited layout-dependent RT biases (<italic>p</italic>&lt;0.05 for 60/75 participants, one-way ANOVA; see examples in <xref ref-type="fig" rid="fig2">Figure 2H</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). We quantified how well the models captured these effects by calculating the Pearsonâs <italic>r</italic> between model/participant mean RTs across each layout for the participants that exhibited significant layout-dependent RT modulation (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). The median Pearsonâs <italic>r</italic> was 0.67, demonstrating good correspondence between model/participant behavior. There was considerable heterogeneity in the particular layout RT biases exhibited by both the participants and models, though responses to trials with the vertical line layout were on average somewhat faster than the other layouts for both participants and models (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplements 1</xref> and <xref ref-type="fig" rid="fig2s3">3</xref>).</p><p>The majority of participants also exhibited both horizontal and vertical position-dependent RT biases (horizontal position: <italic>p</italic>&lt;0.05 for 72/75 participants, vertical position: <italic>p</italic>&lt;0.05 for 69/75 participants, one-way ANOVA; see examples in <xref ref-type="fig" rid="fig2">Figure 2I</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). The fitted models captured these effects adequately: the median Pearsonâs <italic>r</italic> between model/participant mean RTs across stimulus position bins was 0.78 for horizontal position and 0.55 for vertical position (<xref ref-type="fig" rid="fig2">Figure 2J</xref>). While there was substantial variability across both participants and models in the particular position-dependent RT biases they exhibited, most participants/models responded more slowly when the stimuli were positioned close to the horizontal edges and, to a lesser extent, the vertical edges of the task window (<xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplements 1</xref> and <xref ref-type="fig" rid="fig2s3">3</xref>).</p><p>We also examined whether the VAMs captured two other commonly used behavioral metrics that take into account additional information from the RT distribution: RT delta plots and conditional accuracy functions (<xref ref-type="bibr" rid="bib13">De Jong et al., 1994</xref>; <xref ref-type="bibr" rid="bib71">Ridderinkhof, 2002</xref>; <xref ref-type="bibr" rid="bib89">Ulrich et al., 2015</xref>; <xref ref-type="bibr" rid="bib94">White et al., 2011</xref>; <xref ref-type="bibr" rid="bib92">van den Wildenberg et al., 2010</xref>). The participant RT delta plots showed that the congruency effect increased with longer RTs (<xref ref-type="bibr" rid="bib62">Pratte, 2021</xref>), a trend that was captured by the fitted VAMs (<xref ref-type="fig" rid="fig2s4">Figure 2âfigure supplement 4</xref>). In contrast, we observed a mismatch between model and participant behavior for the conditional accuracy functions on incongruent trials (<xref ref-type="fig" rid="fig2s4">Figure 2âfigure supplement 4</xref>). In particular, the participants tended to be less accurate on faster trials, while the models exhibited the opposite trend. This discrepancy may be explained by the lack of dynamic visual processing in the VAM, which has been proposed to account for the preponderance of errors on faster trials often observed in conflict tasks (<xref ref-type="bibr" rid="bib94">White et al., 2011</xref>; <xref ref-type="bibr" rid="bib92">van den Wildenberg et al., 2010</xref>). In the remainder of our analyses, we focus on the mean congruency effect, leaving a full accounting of such dynamic error patterns for future modeling work.</p><p>In summary, the VAM captured individual differences in RTs, accuracy, and congruency effects, and the dependence of RTs on stimulus layout and position. To lay the groundwork for understanding how the learned visual representations of the models relate to these behavioral effects, we sought to characterize the general properties of these representations that enable proficient execution of the task.</p></sec><sec id="s2-2"><title>Representations of task-relevant stimulus information</title><p>To perform LIM well, the models must learn representations that select the task-relevant information (target direction) and diminish the influence of irrelevant information (flanker direction, stimulus layout, and stimulus position). To investigate these representations, we presented the model with a holdout set of LIM stimuli (separate from the training stimuli) and analyzed the resulting unit activity in each CNN layer (<xref ref-type="bibr" rid="bib30">Hohman et al., 2020</xref>; <xref ref-type="bibr" rid="bib99">Zeiler and Fergus, 2013</xref>; <xref ref-type="fig" rid="fig3">Figure 3A</xref>, Methods). The activations from a given layer <italic>l</italic> form an <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix, where <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of stimuli in the holdout set (5000) and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of active units in layer <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (a variable fraction of units in each layer did not respond to any stimuli and were excluded from the activation matrix).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neural representations of target direction.</title><p>(<bold>A</bold>) Schematic of the convolutional neural network (CNN) activations extracted from each network layer. Each layer yields a <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> activation matrix, where <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of stimuli and <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of active units (i.e. feature dimensions) in layer <inline-formula><mml:math id="inf17"><mml:mi>l</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Decoding accuracy of stimulus target direction. (<bold>C</bold>) Normalized mutual information for target direction conveyed by single units, averaged across units. Mutual information was normalized by the entropy of the target direction distribution (possible range = [0, 1]). (<bold>D</bold>) Dimensionality of target representations as measured by the participation ratio of the target-centered activation covariance matrix. (<bold>E</bold>) Proportion of units exhibiting selectivity for target direction. Panels <bold>B-E</bold> show the average across <italic>n</italic> = 75 models; error bars correspond to bootstrap 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Activity of all selective (+) units for one example model.</title><p>Each row shows the activity of one unit for 100 randomly selected stimuli, sorted by target direction. The activity of each unit was centered and normalized by the activity of the stimulus with the largest magnitude activation. The small number of selective (+) units in layer Conv1 are not shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig3-figsupp1-v1.tif"/></fig></fig-group><p>To characterize the emergence of target selectivity, we adopted analysis techniques from the neuroscience literature that aim to determine what stimulus properties are coded by population-level neural activity, analogous to the high-dimensional CNN activations studied here. Specifically, we quantified how well the target direction could be decoded from the activation matrix of a given layer with a linear support vector machine (<xref ref-type="bibr" rid="bib32">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib43">Koren et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Rust and Dicarlo, 2010</xref>) (SVM; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Note that only incongruent trials were used in these analyses, since the classifier could use the flanker direction to classify the target on congruent trials, artificially inflating performance. Decoding performance on holdout data for target direction was near chance (27%) in the first network layer and increased to nearly perfect decoding (â¥ 97%) at layer 4, with a sharp increase between the second and third convolutional layers (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The increase in target decoding accuracy from shallower to deeper network layers is generally consistent with neural recording studies in mammals and other neural networks studies that document more accurate decoding of abstract variables (e.g. object or category identity) in higher visual/cortical regions and deeper neural network layers (<xref ref-type="bibr" rid="bib8">Brincat et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Muratore et al., 2022</xref>; <xref ref-type="bibr" rid="bib86">Tafazoli et al., 2017</xref>).</p><p>We also investigated target selectivity at the single-unit level by quantifying the mutual information between each unitâs activity and target direction (<xref ref-type="bibr" rid="bib54">Muratore et al., 2022</xref>; <xref ref-type="bibr" rid="bib86">Tafazoli et al., 2017</xref>; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). In contrast to the population-level decoding results, information for target direction exhibited a non-monotonic âhunchbackâ profile across the convolutional layers, then increased sharply in the final fully-connected layer. The observation that single-unit information for target direction decreased between the fourth and final convolutional layers indicates that the units become progressively less selective for particular target directions. Since population-level decoding remained high in these layers, this suggests a transition from representing target direction with specialized âtarget neuronsâ to a more distributed, ensemble-level code. Notably, a similar transition in coding properties takes place along the cortical hierarchy, with higher-order cortical regions exhibiting a reduction in units with âpureâ selectivity and a corresponding increase in units with mixed selectivity for multiple task or stimulus features (<xref ref-type="bibr" rid="bib1">Annis et al., 2021</xref>; <xref ref-type="bibr" rid="bib53">Meister et al., 2013</xref>; <xref ref-type="bibr" rid="bib73">Rigotti et al., 2013</xref>). The high proportion of units with mixed selectivity results in a high-dimensional representation in which all task-relevant information can easily be extracted with simple linear decoders (<xref ref-type="bibr" rid="bib11">Cover, 1965</xref>; <xref ref-type="bibr" rid="bib58">Pagan et al., 2013</xref>).</p><p>Consistent with these ideas, we found that the dimensionality of target representations as measured by the participation ratio (Methods) increased sharply in the last two convolutional layers (Conv5âConv6), paralleling the reduction in single-unit information for target direction in these layers (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The high-dimensional representation observed in these layers may enable the VAM to capture the rich stimulus-behavior dependencies present in the participant data. The reduction in dimensionality in the final fully-connected layer, and concomitant increase in single-unit information for target direction, may reflect a strong constraint to select the correct target direction imposed by the task. Notably, the hunchback-shaped profile we observed in the dimensionality of representations has also been observed along visual cortical regions of the rat ventral stream and in other neural network studies (<xref ref-type="bibr" rid="bib2">Ansuini et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Muratore et al., 2022</xref>).</p><p>To more explicitly characterize the degree of directional selectivity in each layer, we quantified the proportion of units that responded preferentially to one of the four target directions. We separated units according to the sign of modulation and degree of directional selectivity: âselective (+)â and âselective (-)â units were more (or less) active for one target direction relative to the other three; âcomplexâ units exhibited significant modulation by target direction without a clear directional preference (Methods).</p><p>The proportion of units that were selective for a particular direction increased steadily from the second to the fourth convolutional layer, with most units exhibiting positive modulation (<xref ref-type="fig" rid="fig3">Figure 3E</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>). Between the fourth and final convolutional layers, the proportion of units with positive target direction modulation decreased, while the proportion of units with negative and complex modulation increased.</p><p>In summary, a strong representation for target direction emerged in the middle convolutional layers, and was initially supported by a simple low-dimensional code, with information for each target direction concentrated in separate populations of directionally-tuned units. In later convolutional layers, target direction was supported by a more distributed, complex, and high-dimensional code, with weaker directional selectivity at the single-unit level.</p></sec><sec id="s2-3"><title>Extraction and suppression of distracting stimulus information</title><p>How do the modelsâ visual representations enable target selectivity for stimuli that vary along several irrelevant dimensions? The mammalian visual system solves the analogous problem of visual object recognition through representations that become increasingly invariant to different object positions, sizes, and lighting conditions (<xref ref-type="bibr" rid="bib16">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib75">Rust and Dicarlo, 2010</xref>). To determine whether the models learned representations that share these characteristics, we initially assessed whether the model representations for target direction are indeed invariant (or more generally, tolerant), to variation in flanker direction, stimulus layout, and horizontal/vertical position. Alternatively, the models could have learned a coding scheme in which different representations are responsible for selecting the target in each distracter context, e.g., with units that respond to the conjunction of particular target direction and layout combinations.</p><p>To assess the degree of representation tolerance, we quantified how well a linear SVM trained to classify target direction in one distracter context generalized to a different distracter context, where the distracter context is defined by a particular type of task-irrelevant information (<xref ref-type="bibr" rid="bib5">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Rust and Dicarlo, 2010</xref>). For example, to assess the degree of tolerance to flanker direction, we split trials into a training set where the flanker direction was fixed to a particular value (e.g. down), and a generalization set with all other flanker directions (e.g. flanker direction = left/right/up). The performance of the classifier on the generalization set measures the extent to which the target representations are tolerant (or invariant) to variability in a given type of distracting information.</p><p>For each type of distracting information, we found that generalization performance was initially near chance (25%) and increased steadily through the network layers (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The lower generalization performance for flanker direction observed in the deepest network layers (â¼60%) can be attributed to the robust congruency effects exhibited by the models (<xref ref-type="fig" rid="fig2">Figure 2D, E</xref>): for the vast majority of incongruent error trials, the model chose the flanker direction, implying that flanker direction has a strong impact on model representations.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Suppression of task-irrelevant information and tolerance in task-relevant representations.</title><p>(<bold>A</bold>) Decoding accuracy of stimulus target direction in a new distracter context (generalization performance). Context was defined by the values of a given stimulus feature (flanker direction, layout, horizontal/vertical position). (<bold>B</bold>) Decoding accuracy of irrelevant stimulus features. (<bold>C</bold>) Normalized mutual information for irrelevant stimulus features conveyed by single units, averaged across units. For each stimulus feature, the mutual information was normalized by the entropy of the stimulus feature distribution (possible range = [0, 1]). All panels show the average across <italic>n</italic> = 75 models; error bars correspond to bootstrap 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig4-v1.tif"/></fig><p>The tolerance of target direction representations to variability in irrelevant stimulus features suggests that the irrelevant information was progressively suppressed from shallower to deeper network layers. To examine this explicitly, we quantified how well each irrelevant stimulus feature (flanker direction, stimulus layout, horizontal/vertical position) could be decoded from the activity in each layer using the same SVM classifier methodology as we did for target direction decoding. We found that decoding accuracy for flanker direction and stimulus layout exhibited a hunchback profile in which decoding performance started low in early layers, increased in intermediate layers, and decreased in later layers (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The decoding accuracy for horizontal and vertical position followed a similar but shifted pattern: there was a steady increase in accuracy until the second-to-last layer, followed by a slight drop in the last layer. Partial (rather than complete) suppression of irrelevant stimulus features is expected given that these features all impact behavior (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Note that the increase in receptive field size that occurs from shallower to deeper layers is necessary but not <italic>sufficient</italic> for accurate decoding of the irrelevant stimulus features, and does not explain the reduction in decoding accuracy in later layers.</p><p>We also examined suppression at the single-unit level by quantifying the mutual information between each unitâs activations and the values of each task-irrelevant stimulus feature (<xref ref-type="bibr" rid="bib86">Tafazoli et al., 2017</xref>; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). The mutual information for a given stimulus feature was normalized by the entropy of the feature distribution to facilitate comparisons between the different stimulus features (<xref ref-type="bibr" rid="bib54">Muratore et al., 2022</xref>). In agreement with the population-level decoding analyses, information for the irrelevant stimulus features exhibited a pronounced hunchback profile in the progression from shallower to deeper network layers. It is noteworthy that the suppression of irrelevant information is more pronounced at the single-unit level in that decoding accuracy remains relatively high in later layers, particularly for flanker direction. This parallels the findings discussed above for target direction, and again suggests a transition from a simple code with populations of units that are selective for particular stimulus features to a more distributed, ensemble-level code.</p></sec><sec id="s2-4"><title>Orthogonality of task-relevant and irrelevant information predicts behavior</title><p>A noteworthy feature of visual attention is that the selectivity and tolerance identified above is not absolute: irrelevant information cannot be filtered out completely, as illustrated by the congruency effects observed in the flanker task and related conflict tasks. A common framework for understanding these behavioral phenomena posits that task-relevant and irrelevant information compete for control over response execution, and that congruency effects arise from incomplete suppression of irrelevant information (<xref ref-type="bibr" rid="bib89">Ulrich et al., 2015</xref>; <xref ref-type="bibr" rid="bib92">van den Wildenberg et al., 2010</xref>). Motivated by these theories, we investigated whether the degree of suppression of irrelevant information in the trained models was correlated with congruency effects across the models we analyzed.</p><p>To this end, we operationalized suppression with two metrics of the model representations that we investigated above: the accuracy of decoders trained to classify flanker direction from the model representations and the mutual information for flanker direction conveyed by single units. We expected to observe a positive correlation between both of these metrics and both RT and accuracy congruency effects: higher decoder accuracy or mutual information for flanker direction corresponds to less suppression and, therefore, higher congruency effects. However, we did not observe a significant positive correlation between either decoding accuracy or mutual information and RT or accuracy congruency effects in any of the model layers, with the exception of a single significant positive correlation between flanker mutual information and accuracy congruency effects in layer Conv3 (<xref ref-type="fig" rid="fig5s1 fig5s1">Figure 5âfigure supplement 1</xref>).</p><p>Given this somewhat surprising negative result, we were motivated to consider an alternative account of congruency effects, one that takes into account the relative geometry of the task-irrelevant (flanker) <italic>and</italic> task-relevant (target) information. In particular, we considered the possibility that task-relevant and irrelevant information could be orthogonalized in the high-dimensional space of neural activity, such that task-relevant information is shielded from distracter interference. The general idea that neural representations for different types of information can be orthogonalized to prevent interference has received support from a number of neural recording studies (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Libby and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib59">Panichello and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib74">Ritz and Shenhav, 2024</xref>).</p><p>To examine whether target and flanker representations are orthogonalized, we first defined target and flanker subspaces from the target direction and flanker direction classifiers used in the decoding analyses described above (Methods). The target direction classifier for a given network layer implicitly defines four decoding vectors, one for each target direction (<xref ref-type="bibr" rid="bib5">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Libby and Buschman, 2021</xref>). We define the subspace of the <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional feature space spanned by these four vectors as the target subspace; the flanker subspace is defined analogously.</p><p>To measure the orthogonality between target and flanker subspaces, we calculated the average of the cosine of the principal angles between the target and flanker subspaces, a metric we refer to as subspace alignment (Methods). Principal angles generalizes the idea of angles between lines or planes to arbitrary dimensions (<xref ref-type="bibr" rid="bib37">Jordan, 1873</xref>). The subspace alignment metric has a simple interpretation: it is equal to one if the subspaces are completely parallel and zero if they are completely orthogonal.</p><p>We first characterized how target/flanker subspace alignment develops across the layers of the trained models (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Given that target direction decoding is poor in the first two layers (&lt;50%; <xref ref-type="fig" rid="fig3">Figure 3B</xref>), the decoding vectors used to define the target subspace are not particularly meaningful, and we do not attempt to interpret the subspace alignment metric in these layers. Beginning at the third convolutional layer, when decoding accuracy for both targets and flankers is high (&gt;90%), we found that target and flanker subspaces are well-aligned. We interpret the high alignment as evidence that the model has learned a common representation for direction that is shared for both targets and flankers. In later layers, we found that target and flanker representations become increasingly orthogonal, consistent with the view that the processing in later layers acts to reduce interference between task-relevant and irrelevant information.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Orthogonality of target/flanker subspaces predicts accuracy congruency effects.</title><p>(<bold>A</bold>) Target/flanker subspace alignment averaged across models. (<bold>B</bold>) Pearsonâs correlation coefficient between target/flanker subspace alignment and accuracy congruency effect calculated across models. (<bold>C</bold>) Target/flanker subspace alignment vs. accuracy congruency effect for layers Conv4âFC1. Each point corresponds to one model; the red line is the linear best fit. For all panels, <italic>n</italic> = 75 models. Error bars in panels <bold>AâB</bold> correspond to bootstrap 95% confidence intervals. Asterisks in panel <bold>B</bold> indicate a significant Pearsonâs <italic>r</italic> (adjusted <italic>p</italic>-value&lt;0.05, permutation test with <italic>n</italic> = 1000 shuffles, Bonferroni correction for seven comparisons).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Absence of correlation between flanker suppression metrics and congruency effects.</title><p>All panels show the Pearsonâs correlation coefficient between the specified suppression and behavior metrics, calculated across models. (<bold>A</bold>) Flanker direction decoding accuracy vs. accuracy congruency effect. (<bold>B</bold>) Mutual information for flanker direction conveyed by single units vs. accuracy congruency effect. (<bold>C</bold>) Flanker direction decoding accuracy vs. response time (RT) congruency effect. (<bold>D</bold>) Mutual information for flanker direction conveyed by single units vs. RT congruency effect. For all panels, <italic>n</italic> = 75 models, error bars correspond to bootstrap 95% confidence intervals. Asterisks indicate a significant Pearsonâs <italic>r</italic> (adjusted <italic>p</italic>-value &lt; 0.05, permutation test with <italic>n</italic> = 1000 shuffles, Bonferroni correction for seven comparisons).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Absence of correlation between target/flanker subspace alignment and response time (RT) congruency effect.</title><p>Pearsonâs correlation coefficient between target/flanker subspace alignment and RT congruency effect across models (<italic>n</italic> = 75 models, error bars correspond to bootstrap 95% confidence intervals). The correlation was not significant for any layer (adjusted <italic>p</italic>-value &gt; 0.05, permutation test with <italic>n</italic> = 1000 shuffles, Bonferroni correction for seven comparisons).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig5-figsupp2-v1.tif"/></fig></fig-group><p>If orthogonalizing target and flanker representations reduces interference from the irrelevant (flanker) information, we should observe a positive correlation between subspace alignment and congruency effects across models, since greater alignment results in more interference. Consistent with this idea, we observed a significant positive correlation between subspace alignment and accuracy congruency effects across models in each layer beginning with the fourth convolutional layer (<xref ref-type="fig" rid="fig5">Figure 5B andC</xref>; adjusted <italic>p</italic>-value&lt;0.05 for layers 3â7, permutation test, Bonferroni correction for seven comparisons). In contrast, we did not observe a significant correlation between target/flanker subspace alignment and RT congruency effects in any network layer, suggesting a mechanistic dissociation between RT and accuracy congruency effects (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>).</p></sec><sec id="s2-5"><title>Representation geometry of task-optimized models</title><p>Researchers who use neural network models to study neural representations typically optimize the model to perform a task, rather than fit the model to behavioral data from the task as we do here <xref ref-type="bibr" rid="bib52">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib93">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib96">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Dezfouli et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Jaffe et al., 2023</xref>; <xref ref-type="bibr" rid="bib85">Sussillo et al., 2015</xref>. This raises the possibility that these two training paradigms induce different representations in the models. To investigate this, we trained CNNs to perform LIM (i.e. output the direction of the target) by minimizing the standard cross-entropy loss function used in image classification tasks, where the training labels are given by the true direction of the target bird in each stimulus. The task-optimized CNNs were identical to those used in the VAMs, except that the outputs of the last layer were converted to softmax-scored probabilities for each direction rather than drift rates. Otherwise, all aspects of the optimization algorithm, CNN architecture, initialization, and training data for the task-optimized models were the same as those used to train the VAMs. We trained one task-optimized model for each VAM using stimulus data from the same participants (<italic>n</italic>=75 task-optimized models).</p><p>We first compared the behavioral outputs of the task-optimized models and the VAMs. We found that the task-optimized models did not exhibit an accuracy congruency effect (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Thus, simply training the model to perform the task is not sufficient to reproduce a behavioral phenomenon widely-observed in conflict tasks. This challenges a core (but often implicit) assumption of the task-optimized training paradigm, namely that training a model to do a task well will result in model representations that are similar to those employed by humans. Indeed, for a number of visual tasks, the representations and behavior of task-optimized CNNs has been observed to differ considerably from those of humans (<xref ref-type="bibr" rid="bib18">Eckstein et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Jacobs and Bates, 2019</xref>, <xref ref-type="bibr" rid="bib36">Jha et al., 2023</xref>; <xref ref-type="bibr" rid="bib64">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Sanders and Nosofsky, 2020</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparison of visual accumulator models (VAMs) and task-optimized models.</title><p>(<bold>A</bold>) Accuracy congruency effect. (<bold>B</bold>) Target/flanker subspace alignment. (<bold>C</bold>) Dimensionality of target representations, as measured by the participation ratio of the target-centered activation covariance matrix. (<bold>D</bold>) Normalized mutual information for target/flanker direction conveyed by single units, averaged across units. Mutual information was normalized by the entropy of the target/flanker direction distribution (possible range = [0, 1]). (<bold>E</bold>) Decoding accuracy of target/flanker direction. (<bold>F</bold>) Proportion of units exhibiting selectivity for target direction in layers Conv5âConv6. All panels show the average across <italic>n</italic> = 75 task-optimized models and <italic>n</italic> = 75 VAMs; error bars correspond to bootstrap 95% confidence intervals. The VAM data shown in panels <bold>AâF</bold> is the same as that shown in <xref ref-type="fig" rid="fig2">Figures 2E</xref>, <xref ref-type="fig" rid="fig5">5A</xref> and <xref ref-type="fig" rid="fig3">3B-E</xref> and <xref ref-type="fig" rid="fig4">Figure 4B, C</xref>, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6âfigure supplement 1.</label><caption><title>Additional analysis of visual accumulator models (VAMs) and task-optimized models.</title><p>(<bold>A</bold>) Normalized mutual information for stimulus layout and horizontal/vertical stimulus position conveyed by single units, averaged across units. Mutual information was normalized by the entropy of the corresponding stimulus feature distribution. (<bold>B</bold>) Decoding accuracy of stimulus layout and horizontal/vertical stimulus position. All panels show the average across <italic>n</italic> = 75 task-optimized models and <italic>n</italic> = 75 VAMs; error bars correspond to bootstrap 95% confidence intervals. The VAM data shown in panels A and B is the same as that shown in <xref ref-type="fig" rid="fig4">Figure 4C and B</xref>, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98351-fig6-figsupp1-v1.tif"/></fig></fig-group><p>Since the task-optimized models do not generate RTs, it is not possible to directly measure RT congruency effects in these models without making additional assumptions about how the CNNâs classification decisions relate to RTs. However, as a coarse proxy for RT, we can examine the confidence of the CNNâs decisions, defined as the softmax-scored logit (probability) of the most probable direction in the final CNN layer. This choice of RT proxy is motivated by some prior studies that have combined CNNs with EAMs (<xref ref-type="bibr" rid="bib1">Annis et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Holmes et al., 2020</xref>; <xref ref-type="bibr" rid="bib88">Trueblood et al., 2021</xref>). These studies explicitly or implicitly derive a measure of decision confidence from the activity of the last CNN layer. The confidence measure is then mapped to the EAM drift rates, such that greater decision confidence generally corresponds to higher drift rates (and therefore shorter RTs).</p><p>We calculated the average confidence of each task-optimized CNN separately for congruent vs. incongruent trials. On average, the task-optimized models showed higher confidence on congruent vs. incongruent trials (<italic>W</italic>=21.0, p&lt;1e-3, Wilcoxon signed-rank test; Cohenâs <italic>d</italic> = 0.99; <italic>n</italic> = 75 models). Therefore, these analyses provide some evidence that task-optimized CNNs have the capacity to exhibit congruency effects, though an explicit comparison of the magnitude of these effects with human data requires additional modeling assumptions (e.g. fitting a separate EAM).</p><p>Above, we showed that VAMs with greater orthogonalization of target and flanker information exhibit smaller accuracy congruency effects (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), providing evidence that the relative geometry of task-relevant and irrelevant representations is a critical determinant of the degree of flanker interference at the behavioral level. Given that the task-optimized models do not exhibit an accuracy congruency effect, we expect that these models would exhibit a higher degree of orthogonalization of target and flanker information. Consistent with this idea, we found that the task-optimized models had lower target/flanker subspace alignment (i.e. higher orthogonalization) for all network layers beginning with the third convolutional layer (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p>A direct consequence of the training paradigms used to train the VAMs is that these models are encouraged to capture dependencies between the stimulus features and behavior (RTs and accuracy), while the task-optimized models are not. As a result, the VAMs may learn more complex representations of the stimuli, since a variety of stimulus featuresâlayout, stimulus position, flanker directionâinfluence behavior (<xref ref-type="fig" rid="fig2">Figure 2</xref>). To investigate this possibility, we compared the dimensionality of target representations between the VAMs and task-optimized models using the participation ratio metric discussed above.</p><p>We found that the dimensionality of the VAM and task-optimized model representations was nearly identical for the first four convolutional layers (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). In contrast, for the final two convolutional layers, the VAMs exhibited a substantially more pronounced expansion of dimensionality than the task-optimized models. In the final fully-connected layer, dimensionality decreased sharply for both types of models, and was somewhat higher for the VAMs.</p><p>The increased dimensionality of VAMsâ target represenations in later network layers is consistent with the view that these models must learn more complex representations of the stimuli in order to successfully capture stimulus-behavior dependencies. It is also noteworthy that the most striking difference between the dimensionality of the VAMs and task-optimized models occurs during the latter part (layers Conv5âConv6) of the expansion phase of the hunchback-shaped dimensionality profile discussed above and observed in prior work (<xref ref-type="bibr" rid="bib2">Ansuini et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Mante et al., 2013</xref>). In these layers, single-unit information for target and flanker directionâthe primary task featuresâdecreases, while population-level decoding of these features remains high (<xref ref-type="fig" rid="fig3">Figures 3BâC </xref>â<xref ref-type="fig" rid="fig4">4BâC</xref>). As discussed above, this dissociation implies a transition from a simple representation of target/flanker direction with separate populations of directionally-tuned units to a more complex and distributed code.</p><p>To determine whether the task-optimized models exhibited this change in coding properties, we quantified the single-unit information and population-level decoding accuracy for target/flanker direction in these models. Relative to the VAMs, the task-optimized models had substantially higher single-unit information for both target and flanker direction in layers Conv5âConv6 (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). The task-optimized models also showed marginally more or roughly equivalent single-unit information for stimulus position in these layers relative to the VAMs, but had less single-unit information for stimulus layout (<xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>).</p><p>In contrast, population-level decoding accuracy for target/flanker direction and stimulus position was similar between the task-optimized models and VAMs in layers Conv5âConv6, though decoding accuracy for stimulus layout was notably lower for the task-optimized models (<xref ref-type="fig" rid="fig6">Figure 6E</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>). These results suggest that the task-optimized models maintained a simpler code for target/flanker direction in the later convolutional layers relative to the VAMs, primarily relying on separate populations of directionally-tuned units. Consistent with this idea, the task-optimized models had a higher proportion of simple selective (+) directionally-tuned units and a lower proportion of complex units in layers Conv5â6 relative to the VAMs (<xref ref-type="fig" rid="fig6">Figure 6F</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The dominant models of decision-making, while providing a good quantitative description of psychophysical data, do not incorporate biologically plausible models of the perceptual processes that are essential for many behaviors (<xref ref-type="bibr" rid="bib20">Evans and Wagenmakers, 2020</xref>). On the other hand, neural network models of vision, while capturing core properties of the primate visual system, do not account for many results from behavioral experiments (<xref ref-type="bibr" rid="bib3">Baker et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Bowers et al., 2022</xref>; <xref ref-type="bibr" rid="bib21">Fel et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Linsley et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Malhotra et al., 2022</xref>). The VAM addresses both of these limitations by integrating neural network models for visual processing with traditional decision-making models in a unified probabilistic framework that can be fitted to visual stimuli and RT data. Leveraging large-scale data from a task with rich visual stimuli, we demonstrate that our framework captures complex dependencies between stimulus features and behavior at the level of individual participants. We also illustrate how congruency effectsâa core behavioral phenomenon observed in conflict tasksâcan be explained in terms of the visual representations of the model. Finally, we document several key differences between the representations learned by models fitted to human behavioral data (the VAMs) and those learned by models trained only to do the task.</p><sec id="s3-1"><title>Processing phases underlying the transformation of sensory information</title><p>To perform the task and capture the behavioral data, the VAMs learned representations thatâlike the mammalian cortexâextract task-relevant information from raw visual inputs. Our analyses of these representations revealed several discrete processing phases that are fruitfully discussed in relation to the changes in target representation dimensionality we observed. Across the layers of the VAMâs CNN, target dimensionality exhibited a prominent hunchback shape profile, corresponding to an initial protracted phase of dimensionality expansion followed by abrupt dimensionality compression in the final network layers. An analogous expansion and subsequent compression of object representation dimensionality has been documented in CNNs trained to classify images and along the rat visual-cortical hierarchy (<xref ref-type="bibr" rid="bib2">Ansuini et al., 2019</xref>; <xref ref-type="bibr" rid="bib54">Muratore et al., 2022</xref>), with some notable differences from our work that we highlight below.</p><p>We speculate that the initial phase of dimensionality expansion can be explained in part by the pruning of low-level stimulus features (e.g. contrast and luminosity) that are correlated across stimuli in the dataset (<xref ref-type="bibr" rid="bib2">Ansuini et al., 2019</xref>). These correlations are particularly strong in our dataset since we used the same background image for each stimulus, resulting in low-dimensional activity in the initial network layers. Conceivably, the initial increase in target representation dimensionality (i.e.layers Conv1âConv4) results from the removal of these correlations, analogous to a whitening transformation (<xref ref-type="bibr" rid="bib2">Ansuini et al., 2019</xref>).</p><p>In the early and middle convolutional layers (Conv1âConv4), we found that the population-level decoding and single-unit information for both task-relevant (target direction) and distracting information (flanker direction, stimulus layout, and position) increased, occurring in parallel with the subtle expansion of dimensionality we observed in these layers. These trends are broadly consistent with observations that the mammalian visual cortex encodes increasingly abstract stimulus properties (e.g. object identity) in downstream brain regions (<xref ref-type="bibr" rid="bib27">GÃ¼Ã§lÃ¼ and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib32">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib75">Rust and Dicarlo, 2010</xref>). The fact that information for distracting stimulus features increased in these layers is especially noteworthy, given that this informationâthe flanker direction, most prominentlyâimpairs performance on the task (<xref ref-type="bibr" rid="bib19">Eriksen and Eriksen, 1974</xref>; <xref ref-type="bibr" rid="bib83">Stoffels and van der Molen, 1988</xref>). While it is tempting to speculate that the VAMs learned to extract distracter information because they were fitted to behavioral data, where the impairments in task performance manifest, the task-optimized models also exhibited increased distracter information in these layers. This suggests an alternative explanation in which the model first extracts more granular, superordinate stimulus representations that group together stimulus features with similar statistics. For example, the initial increase in encoding of target and flanker direction may reflect the representation of a unitary âdirectional signalâ that facilitates the eventual separation of target and flanker direction representations that occurs in later layers. This idea is consistent with our observation that target/flanker subspaces are highly aligned in the middle convolutional layers and become progressively more orthogonal in later network layers.</p><p>In the last convolutional layers (Conv5âConv6), we observed a large expansion in target representation dimensionality, which was notably less pronounced in the task-optimized models compared to the VAMs. We speculate that the increase in dimensionality may be partly due to an increase in mixed selectivity for multiple stimulus features at the single-unit level, similar to the single-unit coding properties observed in primate PFC (<xref ref-type="bibr" rid="bib73">Rigotti et al., 2013</xref>). In general agreement with this idea, we observed a reduction in the proportion of units with selectivity for particular target directions in these layers, and a concomitant increase in units that were modulated by multiple target directions. Relative to the VAMs, the task-optimized models showed a higher proportion of directionally-tuned units in these layers. A notable advantage of high-dimensional neural codes composed of units with mixed selectivity is that many different input-output relationships can be implemented with simple decoders (<xref ref-type="bibr" rid="bib11">Cover, 1965</xref>; <xref ref-type="bibr" rid="bib58">Pagan et al., 2013</xref>; <xref ref-type="bibr" rid="bib73">Rigotti et al., 2013</xref>). Thus, the higher dimensionality and more complex target direction coding observed in the VAMs relative to the task-optimized models may reflect the fact that the VAMs are trained to capture a potentially large number of dependencies between stimulus attributes and behavior, while the task-optimized models need only extract the target direction. Given the impressive capacity of primates to learn complex tasks and arbitrary stimulus behavior relationships, and the abundance of high-dimensional representations across the cortex, models trained to capture the richness of stimulus-behavior dependenciesâsuch as the VAMâmay result in better models of cortical processing than optimizing models to perform tasks.</p></sec><sec id="s3-2"><title>Congruency effects emerge from the relative geometry of task-relevant vs. distracting sensory representations</title><p>One of the key strengths of our modeling framework is that neural representations of stimulus features can be directly related to behavioral outputs. We focused in particular on congruency effects since they are ubiquitously observed in conflict tasks and highlight an inherent limitation of selective attention, namely that humans cannot completely filter out distracting information. Congruency effects are often interpreted in the context of a âdual-processâ model in which automatic or impulsive processing arising from distracting stimulus information and more controlled or deliberate processing of task-relevant information compete for control over behavior (<xref ref-type="bibr" rid="bib72">Ridderinkhof, 2022</xref>; <xref ref-type="bibr" rid="bib89">Ulrich et al., 2015</xref>; <xref ref-type="bibr" rid="bib92">van den Wildenberg et al., 2010</xref>). According to this model, congruency effects result from the incomplete suppression of incorrect response activation in the automatic pathway. Building on this framework, a variety of models have been proposed that successfully capture congruency effects and other behavioral phenomena observed in conflict tasks (<xref ref-type="bibr" rid="bib10">Cohen et al., 1992</xref>; <xref ref-type="bibr" rid="bib89">Ulrich et al., 2015</xref>; <xref ref-type="bibr" rid="bib94">White et al., 2011</xref>).</p><p>Our work differs from these prior modeling efforts in two key ways. First, we did not attempt to âbuild inâ a predetermined implementation of congruency effects. Rather, we fitted a relatively unstructured neural network model to human flanker task data and examined how congruency effects emerged (<xref ref-type="bibr" rid="bib34">Jaffe et al., 2023</xref>). Second, we explicitly modeled how task-relevant (and task-irrelevant) representations are extracted from raw visual stimuli with a biologically-plausible model of the mammalian visual system (a CNN). These features of the VAM allowed us to explore a space of possible explanations for congruency effects that are not readily investigated with other models of conflict tasks. Each CNN layer is composed of many simple neuron-like processing units, enabling a population-level description of task-relevant and irrelevant representations. Prior connectionist models of conflict tasks are also implemented as networks of interacting processing units (<xref ref-type="bibr" rid="bib10">Cohen et al., 1992</xref>), but are much reduced in scale relative to the VAM, precluding a characterization of stimulus feature representations in terms of their high-dimensional geometry as we pursue here. Another relevant attribute of CNNs is that the successive convolution and pooling operations in each layer extract increasingly abstract, task-related representations from raw visual inputs, a hallmark of the mammalian ventral visual system. This enabled a rich characterization of the intermediate visual representations that sequentially transform raw visual stimuli to behavioral outputs.</p><p>Leveraging these features of the VAM framework, we investigated potential explanations for congruency effects in terms of the population-level activity and representation geometry in each network layer. Motivated by the dual-process model of conflict tasks mentioned above, we initially sought evidence for the suppression of task-irrelevant information, as operationalized by population-level decoding accuracy and single-unit information for flanker direction. While we did find evidence of greater suppression in later vs. middle convolutional layers for both of these metrics, variability in the magnitude of this suppression across models was not related to variability in congruency effects.</p><p>Instead, we found that the relative geometry of target and flanker representations was a critical determinant of congruency effects. In particular, models with smaller accuracy congruency effects had more orthogonal (or less aligned) representations of targets and flankers in later network layers, proximal to behavioral outputs. This finding is consistent with the idea that orthogonalization of task-relevant/irrelevant representations shields the task-relevant information from distracters, and parallels findings from neural recording studies in both humans and animals that representations for different sources of information can be orthogonalized in order to prevent interference (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Libby and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib59">Panichello and Buschman, 2021</xref>; <xref ref-type="bibr" rid="bib74">Ritz and Shenhav, 2024</xref>; <xref ref-type="bibr" rid="bib95">Xie et al., 2022</xref>).</p><p>To elaborate on this idea, in models with more aligned (less orthogonal) target/flanker representations in intermediate layers, the task-relevant (target) subspace is effectively âcontaminatedâ by task-irrelevant (flanker) information. In the absence of any corrective process, the task-relevant subspace propagates a mixture of both correct (target) and incorrect (flanker) direction signals forward through the network. At the final readout layer of the network, assuming that the target subspace is well-aligned with the readout weights (<xref ref-type="bibr" rid="bib60">Papyan et al., 2020</xref>), flanker signals within the target subspace then contribute to the drift rate for the (incorrect) flanker direction. This increases the probability of an error, such that models with more aligned target/flanker representations exhibit larger accuracy congruency effects. A corollary of this proposition is that the absolute amount of flanker information in the networkâequivalently, the degree to which flanker information has been suppressedâis not necessarily predictive of congruency effects. This may account for our observations that the measures of suppression we considered are not correlated with accuracy congruency effects across models, and that representations for flanker direction do not appear to be strongly suppressed even in later network layers, as evidenced by high decoding accuracy for flanker direction in both the VAMs and task-optimized models.</p></sec><sec id="s3-3"><title>The role of dynamics in conflict tasks</title><p>One apparent limitation of the VAM as presented here is that it does not have visual processing dynamics, which seem to be required to explain some observations from the flanker task and related conflict tasks. For example, the RTs on incongruent error trials are typically faster than error RTs for congruent trials and RTs for correct trials <xref ref-type="bibr" rid="bib92">van den Wildenberg et al., 2010</xref>, an effect that we confirm is also present in the flanker task variant studied here. This observation can be explained by a âshrinking attention spotlightâ in which response activation from flankers starts high and diminishes over time, resulting in a higher proportion of errors for faster RTs <xref ref-type="bibr" rid="bib94">White et al., 2011</xref>. We speculate that our models were unable to capture these particular error patterns because the visual processing module (CNN) we used does not have any dynamics (e.g. recurrence) that could instantiate such time-varying attention and resultant time-varying drift rates. However, it is not difficult to imagine how the orthogonalization mechanism described above, which explains variability in accuracy congruency effects <italic>across</italic> individuals, could act in concert with other dynamic processes that explain variability in congruency effects <italic>within</italic> individuals (e.g. as a function of RT). In general, any process that dynamically gates the influence of irrelevant sensory information on behavioral outputs could accomplish this, for example, ramping inhibition of incorrect response activation (<xref ref-type="bibr" rid="bib92">van den Wildenberg et al., 2010</xref>), a shrinking attention spotlight (<xref ref-type="bibr" rid="bib94">White et al., 2011</xref>), or dynamics in neural population-level geometry (<xref ref-type="bibr" rid="bib38">Kaufman et al., 2014</xref>). To pursue these ideas, future work may aim to incorporate dynamics into the visual component and decision component of the VAM with recurrent CNNs (<xref ref-type="bibr" rid="bib25">Goetschalckx et al., 2023</xref>; <xref ref-type="bibr" rid="bib56">Nayebi et al., 2018</xref>) and the task-DyVA model (<xref ref-type="bibr" rid="bib34">Jaffe et al., 2023</xref>), respectively.</p></sec><sec id="s3-4"><title>Conclusion</title><p>The VAM is a probabilistic model of psychophysical data that captures how raw sensory inputs are transformed into the abstract representations that guide decisions. Raw (pixel-space) visual stimuli are processed by a biologically-plausible neural network model of vision that outputs the parameters of a traditional decision-making model. Each VAM is fitted to data from a single participant, a feature that allowed us to study how individual differences in behavior emerge from differences in the âbrainsâ of the models. To this end, we found that models with smaller congruency effects had more orthogonal representations for task-relevant and irrelevant information. While we chose to use a CNN to model visual processing, we note that the VAM is not limited to this choice: other sensory encoding models, such as those based on transformer architectures (<xref ref-type="bibr" rid="bib17">Dosovitskiy et al., 2021</xref>), can be readily swapped in to replace the CNN with minimal changes to the underlying VAM implementation. Similarly, the LBA decision-making model we employed is easily replaced with other decision-making models that have a closed-form or easily approximated likelihood, such as the diffusion-decision model and leaky competing accumulator model (<xref ref-type="bibr" rid="bib50">Lo and Ip, 2021</xref>; <xref ref-type="bibr" rid="bib55">Navarro and Fuss, 2009</xref>; <xref ref-type="bibr" rid="bib67">Ratcliff and Rouder, 1998</xref>; <xref ref-type="bibr" rid="bib91">Usher and McClelland, 2001</xref>). In this way, the VAM provides a general probabilistic framework for jointly fitting interpretable decision-making models and expressive neural network architectures of sensory processing with psychophysical data.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Datasets</title><p>We used deidentified Lost in Migration gameplay data from 75 Lumosity users (participants) to train the models included in this paper. The mean (SD) age of this sample at the time of signup was 56.4 (18.6) y; 46.7% identified as female, 48.0% identified as male, and 5.3% did not report their gender. All analysis and modeling was done retrospectively on preexisting Lumosity data that were collected during the normal use of the Lumosity program (at home). All participants consented to the use and disclosure of their deidentified Lumosity data for any purpose as laid out in the Lumosity Privacy Policy (<ext-link ext-link-type="uri" xlink:href="https://www.lumosity.com/en/legal/privacy_policy/">https://www.lumosity.com/en/legal/privacy_policy/</ext-link>). No statistical methods were used to predetermine sample sizes, though our sample sizes are comparable to or larger than those used in related modeling work (<xref ref-type="bibr" rid="bib9">Brown and Heathcote, 2008</xref>; <xref ref-type="bibr" rid="bib94">White et al., 2011</xref>).</p><p>The included participants were selected from a larger pool that met certain inclusion criteria. The selection from this larger pool was done at random, with the following exception: we included all participants under the age of 40 (n=19) to ensure adequate representation of younger participants. The criteria used to define the larger participant pool were as follows: we required that participants had signed up as Lumosity users between June 28, 2015 and June 30, 2020 (inclusive); that they were between the ages of 18 and 89 at the time of signup (inclusive); that their country of origin was Australia, Canada, New Zealand, or the United States; that their preferred language was English; and that they were not employees of Lumos Labs, Inc Accounts created for research purposes were also excluded. We also required that all of a given participantâs Lost in Migration gameplays were done on the web (as opposed to mobile) platform. Finally, we required that participants had at least 200 Lost in Migration gameplays and at least 25,000 trials starting with their 50<sup>th</sup> gameplay. One additional participant with near chance accuracy (33%) on Lost in Migration was excluded from the larger participant pool.</p><p>Trials with very short and very long RTs were excluded and did not count toward the 25,000 trial minimum. Specifically, we excluded trials less than or equal to 250 ms and trials classified as outliers from a criterion based on the median absolute deviation from the median (the MAD): trials with an absolute deviation from the median RT of more than 10 times a given participantâs MAD were excluded.</p><p>The final datasets from each participant used for model training consist of the first 25,000 non-outlier trials starting with their 50<sup>th</sup> gameplay. Data from the first 49 gameplays were excluded to reduce learning-related variability. Approximately 50% of trials were congruent (vs. incongruent).</p></sec><sec id="s4-2"><title>Modeling framework</title><sec id="s4-2-1"><title>Linear ballistic accumulator (LBA) model</title><p>The decision-making component of our modeling framework is the LBA model (<xref ref-type="bibr" rid="bib46">Kumbhar et al., 2020</xref>), tailored to Lost in Migration. Evidence for each of the four possible response directions is accumulated linearly and independently at a response-specific drift rate <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Commitment to a decision occurs when one of the accumulators reaches a fixed threshold parameter <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The RT on a given trial is the duration of this evidence accumulation process plus a constant non-decision time parameter <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that includes both sensory processing and motor execution. Response variability comes from two sources. First, on each trial, the drift rate <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each response <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>â</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is sampled independently from a Gaussian distribution with mean <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and a common SD <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We fix <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to 1 for all models to ensure that the LBA parameters are identifiable (<xref ref-type="bibr" rid="bib12">Dao et al., 2024</xref>; <xref ref-type="bibr" rid="bib28">Gunawan et al., 2020</xref>). Second, the initial evidence for each accumulator is sampled independently from a uniform distribution on the interval [0, <italic>A</italic>], where <italic>A</italic> is a model parameter.</p></sec><sec id="s4-2-2"><title>Visual accumulator model (VAM): Overview</title><p>The VAM is a generalization of the LBA model in which the drift rate means <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> depend on the stimuli <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> through a CNN. For a dataset with <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> trials, the task data are <bold>x</bold> = {response times <bold>t</bold>, choices <bold>c</bold>, stimuli <bold>s</bold>}, where <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We adopt a Bayesian framework and model the joint density of the task data <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula> and LBA parameters <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the drift rate means <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> depend on a CNN with parameters <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (described below). The factor on the left of <xref ref-type="disp-formula" rid="equ1">Equation (11)</xref> is the likelihood of the LBA model, which has a closed-form solution (<xref ref-type="bibr" rid="bib46">Kumbhar et al., 2020</xref>). The factor on the right is the prior distribution over the LBA parameters, specified as a standard multivariate Gaussian <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We express the joint density more compactly as:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi>Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi>Î¶</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To fit the VAM, i.e., learn the posterior distribution over the LBA parameters <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and simultaneously optimize the CNN parameters <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we apply automatic differentiation variational inference (ADVI) (<xref ref-type="bibr" rid="bib45">Kucukelbir et al., 2017</xref>). Rather than attempting to sample from the true posterior directly as in Markov chain Monte Carlo (MCMC) methods, variational inference introduces an approximate posterior density <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and minimizes the Kullback-Leibler (KL) divergence from <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by optimizing <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, we specify the approximate posterior <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as a multivariate Gaussian with mean <italic>Î¼</italic> and unconstrained covariance matrix <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (thus <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Î£</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>We use variational inference since it scales well to large datasets and can handle complicated models (such as the VAM), in contrast to MCMC methods. Leveraging automatic differentiation software (JAX <xref ref-type="bibr" rid="bib7">Bradbury et al., 2018</xref>), we automate the calculation of the derivatives of the variational objective (described below) with respect to both the CNN parameters <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and variational parameters <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-3"><title>Variable transformations</title><p>Note that the LBA parameters <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are restricted to be non-negative, while <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are restricted to be positive semidefinite. However, to optimize the variational objective (defined below), we require that <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <italic><bold>Î¸</bold></italic> have the same support (<xref ref-type="bibr" rid="bib30">Hohman et al., 2020</xref>). To achieve this, we transform <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <italic><bold>Î¸</bold></italic> to both have support on the real line. Specifically, we reparameterize <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> using the Cholesky decomposition as <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Î£</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">L</mml:mi><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mo>âº</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a lower-triangular matrix with entries in <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (so that now <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>). For the LBA parameters, in addition to mapping them to the real line, we must also enforce the constraint that the threshold <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is always greater than the parameter <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> controlling the range of the initial evidence distribution. We enforce this by defining <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>b</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>â</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and taking the log of <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>b</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to map them to the real line (<xref ref-type="bibr" rid="bib1">Annis et al., 2021</xref>). We define the transformed parameters as <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>â</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and we define the transformation that maps <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-4"><title>VAM objective function</title><p>To fit the VAM, we maximize the evidence lower bound (ELBO):<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This is the standard ELBO optimized in variational inference, with an additional dependence on the CNN parameters <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The ELBO is a lower bound on the marginal likelihood of the data <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and maximizing the ELBO is equivalent to minimizing the KL divergence from <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Writing the ELBO in terms of the transformed variables <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have:<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The term <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a Jacobian adjustment for the transformation <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> that maps <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¸</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, required to ensure that the transformed density integrates to one. For the transformation <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> vectorized as <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the Jacobian is given by:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Taking the log of the absolute value of the determinant of <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> gives the required adjustment:<disp-formula id="equ7"><label>(5)</label><mml:math id="m7"><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We will apply stochastic gradient ascent to maximize the ELBO objective function given by <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>, using Monte Carlo (MC) estimates of the expectation and AD to calculate gradients with respect to <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, the gradients of the ELBO with respect to <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> cannot be calculated directly by AD, since the expectation in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref> is taken with respect to <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which depends on <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We work around this by applying the <italic>reparameterization trick</italic> (<xref ref-type="bibr" rid="bib39">Kingma and Welling, 2013</xref>; <xref ref-type="bibr" rid="bib70">Rezende et al., 2014</xref>), which expresses <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as a differentiable transformation of an auxiliary noise variable <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo>â¼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> does not depend on <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In particular, we set <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to be a standard multivariate Gaussian, <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">I</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and define the transformation <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where Î¼ and <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the mean and covariance parameters of the Gaussian approximating density <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Now we estimate the expectation in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref> with MC samples from <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since the expectation no longer depends on <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can use AD directly on these MC estimates to obtain unbiased gradients of the ELBO. We estimate the ELBO for each data point using independent MC samples, since this reduces the variance of the gradients relative to using the same set of MC samples for a given batch of data (<xref ref-type="bibr" rid="bib40">Kingma et al., 2015</xref>). Thus the ELBO objective for the <italic>i</italic>th data point is estimated by:<disp-formula id="equ8"><label>(6)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mi class="mathcal" mathvariant="script">L</mml:mi><mml:mo class="mathcal" mathvariant="script">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo movablelimits="true" form="prefix">det</mml:mo><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msup><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-2-5"><title>VAM inference algorithm and other training details</title><p>The VAM training/inference algorithm is summarized in <bold>Algorithm 1</bold>. The size of the training set was 16,250 samples (65% of the total dataset) for each model. An additional validation set of 3750 samples (15%) was used to monitor model training. The remaining 5000 samples (20%) were used to evaluate model performance (the holdout set). We set the batch size <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to 256 and the number of MC samples used to estimate the ELBO <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to 10. We used the Adam optimizer <xref ref-type="bibr" rid="bib41">Kingma and Ba, 2017</xref> with the following hyperparameters for model training: learning rate = 1<italic>eâ3</italic>, <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The same random seed was used to initialize the parameters of all models. Models were trained using a single NVIDIA GeForce RTX 3060 Ti GPU. Each model took approximately 1 hr to train.</p><p>For a small fraction of attempted model training runs (10/85=11.8%), the model either failed to exceed chance accuracy (3/10 models) or converged to a state in which almost all trials had exclusively negative drift rates (7/10 models). These models were not included in summary analyses.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Algorithm 1. VAM inference algorithm.</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Data</bold>:<bold>x</bold>={RTs <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¦</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, choices <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¦</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, stimuli <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¦</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>}<break/><inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">â</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> Initialize CNN parameters <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and approximate posterior parameters <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">Ï</mml:mi></mml:mstyle></mml:math></inline-formula><break/><bold>while</bold> <italic>not converged</italic> <bold>do</bold><break/>â<inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (sample random minibatch of size <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> from full dataset)<break/>â<inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi>Î¶</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (calculate drift rate means)<break/>â<inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold">I</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>â(draw <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>L</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> samples from the standard multivariate Gaussian)<break/>ââ<inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>â</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Ïµ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Î¼</mml:mi></mml:mstyle></mml:math></inline-formula> (reparameterize)<break/>â<inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">â</mml:mo></mml:mstyle></mml:math></inline-formula>âcalculate ELBO using <xref ref-type="disp-formula" rid="equ8">Equation 6</xref><break/>â<inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">â</mml:mo><mml:msub><mml:mi mathvariant="normal">â</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>(calculate gradients using AD)<break/>â<inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Î¶</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">â</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> update parameters using the Adam optimizer and <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula><break/><bold>end</bold></td></tr></tbody></table></table-wrap></sec><sec id="s4-2-6"><title>Convolutional neural network (CNN)</title><p>We used the same seven-layer CNN architecture for all models (six convolutional layers followed by one fully-connected hidden layer). The number of channels/units used in the seven layers were as follows: 64, 64, 128, 128, 128, 256, 1024. The output layer (after the fully-connected layer) has four channels, one for each drift rate. Each convolutional layer used a 3Ã3 pixel kernel (stride 1, same padding). Each convolutional layer was followed by a ReLU nonlinearity, instance normalization (<xref ref-type="bibr" rid="bib90">Ulyanov et al., 2017</xref>), and a max-pooling layer (2Ã2 pixel window, stride 2), in that order. The fully-connected hidden layer was followed by a ReLU nonlinearity and a dropout layer (dropout rate set to 0.5).</p><p>To speed up model training, the first two convolutional layers were initialized with the parameters from a larger CNN trained on an image classification task. Specifically, the pretrained model used a 16-layer VGG architecture and was trained on the ImageNet dataset (<xref ref-type="bibr" rid="bib14">Deng et al., 2009</xref>; <xref ref-type="bibr" rid="bib80">Simonyan and Zisserman, 2015</xref>). These first two layers were trainable (i.e. not fixed to their initial values). The weights of the other convolutional layers, fully-connected layer, and output layer were initialized randomly using the LeCun normal initializer <xref ref-type="bibr" rid="bib42">Klambauer et al., 2013</xref>; the biases were initialized with zeros.</p></sec><sec id="s4-2-7"><title>Image preprocessing and data augmentation</title><p>The image stimuli used to train the VAM differed from the original Lost in Migration stimuli in a few ways. Information about the current score and time remaining visible at the top of the game window was removed. We also used the same blue background image for all stimuli (the background in the original Lost in Migration changes over the course of the gameplay). Finally, the stimuli were resized from 640Ã480 pixels (widthÃheight) to 128Ã128 pixels.</p><p>We used data augmentation techniques commonly used in other image classification training paradigms to improve the generalization ability of the models. Specifically, for each batch of training data, each image was independently and randomly translated by a small amount. The size of the translation (in pixels) was drawn from a uniform distribution on the interval [0,1] (vertical) and [0,2] (horizontal), rounded to the nearest pixel (horizontal and vertical translations were sampled independently). We also applied a variation of a random elastic image deformation (<xref ref-type="bibr" rid="bib78">Simard et al., 2003</xref>) as implemented in the Augmax Python package (<xref ref-type="bibr" rid="bib29">Heidler, 2022</xref>). Specifically, we used the Warp transformation in Augmax and set the strength parameter to 3 and the coarseness parameter to 32. The transformation was applied to each image independently with a probability of 0.75.</p></sec></sec><sec id="s4-3"><title>Task-optimized models</title><p>The task-optimized CNN models were trained by minimizing the cross-entropy loss function, where the correct label was defined as the true direction of the target bird in each stimulus (i.e. these models were not trained to match the decisions of the human participants, but rather to output the correct target direction). Other than the loss function, all aspects of the training process were the same as those used for the VAM (CNN architecture, optimizer settings, initialization, etc.). For each VAM we trained, we trained one task-optimized model using the same training data (<italic>n</italic>=75 task-optimized models).</p></sec><sec id="s4-4"><title>Analysis methods: Overview</title><p>We analyzed all of the VAMs after 12,800 parameter update steps (corresponding to the 200<sup>th</sup> training epoch), by which point training had converged. We analyzed all of the task-optimized models after 1600 parameter update steps (the 25<sup>th</sup> training epoch) since these models converged much more quickly. All analyses were conducted using a holdout set of <italic>n</italic> = 5000 LIM stimuli/RTs/choices (i.e. data that was not used to train the models). To generate RTs and choices from the trained models, the LIM stimuli from the holdout set were provided as inputs to the CNN, which output mean drift rates for each stimulus. We denote the drift rate mean for the <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> accumulator on trial <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We used the LBA model to sample RTs and choices using these mean drift rates, where the drift rate for the <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> accumulator on trial <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, is sampled from a normal distribution with mean <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and SD = 1. For a very small fraction of trials (mean Â± s.e.m. percent excluded trials: 0.080 Â± 0.011%, <italic>n</italic> = 75 models), all of the sampled drift rates were negative, and thus the RT and choice were undefined. These trials were excluded from all analyses.</p><p>All error bars shown in the figures correspond to bootstrap 95% confidence intervals calculated using 1000 bootstrap samples.</p><sec id="s4-4-1"><title>Behavior analyses</title><p>The RT congruency effect was defined as the mean RT on incongruent trials minus the mean RT on congruent trials, where only correct trials were included in the calculation. The accuracy congruency effect was defined as the accuracy on congruent trials minus the accuracy on incongruent trials.</p><p>To calculate the RT delta plots for a given model/participant, we first calculated the RT deciles (0.1, 0.2, ..., 0.9 quantiles) separately for congruent and incongruent trials, using only correct trials. Within each RT decile, we then calculated the RT congruency effect and mean RT (average of congruent and incongruent mean RTs), forming a delta plot for that participant/model. These mean RTs and congruency effects were then averaged across participants.</p><p>To calculate the conditional accuracy functions for a given model/participant, we calculated the accuracy and mean RT of trials within each RT quintile (0.2, 0.4, 0.6, and 0.8 quantiles) separately for congruent and incongruent trials. As above, these measures were then averaged across participants.</p><p>The models/participants included in the analysis of how RT varied with stimulus layout and horizontal/vertical position were those that exhibited significant modulation of RT by one or more of these stimulus features. Significant modulation was determined by running an ANOVA on the RTs in each stimulus feature bin (e.g. for each layout or each horizontal position bin), using a threshold <italic>p</italic>-value of 0.05. For horizontal stimulus position, we used bins of width = 50 pixels in the original 640Ã480 pixel window space, except for the leftmost and rightmost bins which had width = 60 pixels. For vertical stimulus position, we used bins of width = 25 pixels.</p><p>The number of participants exhibiting significant modulation of RT was 60 for stimulus layout, 72 for horizontal position, and 69 for vertical position (out of 75 total). To determine whether a given participant exhibited significant modulation of accuracy by a given stimulus feature, we used a chi-squared test for equality of proportions across stimulus feature bins (threshold p-value=0.05). No participants exhibited significant modulation of accuracy for any of the stimulus features.</p></sec><sec id="s4-4-2"><title>LBA parameters</title><p>For analyses of the fitted LBA parameters <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <italic>A</italic>, we used the maximum a posteriori (MAP) estimates of these parameters, corresponding to the mean parameter vector of the learned Gaussian posterior density <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Î¸</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For analyses of the mean target and flanker drift rates, we provided the LIM stimuli from the holdout set as inputs to the fitted CNN, which generates the mean drift rates as its outputs. The non-target/non-flanker (other) drift rates were calculated by averaging the drift rates from the two non-target/non-flanker accumulators on incongruent trials or the three non-target accumulators on congruent trials.</p></sec><sec id="s4-4-3"><title>CNN unit activity</title><p>The activation matrices used in the analyses of CNN representations were derived from the responses of units in each layer elicited by the holdout image set. The activations were processed just after the ReLU nonlinearity. For the convolutional layers, we defined the activation of a given unit/channel as the maximum value of that channel calculated across the spatial dimensions (<xref ref-type="bibr" rid="bib80">Simonyan and Zisserman, 2015</xref>). This yields a <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> activation matrix, where <italic>N</italic> is the number of images in the holdout set (5000) and <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of active channels in layer <italic>l</italic>. For the fully-connected layer, the responses of all units were used to define the <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> activation matrix directly.</p><p>Only incongruent trials were used for all of the analyses involving the CNN activations described below, for the following reasons. For the selectivity and tolerance analyses in which we decoded target or flanker direction from the activity in each layer, described in more detail below, note that the target and flanker directions are by definition the same on congruent trials. As such, a classifier trained to decode target direction from congruent trials could achieve perfect accuracy using information in the unit activity attributable to the flankers, rather than targets. Using only incongruent trials ensured that such cross-contamination could not occur. We used only incongruent trials for all of the other analyses of the activations simply for convenience.</p></sec><sec id="s4-4-4"><title>Stimulus feature decoding</title><p>To assess how well particular stimulus features could be decoded from the activity in each layer, we trained a linear SVM to classify the values of that stimulus feature using the <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> activation matrix in each layer. The activation matrix was standardized before training the classifiers, such that each column had zero mean and unit variance. Horizontal and vertical stimulus positions were discretized to enable classification using the same bins as were used in the behavioral analyses.</p><p>The classification task was done in a standard âone-vs-restâ setting: for each value of a given stimulus feature, one sub-classifier was trained on a binary classification task with the chosen value as one class and all other values as the other class, yielding one classifier for each value of the stimulus feature (e.g. four for target direction, seven for stimulus layout). To determine the decision of the combined classifier for a given image, we generated predictions from each sub-classifier and assigned the decision to the sub-classifier with the largest (most confident) prediction. We assessed the overall decoding accuracy of each SVM on a separate test image set. Note that both the training set and test set for the SVMs were subsets of the holdout image set (i.e. the images that were not used to train the VAMs). The SVMs were trained using the LinearSVC model in the scikit-learn Python package <xref ref-type="bibr" rid="bib61">Pedregosa, 2011</xref> with the squared hinge loss function and L2 regularization with the penalty parameter C set to 1.0 (the default settings).</p></sec><sec id="s4-4-5"><title>Tolerance</title><p>To assess the tolerance of model representations to variation in a given stimulus feature (flanker direction, stimulus layout, horizontal position, and vertical position), we trained a linear SVM to classify target direction from the CNN activations using stimuli with the chosen stimulus feature fixed to one value (the training context), and assessed the generalization performance of the SVM on stimuli that contained all other values of that stimulus feature (the generalization context). For example, to assess tolerance to stimulus layout, we trained one SVM to classify target direction using stimuli with the vertical line layout, and assessed the generalization performance of that classifier on stimuli with the six other layouts. We trained one such SVM for each of the seven stimulus layouts, and averaged the generalization performance across these seven SVMs to derive the overall generalization performance measure for a given model and network layer. Other details of the classifiers were the same as those used for the decoding analyses described above.</p></sec><sec id="s4-4-6"><title>Target/flanker subspace alignment</title><p>To calculate the target/flanker subspace alignment metric, we first defined target and flanker subspaces using the SVM classifiers that we trained for the target/flanker decoding analyses. Specifically, for each of the four classifiers, which were trained to classify stimuli as a given target or flanker direction vs. all other target or flanker directions using the CNN activations from a given layer, we extracted the vector orthogonal to the decision hyperplane. In agreement with prior work (<xref ref-type="bibr" rid="bib5">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Libby and Buschman, 2021</xref>), we refer to these vectors as decoding vectors for a given target/flanker direction. Let <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mtext>flnk</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the target decoding row vector and flanker decoding row vector for the <italic>k</italic><sup>th</sup> direction, respectively. We define the matrices formed with these four decoding vectors filling the rows as the target subspace matrix <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the flanker subspace matrix <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>flnk</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. These matrices have dimensions <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>4</mml:mn><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of active units in layer <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each matrix, therefore, spans a subspace of the full <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional space. Our goal is to determine whether the target and flanker subspaces are orthogonal.</p><p>To do so, we employ principal angles between subspaces (<xref ref-type="bibr" rid="bib37">Jordan, 1873</xref>; <xref ref-type="bibr" rid="bib100">Zhu and Knyazev, 2013</xref>), which generalizes the more intuitive notion of angles between lines or planes to arbitrary dimensions. To calculate the principal angles, we require orthonormal bases for the target and flanker subspaces, which we determine using a reduced singular value decomposition (SVD):<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">Î£</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">Î£</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The rows of the <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>4</mml:mn><mml:mo>Ã</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> matrices <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>flnk</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> form an orthonormal basis for the target and flanker subspaces, respectively. The cosines of the principal angles are given by the singular values of <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mtext>flnk</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The average of these singular values is our subspace alignment metric, which ranges from zero (completely orthogonal subspaces) to one (completely aligned/parallel subspaces).</p></sec><sec id="s4-4-7"><title>Participation ratio</title><p>We measured the dimensionality of target representations for a given layer with the participation ratio (<inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext>PR</mml:mtext></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib24">Gao et al., 2017</xref>), defined as:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of active units in layer <italic>l</italic> and <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â¥</mml:mo><mml:mo>â¦</mml:mo><mml:mo>â¥</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>â¥</mml:mo><mml:mo>â¦</mml:mo><mml:mo>â¥</mml:mo><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the eigenvalues of the target-centered activation covariance matrix for layer <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The target-centered activations were obtained by subtracting the centroid of the activation matrix for each target direction from the corresponding trials in the activation matrix (<xref ref-type="bibr" rid="bib65">Rangamani et al., 2023</xref>).</p><p>The participation ratio is a continuous measure of dimensionality ranging from 1 to <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The minimum (<inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext>PR</mml:mtext></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) is obtained when all of the variance in activity is concentrated in a single dimension, such that <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>â¥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The maximum (<inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext>PR</mml:mtext></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) is obtained when the variance is evenly spread across the <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> dimensions, such that all <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> eigenvalues are equal.</p></sec><sec id="s4-4-8"><title>Mutual information</title><p>The activity of each unit in response to the holdout image set was discretized into 10 equally-sized bins, yielding a unit-specific activation distribution <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Stimulus features (horizontal/vertical position, layout, target/flanker direction) were discretized if the values were continuous, or used as is if not, yielding a stimulus feature distribution <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Discretization of the continuous variables was done as described in the decoding methods section. The joint probability mass function of the unit activity and stimulus feature is denoted by <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For a given unit and stimulus feature, the mutual information is given by:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The mutual information for a given stimulus feature was normalized by the entropy of the feature distribution to facilitate comparisons between the features <xref ref-type="bibr" rid="bib54">Muratore et al., 2022</xref>. The entropy is given by:<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-9"><title>Single-unit modulation by target direction</title><p>To identify units that were modulated by target direction, we ran a one-way ANOVA on the z-scored activity of each unit, where each group in the ANOVA was determined by the activity of that unit in response to stimuli for a given target direction (the activity of each unit was z-scored across the stimuli). Units with an ANOVA <italic>p</italic>-value&lt;0.001 were defined as significantly modulated by target direction. These units were further split into three subtypes based on their degree of selectivity and sign of modulation: selective (+), selective (-), and complex units.</p><p>We first selected units that had significantly higher or lower activation for one direction relative to the other three directions, as assessed by Tukeyâs HSD test (<italic>p</italic>&lt;0.05). Within this population, some units had both significantly higher activation for one direction relative to the other three <italic>and</italic> significantly lower activation for one direction relative to the other three. Units that did vs. did not have this property were handled separately. In the former (simpler) case, the units that only had significantly higher activation for one direction relative to the other three were defined as selective (+) units; the units that only had significantly lower activation for one direction relative to the other three were defined as selective (-) units.</p><p>In the latter (more complicated) case, we compared the magnitude of the activation for the positive and negative modulation directions with a rank-sum test. Units for which the magnitude of activity for the positive modulation direction was significantly greater (<italic>p</italic>&lt;0.05) than the magnitude of activity for the negative modulation direction were defined as selective (+) units. Analogous criteria were used to define selective (-) units (with the signs reversed). Units within this pool with rank-sum p-value&gt;0.05 were defined as complex units. Finally, units that were significantly modulated by target direction but that did not meet any of the criteria described above were also defined as complex units.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Methodology, Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Methodology, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Methodology, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Methodology, Project administration, Writing â review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98351-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All of the code (<ext-link ext-link-type="uri" xlink:href="https://github.com/pauljaffe/vam">https://github.com/pauljaffe/vam</ext-link>, copy archived at <xref ref-type="bibr" rid="bib35">Jaffe, 2025</xref>) and data (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.10775513">https://doi.org/10.5281/zenodo.10775513</ext-link>) used to train the VAMs and reproduce our results are publicly-available without restrictions.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Jaffe</surname><given-names>PI</given-names></name><name><surname>Gustavo</surname><given-names>XSR</given-names></name><name><surname>Schafer</surname><given-names>RJ</given-names></name><name><surname>Bissett</surname><given-names>PG</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Data and models for 'An image-computable model of speeded decision-making'</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10775513</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank M Steyvers, M Robinson, D Yamins and members of the NeuroAILab, the Lumos Labs research team, and members of the Poldrack Lab for helpful conversations and comments on this work. We also thank A Kaluszka for providing graphics files of the Lost and Migration stimuli.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Annis</surname><given-names>J</given-names></name><name><surname>Gauthier</surname><given-names>I</given-names></name><name><surname>Palmeri</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Combining convolutional neural networks and cognitive models to predict novel object recognition in humans</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>47</volume><fpage>785</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1037/xlm0000968</pub-id><pub-id pub-id-type="pmid">33151718</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ansuini</surname><given-names>A</given-names></name><name><surname>Laio</surname><given-names>A</given-names></name><name><surname>H.Macke</surname><given-names>J</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Intrinsic Dimension of Data Representations in Deep Neural Networks</source><publisher-name>Adv Neural Inf Process Systems</publisher-name></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>N</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Erlikhman</surname><given-names>G</given-names></name><name><surname>Kellman</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep convolutional networks do not classify based on global object shape</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006613</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006613</pub-id><pub-id pub-id-type="pmid">30532273</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-David</surname><given-names>BM</given-names></name><name><surname>Eidels</surname><given-names>A</given-names></name><name><surname>Donkin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Effects of aging and distractors on detection of redundant visual targets and capacity: do older adults integrate visual targets differently than younger adults?</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e113551</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0113551</pub-id><pub-id pub-id-type="pmid">25501850</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardi</surname><given-names>S</given-names></name><name><surname>Benna</surname><given-names>MK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Munuera</surname><given-names>J</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The geometry of abstraction in the hippocampus and prefrontal cortex</article-title><source>Cell</source><volume>183</volume><fpage>954</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowers</surname><given-names>JS</given-names></name><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>DujmoviÄ</surname><given-names>M</given-names></name><name><surname>Llera Montero</surname><given-names>M</given-names></name><name><surname>Tsvetkov</surname><given-names>C</given-names></name><name><surname>Biscione</surname><given-names>V</given-names></name><name><surname>Puebla</surname><given-names>G</given-names></name><name><surname>Adolfi</surname><given-names>F</given-names></name><name><surname>Hummel</surname><given-names>JE</given-names></name><name><surname>Heaton</surname><given-names>RF</given-names></name><name><surname>Evans</surname><given-names>BD</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name><name><surname>Blything</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep problems with neural network models of human vision</article-title><source>The Behavioral and Brain Sciences</source><volume>46</volume><fpage>1</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1017/S0140525X22002813</pub-id><pub-id pub-id-type="pmid">36453586</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Frostig</surname><given-names>R</given-names></name><name><surname>Hawkins</surname><given-names>P</given-names></name><name><surname>J.Johnson</surname><given-names>M</given-names></name><name><surname>Leary</surname><given-names>C</given-names></name><name><surname>Maclaurin</surname><given-names>D</given-names></name><name><surname>Necula</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>JAX: composable transformations of python+numpy programs</data-title><version designator="a0812cd">a0812cd</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="http://github.com/google/jax">http://github.com/google/jax</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname><given-names>SL</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>von Nicolai</surname><given-names>C</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gradual progression from sensory to task-related processing in cerebral cortex</article-title><source>PNAS</source><volume>115</volume><fpage>E7202</fpage><lpage>E7211</lpage><pub-id pub-id-type="doi">10.1073/pnas.1717075115</pub-id><pub-id pub-id-type="pmid">29991597</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The simplest complete model of choice response time: linear ballistic accumulation</article-title><source>Cognitive Psychology</source><volume>57</volume><fpage>153</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2007.12.002</pub-id><pub-id pub-id-type="pmid">18243170</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Servan-Schreiber</surname><given-names>D</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>A parallel distributed processing approach to automaticity</article-title><source>The American Journal of Psychology</source><volume>105</volume><fpage>239</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.2307/1423029</pub-id><pub-id pub-id-type="pmid">1621882</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</article-title><source>IEEE Transactions on Electronic Computers</source><volume>14</volume><fpage>326</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1109/PGEC.1965.264137</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dao</surname><given-names>VH</given-names></name><name><surname>Gunawan</surname><given-names>D</given-names></name><name><surname>Tran</surname><given-names>M-N</given-names></name><name><surname>Kohn</surname><given-names>R</given-names></name><name><surname>Hawkins</surname><given-names>GE</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Efficient selection between hierarchical cognitive models: cross-validation with variational Bayes</article-title><source>Psychological Methods</source><volume>29</volume><fpage>219</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1037/met0000458</pub-id><pub-id pub-id-type="pmid">35446049</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Jong</surname><given-names>R</given-names></name><name><surname>Liang</surname><given-names>CC</given-names></name><name><surname>Lauber</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Conditional and unconditional automaticity: a dual-process model of effects of spatial stimulus-response correspondence</article-title><source>Journal of Experimental Psychology</source><volume>20</volume><fpage>731</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.20.4.731</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>LJ</given-names></name><name><surname>Kai</surname><given-names>L</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>ImageNet: a large-scale hierarchical image database</article-title><conf-name>2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops</conf-name><conf-loc>Miami, FL</conf-loc><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dezfouli</surname><given-names>A</given-names></name><name><surname>Griffiths</surname><given-names>K</given-names></name><name><surname>Ramos</surname><given-names>F</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Models that learn how humans learn: the case of decision-making and its disorders</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006903</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006903</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Weissenborn</surname><given-names>D</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Dehghani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An image is worth 16x16 words</article-title><conf-name>transformers for image recognition at scale.âInternational Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckstein</surname><given-names>MP</given-names></name><name><surname>Koehler</surname><given-names>K</given-names></name><name><surname>Welbourne</surname><given-names>LE</given-names></name><name><surname>Akbas</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Humans, but not deep neural networks, often miss giant targets in scenes</article-title><source>Current Biology</source><volume>27</volume><fpage>2827</fpage><lpage>2832</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.07.068</pub-id><pub-id pub-id-type="pmid">28889976</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eriksen</surname><given-names>BA</given-names></name><name><surname>Eriksen</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Effects of noise letters upon the identification of a target letter in a nonsearch task</article-title><source>Perception &amp; Psychophysics</source><volume>16</volume><fpage>143</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.3758/BF03203267</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>NJ</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Evidence accumulation models: current limitations and future directions</article-title><source>The Quantitative Methods for Psychology</source><volume>16</volume><fpage>73</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.20982/tqmp.16.2.p073</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fel</surname><given-names>T</given-names></name><name><surname>Rodriguez</surname><given-names>IF</given-names></name><name><surname>Linsley</surname><given-names>D</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Harmonizing the object recognition strategies of deep neural networks with humans</article-title><conf-name>NIPSâ22: Proceedings of the 36th International Conference on Neural Information Processing Systems</conf-name><fpage>9432</fpage><lpage>9446</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>T</given-names></name><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Orthogonal representations for robust context-dependent task performance in brains and neural networks</article-title><source>Neuron</source><volume>110</volume><fpage>1258</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.01.005</pub-id><pub-id pub-id-type="pmid">35085492</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forstmann</surname><given-names>BU</given-names></name><name><surname>Tittgemeyer</surname><given-names>M</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Derrfuss</surname><given-names>J</given-names></name><name><surname>Imperati</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The speed-accuracy tradeoff in the elderly brain: A structural model-based approach</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>17242</fpage><lpage>17249</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0309-11.2011</pub-id><pub-id pub-id-type="pmid">22114290</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A theory of multineuronal dimensionality, dynamics and measurement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/214262</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goetschalckx</surname><given-names>L</given-names></name><name><surname>Govindarajan</surname><given-names>L</given-names></name><name><surname>Ashok</surname><given-names>AK</given-names></name><name><surname>Ahuja</surname><given-names>A</given-names></name><name><surname>Sheinberg</surname><given-names>D</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Computing a human-like reaction time metric from stable recurrent vision models</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>14338</fpage><lpage>14365</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottsdanker</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Age and simple reaction time</article-title><source>Journal of Gerontology</source><volume>37</volume><fpage>342</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1093/geronj/37.3.342</pub-id><pub-id pub-id-type="pmid">7069159</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>GÃ¼Ã§lÃ¼</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gunawan</surname><given-names>D</given-names></name><name><surname>Hawkins</surname><given-names>GE</given-names></name><name><surname>Tran</surname><given-names>M-N</given-names></name><name><surname>Kohn</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>New estimation approaches for the hierarchical linear ballistic accumulator model</article-title><source>Journal of Mathematical Psychology</source><volume>96</volume><elocation-id>102368</elocation-id><pub-id pub-id-type="doi">10.1016/j.jmp.2020.102368</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Heidler</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Augmax</data-title><version designator="ca1249d">ca1249d</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/khdlr/augmax">https://github.com/khdlr/augmax</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hohman</surname><given-names>F</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Robinson</surname><given-names>C</given-names></name><name><surname>Polo Chau</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Summit: scaling deep learning interpretability by visualizing activation and attribution summarizations</article-title><source>IEEE Transactions on Visualization and Computer Graphics</source><volume>26</volume><fpage>1096</fpage><lpage>1106</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2019.2934659</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>WR</given-names></name><name><surname>OâDaniels</surname><given-names>P</given-names></name><name><surname>Trueblood</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A joint deep neural network and evidence accumulation modeling approach to human decision-making with naturalistic images</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1007/s42113-019-00042-1</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>CP</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title><source>Science</source><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1126/science.1117593</pub-id><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>RA</given-names></name><name><surname>Bates</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Comparing the visual representations and performance of humans and deep neural networks</article-title><source>Current Directions in Psychological Science</source><volume>28</volume><fpage>34</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1177/0963721418801342</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaffe</surname><given-names>PI</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Schafer</surname><given-names>RJ</given-names></name><name><surname>Bissett</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Modelling human behaviour in cognitive tasks with latent dynamical systems</article-title><source>Nature Human Behaviour</source><volume>7</volume><fpage>986</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01510-8</pub-id><pub-id pub-id-type="pmid">36658212</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jaffe</surname><given-names>PI</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Vam</data-title><version designator="swh:1:rev:22cff4702d19f3ffd7a64f9fce7bac29cea42ca0">swh:1:rev:22cff4702d19f3ffd7a64f9fce7bac29cea42ca0</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:4052c8b832b58f1212bd02cf153f320985988c3b;origin=https://github.com/pauljaffe/vam;visit=swh:1:snp:ce6cdaf0437d63d7f928b54e8ec8f2ba446ec708;anchor=swh:1:rev:22cff4702d19f3ffd7a64f9fce7bac29cea42ca0">https://archive.softwareheritage.org/swh:1:dir:4052c8b832b58f1212bd02cf153f320985988c3b;origin=https://github.com/pauljaffe/vam;visit=swh:1:snp:ce6cdaf0437d63d7f928b54e8ec8f2ba446ec708;anchor=swh:1:rev:22cff4702d19f3ffd7a64f9fce7bac29cea42ca0</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>A</given-names></name><name><surname>Peterson</surname><given-names>JC</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Extracting low-dimensional psychological representations from convolutional neural networks</article-title><source>Cognitive Science</source><volume>47</volume><elocation-id>e13226</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.13226</pub-id><pub-id pub-id-type="pmid">36617318</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1873">1873</year><article-title>Essai Sur La GÃ©omÃ©trie Ã  n dimensions</article-title><source>Bulletin de La SociÃ©tÃ© MathÃ©matique de France</source><volume>2</volume><fpage>103</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.24033/bsmf.90</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical activity in the null space: permitting preparation without movement</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3643</pub-id><pub-id pub-id-type="pmid">24487233</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auto-encoding variational bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Salimans</surname><given-names>T</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variational dropout and the local reparameterization trick</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.02557">https://arxiv.org/abs/1506.02557</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Klambauer</surname><given-names>G</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Mayr</surname><given-names>A</given-names></name><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Self-normalizing neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koren</surname><given-names>V</given-names></name><name><surname>Andrei</surname><given-names>AR</given-names></name><name><surname>Hu</surname><given-names>M</given-names></name><name><surname>Dragoi</surname><given-names>V</given-names></name><name><surname>Obermayer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pairwise synchrony and correlations depend on the structure of the population code in visual cortex</article-title><source>Cell Reports</source><volume>33</volume><elocation-id>108367</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2020.108367</pub-id><pub-id pub-id-type="pmid">33176154</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kucukelbir</surname><given-names>A</given-names></name><name><surname>Tran</surname><given-names>D</given-names></name><name><surname>Ranganath</surname><given-names>R</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Blei</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automatic differentiation variational inference</article-title><source>Journal of Machine Learning Research</source><volume>18</volume><fpage>430</fpage><lpage>474</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kumbhar</surname><given-names>O</given-names></name><name><surname>Sizikova</surname><given-names>E</given-names></name><name><surname>Majaj</surname><given-names>N</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Anytime prediction as a model of human reaction time</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.12859">https://arxiv.org/abs/2011.12859</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Libby</surname><given-names>A</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rotational dynamics reduce interference between sensory and memory representations</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>715</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00821-9</pub-id><pub-id pub-id-type="pmid">33821001</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Convolutional neural networks as a model of the visual system: past, present, and future</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2017</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01544</pub-id><pub-id pub-id-type="pmid">32027584</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Linsley</surname><given-names>D</given-names></name><name><surname>Eberhardt</surname><given-names>S</given-names></name><name><surname>Sharma</surname><given-names>T</given-names></name><name><surname>Gupta</surname><given-names>P</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What are the visual features underlying human versus machine vision?</article-title><conf-name>2017 IEEE International Conference on Computer Vision Workshop (ICCVW</conf-name><conf-loc>Venice, Italy</conf-loc><fpage>2706</fpage><lpage>2714</lpage><pub-id pub-id-type="doi">10.1109/ICCVW.2017.331</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lo</surname><given-names>CF</given-names></name><name><surname>Ip</surname><given-names>HY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Modified leaky competing accumulator model of decision making with multiple alternatives: the Lie-algebraic approach</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>10923</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-90356-7</pub-id><pub-id pub-id-type="pmid">34035348</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>DujmoviÄ</surname><given-names>M</given-names></name><name><surname>Bowers</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Feature blindness: A challenge for understanding and modelling visual object recognition</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009572</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009572</pub-id><pub-id pub-id-type="pmid">35560155</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>MLR</given-names></name><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signal multiplexing and single-neuron computations in lateral intraparietal area during decision-making</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>2254</fpage><lpage>2267</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2984-12.2013</pub-id><pub-id pub-id-type="pmid">23392657</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Muratore</surname><given-names>P</given-names></name><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Piasini</surname><given-names>E</given-names></name><name><surname>Laio</surname><given-names>A</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2205.13816">https://arxiv.org/abs/2205.13816</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarro</surname><given-names>DJ</given-names></name><name><surname>Fuss</surname><given-names>IG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fast and accurate calculations for first-passage times in Wiener diffusion models</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>222</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2009.02.003</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>J.DiCarlo</surname><given-names>J</given-names></name><name><surname>K.Yamins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task-driven convolutional recurrent models of the visual system</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.00053">https://arxiv.org/abs/1807.00053</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nettelbeck</surname><given-names>T</given-names></name><name><surname>Rabbitt</surname><given-names>PMA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Aging, cognitive performance, and mental</article-title><source>Intelligence</source><volume>16</volume><fpage>189</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/0160-2896(92)90004-B</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Urban</surname><given-names>LS</given-names></name><name><surname>Wohl</surname><given-names>MP</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1132</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1038/nn.3433</pub-id><pub-id pub-id-type="pmid">23792943</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shared mechanisms underlie the control of working memory and attention</article-title><source>Nature</source><volume>592</volume><fpage>601</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id><pub-id pub-id-type="pmid">33790467</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papyan</surname><given-names>V</given-names></name><name><surname>Han</surname><given-names>XY</given-names></name><name><surname>Donoho</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prevalence of neural collapse during the terminal phase of deep learning training</article-title><source>PNAS</source><volume>117</volume><fpage>24652</fpage><lpage>24663</lpage><pub-id pub-id-type="doi">10.1073/pnas.2015509117</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pratte</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Eriksen flanker delta plot shapes depend on the stimulus</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>83</volume><fpage>685</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.3758/s13414-020-02166-0</pub-id><pub-id pub-id-type="pmid">33155124</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rafiei</surname><given-names>F</given-names></name><name><surname>Shekhar</surname><given-names>M</given-names></name><name><surname>Rahnev</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>The neural network RTNet exhibits the signatures of human perceptual decision-making</article-title><source>Nature Human Behaviour</source><volume>8</volume><fpage>1752</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41562-024-01914-8</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7255</fpage><lpage>7269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0388-18.2018</pub-id><pub-id pub-id-type="pmid">30006365</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rangamani</surname><given-names>A</given-names></name><name><surname>Lindegaard</surname><given-names>M</given-names></name><name><surname>Galanti</surname><given-names>T</given-names></name><name><surname>A.Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Feature learning in deep classifiers through intermediate neural collapse</article-title><conf-name>ICMLâ23: Proceedings of the 40th International Conference on Machine Learning</conf-name><fpage>28729</fpage><lpage>28745</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for two-choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Thapar</surname><given-names>A</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The effects of aging on reaction time in a signal detection task</article-title><source>Psychology and Aging</source><volume>16</volume><fpage>323</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1037/0882-7974.16.2.323</pub-id><pub-id pub-id-type="pmid">11405319</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rezende</surname><given-names>DJ</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</source><publisher-name>ACM Digital Library</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ridderinkhof</surname><given-names>RK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Micro- and macro-adjustments of task set: activation and suppression in conflict tasks</article-title><source>Psychological Research</source><volume>66</volume><fpage>312</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1007/s00426-002-0104-7</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ridderinkhof</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>Activation and suppression in conflict tasks: empirical clarification through distributional analyses</chapter-title><person-group person-group-type="editor"><name><surname>Ridderinkhof</surname><given-names>KR</given-names></name></person-group><source>Common Mechanisms in Perception and Action: Attention and Performance XIX</source><publisher-name>Oxford University Press</publisher-name><fpage>494</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1093/oso/9780198510697.003.0024</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritz</surname><given-names>H</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Orthogonal neural encoding of targets and distractors supports multivariate cognitive control</article-title><source>Nature Human Behaviour</source><volume>8</volume><fpage>945</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1038/s41562-024-01826-7</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Dicarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Selectivity and tolerance (âinvarianceâ) both increase as visual information propagates from cortical area V4 to IT</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>12978</fpage><lpage>12995</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0179-10.2010</pub-id><pub-id pub-id-type="pmid">20881116</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>CA</given-names></name><name><surname>Nosofsky</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Training deep networks to construct a psychological feature space for a natural-object category domain</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>229</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1007/s42113-020-00073-z</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Servant</surname><given-names>M</given-names></name><name><surname>Evans</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A diffusion model analysis of the effects of aging in the Flanker Task</article-title><source>Psychology and Aging</source><volume>35</volume><fpage>831</fpage><lpage>849</lpage><pub-id pub-id-type="doi">10.1037/pag0000546</pub-id><pub-id pub-id-type="pmid">32658539</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simard</surname><given-names>PY</given-names></name><name><surname>Steinkraus</surname><given-names>D</given-names></name><name><surname>Platt</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Best practices for convolutional neural networks applied to visual document analysis</article-title><conf-name>Seventh International Conference on Document Analysis and Recognition</conf-name><conf-loc>Edinburgh, UK</conf-loc><fpage>958</fpage><lpage>963</lpage><pub-id pub-id-type="doi">10.1109/ICDAR.2003.1227801</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Effect of an auditory stimulus on the processing of a visual stimulus under single- and dual-tasks conditions</article-title><source>Acta Psychologica</source><volume>51</volume><fpage>61</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(82)90019-1</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008215</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008215</pub-id><pub-id pub-id-type="pmid">33006992</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steyvers</surname><given-names>M</given-names></name><name><surname>Hawkins</surname><given-names>GE</given-names></name><name><surname>Karayanidis</surname><given-names>F</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A large-scale analysis of task switching practice effects across the lifespan</article-title><source>PNAS</source><volume>116</volume><fpage>17735</fpage><lpage>17740</lpage><pub-id pub-id-type="doi">10.1073/pnas.1906788116</pub-id><pub-id pub-id-type="pmid">31427513</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoffels</surname><given-names>EJ</given-names></name><name><surname>van der Molen</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Effects of visual and auditory noise on visual choice reaction time in a continuous-flow paradigm</article-title><source>Perception &amp; Psychophysics</source><volume>44</volume><fpage>7</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.3758/bf03207468</pub-id><pub-id pub-id-type="pmid">3405732</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroop</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1935">1935</year><article-title>Studies of interference in serial verbal reactions</article-title><source>Journal of Experimental Psychology</source><volume>18</volume><fpage>643</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1037/h0054651</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds A naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Safaai</surname><given-names>H</given-names></name><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Vanzella</surname><given-names>W</given-names></name><name><surname>Riggi</surname><given-names>M</given-names></name><name><surname>Buffolo</surname><given-names>F</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22794</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22794</pub-id><pub-id pub-id-type="pmid">28395730</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>JET</given-names></name><name><surname>Shekhar</surname><given-names>S</given-names></name><name><surname>Taylor</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural response time analysis: explainable artificial intelligence using only a stopwatch</article-title><source>Applied AI Letters</source><volume>2</volume><elocation-id>ail2.48</elocation-id><pub-id pub-id-type="doi">10.1002/ail2.48</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trueblood</surname><given-names>JS</given-names></name><name><surname>Eichbaum</surname><given-names>Q</given-names></name><name><surname>Seegmiller</surname><given-names>AC</given-names></name><name><surname>Stratton</surname><given-names>C</given-names></name><name><surname>OâDaniels</surname><given-names>P</given-names></name><name><surname>Holmes</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Disentangling prevalence induced biases in medical image decision-making</article-title><source>Cognition</source><volume>212</volume><elocation-id>104713</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2021.104713</pub-id><pub-id pub-id-type="pmid">33819847</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulrich</surname><given-names>R</given-names></name><name><surname>SchrÃ¶ter</surname><given-names>H</given-names></name><name><surname>Leuthold</surname><given-names>H</given-names></name><name><surname>Birngruber</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatic and controlled stimulus processing in conflict tasks: superimposed diffusion processes and delta functions</article-title><source>Cognitive Psychology</source><volume>78</volume><fpage>148</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2015.02.005</pub-id><pub-id pub-id-type="pmid">25909766</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ulyanov</surname><given-names>D</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Instance normalization: the missing ingredient for fast stylization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1607.08022">https://arxiv.org/abs/1607.08022</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title><source>Psychological Review</source><volume>108</volume><fpage>550</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.108.3.550</pub-id><pub-id pub-id-type="pmid">11488378</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Wildenberg</surname><given-names>WPM</given-names></name><name><surname>Wylie</surname><given-names>SA</given-names></name><name><surname>Forstmann</surname><given-names>BU</given-names></name><name><surname>Burle</surname><given-names>B</given-names></name><name><surname>Hasbroucq</surname><given-names>T</given-names></name><name><surname>Ridderinkhof</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>To head or to heed? beyond the surface of selective action inhibition: a review</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>222</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00222</pub-id><pub-id pub-id-type="pmid">21179583</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible timing by temporal scaling of cortical responses</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>102</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0028-6</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>CN</given-names></name><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Starns</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Diffusion models of the flanker task: discrete versus gradual attentional selection</article-title><source>Cognitive Psychology</source><volume>63</volume><fpage>210</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2011.08.001</pub-id><pub-id pub-id-type="pmid">21964663</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Min</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title><source>Science</source><volume>375</volume><fpage>632</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1126/science.abm0204</pub-id><pub-id pub-id-type="pmid">35143322</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0310-2</pub-id><pub-id pub-id-type="pmid">30643294</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visualizing and understanding convolutional networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</ext-link></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>P</given-names></name><name><surname>Knyazev</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Angles between subspaces and their tangents</article-title><source>Journal of Numerical Mathematics</source><volume>21</volume><elocation-id>e0013</elocation-id><pub-id pub-id-type="doi">10.1515/jnum-2013-0013</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98351.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Radboud University Nijmegen</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study presents an original and promising approach to combine convolutional neural networks of visual processing with evidence accumulation models of decision-making. While the methodological approach is technically sophisticated and the evidence is <bold>solid</bold>, there is still a gap between the model and the behavioral data. The study will be of interest to researchers working in the fields of machine learning and cognitive modeling.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98351.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper introduces a new approach for modeling human behavioral responses using image-computable models. They create a model (VAM) that is a combination of a standard CNN coupled with a standard evidence accumulation model (EAM). The combined model is then trained directly on image-level data using human behavioral responses. This approach is original and can have wide applicability. However, many of the specific findings reported are less compelling.</p><p>Strengths:</p><p>(1) The manuscript presents an original approach of fitting an image-computable model to human behavioral data. This type of approach is sorely needed in the field.</p><p>(2) The analyses are very technically sophisticated.</p><p>(3) The behavioral data are large both in terms of sample size (N=75) and in terms of trials per subject.</p><p>Weaknesses:</p><p>(1) The main advance here thus appears to be methodological rather than conceptual. It's really cool that VAMs are image computable and are also fit to human data. But what we learn about the mind or brain is perhaps more modest.</p><p>(2) In the approach here, a given stimulus is always processed in the same way through the core CNN to produce activations v_k. These v_k's are then corrupted by Gaussian noise to produce drift rates d_k, which can differ from trial to trial even for the same stimulus. In other words, the assumption built into VAM appears to be that the drift rate variability stems entirely from post-sensory (decisional) noise. In contrast, the typical interpretation of EAMs is that the variability in drift rates is sensory. In response to this concern, the authors responded that one can imagine an additional (unmodeled) sensory process that adds variability to the drift rates. However, this process remains unmodeled. The authors motivate their paper by saying &quot;EAMs do not explain how the visual system extracts these representations in the first place&quot; (second sentence of the Abstract). VAM is definitely a step in this direction but there's still a gap between the current VAM implementation and sensory systems.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98351.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In An image-computable model of speeded decision-making, the authors introduce and fit a combined CCN-EAM (a 'VAM') to flanker-task-like data. They show that the VAM can fit mean RTs and accuracies as well as the congruency effect that is present in the data, and subsequently analyze the VAM in terms of where in the network congruency effects arise.</p><p>I have mixed feelings about this manuscript, as I appreciate the innovative efforts to combine CNNs with EAMs in a new class of cognitive models, while also having some reservations from an EAM perspective. The idea of combining these approaches has great potential, and I'm excited to see where this research will lead. However, I do have some concerns about the quality of fit between the behavioral data and the model. Specifically, the RT distributions, delta plots, and conditional accuracy function don't appear to be well-matched by the VAM. The conflict effects on behavioral data are well-established and typically considered crucial to understanding the underlying cognitive process. Unfortunately, it seems that these parts of the data don't fit well with the proposed model.</p><p>This disparity is not entirely surprising. The EAM literature suggests that LBA models might not be suitable for conflict tasks, and the presented results seem to confirm this concern. Conflict EAMs, including the DMC (e.g., Ulrich et al., 2015; Evans &amp; Servant, 2022; Lee &amp; Sewell 2024), propose dynamic drift rates with a fast automatic process that is gradually withdrawn from evidence accumulation over time. This approach results in congruency effects arising from temporal dynamics, not spatial representations.</p><p>In contrast, the VAM imposes static drift rates in the LBA model, leading to an effect between drift rates that translates to changes in representations. However, this account does not adequately explain the behavioral data, and the proposed representational geometry explanation is therefore limited.</p><p>My concerns are addressed in the revised manuscript, but I struggle to understand why the authors distinguish between explaining mean effects across individuals and congruency effects within individuals. These concepts seem related, and issues at the individual level could propagate to the group mean. Furthermore, I find it challenging to accept that dynamics merely act 'in concert' with the orthogonalization mechanism, as it seems possible that an account that uses a time-varying EAM may not require any orthogonalization mechanism in the first place. The orthogonalization mechanism might have arisen because the model does not have the possibility to account for the conflict effect from temporal effects, instead of spatial effects. I could envision a CNN-DMC in which conflict effects arise only at the level of the choice model (e.g., as a time-varying filter that changes which information is read out from the visual system, rather than due to changes in the representations in the visual system itself). This possibility should be acknowledged in the paper, and it would be interesting to discuss how such an account would be tested.</p><p>While I appreciate the technological advancement presented in this paper, my concerns are not about implementation details but rather about the choice of models and their consequences. I believe that a more in-depth exploration of which conclusions can be drawn, and which model comparisons would be required to reach a final conclusion.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98351.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jaffe</surname><given-names>Paul I</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Santiago-Reyes</surname><given-names>Gustavo X</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford, CA</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Schafer</surname><given-names>Robert J</given-names></name><role specific-use="author">Author</role><aff><institution>Lumos Labs</institution><addr-line><named-content content-type="city">San Francisco, CA</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Bissett</surname><given-names>Patrick G</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Poldrack</surname><given-names>Russell A</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>This paper introduces a new approach to modeling human behavioral responses using image-computable models. They create a model (VAM) that is a combination of a standard CNN coupled with a standard evidence accumulation model (EAM). The combined model is then trained directly on image-level data using human behavioral responses. This approach is original and can have wide applicability. However, many of the specific findings reported are less compelling.</p><p>Strengths:</p><p>(1) The manuscript presents an original approach to fitting an image-computable model to human behavioral data. This type of approach is sorely needed in the field.</p><p>(2) The analyses are very technically sophisticated.</p><p>(3) The behavioral data are large both in terms of sample size (N=75) and in terms of trials per subject.</p><p>Weaknesses:</p><p>Major</p><p>(1) The manuscript appears to suggest that it is the first to combine CNNs with evidence accumulation models (EAMs). However, this was done in a 2022 preprint</p><p>(<ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.23.505015v1">https://www.biorxiv.org/content/10.1101/2022.08.23.505015v1</ext-link>) that introduced a network called RTNet. This preprint is cited here, but never really discussed. Further, the two unique features of the current approach discussed in lines 55-60 are both present to some extent in RTNet. Given the strong conceptual similarity in approach, it seems that a detailed discussion of similarities and differences (of which there are many) should feature in the Introduction.</p></disp-quote><p>Thanks for pointing this outâwe agree that the novel contributions of our model (the VAM) with respect to prior related models (including RTNet) should be clarified, and have revised the Introduction accordingly. We include the following clarifications in the Introduction:</p><p>âThe key feature of the VAM that distinguishes it from prior models is that the CNN and EAM parameters are jointly fitted to the RT, choice, and visual stimulus data from individual participants in a unified Bayesian framework. Thus, both the visual representations learned by the CNN and the EAM parameters are directly constrained by behavioral data. In contrast, prior models first optimize the CNN to perform the behavioral task, then separately fit a minimal set of high-level CNN parameters [RTNet, Rafiei et al., 2024] and/or the EAM parameters to behavioral data [Annis et al., 2021; Holmes et al., 2020; Trueblood et al., 2021]. As we will show, fitting the CNN with human dataârather than optimizing the model to perform a taskâhas significant consequences for the representations learned by the model.â</p><p>E.g. in the case of RTNet, the variability of the Bayesian CNN weight distribution, the decision threshold, and the magnitude of the noise added to the images are adjusted to match the average human accuracy (separately for each task condition). RTNet is an interesting and useful model that we believe has complementary strengths to our own work.</p><p>Since there are several other existing models in addition to the VAM and RTNet that use CNNs to generate RTs or RT proxies (by our count, at least six that we cite earlier in the Introduction), we felt it was inappropriate to preferentially include a detailed comparison of the VAM and RTNet beyond the passage quoted above.</p><disp-quote content-type="editor-comment"><p>(2) In the approach here, a given stimulus is always processed in the same way through the core CNN to produce activations v_k. These v_k's are then corrupted by Gaussian noise to produce drift rates d_k, which can differ from trial to trial even for the same stimulus. In other words, the assumption built into VAM appears to be that the drift rate variability stems entirely from post-sensory (decisional) noise. In contrast, the typical interpretation of EAMs is that the variability in drift rates is sensory. This is also the assumption built into RTNet where the core CNN produces noisy evidence. Can the authors comment on the plausibility of VAM's assumption that the noise is post-sensory?</p></disp-quote><p>In our view, the VAM is compatible with a model in which the drift rate variability for a given stimulus is due to sensory noise, since we do not specify the origin of the Gaussian noise added to the drift rates. As the reviewer notes, the CNN component of the VAM processes a given stimulus deterministically, yielding the mean drift rates. This does not preclude us from imagining an additional (unmodeled) sensory process that adds variability to the drift rates. The VAM simply represents this and other hypothetical sources of variability as additive Gaussian noise. We agree however that it is worthwhile to think about the origin of the drift rate variability, though it is not a focus of our work.</p><disp-quote content-type="editor-comment"><p>(3) Figure 2 plots how well VAM explains different behavioral features. It would be very useful if the authors could also fit simple EAMs to the data to clarify which of these features are explainable by EAMs only and which are not.</p></disp-quote><p>In our view, fitting simple EAMs to the data would not be especially informative and poses a number of challenges for the particular task we study (LIM) that are neatly avoided by using the VAM. In particular, as we show in Figure 2, the stimuli vary along several dimensions that all appear to influence behavior: horizontal position, vertical position, layout, target direction, and flanker direction. Since the VAM is stimulus-computable, fitting the VAM automatically discovers how all of these stimulus features influence behavior (via their effect on the drift rates outputted by the CNN). In contrast, fitting a simple EAM (e.g. the LBA model) necessitates choosing a particular parameterization that specifies the relationship between all of the stimulus features and the EAM model parameters. This raises a number of practical questions. For example, should we attempt to fit a separate EAM for each stimulus feature, or model all stimulus features simultaneously?</p><p>Moreover, while we could in principle navigate these issues and fit simple EAMs to the data, we do not intend to claim that simple EAMs fail to explain the relationship between stimulus features and behavior as well as the VAM. Rather, the key strength of the VAM relative to simple EAMs is that it includes a detailed and biologically plausible model of human vision. The majority of the paper capitalizes on this strength by showing how behavioral effects of interest (namely congruency effects) can be explained in terms of the VAMâs visual representations.</p><disp-quote content-type="editor-comment"><p>(4) VAM is tested in two different ways behaviorally. First, it is tested to what extent it captures individual differences (Figure 2B-E). Second, it is tested to what extent it captures average subject data (Figure 2F-J). It wasn't clear to me why for some metrics only individual differences are examined and for other metrics only average human data is examined. I think that it will be much more informative if separate figures examine average human data and individual difference data. I think that it's especially important to clarify whether VAM can capture individual differences for the quantities plotted in Figures 2F-J.</p></disp-quote><p>We would like to clarify that Fig. 2J in fact already shows how well the VAM captures individual differences for the average subject data shown in Fig. 2H (stimulus layout) and Fig. 2I (stimulus position). For a given participant and stimulus feature, we calculated the Pearson's r between model/participant mean RTs across each stimulus feature value. Fig. 2J shows the distribution of these Pearsonâs r values across all participants for stimulus layout and horizontal/vertical position.</p><p>Fig. 2G also already shows how well the VAM captures individual differences in behavior. Specifically, this panel shows individual differences in mean RT attributable to differences in age. For Fig. 2F, which shows how the model drift rates differ on congruent vs. incongruent trials, there is no sensible way to compare the models to the participants at any level of analysis (since the participants do not have drift rates).</p><disp-quote content-type="editor-comment"><p>(5) The authors look inside VAM and perform many exploratory analyses. I found many of these difficult to follow since there was little guidance about why each analysis was conducted. This also made it difficult to assess the likelihood that any given result is robust and replicable. More importantly, it was unclear which results are hypothesized to depend on the VAM architecture and training, and which results would be expected in performance-optimized CNNs. The authors train and examine performance-optimized CNNs later, but it would be useful to compare those results to the VAM results immediately when each VAM result is first introduced.</p></disp-quote><p>Thanks for pointing this outâwe apologize for any confusion caused by our presentation of the CNN analyses. We have added in additional motivating statements, methodological clarifications, and relevant references to our Results, particularly for Figure 3 in which we first introduce the analyses of the CNN representations/activity. In general, each analysis is prefaced by a guiding question or specific rationale, e.g. âHow do the models' visual representations enable target selectivity for stimuli that vary along several irrelevant dimensions?â We also provide numerous references in which these analysis techniques have been used to address similar questions in CNNs or the primate visual cortex.</p><p>We chose to maintain the current organization of our results in which the comparison between the VAM and the task-optimized models are presented in a separate figure. We felt that including analyses of both the VAM and task-optimized models in the initial analyses of the CNN representations would be overwhelming for many readers. As the reviewer acknowledges, some readers may already find these results challenging to follow.</p><disp-quote content-type="editor-comment"><p>(6) The authors don't examine how the task-optimized models would produce RTs. They say in lines 371-2 that they &quot;could not examine the RT congruency effect since the task-optimized models do not generate RTs.&quot; CNNs alone don't generate RTs, but RTs can easily be generated from them using the same EAM add-on that is part of VAM. Given that the CNNs are already trained, I can't see a reason why the authors can't train EAMs on top of the already trained CNNs and generate RTs, so these can provide a better comparison to VAM.</p></disp-quote><p>We appreciate this suggestion, but we judge the suggestion to âtrain EAMs on top of the already trained CNNs and generate RTsâ to be a significant expansion of the scope of the paper with multiple possible roads forward. In particular, one must specify how the outputs of the task-optimized CNN (logits for each possible response) relate to drift rates, and there is no widely-accepted or standard way to do this. Previously proposed methods include transforming representation distances in the last layer to drift rates (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/xlm0000968">https://doi.org/10.1037/xlm0000968</ext-link>), fitting additional subject-specific parameters that map the logits to drift rates</p><p>(<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s42113-019-00042-1">https://doi.org/10.1007/s42113-019-00042-1</ext-link>), or using the softmax-scored model outputs as drift rates directly (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-024-01914-8">https://doi.org/10.1038/s41562-024-01914-8</ext-link>), though in the latter case the RTs are not on the same scale as human data. In our view, evaluating these different methods is beyond the scope of this paper. An advantage of the VAM is that one does not have to fit two separate models (a CNN and a EAM) to generate RTs.</p><p>Nonetheless, we agree that it would be informative to examine something like RTs in the task-optimized models. Our revised Results section now includes an analysis of the confidence of the task-optimized modelsâ decisions, which we use a proxy for RTs:</p><p>âSince the task-optimized models do not generate RTs, it is not possible to directly measure RT congruency effects in these models without making additional assumptions about how the CNN's classification decisions relate to RTs. However, as a coarse proxy for RT, we can examine the confidence of the CNN's decisions, defined as the softmax-scored logit (probability) of the most probable direction in the final CNN layer. This choice of RT proxy is motivated by some prior studies that have combined CNNs with EAMs [Annis et al., 2021; Holmes et al., 2020; Trueblood et al., 2021]. These studies explicitly or implicitly derive a measure of decision confidence from the activity of the last CNN layer. The confidence measure is then mapped to the EAM drift rates, such that greater decision confidence generally corresponds to higher drift rates (and therefore shorter RTs).</p><p>We calculated the average confidence of each task-optimized CNN separately for congruent vs. incongruent trials. On average, the task-optimized models showed higher confidence on congruent vs. incongruent trials (W = 21.0, p &lt; 1e-3, Wilcoxon signed-rank test; Cohen's d = 0.99; n = 75 models). These analyses therefore provide some evidence that task-optimized CNNs have the capacity to exhibit congruency effects, though an explicit comparison of the magnitude of these effects with human data requires additional modeling assumptions (e.g., fitting a separate EAM).â</p><disp-quote content-type="editor-comment"><p>(7) The Discussion felt very long and mostly a summary of the Results. I also couldn't shake the feeling that it had many just-so stories related to the variety of findings reported. I think that the section should be condensed and the authors should be clearer about which explanations are speculations and which are air-tight arguments based on the data.</p></disp-quote><p>We have shortened the Discussion modestly and we have added in some clarifying language to help clarify which arguments are more speculative vs. directly supported by our data.</p><p>Specifically, we added in the phrase âwe speculate thatâ¦â for two suggestions in the Discussion (paragraphs 3 and 5), and we ensured that any other more speculative suggestions contain such clarifying language. We have also added in subheadings in the Discussion to help readers navigate this section.</p><disp-quote content-type="editor-comment"><p>(8) In one of the control analyses, the authors train different VAMs on each RT quantile. I don't understand how it can be claimed that this approach can serve as a model of an individual's sensory processing. Which of the 5 sets of weights (5 VAMs) captures a given subject's visual processing? Are the authors saying that the visual system of a given subject changes based on the expected RT for a stimulus? I feel like I'm missing something about how the authors think about these results.</p></disp-quote><p>We agree that these particular analyses may cause confusion and have removed them from our revised manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>In an image-computable model of speeded decision-making, the authors introduce and fit a combined CCN-EAM (a 'VAM') to flanker-task-like data. They show that the VAM can fit mean RTs and accuracies as well as the congruency effect that is present in the data, and subsequently analyze the VAM in terms of where in the network congruency effects arise.</p><p>Overall, combining DNNs and EAMs appears to be a promising avenue to seriously model the visual system in decision-making tasks compared to the current practice in EAMs. Some variants have been proposed or used before (e.g., doi.org/10.1016/j.neuroimage.2017.12.078 , doi.org/10.1007/s42113-019-00042-1), but always in the context of using task-trained models, rather than models trained on behavioral data. However, I was surprised to read that the authors developed their model in the context of a conflict task, rather than a simpler perceptual decision-making task. Conflict effects in human behavior are particularly complex, and thereby, the authors set a high goal for themselves in terms of the to-be-explained human behavior. Unfortunately, the proposed VAM does not appear to provide a great account of conflict effects that are considered fundamental features of human behavior, like the shape of response time distributions, and specifically, delta plots (doi.org/10.1037/0096-1523.20.4.731). The authors argue that it is beyond the scope of the presented paper to analyze delta plots, but as these are central to studies of human conflict behavior, models that aim to explain conflict behavior will need to be able to fit and explain delta plots.</p><p>Theories on conflict often suggest that negative/positive-trending delta plots arise through the relative timing of response activation related to relevant and irrelevant information.</p><p>Accumulation for relevant and irrelevant information would, as a result, either start at different points in time or the rates vary over time. The current VAM, as a feedforward neural network model, does not appear to be able to capture such effects, and perhaps fundamentally not so: accumulation for each choice option is forced to start at the same time, and rates are a static output of the CNN.</p><p>The proposed solution of fitting five separate VAMs (one for each of five RT quantiles) is not satisfactory: it does not explain how delta plots result from the model, for the same reason that fitting five evidence accumulation models (one per RT quantile) does not explain how response time distributions arise. If, for example, one would want to make a prediction about someone's response time and choice based on a given stimulus, one would first have to decide which of the five VAMs to use, which is circular. But more importantly, this way of fitting multiple models does not explain the latent mechanism that underlies the shape of the delta plots.</p><p>As such, the extensive analyses on the VAM layers and the resulting conclusions that conflict effects arise due to changing representations across layers (e.g., &quot;the selection of task-relevant information occurs through the orthogonalization of relevant and irrelevant representations&quot;) - while inspiring, they remain hard to weigh, as they are contingent on the assumption that the VAM can capture human behavior in the conflict task, which it struggles with. That said, the promise of combining CNNs and EAMs is clearly there. A way forward could be to either adjust the proposed model so that it can explain delta plots, which would potentially require temporal dynamics and time-varying evidence accumulation rates, or perhaps to start simpler and combine CCNs-EAMs that are able to fit more standard perceptual decision-making tasks without conflict effects.</p></disp-quote><p>We thank the reviewer for their thoughtful comments on our work. However, we note that the</p><p>VAM does in fact capture the positive-trending RT delta plot observed in the participant data (Fig. S4A), though the intercepts for models/participants differ somewhat. On the other hand, the conditional accuracy functions (Fig. S4B) reveal a more pronounced difference between model and participant behavior. As the reviewer points out, capturing these effects is likely to require a model that can produce time-varying drift rates, whereas our model produces a fixed drift rate for a given stimulus. We also agree that fitting a separate VAM to each RT quantile is not a satisfactory means of addressing this limitation and have removed these analyses from our revised manuscript.</p><p>However, while we agree that accurately capturing these dynamic effects is a laudable goal, it is in our view also worthwhile to consider explanations for the mean behavioral effect (i.e. the accuracy congruency effect), which can occur independently of any consideration of dynamics. One of our main findings is that across-model variability in accuracy congruency effects is better attributed to variation in representation geometry (target/flanker subspace alignment) vs.</p><p>variation in the degree of flanker suppression. This finding does not require any consideration of dynamics to be valid at the level of explanation we pursue (across-user variability in congruency effects), but also does not preclude additional dynamic processes that could give rise to more specific error patterns. Our revised discussion now includes a section where we summarize and elaborate on these ideas:</p><p>âIt is not difficult to imagine how the orthogonalization mechanism described above, which explains variability in accuracy congruency effects across individuals, could act in concert with other dynamic processes that explain variability in congruency effects within individuals (e.g., as a function of RT). In general, any process that dynamically gates the influence of irrelevant sensory information on behavioral outputs could accomplish this, for example ramping inhibition of incorrect response activation [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2010.00222">https://doi.org/10.3389/fnhum.2010.00222</ext-link>], a shrinking attention spotlight [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cogpsych.2011.08.001">https://doi.org/10.1016/j.cogpsych.2011.08.001</ext-link>], or dynamics in neural population-level geometry [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3643">https://doi.org/10.1038/nn.3643</ext-link>]. To pursue these ideas, future work may aim to incorporate dynamics into the visual component and decision component of the VAM with recurrent CNNs [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1807.00053">https://doi.org/10.48550/arXiv.1807.00053</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2306.11582">https://doi.org/10.48550/arXiv.2306.11582</ext-link>] and the task-DyVA model [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-022-01510-8">https://doi.org/10.1038/s41562-022-01510-8</ext-link>], respectively.â</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>In this article, the authors combine a well-established choice-response time (RT) model (the Linear Ballistic Accumulator) with a CNN model of visual processing to model image-based decisions (referred to as the Visual Accumulator Model - VAM). While this is not the first effort to combine these modeling frameworks, it uses this combination of approaches uniquely.</p><p>Specifically, the authors attempt to better understand the structure of human information representations by fitting this model to behavioral (choice-RT) data from a classic flanker task. This objective is made possible by using a very large (by psychological modeling standards) industry data set to jointly fit both components of this VAM model to individual-level data. Using this approach, they illustrate (among other results) (1) how the interaction between target and flanker representations influence the presence and strength of congruency effects, (2) how the structure of representations changes (distributed versus more localized) with depth in the CNN model component, and (3) how different model training paradigms change the nature of information representations. This work contributes to the ML literature by demonstrating the value of training models with richer behavioral data. It also contributes to cognitive science by demonstrating how ML approaches can be integrated into cognitive modeling. Finally, it contributes to the literature on conflict modeling by illustrating how information representations may lead to some of the classic effects observed in this area of research.</p><p>Strengths:</p><p>(1) The data set used for this analysis is unique and is made publicly available as part of this article. Specifically, they have access to data for 75 participants with &gt;25,000 trials per participant. This scale of data/individual is unusual and is the foundation on which this research rests.</p><p>(2) This is the first time, to my knowledge, that a model combining a CNN with a choice-RT model has been jointly fit to choice-RT data at the level of individual people. This type of model combination has been used before but in a more restricted context. This joint fitting, and in particular, learning a CNN through the choice-RT modeling framework, allows the authors to probe the structure of human information representations learned directly from behavioral data.</p><p>(3) The analysis approaches used in this article are state-of-the-art. The training of these models is straightforward given the data available. The interesting part of this article (opinion of course) is the way in which they probe what CNN has learned once trained. I find their analysis of how distractor and target information interfere with each other particularly compelling as well as their demonstration that training on behavioral data changes the structure of information representations when compared to training models on standard task-optimized data.</p><p>Weaknesses:</p><p>(1) Just as the data in this article is a major strength, it is also a weakness. This type of modeling would be difficult, if not impossible to do with standard laboratory data. I don't know what the data floor would be, but collecting tens of thousands of decisions for a single person is impractical in most contexts. Thus this type of work may live in the realm of industry. I do want to re-iterate that the data for this study was made publicly available though!</p></disp-quote><p>We suspect (but have not systematically tested) that the VAMs can be fitted with substantially less data. We use data augmentation techniques (various randomized image transformations) during training to improve the generalization capabilities of the VAMs, and these methods are likely to be particularly important when training on smaller datasets. One could consider increasing the amount of image data augmentation when working with smaller datasets, or pursuing other forms of data augmentation like resampling from estimated RT distributions (see <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-022-01510-8">https://doi.org/10.1038/s41562-022-01510-8</ext-link> for an example of this). In general, we donât think that prospective users of our approach should be discouraged if they have only a few hundred trials per subject (or less) - itâs worth trying!</p><disp-quote content-type="editor-comment"><p>(2) While this article uses choice-RT data it doesn't fully leverage the richness of the RT data itself. As the authors point out, this modeling framework, the LBA component in particular, does not account for some of the more nuanced but well-established RT effects in this data. This is not a big concern given the already nice contributions of this article and it leads to an opportunity for ongoing investigation.</p></disp-quote><p>We agree that fully capturing the more nuanced behavioral effects you mention (e.g. RT delta plots and conditional accuracy functions) is a worthwhile goal for future researchâsee our response to Reviewer #2 for a more detailed discussion. ----------</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) The phrase in the Abstract &quot;convolutional neural network models of visual processing and traditional EAMs are jointly fitted&quot; made me initially believe that the two models were fitted independently. You may want to re-word to clarify.</p></disp-quote><p>We think that the phrase âjointly fittedâ already makes it clear that both the CNN and EAM parameters are estimated simultaneously, in agreement with how this term is usually used. But we have nonetheless appended some additional clarifying language to that sentence (âin a unified Bayesian frameworkâ).</p><disp-quote content-type="editor-comment"><p>(2) Lines 27-28: EAMs &quot;are the most successful and widely-used computational models of decision-making.&quot; This is only true for the specific type of decision-making examined here, namely joint modeling of choice and response times. Signal detection theory is arguably more widely-used when response times are not modeled.</p></disp-quote><p>Thanks for pointing this out - we have revised the referenced sentence accordingly.</p><disp-quote content-type="editor-comment"><p>(3) Could the authors clarify what is plotted in Figure 2F?</p></disp-quote><p>Fig. 2F shows the drift rates for the target, flanker, and âotherâ (non-target/non-flanker) accumulators averaged over trials and models for congruent vs. incongruent trials. In case this was a source of confusion, we do not show the value of the flanker drift rates on congruent trials because the flanker and target accumulators are identical (i.e. the flanker/congruent drift rates are equivalent to the target/congruent drift rates).</p><disp-quote content-type="editor-comment"><p>(4) Lines 214-7: &quot;The observation that single-unit information for target direction decreased between the fourth and final convolutional layers while population-level decoding remained high is especially noteworthy in that it implies a transition from representing target direction with specialized &quot;target neurons&quot; to a more distributed, ensemble-level code.&quot; Can the authors clarify why this is the only reasonable explanation for these results? It seems like many other explanations could be construed.</p></disp-quote><p>We have added additional clarification to this section and now use more tentative language:</p><p>âThe observation that single-unit information for target direction decreased between the fourth and final convolutional layers indicates that the units become progressively less selective for particular target directions. Since population-level decoding remained high in these layers, this suggests a transition from representing target direction with specialized &quot;target neurons&quot; to a more distributed, ensemble-level code.â</p><disp-quote content-type="editor-comment"><p>(5) Lines 372-376: &quot;Thus, simply training the model to perform the task is not sufficient to reproduce a behavioral phenomenon widely-observed in conflict tasks. This challenges a core (but often implicit) assumption of the task-optimized training paradigm, namely that to do a task well, a training model will result in model representations that are similar to those employed by humans.&quot; While I agree with the general sentiment, I feel that its application here is strange. Unless I'm missing something, in the context of the preceding sentence, the authors seem to be saying that researchers in the field expect that CNNs can produce a behavioral phenomenon (RTs) that is completely outside of their design and training. I don't think that anyone actually expects that.</p></disp-quote><p>We moved the discussion/analyses of RTs to the next paragraph. It should now be clear that this statement refers specifically to the absence of an accuracy congruency effect in the task-optimized models.</p><disp-quote content-type="editor-comment"><p>(6) Lines 387-389: &quot;As a result, the VAMs may learn richer representations of the stimuli, since a variety of stimulus features-layout, stimulus position, flanker direction-influence behavior (Figure 2).&quot; That is certainly true of tasks like this one where an optimal model would only focus on a tiny part of the image, whereas humans are distracted by many features. I'm not sure that this distractibility is the same as &quot;richer representations&quot;. When CNNs classify images based on the background, would the authors claim that they have richer representations than humans?</p></disp-quote><p>We agree that âricherâ may not be the best way to characterize these representations, and have changed it to âmore complexâ.</p><disp-quote content-type="editor-comment"><p>(7) Is it possible that drift rate d_k for each response happens to be negative on a given trial? If so, how is the decision given on such trials (since presumably none of the accumulators will ever reach the boundary)?</p></disp-quote><p>It is indeed possible for all of the drift rates to be negative, though we found that this occurred for a vanishingly small number of trials (mean Â± s.e.m. percent trials/model: 0.080 Â± 0.011%, n = 75 models), as reported in the Methods. These trials were excluded from analyses.</p><disp-quote content-type="editor-comment"><p>(8) Can the authors comment on how they chose the CNN architecture and whether they expect that different architectures will produce similar results?</p></disp-quote><p>Before establishing the seven-layer CNN architecture used throughout the paper, we conducted some preliminary experiments using other architectures that differed primarily in the number of CNN layers. We found that models with significantly fewer than seven layers typically failed to reach human-level accuracy on the task while larger models achieved human-level accuracy but (unsurprisingly) took longer to train.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors)</bold>:</p><p>- In the introduction to this paper (particularly the paragraph beginning in line 33), the authors note that EAMs have typically been used in simplified settings and that they do not provide a means to account for how people extract information from naturalistic stimuli. While I agree with this, the idea of connecting CNNs of visual processing with EAMs for a joint modeling framework has been done. I recommend looking at and referencing these two articles as well as adjusting the tenor of this part of an introduction to better reflect the current state of the literature. For full disclosure, I am one of the authors on these articles. <ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.1007/s42113-019-00042-1">https://link.springer.com/article/10.1007/s42113-019-00042-1</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0010027721001323">https://www.sciencedirect.com/science/article/abs/pii/S0010027721001323</ext-link></p></disp-quote><p>We agreeâthanks for pointing this out. The revised Introduction now discusses prior related models in more detail (including those referenced above) and better clarifies the novel contributions of our model. We specifically highlight that a novel contribution of the VAM is that âthe CNN and EAM parameters are jointly fitted to the RT, choice, and visual stimulus data from individual participants in a unified Bayesian framework.â</p><disp-quote content-type="editor-comment"><p>- The statement in lines 56-58 implies that this is the first article to glue CNNs together with EAMs. I would edit this accordingly based on the prior comment here and references provided. I will note that the second feature of the approach in this paper is still novel and really nice, namely the fact that the CNN and the EAM are jointly fitted. In the aforementioned references, the CNN is trained on the image set, and individual level Bayesian estimation was only applied to the EAM. Thus, it may be useful to highlight the joint estimation aspect of this investigation as well as how the uniqueness of the data available makes it possible.</p></disp-quote><p>Agreedâsee above.</p><disp-quote content-type="editor-comment"><p>- Figure 3c and associated text. I understand the MI analysis you are performing here, however it is difficult to interpret as it stands. In the figure, what does a MI of 0.1 mean?? Can you give some context to that scale? I do find the interpretation of the hunchback shape in lines 210-222 to be somewhat of a stretch. The discussion that precedes (lines 199-209) this is clear and convincing. Can this discussion be strengthened more? And more interpretability of Figure 3c would be helpful; entropic scales can be hard to interpret without some context or scale associated.</p></disp-quote><p>The MI analyses in Fig. 3C (and also Figs. 4C and 6E) show normalized MI, in which the raw MI has been divided by the entropy of the stimulus feature distribution. This normalization facilitates comparing the MI for different stimulus features, which is relevant for Figs. 4C and 6E. The normalized MI has a possible range of [0, 1], where 1 indicates perfect correlation between the two variables and 0 indicates complete independence. We now note in the legend of these figures that the possible normalized MI range is [0, 1], which should help with interpreting these values. Our revised results section for Fig. 3C now also includes some additional remarks on our interpretation of the hunchback shape of the MI.</p><disp-quote content-type="editor-comment"><p>- Lines 244-248 and the analyses in Figure 3 suggest a change in the behavior of the CNN around layer 4. This is just a musing, but what would happen if you just used a 4 layer CNN, or even a 3 layer? This is not just a methods question. Your analysis suggests a transition from localized to distributed information representation. Right now, the EAM only sees the output of the distributed representation. What if it saw the results the more local representations from early layers? Of course, a shallower network may just form the distributed representations earlier, but it would interesting if there were a way to tease out not just the presence of distributed vs local representations, but the utility of those to the EAM.</p></disp-quote><p>Thanks for this interesting suggestion. We did do some preliminary experiments in models with fewer layers, though we only examined the outputs of these models and did not assess their representations. We found that models with 3â5 layers generally failed to achieve human-level accuracy on the task. In principle, one could relate this observation to the representations of these models as a means of assessing the relative utility of distributed/local representations. However, there are confounding factors that one would ideally control for in order to compare models with different numbers of layers in this fashion (namely, the number of parameters).</p><disp-quote content-type="editor-comment"><p>- Section Line 359 (Task optimized models) - It would be helpful to clarify here what these task-optimized models are being trained to do. As I understand it, they are being trained to directly predict the target direction. But are you asking them to learn to predict the true target direction? Or are you training them to predict what each individual responds? I think it is the second (since you have 75 of these), but it's not clear. I looked at the methods and still couldn't get a clear description of this. Also, are you just stripping the LBA off of the end of the CNN and then essentially putting a softmax in its place? If so, it would be helpful to say so.</p></disp-quote><p>The task-optimized models were actually trained to output the true target direction in each stimulus, rather than trained to match the decisions of the human participants. We trained 75 such models since we wanted to use exactly the same stimuli as were used to train each VAM. The task-optimized CNNs were identical to those used in the VAMs, except that the outputs of the last layer were converted to softmax-scored probabilities for each direction rather than drift rates. The Results and Methods section now included additional commentary that clarifies these points.</p><disp-quote content-type="editor-comment"><p>- Line 373-376: This statement is pretty well established at this point in the similarity judgement literature. I recommend looking at and referencing <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13226">https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13226</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41562-020-00951-3">https://www.nature.com/articles/s41562-020-00951-3</ext-link> <ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.1007/s42113-020-00073-z">https://link.springer.com/article/10.1007/s42113-020-00073-z</ext-link></p></disp-quote><p>Thanks for pointing this out. For reference, the statement in question is âThus, simply training the model to perform the task is not sufficient to reproduce a behavioral phenomenon widely-observed in conflict tasks. This challenges a core (but often implicit) assumption of the task-optimized training paradigm, namely that training a model to do a task well will result in model representations that are similar to those employed by humans.â</p><p>We agree that the first and third reference you mention are relevant, and we now cite them along with some other relevant work. In our view, the second reference you mention is not particularly relevant (that paper introduces a new computational model for similarity judgements that is fit to human data, but does not comment on training models to perform tasks vs. fitting to human data).</p><disp-quote content-type="editor-comment"><p>- Line 387-388: &quot;VAMs may learn richer representations&quot;. This is a bit of a philosophical point, but I'll go ahead and mention it. The standard VAM does not necessarily learn &quot;richer&quot; feature representations. Rather, you are asking the VAM and task-optimized models to do different things. As a result, they learn different representations. &quot;Better&quot; or &quot;richer&quot; is in the eye of the beholder. In one view, you could view the VAM performance as sub-par since it exhibits strange artifacts (congruency effects) and the expansion of dimensionality in the VAM representations is merely a side-effect of poor performance. I'm not advocating this view, just playing devils advocate and suggesting a more nuanced discussion of the difference between the VAM and task-optimized models.</p></disp-quote><p>We agreeâthis is a great point. We have changed this statement to read âthe VAMs may learn more complex [rather than richer] representations of the stimuliâ.</p><disp-quote content-type="editor-comment"><p>- Lines 567-570: Here you discuss how the LBA backend of the VAM can't account for shrinking spotlight-like RT effects but that fitting models to different RT quantiles helps overcome this. I find this to be one of the weakest points of the paper (the whole process of fitting RT quantiles separately to begin with). This is just a limitation of the RT component of the model. This is a great paper but this is just a limitation inherent in the model. I don't see a need to qualify this limitation and think it would be better to just point out that this is a limitation of the LBA itself (be more clear that it is the LBA that is the limiting factor here) and that this leaves room for future research. From your last sentence of this paragraph, I agree that recurrent CNNs would be interesting. I will note that RNN choice-RT models are out there (though not with CNNs as part of the model).</p></disp-quote><p>We agree and have revised this section of the Discussion accordingly (see our response to Reviewer #2 for more detail). We also removed the analyses of models trained on separate RT quantiles.</p></body></sub-article></article>