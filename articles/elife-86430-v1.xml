<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86430</article-id><article-id pub-id-type="doi">10.7554/eLife.86430</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Humans parsimoniously represent auditory sequences by pruning and completing the underlying network structure</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-252205"><name><surname>Benjamin</surname><given-names>Lucas</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9578-6039</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-283855"><name><surname>Ana</surname><given-names>Fló</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3260-0559</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-296506"><name><surname>Al Roumi</surname><given-names>Fosca</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9590-080X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-171315"><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2221-9081</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Cognitive Neuroimaging Unit</institution>, <institution>CNRS ERL 9003, INSERM U992</institution>, <addr-line><named-content content-type="city">Paris-Saclay</named-content></addr-line>, <country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-28130"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Reviewing editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution>, <country>Netherlands</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>lucas.benjamin@cea.fr</email> (LB);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>02</day><month>05</month><year>2023</year></pub-date><volume>12</volume><elocation-id>e86430</elocation-id><history><date date-type="received"><day>25</day><month>01</month><year>2023</year></date><date date-type="accepted"><day>28</day><month>04</month><year>2023</year></date></history><permissions><copyright-statement>© 2023, Benjamin et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Benjamin et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86430-v1.pdf"/><abstract><p>Successive auditory inputs are rarely independent, their relationships ranging from local transitions between elements to hierarchical and nested representations. In many situations, humans retrieve these dependencies even from limited datasets. However, this learning at multiple scale levels is poorly understood. Here we used the formalism proposed by network science to study the representation of local and higher-order structures and their interaction in auditory sequences. We show that human adults exhibited biases in their perception of local transitions between elements, which made them sensitive to high-order network structures such as communities. This behavior is consistent with the creation of a parsimonious simpliﬁed model from the evidence they receive, achieved by pruning and completing relationships between network elements. This observation suggests that the brain does not rely on exact memories but on a parsimonious representation of the world. Moreover, this bias can be analytically modeled by a memory/eﬃciency trade-off. This model correctly accounts for previous ﬁndings, including local transition probabilities as well as high-order network structures, unifying sequence learning across scales. We ﬁnally propose putative brain implementations of such bias.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>695710</award-id><principal-award-recipient><name><surname>Dehaene-Lambertz</surname><given-names>Ghislaine</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave their informed consents for participation and publication and this research was approved by the Ethical research committee of Paris-Saclay University under the reference CER-Paris-Saclay-2019-063</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All Data and analysis are publicly available at https://osf.io/e8u7f/</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Lucas Benjamin</collab><collab>Ana Fló Fosca Al Roumi Ghislaine Dehaene-Lambertz</collab></person-group><year iso-8601-date="2022">2022</year><source>Data and Analysis for &quot;Humans parsimoniously represent auditory sequences by pruning and completing the underlying network structure&quot;</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/e8u7f/">https://osf.io/e8u7f/</ext-link><comment>OSF</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-86430-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>