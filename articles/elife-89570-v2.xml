<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89570</article-id><article-id pub-id-type="doi">10.7554/eLife.89570</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89570.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural correlates of perceptual similarity masking in primate V1</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-230361"><name><surname>Chen</surname><given-names>Spencer Chin-Yu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0191-7315</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-53970"><name><surname>Chen</surname><given-names>Yuzhi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-241438"><name><surname>Geisler</surname><given-names>Wilson S</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-53237"><name><surname>Seidemann</surname><given-names>Eyal</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2841-5948</contrib-id><email>eyal@austin.utexas.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Center for Perceptual Systems, University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Department of Psychology, University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Center for Theoretical and Computational Neuroscience</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Department of Neuroscience, University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vt9qd57</institution-id><institution>Department of Neurosurgery, Rutgers University</institution></institution-wrap><addr-line><named-content content-type="city">New Brunswick</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ray</surname><given-names>Supratim</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>09</day><month>04</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP89570</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-06-14"><day>14</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-07-10"><day>10</day><month>07</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.07.06.547970"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-09-18"><day>18</day><month>09</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89570.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-13"><day>13</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89570.2"/></event></pub-history><permissions><copyright-statement>© 2023, Chen et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Chen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89570-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89570-figures-v2.pdf"/><abstract><p>Visual detection is a fundamental natural task. Detection becomes more challenging as the similarity between the target and the background in which it is embedded increases, a phenomenon termed ‘similarity masking’. To test the hypothesis that V1 contributes to similarity masking, we used voltage sensitive dye imaging (VSDI) to measure V1 population responses while macaque monkeys performed a detection task under varying levels of target-background similarity. Paradoxically, we find that during an initial transient phase, V1 responses to the target are enhanced, rather than suppressed, by target-background similarity. This effect reverses in the second phase of the response, so that in this phase V1 signals are positively correlated with the behavioral effect of similarity. Finally, we show that a simple model with delayed divisive normalization can qualitatively account for our findings. Overall, our results support the hypothesis that a nonlinear gain control mechanism in V1 contributes to perceptual similarity masking.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>VSD imaging</kwd><kwd>visual detection</kwd><kwd>orientation masking</kwd><kwd>gain control model</kwd><kwd>neural population dynamics</kwd><kwd>camouflage</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY-016454</award-id><principal-award-recipient><name><surname>Seidemann</surname><given-names>Eyal</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY-024662</award-id><principal-award-recipient><name><surname>Seidemann</surname><given-names>Eyal</given-names></name><name><surname>Geisler</surname><given-names>Wilson S</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>BRAIN Initiative</institution></institution-wrap></funding-source><award-id>U01-NS099720</award-id><principal-award-recipient><name><surname>Seidemann</surname><given-names>Eyal</given-names></name><name><surname>Geisler</surname><given-names>Wilson S</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000185</institution-id><institution>Defense Advanced Research Projects Agency</institution></institution-wrap></funding-source><award-id>DARPA-NESD0-N66001-17-C-4012</award-id><principal-award-recipient><name><surname>Seidemann</surname><given-names>Eyal</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Optical imaging from macaque monkeys performing a detection task reveals the contribution of neural populations in V1 to the phenomenon of camouflage, where detectability decreases with target-background similarity.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Searching for, and detecting, visual targets in our environment is a ubiquitous natural task that our visual system performs exceptionally well. A key feature of behavioral detection performance is that the texture similarity between the target and the background in which it is embedded profoundly affects target detectability. The more similar are the target and the background, the harder it is to detect the target (<xref ref-type="bibr" rid="bib9">Campbell and Kulikowski, 1966</xref>; <xref ref-type="bibr" rid="bib17">Foley, 1994</xref>; <xref ref-type="bibr" rid="bib37">Sebastian et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Stromeyer and Julesz, 1972</xref>; <xref ref-type="bibr" rid="bib45">Watson and Solomon, 1997</xref>; <xref ref-type="bibr" rid="bib47">Wilson et al., 1983</xref>). This phenomenon, which is termed ‘similarity masking’, is the foundation of camouflage.</p><p>An example of similarity masking is illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Detecting a low contrast oriented visual target is easy on a uniform gray background (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Detectability decreases when the target has a similar orientation as the background (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The neural basis of similarity masking is not well understood. The main goal of the current study was to test the hypothesis that neural interactions between the representations of the target and background in the primary visual cortex (V1) contribute to the perceptual effect of similarity masking.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Target-background similarity masking and behavioral task.</title><p>(<bold>A</bold>) Low contrast orientated targets can be easily detected on uniform background. (<bold>B</bold>) Similarity masking is induced by an orientated background. The same additive target from (<bold>A</bold>) becomes hard to detect when target orientation matches background orientation (see also perceptual demonstration in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). (<bold>C–E</bold>) Orientation masking was assessed in two awake behaving macaque monkeys performing a target detection task. Monkey commence the task by fixating at the small bright square (<bold>C</bold>). A few moments later, a 4° raise-cosine-masked background grating was flashed at ~3° eccentricity for target detection (<bold>D</bold>). The horizontal white bar represents one degree of visual angle. In 50% of the trials, a small additive horizontal Gabor target was also added to the background (<bold>E</bold>). The monkey indicated the presence of the target by making a saccade to the target location, and indicated target absent by maintaining gaze at the fixation point. The Gabor target was always the same – a cosine centered, horizontal Gabor at 4cpd on 0.33° FWHM envelope. The background grating was also cosine-centered at 4cpd such that the background completely aligned with the target when they were the same orientation (as in <bold>B</bold>). Orientation of the grating ranged from 0° to 90° with respect to the Gabor target and was randomized between trials. Bg – background; TBg – target plus background.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig1-v2.tif"/></fig><p>The responses of visual neurons to a target can be strongly modulated by the context in which the stimulus is presented. Such contextual modulations have powerful, complex and diverse effects in the visual cortex (<xref ref-type="bibr" rid="bib3">Allman et al., 1985</xref>; <xref ref-type="bibr" rid="bib5">Angelucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Angelucci and Bressloff, 2006</xref>; <xref ref-type="bibr" rid="bib7">Bai et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Cavanaugh et al., 2002b</xref>; <xref ref-type="bibr" rid="bib24">Henry et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Michel et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Sceniak et al., 1999</xref>; <xref ref-type="bibr" rid="bib41">Shushruth et al., 2012</xref>). Most of these effects reflect sublinear interactions between the target and the background, suggesting that they could potentially contribute to behavioral masking effects. If nonlinear computations in V1 contribute to similarity masking, we would predict that the signals evoked by a target will be maximally reduced by a background that is similar to the target.</p><p>We tested this hypothesis by measuring V1 population responses in macaque monkeys while they performed a visual detection task under masking conditions (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>). Because the nature of contextual modulations in V1 is complex, a second goal of our study was to quantitatively characterize the spatiotemporal dynamics of V1 population responses to different combinations of targets and backgrounds.</p><p>As a first step, we characterized the behavioral effects of similarity masking in two macaque monkeys, demonstrating clear effects of similarity on target detectability and reaction times. These results confirm that macaque monkeys are a good animal model for human similarity masking.</p><p>Second, we used voltage-sensitive dye imaging (VSDI; <xref ref-type="bibr" rid="bib20">Grinvald and Hildesheim, 2004</xref>; <xref ref-type="bibr" rid="bib38">Seidemann et al., 2002</xref>; <xref ref-type="bibr" rid="bib40">Shoham et al., 1999</xref>) to measure V1 population responses at two scales: the scale of the retinotopic map and the scale of orientation columns, while the monkeys performed the similarity masking detection task. To study the effect of similarity masking on the neural detection sensitivity, we constructed a task-specific decoder at each scale. Each decoder first pools the responses using a scale-dependent spatial template and then combines these responses over time to form a decision variable. The distributions of the decision variable in target-present vs. target-absent trials are used to compute neural sensitivity that can be compared to behavioral sensitivity (<xref ref-type="bibr" rid="bib39">Seidemann and Geisler, 2018</xref>).</p><p>We found that V1 population responses to the target and background display two distinct phases. An initial transient phase that starts at response onset, and a second phase that lasts until stimulus offset or the animal’s response. Surprisingly, the first phase displays a paradoxical effect; during this phase the target evoked response is strongest when the target and background are similar and is therefore anti-correlated with behavior. This effect reverses in the second phase so that in this phase the target-evoked response is reduced with increased target-background similarity. V1 responses during this second phase are therefore consistent with behavior.</p><p>We also observed complex spatiotemporal dynamics of the population response to the target and background stimuli, including a repulsion of V1 columnar-scale representation of target orientation in the direction away from the background orientation.</p><p>Finally, we show that a simple dynamic population gain control model can qualitatively account for our physiological and behavioral results, and that the estimated properties of the gain-control mechanism are consistent with a principled computational approach to feature encoding and decoding. Overall, our results are consistent with the hypothesis that contextual interactions between the representations of the target and background in V1 are likely to contribute to the perceptual phenomena of similarity masking.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral effect of target-background similarity masking</title><p>To study the neural basis of visual similarity masking, we trained two monkeys (<italic>Macaca mulatta</italic>) to perform a visual detection task in which a small horizontal target appeared on a larger background at a known location in half of the trials (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). The monkey indicated target absence by maintaining fixation and target presence by making a saccadic eye movement to the target location as soon as it was detected. Within a block of trials, the contrast of the target and the background were fixed, while the orientation of the background varied randomly from trial to trial, allowing us to test for the effect of target-background orientation similarity on behavioral and neural detection sensitivities.</p><p>We tested the behavioral effect of similarity masking over five combinations of target and background contrasts (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). For each combination, we measured the behavioral sensitivity as a function of background orientation. Performance with no background (uniform gray screen) served as a baseline (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, dashed horizontal lines). Performance as a function of background orientation was fitted with an inverted Gaussian function. At all five target and background contrast combinations, detection sensitivity was lowest when background and target orientations matched, confirming the expected similarity masking effect from human subjects. These results demonstrate that macaque monkeys are a good animal model for studying the neural basis of human similarity masking.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral effect of target-background similarity masking.</title><p>(<bold>A</bold>) Target detection performance in monkeys was affected by the orientation of the background grating over a variety of target contrast (T## = ##% contrast target) and background contrast levels (Bg## = ##% contrast background). Signal detection measure d-prime (d’) of the target is plotted for uniform background (dotted lines), and for each background orientation (markers). In most cases, there was a general performance reduction from uniform background to a grating background. Additionally, performance was further reduced when the background orientation was more aligned to the target (0°). A fitted Gaussian (solid line) illustrates the performance change due to orientation masking. d’ was calculated from the hit rate (correctly reporting target present) and the false alarm rate (reporting target present when it was absent). The relationship between d’ and optimum performance level in percent correct is plotted in (<bold>B</bold>). (<bold>C</bold>) Reaction time – calculated from stimulus onset to saccade initiation for Hit trials – is plotted for uniform background (dotted lines) and for each background orientation (solid lines). Error bars indicate the standard error of the mean. Data were pooled within each monkey across experiments. Each experiment contains a single combination of target and background contrast levels, with uniform background and orientated background trials assessed in separate blocks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig2-v2.tif"/></fig><p>The supplementary information includes a perceptual demonstration of similarity masking for a wide range of target amplitudes, orientations, and spatial-frequencies (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). This demonstration can give the reader an intuitive sense of the masking effects studied here.</p><p>We also examined the effect of target-background orientation similarity on the monkeys’ reaction times (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We find two distinct effects of orientation similarity on reaction times. At higher target and background contrasts, reaction times are maximal when the background and target have the same orientation (when detectability is lowest and the task is hardest) and monotonically decrease as target-background similarity decreases (detectability increases and the task becomes easier). Surprisingly, at lower target and background contrasts, reaction times are low when the background matches target orientation, then increases as the background-target orientation difference increases, and then drops again when the background approaches the orthogonal orientation to the target. Thus, under these conditions, we see an interesting decoupling between difficulty and reaction time, so that reaction times can be shortest in the harder conditions. This surprising effect is present in both monkeys. Some of the complex neural dynamics described below could explain this interesting effect (see Discussion).</p><p>Our next goal was to test the hypothesis that contextual interactions between the representations of the target and background in V1 contribute to the observed behavioral similarity masking results.</p></sec><sec id="s2-2"><title>Neural population responses to target and background stimuli in macaque V1</title><p>While the monkeys performed the similarity masking detection task, we used VSDI to measure V1 population responses to the target and the background. In each cranial window, we first used a fast and efficient VSDI protocol to obtain a detailed retinotopic map (<xref ref-type="bibr" rid="bib48">Yang et al., 2007</xref>). We then positioned the target so that its neural representation fell at the center of our imaging area.</p><p>The target elicits V1 population activity at two fundamental spatial scales. At the large retinotopic scale, the target evokes an activity envelope that spreads over several mm<sup>2</sup> and is well fitted by a two-dimensional (2D) Gaussian (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, top row; <xref ref-type="bibr" rid="bib13">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Sit et al., 2009</xref>). Our 8x8 mm<sup>2</sup> imaging area allows us to capture this entire target-responsive region. Because the background is much larger than the target and is centered at the same location in the visual field, it produces a relatively uniform response within the imaging area (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, second row). Similarly, the target-plus-background stimulus elicits activity within the entire imaged area, with a relatively elevated activity at the retinotopic region corresponding to the target location (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, 3rd row). However, the target-evoked response in the presence of the background (response to target plus background minus response to the background alone) appears significantly weaker than the response to the target alone (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, 1st vs. 4th row). This reduced target-evoked response in the presence of the background could contribute to the perceptual masking effect of the background. Our goal here was to determine how this sublinear interaction between the response to the target and background depends on target-background similarity in orientation.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>VSD response and decoder schematic.</title><p>(<bold>A</bold>) A cranial chamber and a transparent artificial dura provided chronic imaging access to V1 of the monkey. V2 is completely hidden in the lunate sulcus based on the retinotopic map taken in a separate imaging session (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Imaging ROI and target response at 2 SD of the fitted Gaussian from the example recording in <bold>B-E</bold> are illustrated. (<bold>B</bold>) Example recording of the voltage-sensitive dye (VSD) response in an 8x8 mm imaging region-of-interest (ROI). Response to the Gabor target on uniform background could be easily identified in VSD (top row). The visual span of the background grating extended beyond the coverage of the imaging window, evoking an encompassing response over the entire ROI (2nd row). When the background was presented with the additive target, the response to the target was diminished (3rd and 4th rows). Each VSD response map were averaged across 50 trials, over 5 frames captured at 100 Hz. T only – target only; Bg – background only; TBg – target and background; ΔTBg – target and background minus background only. (<bold>C</bold>) Target response was extracted at the retinotopic scale by estimating its response profile with a two-dimensional Gaussian. The profile was estimated from response from a separate recording block on each experiment day. To optimized signal-to-noise, in this recording block, the target was flashed repeatedly at 5 Hz while the monkey maintained fixation. The effect of spatially correlated VSD noise was minimized by estimating a whitening kernel from trials without stimulus presentation (see Methods). (<bold>D</bold>) Target response was extracted at the columnar scale by estimating the orientation map within the imaging area. This was constructed from full-field gratings flashed at 5 Hz in a separate recording block on each experiment day (see Methods). The columnar map in the 0°–90° axis was extracted and windowed down to the retinotopic profile to identify the columnar scale response of the target.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Retinotopy of imaging chambers and target placement positions.</title><p>A separate VSD imaging experiment was conducted for each chamber with special retinotopic scanning stimuli (see Methods). (<bold>A</bold>) Photograph of the imaging chamber overlaid with contour lines (green) of polar angles relative to the horizontal meridian of the visual field. Angular values were anti-clockwise from the right horizontal meridian. The V1-V2 border (dashed orange line) is at 270°. This border is hard to estimate and is believed to be 0.7 mm from the lunate in Monkey T’s left visual cortex chamber (col 2). For the other two chambers, the V1-V2 border is estimated to be inside the lunate sulcus (dashed red line). (<bold>B</bold>) The same imaging areas as in A marked with both the angular (green) and eccentricity (red) contour lines. Area with good VSD staining on the day of retinotopy is marked in white. Red dots in A and B indicate where the target was placed. The position of the target in cols 2–3 was moved between experiments to optimize for the imaging outcome as a response to the quality of VSD staining of the day. Scale bars indicate 4 mm in length.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig3-figsupp1-v2.tif"/></fig></fig-group><p>In addition to the retinotopic-scale response envelope, fine scale response modulations at a scale of individual orientation columns (width of ~0.3 mm) reflect the orientation of the target and background. These columnar scale modulations have a relatively small amplitude and therefore appear as small ripples riding on top of the larger retinotopic response envelope. The relatively smaller VSDI responses at the columnar scale is due to a mixture of robust non-orientation selective V1 population responses in V1 as well as optical and biological blurring (<xref ref-type="bibr" rid="bib15">Chen et al., 2012</xref>). We can selectively access the columnar scale signals by spatially filtering the responses at the scale of the orientation columns. Despite their small relative amplitude, these columnar-scale signals provide high-quality single-trial orientation decoding (<xref ref-type="bibr" rid="bib8">Benvenuti et al., 2018</xref>).</p></sec><sec id="s2-3"><title>Retinotopic-scale effect of target-background similarity masking</title><p>To study the effect of similarity masking on V1 responses at the retinotopic scale, we used an optimal linear decoder of V1 population responses (<xref ref-type="bibr" rid="bib13">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bib14">Chen et al., 2008</xref>) that allows us to assess the neural detection sensitivity of V1 population responses (i.e. how well one can detect the target from single-trial V1 population responses) (<xref ref-type="fig" rid="fig3">Figure 3C–D</xref>). The retinotopic decoder takes into account the location and shape of the envelope of the target-evoked response (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), as well as the structure of the noise covariance matrix (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2006</xref>).</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> summarizes the dynamics of the retinotopic template output in response to the V1 signals across all of our experiments for two combinations of target and background contrasts (see <xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for the full set of tested target/background combinations). When presented on a uniform gray background, the target-related retinotopic signal begins to rise ~40 ms after target onset, reaches its peak ~100 ms after stimulus onset, and remains high for the next 100ms (<xref ref-type="fig" rid="fig4">Figure 4B and H</xref>, black curve). However, when the same target is added to the background, the target-related retinotopic signals display a wide range of responses that depend on background orientation (<xref ref-type="fig" rid="fig4">Figure 4B and H</xref>, colored curves).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Retinotopic template dynamics and correlation to behavior.</title><p>(<bold>A–F</bold>) Average response and dynamics from recordings with 12% target contrast (T12) and 7% background contrast (Bg7). (<bold>A</bold>) Response time course from stimulus onset (t=0ms) for background only trials. Background orientation are identified by color. Backgrounds with the same clockwise and anticlockwise orientation disparity from the target were pooled. (<bold>B</bold>) Response time course with the same additive target on different oriented background. Response of the target on uniform gray background is illustrated in black. (<bold>C</bold>) The target-evoked response time course was obtained by subtracting background only response (<bold>A</bold>) from response to target &amp; background (<bold>B</bold>). Target evoked response was initially strongest for backgrounds close to 0° (red), then inverted around t=100ms such that response became the stronger for background closer to 90° (blue). (<bold>D</bold>) Response was averaged over 50–200ms and fitted with a Gaussian (gray) to illustrate the change in response magnitude with respect to background orientation. The neural-behavioral correlation of the response against behavioral response (<bold>F</bold>) is printed with <italic>p</italic> significance value. Here, response to clockwise and anti-clockwise background orientations are plotted separately. Size of markers indicate the number of trials tested for each orientation. Black line indicates the response of the target only trials integrated over the same window. (<bold>E</bold>) The animals’ behavior performance was anti-correlated with the initial phase of the retinotopic response, and was more aligned in the latter phase. Correlation coefficient was calculated across background orientations between each frame of the retinotopic response in (<bold>C</bold>) against the overall behavior performance in (<bold>F</bold>). Red dots indicate frames reaching statistical significance (p&lt;0.05, <italic>t</italic>-test for correlation coefficient, see Methods). The neural-behavioral correlation crosses from negative to positive at t=130ms. (<bold>F</bold>) Behavior performance in d’ was calculated as described in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Size of markers indicate the number of trials tested for each orientation. Data was pooled across 8 experiments from both monkeys (see <xref ref-type="table" rid="table1">Table 1</xref>). (<bold>G–L</bold>) Same as (<bold>A–F</bold>) for recordings with 24% target contrast (T24) and 12% background contrast (Bg12). Similar trends were observed. The neural-behavioral correlation crosses from negative to positive at t=96ms in (<bold>K</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Retinotopic template dynamics and correlation to behavior for all combinations of background and target contrast levels.</title><p>Retinotopic response time course using doubly-whitened retinotopic decoding template illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>A</bold>) Response time course from stimulus onset (t=0ms), grouped by different combinations of target and background contrast levels. Background orientation is indicated by color. Backgrounds with the same clockwise and anticlockwise orientation disparity from the target were pooled. Response of the target on uniform gray background is illustrated in black. (<bold>B</bold>) The target-evoked response time course was obtained by subtracting the background only response (<bold>A</bold>, top row) from the background-plus-target response (rest of <bold>A</bold>) from the same experiments. (<bold>C</bold>) Response was averaged over 50–200ms and fitted with a Gaussian (gray) to illustrate the change in response magnitude with respect to background orientation. Neural-behavioral correlation of the integrated response against the behavior performance (<xref ref-type="fig" rid="fig2">Figure 2A</xref> pooled across monkeys) are printed with p significance values. Here, response to clockwise and anti-clockwise background orientations are plotted separately. Size of markers indicate the number of trials tested for each orientation. Black line indicates the response of the target only trials integrated over the same window. (<bold>D</bold>) Correlation coefficient was calculated across background orientations between each frame of the retinotopic response in (<bold>B</bold>) against the overall behavioral performance for each combination of background and target contrast (<xref ref-type="fig" rid="fig2">Figure 2A</xref> pooled across monkeys). Red dots indicate frames reaching statistical significance (p&lt;0.05, <italic>t</italic>-test for correlation coefficient, see Methods). Data was pooled across experiments from both monkeys (see <xref ref-type="table" rid="table1">Table 1</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Retinotopic and columnar integrated response and correlation to behavior: correct trials vs all trials.</title><p>Response dynamics calculated over all trials as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig5">Figure 5</xref> are replotted in odd rows for comparison against the same stimulus conditions using only correct trials (hits and correct rejections only, even rows). (<bold>A</bold>) Behavioral performance replotted from <xref ref-type="fig" rid="fig4">Figure 4F and L</xref>. Behavioral performance was calculated using all trials (correct and incorrect), and was used for the calculation of correlations when the template response included all trials or just the correct trials. (<bold>B, D</bold>) Integrated response over 50–200ms from stimulus onset in the same format as <xref ref-type="fig" rid="fig4">Figure 4D</xref>. (<bold>C, E</bold>) Behavioral correlations in the same format as <xref ref-type="fig" rid="fig4">Figure 4E</xref>. The vertical lines mark the time of the cross-over from negative to positive correlations (the vertical lines are labeled with the time in ms).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig4-figsupp2-v2.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Experiment summary.</title><p>Experiment counts and the total number of trials included in the analysis presented in <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig4">4</xref>, <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig7">7</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref> and <xref ref-type="fig" rid="fig4s2">2</xref>; and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. Experiments with ineffective VSD staining were excluded from # Experiments. Trials with excessive motion or inconsistent EKG were excluded from # Total trials. Age of monkey reported at the time of the last listed experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"><break/><break/></th><th align="left" valign="bottom">Target Contrast</th><th align="left" valign="bottom">Background Contrast</th><th align="left" valign="bottom"># Experiments</th><th align="left" valign="bottom"># Total Trials</th><th align="left" valign="bottom"># Hits</th><th align="left" valign="bottom"># Misses</th><th align="left" valign="bottom"># Correct Rejects (CR)</th><th align="left" valign="bottom"># False Alarms (FA)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="3"><bold>Monkey H</bold><break/>Male 8 years old</td><td align="left" valign="bottom">T12%</td><td align="left" valign="bottom">---</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">52</td><td align="left" valign="bottom">26</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">1</td></tr><tr><td align="left" valign="bottom">T12%</td><td align="left" valign="bottom">Bg7%</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">593</td><td align="left" valign="bottom">214</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">253</td><td align="left" valign="bottom">46</td></tr><tr><td align="left" valign="bottom">T12%</td><td align="left" valign="bottom">Bg12%</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">397</td><td align="left" valign="bottom">125</td><td align="left" valign="bottom">74</td><td align="left" valign="bottom">121</td><td align="left" valign="bottom">77</td></tr><tr><td align="left" valign="bottom" rowspan="7"><bold>Monkey T</bold><break/>Male 7 years old</td><td align="left" valign="bottom">T12%</td><td align="left" valign="bottom">---</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">490</td><td align="left" valign="bottom">185</td><td align="left" valign="bottom">59</td><td align="left" valign="bottom">228</td><td align="left" valign="bottom">18</td></tr><tr><td align="left" valign="bottom">T12%</td><td align="left" valign="bottom">Bg7%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">1660</td><td align="left" valign="bottom">644</td><td align="left" valign="bottom">189</td><td align="left" valign="bottom">740</td><td align="left" valign="bottom">87</td></tr><tr><td align="left" valign="bottom">T12%</td><td align="left" valign="bottom">Bg12%</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">1748</td><td align="left" valign="bottom">704</td><td align="left" valign="bottom">169</td><td align="left" valign="bottom">683</td><td align="left" valign="bottom">192</td></tr><tr><td align="left" valign="bottom">T24%</td><td align="left" valign="bottom">---</td><td align="left" valign="bottom">16</td><td align="left" valign="bottom">546</td><td align="left" valign="bottom">272</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">267</td><td align="left" valign="bottom">7</td></tr><tr><td align="left" valign="bottom">T24%</td><td align="left" valign="bottom">Bg7%</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">1012</td><td align="left" valign="bottom">501</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">450</td><td align="left" valign="bottom">56</td></tr><tr><td align="left" valign="bottom">T24%</td><td align="left" valign="bottom">Bg12%</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">1320</td><td align="left" valign="bottom">654</td><td align="left" valign="bottom">12</td><td align="left" valign="bottom">599</td><td align="left" valign="bottom">55</td></tr><tr><td align="left" valign="bottom">T24%</td><td align="left" valign="bottom">Bg24%</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">388</td><td align="left" valign="bottom">204</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">155</td><td align="left" valign="bottom">27</td></tr></tbody></table></table-wrap><p>Our main interest here is in the target-evoked response in the presence of the background (<xref ref-type="fig" rid="fig4">Figure 4C, I</xref>), which can be extracted by subtracting the response to the background alone (<xref ref-type="fig" rid="fig4">Figure 4A and G</xref>) from the response to the target plus background (<xref ref-type="fig" rid="fig4">Figure 4B and H</xref>). If V1 contextual interactions at the retinotopic scale contribute to the behavioral effect of similarity masking, we would expect that target-evoked responses would be weakest with high target-background similarity (similar target and background orientations) and strongest with low target-background similarity (orthogonal target and background orientations).</p><p>Surprisingly, we find that the target-evoked response in V1 displays two distinct phases, with the early phase showing a paradoxical neural dependence on target-background orientation similarity that is anti-correlated with the behavioral masking effect, and with a later phase that is consistent with the behavioral masking effect. Specifically, in the early phase which starts at response onset, the target-evoked response is highest when the background matches the target orientation even though behaviorally this is the condition in which detection performance is the worst. However, after this initial phase, the high-similarity target-evoked response starts to drop, while the low-similarity target-evoked response continues to build up, so that in the later stages the target-evoked response is strongest on the dissimilar background and weakest on the similar background, consistent with the behavioral effect of similarity.</p><p>To quantify the relation between the effects of target-background orientation similarity on V1 population responses and behavior, we computed the correlation between the effect of orientation similarity on behavior (<xref ref-type="fig" rid="fig4">Figure 4F and L</xref>) and its effect on the target-evoked neural responses in individual 10ms frames (<xref ref-type="fig" rid="fig4">Figure 4E and K</xref>). This analysis reveals a robust paradoxical negative correlation between the early neural V1 response and behavior, weak positive correlation between the late neural V1 response and behavior, and no correlation between behavior and the integrated neural response. This result was obtained from averaging the response across all trials irrespective of whether the monkey made the correct decision.</p><p>To examined whether decision- and/or attention-related signals have a major contribution to the observed biphasic dynamics, we repeated the analysis on only the hits and correct rejection trials (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2B–C</xref>). Our results are qualitatively the same for the subset of correct trials, indicating that decision- and/or attention-related signals are unlikely to play a major role in the observed dynamics.</p><p>Because the target and background are defined by their orientation, the correspondence between the neural signals in V1 and behavior may be better captured by V1 responses at the columnar scale. Our next step was therefore to examine the dynamics of the columnar-scale target-evoked responses in V1.</p></sec><sec id="s2-4"><title>Neural effects of target-background similarity masking at the scale of orientation columns</title><p>To study the effect of similarity masking on V1 responses at the columnar scale, we developed a linear columnar decoder of the VSDI signals (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). The columnar decoder takes into account the location of the orientation columns within the retinotopic envelope of the target-evoked response. Because the target is horizontal, the output of the columnar template is expected to be positive for the horizontal target and background stimuli and negative for the vertical background stimulus (since the horizontal and vertical columnar maps are anti-correlated).</p><p>As with the output of the retinotopic-scale template, the output of the columnar-scale template displays two distinct phases. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the time course of the columnar template signals to background alone (<xref ref-type="fig" rid="fig5">Figure 5A and G</xref>), the target plus background (<xref ref-type="fig" rid="fig5">Figure 5B and H</xref>), and target-evoked response in the presence of the background (<xref ref-type="fig" rid="fig5">Figure 5C, I</xref>). In the early phase, the target-evoked response is highest when the background and target have similar orientations, producing a paradoxical neural response that is anti-correlated with the behavioral masking effect (<xref ref-type="fig" rid="fig5">Figure 5E and K</xref>). In the second phase, the trend reverses and the target-evoked response is strongest on the dissimilar background and weakest on the similar background, consistent with the behavioral effect of similarity (<xref ref-type="fig" rid="fig5">Figure 5F and L</xref>). Similar results were obtained with other target and background contrast combinations (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Columnar template dynamics and correlation to behavior.</title><p>Same format as <xref ref-type="fig" rid="fig4">Figure 4</xref> with the response examined at the columnar scale. The biphasic response time course observed in the retinotopic scale was more pronounced at the columnar scale. (<bold>A–F</bold>) Averaged response and dynamics from recordings with 12% target contrast (T12) and 7% background contrast (Bg7). (<bold>G–L</bold>) Averaged response and dynamics from recordings with 24% target contrast (T24) and 12% background contrast (Bg12). Here, positive response represents relatively stronger activation of the neurons tuned to the target orientation (0°), and negative response represent stronger activation for neurons tuned to the orthogonal orientation (90°). Data pooling and counts are the same as reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Behavioral correlation crosses from negative to positive at t=99ms in (<bold>E</bold>), and t=66ms in (<bold>K</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Columnar template dynamics and correlation to behavior for all combinatory background and target contrast levels.</title><p>(<bold>A–D</bold>) Same format as <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> with the averaged response examined at the columnar scale. The columnar template extracts the relative strength between neural activity aligned and orthogonal to the target orientation. Positive response represents relatively stronger activation of the neurons tuned to the target orientation (0°), and negative response represent stronger activation for neurons tuned to the orthogonal orientation (90°). Data pooling and counts are the same as reported in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig5-figsupp1-v2.tif"/></fig></fig-group><p>Again, to examined whether decision- and/or attention-related signals have a major contribution to the observed biphasic dynamics at the columnar scale, we examined the behavioral correlations with hits and correct rejection trials only. We found only minor differences in the target-evoked response and behavioral correlations (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2D–E</xref>), indicating that the observed biphasic dynamics at the columnar scale are unlikely to have a major top-down contribution.</p><p>Because the first phase of the response is shorter than the second phase, when V1 response is integrated over both phases, the overall response is positively correlated with the behavioral masking effect (<xref ref-type="fig" rid="fig5">Figure 5D and J</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>). Therefore, our results suggest that the neural masking effect at the columnar scale in V1 could play a major role in the behavioral similarity masking effects.</p></sec><sec id="s2-5"><title>Dynamics of columnar-scale orientation population trajectories</title><p>Our decoding analysis focuses on the columnar-scale orientation signals along the 0°–90° axis and reveals complex columnar-scale dynamic interactions between the target-evoked response and the response evoked by the background (<xref ref-type="fig" rid="fig5">Figure 5</xref>). To examine these dynamics in more detail, we performed two types of population-vector analyses (<xref ref-type="fig" rid="fig6">Figure 6</xref>). We began by assigning each pixel within the retinotopic footprint of the target-evoked response to one of 12 equally spaced preferred orientations, creating 12 orientation selective clusters of pixels (<xref ref-type="fig" rid="fig6">Figure 6B–C</xref>). We then computed for each stimulus the response in each orientation selective cluster in each frame and displayed, in the first analysis, the population orientation tuning curve as a function of time (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), and in the second analysis the population vector dynamic trajectory in the polar space spanned by the 12 orientations (<xref ref-type="fig" rid="fig6">Figure 6E</xref>); that is the orientation <inline-formula><mml:math id="inf1"><mml:semantics><mml:mi>θ</mml:mi></mml:semantics></mml:math></inline-formula> and magnitude <inline-formula><mml:math id="inf2"><mml:semantics><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> of the peak of the population response over time.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Columnar orientation estimation by populations tuning.</title><p>(<bold>A</bold>) The orientation map obtained for each experiment as described in <xref ref-type="fig" rid="fig3">Figure 3</xref> was windowed to the retinotopic profile of the target. (<bold>B</bold>) Each pixel was assigned to one of 12 equally spaced orientation selective cluster maps by its preferred orientation. (<bold>C</bold>) The orientation selective decomposition of VSD response. To a grating stimulus oriented at 0°, the population tuning curve peaks at 0° (solid curve); likewise, the population peak would shift to 45° for a 45° grating (dotted curve). Note that this population response only represents the relative difference in preferred orientation (balanced positive and negative values); the overall neural response offset (retinotopic response) is not captured by this approach. (<bold>D</bold>) Example of a full population response time course from stimulus onset (t=0ms). (<bold>E</bold>) Population response can be summed to a complex vector representing the overall population tuning orientation and magnitude.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig6-v2.tif"/></fig><p>The first population vector analysis reveals that V1 responses to the target or background alone are consistent with the stimulus orientation. In background only trials, shortly after stimulus onset the peak of the population tuning curve closely matches background orientation (<xref ref-type="fig" rid="fig7">Figure 7A and F</xref>, top row, red arrow and horizontal line). Similar results were obtained in the target only trials, where the peak of the population response tuning curve matches target orientation (<xref ref-type="fig" rid="fig7">Figure 7A and F</xref>, 4th row, green arrow and horizontal line).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Dynamics of orientation population response.</title><p>(<bold>A</bold>) Population tuning time course for trials with 12% contrast targets (T12) and 7% contrast backgrounds (Bg7). Averaged response time courses are presented as a heatmap with the y-axis representing the preferred orientation (from <xref ref-type="fig" rid="fig6">Figure 6A–C</xref>) and the x-axis time (see key on bottom right) to illustrate the change in the tuning over time. <italic>Row 1:</italic> Heatmaps for background only trials exhibit clear population tuning in the orientation of the background grating (red horizontal line). <italic>Row 2:</italic> Heatmaps for background with additive target showing population response dominated by the background orientation rather than the target orientation (0°). <italic>Row 3:</italic> The target evoked response is obtained by subtracting the background only response Row 1 from the target &amp; background response in Row 2. Masking of the target evoked response was strong for backgrounds oriented near the target orientation (0°). With the background orthogonal to the target, population tuning in the target orientation can be identified. White line identifies the orientation of the population vector (peak tuning) wherever the normalized amplitude of the vector average was great than 0.2 (see Methods). Depending on background orientation, peak tuning appears to be offset from the orientation of the target (e.g. at Bg –45°). <italic>Row 4:</italic> Heatmap for the target only trials demonstrated clear population tuning in the target orientation (0°, green horizontal line). (<bold>B–E</bold>) Averaged response in (<bold>A</bold>) represented as a population vector form and illustrated as a continuous trajectory for each background orientation (color coded). (<bold>B</bold>) Population tuning trajectory for background only trials. The trajectories commenced in the center of the circle (white dot) and adhered closely to the orientation of the background. Dot on each trajectory indicates the position of the population tuning vector at 100ms. (<bold>C</bold>) Population tuning trajectory for background with additive target illustrating the biphasic response of this combined stimulus. In the early phase, the heading of the trajectory was a mixture of the background and target (0°) orientations, dominated more by the background. In the late phase, the trajectory made a sharp turn (t≈100ms) such that trajectories appeared to head towards a convergent point on the positive x-axis. (<bold>D</bold>) The trajectory for the target evoked response, calculated by subtracting the background only response (<bold>B</bold>) from the corresponding background &amp; target (<bold>C</bold>). The target evoked response was weak and noisy, but was heading in the general direction of the target orientation (0°). (<bold>E</bold>) The population tuning trajectory for the target only trials illustrating clear tuning in the target orientation (0°). (<bold>F–J</bold>) Same as (<bold>A–E</bold>) for trials with 24% contrast targets (T24) and 12% contrast backgrounds (Bg12). Data was pooled and averaged across both monkeys.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig7-v2.tif"/></fig><p>Target plus background responses display complex spatiotemporal dynamics. To examine the dynamics of the target-evoked response in the presence of the background, we subtracted the background only response from the target plus background response (<xref ref-type="fig" rid="fig7">Figure 7A and F</xref>, 3rd row). The results reveal complex target-background interactions which could lead to a population tuning peak (white curve) that significantly deviates from the target orientation. For example, in some conditions, we observe an orientation tuning peak that is repelled from target orientation in the direction away from background orientation. An interesting goal of future studies would be to examine potential perceptual correlates of these interactions.</p><p>In the second population vector analysis, we plotted the response trajectories for each stimulus using the vector representation in polar coordinates (<xref ref-type="fig" rid="fig7">Figure 7B–E and G–J</xref>). After stimulus onset, the population vector for background only moves in the direction corresponding to the background orientation (<xref ref-type="fig" rid="fig7">Figure 7B and G</xref>) and for target only moves in direction corresponding to the target (<xref ref-type="fig" rid="fig7">Figure 7E and J</xref>).</p><p>The trajectories in the target plus background conditions are more complex. For example, when background orientation is at +/-45 deg to the target, the population response is initially dominated by the background, but then in mid-flight, the population response changes direction and turns toward the direction of the target orientation.</p><p>Such complex interactions can be used to constrain models of V1 population response.</p></sec><sec id="s2-6"><title>Dynamic gain control model qualitatively captures similarity masking effects in V1</title><p>Our next goal was to determine whether the observed interactions between the background- and target-evoked responses can be qualitatively captured by a gain control model. In this model, orientation columnar response was tuned to one of 12 equally spaced orientations. The responses of each orientation column were specified by the simple normalization model summarized in <xref ref-type="fig" rid="fig8">Figure 8A</xref>. Specifically, the spatiotemporal input stimulus generates an excitation signal and a normalization signal that are both linear with the input root mean square (rms) contrast. The normalization signal is then combined with a normalization constant to obtain the normalization factor. The normalized response is obtained by dividing the excitation signal by the normalization factor. The final response is then obtained by applying a response exponent <inline-formula><mml:math id="inf3"><mml:mi>p</mml:mi></mml:math></inline-formula>, which is similar to applying a spiking nonlinearity. Importantly, the excitation and normalization signals can differ in their spatial extent, orientation tuning width, and temporal impulse response (see Methods for model parameters).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Delayed normalization model qualitatively captures key orientation masking response features.</title><p>(<bold>A</bold>) Schematic of the normalization model showing the visual input being processed by separate excitatory and normalization signal pathways. The normalization pathway in particular was modeled with a slightly delayed temporal kinetics and wider orientation tuning curves. The excitatory signal undergoes divisive normalization prior to neural output. (<bold>B–D</bold>) Modeled columnar response output with target contrast of 24% and background contrast of 12%. Modeled response normalized to the target only response averaged over 50–200ms as plotted in (<bold>E</bold>). (<bold>B</bold>) Modeled response of oriented backgrounds as in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. (<bold>C</bold>) Modeled response of background with additive target as in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, and the modeled response of the small Gabor target in black. (<bold>D</bold>) The target evoked response was obtained by subtracting the background only response (<bold>B</bold>) from (<bold>C</bold>), matching the biphasic observation in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. (<bold>E</bold>) Model response integrated over 50–200ms in the same format as <xref ref-type="fig" rid="fig5">Figure 5D</xref>. (<bold>F</bold>) Correlation of modeled behavioral performance (Gaussian fit in <xref ref-type="fig" rid="fig5">Figure 5L</xref>) against each time frame of the modeled response, illustrating the early phase where the response was negatively correlated to behavioral choice, and the late phase with positive correlations. (<bold>G–J</bold>) Modeled time course of the population tuning vector. (<bold>G</bold>) Modeled populating tuning trajectory of the background only stimuli (color coded) as in <xref ref-type="fig" rid="fig7">Figure 7B</xref>. (<bold>H</bold>) Modeled populating tuning trajectory of the background with additive target illustrating the turn towards a convergent point on the x-axis as in <xref ref-type="fig" rid="fig7">Figure 7C</xref>. (<bold>I</bold>) Modeled target evoked response trajectory from subtracting (<bold>F</bold>) from (<bold>G</bold>). (<bold>J</bold>) Modeled populating tuning trajectory of the target only trials as in <xref ref-type="fig" rid="fig7">Figure 7E</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Divisive normalization model with different normalization spatial extents.</title><p>The spatial extent of the normalization kernel was varied to determine its effect on key model outputs from <xref ref-type="fig" rid="fig8">Figure 8</xref>. The size of <italic>s<sub>n</sub></italic> was varied while all other parameters were at the same values as in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Target contrast was modeled at 24% and background contrast at 12% (as in <xref ref-type="fig" rid="fig5">Figure 5G–L</xref>). (<bold>A</bold>) Illustration of the size of the normalization spatial kernel. <xref ref-type="fig" rid="fig8">Figure 8</xref> was simulated with <italic>s<sub>n</sub></italic> = 0.28° (row 1). (<bold>B–D</bold>) Model response corresponding to <xref ref-type="fig" rid="fig8">Figure 8B–D</xref>. (<bold>E</bold>) Model response integrated over 50–200ms in the same format as <xref ref-type="fig" rid="fig8">Figure 8E</xref>. (<bold>F</bold>) Correlation of modeled behavioral performance (Gaussian fit in <xref ref-type="fig" rid="fig5">Figure 5L</xref>) against each time frame of the modeled response. (<bold>G</bold>) Modeled populating tuning trajectory corresponding to <xref ref-type="fig" rid="fig8">Figure 8H</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Divisive normalization model with different background spatial extents.</title><p>The spatial extent of the background grating (<italic>σ<sub>Bg</sub></italic>) was explored for its effect on key model outputs from <xref ref-type="fig" rid="fig8">Figure 8</xref> (row 1). The size of <italic>σ<sub>Bg</sub></italic> was varied while all other parameters were at the same values as in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Target contrast was modeled at 24% and background contrast at 12% (as in <xref ref-type="fig" rid="fig5">Figure 5G–L</xref>). (<bold>A</bold>) Illustration of the additive Gabor target on the background grating. <xref ref-type="fig" rid="fig8">Figure 8</xref> was with a full field, uniform contrast grating covering the entire square simulation area (row 1). To simulate different background sizes, a Gaussian mask was applied co-centric to the target and with a spatial <italic>σ<sub>Bg</sub></italic> relative to the size of the target (<italic>σ<sub>T</sub></italic>) as labeled. (<bold>B–D</bold>) Model response corresponding to <xref ref-type="fig" rid="fig8">Figure 8B–D</xref>. (<bold>E</bold>) Model response integrated over 50–200ms in the same format as <xref ref-type="fig" rid="fig8">Figure 8E</xref>. (<bold>F</bold>) Modeled populating tuning trajectory corresponding to <xref ref-type="fig" rid="fig8">Figure 8H</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig8-figsupp2-v2.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>Divisive normalization model with different background contrasts.</title><p>The contrast level of the background grating was explored for its effect on key model outputs from <xref ref-type="fig" rid="fig8">Figure 8</xref> (row 2). Rows 1–4: The contrast of the background was varied while all other parameters were at the same values as in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Rows 5–8: Contrast of background was varied with the background set to the same size as the Gabor target, while all other parameters were at the same values as in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Target contrast was 24% for all rows (as in <xref ref-type="fig" rid="fig8">Figure 8</xref>), and the background contrast for each row is as labeled. (<bold>A</bold>) Illustration of the additive Gabor target on the background grating. (<bold>B–D</bold>) Model response corresponding to <xref ref-type="fig" rid="fig8">Figure 8B–D</xref>. (<bold>E</bold>) Model response integrated over 50–200ms in the same format as <xref ref-type="fig" rid="fig8">Figure 8E</xref>. (<bold>F</bold>) Modeled populating tuning trajectory corresponding to <xref ref-type="fig" rid="fig8">Figure 8H</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig8-figsupp3-v2.tif"/></fig><fig id="fig8s4" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 4.</label><caption><title>Divisive normalization model with different normalization signal orientation tuning width.</title><p>The orientation tuning width (<italic>σ<sub>n</sub></italic>) of the normalization signal was explored for its effect on key model outputs from <xref ref-type="fig" rid="fig8">Figure 8</xref>. The size of <italic>σ<sub>n</sub></italic> was varied while all other parameters were at the same values as in <xref ref-type="fig" rid="fig8">Figure 8</xref>. Target contrast was modeled at 24% and background contrast at 12% (as in <xref ref-type="fig" rid="fig5">Figure 5G–L</xref>). (<bold>A</bold>) Illustration of the normalization signal orientation tuning. <xref ref-type="fig" rid="fig8">Figure 8</xref> was simulated with <italic>σ<sub>e</sub></italic> = 15° and <italic>σ<sub>n</sub></italic> = 20° (row 1). (<bold>B–D</bold>) Model response corresponding to <xref ref-type="fig" rid="fig8">Figure 8B–D</xref>. (<bold>E</bold>) Model response integrated over 50–200ms in the same format as <xref ref-type="fig" rid="fig8">Figure 8E</xref>. (<bold>F</bold>) Correlation of modeled behavioral performance (Gaussian fit in <xref ref-type="fig" rid="fig5">Figure 5L</xref>) against each time frame of the modeled response. (<bold>G</bold>) Modeled populating tuning trajectory corresponding to <xref ref-type="fig" rid="fig8">Figure 8H</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89570-fig8-figsupp4-v2.tif"/></fig></fig-group><p>We find that this simple model can qualitatively captures our key results. First, in response to background alone (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), the modeled population vector peaked at ~100ms after stimulus onset and then dropped to a lower amplitude, as in our data (<xref ref-type="fig" rid="fig5">Figure 5A and G</xref>). This reduction in response amplitude was due to normalization signal that was delayed relative to the excitation signal. Second, as in the real data, response to the target plus background is less than the sum of the responses to each component separately. Third, as in our physiological results, the target-evoked response in the presence of the background is biphasic, having a brief early component in which the response is enhanced by target-background similarity, and a longer-lasting late component in which the response is suppressed by target-background similarity (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). This leads to an early phase in which the response is anticorrelated with the behavioral effect of similarity masking, and a late phase and an integrated response that are positively correlated with the behavioral effect of similarity masking (<xref ref-type="fig" rid="fig8">Figure 8E and F</xref>).</p><p>Finally, this simple model can also display the curved trajectories of the population vector in response to the target plus background (compare <xref ref-type="fig" rid="fig8">Figure 8H</xref> to <xref ref-type="fig" rid="fig7">Figure 7C and H</xref>).</p><p>Additional results show that the model is relatively insensitive to the spatial extent of the normalization signal (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). The model predicts very similar temporal dynamics with the spatial extent of the background mask as small as twice the size of the target (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). When the background and target are the same size, the model predicts that sufficiently high contrast background will also drive the same biphasic temporal dynamics (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>, rows 7 and 8). To account for the orientation-dependent neural and behavioral masking effects, the model requires an orientation tuned normalization (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref>).</p><p>Overall, our results suggest that a simple model with delayed and orientation tuned divisive gain control can qualitatively capture the complex spatiotemporal dynamics of V1 population responses to localized oriented targets added to oriented backgrounds.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>To test the hypothesis that nonlinear computations in V1 contribute to the perceptual effect of similarity masking, we used voltage-sensitive dye imaging (VSDI) to measure neural population responses from V1 in two macaque monkeys while they performed a visual detection task in which a small oriented target was detected in the presence of a larger background of varied orientations. Like human observers, the monkeys were strongly affected by the orientation similarity of the target and the background (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Their detection threshold increased with increased target-background orientation similarity, while their reaction times showed complex, and in some cases non-monotonic, dependency on target-background orientation similarity.</p><p>To quantify the neural effects of similarity masking, we measured neural sensitivity to the target at two fundamental spatial scales of V1 topographic representations. The large scale of the retinotopic map and the finer scale of the columnar orientation map. We discovered that at both scales, V1 population responses to the target and background display two distinct phases (<xref ref-type="fig" rid="fig4">Figure 4B and H</xref>, <xref ref-type="fig" rid="fig5">Figure 5B and H</xref>). An initial transient phase in which target-evoked V1 response is strongest when the target and background have similar orientations. At this early phase, V1 responses are therefore paradoxically anti-correlated with the behavioral effect of similarity masking (<xref ref-type="fig" rid="fig4">Figure 4E and K</xref>, <xref ref-type="fig" rid="fig5">Figure 5E and K</xref>). In the second phase, the masking effect reverses, and the target-evoked response is maximally reduced when the target and background are similar. In this second sustained phase, V1 population responses are therefore consistent with the behavioral similarity masking effect. To explore the possibility that these biphasic dynamics reflect contributions from decision- and/or attention-related top-down signals rather than from low-level nonlinear encoding mechanisms in V1, we re-examined our results while excluding error trials (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). We found that the biphasic dynamics hold even for the subset of correct trials, reducing the likelihood that decision/attention-related signals play a major role in explaining our results.</p><p>The positive correlation between the neural and behavioral masking effects occurred earlier (<xref ref-type="fig" rid="fig5">Figure 5E and K</xref> vs. <xref ref-type="fig" rid="fig4">Figure 4E and K</xref>) and was more robust at the columnar scale than at the retinotopic scale (<xref ref-type="fig" rid="fig5">Figure 5D and J</xref> vs. <xref ref-type="fig" rid="fig4">Figure 4D and J</xref>; see also <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>). In addition, while the temporally integrated columnar response was positively correlated with behavior across all target and background contrasts tested (<xref ref-type="fig" rid="fig5">Figure 5E and K</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D</xref>), the integrated retinotopic responses were uncorrelated, or in some cases anticorrelated, with behavior (<xref ref-type="fig" rid="fig4">Figure 4E and K</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref>). These results suggest that behavioral performance in our task is dominated by columnar scale V1 signals in the second phase of the response. To the best of our knowledge, this is the first demonstration of such decoupling between V1 responses at the retinotopic and columnar scales, and the first demonstration that columnar scale signals are a better predictor of behavioral performance in a detection task.</p><p>Due to the challenges of setting up these experiments, we were unable to collect all target/background contrast combinations from both monkeys. However, in the common conditions, the results appear similar in the two animals, and the key results seem to be robust to the contrast combination in the animal where a wider range of contrast combinations was tested (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>We find that when the target and background have similar orientations, columnar-scale information about the target is restricted to the first phase of the response and then largely disappears during the second phase of the response. These physiological results could be related to the surprising mismatch between task difficulty and reaction times (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Rather than having reaction times that monotonically increase with task difficulty, in our masking detection task, reaction times can be shortest when target and background orientations match, even though it is hardest to detect the target under these conditions. The short reaction time to this stimulus may be the consequence of the target information being best represented in the early phase of the response.</p><p>The nature of contextual modulations in V1 is quite complex (<xref ref-type="bibr" rid="bib5">Angelucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Angelucci and Bressloff, 2006</xref>; <xref ref-type="bibr" rid="bib7">Bai et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Cavanaugh et al., 2002b</xref>; <xref ref-type="bibr" rid="bib24">Henry et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Michel et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Polat et al., 1998</xref>; <xref ref-type="bibr" rid="bib32">Sceniak et al., 1999</xref>; <xref ref-type="bibr" rid="bib41">Shushruth et al., 2012</xref>). A second goal of our study was to quantitatively characterize the spatiotemporal dynamics of columnar-scale V1 population responses to targets and backgrounds of different orientations and contrasts. Using a dynamic population vector analysis, we find that in the presence of an oriented background, the peak of the population orientation tuning to the target can deviate significantly from target orientation. For example, in some conditions, we observe a population orientation tuning peak that is repelled away from target orientation in the direction opposite to background orientation (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). These orientation-dependent interactions could contribute to non-veridical perceptual representations of orientation such as in the well-known tilt illusion effect (<xref ref-type="bibr" rid="bib16">Clifford, 2014</xref>; <xref ref-type="bibr" rid="bib35">Schwartz et al., 2007</xref>; <xref ref-type="bibr" rid="bib46">Wenderoth and Johnstone, 1987</xref>). An important goal for future studies would be to test for this possibility.</p><p>Using the population vector analysis, we find that columnar scale V1 representations are initially dominated by the orientation of the background. The target orientation then appears in the second phase of the response, which leads to curved population vector trajectories (<xref ref-type="fig" rid="fig7">Figure 7C and H</xref>). Identifying possible perceptual consequences of such dynamic and complex trajectories, and understanding the neural circuit mechanisms that give rise to such responses, are two important goals for future work.</p><p>Nonlinear response properties in V1 are commonly modeled as a consequence of a divisive gain control mechanism (<xref ref-type="bibr" rid="bib2">Albrecht and Geisler, 1991</xref>; <xref ref-type="bibr" rid="bib10">Carandini and Heeger, 1994</xref>; <xref ref-type="bibr" rid="bib22">Heeger, 1991</xref>; <xref ref-type="bibr" rid="bib23">Heeger, 1992</xref>; <xref ref-type="bibr" rid="bib42">Sit et al., 2009</xref>). As a first step toward understanding the mechanisms that could give rise to the observed V1 responses, we tested whether a simple dynamic gain control model could account for our findings (<xref ref-type="fig" rid="fig8">Figure 8</xref>). We find that a simple gain control model can qualitatively account for our results, but that in order to do so, the model has to display two important properties. First, to account for the biphasic nature of V1 response, the divisive normalization signals have to be delayed relative to the excitatory signal. Second, in order to account for the reduced neural sensitivity with target-background similarity in the second phase of the response, the divisive normalization signal has to be orientation selective (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplement 4</xref>). Because in primates and carnivores, robust orientation selectivity first emerges in V1 (<xref ref-type="bibr" rid="bib25">Hubel and Wiesel, 1959</xref>; <xref ref-type="bibr" rid="bib26">Hubel and Wiesel, 1968</xref>), these results suggest that a significant portion of the nonlinear interactions observed in the current study originate in V1 rather than being inherited from the ascending inputs that V1 receives from the LGN. While our experimental and computational results point to a delayed gain control signal that operates at the level of V1, they do not directly speak to the circuit and biophysical mechanisms that contribute to the implementation of this gain control in V1. Multiple candidate mechanisms for implementing gain control in V1 have been proposed (<xref ref-type="bibr" rid="bib5">Angelucci et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Angelucci and Bressloff, 2006</xref>; <xref ref-type="bibr" rid="bib29">Ozeki et al., 2009</xref>; <xref ref-type="bibr" rid="bib31">Rubin et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Tsodyks et al., 1997</xref>). Our results provide new and powerful constraints for such mechanistic models.</p><p>A key difference between our study and previous center-surround studies (e.g. <xref ref-type="bibr" rid="bib11">Cavanaugh et al., 2002a</xref>; <xref ref-type="bibr" rid="bib12">Cavanaugh et al., 2002b</xref>; <xref ref-type="bibr" rid="bib24">Henry et al., 2020</xref>; <xref ref-type="bibr" rid="bib41">Shushruth et al., 2012</xref>) is the stimuli that we used. First, in our experiments, the target and the mask were additive, while in most previous center-surround studies the target occludes the background. Such studies therefore restrict the mask to the surround while our study allows target-mask interactions at the center. Second, most previous center-surround studies have a sharp-edged target/surround border, while in our experiments no sharp edges were present. Unpublished results from our lab suggest that such sharp edges have a large impact on V1 population responses. Third, our stimuli were flashed for a short interval of 250ms corresponding to a typical duration of a fixation in natural vision, while most previous center-surround studies used either longer-duration drifting stimuli or very short-duration random-order stimuli for reverse-correlation analysis.</p><p>Because our targets are added to the background rather than occluding it, it is likely that a significant portion of the behavioral and neural masking effects that we observe come from target-mask interactions at the target location rather than from the effect of the mask in the surround. Several lines of evidence support this possibility. First, in human subjects, perceptual similarity masking effects can be almost entirely accounted for by target-mask interactions at the target location and are recapitulated when the mask has the same size and location as the target (<xref ref-type="bibr" rid="bib37">Sebastian et al., 2017</xref>). There is a reduction in masking when the background is windowed to the target envelope, but this effect is due to removing background within the target envelope (<xref ref-type="bibr" rid="bib37">Sebastian et al., 2017</xref>). Second, in our computational model (<xref ref-type="fig" rid="fig8">Figure 8</xref>), the effects of mask orientation on the dynamics of the response are qualitatively similar if the mask is restricted to the size and location of the target and mask contrast is increased (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). Third, in our model, the results are qualitatively the same when the spatial pooling region for the normalization signal is the same as that for the excitation signal (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). These considerations suggest that center-surround interactions may not be necessary for neural and behavioral masking effects with additive targets.</p><p>Finally, we note that the tuned similarity normalization that explains the neural and behavioral similarity-masking effects reported here is consistent with a principled encoding strategy for feature detection under natural conditions. When viewing a static scene under natural conditions, human and non-human primates make 3–4 saccadic eye movements per second, with fixations between saccades of 200–300ms. Given the typical size of the saccades, most visual receptive fields are stimulated during each fixation by a largely statistically independent random sample of natural image (<xref ref-type="bibr" rid="bib18">Frazor and Geisler, 2006</xref>). Analysis of the responses of linear receptive fields to random samples of natural image shows that the standard deviation of the response increases in proportion to the product of the luminance, the contrast and the similarity of the natural background to the receptive field (<xref ref-type="bibr" rid="bib37">Sebastian et al., 2017</xref>). Thus, divisive normalization by the product of luminance, contrast and similarity causes the standard deviation of the responses across natural images to be much more constant (i.e. nearly independent of the luminance, contrast and similarity of the background within the receptive field). This more constant standard deviation makes it possible, with relatively simple decoders, to reach near optimal feature-detection performance, under the high levels of stimulus uncertainty that occur under natural conditions (<xref ref-type="bibr" rid="bib34">Schwartz and Simoncelli, 2001</xref>; <xref ref-type="bibr" rid="bib37">Sebastian et al., 2017</xref>).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>All procedures have been approved by the University of Texas Institutional Animal Care (IACUC protocol #AUP-2016–00274) and Use Committee and conform to NIH standards.</p><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Voltage-sensitive dye</td><td align="left" valign="bottom">RH1691; RH1838</td><td align="left" valign="bottom">Optical Imaging Inc.</td><td align="left" valign="bottom">RH1691; RH1838</td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Widefield voltage-sensitive dye imaging</title><p>The experimental technique for widefield voltage-sensitive dye (VSD) imaging of neural response in awake, behaving macaques was adapted from previous studies (<xref ref-type="bibr" rid="bib7">Bai et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bib14">Chen et al., 2008</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2012</xref>). Briefly, two adult male rhesus macaque monkeys (Monkey H, 8 years old; and Monkey T, 7 years old) were implanted with a metal head post and metal recording chambers located over the dorsal portion of V1, a region representing the lower contralateral visual field at eccentricities of 2–5°. Craniotomy and durotomy were performed. A transparent artificial dura made of silicone was used to protect the brain while allowing optical access for imaging (<xref ref-type="bibr" rid="bib6">Arieli et al., 2002</xref>; <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Experiments were conducted in the left hemisphere chamber of Monkey H, and in both left and right hemisphere chambers of Monkey T.</p><p>VSD imaging was used to record neural population activity at a high resolution in space and time (<xref ref-type="bibr" rid="bib40">Shoham et al., 1999</xref>). Before each experiment, VSD (RH1691 or RH1838, Optical Imaging, Inc) was topically applied on the cortex for 2 hr to allow the VSD molecules to bind to cellular membranes. In Monkey H, fluorescence from neural activity was recorded using Imager 3001 (Optical Imaging, Inc) using a tungsten-halogen light source (Zeiss). An infrared eye-tracker (Dr Bouis Inc) was used to monitor eye position. In Monkey T, florescence was recorded using custom Matlab software interfaced to PCO Edge 4.2 sCMOS camera (Excelitas PCO GmbH) using X-Cite 110LED light source (Excelitas Technologies Corp). Eye position was monitored using an Eyelink 100 Plus video eye-tracker (SR Research Ltd).</p><p>Both imaging systems were interfaced to a double-SLR-lens-macro system with housing for dichroic mirrors in between the two SLR lenses. The combination of a 50 mm fixed-focus objective lens (cortex end, Nikkor 50 mm f/1.2) and an 85 mm fixed-focused (Canon EF 85 mm f/1.2 L USM) camera lens provided 1.7 x magnification, corresponding to imaging approximately an 8x8 mm<sup>2</sup> area of the cortex. Fluorescence signals were measured through a dichroic mirror (650 nm long‐pass filter) and an emission filter (RG 665). VSD molecules were excited by light at 630  nm. Imaging data were collected at 512×512 resolution at 100  Hz. Data acquisition was time locked to the animal’s heartbeat (EKG QR up-stroke, HP Patient Monitor HP78352C). More details about optical imaging with VSD in behaving monkeys are described elsewhere (<xref ref-type="bibr" rid="bib7">Bai et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bib14">Chen et al., 2008</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2012</xref>).</p><p>Prior to the main experiments, VSD imaging was used to obtain a precise retinotopic map of the entire recording (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>; <xref ref-type="bibr" rid="bib48">Yang et al., 2007</xref>). In two out of three chambers, retinotopic maps indicate that V1 extended into the lunate sulcus. In the third chamber, V1 terminated ~0.75 mm from the lunate sulcus. The area used for decoding analysis was chosen to entirely lie within V1.</p></sec><sec id="s4-2"><title>Behavioral task with optical stimulation</title><p>Monkeys were trained to detect a small additive horizontal Gabor target (4cpd, with σ=0.14°, 0.33° FWHM envelope) centered on a sinusoidal grating background mask of the same spatial frequency (4° raised-cosine windowed). The background grating was oriented at 0°, ±15°, ±30°, ±45°, ±60, and 90° from the Gabor target orientation. Both the Gabor and background grating were bright-centered – that is, the 0° orientation background was completely in phase with the target. The contrast of the target and background were varied in combinations of levels, reported in Michelson contrast:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>For each experiment, a Fixation recording block and a Detection recording block were made using the same target and background conditions. In both blocks, the target and background were centered at a fixed position for each experiment corresponding to the working cortical chamber. This position varied between experiments from 1.6 to 3 deg of visual angle eccentricity from the fixation point and from 20 to 50 deg of polar angle from the vertical meridian in the corresponding hemifield (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). At these coordinates, the spatial extent of the target was fully imaged through the cortical window, and the larger oriented background uniformly activated the entire imaging area.</p><p>In the Fixation block, the monkeys were required to remain fixated for each imaging trial while either the target or full-field sinusoidal gratings at 100% contrast were flashed at 5 Hz (60ms ON, 140ms OFF) for 1.0 s. These recordings were processed to obtain retinotopic and columnar orientation response maps that were used to decode the detection recording responses from the same experiment day (<xref ref-type="fig" rid="fig3">Figure 3C–E</xref>).</p><p>In the Detection blocks, a background with random orientation appeared on every trial, with a 50% chance of an accompanying Gabor target (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>). The monkeys were tasked to report the presence of the target. Each trial began with fixation on a bright 0.1° square. An auditory tone and the dimming of the fixation square cued the monkey to the start of the detection task trial. 250ms later, the background with or without the target was presented. The monkeys were trained to maintain gaze at the fixation cue on target absent trials or saccade to and hold gaze (for 150ms) at the target position to indicate target detection (with a 75ms minimum allowed reaction time). When the target was present, it remained on screen for a maximum of 250ms or was extinguished immediately upon the monkeys’ saccade initiation. The monkey was given 600ms to make the saccade or to hold fixation and was subsequently rewarded on correct choices: stay (correct reject) on target absent trials, or saccade to target (hit) on target present trials. The target and background contrast level were fixed for each recording block. The probability of each orientated background and of target presence were balanced for each recording block.</p><p>A separate target only Detection block on uniform gray background was also taken on each experiment day using the same routine as above. This block was later used as the reference data to normalize response amplitudes across experiment days.</p><p>Experiments were conducted with custom code using TEMPO real-time control system (Reflective Computing). The visual stimulus was presented on a Sony CRT (1024x768 @ 100 Hz), distanced 108 cm from the animal (50 pixels-per-degree), with mean luminance 50 cd/m<sup>2</sup>. The visual stimulus was generated using in-house real-time graphics software (glib).</p></sec><sec id="s4-3"><title>Behavior performance and reaction time</title><p>Behavior performance was calculated for each background orientation and reported in units of detection sensitivity index d’ (d-prime). D-prime and criterion were estimated as:<disp-formula id="equ2"> ,<mml:math id="m2"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>⟮</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟯</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>⟮</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟯</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>⟮</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟯</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>⟮</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟯</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf4"><mml:msup><mml:mrow><mml:mi>Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the inverse transform of the cumulative normal distribution; and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represent the proportion of hits and false alarms respectively. To avoid leaving out the data in some conditions (e.g., when there are no false alarms), we scaled all the proportions to be between 0.005 and 0.995 (<inline-formula><mml:math id="inf7"><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn><mml:mo>+</mml:mo><mml:mn>0.99</mml:mn><mml:mo>∙</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>).</p><p>Mapping between the unbiased percentage correct response and d’ is as follows and is depicted in <xref ref-type="fig" rid="fig2">Figure 2B</xref>:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mspace width="2em"/><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>D-prime performance across orientations was fitted with an inverted, dc-shifted Gaussian:<disp-formula id="equ4"><mml:math id="m4"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:math></disp-formula></p><p>The reaction times of the animals were calculated from the stimulus onset to the onset of saccade. Consequently, there was no measurement of reaction time on trials where the animals remain fixated at the fixation point.</p></sec><sec id="s4-4"><title>VSD imaging</title><p>For each trial, an image sequence was captured for a total of 1.2 s including pre-stimulus and post-stimulus frames. The image sequence was analyzed to extract the response using a variant of the previous reported routines (<xref ref-type="bibr" rid="bib7">Bai et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bib14">Chen et al., 2008</xref>; <xref ref-type="bibr" rid="bib15">Chen et al., 2012</xref>).</p><p>Image stabilization was introduced as the first stage of pre-processing to de-accentuate blood vessel edges in the ΔF/F response map caused by micro movements of the camera and/or the cortex during imaging. The image intensity across time at each individual pixel was modeled with separable motion-free (<inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) and motion-related (<inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) components as follows:<disp-formula id="equ5"><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>For each trial, a single global motion vector <inline-formula><mml:math id="inf10"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> was obtained by estimating the translational motion of the center portion of the images (1/4 of the imaging area). The motion coefficients <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each pixel was then obtained using least squares fitting to the model. The motion-corrected image is <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> . This approach to image stabilization (compared to traditional image registration approach) has the advantage of correcting for non-rigid movements (rotations, expansion/contractions, affine transformation, local distortions, etc.) and sub-pixel motion.</p></sec><sec id="s4-5"><title>Retinotopic and columnar template decoding</title><p>Template decoding was used to summarize the retinotopic and columnar response for each image frame. The retinotopic response map of the target and columnar orientation map of the imaging area were estimated from the Fixation blocks, in which response were stimulated with visual presentation at 5 Hz. The preprocessing steps for the Fixation blocks were: image stabilization, 5 Hz FFT response extraction, ΔF/F normalization, then down-sampling to 128x128 pixels (from 512x512). Baseline florescence (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) was estimated by the average florescence over frames –80 to 0ms relative to stimulus onset. ΔF/F was calculated as:<disp-formula id="equ6"><mml:math id="m6"><mml:mfrac><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfrac><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>The retinotopic response map of the Gabor target (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) was estimated by fitting a 2D Gaussian over the 5 Hz flashing Gabor amplitude response. The full-ROI (8x8 mm<sup>2</sup> imaging area) columnar orientation response maps (<inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) were estimated from the flashing full-field grating, where the 5 Hz FFT grating response amplitudes were bandpass filtered from 0.8 to 3.0 cycles/mm and the orientation tuning of each pixel was estimated as described previously (<xref ref-type="bibr" rid="bib15">Chen et al., 2012</xref>). Subsequently, the full-ROI orientation map was windowed by the retinotopic map to co-localize the retinotopic and the columnar decoders. The columnar map, comprised of pixelwise response magnitude (<inline-formula><mml:math id="inf16"><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) and tuning angle (<inline-formula><mml:math id="inf17"><mml:mi>θ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>), was represented in modified Euler’s form:<disp-formula id="equ7"><mml:math id="m7"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mi>θ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></disp-formula></p><p>These maps served as templates for decoding the retinotopic and columnar response from the Detection blocks. To further reduce the effect of motion artefacts, a pixel-wise reliability weighted approach was adopted. The Detection blocks were preprocessed with image stabilization, down-sampling to 128x128, and ΔF/F normalization as above. From the pre-processed images, the retinotopic scale variance <inline-formula><mml:math id="inf18"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and columnar scale variance <inline-formula><mml:math id="inf19"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> were calculated to be used as reliability weights. The variance was obtained from condition-mean subtracted residuals taken across all frames across trials, with the ΔF/F response (<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) bandpass filtered between 0.8 and 3.0 cycles/mm (<inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) for the columnar scale variance. Reliability weighting was implemented by normalizing each template pixel by the corresponding pixel-wise variance.<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Two different columnar decoding methods were employed. The first examined the overall response aligned to the orientation of the Gabor (0° orientation tuning axis). A second columnar decoding scheme was employed to examine the full orientation population response. This second scheme was comprised of 12 decoders that evenly partitioned the orientation space, such that each decoder contained the column response magnitude in a subset of pixels tuned with ±7.5° centered at –75° to 90° in 15° steps. Each decoder therefore represents the population response of similarly tuned neural ensembles spanning the orientation space every 15°. Each decoder was normalized by the summed response magnitude of its pixel subset; in this way, each sub-population response was equally represented in the population tuning curve.</p><p>The formulation for the reliability-weighted templates and the decoding is summarized below for the retinotopic response time course (<inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>), columnar response time course (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>), columnar population tuning time course (<inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>).<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-6"><title>Response pooling across experiments</title><p>Across experiments, the varying effectiveness of VSD staining led to large variations in the noise level and amplitude of the ΔF/F response. For pooling data across experiments, response amplitudes were normalized based on singular value decomposition (SVD). Here, the decoded response of the reference target only Detection block was used. These were trials of varying target contrast levels on the uniform, mean luminance background instead of the oriented grating background, collected on the same day. The assumption is that the neural response amplitude and dynamics to the target should be the same irrespective of the experiment day. Retinotopic and columnar decoder responses for the target only block were calculated as with the background detection block as described above. Target contrast response for each experiment were interpolated so that all assessed target contrast levels across experiments were represented, then SVD was performed over the frames –100 to 200ms about the stimulus onset across all experiments. In this way, the first component of the SVD (SVD1) represented the average neural dynamics of the target contrast response, and the SVD1 coefficients represent the magnitude of this target contrast response each experiment day. Response from each experiment was then scaled to match the experiment with the largest SVD1 coefficient. This was performed for each decoded response separately. When pooling data for the mean and standard deviation, the inverse of the scaling factor squared was used as the reliability weighting.<disp-formula id="equ12"> ,<mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Lastly, responses are expressed in a modified z-score, obtained by normalizing the response by its standard deviation. To calculate this standard deviation, responses were grouped by presented stimuli, and the means of each group was subtracted. The residuals from the mean-subtraction from frames 50–250ms post stimulus onset was pooled according to the aforementioned experiment reliability weights <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to obtain the response standard deviation.</p><p>Response beyond the saccade may contain unwanted signals. Frames beyond the reaction time for each trial were therefore omitted from summary statistics. For the integrating response within trials, responses were averaged up to the frame of saccade. For frame-by-frame averaging across trials for response time course, trials were dropped out from the averaging beyond their reaction time frame.</p></sec><sec id="s4-7"><title>Descriptive trends across background orientation</title><p>Trends were fitted to the normalized VSD response across background orientations (e.g. gray curve in <xref ref-type="fig" rid="fig4">Figure 4D</xref>). VSD responses were first averaged over a specific range of frames, and for illustration only the trends of the averaged response across orientations were fitted with either a flat line or a Gaussian.<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:msub><mml:mover><mml:mi>r</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:msub><mml:mover><mml:mi>r</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The best fitting trend by the F-test were chosen for display in the figures.<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mfrac><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-8"><title>Behavior correlation</title><p>The retinotopic and the columnar response time courses were correlated against the monkey’s behavior judgement. Pearson’s correlation coefficient was estimated between the instantaneous response in time and overall behavior sensitivity index <italic>d</italic>’. To account for the different trial counts between background orientations, a trial count weighted version of the correlation coefficient was adopted:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∑</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The p-value for the weighted coefficients were estimated using the standard t-score replacing the degree of freedom with an entropy based effective estimate from the weights.<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>ρ</mml:mi><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:msqrt></mml:mrow><mml:msqrt><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>∼</mml:mo><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"> <mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-9"><title>Normalization model of orientation masking dynamics</title><p>A simple model of the neuronal population response with divisive normalization described our results qualitatively. In this model, orientation columnar response was tuned to one of 12 different orientations: –75° to 90° in 15° increments. The responses of each orientation column were specified by the simple normalization model summarized in <xref ref-type="fig" rid="fig8">Figure 8</xref>. The input stimulus is specified by the contrast and orientation of the target, the contrast and orientation of the background, and the duration of the stimulus: <inline-formula><mml:math id="inf26"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>. In the model, the input stimulus generates an excitation signal <inline-formula><mml:math id="inf27"><mml:semantics><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> that is linear with stimulus contrast and a normalization signal <inline-formula><mml:math id="inf28"><mml:semantics><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> that is also linear with stimulus contrast. Without loss of generality, all signals were scaled by the 24% contrast target only response averaged over 50–200ms. These excitation and normalization signals are controlled by the excitation and normalization parameters, <inline-formula><mml:math id="inf29"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Ω</mml:mi></mml:mstyle><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Ω</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, described below. The normalization signal is then combined with a normalization constant <inline-formula><mml:math id="inf31"><mml:semantics><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> to obtain the normalization factor. The normalization constant limits how small the normalization factor can become. The normalized response is obtained by dividing the excitation signal by the normalization factor. The final response is then obtained by applying a response exponent <italic>p</italic>, which is similar to applying a spiking nonlinearity:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mroot><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mi>p</mml:mi></mml:mroot></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="fig" rid="fig8">Figure 8</xref>, we show the final responses corresponding to the center of the stimulus <inline-formula><mml:math id="inf32"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. The excitatory response at that location for background, and target plus background, is obtained by convolving the effective input contrast signals with the spatial-temporal impulse-response function and evaluating at <inline-formula><mml:math id="inf33"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula><disp-formula id="equ22"><mml:math id="m22"><mml:semantics><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> </mml:mrow></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf34"><mml:semantics><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf35"><mml:semantics><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> are the effective input contrast signals, and <inline-formula><mml:math id="inf36"><mml:semantics><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> is the spatiotemporal impulse response function. The effective excitatory contrast of the background for the orientation channel with preferred orientation <inline-formula><mml:math id="inf37"><mml:semantics><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is given by<disp-formula id="equ23"><mml:math id="m23"><mml:semantics><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf38"><mml:semantics><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> is the background contrast, <inline-formula><mml:math id="inf39"><mml:semantics><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the falloff parameter of column’s orientation turning function, and <inline-formula><mml:math id="inf40"><mml:semantics><mml:mrow><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> is a temporal pulse function of width <inline-formula><mml:math id="inf41"><mml:semantics><mml:mi>D</mml:mi></mml:semantics></mml:math></inline-formula>. Similarly, the effective target contrast is given by<disp-formula id="equ24"><mml:math id="m24"><mml:semantics><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The effective excitatory contrast for target plus background is slightly more complicated because there can be some contrast summation or cancellation depending on the phase and orientation of the target relative to the background<disp-formula id="equ25"><mml:math id="m25"><mml:semantics><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf42"><mml:semantics><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> is the contrast correction factor,<disp-formula id="equ26"><mml:math id="m26"><mml:semantics><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The spatiotemporal impulse response function is the separable product of a Gaussian distribution and a gamma distribution<disp-formula id="equ27"><mml:math id="m27"><mml:semantics><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf43"><mml:semantics><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the standard deviation of the 2D Gaussian and <inline-formula><mml:math id="inf44"><mml:semantics><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> are the two parameters of a gamma distribution: <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The formulas for the normalization signal are the same as for the excitatory signal, except the four parameters are allowed to differ: <inline-formula><mml:math id="inf46"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Ω</mml:mi></mml:mstyle><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math id="inf47"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Ω</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>.</p><p>Parameter values from known properties of single neurons in primary visual cortex were adopted. The response exponent <inline-formula><mml:math id="inf48"><mml:mi>p</mml:mi></mml:math></inline-formula> was constrained to 2.0, consistent with single neuron contrast-response functions (<xref ref-type="bibr" rid="bib1">Albrecht and Hamilton, 1982</xref>; <xref ref-type="bibr" rid="bib19">Geisler and Albrecht, 1997</xref>; <xref ref-type="bibr" rid="bib36">Sclar et al., 1990</xref>). It was assumed that the peak orientation of the excitatory signal and suppressive normalization signal <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were the same (<xref ref-type="bibr" rid="bib11">Cavanaugh et al., 2002a</xref>), but that the orientation bandwidth of the normalization signal was greater than that of the excitation signal, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib12">Cavanaugh et al., 2002b</xref>), and that the spatial pooling region for the normalization signal was larger than that for the excitation signal, <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib11">Cavanaugh et al., 2002a</xref>; <xref ref-type="bibr" rid="bib12">Cavanaugh et al., 2002b</xref>; <xref ref-type="bibr" rid="bib27">Levitt and Lund, 2002</xref>; <xref ref-type="bibr" rid="bib33">Sceniak et al., 2001</xref>). As shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>, we found that setting <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> had little effect on the modeling of our empirical results (<xref ref-type="fig" rid="fig5">Figure 5</xref>); for simplicity, our final model assumed <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Finally, it was assumed that the temporal dynamics of the normalization signal, determined by parameters <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , are slower than those for the excitation signal, determined by parameters <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib21">Groen et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Zhou et al., 2019</xref>).</p><p>The following were the parameter values used in <xref ref-type="fig" rid="fig8">Figure 8</xref>: <italic>r</italic><sub>0</sub>=0.03125,<disp-formula id="equ28"><mml:math id="m28"><mml:msub><mml:mrow><mml:mi>Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:msup><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ29"><mml:math id="m29"><mml:msub><mml:mrow><mml:mi>Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10.64</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:msup><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>All analyses were done using Matlab R2018a.</p></sec><sec id="s4-10"><title>Statistics</title><p>Two animals were examined to verify the consistency of experimental approach and results. Multiple recordings were made from the same animals. The number of recordings were based on previous experience; no statistical method was used to predetermine sample size.</p><p>Statistical analyses were conducted in Matlab (R2018a).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All procedures have been approved by the University of Texas Institutional Animal Care (IACUC protocol #AUP-2016-00274) and Use Committee and conform to NIH standards.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Supplementary perceptual masking demonstration.</title></caption><media xlink:href="elife-89570-supp1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89570-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data and script necessary to regenerate figures and statistics reported are available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.10815850">https://doi.org/10.5281/zenodo.10815850</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>SC</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Neural Correlates of Perceptual Similarity Masking in Primate V1</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10815850</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank We thank members of Seidemann and Geisler laboratories for their assistances with this project. This work was supported by NIH grants EY-016454 to ES, EY-024662 to WSG and ES, BRAIN U01-NS099720 to ES and WSG, and DARPA-NESD0-N66001-17-C-4012 to ES.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>DG</given-names></name><name><surname>Hamilton</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Striate cortex of monkey and cat: contrast response function</article-title><source>Journal of Neurophysiology</source><volume>48</volume><fpage>217</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1152/jn.1982.48.1.217</pub-id><pub-id pub-id-type="pmid">7119846</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>DG</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Motion selectivity and the contrast-response function of simple cells in the visual cortex</article-title><source>Visual Neuroscience</source><volume>7</volume><fpage>531</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1017/s0952523800010336</pub-id><pub-id pub-id-type="pmid">1772804</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allman</surname><given-names>J</given-names></name><name><surname>Miezin</surname><given-names>F</given-names></name><name><surname>McGuinness</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Stimulus specific responses from beyond the classical receptive field: neurophysiological mechanisms for local-global comparisons in visual neurons</article-title><source>Annual Review of Neuroscience</source><volume>8</volume><fpage>407</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.08.030185.002203</pub-id><pub-id pub-id-type="pmid">3885829</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelucci</surname><given-names>A</given-names></name><name><surname>Bressloff</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Contribution of feedforward, lateral and feedback connections to the classical receptive field center and extra-classical receptive field surround of primate V1 neurons. Perception, Part 1, Fundamentals of Vision: Low and Mid-Level Processes in Perception</article-title><source>Progress in Brain Research</source><volume>154</volume><fpage>93</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)54005-1</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelucci</surname><given-names>A</given-names></name><name><surname>Bijanzadeh</surname><given-names>M</given-names></name><name><surname>Nurminen</surname><given-names>L</given-names></name><name><surname>Federer</surname><given-names>F</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Bressloff</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Circuits and mechanisms for surround modulation in visual cortex</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>425</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031418</pub-id><pub-id pub-id-type="pmid">28471714</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Slovin</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dural substitute for long-term imaging of cortical activity in behaving monkeys and its clinical implications</article-title><source>Journal of Neuroscience Methods</source><volume>114</volume><fpage>119</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/s0165-0270(01)00507-6</pub-id><pub-id pub-id-type="pmid">11856563</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Similar masking effects of natural backgrounds on detection performances in humans, macaques, and macaque-V1 population responses</article-title><source>Journal of Neurophysiology</source><volume>125</volume><fpage>2125</fpage><lpage>2134</lpage><pub-id pub-id-type="doi">10.1152/jn.00275.2020</pub-id><pub-id pub-id-type="pmid">33909494</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benvenuti</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Ramakrishnan</surname><given-names>C</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Scale-invariant visual capabilities explained by topographic representations of luminance and texture in primate V1</article-title><source>Neuron</source><volume>100</volume><fpage>1504</fpage><lpage>1512</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.020</pub-id><pub-id pub-id-type="pmid">30392796</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>FW</given-names></name><name><surname>Kulikowski</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Orientational selectivity of the human visual system</article-title><source>The Journal of Physiology</source><volume>187</volume><fpage>437</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1966.sp008101</pub-id><pub-id pub-id-type="pmid">5972183</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Summation and division by neurons in primate visual cortex</article-title><source>Science</source><volume>264</volume><fpage>1333</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1126/science.8191289</pub-id><pub-id pub-id-type="pmid">8191289</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanaugh</surname><given-names>JR</given-names></name><name><surname>Bair</surname><given-names>W</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2002">2002a</year><article-title>Nature and interaction of signals from the receptive field center and surround in macaque V1 neurons</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>2530</fpage><lpage>2546</lpage><pub-id pub-id-type="doi">10.1152/jn.00692.2001</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanaugh</surname><given-names>JR</given-names></name><name><surname>Bair</surname><given-names>W</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2002">2002b</year><article-title>Selectivity and spatial distribution of signals from the receptive field surround in macaque V1 Neurons</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>2547</fpage><lpage>2556</lpage><pub-id pub-id-type="doi">10.1152/jn.00693.2001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Optimal decoding of correlated neural population responses in the primate visual cortex</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1412</fpage><lpage>1420</lpage><pub-id pub-id-type="doi">10.1038/nn1792</pub-id><pub-id pub-id-type="pmid">17057706</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Optimal temporal decoding of neural population responses in a reaction-time visual detection task</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>1366</fpage><lpage>1379</lpage><pub-id pub-id-type="doi">10.1152/jn.00698.2007</pub-id><pub-id pub-id-type="pmid">18199810</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Palmer</surname><given-names>CR</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The relationship between voltage-sensitive dye imaging signals and spiking activity of neural populations in primate V1</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>3281</fpage><lpage>3295</lpage><pub-id pub-id-type="doi">10.1152/jn.00977.2011</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clifford</surname><given-names>CWG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The tilt illusion: phenomenology and functional implications</article-title><source>Vision Research</source><volume>104</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2014.06.009</pub-id><pub-id pub-id-type="pmid">24995379</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foley</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Human luminance pattern-vision mechanisms: masking experiments require a new model</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>11</volume><fpage>1710</fpage><lpage>1719</lpage><pub-id pub-id-type="doi">10.1364/josaa.11.001710</pub-id><pub-id pub-id-type="pmid">8046537</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazor</surname><given-names>RA</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Local luminance and contrast in natural images</article-title><source>Vision Research</source><volume>46</volume><fpage>1585</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.06.038</pub-id><pub-id pub-id-type="pmid">16403546</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Albrecht</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Visual cortex neurons in monkeys and cats: detection, discrimination, and identification</article-title><source>Visual Neuroscience</source><volume>14</volume><fpage>897</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1017/s0952523800011627</pub-id><pub-id pub-id-type="pmid">9364727</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Hildesheim</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>VSDI: A new era in functional imaging of cortical dynamics</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>874</fpage><lpage>885</lpage><pub-id pub-id-type="doi">10.1038/nrn1536</pub-id><pub-id pub-id-type="pmid">15496865</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Piantoni</surname><given-names>G</given-names></name><name><surname>Montenegro</surname><given-names>S</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Devore</surname><given-names>S</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Dugan</surname><given-names>P</given-names></name><name><surname>Friedman</surname><given-names>D</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Temporal dynamics of neural responses in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>7562</fpage><lpage>7580</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1812-21.2022</pub-id><pub-id pub-id-type="pmid">35999054</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Nonlinear model of neural responses in cat visual cortex</article-title><source>American PsycNet</source><volume>1</volume><fpage>119</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.7551/mitpress/2002.001.0001</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Normalization of cell responses in cat striate cortex</article-title><source>Visual Neuroscience</source><volume>9</volume><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1017/s0952523800009640</pub-id><pub-id pub-id-type="pmid">1504027</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname><given-names>CA</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distinct spatiotemporal mechanisms underlie extra-classical receptive field modulation in macaque V1 microcircuits</article-title><source>eLife</source><volume>9</volume><elocation-id>e54264</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54264</pub-id><pub-id pub-id-type="pmid">32458798</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields and functional architecture of monkey striate cortex</article-title><source>The Journal of Physiology</source><volume>195</volume><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id><pub-id pub-id-type="pmid">4966457</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>JB</given-names></name><name><surname>Lund</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The spatial extent over which neurons in macaque striate cortex pool visual signals</article-title><source>Visual Neuroscience</source><volume>19</volume><fpage>439</fpage><lpage>452</lpage><pub-id pub-id-type="doi">10.1017/s0952523802194065</pub-id><pub-id pub-id-type="pmid">12511077</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>MM</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nonlinear lateral interactions in V1 population responses explained by a contrast gain control model</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>10069</fpage><lpage>10079</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0246-18.2018</pub-id><pub-id pub-id-type="pmid">30282725</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozeki</surname><given-names>H</given-names></name><name><surname>Finn</surname><given-names>IM</given-names></name><name><surname>Schaffer</surname><given-names>ES</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Inhibitory stabilization of the cortical network underlies visual surround suppression</article-title><source>Neuron</source><volume>62</volume><fpage>578</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.028</pub-id><pub-id pub-id-type="pmid">19477158</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polat</surname><given-names>U</given-names></name><name><surname>Mizobe</surname><given-names>K</given-names></name><name><surname>Pettet</surname><given-names>MW</given-names></name><name><surname>Kasamatsu</surname><given-names>T</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Collinear stimuli regulate visual responses depending on cell’s contrast threshold</article-title><source>Nature</source><volume>391</volume><fpage>580</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1038/35372</pub-id><pub-id pub-id-type="pmid">9468134</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The stabilized supralinear network: A unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><volume>85</volume><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id><pub-id pub-id-type="pmid">25611511</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sceniak</surname><given-names>MP</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name><name><surname>Shapley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Contrast’s effect on spatial summation by macaque V1 neurons</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>733</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1038/11197</pub-id><pub-id pub-id-type="pmid">10412063</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sceniak</surname><given-names>MP</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name><name><surname>Shapley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual spatial characterization of macaque V1 neurons</article-title><source>Journal of Neurophysiology</source><volume>85</volume><fpage>1873</fpage><lpage>1887</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.85.5.1873</pub-id><pub-id pub-id-type="pmid">11353004</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural signal statistics and sensory gain control</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1038/90526</pub-id><pub-id pub-id-type="pmid">11477428</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Space and time in visual context</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>522</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1038/nrn2155</pub-id><pub-id pub-id-type="pmid">17585305</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sclar</surname><given-names>G</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>Lennie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Coding of image contrast in central visual pathways of the macaque monkey</article-title><source>Vision Research</source><volume>30</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90123-3</pub-id><pub-id pub-id-type="pmid">2321355</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebastian</surname><given-names>S</given-names></name><name><surname>Abrams</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Constrained sampling experiments reveal principles of detection in natural scenes</article-title><source>PNAS</source><volume>114</volume><fpage>E5731</fpage><lpage>E5740</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619487114</pub-id><pub-id pub-id-type="pmid">28652323</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seidemann</surname><given-names>E</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Slovin</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Dynamics of depolarization and hyperpolarization in the frontal cortex and saccade goal</article-title><source>Science</source><volume>295</volume><fpage>862</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1126/science.1066641</pub-id><pub-id pub-id-type="pmid">11823644</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seidemann</surname><given-names>E</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Linking V1 Activity to Behavior</article-title><source>Annual Review of Vision Science</source><volume>4</volume><fpage>287</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-102016-061324</pub-id><pub-id pub-id-type="pmid">29975592</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shoham</surname><given-names>D</given-names></name><name><surname>Glaser</surname><given-names>DE</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Kenet</surname><given-names>T</given-names></name><name><surname>Wijnbergen</surname><given-names>C</given-names></name><name><surname>Toledo</surname><given-names>Y</given-names></name><name><surname>Hildesheim</surname><given-names>R</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Imaging cortical dynamics at high spatial and temporal resolution with novel blue voltage-sensitive dyes</article-title><source>Neuron</source><volume>24</volume><fpage>791</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81027-2</pub-id><pub-id pub-id-type="pmid">10624943</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shushruth</surname><given-names>S</given-names></name><name><surname>Mangapathy</surname><given-names>P</given-names></name><name><surname>Ichida</surname><given-names>JM</given-names></name><name><surname>Bressloff</surname><given-names>PC</given-names></name><name><surname>Schwabe</surname><given-names>L</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Strong recurrent networks compute the orientation tuning of surround modulation in the primate primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>308</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3789-11.2012</pub-id><pub-id pub-id-type="pmid">22219292</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sit</surname><given-names>YF</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name><name><surname>Miikkulainen</surname><given-names>R</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Complex dynamics of V1 population responses explained by a simple gain-control model</article-title><source>Neuron</source><volume>64</volume><fpage>943</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.041</pub-id><pub-id pub-id-type="pmid">20064399</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stromeyer</surname><given-names>CF</given-names></name><name><surname>Julesz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Spatial-frequency masking in vision: critical bands and spread of masking*</article-title><source>Journal of the Optical Society of America</source><volume>62</volume><elocation-id>1221</elocation-id><pub-id pub-id-type="doi">10.1364/JOSA.62.001221</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>MV</given-names></name><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Paradoxical effects of external modulation of inhibitory interneurons</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4382</fpage><lpage>4388</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04382.1997</pub-id><pub-id pub-id-type="pmid">9151754</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>AB</given-names></name><name><surname>Solomon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Model of visual contrast gain control and pattern masking</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>14</volume><fpage>2379</fpage><lpage>2391</lpage><pub-id pub-id-type="doi">10.1364/josaa.14.002379</pub-id><pub-id pub-id-type="pmid">9291608</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wenderoth</surname><given-names>P</given-names></name><name><surname>Johnstone</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Possible neural substrates for orientation analysis and perception</article-title><source>Perception</source><volume>16</volume><fpage>693</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1068/p160693</pub-id><pub-id pub-id-type="pmid">3331424</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>HR</given-names></name><name><surname>McFarlane</surname><given-names>DK</given-names></name><name><surname>Phillips</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Spatial frequency tuning of orientation selective units estimated by oblique masking</article-title><source>Vision Research</source><volume>23</volume><fpage>873</fpage><lpage>882</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(83)90055-x</pub-id><pub-id pub-id-type="pmid">6636547</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Seidemann</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rapid and precise retinotopic mapping of the visual cortex obtained by voltage-sensitive dye imaging in the behaving monkey</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1002</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1152/jn.00417.2007</pub-id><pub-id pub-id-type="pmid">17522170</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predicting neuronal dynamics with a delayed gain control model</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007484</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007484</pub-id><pub-id pub-id-type="pmid">31747389</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89570.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ray</surname><given-names>Supratim</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study used Voltage Sensitive Dye Imaging (VSDI) to measure neural activity in the primary visual cortex of monkeys trained to detect an oriented grating target that was presented either alone or against an oriented mask. The authors show <bold>convincingly</bold> that the initial effect of the mask ran counter to the behavioral effects of the mask, a pattern that reversed in the latter phase of the response. They interpret these results in terms of influences from the receptive field center, and although an alternative view that emphasizes the role of the receptive field surround also seems reasonable, this study stands as an interesting contribution to our understanding of mechanisms of visual perception.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89570.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This is a clear account of some interesting work. The experiments and analyses seem well done and the data are useful. It is nice to see that VSDI results square well with those from prior extracellular recordings.</p><p>The authors have done a good job responding to the main points of my previous review. One important question remains, as stated in that review:</p><p>&quot;My reading is that this is primarily a study of surround suppression with results that follow pretty directly from what we already know from that literature, and although they engage with some of the literature they do not directly mention surround suppression in the text. Their major effect - what they repeatedly describe as a &quot;paradoxical&quot; result in which the responses initially show a stronger response to matched targets and backgrounds and then reverse - seems to pretty clearly match the expected outcome of a stimulus that initially evokes additional excitation due to increased center contrast followed by slightly delayed surround suppression tuned to the same peak orientation. Their dynamics result seems entirely consistent with previous work, e.g. Henry at al 2020, particularly their Fig. 3 <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/54264">https://elifesciences.org/articles/54264</ext-link>, so it seems like a major oversight to not engage with that work at all, and to explain what exactly is new here.&quot;</p><p>Their rebuttal of my first review is not convincing -- I still believe that surround influences are important and perhaps predominant in determining the outcome of the experiments. This is particularly clear for the &quot;paradoxical&quot; dynamics that they observe, which seem exactly to reflect the behavior of the surround.</p><p>The authors' arguments to the contrary are based on three main points. First, their stimuli cover the center and surround, unlike those of many previous experiments, so they argue that this somehow diminishes the impact of the surround. But the argument is not accompanied by data showing the effects of center stimuli alone or surround stimuli alone. Second, their model -- a normalization model -- does not need surround influences to account for the masking effect. Third, they cite human psychophysical masking results from their collaborators (Sebastian et al 2017), but do not cite an equally convincing demonstration that surround contrast creates potent orientation selective masking when presented alone (Petrov et al 2005, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2871-05.2005">https://doi.org/10.1523/JNEUROSCI.2871-05.2005</ext-link>).</p><p>At the end of the day, these issues will be resolved by further experiments, not argumentation. The paper stands as an excellent contribution, but it might be wise for the authors to be less doctrinaire in their interpretations.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89570.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>In this experiment, Voltage Sensitive Dye Imaging (VSDI) was used to measure neural activity in macaque primary visual cortex in monkeys trained to detect an oriented grating target that was presented either alone or against an oriented mask. Monkeys' ability to detect the target (indicated by a saccade to its location) was impaired by the mask, with the greatest impairment observed when the mask was matched in orientation to the target, as is also the case in human observers. VSDI signals were examined to test the hypothesis that the target-evoked response would be maximally suppressed by the mask when it matched the orientation of the target. In each recording session, fixation trials were used to map out the spatial response profile and orientation domains that would then be used to decode the responses on detection trials. VSDI signals were analyzed at two different scales: a coarse scale of the retinotopic response to the target and a finer scale of orientation domains within the stimulus-evoked response. Responses were recorded in three conditions: target alone, mask alone, and target presented with mask. Analyses were focused on the target evoked response in the presence of the mask, defined to be the difference in response evoked by the mask with target (target present) versus the mask alone (target absent). These were computed across five 50 msec bins total, 250 msec, which was the duration of the mask (target present trials, 50% of trials) / mask + target (target present trials, 50% of trials). Analyses revealed that in an initial (transient) phase the target evoked response increased with similarity between target and mask orientation. As the authors note, this is surprising given that this was the condition where the mask maximally impaired detection of the target in behavior. Target evoked responses in a later ('sustained') phase fell off with orientation similarity, consistent with the behavioral effect. When analyzed at the coarser scale the target evoked response, integrated over the full 250 msec period showed a very modest dependence on mask orientation. The same pattern held when the data were analyzed on the finer orientation domain scale, with the effect of the mask in the transient phase running counter to the perceptual effect of the mask and the sustained response correlating the perceptual effect. The effect of the mask was more pronounced when analyzed at the scale.</p><p>Strengths</p><p>The work is on the whole very strong. The experiments are thoughtfully designed, the data collection methods are good, and the results are interesting. The separate analyses of data at a coarse scale that aggregates across orientation domains and a more local scale of orientation domains is a strength and it is reassuring that the effects at the more localized scale are more clearly related to behavior, as one would hope and expect. The results are strengthened by modeling work shown in Figure 8, which provides a sensible account of the population dynamics. The analyses of the relationship between VSDI data and behavior are well thought out and the apparent paradox of the anti-correlation between VSDI and behavior in the initial period of response, followed by a positive correlation in the sustained response period is intriguing.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89570.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Spencer Chin-Yu</given-names></name><role specific-use="author">Author</role><aff><institution>Rutgers University</institution><addr-line><named-content content-type="city">Piscataway NJ</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yuzhi</given-names></name><role specific-use="author">Author</role><aff><institution>University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Geisler</surname><given-names>Wilson S</given-names></name><role specific-use="author">Author</role><aff><institution>University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Seidemann</surname><given-names>Eyal</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the current reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This important study used Voltage Sensitive Dye Imaging (VSDI) to measure neural activity in the primary visual cortex of monkeys trained to detect an oriented grating target that was presented either alone or against an oriented mask. The authors show convincingly that the initial effect of the mask ran counter to the behavioral effects of the mask, a pattern that reversed in the latter phase of the response. They interpret these results in terms of influences from the receptive field center, and although an alternative view that emphasizes the role of the receptive field surround also seems reasonable, this study stands as an interesting and important contribution to our understanding of mechanisms of visual perception.</p><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>This is a clear account of some interesting work. The experiments and analyses seem well done and the data are useful. It is nice to see that VSDI results square well with those from prior extracellular recordings.</p><p>The authors have done a good job responding to the main points of my previous review. One important question remains, as stated in that review:</p><p>&quot;My reading is that this is primarily a study of surround suppression with results that follow pretty directly from what we already know from that literature, and although they engage with some of the literature they do not directly mention surround suppression in the text. Their major effect - what they repeatedly describe as a &quot;paradoxical&quot; result in which the responses initially show a stronger response to matched targets and backgrounds and then reverse - seems to pretty clearly match the expected outcome of a stimulus that initially evokes additional excitation due to increased center contrast followed by slightly delayed surround suppression tuned to the same peak orientation. Their dynamics result seems entirely consistent with previous work, e.g. Henry at al 2020, particularly their Fig. 3 <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/54264">https://elifesciences.org/articles/54264</ext-link>, so it seems like a major oversight to not engage with that work at all, and to explain what exactly is new here.&quot;</p><p>Their rebuttal of my first review is not convincing -- I still believe that surround influences are important and perhaps predominant in determining the outcome of the experiments. This is particularly clear for the &quot;paradoxical&quot; dynamics that they observe, which seem exactly to reflect the behavior of the surround.</p><p>The authors' arguments to the contrary are based on three main points. First, their stimuli cover the center and surround, unlike those of many previous experiments, so they argue that this somehow diminishes the impact of the surround. But the argument is not accompanied by data showing the effects of center stimuli alone or surround stimuli alone. Second, their model -- a normalization model -- does not need surround influences to account for the masking effect. Third, they cite human psychophysical masking results from their collaborators (Sebastian et al 2017), but do not cite an equally convincing demonstration that surround contrast creates potent orientation selective masking when presented alone (Petrov et al 2005, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2871-05.2005">https://doi.org/10.1523/JNEUROSCI.2871-05.2005</ext-link>).</p><p>At the end of the day, these issues will be resolved by further experiments, not argumentation. The paper stands as an excellent contribution, but it might be wise for the authors to be less doctrinaire in their interpretations.</p></disp-quote><p>We thank the reviewer for their positive comments and constructive criticism. In general, we agree with the reviewer’s comments. Importantly, we do not claim that there is no effect from the surround. What we say in the discussion is:</p><p>“Because our targets are added to the background rather than occluding it, it is likely that a significant portion of the behavioral and neural masking effects that we observe come from target-mask interactions at the target location rather than from the effect of the mask in the surround.”</p><p>We still stand by this assessment. We also make the point that, at least within the framework of our delayed normalization model, there is no need for the normalization mechanism to extend beyond the center mechanism to account for our results, and even if the normalization mechanism is somewhat larger than the center, the overlap region at the center would still have a large contribution to the modulations. Overall, we agree that these issues will be need to be resolved by future experiments.</p><p>For the reasons discussed in our previous reply, we disagree with the reviewers’ statement “…this is primarily a study of surround suppression with results that follow pretty directly from what we already know from that literature”. For similar reasons we disagree with the statement “It is nice to see that VSDI results square well with those from prior extracellular recordings”.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary</p><p>In this experiment, Voltage Sensitive Dye Imaging (VSDI) was used to measure neural activity in macaque primary visual cortex in monkeys trained to detect an oriented grating target that was presented either alone or against an oriented mask. Monkeys' ability to detect the target (indicated by a saccade to its location) was impaired by the mask, with the greatest impairment observed when the mask was matched in orientation to the target, as is also the case in human observers. VSDI signals were examined to test the hypothesis that the target-evoked response would be maximally suppressed by the mask when it matched the orientation of the target. In each recording session, fixation trials were used to map out the spatial response profile and orientation domains that would then be used to decode the responses on detection trials. VSDI signals were analyzed at two different scales: a coarse scale of the retinotopic response to the target and a finer scale of orientation domains within the stimulus-evoked response. Responses were recorded in three conditions: target alone, mask alone, and target presented with mask. Analyses were focused on the target evoked response in the presence of the mask, defined to be the difference in response evoked by the mask with target (target present) versus the mask alone (target absent). These were computed across five 50 msec bins total, 250 msec, which was the duration of the mask (target present trials, 50% of trials) / mask + target (target present trials, 50% of trials). Analyses revealed that in an initial (transient) phase the target evoked response increased with similarity between target and mask orientation. As the authors note, this is surprising given that this was the condition where the mask maximally impaired detection of the target in behavior. Target evoked responses in a later ('sustained') phase fell off with orientation similarity, consistent with the behavioral effect. When analyzed at the coarser scale the target evoked response, integrated over the full 250 msec period showed a very modest dependence on mask orientation. The same pattern held when the data were analyzed on the finer orientation domain scale, with the effect of the mask in the transient phase running counter to the perceptual effect of the mask and the sustained response correlating the perceptual effect. The effect of the mask was more pronounced when analyzed at the scale.</p><p>Strengths</p><p>The work is on the whole very strong. The experiments are thoughtfully designed, the data collection methods are good, and the results are interesting. The separate analyses of data at a coarse scale that aggregates across orientation domains and a more local scale of orientation domains is a strength and it is reassuring that the effects at the more localized scale are more clearly related to behavior, as one would hope and expect. The results are strengthened by modeling work shown in Figure 8, which provides a sensible account of the population dynamics. The analyses of the relationship between VSDI data and behavior are well thought out and the apparent paradox of the anti-correlation between VSDI and behavior in the initial period of response, followed by a positive correlation in the sustained response period is intriguing.</p></disp-quote><p>We thank the reviewer for their positive comments.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>None, except perhaps for a more balanced representation of the &quot;surround&quot; possibility in the Discussion. The Petrov et al paper (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2871-05.2005">https://doi.org/10.1523/JNEUROSCI.2871-05.2005</ext-link>) should be considered and cited.</p></disp-quote><p>As discussed above, we believe that our discussion of possible contribution from the surround is balanced. While the paper by Petrov et al is interesting, the stimuli used to study the surround effects are quite different (e.g., gap between center and surround, and the sharp edge of the surround inner boundary) so direct comparison with our results is not possible.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>The authors have addressed the questions/suggestions I raised in my review.</p></disp-quote><p>The following is the authors’ response to the original reviews.</p><p>We thank the reviewers for their helpful comments and suggestions.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>This is an important contribution that extends earlier single-unit work on orientation-specific center-surround interactions to the domain of population responses measured with Voltage Sensitive Dye (VSD) imaging and the first to relate these interactions to orientation-specific perceptual effects of masking. The authors provide convincing evidence of a pattern of results in which the initial effect of the mask seems to run counter to the behavioral effects of the mask, a pattern that reversed in the latter phase of the response. It seems likely that the physiological effects of masking reported here can be attributed to previously described signals from the receptive field surround.</p></disp-quote><p>We thank the reviewers for bringing up the relation of our results to findings from previous orientation-specific center-surround interactions studies. In our final manuscript, we added a paragraph discussing this important issue. Briefly, for multiple reasons, we believe that the orientation-dependent behavioral and neural masking effects that we observe are unlikely to depend on previously described center-surround interactions in V1. First, in human subjects, perceptual similarity masking effects are almost entirely accounted for by target-mask interactions at the target location and are recapitulated when the mask has the same size and location as the target (Sebastian et al 2017). Second, in our computational model, the effect of mask orientation on the dynamics of the response are qualitatively the same if the mask is restricted to the size and location of the target while mask contrast is increased (Fig. 8 – figure supplement 3). Third, in our model, the results are qualitatively the same when the spatial pooling region for the normalization signal is the same as that for the excitation signal (Fig. 8 – figure supplement figure 1). These considerations suggest that center-surround interactions may not be necessary for neural and behavioral similarity masking effects with additive targets.</p><p>We would also like to point out some key differences between the stimuli that we use and the ones used in most previous center-surround studies. First, in our experiments, the target and the mask were additive, while in most previous center-surround studies the target occludes the background. Such studies therefore restrict the mask effect to the surround, while in our study we allow target-mask interactions at the center. Second, most center-surround studies have a sharp-edged target/surround, while in our experiments no sharp edges were present. Unpublished results from our lab suggest that such sharp edges have a large impact on V1 population responses. A third key difference is that our stimuli were flashed for a short interval of 250 ms corresponding to a typical duration of a fixation in natural vision, while most previous center-surround studies used either longer-duration drifting stimuli or very short-duration random-order stimuli for reverse-correlation analysis.</p><p>In addition, we would like to emphasize that our results go beyond previous studies in two important ways. First, we study the effect of similarity masking in behaving animals and quantitatively compare the effect of similarity masking on behavior and physiology in the same subjects and at the same time. Second, VSD imaging allows us to capture the dynamics of superficial V1 population responses over the entire population of millions of neurons activated by the target at two important spatial scales. Such results therefore complement electrophysiological studies that examine the activity of a very small subset of the active neurons.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>This is a clear account of some interesting work. The experiments and analyses seem well done and the data are useful. It is nice to see that VSDI results square well with those from prior extracellular recordings. But the work may be less original than the authors propose, and their overall framing strikes me as odd. Some additional clarifications could make the contribution more clear.</p></disp-quote><p>Please see our reply above regarding the agreement with previous studies and framing.</p><disp-quote content-type="editor-comment"><p>My reading is that this is primarily a study of surround suppression with results that follow pretty directly from what we already know from that literature, and although they engage with some of the literature they do not directly mention surround suppression in the text. Their major effect - what they repeatedly describe as a &quot;paradoxical&quot; result in which the responses initially show a stronger response to matched targets and backgrounds and then reverse - seems to pretty clearly match the expected outcome of a stimulus that initially evokes additional excitation due to increased center contrast followed by slightly delayed surround suppression tuned to the same peak orientation. Their dynamics result seems entirely consistent with previous work, e.g. Henry et al 2020, particularly their Fig. 3 <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/54264">https://elifesciences.org/articles/54264</ext-link>, so it seems like a major oversight to not engage with that work at all, and to explain what exactly is new here.</p></disp-quote><p>We thank the reviewer for the pointing out this previous work which we now cite in the final version of the manuscript. For the reasons discussed above, while this study is interesting and related to our work, we believe that our results are quite distinct.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>In the discussion (lines 315-316), they state &quot;in order to account for the reduced neural sensitivity with target-background similarity in the second phase of the response, the divisive normalization signal has to be orientation selective.&quot; I wonder whether they observed this in their modeling. That is, how robust were the normalization model results to the values of sigma_e and sigma_n? It would be useful to know how critical their various model parameters were for replicating the experimental effects, rather than just showing that a good account is possible.</p></list-item></list></disp-quote><p>Thank you for this suggestion. In the final manuscript we include a supplementary figure that shows how the model’s predictions are affected by the orientation tuning and spatial extent of the normalization signal, and by the size and contrast of the mask (Fig. 8 – figure supplement 1-4).</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>The majority of their target/background contrast conditions were collected only in one animal. This is a minor limitation for work of this kind, but it might be an issue for some.</p></list-item></list></disp-quote><p>We agree that this is a limitation of the current study. These are challenging experiments and we were unable to collect all target/background contrast combinations from both monkeys. However, in the common conditions, the results appear similar in the two animals, and the key results seem to be robust to the contrast combination in the animal in which a wider range of contrast combinations was tested. We added these points to the discussion in the final manuscript.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>The authors point out (line 193-195) that &quot;Because the first phase of the response is shorter than the second phase, when V1 response is integrated over both phases, the overall response is positively correlated with the behavioral masking effect.&quot; I wonder if this could be explored a bit more at the behavioral level - i.e. does the &quot;similarity masking&quot; they are trying to explain show sensitivity to presentation time?</p></list-item></list></disp-quote><p>We agree that testing the effect of stimulus duration on similarity masking is interesting, but unfortunately, it is beyond the scope of the current study. We would also like to point out that the duration of the presentation was selected to match the typical time of fixation during natural behaviors, so much shorter or much longer stimulus durations would be less relevant for natural vision.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>From Fig. 3 it looks like the imaging ROI may include some opercular V2. If so, it's plausible that something about the retinotopic or columnar windowing they used in analysis may remove V2 signals, but they don't comment. Maybe they could tell us how they ensured they only included V1?</p></list-item></list></disp-quote><p>We thank the reviewer for this comment. As part of our experiments, we extract a detailed retinotopic map for each chamber, so we were able to ensure that the area used for the decoding analysis lays entirely within V1. We now incorporate this information in the final manuscript (Fig. 3 – figure supplement 1).</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>In the discussion (lines 278-283) they say &quot;The positive correlation between the neural and behavioral masking effects occurred earlier and was more robust at the columnar scale than at the retinotopic scale, suggesting that behavioral performance in our task is dominated by columnar scale signals in the second phase of the response. To the best of our knowledge, this is the first demonstration of such decoupling between V1 responses at the retinotopic and columnar scales, and the first demonstration that columnar scale signals are a better predictor of behavioral performance in a detection task.&quot; I am having trouble finding where exactly they demonstrate this in the results. Is this just by comparison of Figs. 4E,K and 5E,K? I may just be missing something here, but the argument needs to be made more clearly since much of their claim to originality rests on it.</p></list-item></list></disp-quote><p>We thank the reviewer for this comment. In the final manuscript we are more explicit when we discuss this point and refer to the relevant panels in Figs. 4, 5 and their figure supplements. To substantiate this key claim, we also report the timing of the transition between the two phases in all temporal correlation panels and report the neural-behavioral correlation for the integration period.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary</p><p>In this experiment, Voltage Sensitive Dye Imaging (VSDI) was used to measure neural activity in macaque primary visual cortex in monkeys trained to detect an oriented grating target that was presented either alone or against an oriented mask. Monkeys' ability to detect the target (indicated by a saccade to its location) was impaired by the mask, with the greatest impairment observed when the mask was matched in orientation to the target, as is also the case in human observers. VSDI signals were examined to test the hypothesis that the target-evoked response would be maximally suppressed by the mask when it matched the orientation of the target. In each recording session, fixation trials were used to map out the spatial response profile and orientation domains that would then be used to decode the responses on detection trials. VSDI signals were analyzed at two different scales: a coarse scale of the retinotopic response to the target and a finer scale of orientation domains within the stimulus-evoked response. Responses were recorded in three conditions: target alone, mask alone, and target presented with mask. Analyses were focused on the target evoked response in the presence of the mask, defined to be the difference in response evoked by the mask with target (target present) versus the mask alone (target absent). These were computed across five 50 msec bins total, 250 msec, which was the duration of the mask (target present trials, 50% of trials) / mask + target (target present trials, 50% of trials). Analyses revealed that in an initial (transient) phase the target evoked response increased with similarity between target and mask orientation. As the authors note, this is surprising given that this was the condition where the mask maximally impaired detection of the target in behavior. Target evoked responses in a later ('sustained') phase fell off with orientation similarity, consistent with the behavioral effect. When analyzed at the coarser scale the target evoked response, integrated over the full 250 msec period showed a very modest dependence on mask orientation. The same pattern held when the data were analyzed on the finer orientation domain scale, with the effect of the mask in the transient phase running counter to the perceptual effect of the mask and the sustained response correlating the perceptual effect. The effect of the mask was more pronounced when analyzed at the scale.</p><p>Strengths</p><p>The work is on the whole very strong. The experiments are thoughtfully designed, the data collection methods are good, and the results are interesting. The separate analyses of data at a coarse scale that aggregates across orientation domains and a more local scale of orientation domains is a strength and it is reassuring that the effects at the more localized scale are more clearly related to behavior, as one would hope and expect. The results are strengthened by modeling work shown in Figure 8, which provides a sensible account of the population dynamics. The analyses of the relationship between VSDI data and behavior are well thought out and the apparent paradox of the anti-correlation between VSDI and behavior in the initial period of response, followed by a positive correlation in the sustained response period is intriguing.</p><p>Points to Consider / Possible Improvements</p><p>The biphasic nature of the relationship between neural and behavioral modulation by the mask and the surprising finding that the two are anticorrelated in the initial phase are left as a mystery. The paper would be more impactful if this mystery could be resolved.</p></disp-quote><p>We thank the reviewer for the positive comments. In our view, while our results are surprising, there may not be a remaining mystery that needs to be resolved. As our model shows, the biphasic nature of V1’s response can be explained by a delayed orientation-tuned gain control. Our results are consistent with the hypothesis that perception is based on columnar-scale V1 signals that are integrated over an approximately 200 ms long period that incorporates both the early and the late phase of the response, since such decoded V1 signals are positively correlated with the behavioral similarity masking effect (Fig. 5D, J; Fig. 5 – figure supplement 1). We now explain this more clearly in the discussion of our final manuscript.</p><disp-quote content-type="editor-comment"><p>The finding is based on analyses of the correlation between behavior and neural responses. This appears in the main body of the manuscript and is detailed in Figures S1 and S2, which show the correlation over time between behavior and target response for the retinotopic and columnar scale.</p><p>One possible way of thinking of this transition from anti- to positive correlation with behavior is that it might reflect the dynamics of a competitive interaction between mask and target, with the initial phase reflecting predominantly the mask response, with the target emerging, on some trials, in the latter phase. On trials when the mask response is stronger, the probability of the target emerging in the latter phase, and triggering a hit, might be lower, potentially explaining the anticorrelation in the initial phase. The sustained response may be a mixture of trials on which the target response is or is not strong enough to overcome the effect of the mask sufficiently to trigger target detection.</p><p>It would, I think, be worth examining this by testing whether target dynamics may vary, depending on whether the monkey detected the target (hit trials) or failed to detect the target (miss trials). Unless I missed it I do not think this analysis was done. Consistent with this possibility, the authors do note (lines 226-229) that &quot;The trajectories in the target plus mask conditions are more complex. For example, when mask orientation is at +/- 45 deg to the target, the population response is initially dominated by the mask, but then in mid-flight, the population response changes direction and turns toward the direction of the target orientation.&quot; This suggests (to this reviewer, at least) that the emergence of a positive correlation between behavioral and neural effects in the latter phase of the response could reflect either a perceptual decision that the target is present or perhaps deployment of attention to the location of the target.</p><p>It may be that this transition reflected detection, in which it might be more likely on hit trials than miss trials. Given the SNR it would presumably be difficult to do this analysis on a trial-by-trial basis, but the hit and miss trials (which make each make up about 1/2 of all trials) could be averaged separately to see if the mid-flight transition is more prominent on hit trials. If this is so for the +/- 45 degree case it would be good to see the same analysis for other combinations of target and mask. It would also be interesting to separate correct reject trials from false alarms, to determine whether the mid-flight transition tends to occur on false alarm trials.</p><p>If these analyses do not reveal the predicted pattern, they might still merit a supplemental figure, for the sake of completeness.</p></disp-quote><p>We thank the reviewer for suggesting this interesting possibility. The original analysis in the manuscript was based on both correct and incorrect trials, raising the possibility that our results reflect some contribution from decision- and/or attention-related signals rather than from low-level nonlinear encoding mechanisms in V1 that we postulate in our model (Fig. 8). To explore this possibility, we re-examined our results while excluding error trials. We found that our key results from Figs 4 and 5 – namely that there is an early transient phase in which the neural and behavioral similarity effects are anti-correlated, and a later sustained phase in which they are positively correlated – hold even for the subset of correct trials, reducing the possibility that decision/attention-related signals play a major role in explaning our results. We now include the results of this analysis as a supplementary figure in the final manuscript (Fig. 4 – figure supplement 2). While there may be some interesting differences in the response dynamics between correct and incorrect trials, the current study was not designed to address this question and the large number of conditions and small number of repeats that it necessitated make this data set suboptimal for examining these phenomena.</p><p>References</p><p>Sebastian S, Abrams J, Geisler WS. 2017. Constrained sampling experiments reveal principles of detection in natural scenes. Proc Natl Acad Sci U S A 114: E5731-e40</p></body></sub-article></article>