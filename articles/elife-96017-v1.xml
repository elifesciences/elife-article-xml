<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">96017</article-id><article-id pub-id-type="doi">10.7554/eLife.96017</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.96017.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A deep learning approach for automated scoring of the Rey–Osterrieth complex figure</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Langer</surname><given-names>Nicolas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6038-9471</contrib-id><email>n.langer@psychologie.uzh.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Weber</surname><given-names>Maurice</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hebling Vieira</surname><given-names>Bruno</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8770-7396</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Strzelczyk</surname><given-names>Dawid</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Wolf</surname><given-names>Lukas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Pedroni</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Heitz</surname><given-names>Jonathan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Müller</surname><given-names>Stephan</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Schultheiss</surname><given-names>Christoph</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Troendle</surname><given-names>Marius</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lasprilla</surname><given-names>Juan Carlos Arango</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rivera</surname><given-names>Diego</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7477-1893</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Scarpina</surname><given-names>Federica</given-names></name><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Qianhua</given-names></name><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Leuthold</surname><given-names>Rico</given-names></name><xref ref-type="aff" rid="aff11">11</xref><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Wehrle</surname><given-names>Flavia</given-names></name><xref ref-type="aff" rid="aff12">12</xref><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Jenni</surname><given-names>Oskar</given-names></name><xref ref-type="aff" rid="aff12">12</xref><xref ref-type="fn" rid="con17"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Brugger</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff13">13</xref><xref ref-type="fn" rid="con18"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zaehle</surname><given-names>Tino</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3673-4869</contrib-id><xref ref-type="aff" rid="aff14">14</xref><xref ref-type="fn" rid="con19"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lorenz</surname><given-names>Romy</given-names></name><xref ref-type="aff" rid="aff15">15</xref><xref ref-type="aff" rid="aff16">16</xref><xref ref-type="fn" rid="con20"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ce</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con21"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>Methods of Plasticity Research, Department of Psychology, University of Zurich</institution></institution-wrap><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution>University Research Priority Program (URPP) Dynamics of Healthy Aging</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>Neuroscience Center Zurich (ZNZ), University of Zurich and ETH Zurich</institution></institution-wrap><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a28rw58</institution-id><institution>Department of Computer Science, ETH Zurich</institution></institution-wrap><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02nkdxk79</institution-id><institution>Virginia Commonwealth University</institution></institution-wrap><addr-line><named-content content-type="city">Richmond</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02z0cah89</institution-id><institution>Department of Health Science, Public University of Navarre</institution></institution-wrap><addr-line><named-content content-type="city">Pamplona</named-content></addr-line><country>Spain</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/023d5h353</institution-id><institution>Instituto de Investigación Sanitaria de Navarra (IdiSNA)</institution></institution-wrap><addr-line><named-content content-type="city">Pamplona</named-content></addr-line><country>Spain</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/048tbm396</institution-id><institution>'Rita Levi Montalcini' Department of Neurosciences, University of Turin</institution></institution-wrap><addr-line><named-content content-type="city">Turin</named-content></addr-line><country>Italy</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033qpss18</institution-id><institution>IRCCS Istituto Auxologico Italiano, UO di Neurologia e Neuroriabilitazione, Ospedale San Giuseppe</institution></institution-wrap><addr-line><named-content content-type="city">Piancavallo</named-content></addr-line><country>Italy</country></aff><aff id="aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05201qm87</institution-id><institution>Huashan Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff11"><label>11</label><institution>Smartcode</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff12"><label>12</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035vb3h42</institution-id><institution>University Children's Hospital Zurich, Child Development Center</institution></institution-wrap><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff13"><label>13</label><institution>Rehabilitation Center</institution><addr-line><named-content content-type="city">Valens</named-content></addr-line><country>Switzerland</country></aff><aff id="aff14"><label>14</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03m04df46</institution-id><institution>University Hospital Magdeburg University Department of Neurology</institution></institution-wrap><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff><aff id="aff15"><label>15</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026nmvv73</institution-id><institution>Max Planck Institute for Biological Cybernetics</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff16"><label>16</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhou</surname><given-names>Juan Helen</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tgyzw49</institution-id><institution>National University of Singapore</institution></institution-wrap><country>Singapore</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP96017</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-01-23"><day>23</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-01-25"><day>25</day><month>01</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.06.15.496291"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-21"><day>21</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96017.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-16"><day>16</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96017.2"/></event></pub-history><permissions><copyright-statement>© 2024, Langer et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Langer et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-96017-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-96017-figures-v1.pdf"/><abstract><p>Memory deficits are a hallmark of many different neurological and psychiatric conditions. The Rey–Osterrieth complex figure (ROCF) is the state-of-the-art assessment tool for neuropsychologists across the globe to assess the degree of non-verbal visual memory deterioration. To obtain a score, a trained clinician inspects a patient’s ROCF drawing and quantifies deviations from the original figure. This manual procedure is time-consuming, slow and scores vary depending on the clinician’s experience, motivation, and tiredness. Here, we leverage novel deep learning architectures to automatize the rating of memory deficits. For this, we collected more than 20k hand-drawn ROCF drawings from patients with various neurological and psychiatric disorders as well as healthy participants. Unbiased ground truth ROCF scores were obtained from crowdsourced human intelligence. This dataset was used to train and evaluate a multihead convolutional neural network. The model performs highly unbiased as it yielded predictions very close to the ground truth and the error was similarly distributed around zero. The neural network outperforms both online raters and clinicians. The scoring system can reliably identify and accurately score individual figure elements in previously unseen ROCF drawings, which facilitates explainability of the AI-scoring system. To ensure generalizability and clinical utility, the model performance was successfully replicated in a large independent prospective validation study that was pre-registered prior to data collection. Our AI-powered scoring system provides healthcare institutions worldwide with a digital tool to assess objectively, reliably, and time-efficiently the performance in the ROCF test from hand-drawn images.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>neuropsychology</kwd><kwd>Rey–Osterrieth complex figure</kwd><kwd>ROCF</kwd><kwd>deep learning</kwd><kwd>crowdsourced human intelligence</kwd><kwd>memory deficit</kwd><kwd>non-verbal visual memory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>URPP &quot;Dynamics of Healthy Aging&quot;</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Langer</surname><given-names>Nicolas</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>BRIDGE 40B2-0_187132</award-id><principal-award-recipient><name><surname>Langer</surname><given-names>Nicolas</given-names></name><name><surname>Zhang</surname><given-names>Ce</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>10001C_197480</award-id><principal-award-recipient><name><surname>Langer</surname><given-names>Nicolas</given-names></name><name><surname>Hebling Vieira</surname><given-names>Bruno</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007352</institution-id><institution>State Secretariat for Education, Research and Innovation</institution></institution-wrap></funding-source><award-id>MB22.00036</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ce</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A deep learning-based system objectively and reliably scores the Rey–Osterrieth complex figure test, enhancing the assessment of visual memory deficits from hand-drawn images in clinical settings.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neurological and psychiatric disorders are among the most common and debilitating illnesses across the lifespan. In addition, the aging of our population, with the increasing prevalence of physical and cognitive disorders, poses a major burden on our society with an estimated economic cost of 2.5 trillion US$ per year (<xref ref-type="bibr" rid="bib28">Trautmann et al., 2016</xref>). Currently, neuropsychologists typically use paper-pencil tests to assess individual neuropsychological functions and brain dysfunctions, including memory, attention, reasoning, and problem-solving. Most neuropsychologists around the world use the Rey–Osterrieth complex figure (ROCF) in their daily clinical practice (<xref ref-type="bibr" rid="bib23">Rabin et al., 2005</xref>; <xref ref-type="bibr" rid="bib24">Rabin et al., 2016</xref>), which provides insights into a person’s non-verbal visuo-spatial memory capacity in healthy and clinical populations of all ages, from childhood to old age (<xref ref-type="bibr" rid="bib26">Shin et al., 2006</xref>).</p><p>Our estimation revealed that a single neuropsychological division (e.g. at the University Hospital Zurich) scores up to 6000 ROCF drawings per year. The ROCF test has several advantages as it does not depend on auditory processing and differences in language skills that are omnipresent in cosmopolitan societies (<xref ref-type="bibr" rid="bib20">Osterrieth, 1944</xref>; <xref ref-type="bibr" rid="bib27">Somerville et al., 2000</xref>). The test has adequate psychometric properties (e.g. sufficient test–retest reliability [<xref ref-type="bibr" rid="bib18">Meyers and Volbrecht, 1999</xref>; <xref ref-type="bibr" rid="bib16">Levine et al., 2004</xref>] and internal consistency [<xref ref-type="bibr" rid="bib3">Berry et al., 1991</xref>; <xref ref-type="bibr" rid="bib8">Fastenau et al., 1996</xref>]). Furthermore, the ROCF test has demonstrated to be sensitive to discriminate between various clinical populations (<xref ref-type="bibr" rid="bib1">Alladi et al., 2006</xref>) and the progression of Alzheimer’s disease (<xref ref-type="bibr" rid="bib29">Trojano and Gainotti, 2016</xref>).</p><p>The ROCF test consists of three test conditions: First, in the <italic>Copy condition</italic> subjects are presented with the ROCF and are asked to draw a copy of the same figure. Subsequently, the ROCF figure and the drawn copy are removed and the subject is instructed to reproduce the figure from memory immediately (<xref ref-type="bibr" rid="bib26">Shin et al., 2006</xref>) or 3 min after the Copy condition (<xref ref-type="bibr" rid="bib17">Meyers et al., 1996</xref>) (<italic>Immediate Recall condition</italic>). After a delay of 30 min, the subject is required to draw the same figure once again (<italic>Delayed Recall</italic> condition). For further description of the clinical usefulness of the ROCF please refer to <xref ref-type="bibr" rid="bib26">Shin et al., 2006</xref>.</p><p>The current quantitative scoring system (<xref ref-type="bibr" rid="bib17">Meyers et al., 1996</xref>) splits the ROCF into 18 identifiable elements (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>), each of which is considered separately and marked on the accuracy in both distortion and placement according to a set of rules and scored between 0 and 2 points (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Thus, the scoring is currently undertaken manually by a trained clinician, who inspects the reproduced ROCF drawing and tracks deviations from the original figure, which can take up to 15 min per figure (individually <italic>Copy</italic>, <italic>Immediate Recall</italic>, and <italic>Delayed Recall</italic> conditions).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of retrospective dataset.</title><p>(<bold>A</bold>) Rey–Osterrieth complex figure (ROCF) figure with 18 elements. (<bold>B</bold>) Demographics of the participants and clinical population of the retrospective dataset. (<bold>C</bold>) Examples of hand-drawn ROCF images. (<bold>D</bold>) The pie chart illustrates the proportion of the different clinical conditions of the retrospective dataset. (<bold>E</bold>) Performance in the copy and (immediate) recall condition across the lifespan in the retrospective dataset. (<bold>F</bold>) Distribution of the number of images for each total score (online raters).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Original scoring system according to Osterrieth.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>World maps depict the worldwide distribution of the origin of the data.</title><p>(<bold>A</bold>) Retrospective data. (<bold>B</bold>) Prospective data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>The graphical user interface of the crowdsourcing application.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Overview of prospective dataset.</title><p>(<bold>A</bold>) Demographics of the participants of the prospectively collected data. (<bold>B</bold>) Performance in the copy and (immediate) recall condition across the lifespan in the prospectively collected data. (<bold>C</bold>) Distribution of number of images for each total score for the prospectively collected data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>The user interface for the tablet- (and smartphone-) based application.</title><p>The application enables explainability by providing a score for each individual item. Furthermore, the total score is displayed. The user can also compare the individual with a choosable norm population.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig1-figsupp5-v1.tif"/></fig></fig-group><p>One major limitation of this quantitative scoring system is that the criteria of what position and distortion is considered ‘accurate’ or ‘inaccurate’ may vary from clinician to clinician (<xref ref-type="bibr" rid="bib10">Groth-Marnat, 2000</xref>; <xref ref-type="bibr" rid="bib26">Shin et al., 2006</xref>; <xref ref-type="bibr" rid="bib6">Canham et al., 2000</xref>). In addition, the scoring might vary as a function of motivation and tiredness or because the clinicians may be unwittingly influenced by interaction with the patient. Therefore, an automated system that offers reliable, objective, robust, and standardized scoring, while saving clinicians’ time, would be desirable from an economic perspective and more importantly leads to more accurate scoring and subsequently diagnosing.</p><p>In recent years, computerized image analysis and machine-learning methods have entered the clinical neuropsychological diagnosis field providing the potential for establishing quantitative scoring methods. Using machine-learning methods, such as convolutional neural networks and support vector machines, studies have successfully recognized visual structures of interest produced by subjects in the Bender Gestalt Test (<xref ref-type="bibr" rid="bib4">Bin Nazar et al., 2017</xref>) and the Clock Draw Task (<xref ref-type="bibr" rid="bib14">Kim et al., 2011</xref>; <xref ref-type="bibr" rid="bib11">Harbi et al., 2016</xref>) – both are less frequently applied screening tests for visuo-spatial and visuo-constructive (dis-)abilities (<xref ref-type="bibr" rid="bib32">Webb et al., 2021</xref>). Given the wide application of the ROCF, it is not surprising that we are not the first to take steps toward a machine-based scoring system. <xref ref-type="bibr" rid="bib6">Canham et al., 2000</xref> have developed an algorithm to automatically identify a selection of parts of the ROCF (6 of 18 elements) with great precision. This approach provides first evidence for the feasibility of automated segmentation and feature recognition of the ROCF. More recently, <xref ref-type="bibr" rid="bib30">Vogt et al., 2019</xref> developed an automated scoring using a deep neural network. The authors reported a <italic>r</italic> = 0.88 Pearson correlation with human ratings, but equivalence testing demonstrated that the automated scoring did not produce strictly equivalent total scores compared to the human ratings. Moreover, it is unclear if the reported correlation was observed in an independent test dataset, or in the training set. Finally, <xref ref-type="bibr" rid="bib22">Petilli et al., 2021</xref> have proposed a novel tablet-based digital system for the assessment of the ROCF task, which provides the opportunity to extract a variety of parameters such as spatial, procedural, and kinematic scores. However, none of the studies described have been able to produce machine-based scores according to the original scoring system currently used in clinics (<xref ref-type="bibr" rid="bib17">Meyers et al., 1996</xref>) that are equivalent or superior to human raters. A major challenge in developing automated scoring systems is to gather a rich enough set of data with instances of all possible mistakes that humans can make. In this study, we have trained convolutional neural networks with over 20,000 digitized ROCFs from different populations regarding age and diagnostic status (healthy individuals or individuals with neurological and psychiatric disorders [e.g. Alzheimer, Parkinson]).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A human MSE and clinicians’ scoring</title><p>We have harnessed a large pool (~5000) of human workers (crowdsourced human intelligence) to obtain ground truth scores and compute the <italic>human MSE</italic>. An average of 13.38 (sd = 2.23) crowdsource-based scores per figure was gathered. The <italic>average human MSE</italic> over all images is 16.3, and the <italic>average human MAE</italic> is 2.41.</p><p>For a subset (4030) of the 20,225 ROCF images, we had access to scores conducted by different professional clinicians. This enabled us to analyze and compare the scoring of professionals to the crowdsourcing results. The clinician MSE over all images is 9.25 and the clinician MAE is 2.15, indicating a better performance of the clinicians compared to the average human rater.</p></sec><sec id="s2-2"><title>Machine-based scoring system</title><p>The multilabel classification network achieved a total score mean squared error (MSE) of 3.56 and mean absolute error (MAE) of 1.22, which is already considerably lower than the corresponding human performance. Implementing additional data augmentation (DA) has led to a further improvement in accuracy with an MSE of 3.37 and MAE of 1.16. Furthermore, when combining DA with test-time augmentation (TTA) leads to a further improvement in performance with an MSE of 3.32 and MAE of 1.15.</p><p>The regression-based network variant led to a further slight improvement in performance, reducing the total score MSE to 3.07 and the MAE to 1.14. Finally, our best model results from combining the regression model with the multilabel classification model in the following way: For each item of the figure we determine whether to use the regressor or classifier based on its performance on the validation set (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Aggregating the two models in this way leads to an MSE of 3.00 and an MAE of 1.11. <xref ref-type="fig" rid="fig2">Figure 2C</xref> presents the error for all combinations of DA and TTA with the multilabel classification and regression model separately and in combination. During the experiments, it became apparent that for the multilabel classification network applying both DA and TTA jointly improves the model’s performance (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). Somewhat surprisingly, applying these elements to the multihead regression model did not improve the performance compared to the non-augmented version of the model. The exact performance metrics (MSE, MAE, and <italic>R</italic>-squared) of all model variants are reported in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref> and for each figure element in <xref ref-type="supplementary-material" rid="fig2sdata2">Figure 2—source data 2</xref>. In addition, the model performance was replicated in the independent prospective validation study (i.e. MSE = 3.32; MAE = 1.13). The performance metrics of each figure element for the independent prospective validation study are reported in <xref ref-type="supplementary-material" rid="fig2sdata3">Figure 2—source data 3</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Model architecture and performance evaluation.</title><p>(<bold>A</bold>) Network architecture, constituted of a shared feature extractor and 18 item-specific feature extractors and output blocks. The shared feature extractor consists of three convolutional blocks, whereas item-specific feature extractors have one convolutional block with global max pooling. Convolutional blocks consist of two convolution and batch normalization pairs, followed by max pooling. Output blocks consist of two fully connected layers. ReLU activation is applied after batch normalization. After pooling, dropout is applied. (<bold>B</bold>) Item-specific mean absolute error (MAE) for the regression-based network (blue) and multilabel classification network (orange). In the final model, we determine whether to use the regressor or classifier network based on its performance in the validation dataset, indicated by an opaque color in the bar chart. In case of identical performance, the model resulting in the least variance was selected. (<bold>C</bold>) Model variants were compared and the performance of the best model in the original, retrospectively collected (green) and the independent, prospectively collected (purple) test set is displayed; Clf: multilabel classification network; Reg: regression-based network; NA: no augmentation; DA: data augmentation; TTA: test-time augmentation. (<bold>D</bold>) Convergence analysis revealed that after ~8000 images, no substantial improvements could be achieved by including more data. (<bold>E</bold>) The effect of image size on the model performance is measured in terms of MAE. The error bars in all subplots indicate the 95% confidence interval.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>The performance metrics for all model variants.</title><p>NA: non-augmented, DA: data augmentation is performed during training, TTA: test-time augmentation. The 95% confidence interval is shown in square brackets.</p></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-96017-fig2-data1-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Per-item and total performance estimates for the final model of the retrospective data.</title><p>Mean absolute error (MAE), mean squared error (MSE), and <italic>R</italic>² are estimated directly from the estimated scores.</p></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-96017-fig2-data2-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="fig2sdata3"><label>Figure 2—source data 3.</label><caption><title>Per-item and total performance estimates for the final model with prospective data.</title><p>Mean absolute error (MAE), mean squared error (MSE), and <italic>R</italic>² are estimated directly from the estimated scores.</p></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-96017-fig2-data3-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig2-v1.tif"/></fig><p>We further conducted a more fine-grained analysis of our best model. <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the score for each figure in the dataset contrasted with the ground truth score (i.e. median of online raters). In addition, we computed the difference between the ground truth score and predicted score revealing that errors made by our model are concentrated closely around 0 (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), while the distribution of clinician’s errors is much more spread out (<xref ref-type="fig" rid="fig3">Figure 3D, E</xref>). It is interesting to note that, like the clinicians, our model exhibits a slight bias toward higher scores, although much less pronounced. Importantly, the model does not demonstrate any considerable bias toward specific figure elements. In contrast to the clinicians, the MAE is very balanced across each individual item of the figure (<xref ref-type="fig" rid="fig3">Figure 3C, F</xref> and <xref ref-type="supplementary-material" rid="fig2sdata2">Figure 2—source data 2</xref>). Finally, a detailed breakdown of the expected performance across the entire range of total scores is displayed for the model (<xref ref-type="fig" rid="fig3">Figure 3G</xref> and <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>), the clinicians (<xref ref-type="fig" rid="fig3">Figure 3H</xref>), and average online raters (<xref ref-type="fig" rid="fig3">Figure 3I</xref>). As can be seen, the model exhibits a comparable MAE for the entire range of total scores, although there is a trend that high scores exhibit lower MAEs. These results were confirmed in the independent prospective validation study (see <xref ref-type="fig" rid="fig3">Figure 3G–I</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, and <xref ref-type="supplementary-material" rid="fig3sdata2">Figure 3—source data 2</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Contrasting the ratings of our model (<bold>A</bold>) and clinicians (<bold>D</bold>) against the ground truth revealed a larger deviation from the regression line for the clinicians.</title><p>A jitter is applied to better highlight the dot density. The distribution of errors for our model (<bold>B</bold>) and the clinicians ratings (<bold>E</bold>) is displayed. The mean absolute error (MAE) of our model (<bold>C</bold>) and the clinicians (<bold>F</bold>) is displayed for each individual item of the figure (see also <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>). The corresponding plots for the performance on the prospectively collected data are displayed in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. The model performance for the retrospective (green) and prospective (purple) sample across the entire range of total scores for model (<bold>G</bold>), clinicians (<bold>H</bold>), and online raters (<bold>I</bold>) is presented. The error bars in all subplots indicate the 95% confidence interval.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Performance per total score interval with retrospective data.</title><p>Thirty-seven intervals were evaluated, across the whole range of scores. We evaluated mean absolute error (MAE) and mean squared error (MSE) for the total score within each interval.</p></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-96017-fig3-data1-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>Performance per total score interval with prospective data.</title><p>Thirty-seven intervals were evaluated, across the whole range of scores. We evaluated mean absolute error (MAE) and mean squared error (MSE) for the total score within each interval.</p></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-96017-fig3-data2-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Detailed performance of the model on the prospective data.</title><p>Contrasting the ratings of our model. (<bold>A</bold>) Against the ground truth. A jitter is applied to better highlight the dot density. (<bold>B</bold>) The distribution of errors for our model on the prospective data is displayed. (<bold>C</bold>) The mean absolute error (MAE) of our model is displayed for each individual item of the figure (see also <xref ref-type="supplementary-material" rid="fig3sdata2">Figure 3—source data 2</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>The standard deviation of the human raters is displayed across differently scored drawings.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig3-figsupp2-v1.tif"/></fig></fig-group><p>In addition, we have conducted a comprehensive model performance analysis to evaluate our model’s performance across different ROCF conditions (copy and recall), demographics (age and gender), and clinical statuses (healthy individuals and patients) (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). These results have been confirmed in the prospective validation study (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Furthermore, we included an additional analysis focusing on specific diagnoses to assess the model’s performance in diverse patient populations (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Our findings demonstrate that the model maintains high accuracy and generalizes well across various demographics and clinical conditions.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model performance across ROCF conditions, demographics, and clinical subgroups in the retrospective dataset.</title><p>(<bold>A</bold>) Displayed are the mean absolute error and bootstrapped 95% confidence intervals of the model performance across different Rey–Osterrieth complex figure (ROCF) conditions (copy and recall), demographics (age and gender), and clinical statuses (healthy individuals and patients) for the retrospective data. (<bold>B</bold>) Model performance across different diagnostic conditions. (<bold>C, D</bold>) The number of subjects in each subgroup is depicted. The same model performance analysis for the prospective data is reported in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Model performance across ROCF conditions, demographics, and clinical subgroups in prospective dataset.</title><p>(<bold>A</bold>) Displayed are the mean absolute error and bootstrapped 95% confidence intervals of the model performance across different Rey–Osterrieth complex figure (ROCF) conditions (copy and recall), demographics (age and gender), and clinical statuses (healthy individuals and patients) for the prospective data. (<bold>B</bold>) The number of subjects in each subgroup is depicted. Please note, that we did not have sufficient information on the specific patient diagnoses in the prospective data to decompose the model performance for specific clinical conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig4-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Robustness analysis</title><p>Using DA did not critically improve the accuracy, which is likely due to the fact that our dataset is already large and diverse enough. However, DA does significantly improve robustness against semantic transformations, as can be seen from <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. In particular, using our DA pipeline, the model becomes much more robust against rotations and changes in perspective. On the other hand, for changes to brightness and contrast, we could not find a clear trend. This is however not surprising as the DA pipeline does not explicitly encourage the model to be more robust against these transformations. Overall, we demonstrate that our scoring system is highly robust for realistic changes in rotations, perspective, brightness, and contrast of the images (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Performance is degraded only under unrealistic and extreme transformations.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Robustness to geometric, brightness, and contrast variations.</title><p>The mean absolute error (MAE) is depicted for different degrees of transformations, including (<bold>A</bold>) rotations; (<bold>B</bold>) perspective change; (<bold>C</bold>) brightness decrease; (<bold>D</bold>) brightness increase; (<bold>E</bold>) contrast change. In addition examples of the transformed Rey–Osterrieth complex figure (ROCF) draw are provided. The error bars in all subplots indicate the 95% confidence interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Effect of data augmentation.</title><p>The mean absolute error (MAE) for the model with data augmentation and without data augmentation is depicted for different degrees of transformations, including (<bold>A</bold>) rotations; (<bold>B</bold>) perspective change; (<bold>C</bold>) brightness decrease; (<bold>D</bold>) brightness increase; (<bold>E</bold>) contrast change.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96017-fig5-figsupp1-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we developed an AI-based scoring system for a non-verbal visuo-spatial memory test that is being utilized in clinics around the world on a daily basis. For this, we trained two variations of deep learning systems and used a rich dataset of more than 20k hand-drawn ROCF images covering the entire lifespan, different research and clinical environments as well as representing global diversity. By leveraging human crowdsourcing we obtained unbiased high-precision training labels. Our best model results from combining a multihead convolutional neural network for regression with a network for multilabel classification. DA and TTA were used to improve the accuracy and made our model more robust against geometric transformations like rotations and changes in perspective.</p><p>Overall, our results provide evidence that our AI-scoring tool outperforms both amateur raters and clinicians. Our model performed highly unbiased as it yielded predictions very close to the ground truth and the error was similarly distributed around zero. Importantly, these results have been replicated in an independent prospectively collected dataset. The scoring system reliably identified and accurately scored individual figure elements in previously unseen ROCF drawings, which facilitates the explainability of the AI-scoring system. Comparable performance for each figure element indicates no considerable bias toward specific figure compositions. Furthermore, the error of our scoring system is rather balanced across the entire range of total scores. An exception are the highest scored images displaying the smallest error, which has two explanations: First, compared to low-score images, for which the same score can be achieved by numerous combinations of figure elements, the high-score images by nature do not have as many possible combinations. Second, the dataset contains a proportional larger amount of available training data for the high-score images.</p><p>While the ROCF is one of the most commonly employed neuropsychological test to evaluate non-verbal visuo-spatial memory in clinical setting (<xref ref-type="bibr" rid="bib26">Shin et al., 2006</xref>), the current manual scoring in clinics is time-consuming (5–15 min for each drawing), requires training, and ultimately depends on subjective judgments, which inevitably introduces human scoring biases (<xref ref-type="bibr" rid="bib31">Watkins, 2017</xref>; <xref ref-type="bibr" rid="bib9">Franzen, 2000</xref>) and low inter-rater reliability (<xref ref-type="bibr" rid="bib9">Franzen, 2000</xref>; <xref ref-type="bibr" rid="bib13">Huygelier et al., 2020</xref>; see also <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Thus, an objective and reliable automated scoring system is in great demand as it can overcome the problem of intra- and inter-rater variability. More importantly, such an automated system can remove a time-consuming, tedious, and repetitive task from clinicians and thereby helps to reduce the workload of clinicians and/or allow more time for more patient-centered care. Overall, automation can support clinicians in making healthcare decisions more accurate and timely.</p><p>Over the past years, deep learning has made significant headway in achieving above human-level performance on well-specified and isolated tasks in computer vision. However, unleashing the power of deep learning depends on the availability of large training sets with accurate training labels. We obtained over 20k hand-drawn ROCF images and invested immense time and labor to manually scan the ROCF drawings. This effort was only possible by orchestrating across multiple clinical sites and having funds available to hire the workforce to accomplish the digitization. Our approach highlights the need to intensify national and international effort of the digitalization of health record data. Although electronic health records are increasingly available (e.g. quadrupled in the US from 2007 to 2012 [<xref ref-type="bibr" rid="bib12">Hsiao et al., 2014</xref>]), challenges remain in terms of handling, processing, and moving such big data. Improved data accessibility and better consensus in organization and sharing could simplify and foster similar approaches in neuropsychology and medicine.</p><p>Another important difficulty is that data typically comes from various sources and thus exhibits large heterogeneity in terms of quality, size, format of the images, and crucially the quality of the labeled dataset. High-quality labeled and annotated datasets are the foundation of a successful computer vision system. A novelty of our approach is that we have trained a large pool of human internet workers (crowdsourced human intelligence) to score ROCFs drawings guided by our self-developed interactive web application. Each element of the figure was scored by several human workers (13 workers on average per figure). To derive ground truth scores, we took the median score, as it has the advantage of being robust to outliers. To further ensure high-quality data annotation, we identified and excluded crowdsourcing participants that have a high level of disagreement (&gt;20% disagreement) with this rating from trained clinicians, who carefully scored manually a subset of the data in the same interactive web application. Importantly our twofold innovative approach that combines the digitization of neuropsychological tests and the high-quality scoring using crowdsourced intelligence can provide a roadmap for how AI can be leveraged in neuropsychological testing more generally as it can be easily adapted and applied to various other neuropsychological tests (e.g. Clock Drawing Test [<xref ref-type="bibr" rid="bib19">Morris, 1994</xref>], Taylor Complex Figure Test [<xref ref-type="bibr" rid="bib2">Awad et al., 2004</xref>], Hamasch 5-point test [<xref ref-type="bibr" rid="bib25">Regard et al., 1982</xref>]).</p><p>Another important prerequisite of using AI for the automated scoring of neuropsychological tests is the availability of training data that is sufficiently diverse and obtains sufficient examples from all possible scores. Recent examples in computer vision have demonstrated that insufficient diversity in training data can lead to biases and adverse consequences (<xref ref-type="bibr" rid="bib5">Buolamwini and Gebru, 2018</xref>; <xref ref-type="bibr" rid="bib7">Ensign et al., 2017</xref>). Given that our training dataset included data from children and a representative sample of patients (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>), exhibiting a range of neurological disorders, drawings were frequently extremely dissimilar to the target figure. Importantly, our scoring system delivers accurate scores even in cases where drawings are distorted (e.g. caused by fine motor impairments) or omissions (e.g. due to visuo-spatial deficits), which are typical impairment patterns in neurological patients. In addition, our scoring system is robust against different semantic transformations (e.g. rotations, perspective change, brightness) which are likely to occur in real-life scenarios, when the examiner takes a photo of the ROCF drawing, which naturally will lead to varying viewpoints and illumination conditions. Thus, this robustness is a pivotal prerequisite for any potential clinical utility.</p><p>To improve the usability of our system and guarantee clinical utility, we have integrated our model into a tablet- (and smartphone-) based application, in which users can take a photo (or upload an image) of an ROCF drawing and the application instantly provides a total score. The application is currently in beta testing with selected clinicians in real-world settings. Once beta testing is complete, the application will be made publicly accessible to clinicians and healthcare institutions worldwide, with detailed access and usage instructions available on our <ext-link ext-link-type="uri" xlink:href="https://www.psychology.uzh.ch/en/areas/nec/plafor/research/rfp.html">website</ext-link>. Importantly, the automated scoring system also maintains explainability by providing visualization of detailed score breakdowns (i.e. item-specific scores), which is a highly valued property of AI-assisted medical decision making and key to help interpret the score and communicate them to the patient (see <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). The scoring is completely reproducible and thus facilitates valid large-scale comparisons of ROCF data, which enable population-based cognitive screening and (assisted) self-assessments.</p><p>In summary, we have created a highly robust and automated AI-based scoring tool, which provides unbiased, reproducible, and immediate scores for the ROCF test in research and clinical environments. Our current machine-based automated quantitative scoring system outperforms both amateur raters and clinicians.</p><p>The present findings demonstrate that automated scoring systems can be developed to advance the quality of neuropsychological assessments by diminishing reliance on subjective ratings and efficiently improving scoring in terms of time and costs. Our innovative approach can be translated to many different neuropsychological tests, thus providing a roadmap for paving the way to AI-guided neuropsychology.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data</title><p>For our experiments, we used a dataset of 20,225 hand-drawn ROCF images collected from 90 different countries (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) as well as various research and clinical settings. This large dataset spans the complete range of ROCF scores (<xref ref-type="fig" rid="fig1">Figure 1F</xref>) which allows the development of a more robust and generalizable automated scoring system. Our convergence analysis suggests that the error converges when approaching 10,000 digitized ROCFs. The data is collected from various populations regarding age and diagnostic status (healthy or with neurological and/or psychiatric disorder), shown, respectively, in <xref ref-type="fig" rid="fig1">Figure 1E and D</xref>. The demographics of the participants and some example images are shown, respectively, in <xref ref-type="fig" rid="fig1">Figure 1B and C</xref>. For a subset of the figures (4030), the clinician’s scores were available. For each figure only one clinician rating is available. The clinicians ratings were derived from six different international clinics (University Hospital Zurich, Switzerland; University Children’s Hospital Zurich, Switzerland; BioCruces Health Research Institute, Spain; I.R.C.C.S. Istituto Auxologico Italiano, Ospedale San Giuseppe, Italy; Huashan Hospital, China; University Hospital Magdeburg, Germany).</p><p>The study was approved by the Institutional Ethics Review Board of the ‘Kantonale Ethikkommission’ (BASEC-Nr. 2020-00206). All collaborators have written informed consent and/or data usage agreements for the recorded drawings from the participants. The authors assert that all procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Helsinki Declaration of 1975, as revised in 2008. To ensure generalizability and clinical utility, the model performance was replicated in a large independent prospective validation study that was pre-registered prior to data collection (<ext-link ext-link-type="uri" xlink:href="https://osf.io/82796">https://osf.io/82796</ext-link>). For the independent prospective validation study, an additional dataset was collected and contained 2498 ROCF images from 961 healthy adults from and 288 patients with various neurological and psychiatric disorders. Further information about the participants demographics and performance in the copy and recall condition can be found in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>.</p></sec><sec id="s4-2"><title>Convergence analysis</title><p>To get an estimate on the number of ROCFs needed to train our models we conducted a convergence analysis. That is, for a fixed test set with 4045 samples, we used different fractions of the remaining dataset to train the multilabel classification model, ending up with training set sizes of 1000, 2000, 3000, 4000, 6000, 8000, 12,000, and 16,000 samples. By evaluating the performance (i.e. MAE) of the resulting models on the fixed test set, we determined what amount of data is required for the model performance to converge.</p><p>With ~3000 images, we obtain diminishing mean MAEs. After ~10,000 images, no substantial improvements could be achieved by including more data, as can be seen from the progression plot in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. From this, we conclude that the acquired dataset is rich and diverse enough to obtain a powerful deep learning model for the task at hand. In addition, this opens up an avenue for future research in that improvements to the model performance are likely to be achieved via algorithmic improvements, rather than via data-centric efforts.</p></sec><sec id="s4-3"><title>Crowdsourced human intelligence for ground truth score</title><p>To reach high accuracy in predicting individual sample scores of the ROCFs, it is imperative that the scores of the training set are based on a systematic scheme with as little human bias as possible influencing the score. However, our analysis (see results section) and previous work (<xref ref-type="bibr" rid="bib6">Canham et al., 2000</xref>) suggested that the scoring conducted by clinicians may not be consistent, because the clinicians may be unwittingly influenced by the interaction with the patient/participant or by the clinicians factor (e.g. motivation and fatigue). For this reason, we have harnessed a large pool (~5000) of human workers (crowdsourced human intelligence) who scored ROCFs, guided by our self-developed interactive web application (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). In this web application, the worker was first trained by guided instructions and examples. Subsequently, the worker rated each area of real ROCF drawings separately to guarantee a systematic approach. For each of the 18 elements in the figure, participants were asked to answer three binary questions: Is the item visible and recognizable? Is the item in the right place? Is the item drawn correctly? This corresponds to the original Osterrieth scoring system (<xref ref-type="bibr" rid="bib17">Meyers et al., 1996</xref>; see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The final assessment consisted of answers to these questions for each of the 18 items and enabled the calculation of item-wise scores (possible scores: 0, 0.5, 1, and 2), which enabled to compute the total score for each image (i.e. sum over all 18 item-wise scores: range total score: 0–36).</p><p>Since participants are paid according to how many images they score, there is a danger of collecting unreliable scores when participants rush through too quickly. In order to avoid taking such assessments into account, 600 images have also been carefully scored manually by trained clinicians at the neuropsychological unit of the University Hospital Zurich (in the same interactive web application). We ensured that each crowdsourcing worker rated at least two of these 600 images, which resulted in 108 ratings (2 figures * 18 items * 3 questions) to compute a level of disagreement. The assessments of crowdsourcing participants that have a high level of disagreement (&gt;20% disagreement) with this rating from clinicians are considered cheaters and are excluded from the dataset. After this data cleansing, there remained an average of 13.38 crowdsource-based scores per figure. In order to use this information for training an algorithm, we require one ground truth score for each image. We assume that the scores approximately follow a normal distribution centered around the true score. With this in mind, we have obtained the ground truth for each drawing by computing the median for each item in the figure, and then summed up the medians to get the total score for the drawing in question. Taking the median has the advantage of being robust to outliers. The identical crowdsourcing approach has also been used to obtain ground truth scores for the independent prospective validation study (an average of 10.36 crowdsource-based scores per figure; minimum number of ratings = 10).</p></sec><sec id="s4-4"><title>Human MSE</title><p>As described in the previous section, there are multiple independent scores (from different crowdsourcing workers) available for each image. It frequently happens that two people scoring the same image produce different scores. In order to quantify the disagreement among the crowdsourcing participants, we calculated the empirical standard deviation of the scores for each image. With <italic>s</italic>1, <italic>…</italic>, <italic>sk</italic> referring to the <italic>k</italic> crowdsourcing scores for a given image and <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mi>s</mml:mi><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mstyle></mml:math></inline-formula> to their mean, the empirical standard deviation is calculated as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The mean empirical standard deviation is 3.25. Using as ground truth the sum of item score medians, we can also calculate a <italic>human MSE</italic>. Assuming that the sum of median scores of all items of an image is the ground truth, the average rating performance of human raters can be estimated by computing the MSE and the MAE of the human ratings by first computing the mean error for each human rater, and then computing the mean of all individual MSEs. These metrics describe how close one assessment is to this ground truth on average and let us make a statement on the difficulty of the task. Reusing above notation, we denote the k crowdsourcing scores for a given image by <italic>s</italic>1, …, <italic>sk</italic>. Let <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> be their median. For an image or item, we define the <italic>human MSE</italic> as<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Similarly, we define the human MAE as<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Based on clinician ratings, we define both <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in a similar fashion. These metrics are used to compare individual crowdsourced scores, clinician scores, and AI-based scores.</p></sec><sec id="s4-5"><title>Convolutional neural network</title><p>For the automated scoring of ROCF drawings, two variations of deep learning systems were implemented: a regression-based network and a multilabel classification network. The multilabel classification problem is based on the assumption that one drawing can contain multiple classes. That is, the scores for a specific element correspond to four mutually exclusive classes (i.e. 0, 0.5, 1, and 2), which corresponds to the original Osterrieth scoring system (<xref ref-type="bibr" rid="bib17">Meyers et al., 1996</xref>; see also <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Each class can appear simultaneously with score classes corresponding to other elements in the figure. Although the scores for different elements share common features, such as texture or drawing style, which are relevant to the element score, the scoring of each element can be viewed as an independent process, taking these shared features into account. These considerations are the starting point for the architectural design of our scoring system, shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. The architecture consists of a shared feature extractor network and 18 parallel decision heads, one for each item score. The shared feature extractor is composed of three blocks, introducing several model hyperparameters: Each block consists of a 3 × 3 convolutional layer, batch normalization, 3 × 3 convolutional layer, batch normalization and max pooling with 2 × 2 stride. The ReLU activation function is applied after each batch normalization and dropout is applied after max pooling. The output of each block is a representation with 64, 128, and 256 channels, respectively. Each of the 18 per-item networks is applied on this representation. These consist of a similar feature extractor block, but with global max pooling instead, which outputs a 512-dimensional representation that is subsequently fed into a fully connected layer, that outputs a 1024-dimensional vector, followed by batch normalization, ReLU, dropout, and an output fully connected layer.</p><p>Two variations of this network were implemented: a regression-based network and a multilabel classification network. They differ in the output layer and in the loss function used during training. The regression-based network outputs a single number, while the multilabel classification network outputs class probabilities, each of which corresponds to one of the four different item scores {0, 0.5, 1.0, 2.0}. Second, the choice of loss functions also incurs different dynamics during optimization. The MSE loss used for the regression model penalizes smaller errors less than big errors, for example, for an item with a score 0.5, predicting the score 1.0 is penalized less than when the model predicts 2.0. In other words, the MSE loss naturally respects the ordering in the output space. This is in contrast to the multilabel classification model for which a cross entropy loss was used for each item, which does not differentiate between different classes in terms of ‘how wrong’ the model is: in the example above, 1.0 is considered equally wrong as 2.0.</p></sec><sec id="s4-6"><title>Data augmentation</title><p>In a further study, we investigated the effect of DA during training from two perspectives. First, DA is a standard technique to prevent machine-learning models from overfitting to training data. The intuition is that enriching the dataset with perturbed versions of the original images leads to better generalization by enriching the dataset with an additional and more diverse training set. Second, DA can also help in making models more robust against semantic transformations like rotations or changes in perspective. These transformations are particularly relevant for the present application since users in real-life are likely to take pictures of drawings which might be slightly rotated or with a slightly tilted perspective. With these intuitions in mind, we randomly transformed drawings during training. Each transformation was a combination of Gaussian blur, a random perspective change and a rotation with angles chosen randomly between −10° and 10°. The DA did not include generative models. Initially, we explored using generative models, specifically generative adversarial networks (GANs), for DA to address the scarcity of low-score images compared to high-score images. However, due to the extensive available dataset, we did not observe any substantial performance improvements in our model. Nevertheless, future studies could explore generative models, such as variational autoencoders or Bayesian networks, which can then be tested on the data from the current prospective study and compared with our results.</p></sec><sec id="s4-7"><title>Training and validation</title><p>To evaluate our model, we set aside 4045 of the 20,225 ROCF drawings as a testing set, corresponding to 20% of the dataset. The remaining 16,180 images were used for training and validation. Specifically, we used 12,944 (80%) drawings for training and 3236 (20%) as a validation set for early stopping.</p><p>The networks and training routines were implemented in PyTorch 1.8.1 (<xref ref-type="bibr" rid="bib21">Paszke et al., 2019</xref>). The training procedure introduces additional hyperparameters: Our main model was trained on drawings of size 232 × 300 (see <xref ref-type="fig" rid="fig2">Figure 2D</xref> and below on details on the image resolution analysis), using batches of 16 images. Model parameters were optimized by the Adam optimizer (<xref ref-type="bibr" rid="bib15">Kingma and Ba, 2014</xref>) with the default parameters <italic>β</italic>1 = 0.9, <italic>β</italic>2 = 0.999, and <italic>ε</italic> = 1e−8. The initial learning rate, set to 0.01, was chosen to facilitate stable model convergence. It decayed exponentially by a factor of 0.95 per epoch, gradually reducing the step size to prevent overshooting the optimal solution during training. To prevent overfitting dropout rates were set to 0.3 and 0.5 in the convolutional and fully connected layers, respectively. Additionally, we used the validation split (20%) of the training data for early stopping. Since our dataset is imbalanced and contains a disproportionately high number of high-score drawings, we sampled the drawings in a way that resulted in evenly distributed scores in a single batch. We trained the model for 75 epochs and picked the weights corresponding to the epoch with the smallest validation loss. The multilabel classification model was trained with cross entropy loss applied to each item classifier independently so that the total loss is obtained by summing the individual loss terms. The regression model was trained in an analogous manner except that we used the MSE for each item score, instead of the cross entropy loss.</p></sec><sec id="s4-8"><title>Performance evaluation</title><p>We evaluated the overall performance of the final model on the test set based on MSE, MAE, and <italic>R</italic>-squared. Even though item-specific scores are discrete and we also implement a classification-based approach, we chose not to rely on accuracy to assess performance. Because accuracy penalizes errors equally, minor differences in scores can lead to arbitrarily small accuracy, or even worse scores than a model that leads to bigger differences on average. MAE gives a better picture of how accurate the model is.</p><p>Performance metrics (MSE, MAE, and <italic>R</italic>-squared) were examined for the total score and additionally for each of the 18 items individually. To probe the performance of the model across a wide range of scores, we also obtained MSE and MAE for each total score. Given predictions <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, true scores <inline-formula><mml:math id="inf6"><mml:mi>y</mml:mi></mml:math></inline-formula>, and the average of true scores <inline-formula><mml:math id="inf7"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>̄</mml:mo></mml:mover></mml:math></inline-formula>, MSE, MAE, and <italic>R</italic>-squared are defined, respectively, as<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mtext> </mml:mtext><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mtext> </mml:mtext><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In addition, we evaluated the performance in a pre-registered (<ext-link ext-link-type="uri" xlink:href="https://osf.io/82796">https://osf.io/82796</ext-link>) independent prospective validation study. Importantly, both datasets (original test set i.e. 4045 images) and the independent prospective dataset (i.e. 2498 images) were never seen by the neural networks.</p></sec><sec id="s4-9"><title>Test-time augmentation</title><p>TTA was performed to promote robustness at test-time to slight variations in input data. First, we experimented with the same augmentations that we used in our DA training policy, namely randomly applying the perspective change, resizing the image, and applying rotations. In addition to the original image, we fed four to seven randomly transformed drawings (see description of DA for the possible transformations) to the model and averaged the output logits to obtain a prediction. In addition, we experimented with approaches exclusively applying either random rotations or random perspective change. None of these random procedures improved the model performance. Nonetheless, performance improvement was achieved by applying deterministic rotations along positive and negative angles on the unit circle. We performed five rotations, uniformly distributed between −2° and 2°. For this choice, computational restrictions for the model’s intended use case were considered. Specifically, time complexity increases linearly with the number of augmentations. In keeping a small set of augmentations, we believe that small perturbations to rotation present an obvious correspondence to the variations the model will be exposed to in a real application.</p></sec><sec id="s4-10"><title>Final model selection</title><p>Both the model and training procedure contain various hyperparameters and even optional building blocks such as using DA during training, as well as applying TTA during inference. After optimizing the hyperparameters of each model independently, all of the possible variants that emerge from applying DA and TTA were explored on both classification and regression models. Therefore, the space of possible model variants is spanned by the non-augmented model, the model trained with DA, the model applying TTA during inference, as well as the model variant applying both aforementioned building blocks.</p><p>To take advantage of particularities in both models, a mixture of the best performing regression and classification models was obtained. In this model, the per-item performances of both models are compared on the held-out validation set. During inference, for each item, we output the prediction of the best performing model. Therefore, on a figure-wide scale, the prediction of the combined model uses both classification and regression models concurrently, while not combining the models’ per-item predictions.</p></sec><sec id="s4-11"><title>Robustness analysis</title><p>We investigated the robustness of our model against different semantic transformations which are likely to occur in real-life scenarios. These were rotations, perspective changes, and changes to brightness and contrast. To assess the robustness against these transformations, we transformed images from our test set with different transformation parameters. For rotations, we rotated drawings with angles chosen randomly from increasingly higher orders, both clockwise and counterclockwise. That is, we sampled angles between 0° and 5°, between 5° and 10°, and so on, up to 40° to 45°. The second transform we investigated were changes in perspective as these are also likely to occur, for example when photos of drawings are taken with a camera in a tilted position. The degree of perspective change was guided by a parameter between 0 and 1, where 0 corresponds to the original image and 1 corresponds to an extreme change in perspective, making the image almost completely unrecognizable. Furthermore, due to changes in light conditions, an image might appear brighter or with a different contrast. We used brightness parameters between 0.1 and 2.0, where values below 1.0 correspond to a darkening of the image and values above 1.0 correspond to a brighter image. Similarly, for contrast, we varied the contrast parameter between 0.1 and 2.0, with values above (below) 1.0 corresponding to high (low) contrast images.</p></sec><sec id="s4-12"><title>Image resolution analysis</title><p>The ROCF images in our dataset have varying resolutions, ranging from 100 × 140 pixels to 3500 × 5300 pixels. Since our models are trained on a fixed input resolution, we investigated the effect of different resolutions on the model performance measured in terms of MAE. To that end, we trained the multilabel classification model with and without DA for inputs of size 78 × 100, 116 × 150, 232 × 300, and 348 × 450. In principle, when using smaller images, then more information is lost due to resizing, while, on the other hand, a resolution which is too large requires bigger models due to increased complexity in the underlying distribution. In addition, using our DA pipeline with small images might negatively affect the performance since the interpolation techniques used in the semantic transformations like rotations, potentially leads to an additional loss of information. We observed this in our experiments which showed that inputs of size 232 × 300 yielded the best performance, both for the model with and without DA (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Thus, all subsequent analyses were performed with images of size 232 × 300.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Resources, Data curation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Resources, Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con12"><p>Resources, Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con13"><p>Resources, Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con14"><p>Resources, Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con15"><p>Conceptualization, Software, Methodology</p></fn><fn fn-type="con" id="con16"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con17"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con18"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con19"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con20"><p>Conceptualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con21"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study was approved by the Institutional Ethics Review Board of the 'Kantonale Ethikkommission' (BASEC-Nr. 2020-00206). All collaborators have written informed consent and/or data usage agreements for the recorded drawings from the participants. The authors assert that all procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Helsinki Declaration of 1975, as revised in 2008.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-96017-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The clinical dataset cannot be shared publicly due to the absence of consent from patients for data sharing. The Prolific dataset, deidentified raw data (i.e. image of the ROCF) can be accessed through OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/uea6f">https://osf.io/uea6f</ext-link>). Processed data used in analyses, such as summary statistics and numbers used to plot figures in the manuscript, are available as source data. All preprocessing and analysis scripts used in this study are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/methlabUZH/rey-figure">https://github.com/methlabUZH/rey-figure</ext-link>; copy archived at <xref ref-type="bibr" rid="bib33">Weber, 2024</xref>). Researchers interested in accessing any data or materials should contact the corresponding author for further instructions and to discuss the appropriate access procedures.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Langer</surname><given-names>N</given-names></name><name><surname>Strzelczyk</surname><given-names>D</given-names></name></person-group><source>Open Science Framework</source><year iso-8601-date="2024">2024</year><data-title>Open Science Framework</data-title><pub-id pub-id-type="accession" xlink:href="https://osf.io/uea6f">uea6f</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the URPP 'Dynamics of Healthy Aging' and BRIDGE [40B2-0_187132], which is a joint programme of the Swiss National Science Foundation SNSF and Innosuisse. Furthermore, BHV is funded by the Swiss National Science Foundation [10001C_197480]. Finally, CL is supported by the Swiss State Secretariat for Education, Research and Innovation (SERI) under contract number MB22.00036. None of the authors have been paid to write this article by a pharmaceutical company or other agency. Authors were not precluded from accessing data in the study, and they accept responsibility to submit for publication.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alladi</surname><given-names>S</given-names></name><name><surname>Arnold</surname><given-names>R</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name><name><surname>Nestor</surname><given-names>PJ</given-names></name><name><surname>Hodges</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mild cognitive impairment: applicability of research criteria in a memory clinic and characterization of cognitive profile</article-title><source>Psychological Medicine</source><volume>36</volume><fpage>507</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1017/S0033291705006744</pub-id><pub-id pub-id-type="pmid">16426486</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Awad</surname><given-names>N</given-names></name><name><surname>Tsiakas</surname><given-names>M</given-names></name><name><surname>Gagnon</surname><given-names>M</given-names></name><name><surname>Mertens</surname><given-names>VB</given-names></name><name><surname>Hill</surname><given-names>E</given-names></name><name><surname>Messier</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Explicit and objective scoring criteria for the taylor complex figure test</article-title><source>Journal of Clinical and Experimental Neuropsychology</source><volume>26</volume><fpage>405</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1080/13803390490510112</pub-id><pub-id pub-id-type="pmid">15512929</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>DTR</given-names></name><name><surname>Allen</surname><given-names>RS</given-names></name><name><surname>Schmitt</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Rey-Osterrieth complex figure: Psychometric characteristics in a geriatric sample</article-title><source>Clinical Neuropsychologist</source><volume>5</volume><fpage>143</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1080/13854049108403298</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bin Nazar</surname><given-names>H</given-names></name><name><surname>Moetesum</surname><given-names>M</given-names></name><name><surname>Ehsan</surname><given-names>S</given-names></name><name><surname>Siddiqi</surname><given-names>I</given-names></name><name><surname>Khurshid</surname><given-names>K</given-names></name><name><surname>Vincent</surname><given-names>N</given-names></name><name><surname>McDonald-Maier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Classification of Graphomotor Impressions Using Convolutional Neural Networks: An Application to Automated Neuro-Psychological Screening Tests</article-title><conf-name>2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR</conf-name><pub-id pub-id-type="doi">10.1109/ICDAR.2017.78</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Buolamwini</surname><given-names>J</given-names></name><name><surname>Gebru</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</article-title><conf-name>Proceedings of the 1st Conference on Fairness, Accountability and Transparency</conf-name></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Canham</surname><given-names>RO</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name><name><surname>Tyrrell</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Automated scoring of a neuropsychological test: the Rey Osterrieth complex figure</article-title><conf-name>EUROMICRO Workshop on Multimedia and Telecommunications</conf-name><pub-id pub-id-type="doi">10.1109/EURMIC.2000.874519</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ensign</surname><given-names>D</given-names></name><name><surname>Friedler</surname><given-names>SA</given-names></name><name><surname>Neville</surname><given-names>S</given-names></name><name><surname>Scheidegger</surname><given-names>C</given-names></name><name><surname>Venkatasubramanian</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Runaway Feedback Loops in Predictive Policing</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.09847">http://arxiv.org/abs/1706.09847</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fastenau</surname><given-names>PS</given-names></name><name><surname>Bennett</surname><given-names>JM</given-names></name><name><surname>Denburg</surname><given-names>NL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Application of psychometric standards to scoring system evaluation: is “new” necessarily “improved”?</article-title><source>Journal of Clinical and Experimental Neuropsychology</source><volume>18</volume><fpage>462</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1080/01688639608409003</pub-id><pub-id pub-id-type="pmid">8877628</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Franzen</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2000">2000</year><chapter-title>Validity as applied to neuropsychological assessment</chapter-title><person-group person-group-type="editor"><name><surname>Franzen</surname><given-names>MD</given-names></name></person-group><source>Reliability and Validity in Neuropsychological Assessment</source><publisher-name>Springer</publisher-name><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1007/978-1-4757-3224-5_5</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Groth-Marnat</surname><given-names>Gary</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Neuropsychological Assessment in Clinical Practice: A Guide to Test Interpretation and Integration</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harbi</surname><given-names>Z</given-names></name><name><surname>Hicks</surname><given-names>Y</given-names></name><name><surname>Setchi</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Clock drawing test digit recognition using static and dynamic features</article-title><source>Procedia Computer Science</source><volume>96</volume><fpage>1221</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2016.08.166</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsiao</surname><given-names>CJ</given-names></name><name><surname>Hing</surname><given-names>E</given-names></name><name><surname>Ashman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Trends in electronic health record system use among office-based physicians: United States, 2007-2012</article-title><source>National Health Statistics Reports</source><volume>1</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="pmid">24844589</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huygelier</surname><given-names>H</given-names></name><name><surname>Moore</surname><given-names>MJ</given-names></name><name><surname>Demeyere</surname><given-names>N</given-names></name><name><surname>Gillebert</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Non-spatial impairments affect false-positive neglect diagnosis based on cancellation tasks</article-title><source>Journal of the International Neuropsychological Society</source><volume>26</volume><fpage>668</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1017/S1355617720000041</pub-id><pub-id pub-id-type="pmid">32223770</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Cho</surname><given-names>YS</given-names></name><name><surname>Do</surname><given-names>EYL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Computational clock drawing analysis for cognitive impairment screening</article-title><conf-name>Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction</conf-name><pub-id pub-id-type="doi">10.1145/1935701.1935768</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levine</surname><given-names>AJ</given-names></name><name><surname>Miller</surname><given-names>EN</given-names></name><name><surname>Becker</surname><given-names>JT</given-names></name><name><surname>Selnes</surname><given-names>OA</given-names></name><name><surname>Cohen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Normative data for determining significance of test-retest differences on eight common neuropsychological instruments</article-title><source>The Clinical Neuropsychologist</source><volume>18</volume><fpage>373</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1080/1385404049052420</pub-id><pub-id pub-id-type="pmid">15739809</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>JE</given-names></name><name><surname>Bayless</surname><given-names>JD</given-names></name><name><surname>Meyers</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Rey complex figure: memory error patterns and functional abilities</article-title><source>Applied Neuropsychology</source><volume>3</volume><fpage>89</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1207/s15324826an0302_8</pub-id><pub-id pub-id-type="pmid">16318537</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>JE</given-names></name><name><surname>Volbrecht</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Detection of malingerers using the rey complex figure and recognition trial</article-title><source>Applied Neuropsychology</source><volume>6</volume><fpage>201</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1207/s15324826an0604_2</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Clock Drawing: A Neuropsychological Analysis</source><publisher-loc>USA</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Osterrieth</surname><given-names>Pa</given-names></name></person-group><year iso-8601-date="1944">1944</year><source>Le Test de Copie d’une Figure Complexe: Contribution à l’étude de La Perception et de La Mémoire</source><publisher-name>Delachaux et Niestlé</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title><conf-name>In Proceedings of the 33rd International Conference on Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petilli</surname><given-names>MA</given-names></name><name><surname>Daini</surname><given-names>R</given-names></name><name><surname>Saibene</surname><given-names>FL</given-names></name><name><surname>Rabuffetti</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Automated scoring for a Tablet-based Rey Figure copy task differentiates constructional, organisational, and motor abilities</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>14895</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-94247-9</pub-id><pub-id pub-id-type="pmid">34290339</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabin</surname><given-names>LA</given-names></name><name><surname>Barr</surname><given-names>WB</given-names></name><name><surname>Burton</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Assessment practices of clinical neuropsychologists in the United States and Canada: A survey of INS, NAN, and APA Division 40 members</article-title><source>Archives of Clinical Neuropsychology</source><volume>20</volume><fpage>33</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.acn.2004.02.005</pub-id><pub-id pub-id-type="pmid">15620813</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabin</surname><given-names>LA</given-names></name><name><surname>Paolillo</surname><given-names>E</given-names></name><name><surname>Barr</surname><given-names>WB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stability in test-usage practices of clinical neuropsychologists in the United States and Canada Over A 10-year period: a follow-up survey of INS and NAN members</article-title><source>Archives of Clinical Neuropsychology</source><volume>31</volume><fpage>206</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1093/arclin/acw007</pub-id><pub-id pub-id-type="pmid">26984127</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regard</surname><given-names>M</given-names></name><name><surname>Strauss</surname><given-names>E</given-names></name><name><surname>Knapp</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Children’s production on verbal and non-verbal fluency tasks</article-title><source>Perceptual and Motor Skills</source><volume>55</volume><fpage>839</fpage><lpage>844</lpage><pub-id pub-id-type="doi">10.2466/pms.1982.55.3.839</pub-id><pub-id pub-id-type="pmid">7162920</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>MS</given-names></name><name><surname>Park</surname><given-names>SY</given-names></name><name><surname>Park</surname><given-names>SR</given-names></name><name><surname>Seol</surname><given-names>SH</given-names></name><name><surname>Kwon</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Clinical and empirical applications of the Rey-Osterrieth Complex Figure Test</article-title><source>Nature Protocols</source><volume>1</volume><fpage>892</fpage><lpage>899</lpage><pub-id pub-id-type="doi">10.1038/nprot.2006.115</pub-id><pub-id pub-id-type="pmid">17406322</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somerville</surname><given-names>J</given-names></name><name><surname>Tremont</surname><given-names>G</given-names></name><name><surname>Stern</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The Boston Qualitative Scoring System as a measure of executive functioning in Rey-Osterrieth Complex Figure performance</article-title><source>Journal of Clinical and Experimental Neuropsychology</source><volume>22</volume><fpage>613</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1076/1380-3395(200010)22:5;1-9;FT613</pub-id><pub-id pub-id-type="pmid">11094396</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trautmann</surname><given-names>S</given-names></name><name><surname>Rehm</surname><given-names>J</given-names></name><name><surname>Wittchen</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The economic costs of mental disorders</article-title><source>EMBO Reports</source><volume>17</volume><fpage>1245</fpage><lpage>1249</lpage><pub-id pub-id-type="doi">10.15252/embr.201642951</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trojano</surname><given-names>L</given-names></name><name><surname>Gainotti</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Drawing disorders in Alzheimer’s disease and other forms of dementia</article-title><source>Journal of Alzheimer’s Disease</source><volume>53</volume><fpage>31</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.3233/JAD-160009</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogt</surname><given-names>J</given-names></name><name><surname>Kloosterman</surname><given-names>H</given-names></name><name><surname>Vermeent</surname><given-names>S</given-names></name><name><surname>Van Elswijk</surname><given-names>G</given-names></name><name><surname>Dotsch</surname><given-names>R</given-names></name><name><surname>Schmand</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automated scoring of the Rey-Osterrieth Complex Figure Test using a deep-learning algorithm</article-title><source>Archives of Clinical Neuropsychology</source><volume>34</volume><elocation-id>836</elocation-id><pub-id pub-id-type="doi">10.1093/arclin/acz035.04</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The reliability of multidimensional neuropsychological measures: from alpha to omega</article-title><source>The Clinical Neuropsychologist</source><volume>31</volume><fpage>1113</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1080/13854046.2017.1317364</pub-id><pub-id pub-id-type="pmid">28429633</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>SS</given-names></name><name><surname>Moore</surname><given-names>MJ</given-names></name><name><surname>Yamshchikova</surname><given-names>A</given-names></name><name><surname>Kozik</surname><given-names>V</given-names></name><name><surname>Duta</surname><given-names>MD</given-names></name><name><surname>Voiculescu</surname><given-names>I</given-names></name><name><surname>Demeyere</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Validation of an automated scoring program for a digital complex figure copy task within healthy aging and stroke</article-title><source>Neuropsychology</source><volume>35</volume><fpage>847</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1037/neu0000748</pub-id><pub-id pub-id-type="pmid">34618514</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>rey-figure</data-title><version designator="swh:1:rev:6925945e681ed01bea62197e9243db247b6ab641">swh:1:rev:6925945e681ed01bea62197e9243db247b6ab641</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:54fb0f16698545cadd2894a588e5fa5cb1a63d66;origin=https://github.com/methlabUZH/rey-figure;visit=swh:1:snp:d96abc569dd51bb9b67d067d1ca02bc0135e7254;anchor=swh:1:rev:6925945e681ed01bea62197e9243db247b6ab641">https://archive.softwareheritage.org/swh:1:dir:54fb0f16698545cadd2894a588e5fa5cb1a63d66;origin=https://github.com/methlabUZH/rey-figure;visit=swh:1:snp:d96abc569dd51bb9b67d067d1ca02bc0135e7254;anchor=swh:1:rev:6925945e681ed01bea62197e9243db247b6ab641</ext-link></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96017.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Juan Helen</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tgyzw49</institution-id><institution>National University of Singapore</institution></institution-wrap><country>Singapore</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The methods and findings of the current work are <bold>important</bold> and well-grounded. The strength of the evidence presented is <bold>convincing</bold> and backed up by rigorous methodology. The work, when elaborated on how to access the app, will have far-reaching implications for current clinical practice.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96017.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors aimed to develop and validate an automated, deep learning-based system for scoring the Rey-Osterrieth Complex Figure Test (ROCF), a widely used tool in neuropsychology for assessing memory deficits. Their goal was to overcome the limitations of manual scoring, such as subjectivity and time consumption, by creating a model that provides automatic, accurate, objective, and efficient assessments of memory deterioration in individuals with various neurological and psychiatric conditions.</p><p>Strengths:</p><p>Comprehensive Data Collection: The authors collected over 20,000 hand-drawn ROCF images from a wide demographic and geographic range, ensuring a robust and diverse dataset. This extensive data collection is critical for training a generalizable and effective deep learning model.</p><p>Advanced Deep Learning Approach: Utilizing a multi-head convolutional neural network to automate ROCF scoring represents a sophisticated application of current AI technologies. This approach allows for detailed analysis of individual figure elements, potentially increasing the accuracy and reliability of assessments.</p><p>Validation and Performance Assessment: The model's performance was rigorously evaluated against crowdsourced human intelligence and professional clinician scores, demonstrating its ability to outperform both groups. The inclusion of an independent prospective validation study further strengthens the credibility of the results.</p><p>Robustness Analysis Efficacy: The model underwent a thorough robustness analysis, testing its adaptability to variations in rotation, perspective, brightness, and contrast. Such meticulous examination ensures the model's consistent performance across different clinical imaging scenarios, significantly bolstering its utility for real-world applications.</p><p>Appraisal and discussion:</p><p>By leveraging a comprehensive dataset and employing advanced deep learning techniques, they demonstrated the model's ability to outperform both crowdsourced raters and professional clinicians in scoring the ROCF. This achievement represents a significant step forward in automating neuropsychological assessments, potentially revolutionizing how memory deficits are evaluated in clinical settings. Furthermore, the application of deep learning to clinical neuropsychology opens avenues for future research, including the potential automation of other neuropsychological tests and the integration of AI tools into clinical practice. The success of this project may encourage further exploration into how AI can be leveraged to improve diagnostic accuracy and efficiency in healthcare.</p><p>However, the critique regarding the lack of detailed analysis across different patient demographics, the inadequacy of network explainability, and concerns about the selection of median crowdsourced scores as ground truth raises questions about the completeness of their objectives. These aspects suggest that while the aims were achieved to a considerable extent, there are areas of improvement that could make the results more robust and the conclusions stronger.</p><p>Comments on revised version:</p><p>I appreciate the opportunity to review this revised submission. Having considered the other reviews, I believe this study presents an important advance in using AI methods for clinical applications, which is both innovative and has implications beyond a single subfield.</p><p>The authors have developed a system using fundamental AI that appears sufficient for clinical use in scoring the Rey-Osterrieth Complex Figure (ROCF) test. In human neuropsychology, tests that generate scores like this are a key part of assessing patients. The evidence supporting the validity of the AI scoring system is compelling. This represents a valuable step towards evaluating more complex neurobehavioral functions.</p><p>However, one area where the study could be strengthened is in the explainability of the AI methods used. To ensure the scores are fully transparent and consistent for clinical use, it will be important for future work to test the robustness of the approach, potentially by comparing multiple methods. Examining other latent variables that can explain patients' cognitive functioning would also be informative.</p><p>In summary, I believe this study provides an important proof-of-concept with compelling evidence, while also highlighting key areas for further development as this technology moves towards real-world clinical applications.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96017.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors aimed to develop and validate a machine-learning driven neural network capable of automatic scoring of the Rey-Osterrieth Complex Figure. They aimed to further assess the robustness of the model to various parameters such as tilt and perspective shift in real drawings. The authors leveraged the use of a huge sample of lay workers in scoring figures and also a large sample of trained clinicians to score a subsample of figures. Overall, the authors found their model to have exceptional accuracy and perform similarly to crowdsourced workers and clinicians with, in some cases, less degree of error/score dispersion than clinicians.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96017.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This study presented a valuable inventory of scoring a neuropsychological test, ROCFT, with constructing an artificial intelligence model.</p><p>Comments on latest version:</p><p>The authors made the system with fundamental AI that is sufficient for clinical use for humans. In human neuropsychology, the test that generates the score is fundamental and relatively easy. Neuropsychologists apply patients to many tests; therefore, the present system is one of them, where we cannot tell the total neurofunction of a patient. The evidence for scoring is thought to be compelling quality, enough for clinical use now and we progress to evaluate other more complicated human neuropsychological functions. For example, persons with dementia change their performance easily when they feel other emotions (worry, boredom, etc.) and notice other stimulation (announcements in the hospital, a walking nurse by chance, etc.). The score of ROCF is definitely changing, compelling the effort of AI scoring. We should grasp this behavior of humans with diverse tests totally. Therefore, scoring AI with compelling quality is a fundamental step for the next, evaluation against the changeable and ambiguous neurobehavior of humans.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96017.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Langer</surname><given-names>Nicolas</given-names></name><role specific-use="author">Author</role><aff><institution>University of Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Weber</surname><given-names>Maurice</given-names></name><role specific-use="author">Author</role><aff><institution>ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Hebling Vieira</surname><given-names>Bruno</given-names></name><role specific-use="author">Author</role><aff><institution>University of Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Strzelczyk</surname><given-names>Dawid</given-names></name><role specific-use="author">Author</role><aff><institution>University of Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Wolf</surname><given-names>Lukas</given-names></name><role specific-use="author">Author</role><aff><institution>ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Pedroni</surname><given-names>Andreas</given-names></name><role specific-use="author">Author</role><aff><institution>University of Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Heitz</surname><given-names>Jonathan</given-names></name><role specific-use="author">Author</role><aff><institution>University of Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Müller</surname><given-names>Stephan</given-names></name><role specific-use="author">Author</role><aff><institution>ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Schultheiss</surname><given-names>Christoph</given-names></name><role specific-use="author">Author</role><aff><institution>ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Troendle</surname><given-names>Marius</given-names></name><role specific-use="author">Author</role><aff><institution>University of Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Lasprilla</surname><given-names>Juan Carlos Arango</given-names></name><role specific-use="author">Author</role><aff><institution>Virginia Commonwealth University</institution><addr-line><named-content content-type="city">Virgina</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Rivera</surname><given-names>Diego</given-names></name><role specific-use="author">Author</role><aff><institution>Public University of Navarre</institution><addr-line><named-content content-type="city">Navarre</named-content></addr-line><country>Spain</country></aff></contrib><contrib contrib-type="author"><name><surname>Scarpina</surname><given-names>Federica</given-names></name><role specific-use="author">Author</role><aff><institution>I.R.C.C.S. Istituto Auxologico Italiano</institution><addr-line><named-content content-type="city">Milano</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Qianhua</given-names></name><role specific-use="author">Author</role><aff><institution>Huashan Hospital</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Leuthold</surname><given-names>Rico</given-names></name><role specific-use="author">Author</role><aff><institution>Smartcode</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Wehrle</surname><given-names>Flavia</given-names></name><role specific-use="author">Author</role><aff><institution>University Children's Hospital Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Jenni</surname><given-names>Oskar</given-names></name><role specific-use="author">Author</role><aff><institution>University Children's Hospital Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Brugger</surname><given-names>Peter</given-names></name><role specific-use="author">Author</role><aff><institution>Rehabilitation Center</institution><addr-line><named-content content-type="city">Valens</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Zaehle</surname><given-names>Tino</given-names></name><role specific-use="author">Author</role><aff><institution>Otto-von-Guericke University</institution><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Lorenz</surname><given-names>Romy</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Biological Cybernetics</institution><addr-line><named-content content-type="city">Tuebingen</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ce</given-names></name><role specific-use="author">Author</role><aff><institution>ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>Comment #1: Insufficient Network Analysis for Explainability: The paper does not sufficiently delve into network analysis to determine whether the model's predictions are based on accurately identifying and matching the 18 items of the ROCF or if they rely on global, item-irrelevant features. This gap in analysis limits our understanding of the model's decision-making process and its clinical relevance.</p></disp-quote><p>Response #1: Thank you for your comment. We acknowledge the importance of understanding the decision-making process of AI models is crucial for their acceptance and utility in clinical settings. However, we believe that our current approach, which focuses on providing individual scores for each of the 18 items of the Rey-Osterrieth Complex Figure (ROCF), inherently offers a higher level of explainability and practical utility for clinicians than a network analysis could. Our multi-head convolutional neural network is designed with a dedicated output head for each of the 18 items in the ROCF, and thus provides separate scores for each of the 18 items in the ROCF. This architecture helps that the model focuses on individual elements rather than relying on global, item-irrelevant features.</p><p>This item-specific approach directly aligns with the traditional clinical assessment method, thereby making the results more interpretable and actionable for clinicians. The individual scores for each item provide detailed insights into a patient's performance. Clinicians can use these scores to identify specific areas of strength and weakness in a patient's visuospatial memory and drawing abilities.</p><p>Furthermore, we evaluated the model's performance on each of the 18 items separately, providing detailed metrics that show consistent accuracy across all items. This item-level performance analysis offers clear evidence that the model is not relying on irrelevant global features but is indeed making decisions based on the specific characteristics of each item. We believe that our approach provides a level of explainability that is directly useful and relevant to clinical practitioners.</p><disp-quote content-type="editor-comment"><p>Comment #2: Generative Model Consideration: The critique suggests exploring generative models to model the joint distribution of images and scores, which could offer deeper insights into the relationship between scores and specific visual-spatial disabilities. The absence of this consideration in the study is seen as a missed opportunity to enhance the model's explainability and clinical utility.</p></disp-quote><p>Response #2: Thank you for your thoughtful comment and the suggestion to explore generative models. We appreciate the potential benefits that generative models to model the joint distribution of images and scores. However, we chose not to pursue this approach in our study for several reasons: First, our primary goal was to develop a model that provides accurate and interpretable scores for each of the 18 individual items in the ROCF figure. Second, generative models, while powerful, would add a layer of complexity that might diminish the clarity and immediate clinical applicability of our results. Generative models, (particularly deep learning-based) can be challenging to interpret in terms of how they make decisions or why they produce specific outputs. This lack can be a concern in critical applications involving neurological and psychiatric disorders. Clinicians require tools that provide clear insights without the need for additional layers of analysis. Our current model provides detailed, item-specific scores that clinicians can directly use to assess visuospatial memory and drawing abilities. Initially, we explored using generative models (i.e. GANs) for data augmentation to address the scarcity of low-score images compared to high-score images. Moreover, for the low-score images, the same score can be achieved by numerous combinations of figure elements. However, due to our extensive available dataset, we did not observe any substantial performance improvements in our model. Nevertheless, future studies could explore generative models, such as Variational Autoencoders (VAEs) or Bayesian Networks, and test them on the data from the current prospective study to compare their performance with our results.</p><p>In the revised manuscript, we have included additional sentences discussing the potential use of generative models and their implications for future research.</p><p>“The data augmentation did not include generative models. Initially, we explored using generative models, specifically GANs, for data augmentation to address the scarcity of low-score images compared to high-score images. However, due to the extensive available dataset, we did not observe any substantial performance improvements in our model. Nevertheless, Future studies could explore generative models, such as Variational Autoencoders (VAEs) or Bayesian Networks, which can then be tested on the data from the current prospective study and compared with our results.”</p><disp-quote content-type="editor-comment"><p>Comment #3: Lack of Detailed Model Performance Analysis Across Subject Conditions: The study does not provide a detailed analysis of the model's performance across different ages, health conditions, etc. This omission raises questions about the model's applicability to diverse patient populations and whether separate models are needed for different subject types.</p></disp-quote><p>Response #3: Thank you for your this important comment. Although the initial version of our manuscript already provided detailed “item-specific” and “across total scores” performance metrics, we recognize the importance of including detailed analyses across different patient demographics to enhance the robustness and applicability of our findings. In response to your comment, we have conducted additional analyses that provide a comprehensive evaluation of model performance across various patient demographics, such as age groups, gender, and different neurological and psychiatric conditions. This additional analysis demonstrates the generalizability and reliability of our model across diverse populations. We have included these analyses in the revised manuscript.</p><p>“In addition, we have conducted a comprehensive model performance analysis to evaluate our model's performance across different ROCF conditions (copy and recall), demographics (age, gender), and clinical statuses (healthy individuals and patients) (Figure 4A). These results have been confirmed in the prospective validation study (Supplementary Figure S6). Furthermore, we included an additional analysis focusing on specific diagnoses to assess the model's performance in diverse patient populations (Figure 4B). Our findings demonstrate that the model maintains high accuracy and generalizes well across various demographics and clinical conditions.”</p><disp-quote content-type="editor-comment"><p>Comment #4: Data Augmentation: While the data augmentation procedure is noted as clever, it does not fully encompass all affine transformations, potentially limiting the model's robustness.</p></disp-quote><p>Response #4: We appreciate your feedback on our data augmentation strategy. We acknowledge that while our current approach significantly improves robustness against certain semantic transformations, it may not fully cover all possible affine transformations.</p><p>Here, we provide further clarification and justification for our chosen methods and their impact on the model's performance: In our study, we implemented a data augmentation pipeline to enhance the robustness of our model against common and realisitc geometric and semantic-preserving transformations. This pipeline included rotations, perspective changes, and Gaussian blur, which we found to be particularly effective in improving the model's resilience to variations in input data. These transformations are particularly relevant for the present application since users in real-life are likely to take pictures of drawings that might be slightly rotated or with a slightly tilted perspective. With these intuitions in mind, we randomly transformed drawings during training. Each transformation was a combination of Gaussian blur, a random perspective change, and a rotation with angles chosen randomly between -10° and 10°. These transformations are representative of realistic scenarios where images might be slightly tilted or photographed from different angles. We intentionally did not explicitly address all affine transformations, such as shearing or more complex geometric transformations because these transformations could alter the score of individual items of the ROCF and would be disruptive to the model.</p><p>As noted in our manuscript and demonstrated in supplementary Figure S1, the data augmentation pipeline significantly improved the model's robustness against rotations and changes in perspective. Importantly, our tablet-based scoring application can further ensure that the photos taken do not exhibit excessive semantic transformations. By leveraging the gyroscope built into the tablet, the application can help users align the images properly, minimizing issues such as excessive rotation or skew. This built-in functionality helps maintain the quality and consistency of the images, reducing the likelihood of significant semantic transformations that could affect model performance.</p><disp-quote content-type="editor-comment"><p>Comment #5: Additionally, the rationale for using median crowdsourced scores as ground truth, despite evidence of potential bias compared to clinician scores, is not adequately justified.</p></disp-quote><p>Response #5: Thank you for this valuable comment. Clarifying the rationale behind using the median score of crowdsourcing as the ground truth is indeed important. To reach high accuracy in predicting individual sample scores of the ROCFs, it is imperative that the scores of the training set are based on a systematic scheme with as little human bias as possible influencing the score. However, our analysis (see results section) and previous work (Canham et al., 2000) suggested that the scoring conducted by clinicians may not be consistent, because the clinicians may be unwittingly influenced by the interaction with the patient/participant or by the clinicians factor (e.g. motivation and fatigue). For this reason and the incomplete availability of clinician scores for all figures (i.e. for 19% of the 20’225 figures), we did not use the clinicians scores as ground truth scores. Instead, we have trained a large pool (5000 workers) of human internet workers (crowdsourcing) to score ROCFs drawings guided by our self-developed interactive web application. Each element of the figure was scored by several human workers (13 workers on average per figure). We have obtained the ground truth for each drawing by computing the median for each item in the figure, and then summed up the medians to get the total score for the drawing in question. To further ensure high-quality data annotation, we identified and excluded crowdsourcing participants that have a high level of disagreement (&gt;20% disagreement) with this rating from trained clinicians, who carefully scored manually a subset of the data in the same interactive web application.</p><p>We chose the median score for several reasons: (1) the median score is less influenced by outliers compared to the mean. Given the variability of scoring between different clinicians and human workers (see human MSE and clinician MSE), using the median ensures that the ground truth is not skewed by extreme values, leading to more stable and reliable scores. (2) Crowdsource data do not always follow a normal distribution. In cases where the distribution is skewed or not symmetric, the median can be a more representative measure of the center. (3) The original scoring system involves ordinal scales (0,0.5,1,2). For ordinal scales, the median is often more appropriate than the mean. Finally, by aggregating multiple scores from a large pool of crowdsourced raters, the median provides a consensus that reflects the most common assessment. This approach mitigates the variability introduced by individual rater biases and ensures a more consistent ground truth. In clinical settings, the consensus of multiple expert opinions often serves as the benchmark for assessments. The use of median scores mirrors this practice, providing a ground truth that is representative of collective human judgment.</p><p>Canham, R. O., S. L. Smith, and A. M. Tyrrell. 2000. “Automated Scoring of a Neuropsychological Test:</p><p>The Rey Osterrieth Complex Figure.” Proceedings of the 26th Euromicro Conference. EUROMICRO 2000. Informatics: Inventing the Future. https://doi.org/10.1109/eurmic.2000.874519.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>Comment #1: There is no detail on how the final scoring app can be accessed and whether it is medical device-regulated.</p></disp-quote><p>Response #1: We appreciate the opportunity to provide more information about the current status and plans for our scoring application. At this stage, the final scoring app is not publicly accessible as it is currently undergoing rigorous beta testing with a select group of clinicians in real-world settings. The feedback from these clinicians is instrumental in refining the app’s features, interface, and overall functionality to improve its usability and user experience. This ensures that the app meets the high standards required for clinical tools. Following the successful completion of the beta testing phase, we aim to seek FDA approval for the scoring app. Achieving this regulatory milestone will guarantee that the app meets the stringent requirements for medical devices, providing an additional layer of confidence in its safety and efficacy for clinical use. Once FDA approval is obtained, we plan to make the app publicly accessible to clinicians and healthcare institutions worldwide. Detailed instructions on how to access and use the app will be provided at that time on our website (https://www.psychology.uzh.ch/en/areas/nec/plafor/research/rfp.html).</p><disp-quote content-type="editor-comment"><p>Comment #2: No discussion on the difference in sample sizes between the pre-registration of the prospective study and the results (e.g., aimed for 500 neurological patients but reported data from 288). Demographics for the assessment of the representation of healthy and non-healthy participants were not present.</p></disp-quote><p>Response #2: Thank you for your comment. We believe there might have been a misunderstanding regarding our preregistration details. In the preregistration, we planned to prospectively acquire ROCF drawings from 1000 healthy subjects. Each subject should have drawn two ROCF drawings (copy and memory condition). Consequently, 2000 samples should have been collected. In addition, in our pre-registration plan, we aimed to collect 500 drawings from patients (i.e. 250 patients), not 500 patients as the reviewer suggested (https://osf.io/82796). Thus in total, the goal was to obtain 2500 ROCF figures. The final prospective data set, which contained 2498 ROCF images from 961 healthy adults and 288 patients very closely matches the sample size, we aimed for in the the pre-registration. We do not see a necessity to discuss this negligible discrepancy in the main manuscript. The prospective data set remains substantial and sufficient to test our model on the independent prospective data set. Importantly, we want to highlight that the test set in the retrospective data set (4045 figures) was also never seen by the model. Both the retrospective and prospective data sets demonstrate substantial global diversity as the data has been collected in 90 different countries. Please note, that Supplementary Figures S2 &amp; S3 provide detailed demographics of the participants in the prospectively collected data, present their performance in the copy and (immediate) recall condition across the lifespan, and the worldwide distribution of the origin of the data.</p><disp-quote content-type="editor-comment"><p>Comment #3: Supplementary Figure S1 &amp; S4 is poor quality, please increase resolution.</p></disp-quote><p>Response #3: We apologize for the poor quality of Supplementary Figures S1 and S4 in the initial submission. In the revised version of our submission, we have increased the resolution of both Supplementary Figure S1 and Supplementary Figure S4 to ensure that all details are clearly visible and the figures are of high quality.</p><disp-quote content-type="editor-comment"><p>Comment #4: Regarding medical device regulation; if the app is to be used in clinical practice (as it generates a score and classification of performance), I believe such regulation is necessary - but there are ways around it. This should be detailed.</p></disp-quote><p>Response #4: We agree that regulation is essential for any application intended for use in clinical practice, particularly one that generates scores and classifications of performance. As discussed in response #1, the final scoring application is currently undergoing intensive beta testing in real-world settings with a limited group of clinicians and is therefore not publicly accessible at this time. We are fully committed to obtaining the necessary regulatory approvals before the app is made publicly accessible for clinical use. Once the beta testing phase is complete and the app has been refined based on clinician feedback, we will prepare and submit a comprehensive regulatory dossier. This submission will include all necessary data on the app's development, testing, validation, and clinical utility. We are adhering to relevant regulatory standards and guidelines, such as ISO 13485 for medical devices and the FDA's guidance on software as a medical device (SaMD).</p><disp-quote content-type="editor-comment"><p>Comment #7: Need to clarify that work was already done and pre-printed in 2022 for the main part of this study, and that this paper contributes to an additional prospective study.</p></disp-quote><p>Response #7: We would like to clarify that the pre-print the reviewer is referring to is indeed the current paper submitted to ELife. The submitted paper includes both the work that was pre-printed in 2022 and the additional prospective study, as you correctly identified.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Comment #1: The considerable effort and cost to make the model only for an existing neuropsychological test.</p></disp-quote><p>Response #1: We acknowledge that significant effort and resources were dedicated to developing our model for the Rey-Osterrieth Complex Figure (ROCF) test. Below, we provide a detailed rationale for this investment and the broader implications of our work. The ROCF test is one of the most widely used neuropsychological assessments worldwide, providing critical insights into visuospatial memory and executive function. While the initial effort and cost are substantial, the long-term benefits of an automated, reliable, objective, fast and widely applicable neuropsychological assessment tool justify the investment. The scoring application will significantly reduce the time for scoring the test and thus provide more efficient use of clinical resources, and the potential for broader applications makes this a worthwhile endeavor. The methods and infrastructure developed for this model can be adapted and scaled to other neuropsychological tests and assessments (e.g. Taylor Figure).</p><disp-quote content-type="editor-comment"><p>Comment #2: I was truly impressed by the authors' establishment of a system that organizes the methods and fields of diverse specialties in such a remarkable way. I know the primary purpose of ROCFT. However, beyond the score, neuropsychologically, these are observed by specialists while ROCFT and that is attractive of the test: the turn of each stroke (e.g., from right to left, from the main structure to the margin or small structure), the process to total completeness as a figure, e.g., confidential speed and concentration, the boldness of strokes, unnatural fragmentation of strokes, the not deviated place in a paper, turning of the figure itself (before the scanning level), the total size, the level compared with the age, education, and experiences of the patient. Those are reflected by the disease, visuospatial intelligence, executive function, and ability to concentrate. Scores are crucial, but by observing the drawing process, we can obtain diverse facts or parts of symptoms that imply the complications of human behavior.</p></disp-quote><p>Response #2: Thank you for your insightful comments and observations regarding our system for organizing diverse specialties within the ROCFT methodology. We agree that beyond the numerical scores, the detailed observation of the drawing process provides invaluable neuropsychological insights. How strokes are executed, from their direction and placement to the overall completion process, offers a nuanced understanding of factors like spatial orientation, concentration, and executive function. In fact, we are working on a ROCF pen tracking application, which enables the patient to draw the ROCF with a digital pen on a tablet. The tablet can (1) assess the sequence order of drawing the items and the number of strokes, (2) record the exact coordinate of each drawn pixel at each time point of the assessment, (3) measure the duration for each pen stroke as well as total drawing time, and (4) assess the pen stroke pressure. Through this, we aim to extract additional information on processing speed, concentration, and other cognitive domains. However, this development is outside the scope of the current manuscript.</p></body></sub-article></article>