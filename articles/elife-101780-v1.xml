<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">101780</article-id><article-id pub-id-type="doi">10.7554/eLife.101780</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.101780.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A general framework for characterizing optimal communication in brain networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Fakhar</surname><given-names>Kayson</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0615-1777</contrib-id><email>kayson.fakhar@mrc-cbu.cam.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund11"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hadaeghi</surname><given-names>Fatemeh</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Seguin</surname><given-names>Caio</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dixit</surname><given-names>Shrey</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Messé</surname><given-names>Arnaud</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9081-3088</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zamora-López</surname><given-names>Gorka</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Misic</surname><given-names>Bratislav</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0307-2862</contrib-id><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>MRC Cognition and Brain Sciences Unit, University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00g30e956</institution-id><institution>Institute of Computational Neuroscience, University Medical Center Eppendorf-Hamburg, Hamburg University, Hamburg Center of Neuroscience</institution></institution-wrap><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>Department of Psychological and Brain Sciences, Indiana University</institution></institution-wrap><addr-line><named-content content-type="city">Bloomington</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Department of Psychology, Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution>International Max Planck Research School on Cognitive Neuroimaging</institution><addr-line><named-content content-type="city">Barcelona</named-content></addr-line><country>Spain</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04n0g0b29</institution-id><institution>Center for Brain and Cognition, Pompeu Fabra University</institution></institution-wrap><addr-line><named-content content-type="city">Barcelona</named-content></addr-line><country>Spain</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04n0g0b29</institution-id><institution>Department of Information and Communication Technologies, Pompeu Fabra University</institution></institution-wrap><addr-line><named-content content-type="city">Barcelona</named-content></addr-line><country>Spain</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McConnell Brain Imaging Centre, Montréal Neurological Institute, McGill University</institution></institution-wrap><addr-line><named-content content-type="city">Montréal</named-content></addr-line><country>Canada</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Department of Health Sciences, Boston University</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schneider-Mizell</surname><given-names>Casey M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dcv1019</institution-id><institution>Allen Institute for Brain Science</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>17</day><month>04</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP101780</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-08-09"><day>09</day><month>08</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-06-12"><day>12</day><month>06</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.06.12.598676"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-11"><day>11</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101780.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-03-18"><day>18</day><month>03</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.101780.2"/></event></pub-history><permissions><copyright-statement>© 2024, Fakhar et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Fakhar et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-101780-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-101780-figures-v1.pdf"/><abstract><p>Efficient communication in brain networks is foundational for cognitive function and behavior. However, how communication efficiency is defined depends on the assumed model of signaling dynamics, e.g., shortest path signaling, random walker navigation, broadcasting, and diffusive processes. Thus, a general and model-agnostic framework for characterizing optimal neural communication is needed. We address this challenge by assigning communication efficiency through a virtual multi-site lesioning regime combined with game theory, applied to large-scale models of human brain dynamics. Our framework quantifies the exact influence each node exerts over every other, generating optimal influence maps given the underlying model of neural dynamics. These descriptions reveal how communication patterns unfold if regions are set to maximize their influence over one another. Comparing these maps with a variety of brain communication models showed that optimal communication closely resembles a broadcasting regime in which regions leverage multiple parallel channels for information dissemination. Moreover, we found that the brain’s most influential regions are its rich-club, exploiting their topological vantage point by broadcasting across numerous pathways that enhance their reach even if the underlying connections are weak. Altogether, our work provides a rigorous and versatile framework for characterizing optimal brain communication, and uncovers the most influential brain regions, and the topological features underlying their influence.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain networks</kwd><kwd>communication</kwd><kwd>network dynamics</kwd><kwd>propagation</kwd><kwd>communication efficiency</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB 936-178316478</award-id><principal-award-recipient><name><surname>Fakhar</surname><given-names>Kayson</given-names></name><name><surname>Dixit</surname><given-names>Shrey</given-names></name><name><surname>Messé</surname><given-names>Arnaud</given-names></name><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>TRR169</award-id><principal-award-recipient><name><surname>Fakhar</surname><given-names>Kayson</given-names></name><name><surname>Hadaeghi</surname><given-names>Fatemeh</given-names></name><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SPP 2041/GO 2888/2-2</award-id><principal-award-recipient><name><surname>Fakhar</surname><given-names>Kayson</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SPP 2041/HI 1286/7-1</award-id><principal-award-recipient><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB 1461</award-id><principal-award-recipient><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Human Brain Project</institution></institution-wrap></funding-source><award-id>EU (SGA2)</award-id><principal-award-recipient><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Discovery Grant RGPIN #017-04265</award-id><principal-award-recipient><name><surname>Misic</surname><given-names>Bratislav</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009408</institution-id><institution>Brain Canada</institution></institution-wrap></funding-source><award-id>Future Leaders Fund</award-id><principal-award-recipient><name><surname>Misic</surname><given-names>Bratislav</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000024</institution-id><institution>Canadian Institutes of Health Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Misic</surname><given-names>Bratislav</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution>Human Brain Project</institution></institution-wrap></funding-source><award-id>EU(SGA3)</award-id><principal-award-recipient><name><surname>Hilgetag</surname><given-names>Claus C</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011730</institution-id><institution>Templeton World Charity Foundation</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.54224/30510</award-id><principal-award-recipient><name><surname>Fakhar</surname><given-names>Kayson</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A game-theoretical framework for uncovering how optimal signal transmission takes place in a given network, here applied to large-scale brain networks.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In the realm of network science, the human brain represents a perfect example of a complex system. Viewing the brain as a network has uncovered key topological characteristics, including the presence of densely interconnected modules, central hubs, and hierarchical structures in information processing (<xref ref-type="bibr" rid="bib20">Bullmore and Sporns, 2009</xref>; <xref ref-type="bibr" rid="bib116">Sporns et al., 2004</xref>). This perspective, particularly obtained by analysis of large-scale structural brain networks — the connectome — elucidated that a range of network characteristics arise from the interplay between two conflicting evolutionary imperatives: minimizing wiring costs on one hand and maximizing signaling efficiency on the other (<xref ref-type="bibr" rid="bib12">Betzel et al., 2016a</xref>; <xref ref-type="bibr" rid="bib56">Gulyás et al., 2015</xref>; <xref ref-type="bibr" rid="bib62">Kaiser and Hilgetag, 2006</xref>).</p><p>The brain is confined within a three-dimensional space, making the formation and maintenance of long-range connections metabolically expensive. Conversely, a network predominantly comprising short-range connections, although economical in wiring costs, necessitates multiple intermediate nodes for information relay to distant nodes, diminishing effective communication capacity (<xref ref-type="bibr" rid="bib21">Bullmore and Sporns, 2012</xref>; <xref ref-type="bibr" rid="bib126">Wang and Clandinin, 2016</xref>). It has been shown that this dichotomy leads to a compromise, where predominantly short-range connections among neighboring regions coexist with a selective set of long-range connections that function as communication shortcuts (<xref ref-type="bibr" rid="bib8">Bassett and Bullmore, 2017</xref>; <xref ref-type="bibr" rid="bib25">Chen et al., 2013</xref>).</p><p>In this view, signaling efficiency between nodes is graded based on the number of steps from a source to a target, presupposing information transmission <italic>exclusively</italic> along one path, the shortest path (<xref ref-type="bibr" rid="bib69">Latora and Marchiori, 2001</xref>). However, it has been argued that such a conceptualization of signaling unrealistically requires brain regions to possess global knowledge of the network’s connectivity (<xref ref-type="bibr" rid="bib5">Avena-Koenigsberger et al., 2017</xref>). To address this issue, alternative <bold>communication models (CMs)</bold> have been developed, accommodating other conceptualizations, such as cascading dynamics and diffusive processes, where the information permeates through the network via parallel pathways. Consequently, alternative conceptualizations of how distant brain regions interact yield diverse interpretations of communication efficiency (<xref ref-type="bibr" rid="bib110">Seguin et al., 2023</xref>). For instance, as mentioned above, the <bold>shortest path efficiency (SPE)</bold>, derived from the inverse of the shortest path length, reflects efficiency under the assumption that information flows along the shortest path only. In contrast, <bold>diffusion efficiency (DE)</bold>, defined by the inverse of the average path length of an unbiased random walker traversing from the source to the target, assumes information to propagate along parallel pathways with no specific preference for any of them. Quantifying how much a source can effectively influence its target depends on which perspective is chosen. Given SPE, the source has optimal influence over its unconnected target, if the number of intermediate nodes along the shortest path between them is minimal. Given DE, the source has optimal influence over its unconnected target, if it has the largest number of pathways connected with it, regardless of how long they are. Thus, the optimality of signaling depends on assumptions about the regime in which the information propagates in the network.</p><p>Moreover, despite this plurality of signaling conceptualizations, these models of propagation simplify communication patterns to linear, discrete interactions, thereby abstracting the complexities inherent in biological systems, such as non-linear interactions, oscillatory behaviors, and conductance delays (<xref ref-type="bibr" rid="bib110">Seguin et al., 2023</xref>; <xref ref-type="bibr" rid="bib118">Suárez et al., 2020</xref>). A plethora of biophysical models have been developed to address such details by modeling the behavior of regions as the average activity of biophysically grounded neuronal populations (<xref ref-type="bibr" rid="bib11">Bettinardi et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Breakspear, 2017</xref>). This mean-field formalization of brain activity comes with its own definitions of communication, such as communication through coherence, positing neuronal synchronization as a pivotal mechanism (<xref ref-type="bibr" rid="bib19">Buehlmann and Deco, 2010</xref>; <xref ref-type="bibr" rid="bib45">Fries, 2015</xref>; <xref ref-type="bibr" rid="bib125">Vinck et al., 2023</xref>).</p><p>Therefore, while it is acknowledged that the brain’s structural evolution favors optimal inter-regional communication, a consensus on the precise model of communication within brain networks and subsequently its optimality, remains missing due to conflicting assumptions about signal propagation. Addressing this issue is of particular interest, for several reasons. For example, the identification of structural features that improve information flow in brain networks elucidates their role in disease and may allow targeting them for specific therapeutic interventions. Such insights may also guide the design of neuromorphic systems that achieve a similar level of resource management as the brain (<xref ref-type="bibr" rid="bib71">Lennie, 2003</xref>; <xref ref-type="bibr" rid="bib90">Padamsey and Rochefort, 2023</xref>; <xref ref-type="bibr" rid="bib111">Senn et al., 2023</xref>).</p><p>To provide a mathematically rigorous definition of optimal communication, we resort to game theory. This field analyzes interactions among agents to offer a normative account, that is, to prescribe <italic>how they should behave</italic> in a defined scenario, given their goals, the rules of the game, and the consequences of others’ decisions (<xref ref-type="bibr" rid="bib17">Binmore, 2007</xref>). Notably, game theoretical solutions are well established in fields where players lack ‘agency.’ In behavioral ecology, for example, game theoretical models are used to define normative descriptions for how animals should behave in scenarios involving optimizing a goal, such as foraging or reproducing (<xref ref-type="bibr" rid="bib30">Davies et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Dugatkin and Reeve, 2000</xref>). Thus, this theoretical approach is well suited for investigating a pivotal question in neuroscience: How should brain regions interact, considering the limitations imposed by the structural connectome and the behavior of other regions, with the aim of optimizing their communication efficiency? This question is framed by two principal constraints and one overarching objective. The first constraint is imposed by the <italic>brain’s network structure</italic>, delineating interactions between nodes. The second constraint is the <italic>behavior of other nodes</italic>, dictating their response to incoming signals. The objective, in this context, is to maximize signaling efficiency – conceptualized as the extent of a node’s influence over others. Such a formulation gives rise to a concrete definition of optimality as an equilibrium point where no region can increase its influence on others any further. Notably, this game-theoretical approach offers a model-agnostic and data-driven toolkit to define communication within brain networks. By ‘model agnostic’ we mean that the game can be constructed using any model of information propagation and local dynamics incorporated into an arbitrary connectome structure. It then defines optimality in the sense that, given the defined game, how much each node can influence other nodes. Note that optimality in this framework depends on the defined game, meaning that optimal communication can differ given different network architectures and models of communication.</p><p>In the present work (summarized in <xref ref-type="fig" rid="fig1">Figure 1</xref>), we primarily used the human structural and functional connectomes, large-scale models of dynamics, and a game-theoretical perspective of signaling to address three questions: First, how does the communication landscape look like in the state of <bold>optimal signal propagation (OSP)</bold>, where nodes maximize their influence on each other given the structural and dynamical constraints? Second, which model of information propagation aligns best with the data-driven formulation of signaling? And third, which brain regions are the most influential ones and why?</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the present work.</title><p>We used the human connectome and a linear model of local node dynamics as the main network model. However, other computational models can be used, building upon different assumptions of local dynamics and other network architectures. Here, we explored Macaque and mouse connectomes, as well as nonlinear models including a neural mass model of local dynamics. These two families of variables, network structure and local dynamics, define our ‘game constraints,’ since they dictate how information flows in the network and how nodes respond to incoming signals. For every game, i.e., a computational model such as a linear model of dynamics on the human connectome, the multi-perturbation Shapley value analysis (MSA) approach (<xref ref-type="fig" rid="fig2">Figure 2</xref>) uncovered the ‘optimal influence’ landscape of the network. This landscape describes a game-theoretical equilibrium point at which nodes cannot unilaterally increase their influence on each other any further. We compared the optimal influence profile of nodes at this point with various communication models and analyzed the most influential nodes in such a signal propagation regime. SPE: Shortest path efficiency, NE: Navigation efficiency, SI: Search information.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig1-v1.tif"/></fig><p>To answer the first question, we used <bold>multi-perturbation Shapley value analysis (MSA)</bold> and approximately 7000 million combinations of virtual lesions to precisely quantify the amount of influence exerted by each brain region (node) over every other node. We call this communication landscape the ‘<bold>Optimal Influence (OI)</bold>.’ To answer the second question, we systematically compared putative models of neural communication against the derived OI and found <bold>Communicability (CO)</bold> (<xref ref-type="bibr" rid="bib36">Estrada and Hatano, 2008</xref>) to capture a substantial variation observed in OI. CO is a non-conservative cascade model that posits regions to broadcast their signals across multiple parallel pathways. However, our results indicate that CO tends to underestimate the actual influence exerted via longer pathways. Thus, we proposed complementary analytical models that better replicate the OI. Further investigations using a nonlinear model of dynamics and a <bold>neural mass model (NMM)</bold> allowed us to determine which of the two-game constraints more strongly shape neural signaling, the global network architecture or local dynamics. We found that all models of local dynamics yielded effectively the same landscape, suggesting that network topology is a more important constraint than specific models of local dynamics. Finally, to address the third question, we revealed that areas located on the medial surface of the brain, including the precuneus, anterior cingulate cortex, and superior prefrontal cortex, exerted the strongest influence across the brain network. Notably, these regions are components of the cortical rich-club organization, which is hypothesized to act as the backbone of large-scale brain communication. Together, our results suggest that signaling in the human brain is best captured by broadcasting-like dynamics, and to optimize their influence, hub regions propagate information over multiple pathways.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Characterizing the state of optimal signal propagation</title><p>To elucidate the concept of OSP, we start with an intuitive example. Consider a scenario where an orchestra attempts to distribute funds raised from a performance. A proposal would be to divide the funds equally among all members, a strategy that leads to the ‘egalitarian value’ by which every musician is paid the same amount (<xref ref-type="bibr" rid="bib4">Algaba et al., 2019b</xref>). However, considering discrepancies in the effort of individual players — ranging from ensemble players to soloists and a conductor — makes this strategy <italic>unfair</italic> for those who contributed more. Assuming that everyone is set to maximize their share, the egalitarian proposal is rejected, and counterproposals follow until the <italic>optimally fair solution</italic> is found that is proportional to the invested contribution of every player. One can see this iterative process as traversing a convex solution space with a fixed point, representing the optimal solution. Any other solution is either incorrect, e.g., the solution results in shares that add up to a different value rather than the raised fund, or suboptimal, meaning that a better solution exists which, given enough time, will be selected eventually. Note that, by definition, there cannot be two equally optimal solutions into which the payoff is divided, while it is possible for multiple players to receive the same share, as with the egalitarian strategy.</p><p>This complexity mirrors the challenge we address in our study: <italic>accurately decomposing the activity profile of each target node by identifying the precise contribution of each source node</italic>. This problem and the space of all possible solutions for a network of three nodes is depicted in (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). For every target node in a network of <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi></mml:math></inline-formula> nodes, the space of all possible solutions encompasses <inline-formula><mml:math id="inf2"><mml:mi>N</mml:mi></mml:math></inline-formula> dimensions, with the set of potential solutions having an arbitrarily complex structure. Consequently, finding the one optimal solution in this space requires a systematic and extensive exploration of different decomposition solutions. To address this issue, Shapley provided an axiomatic framework that finds the optimal solution (called the Shapley value, see Discussion about its distinction from SHAP values) using information from all possible ways the players can form coalitions (<xref ref-type="bibr" rid="bib114">Shapley, 1997</xref>). This algorithm serves as an external authority that, instead of navigating the solution space using proposals and counterproposals, computes the contribution of each player by systematically removing them from the game and tracking the game outcome.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual summary of the multi-perturbation Shapley value analysis (MSA) approach.</title><p>(<bold>A</bold>) A simple toy network with two sources and one target, in which the sources have different amounts of influence on the target. (<bold>B</bold>) For this toy network, the space of all possible ways in which the activity of the target node can be decomposed into contributions from source nodes has two dimensions. There exists a subset of potential solutions where the activity of the target node is correctly decomposed, but the decomposition is not optimal. There exists one equilibrium point where the decomposition is correct and optimal. (<bold>C</bold>) For every game, that is, the simulation of a whole-brain computational model, MSA performs extensive multi-site lesioning analysis to uncover the influence of every node on every other node. For every target node, MSA lesions combinations of source nodes, track how the activity of the target node changes given each perturbation and, for all source nodes, computes the difference between two scenarios: one with the source node included in the lesion-set, and the other where the source node is not lesioned. The difference between these two cases defines the contribution of the source to that specific coalition/combination of sources. Averaged over all contributions, the time-varying contribution of each source is then inferred. Iterating over all nodes results in the ‘optimal influence’ landscape that can be decomposed into two components: Direct influences, where nodes influence their connected neighbors, and indirect influences, where nodes influence distant unconnected nodes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Comparing the optimal influence matrices from 1000 and 10,000 lesion samples per source node.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig2-figsupp1-v1.tif"/></fig></fig-group><p>To do so, we used the approach of <bold>MSA</bold> (<xref ref-type="bibr" rid="bib37">Fakhar, 2021</xref>; <xref ref-type="bibr" rid="bib64">Keinan et al., 2004a</xref>) that applies exhaustive multi-site virtual lesioning across all combinations of source nodes while tracking the resultant changes in the target node, as illustrated in (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) The approach is analogous to repeatedly performing the same musical piece while each time a subset of players is excluded, thereby discerning each individual’s contribution to the performed track (see, e.g. <xref ref-type="bibr" rid="bib42">Fakhar et al., 2024c</xref>). For instance, the contribution of a cello player to the song is easily distinguished by taking the difference between two pieces, once with the cello player included and once without. By evaluating all possible combinations of players, higher-order interactions are then accounted for (e.g. contributions of the entire string ensemble), and thus the exact contribution of each player across all possible configurations is inferred (see section <italic>Game-theoretical Framework</italic> in Materials and Methods for more details). In essence, our framework reveals the unique itemized description of each player’s contribution to the overall outcome. This detailed analysis allows for a precise division of payoffs, ensuring that players have no incentive to deviate from their allocated share (<xref ref-type="bibr" rid="bib55">Gul, 1989</xref>; <xref ref-type="bibr" rid="bib93">Pérez-Castrillo and Wettstein, 2001</xref>). The resulting game-theoretical equilibrium state, here termed the optimal signal propagation state, represents the only fair allocation of nodal influence in which no better solution can be found. Note that the ‘equilibrium’ here refers to the unique point in the solution space of the division problem and not the neural space (See Discussion). The MSA begins by defining a ‘game.’ To derive OSP, this game is formulated as a model of dynamics, such as a network of interacting nodes. These can range from abstract epidemic and excitable models (<xref ref-type="bibr" rid="bib46">Garcia et al., 2012</xref>; <xref ref-type="bibr" rid="bib80">Messé et al., 2015a</xref>) to detailed spiking neural networks (<xref ref-type="bibr" rid="bib95">Pronold et al., 2024</xref>) and to mean-field models of the whole brain dynamics, as chosen here (see below). The model should ideally be fitted to reflect real data dynamics, after which MSA systematically lesions all nodes to derive the OSP. Put together, the framework is general and model-agnostic in the sense that it accommodates a wide range of network models built on different empirical datasets, from human neuroimaging and electrophysiology to invertebrate calcium imaging and anything in between. In essence, the framework is not bounded to specific modeling paradigms, which then allows direct comparison among different models (e.g. see section Global Network Topology is More Influential Than Local Node Dynamics).</p><p>Specifically, in this work, the ‘game’ was conceptualized as a large-scale model of the brain dynamics approximating the empirical <bold>functional connectivity (FC)</bold> by associating a differential equation to each node of the given <bold>structural connectivity (SC)</bold>. By utilizing the consensus SC and averaged FC data from 70 healthy young individuals (<xref ref-type="bibr" rid="bib52">Griffa et al., 2019</xref>; <xref ref-type="bibr" rid="bib112">Shafiei et al., 2019</xref>), we calibrated a linearized Wilson-Cowan model, as detailed in the ‘Large-scale Computational Models of the Brain Dynamics’ section under Materials and methods. Then, MSA was used to perform systematic in-silico lesioning of combinations of source nodes while tracking the altered activity of a target node. This iterative process over all nodes elucidates the comprehensive influence landscape of every node on every other node within the network (illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>). This landscape depicts the extent of influence one node exerts over another under the state of OSP.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> reveals that brain regions exert influence not only on their directly connected neighbors, but also on distant and unconnected regions. This observation suggests that optimal communication within the brain requires the propagation of signals beyond immediate connections, traversing through intermediate regions to form multi-step processing pathways. This notion is supported by evidence of significant functional coupling between spatially distant, unconnected regions (<xref ref-type="bibr" rid="bib118">Suárez et al., 2020</xref>). We then reorganized the OI matrix according to established large-scale functional brain modules, i.e., the resting-state networks, as depicted in <xref ref-type="fig" rid="fig3">Figure 3</xref>. We calculated two key metrics: firstly, the average intra- and inter-module influence, which quantifies the normalized aggregate influence that regions belonging to a module exert both internally and on other modules. Second, we assessed the heterogeneity of these influences to determine the diversity of communication patterns within and between modules. A low heterogeneity indicates a narrow distribution of normalized total influence among nodes, measured as the standard deviation, whereas high heterogeneity suggests a wide variation in interaction strengths. We then asked <italic>whether any modules exhibited deviations from the expected levels of influence and heterogeneity</italic>. We derived the expected level via the egalitarian strategy where influence is uniformly distributed across all modules, corresponding to an equal division of 100% of the flow among the nine modules. Our analysis revealed a generally balanced communication landscape, with the largest deviations being approximately 3%. However, the default mode network stood out, demonstrating both substantial average intra- and inter-module influence and lower heterogeneity (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This finding implies that nodes in the default mode network have a large and relatively similar amount of influence over each other and the rest of the brain. In contrast, the auditory network is characterized by a more varied and modest level of influence, predominantly within its own module, but extending to others as well. Together, the results presented in <xref ref-type="fig" rid="fig3">Figure 3</xref> suggest that while there is a relatively homogeneous level of influence within and between different cognitive modules, the default mode network distinguishes itself as a key orchestrator of brain-wide communication dynamics.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Large-scale network organization of the optimal influence landscape.</title><p>Optimal influences were reorganized according to nine large-scale functional network modules and characterized by two metrics: 1. Average influence, which captures the normalized total influence of a given module within itself and to other modules. 2. Heterogeneity of influences, accounting for the normalized variation of influences within a functional module and to other modules. The egalitarian value represents a naive strategy for assigning contributions in which all players are assumed to contribute equally, i.e., by equal division of the total influence among all nine functional modules. The bars represent how much each module deviates positively or negatively from the egalitarian assumption.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig3-v1.tif"/></fig><p>Collectively, our findings in this section indicate that manipulating a single region can impact the global dynamics of the entire brain (also as shown by <xref ref-type="bibr" rid="bib51">Grayson et al., 2016</xref>; <xref ref-type="bibr" rid="bib96">Rabuffo et al., 2023</xref>; <xref ref-type="bibr" rid="bib128">Young et al., 2000</xref>), extending beyond anatomically connected regions. As a result, a critical question arises: <italic>Which poly-synaptic signaling conceptualization most accurately encapsulates these observed indirect influences?</italic></p></sec><sec id="s2-2"><title>Optimal signaling via broadcasting over parallel pathways</title><p>To determine which model of signal propagation most accurately captures the indirect influence exerted by brain regions on one another, we explored a spectrum of communication models, spanning from routing strategies to diffusive processes (<xref ref-type="bibr" rid="bib110">Seguin et al., 2023</xref>; <xref ref-type="bibr" rid="bib107">Seguin et al., 2020</xref>). Routing strategies postulate that information travels predominantly along, or with respect to the shortest path. As introduced before, SPE is straightforward in its approach, focusing on the shortest path alone. <bold>Navigation Efficiency (NE)</bold>, on the other hand, is a geometrically greedy routing model, aiming to minimize the physical distance to a target node at each step. <bold>Search Information (SI)</bold> quantifies the amount of information required for a random walker to find and navigate along the shortest path. Therefore, the more the required information, the more difficult the communication between two nodes is assumed to be. Conversely, at the opposite end of the spectrum lie diffusion processes that envision information cascades throughout the network. This could involve longer, even recurrent routes to the same nodes. The efficiency of signaling under these processes is then quantified by CO and DE. CO adopts a broadcasting strategy, whereby information spreads from the source and reaches the target through multiple pathways, while being subjected to exponential decay with each step. This decay inherently reduces the impact of longer pathways. DE, in contrast, does not incorporate attenuation, focusing instead on counting the steps a random walker takes from source to target. For detailed descriptions of these models, refer to the ‘Communication models’ section under Materials and methods.</p><p>A simple pairwise Pearson’s correlation between OI and each CM matrix showed a large positive correlation between OI and CO (R<sup>2</sup>=0.88; all p-values in this work are strictly smaller than 0.0001 unless mentioned otherwise) and a large negative relationship with SI (R<sup>2</sup>=0.68) while other metrics such as SPE, NE, and DE exhibited nonlinear relationships with OI. This finding (<xref ref-type="fig" rid="fig4">Figure 4</xref>) suggests that, although all conceptualizations capture a degree of OI, the one assuming an attenuating diffusion dynamic for the signal propagation over multiple pathways most effectively mirrors OI. This result aligns with existing studies indicating CO’s efficacy in capturing the spread of electrical stimulation to unconnected nodes (<xref ref-type="bibr" rid="bib108">Seguin et al., 2022a</xref>), predicting large-scale functional modules more accurately than SC (<xref ref-type="bibr" rid="bib109">Seguin et al., 2022b</xref>), and determining the degree of compensation by other regions following perturbation (<xref ref-type="bibr" rid="bib13">Betzel et al., 2016b</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Comparison of optimal influences with other putative communication measures.</title><p>Pairwise Pearson’s correlation between each communication measure and optimal influences, to characterize the alignment with existing communication models. Communicability (<italic>r</italic>=0.94) and Search Information (<italic>r</italic>=−0.83) had the best alignment in this univariate setting.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comparing optimal influence with functional connectivity.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig4-figsupp1-v1.tif"/></fig></fig-group><p>However, we noticed that CO slightly underestimates the true influence of longer pathways. To address this limitation, three alternative approaches were explored: adjusting communicability with a scaling factor (<bold>scaled communicability; scaled CO</bold>) (<xref ref-type="bibr" rid="bib131">Zamora-López et al., 2016</xref>), using a linear instead of exponential attenuation (<bold>Linear Attenuation Model; LAM</bold>), and employing the covariance structure of a <bold>spatial autoregressive model (SAR)</bold> that considers linear attenuation and regional co-fluctuation. Each of these models incorporates a single adjustable parameter: a scaling factor for scaled CO, an attenuation factor for LAM, and a degree of spatial influence for SAR. After fitting these parameters to replicate OI, we found that, except for SAR, the other two models failed to rectify the issue (as shown in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). While scaled CO and LAM showed marginal improvements in variance explained (R²=0.89 and 0.9, respectively) compared to CO (R<sup>2</sup>=0.88), SAR achieved a near-perfect performance (R²=0.997). We then fitted SAR to CO instead of OI to evaluate to which extent CO captures the real degree of spatial influence. Varying the SAR parameter indicated that CO’s positioning on the spectrum is considerably skewed towards a steep attenuation (0.06), in contrast to OI’s more moderate rate of spatial decay (0.43). This finding supports our intuition that CO is overly strict with discounting longer pathways, since it assumes the signal to fade almost seven times faster than it actually does. It also indicates that a wide range of discount factors between 0.2 and 0.7 performs similarly well (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), relaxing the need to search for an exact value (same applies to the scaling factor of CO and attenuation factor of LAM; see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Strategies to alleviate the underestimated influence of long-range pathways.</title><p>(<bold>A</bold>) Difference between adjacency matrices of tunable models and OI that indicates where these models underestimate (red) and overestimate (blue) the influence of nodes on each other. (<bold>B</bold>) SAR obtains a correlation coefficient of <italic>r</italic>=0.99 by increasing the depth by which signals can travel, however, the plot on the bottom shows that a wide range of values from 0.2 to 0.7 can be chosen with a small degradation of fit. SAR: Spatial Auto-Regressive model, CO: Communicability, OI: Optimal Influence.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Fitting curve of the scaled communicability (left) and the linear attenuation model (right) to the optimal influence matrix of the human connectome.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Fitting curves of the spatial autoregressive model (top), scaled communicability (middle) and the linear attenuation model (bottom) to the optimal influence matrix of macaque (left) and mouse (right) connectomes.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig5-figsupp2-v1.tif"/></fig></fig-group><p>We subsequently explored the depth of influence in terms of the number of hops, specifically addressing how far a signal travels before its causal impact substantially diminishes. To this end, we used a linear regression model, fitted it to results from progressively diffused signals in the network. Put simply, we iteratively predicted the OI starting from walks of zero length (no spread of influence) and incrementally considered longer walks, applying a monotonic discount to additional steps. The analysis, as depicted in (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), demonstrated peak performance at the sixth step, achieving an R² of 0.75. This means that the summation of only the first six steps captures 75% of the variance in OI (also see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for the result from an exponential discount instead of linear). Furthermore, a multivariate Lasso regularized linear regression model was trained on all walks simultaneously to identify a parsimonious combination that most effectively predicts OI (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Aligning with the result from the univariate model, walks of length five, six, and the eighth steps (excluding seventh) were contributing the most, allowing the model to predict OI with a high degree of accuracy (R²=0.96). These results imply that signal influence notably decreases after approximately eight processing steps, despite the network’s diameter being around ten steps, meaning that the information flow between the most distant nodes is heavily degraded (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). While information from the initial eight steps is sufficient to explain 96% of the OI, the superior predictive performance of models such as SAR, LAM, and CO suggests that incorporating all possible paths yields additional, albeit small, information. Moreover, the real advantage of communication models lies in their relative simplicity compared to using a regularized multivariate model. Altogether, it is important to note that the longer processing pathways that are missed by CO, Scaled CO, and LAM still influence the target node, but based on comparing how much SAR gained in predictive performance, they account for about 5% of the whole communication dynamics. This finding provides a rough estimate of how much mismatch, and in which pathways, are expected when using CO, scaled CO, and LAM in human neuroimaging data. However, when these communication models were applied to mouse and macaque connectomes, a discrepancy in capturing OI in directed networks emerged (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). SAR marginally outperformed LAM and scaled CO in the mouse connectome (<italic>r</italic>=0.21, 0.18, 0.17, respectively) but showed significantly better performance in the macaque (<italic>r</italic>=0.92, 0.24, 0.23, respectively). The enhanced performance from mouse to macaque hints at the potential influence of network size, considering the macaque network comprises 29 nodes compared to the mouse’s 112. Nonetheless, the specific effects of network size, density, reciprocity, weight distribution, and normalization, as well as other global characteristics on modeling OI necessitate further systematic investigation using synthetic network models. This finding delineates the limitations of current CMs in accurately depicting optimal signal propagation in large, directed networks. However, the observed inverted U-shaped fitting curves of LAM and scaled CO (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) suggest a slower spatial decay in signal propagation than assumed by models like CO. In essence, despite the struggle of these models to precisely capture the optimal signal flow in larger directed networks, there is a consensus that the influence of regions extends further than traditionally postulated by communication models.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Uni-and-multivariate models of OI.</title><p>(<bold>A</bold>) Predictive power of a linear regression model trained on walks up to 15 broadcasting steps. (<bold>B</bold>) A regularized multivariate model trained on all steps shows a small set of steps to be sufficient to explain 96% of OI on average. Each dot represents one cross-validation trial (N=100). (<bold>C</bold>) Sets of regularized multivariate models were trained on network features at each degree of spatial influence for spatial autoregressive model (SAR) and linear attenuation model (LAM) to capture the impact of this parameter on the performance of the statistical model. The dashed line on the left shows a scenario when the degree of spatial influence is zero, effectively rendering SAR an identity matrix. The second dashed line indicates where the model performed the best, explaining 99% OI. (<bold>D</bold>) Bars represent the contribution of each feature to the model, averaged over degrees of influence. Error bars indicate the 95% confidence interval. OI: Optimal influence.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>The amount of which univariate and multivariate models can predict optimal influence from summing individual walks, discounted exponentially (as with communicability).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig6-figsupp1-v1.tif"/></fig></fig-group><p>In our final analysis of this section, we sought to determine if other communication models, despite their complex relationship with OI, could surpass CO in predicting it. Additionally, it is possible for more intuitive variables, such as the fiber length or Euclidean distance, to provide better predictive performance. To see if OI is better predicted using available variables, including CMs, another multivariate regularized linear regression model was employed (illustrated in <xref ref-type="fig" rid="fig6">Figure 6C</xref>). Excluding SAR, the model successfully explained 93% of the variance in OI using just three variables: LAM, CO, and SPE, with LAM making the most significant contribution. We then trained a set of models with the same variables but different attenuation factors of LAM and SAR to observe the dynamics of feature importance across different factors. To our surprise, at its optimal parameter value, SAR emerged as the predominant predictor, effectively simplifying the statistical model to a univariate format and once again perfectly predicting OI (as seen in <xref ref-type="fig" rid="fig6">Figure 6C</xref>). SAR maintained its dominance as the most critical variable over a range from 0.1 to 0.8, after which CO became more influential. This shift underscores the role of discounting the signal, elucidating why DE was the least effective model in our study. The reason is that, at around <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> where no discount is considered, SAR converges to DE, rendering it uninformative, whereas LAM retains its distinctiveness, surpassing both CO and SAR. Averaged over all values of <inline-formula><mml:math id="inf4"><mml:mi mathvariant="normal">α</mml:mi></mml:math></inline-formula> shows that SAR was the most important feature, followed by CO and LAM (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) which all follow broadcasting-like communication dynamics .</p><p>Collectively, our findings indicate that optimal influence in brain networks requires signal propagation across multiple pathways, akin to a broadcasting approach. The influence diminishes with each processing step, but not as quickly as it is assumed by CO. Moreover, a simple statistical model, i.e., SAR, accurately predicts the network’s influence structure at the optimal signal propagation regime. Importantly, previous studies indicated that SAR performs as well as many other biophysically detailed NMMs in predicting the FC (<xref ref-type="bibr" rid="bib79">Messé et al., 2014</xref>; <xref ref-type="bibr" rid="bib81">Messé et al., 2015b</xref>). Interestingly, comparing the performance of multivariate models against simple CMs suggests that little is gained from additional variables and more computationally expensive predictive models.</p><p>It is important to acknowledge that all CMs, including SAR, are predicated on a linear dynamic model, prioritizing simplicity over biophysical complexity. However, brain regions are argued to perform nonlinear operations, show oscillatory patterns of activity and show metastability by switching between multiple states to support cognition and behavior. Therefore, to affirm the notion of optimal communication in brain networks, it is essential to compare these findings with more realistic dynamics that capture such complex phenomena.</p></sec><sec id="s2-3"><title>Global network topology is more influential than local node dynamics</title><p>Previous research has demonstrated that linear models of local dynamics are as effective, if not more so, as nonlinear models in capturing the macroscopic dynamics of the brain (<xref ref-type="bibr" rid="bib1">Abdelnour et al., 2014</xref>; <xref ref-type="bibr" rid="bib82">Messé et al., 2023</xref>; <xref ref-type="bibr" rid="bib79">Messé et al., 2014</xref>; <xref ref-type="bibr" rid="bib86">Nozari et al., 2024</xref>). This observation led us to hypothesize that optimal signal propagation in large-scale brain networks, as measured by fMRI-BOLD signal, might also be adequately represented by linear communication models. In other words, we hypothesized that <italic>optimal signal propagation should follow the same line of reasoning and be relatively consistent across different dynamical models</italic>. The equilibrium state in decomposing contributions should remain consistent across different dynamical models, irrespective of their complexity. This expectation would hold true unless the local dynamics — how regions process incoming signals — impose substantial constraints on information flow within the network.</p><p>To evaluate this hypothesis, we incorporated two additional models into our analysis: one using a nonlinear <bold>tangent hyperbolic (Tanh)</bold> transfer function, contrasting it with the linear function used in the linear model, and the other one a well-established oscillatory NMM known as the Hopf/Stuart-Landau model, operating in its critical metastable regime (<xref ref-type="bibr" rid="bib31">Deco et al., 2017</xref>). Pearson’s correlation conducted to compare these models with the linear model confirmed our hypothesis. As depicted in (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), both the nonlinear and Hopf models showed high correlations with the linear model (R<sup>2</sup>=0.98 and 0.94, respectively). Furthermore, we investigated the influence of local dynamics on OI by contrasting a model fitted to FC with one that was not, hence producing dynamics that poorly resemble the FC. The resulting scatter plot of OI for these models, as shown in (<xref ref-type="fig" rid="fig7">Figure 7B</xref>), indicated another strong correlation (R<sup>2</sup>=0.93). Collectively, these findings suggest that the equilibrium state of optimal signal propagation does not substantially vary even when employing more complex dynamical models. The reason is that, as previously found <xref ref-type="bibr" rid="bib42">Fakhar et al., 2024c</xref>, while nonlinear transformations significantly impact the <italic>time-varying</italic> structure of individual contributions, they do not alter the overall amplitude of them, which in this context, is viewed as the <italic>total</italic> amount of influence one node exerts over another. Scaling, on the other hand, that determines the magnitude of these contributions, is inherently governed by the structural connectome across all models. Consequently, <italic>the total influence</italic> of one node on another is predominantly dictated by network topology rather than the specific dynamical model employed. However, <italic>the time-resolved</italic> pattern of fluctuations is specified by the model, which here is simply neglected as a direct consequence of collapsing the temporal information (refer to <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for more details).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Correlations among linear, nonlinear, and neural mass models of local dynamics.</title><p>(<bold>A</bold>) OI matrices for a linear and a nonlinear model show a near-perfect correlation. Moreover, a Hopf model also has a strong correlation with the linear model, indicating that a linear model of local dynamics represents the communication in brain networks under optimal influence (OI) as well as more complex node models (<bold>B</bold>) Scatter plot between two settings of the same linear model, one fitted to resting-state fMRI and the other using a random value for the global coupling parameter. The finding provides evidence for the greater role of network’s structure compared to nodal dynamics in shaping brain-wide optimal influence.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Temporal characteristics of optimal influence, given the Hopf model.</title><p>The part above shows how the amplitude of influence between two regions relates to the structural weight and the topological distance between them. The heatmaps in the middle represent the brain-wide incoming and outgoing influence on and from the posterior cingulate cortex (PCC). Together with the column-averaged time course of these influences, the middle plot suggests that PCC receives a diverse range of incoming influences but broadcasts the same message to the rest of the network. This is also apparent from the zoomed-in view of the heatmap at the bottom.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig7-figsupp1-v1.tif"/></fig></fig-group><p>In summary, our findings indicate that linear models of dynamics effectively capture the degree of influence regions exert on each other under the optimal signal propagation state. Findings from this section not only corroborate previous research suggesting that macroscopic dynamics of large-scale brain networks are well approximated by linear models, but also support the hypothesis posited in the preceding section. Together, these two aspects suggest that it is the structural connectivity that shapes the landscape of optimal signaling in brain networks, rather than the models of local dynamics, since OIs are remarkably similar irrespective of the biophysical complexity embedded in models of local dynamics. The equilibrium reached in this game-theoretical framework, as demonstrated, is accurately represented by simplified abstract models of signal propagation. Thus, not only do simple communication models such as SAR perform on a par with NMMs in predicting empirical FC, but also, they represent how information optimally flows in large-scale brain networks.</p></sec><sec id="s2-4"><title>Rich-clubs harness parallel pathways to amplify their influence</title><p>It has been shown that a set of cortical hubs exerts a relatively large wiring cost to link with other distant regions, forming a rich-club organization (<xref ref-type="bibr" rid="bib123">van den Heuvel and Sporns, 2011</xref>; <xref ref-type="bibr" rid="bib130">Zamora-López et al., 2011</xref>; <xref ref-type="bibr" rid="bib129">Zamora-López et al., 2010</xref>). Thus, these topologically central nodes are argued to be the backbone of information transfer across multiple distant specialized modules (<xref ref-type="bibr" rid="bib59">Harriger et al., 2012</xref>; <xref ref-type="bibr" rid="bib124">van den Heuvel et al., 2012</xref>). Previous works proposed that these connections play a crucial role in information processing by diversifying areal input-output connectivity (<xref ref-type="bibr" rid="bib14">Betzel and Bassett, 2018</xref>), leading to a richer functional repertoire (<xref ref-type="bibr" rid="bib131">Zamora-López et al., 2016</xref>), or integrating information from the whole brain (<xref ref-type="bibr" rid="bib49">Goulas et al., 2015</xref>). Moreover, it was shown that the connections among adjacent regions are stronger than those among distant areas (<xref ref-type="bibr" rid="bib16">Beul et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Lynn et al., 2024</xref>). Empirical findings and modeling work have suggested an exponential distant rule by which the strength of connections decays exponentially with spatial distance (<xref ref-type="bibr" rid="bib35">Ercsey-Ravasz et al., 2013</xref>; <xref ref-type="bibr" rid="bib77">Markov et al., 2013</xref>). For instance, in our human connectome dataset, the correlation between structural weights and fiber lengths is –0.6 (See <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). Together, these findings pose a question, <italic>if and how do rich-club regions keep their line of communication optimal given these relatively weak connection strengths of long-distance projections?</italic></p><p>To answer this question, we first investigated the relationship between the strength of connections between pairs of connected regions and the respective influence they assert on each other. Intuitively, the stronger a connection is, the greater its influence, leading to stronger communication between two regions. This relation has also been supported experimentally by comparing the effect of optogenetics stimulation of a region over its connected neighbors (<xref ref-type="bibr" rid="bib67">Kim et al., 2023</xref>). Our result corroborated this finding and showed a strong correlation between the structural weight and the amount of influence nodes assert on their connected neighbors (<italic>r</italic>=0.95). However, several pair-wise interactions fall well above the regression line (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), suggesting that some regions assert strong influence on others despite their weak connection weights (likewise noticeable in both mouse and macaque connectomes; <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2A</xref>). This ‘bump’ is also captured by SAR, LAM, and CO (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2B</xref>), providing further evidence that these simple models can reproduce optimal information flow among connected regions. As (<xref ref-type="fig" rid="fig8">Figure 8B, C and D</xref>) shows, these connections mostly lie between regions located at the medial surface of the cortex, coinciding with rich-club regions (<xref ref-type="bibr" rid="bib122">van den Heuvel et al., 2010</xref>; <xref ref-type="bibr" rid="bib123">van den Heuvel and Sporns, 2011</xref>). We then computed the amount that each node influences other <italic>unconnected</italic> regions (<xref ref-type="fig" rid="fig8">Figure 8F</xref>) and found the same regions to be the most influential, providing more evidence that this influence is not related to the weak link itself, but the node’s overall connectivity. To test this hypothesis, we first computed several graph centrality measures, built both on the shortest-path and random walk signaling conceptualization. A Spearman’s rank correlation between how influential a node is and how central it is suggests two conclusions (<xref ref-type="fig" rid="fig8">Figure 8E</xref>): First, all centrality measures show a moderate to strong positive correlation with the amount of influence nodes assert over the rest of the network, directly and indirectly. Second, those with the largest <italic>indirect</italic> influence (indicated with red) always lie on the top-right of the data cloud, supporting the idea that they are topologically central. We further utilized null and synthetic networks to see if we can delineate the role of connection strength and connectivity pattern in signal propagation. Although less noticeable, the trend persisted when the network topology was shuffled while nodal strength was preserved. It also persisted when the weights were shuffled while the topology was held constant. The trend only disappeared when both topology <italic>and</italic> weights were allocated randomly in synthetic random networks (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2C, D, and E</xref>).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Hubs, rich-club nodes, and their influence.</title><p>(<bold>A</bold>) Scatter plot of the influences of nodes on their connected neighbors (direct influence in <xref ref-type="fig" rid="fig2">Figure 2</xref>) versus the strength of the underlying connections. Despite the overall strong correlation between the total influence and the strength of structural connections, residuals (depicted by warm colors) reveal several weak connections with more influence than expected by their connection weights. (<bold>B</bold>) The position of weak connections with strong influence in the adjacency matrix. (<bold>C</bold>) The position of these connections in the structural brain network shows that they are mainly long-range connections among the hub regions of the cortex. (<bold>D</bold>) shows those hub regions. (<bold>E</bold>) The relationship between the influence of a region and its centrality, by four centrality measures (see Material and Methods, Graph-theoretical Measures). (<bold>F</bold>) The most indirectly influential nodes (indicated by red points in E) that can propagate information to distant and unconnected regions. DMPFC: Dorsomedial prefrontal cortex, PCC: Posterior cingulate cortex, ACC: Anterior cingulate cortex, SFC: Superior frontal cortex, SMA: Supplementary motor area, VLPFC: Ventrolateral prefrontal cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>The relationship between the log-normalized structural weights and the fiber length, which had a correlation of –0.6, suggesting that the longer the fiber, the weaker its strength is.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig8-figsupp1-v1.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Further analysis of the influence of weak connections in null, synthetic, and supplementary networks.</title><p>(<bold>A</bold>) The relationship between structural weights and the influence of two connected regions in the mouse and macaque connectome supports the results from the human connectome. There exists many weak connections that exert more influence than expected from their connection weights (the ‘bump’) (<bold>B</bold>) Communication models capture the bump to different extent. (<bold>C</bold>) The bump is also apparent in the strength-preserved null model, although localized in weaker connections. (<bold>D</bold>) Two synthetic networks in which both the weights and the topology is randomized did not replicate the bump. (<bold>E</bold>) Topology-preserved weight-shuffled networks replicated the bump, however, as with <bold>C</bold> they are localized to weak connections, suggesting a complementary role for both weight distribution and topological features.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig8-figsupp2-v1.tif"/></fig></fig-group><p>We then compared the edge betweenness centrality of every connection with its strength, under both shortest-path and random walk conceptualization. <bold>Shortest-path edge-betweenness centrality (SPEBC)</bold> of a link, measures how many shortest paths in the network go through this specific connection, while <bold>random walk edge-betweenness centrality (RWEBC)</bold> counts the number of times a random walker traversed this link to move from a node to another. While the trend is missing when compared to the connection weight, it is apparent when compared to the influence, suggesting that the effect cannot be captured by how central <italic>individual links</italic> are given these two signaling strategies (<xref ref-type="fig" rid="fig9">Figure 9A and B</xref>). Together, these findings propose that neither connectivity nor weights <italic>alone</italic> are responsible for the emergence of efficient signal propagation between weakly connected nodes. But the signature of more influence compared to the underlying weight is only observed when central nodes are allocated with weak links.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Relationship between the centrality of individual connections and their influence.</title><p>(<bold>A</bold>) Strength of individual connections versus their centrality in terms of shortest-path edge betweenness centrality. The lower panel shows the same relationship, but for the amount of influence a node has over its connected neighbor compared to the centrality of their connections. (<bold>B</bold>) Depicts the same information for the random walk (diffusive) model of information flow. In both A and B, this relationship is compared against a null model in which the structural connectivity is shuffled while preserving the strength of each node, i.e., the weighted sum of its connections.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>The relationship between influential nodes (in red) and their controllability measures.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig9-figsupp1-v1.tif"/></fig></fig-group><p>Lastly, to further investigate this phenomenon, we systematically changed the degree of spatial influence for a SAR model while calculating the spread of data along the second <bold>principal component (PC)</bold>. The first PC captures the main direction of variability between the influence and connection strength, which is the observed strong relationship. However, the second PC captures how much data points are spread orthogonal to the main axis, indicating the amount of which weak links are influential. In other words, this metric measures the variability around the regression line, a small variability indicates a robust prediction of influence from the connection weight, showing their tight coupling. In contrast, large variability approximates how much communication between two nodes was independent of the connection strength, potentially relying on other parallel pathways (<xref ref-type="fig" rid="fig10">Figure 10</xref>). We found that a steep decay factor leads to low variability, as nodes could only influence their connected neighbors before the signal subsides. A shallower decay rate, on the other hand, resulted in more variability since the signal could traverse longer parallel pathways, echoing in the network and reach the target from multiple fronts (<xref ref-type="fig" rid="fig10">Figure 10</xref>). This finding provides further evidence for central nodes utilizing not only the direct path but other longer paths to compensate for the weak direct connection strength.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>How influence decouples from the underlying structural connections.</title><p>The spread of points along the second principal component, i.e., orthogonal to the regression line, across different values of spatial influence. A very small degree of spatial influence represents a case in which the signal can only reach the neighboring nodes, while higher values indicate influences that extend over longer pathways and reverberate in the network, effectively decoupled from the underlying structural weight.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-101780-fig10-v1.tif"/></fig><p>Moreover, we found that these nodes also have both larger average controllability and modal controllability (see <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>), providing insight about their potential role in steering the dynamics of the brain. Average controllability measures the ability of a node to push the global dynamics into easy-to-reach states, while the modal controllability measures the power of a node to push the network towards farther and hard-to-reach states. It has been shown that the nodal strength has a strong positive correlation with average controllability and a strong negative correlation with modal controllability (<xref ref-type="bibr" rid="bib54">Gu et al., 2015</xref>). Our results also support this finding, with the Spearman’s rank correlation of nodal influence and average controllability being 0.86 and –0.63 for modal controllability. However, the influential nodes are clustered in the top-right corner of the scatter plot for modal controllability (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). This finding suggests that these nodes steer the network to both easy-to-reach and hard-to-reach dynamical states due to their extensive local and long-range connectivity.</p><p>Altogether, results from this section propose a mechanism with which hub regions optimally communicate with their distant counterparts, given the structurally weak connections between them. These regions harness their topologically central positions to broadcast their signals not only via one (their direct) connection but other parallel indirect ones to direct the global dynamics of the brain towards different states, even those that are energetically costly to reach. In other words, the weak connection between two ‘peripheral’ nodes results in weak communication, though, this inefficiency is compensated by central nodes that have access to many pathways to amplify their signal.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we explored the characteristics of brain communication in a state of optimal signal propagation. We employed a game-theoretical framework to uncover how nodes influence each other optimally, given a set of constraints. We defined these constraints as the network’s structural topology and its functional activity, which emerged from three increasingly complex large-scale brain models. These models ranged from a linear model, where the output of each node is the weighted sum of its inputs, via a nonlinear model, in which the node modifies the incoming signals using a Tanh function, to a Hopf model that, relative to the other two models, represents more biophysical realism, such as conductance delay, metastability, and oscillatory dynamics of the brain signals, at the cost of greater computational complexity. Our results indicate that the strength by which nodes influence each other depends less on the local node model and more on the global network structure, permitting us to apply a wide range of graph theoretical metrics that fundamentally relate to the linear model. For instance, comparing putative communication models, we found that the optimal signal propagation in brain networks is better captured by frameworks that conceptualize communication to follow a broadcasting strategy. This means that, although signals travel along multiple parallel pathways, they subside after each processing step and degrade after six to eight steps. Finally, we show that topologically central brain regions, such as precuneus, prefrontal cortex, and anterior cingulate cortex, amplify their influence over remote regions by harnessing such parallel pathways. These results have several implications and caveats that are discussed below.</p><sec id="s3-1"><title>Interpreting game-theoretical solutions for influence decomposition</title><p>Here, we argue for developing normative formulations of brain signaling and network organization. This approach follows similar studies in synthetic brain networks, where a work also using game theory showed how nodes aiming at maximizing the network’s navigability while minimizing their wiring cost organize in structures similar to empirical brain networks (<xref ref-type="bibr" rid="bib56">Gulyás et al., 2015</xref>). Interestingly, related work (<xref ref-type="bibr" rid="bib104">Samoylenko et al., 2023</xref>) addressed the long-standing question of why six degrees of separation exist in social networks (<xref ref-type="bibr" rid="bib120">Travers and Milgram, 1977</xref>). Using normative modeling provided by game theory, the authors found the reason to be a trade-off between a node’s aspiration to maximize its centrality, while minimizing its connection maintenance cost (<xref ref-type="bibr" rid="bib104">Samoylenko et al., 2023</xref>). In the present work, we also found that the first eight steps of propagation are sufficient to reconstruct OI, with a peak on the sixth step.</p><p>As noted in the introduction, OI is model-agnostic, here, we leveraged this liberty to compare signaling under different models of local dynamics, primarily built upon undirected human connectome. We also included different modalities, e.g., tract tracing in Macaque (see Structural and functional connectomes under Materials and methods) to confirm the influence of weak connections is not inflated due to imaging limitations (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2A</xref>). The game theoretical formulation of signaling allows for systematic comparison among many combinations of modeling choices and data sources. Nonetheless, in this work, we assumed full observability, i.e., complete empirical knowledge of brain structure and function that is not necessarily practically given. Although a detailed investigation of this issue is needed, mathematical principles behind the method suggest that the framework can isolate the unobserved influences. In these cases, activity of the target node is decomposed such that the influence from the observed sources is precisely mapped, while the unobserved influences form an extra term, capturing anything that is left unaccounted for, see <xref ref-type="bibr" rid="bib3">Algaba et al., 2019a</xref>; <xref ref-type="bibr" rid="bib42">Fakhar et al., 2024c</xref> for more technical details.</p><p>More on the mathematical principles, the Shapley value has been shown to converge to a Nash equilibrium in some games, for instance, bargaining in which players have to divide a sum among themselves while maximizing their payoffs, allowing us to draw a connection between this value and the notion of the equilibrium state (<xref ref-type="bibr" rid="bib55">Gul, 1989</xref>). interestingly, this finding has been experimentally replicated, where subjects played a simplified bidding game and frequently settled on division solutions very similar to their Shapley values (<xref ref-type="bibr" rid="bib28">Chessa et al., 2022</xref>). Thus, it is important to note that the equilibrium discussed in the current paper is a game theoretical equilibrium in the solution space (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and not necessarily associated with the neural dynamics. Related to this point is the notion of optimality. Here, by optimal influence, we refer to <italic>the optimal solution for decomposing influences, from the cooperative game theoretical perspective, that translates to the non-cooperative game-theoretical definition of optimality as the largest amount of influence that a node can potentially assert on a target given the game constraints</italic>. As depicted in (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), optimality here refers to the equilibrium point in the solution space. We call it optimal because, as shown before (<xref ref-type="bibr" rid="bib114">Shapley, 1997</xref>), the Shapley value is a unique solution, and in fact the only solution that satisfies three of the four axioms related to fairness (<xref ref-type="bibr" rid="bib4">Algaba et al., 2019b</xref>; <xref ref-type="bibr" rid="bib98">Roth, 1988</xref>) which guarantees stability and dominance of the solution compared to others. Lastly, the Shapley value is being used in the context of interpretability of machine learning features by frameworks such as <bold>SHAP (SHapley Additive exPlanations</bold>; see <xref ref-type="bibr" rid="bib26">Chen et al., 2022a</xref>; <xref ref-type="bibr" rid="bib72">Lundberg and Lee, 2017</xref>). However, what is done in the present work is fundamentally different from SHAP. Although both frameworks compute Shapley values, SHAP does it for input features of machine learning algorithms. In contrast, here we compute the Shapley value of each node of the network, with respect to all other nodes, at every time point. This warrants a different interpretation of the results compared to the SHAP framework.</p><p>There is a large body of research aiming at decomposing the amount of influence originating from a set of source nodes on a target node. Some attempts have used information theory, such as causal information (<xref ref-type="bibr" rid="bib6">Ay and Polani, 2008</xref>), and information transfer (<xref ref-type="bibr" rid="bib85">Novelli and Lizier, 2021</xref>), as well as using <bold>Partial Information Decomposition (PID)</bold> (<xref ref-type="bibr" rid="bib34">Ehrlich et al., 2022</xref>; <xref ref-type="bibr" rid="bib57">Gutknecht et al., 2021</xref>; <xref ref-type="bibr" rid="bib74">Luppi et al., 2024</xref>). In PID, the information transferred can be further decomposed into synergistic, redundant and unique contributions. In the current work, we deliberately used the wording of influence instead of information, since a formal link between these two properties still needs to be established (but see <xref ref-type="bibr" rid="bib7">Ay et al., 2019</xref> for an example of equating them). Our framework, however, does not measure the amount of information transferred as defined by information theory; it also does not explicitly decompose the influence into subparts. What it does is to characterize the amount of influence one node has over another at any time points, given a modeling paradigm (see <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). This influence embeds higher order interactions, such as synergistic and redundant interactions, but eventually summarizes them into one value per time point, describing the average contribution of a node to its targets given its contribution to all possible coalitions of sources. Therefore, we believe that our framework complements the rich body of previous research by providing another level of rigorous granularity, specifically<italic>,</italic> time-resolved exact influence. Further research is needed to establish where the two perspectives of game theory and information theory converge, and how combining them could provide insights about communication in brain networks.</p><p>On a related note, due to its reliance on multi-site lesioning, MSA has been used to address the complexity resulting from higher order interactions in the context of causal inference (<xref ref-type="bibr" rid="bib38">Fakhar et al., 2022</xref>; <xref ref-type="bibr" rid="bib39">Fakhar and Hilgetag, 2022</xref>; <xref ref-type="bibr" rid="bib76">Malherbe et al., 2021</xref>; <xref ref-type="bibr" rid="bib134">Zavaglia et al., 2023</xref>). Following the well-established Woodwardian account of causality, that is<italic>,</italic> the manipulability account (<xref ref-type="bibr" rid="bib127">Woodward, 2005</xref>), MSA at its core is a causal inference framework. The manipulability account of causality (<xref ref-type="bibr" rid="bib127">Woodward, 2005</xref>) states that ‘C is a cause of E, if manipulating C results in tractable changes in E.’ Here, we define manipulation as lesioning, as is customary in neuroscience (<xref ref-type="bibr" rid="bib2">Adolphs, 2016</xref>; <xref ref-type="bibr" rid="bib61">Joutsa et al., 2023</xref>; <xref ref-type="bibr" rid="bib115">Siddiqi et al., 2022</xref>; <xref ref-type="bibr" rid="bib121">Vaidya et al., 2019</xref>). We then address the problem of ‘counterfactuals’ by exploring all possible ways that causes can interact at any time-point to produce an effect, leading to one of the most detailed accounts of causality. Therefore, placing our method in the large family of <bold>effective connectivity (EC)</bold> methods appears as a natural next step. However, here we decided to opt out of this comparison in favor of communication models, since they conceptualize brain-wide causal interactions, which involve both direct and indirect interactions, uncovering causal pathways instead (<xref ref-type="bibr" rid="bib97">Ross, 2018</xref>). Expanding causal relationships to pathways of different lengths allows for multiple causes to interact and jointly produce an effect. Moreover, comparing our findings with those from EC approaches would involve considering a host of methods, not limited to but including spectral dynamic causal modeling (<xref ref-type="bibr" rid="bib106">Seguin et al., 2019</xref>), partial correlation analysis, Granger causality, dynamic differential covariance (<xref ref-type="bibr" rid="bib27">Chen et al., 2022b</xref>; <xref ref-type="bibr" rid="bib68">Laasch et al., 2024</xref>), transfer entropy (<xref ref-type="bibr" rid="bib85">Novelli and Lizier, 2021</xref>), and so forth. We believe that this worthwhile comparison calls for a separate and extensive analysis that is beyond the scope of the current work.</p><p>Finally, our framework is model-agnostic, meaning that the game and its constraints should be defined first. This approach comes with great liberty to explore different dynamical models. However, we acknowledge that this freedom can be a double-edged sword. On the one hand, it allows researchers to investigate a multitude of questions, potentially providing meaningful insight into the mechanism of communication and its role in behavior. On the other hand, it can result in ill-conceived games that lead to misinterpretations. For instance, here the time-resolved influence of all nodes over each other is a three-dimensional array. Conventionally, it would be tempting to average over the time points and produce a time-averaged influence matrix. However, since our models were stationary, the average value of the vector would be highly noisy, providing little insight into ‘how much’ nodes modulate each other’s activity. Instead, we computed the variance to capture this feature, but one might as well compute alternative metrics, such as standard deviation, entropy, energy, where each requires careful interpretation. Moreover, variance alone might be a poor indicator of influence if the network has inhibitory connections, as both biological and <bold>artificial neural networks (ANNs)</bold> tend to have. By contrast, the present work focuses on conventional neuroimaging datasets with non-negative connection weights.</p><p>In sum, it is crucial to define every step of the game carefully to avoid misinterpretations. It is also important to keep in mind the provided definitions for optimality, equilibrium and influence to avoid confusion in interpreting the results of the game-theoretical analysis.</p></sec><sec id="s3-2"><title>Broadcasting as an optimal signaling strategy</title><p>Many previous studies in network neuroscience have relied on a graph theoretical definition of efficiency that is based on the shortest path distance (<xref ref-type="bibr" rid="bib117">Sporns, 2018</xref>). Here, we show that the underlying signal propagation dynamics in large-scale brain networks are likely to follow a broadcasting regime. This finding implies that broadcasting goes beyond signaling along the shortest path but does not downplay its role. In other words, we show that the shortest path is critical, but not the only path along which a signal traverses. The broadcasting mechanism also circumvents a major problem of the shortest path signaling concept, that nodes would have to find the shortest path to every other node in the network without access to global information on the network organization. By broadcasting, nodes simply transmit their signal via their outgoing connections in every direction. And since the signal degrades over every step, nodes topologically closest to the source receive the signal most strongly. Put differently, navigation along the shortest path emerges naturally and without a need for centralized routing strategies, simply because the closest node receives more signal compared to the farther ones. This line of reasoning is compatible with the result depicted in (<xref ref-type="fig" rid="fig8">Figure 8E</xref>) where closeness centrality predicted the influential nodes with such great accuracy compared to other centrality measures. Additionally, communication models, such as communicability, are thought to be energetically costlier than routing strategies, such as signaling exclusively along the shortest paths (<xref ref-type="bibr" rid="bib110">Seguin et al., 2023</xref>). The reason for this argument is the fact that communicability accounts for paths of all length. However, as shown here, paths longer than seven steps are unlikely to be meaningfully contributing. Supporting this finding, Griffa and colleagues <xref ref-type="bibr" rid="bib53">Griffa et al., 2023</xref> used information theory and found that not only parallel communication is present in the human cortex, but also pathways longer than five steps are not as effective. These observations suggest that regions balance between robustness and energetic cost of signal transmission by using a handful of parallel pathways. Intriguingly, (<xref ref-type="bibr" rid="bib53">Griffa et al., 2023</xref>) reported that the capacity for parallel communication is larger in the human cortex compared to macaque and mouse, proposing an evolutionary advantage for broadcasting over multiple pathways (<xref ref-type="bibr" rid="bib53">Griffa et al., 2023</xref>). They also found that these pathways are more frequently placed between different functional modules, rather than within them, which aligns with our finding that hub regions harness parallel pathways to compensate for weak structural connections among them.</p><p>Another implication of this finding is that metrics such as global efficiency, representing the average shortest-path distance between all nodes, are not ideal for capturing the signaling dynamics of the brain. As Zamora-Lopez and Gilson argue (<xref ref-type="bibr" rid="bib132">Zamora-López and Gilson, 2024</xref>), all such graph theoretical metrics assume some sort of dynamics. It is important to first establish the correct dynamics and then apply the corresponding metrics. Prompted by our findings, we would suggest that, when possible, metrics based on the shortest path distance should be replaced with others based on communicability, LAM, or SAR. For instance, instead of using the average shortest path as a measure of global network efficiency, one may compute the average communicability as <italic>global broadcasting efficiency</italic>. Similarly, closeness centrality may be redefined as the <italic>broadcasting strength</italic> by taking the average node-wise communicability instead of the average node-wise shortest-path distance.</p><p>Next, it is not yet clear whether the broadcasted signal is modified along longer chains in the network, or just relayed to boost the transmitted information. If the signal is modified, then it is likely that these pathways serve a computational role. Previous work suggests that within-module communication involves more redundant information compared to communication across modules which is mainly synergistic (<xref ref-type="bibr" rid="bib74">Luppi et al., 2024</xref>; <xref ref-type="bibr" rid="bib73">Luppi et al., 2022</xref>). Interpreting our work in this perspective, information travels along fewer parallel pathways within a functional module to be processed locally, resulting in higher redundancy. However, signals are likely modified along the parallel pathways and combined with incoming signals from other modules, leading to a greater synergy.</p><p>Interestingly, the broadcasting conceptualization is argued to be a better model of information propagation in social networks compared to a random-walker-based navigation (<xref ref-type="bibr" rid="bib47">Ghosh et al., 2023</xref>). In neuroscience, the broadcasting model of information propagation bears qualitative resemblance with concepts such as traveling waves of excitation (<xref ref-type="bibr" rid="bib83">Muller et al., 2018</xref>), bridging a gap between communication models and communication via oscillation. Notably, we found a correlation of 0.23 between OI and FC (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), that is close to what has been previously reported by comparing <bold>cortico-cortical evoked potentials (CCEPs)</bold> with FC (<xref ref-type="bibr" rid="bib66">Keller et al., 2011</xref>). Moreover, it has been proposed that at least a subset of CCEPs propagate as traveling waves (<xref ref-type="bibr" rid="bib23">Campbell et al., 2023</xref>). Together, these works, and our findings suggest that, first, correlation-based measures such as FC capture the propagation of information in brain networks to a limited extent, and second an approach combining CMs with traveling waves could provide biophysical mechanisms for simple signal propagation models based on broadcasting regime such as communicability. Nonetheless, our framework is grounded in game theory where its fundamental assumption is that nodes aim at maximizing their influence over each other, given the existing constraints. This assumption is well explored using various theoretical frameworks (<xref ref-type="bibr" rid="bib19">Buehlmann and Deco, 2010</xref>; <xref ref-type="bibr" rid="bib21">Bullmore and Sporns, 2012</xref>; <xref ref-type="bibr" rid="bib29">Chklovskii et al., 2002</xref>; <xref ref-type="bibr" rid="bib70">Laughlin and Sejnowski, 2003</xref>; <xref ref-type="bibr" rid="bib87">O’Byrne and Jerbi, 2022</xref>) and remains open to further empirical investigation. Here, we used game theory to mathematically formalize a <italic>theoretical optimum</italic> for communication in brain networks. Our findings then provide a possible mechanism for achieving this optimality through broadcasting. Based on our results, we speculate that, there exist an optimal broadcasting strength that balances robustness of the signal against its metabolic cost. This hypothesis is reminiscent of the concept of brain criticality, which suggests the brain to be positioned in a state in which the information propagates maximally and efficiently (<xref ref-type="bibr" rid="bib87">O’Byrne and Jerbi, 2022</xref>; <xref ref-type="bibr" rid="bib103">Safavi et al., 2024</xref>). Together, we suggest broadcasting to be the possible mechanism with which communication is optimized in brain networks, however, further research directions include investigating whether signaling within brain networks indeed aligns with a game-theoretic definition of optimality. Additionally, if it does, subsequent studies could then examine how deviations from optimal communication contribute to or result from various brain states or neurological and psychiatric disorders.</p><p>Finally, we found that the rich-club organization of the human cortex is a major contributor to shaping brain-wide communication dynamics. An inspiring work using normative modeling suggested that hierarchies and rich-club organization are spandrels of modules and hubs (<xref ref-type="bibr" rid="bib102">Rubinov, 2016</xref>). Spandrels are evolutionary features that evolve alongside adaptive phenotypes, while themselves serve no adaptive function. In other words, the rich-club organization is proposed to be the byproduct of modular organization while serving no adaptive function by itself (<xref ref-type="bibr" rid="bib50">Gould and Lewontin, 1979</xref>). Our work provides evidence that, although the rich club organization could have emerged as a byproduct of modular organization, it might not be a purely ornamental spandrel, as the rich club regions represent the most influential regions of the brain (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Previous works relating rich club regions to signaling also indicated an integrative role of these regions, which have longer temporal receptive fields compared to others, allowing them to integrate more information over time (<xref ref-type="bibr" rid="bib24">Chaudhuri et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Hasson et al., 2008</xref>). Interestingly, our results depicted in <xref ref-type="fig" rid="fig8">Figures 8</xref> and <xref ref-type="fig" rid="fig10">10</xref> support this idea. We found that rich club regions integrate signals over longer and parallel structural pathways, enhancing their broadcasting capacity. Moreover, a recent empirical work identified disturbances in these regions and their broadcasting efficiency in patients suffering from disorders of consciousness (<xref ref-type="bibr" rid="bib91">Panda et al., 2023</xref>). Put together, these findings suggest a causal functional role for the rich-club organization of the human cerebral cortex.</p></sec><sec id="s3-3"><title>Role of network dynamics in optimal signaling</title><p>Although our work suggests a greater role for the network’s structure in shaping the state of optimal signal propagation in the network compared to the specifics of the local node dynamics, this finding does not mean that the local dynamics are irrelevant. We believe that the inequality between the network’s structure and dynamics in determining signal flow arises from factors such as the assumed homogeneity of nodes in our network models. Here, nodes are assumed to be identical objects with identical features and dynamics. This assumption certainly does not apply in the brain, and a growing body of network models that incorporate regional differences aim at addressing this issue (<xref ref-type="bibr" rid="bib9">Bazinet et al., 2023</xref>; <xref ref-type="bibr" rid="bib32">Deco et al., 2021</xref>). As mentioned before, our framework is model agnostic, so it is also possible to interrogate networks with heterogeneous models of dynamics. For applying the present approach to heterogeneous networks, we hypothesize that, first, how nodes influence one another further decouples from the structure (<xref ref-type="bibr" rid="bib38">Fakhar et al., 2022</xref>), and second, simple communication models fail to replicate the OI as well as they have done in this work. Thus, further work can shine light on the interplay between the network’s structure and its dynamics in shaping OI, since not only heterogeneous network models are biologically more plausible, but also, they can help to investigate how information optimally flows in networks where nodes respond differently to incoming signals.</p></sec><sec id="s3-4"><title>Limitations and further directions</title><p>A conceptual note is that OI, as a normative framework, describes the landscape of communication dynamics if nodes maximize their influence. As in other game-theoretical models, this assumption might not fully hold. The derived landscape is a prediction that would need to be experimentally tested using in-vivo multi-site lesioning experiments, which is currently not feasible. However, recent work indirectly supports our findings by showing that propagation of electrical stimulation in the human cortex is well captured by communicability (<xref ref-type="bibr" rid="bib108">Seguin et al., 2022a</xref>), which we found to have one of the largest correlations with OI. There are also technical limitations of our framework, specifically its computational cost, since one needs potentially to simulate millions of in-silico lesion combinations for each target node. For instance, in this work, we performed around half a billion lesion simulations for just one game, such as a linear model on the human connectome (see section Game-theoretical Framework in Material and Methods). Performing such an intensive analysis is not possible on a simple laptop. Therefore, due to its brute-force approach, our framework provides rigorous results at the expense of high computational effort, apart from the fact that it is yet to be used in-vivo (but see <xref ref-type="bibr" rid="bib133">Zavaglia and Hilgetag, 2016</xref>).</p><p>Finally, we focused on the influence of nodes on each other without relating them to behavior or cognitive function. Moreover, we did not investigate how this influence landscape changes due to brain disorders, aging, and other factors. Future work may take on these challenges, for instance, by tracking the dynamics of OI during neurodegenerative disease progression.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>In this study, we used multiple Python libraries, including MSApy (<xref ref-type="bibr" rid="bib37">Fakhar, 2021</xref>), Neurolib (<xref ref-type="bibr" rid="bib22">Cakan et al., 2023</xref>), Netneurotools, BCTpy (<xref ref-type="bibr" rid="bib99">Rubinov and Sporns, 2010</xref>), nctpy (<xref ref-type="bibr" rid="bib92">Parkes et al., 2022</xref>), and Networkx (<xref ref-type="bibr" rid="bib58">Hagberg et al., 2008</xref>). The Jupyter notebooks and Python functions to reproduce this work are available at the GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/kuffmode/OI-and-CMs">https://github.com/kuffmode/OI-and-CMs</ext-link> (copy archived at <xref ref-type="bibr" rid="bib40">Fakhar and Dixit, 2024a</xref>).</p><p>The simulation results are available at: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10849223">https://zenodo.org/records/10849223</ext-link>.</p><p>Additionally, a dedicated user-friendly Python library to compute OI and some communication models is developed and can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/kuffmode/YANAT">https://github.com/kuffmode/YANAT</ext-link> (copy archived at <xref ref-type="bibr" rid="bib41">Fakhar and Dixit, 2024b</xref>).</p><p>All connectomes are available as a part of the Netneurotools library: <ext-link ext-link-type="uri" xlink:href="https://github.com/netneurolab/netneurotools">https://github.com/netneurolab/netneurotools</ext-link> (copy archived at <xref ref-type="bibr" rid="bib10">Bazinet et al., 2025</xref>).</p><sec id="s4-1"><title>Structural and functional connectomes</title><p>We analyzed three publicly available connectomes of human, macaque, and mouse to address differences in invasive versus non-invasive imaging modalities and reconstruction techniques.</p><sec id="s4-1-1"><title>Human connectome</title><p>Structural and functional connectivity datasets were acquired from a cohort of 70 healthy subjects at the Lausanne University Hospital in Switzerland, and the details are described elsewhere (<xref ref-type="bibr" rid="bib52">Griffa et al., 2019</xref>; <xref ref-type="bibr" rid="bib113">Shafiei et al., 2020</xref>; <xref ref-type="bibr" rid="bib112">Shafiei et al., 2019</xref>). Briefly, the group comprised individuals with an average age of 28.8 y (standard deviation 9.1 y), including 27 females, and all were scanned using a 3 Tesla MRI machine. A deterministic streamline tractography was employed to create individual SC matrices from each participant’s <bold>diffusion spectrum imaging (DSI)</bold> data, which were recorded at five distinct levels of brain parcellation. However, here we have used only one with relatively high-resolution (219 cortical regions). The strength of each structural connection within these matrices was approximated by the density of fiber tracts.</p><p>To mitigate the size disparities between brain regions and counteract the inherent preference for longer fiber tracts in the tractography technique, a normalization procedure was applied. This involved adjusting the streamline counts by the average surface areas of the connected regions and the mean streamline lengths. The culmination of this process was the formation of a unified, group-average weighted SC matrix, which was derived using a consensus strategy that ensured the individual edge length distributions were conserved (<xref ref-type="bibr" rid="bib15">Betzel et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Griffa et al., 2019</xref>).</p><p>Functional brain data were gathered from the same group of subjects through <bold>resting-state functional MRI (rs-fMRI)</bold> scans conducted with open eyes. The initial processing steps of this functional data included adjustments for several physiological variables, notably the influence of white matter, cerebrospinal fluid, and motion artifacts that comprise three axes of translation and rotation as determined by rigid body co-registration. Following these corrections, the <bold>blood oxygen level-dependent (BOLD)</bold> time series were subjected to a low pass filtering process, utilizing a temporal Gaussian filter set to a full-width half maximum of 1.92 s. The analysis excluded the first four temporal points from each scan and further data refinement was achieved through high-motion frame censoring, a method detailed in Power and colleagues (<xref ref-type="bibr" rid="bib94">Power et al., 2012</xref>).</p><p>For the assessment of FC, the study calculated zero-lag Pearson correlation coefficients, which measured the functional relationships between pairs of brain regions in each individual’s rs-fMRI time series. Lastly, the group-average FC matrix was computed as the mean of these individual pairwise connectivity values, which includes negative values as well.</p></sec><sec id="s4-1-2"><title>Macaque connectome</title><p>The connectivity matrix for the macaque was derived from retrograde tract-tracing experiments and is provided by <xref ref-type="bibr" rid="bib77">Markov et al., 2013</xref>. These experiments involved the use of fluorescent tracers in a group of 28 macaque monkeys. The projections that were reconstructed from these experiments were mapped according to a division of 91 cortical areas. This division was based on a combination of histological examinations and atlas-based references. In each tract-tracing experiment, Markov and colleagues quantified the number of neurons that were labeled in each of these 91 areas. The count of labeled neurons was then adjusted by subtracting the number of neurons that were native to the site of tracer injection. The outcome of this process was a 29×91 matrix, which represents the connection weights extending from each injection site to other regions of the brain. Here, we focused on the subset of this matrix that describes the weighted and directed connections between 29 cortical areas, leading to a 29×29 SC matrix.</p></sec><sec id="s4-1-3"><title>Mouse connectome</title><p>The connectivity matrix for the mouse was compiled using tract-tracing data openly accessible from the Allen Institute Mouse Brain Connectivity Atlas (<xref ref-type="bibr" rid="bib88">Oh et al., 2014</xref>). Briefly, the process involved injecting anterograde recombinant adeno-associated viral tracers into designated areas within the right hemisphere of mouse brains. Three weeks following the injection, during which the viral tracer projections developed, the brains were extracted for reconstruction. These reconstructions were then standardized and aligned with the Allen Reference Atlas' common coordinate framework.</p><p>Nodes in the network were identified based on a specialized parcellation derived from the Allen Developing Mouse Brain Atlas. This parcellation initially included 65 areas in each hemisphere, but 9 areas were excluded as they did not participate in any tract-tracing experiments. Consequently, the network that was analyzed constituted 112 regions (<xref ref-type="bibr" rid="bib101">Rubinov et al., 2015</xref>). Edges represent axonal projections between different areas, and they were quantified as normalized connection densities. Specifically, this measure represents the number of connections per unit volume from a source area to a target area.</p></sec></sec><sec id="s4-2"><title>Large-scale computational models of the brain dynamics</title><p>In this work, we employed three models with increasing biological fidelity. The most abstract and simplistic model is the linear model, where the output of a node is a weighted sum of its inputs. The nonlinear model extends the linear model by applying a nonlinear transformation to the input (here,  <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) And lastly, the neural mass model describes nodes as Stuart-Landau oscillators. Below are the details of each model.</p><sec id="s4-2-1"><title>Linear and nonlinear models</title><p>The linear model we used, also known as the multivariate Ornstein-Uhlenbeck process, follows the continuous dynamical system equations described in <xref ref-type="bibr" rid="bib43">Fernández Galán, 2008</xref>.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Independent Gaussian noise <inline-formula><mml:math id="inf6"><mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:math></inline-formula> with mean zero and unit variance were presented to each node. Here, <inline-formula><mml:math id="inf7"><mml:mi>τ</mml:mi></mml:math></inline-formula> denotes the relaxation time of the node, which was set to 0.02. It should be noted that, in practice, the adjacency matrix is normalized to ensure that its largest eigenvalue <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Consequently, the practical decay rate aligns with the product of the relaxation time, <inline-formula><mml:math id="inf9"><mml:mi>τ</mml:mi></mml:math></inline-formula>, and the maximum eigenvalue, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the strength of the noise and was set to 0.05 in all experiments. Given the empirical functional connectivity matrices, we optimized the coupling parameter, <inline-formula><mml:math id="inf12"><mml:mi>g</mml:mi></mml:math></inline-formula>, to maximize correlation between empirical and simulated FCs. With our modeling setting, <inline-formula><mml:math id="inf13"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.74</mml:mn></mml:math></inline-formula> demonstrated the best fit, with a correlation of 0.23. The nonlinear model modifies the linear one simply by adding a nonlinear transformation and a larger input noise of 2 instead of 0.05 to go beyond the linear part of the function, leading to the equation below:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-2-2"><title>Neural mass model</title><p>Furthermore, we explored SC networks governed by Hopf/Stuart-Landau equations (<xref ref-type="bibr" rid="bib31">Deco et al., 2017</xref>). This model, recognized as the canonical approach for examining the shift from noisy to oscillatory dynamics, elucidates the behavior of a nonlinear oscillating system near the Hopf bifurcation. In essence, the dynamics of each network node are captured by the following complex equation:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a complex representation of the node state, with <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> representing Gaussian noise characterized by a standard deviation <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>. This system undergoes a supercritical bifurcation at <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, where it shifts from a stable fixed point (i.e. <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) to a limit cycle oscillation with frequency <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:math></inline-formula> .</p><p>By decomposing the complex state into its Cartesian components and incorporating the influence of other nodes in the input of each node, we derived a set of coupled equations (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) to govern the entire brain dynamics. This formulation enables the representation of the influence of nodes on each other’s temporal state through a diffusive interaction, where <inline-formula><mml:math id="inf20"><mml:mi>g</mml:mi></mml:math></inline-formula> serves as a global coupling factor.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Similar to the other two models, the global dynamics of the network are influenced by model’s parameters. Here, we precisely tuned the global coupling, <inline-formula><mml:math id="inf21"><mml:mi>g</mml:mi></mml:math></inline-formula> and the bifurcation parameter, <inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, to match the empirical FC. Setting <inline-formula><mml:math id="inf23"><mml:mi mathvariant="normal">g</mml:mi></mml:math></inline-formula> at 5.6 and <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:math></inline-formula> for all nodes, yields a correlation of 0.35. Other parameters, as detailed in the Neurolib library, remain unaltered as follows. The model features a diffusive coupling type, a signal velocity of 20.0 m/s, a global coupling strength set at 5.8, a 5.0 ms Ornstein-Uhlenbeck timescale, Ornstein-Uhlenbeck noise intensity fixed at 0.05 mV/ms/sqrt(ms), a mean value of Ornstein-Uhlenbeck process maintained at 0.0 mV/ms, a Hopf bifurcation parameter set to 0.15, and an oscillator frequency of 32 Hz. We collected state variables, <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, representing signals acquired from various brain regions for subsequent analysis.</p></sec></sec><sec id="s4-3"><title>Game-theoretical framework</title><p>MSA is built upon Shapley values, which quantify a player’s <italic>fair</italic> share of a collectively generated outcome (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Generally, Shapley values are calculated by adding a player to all possible coalitions and observing the value they bring to the coalition (<xref ref-type="bibr" rid="bib65">Keinan et al., 2004b</xref>). Formally, the contribution of a player <inline-formula><mml:math id="inf26"><mml:mi>i</mml:mi></mml:math></inline-formula> to a coalition <inline-formula><mml:math id="inf27"><mml:mi>S</mml:mi></mml:math></inline-formula> is given by:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> represents the value of coalition <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The Shapley value <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the average of these contributions across all permutations <inline-formula><mml:math id="inf31"><mml:mi>R</mml:mi></mml:math></inline-formula> of the player set:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="fraktur">R</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf32"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula> is the set of all permutations, and <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the coalition formed by ordering <inline-formula><mml:math id="inf34"><mml:mi>R</mml:mi></mml:math></inline-formula> up to player <inline-formula><mml:math id="inf35"><mml:mi>i</mml:mi></mml:math></inline-formula> . However, due to the computational complexity of calculating Shapley values for large sets, MSA relies on an unbiased estimator by sampling permutations. We employed a sample size <inline-formula><mml:math id="inf36"><mml:mi>m</mml:mi><mml:mo>≪</mml:mo><mml:mi>N</mml:mi><mml:mo>!</mml:mo></mml:math></inline-formula>, generating <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> unique permutations for every target region, where <inline-formula><mml:math id="inf38"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of source regions.</p><p>In this work, we iterated this process over every node (target node) by systematically lesioning other nodes (source nodes) and tracking the time-resolved difference between when the sources were lesioned and when they were not. Lesioning was modeled by setting the incoming and outgoing connections of sources to zero. For every source node, we sampled m=1,000 permutations. However, we confirmed that the algorithm converged by comparing it against a larger sample size of m=10,000, which resulted in a correlation coefficient of 1.0 (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We also ran the analysis for 10 repetitions and averaged the resulting influence matrices. The final matrix has a shape of (number of regions × number of regions × simulation time) that we reduced to a two-dimensional matrix (number of regions ×number of regions) by taking the variance of each influence profile. Altogether, each of the experiments on the human connectome resulted in roughly 480 million in-silico lesions (219 targets × 219 sources × 1,000 combination of lesions per source for each target ×10 trials). Due to the computational implausibility of the neural mass model, we ran the analysis only once instead of 10 repetitions, thus 48 million lesions. Together with all control analyses, the total number of in-silico lesions for the human connectome amounts to 7200 million:</p><p>480 m for the linear model, 480 m for the nonlinear, 48 m for the Hopf, 480 m for the topology-shuffled null model, 480 m for the weight-shuffled null model, 4,800 m for the case where we used 10,000 lesion combinations per source for each target, and 480 m for a test case where the coupling was set to zero. All experiments were conducted on a high-performance computing facility provided by the Institute of Computational Neuroscience, University Hospital of Hamburg.</p></sec><sec id="s4-4"><title>Communication models and measures</title><p>In this section, we introduce the communication models and measures employed to analyze the dynamics of information flow within the studied network. The corresponding formulas and procedures for computation are presented in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Communication models and measures.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Models</th><th align="left" valign="bottom">Measures<sup><xref ref-type="table-fn" rid="table1fn1">*</xref></sup></th><th align="left" valign="bottom">References</th></tr></thead><tbody><tr><td align="left" valign="middle">Shortest Path Efficiency <break/>Floyd-Warshall algorithm</td><td align="left" valign="middle"><inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib69">Latora and Marchiori, 2001</xref>; <xref ref-type="bibr" rid="bib107">Seguin et al., 2020</xref></td></tr><tr><td align="left" valign="middle">Navigation Efficiency</td><td align="left" valign="middle"><inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">Λ</mml:mi></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib107">Seguin et al., 2020</xref>; <xref ref-type="bibr" rid="bib105">Seguin et al., 2018</xref></td></tr><tr><td align="left" valign="middle">Diffusion Efficiency</td><td align="left" valign="middle"><inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>D</mml:mi><mml:mi>E</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Z</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ω</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>ω</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>ω</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib48">Goñi et al., 2013</xref>; <xref ref-type="bibr" rid="bib107">Seguin et al., 2020</xref></td></tr><tr><td align="left" valign="middle">Search Information</td><td align="left" valign="middle"><inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mi>I</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Π</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo>⋯</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib106">Seguin et al., 2019</xref>; <xref ref-type="bibr" rid="bib107">Seguin et al., 2020</xref></td></tr><tr><td align="left" valign="middle">Communicability</td><td align="left" valign="middle"><inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib27">Chen et al., 2022b</xref>; <xref ref-type="bibr" rid="bib36">Estrada and Hatano, 2008</xref></td></tr><tr><td align="left" valign="middle">Scaled Communicability</td><td align="left" valign="middle"><inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib47">Ghosh et al., 2023</xref>; <xref ref-type="bibr" rid="bib79">Messé et al., 2014</xref></td></tr><tr><td align="left" valign="middle">Linear Attenuation</td><td align="left" valign="middle"><inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib48">Goñi et al., 2013</xref></td></tr><tr><td align="left" valign="middle">Spatial Autoregressive</td><td align="left" valign="middle"><inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><xref ref-type="bibr" rid="bib131">Zamora-López et al., 2016</xref></td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>Throughout the table, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the adjacency matrix corresponding to a given graph, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> vertices (nodes).<inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> signifies the connection strength from vertex i to vertex j. Anatomical connections are indicated by <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for connected region pairs and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for unconnected pairs. <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the matrix of path lengths, defined as the reciprocal of <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>W</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the path length between two connected nodes, i and j. Further, <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the sequence of nodes visited along the shortest path between nodes i and j. Given the spatial organization of nodes in empirical structural connectivity, <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> includes the Euclidean distance between source, s, and destination, t nodes in finding optimal paths. The term <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>W</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> signifies the normalized adjacency matrix, where each element <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained by dividing the original connection strength, <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by the sum of all connection strengths in the corresponding row of the matrix <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the identity matrix, <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> stands for transposed inverse matrix, and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the spectral radius of the adjacency matrix, <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.<inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the eigenvalues of a given matrix.</p></fn></table-wrap-foot></table-wrap><p>The structural connectivity matrix <inline-formula><mml:math id="inf69"><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> denotes the strength of pairwise connections between <inline-formula><mml:math id="inf70"><mml:mi>N</mml:mi></mml:math></inline-formula> brain regions. We define the connection length matrix <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the travel cost between regions <inline-formula><mml:math id="inf73"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mi>j</mml:mi></mml:math></inline-formula>. This conversion from connection weights to lengths is required for network communication models that optimize for minimal transmission cost of signals, e.g., shortest path efficiency.</p><sec id="s4-4-1"><title>Shortest path efficiency</title><p>Shortest Path Efficiency measures the effectiveness of communication along the most direct route between two nodes in a network. Here, the Floyd-Warshall algorithm was employed to determine the sequence of regions <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that minimizes the total transmission cost for signals traveling between regions <inline-formula><mml:math id="inf76"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This cost, denoted by <inline-formula><mml:math id="inf78"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is defined as the sum <inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Subsequently, the shortest path efficiency (SPE) between two regions is quantified as the reciprocal of the minimum transmission cost, expressed as <inline-formula><mml:math id="inf80"><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> as detailed in <xref ref-type="bibr" rid="bib69">Latora and Marchiori, 2001</xref>.</p></sec><sec id="s4-4-2"><title>Navigation efficiency</title><p>Navigation within the network employs a greedy protocol that assumes signal transmission to minimize the inter-regional Euclidean distance. The process involves iterative progression from a source region <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to a target region <inline-formula><mml:math id="inf82"><mml:mi>j</mml:mi></mml:math></inline-formula>. At each step, the neighbor spatially closest to <inline-formula><mml:math id="inf83"><mml:mi>j</mml:mi></mml:math></inline-formula> is chosen as the next node in the path (<xref ref-type="bibr" rid="bib105">Seguin et al., 2018</xref>). This sequence continues until the target is reached, marking successful navigation, or a previously visited node is encountered, indicating a failure in navigation. The cumulative length of a successful path is denoted by <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the sequence of nodes traversed. If navigation fails, <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is set to infinity. Navigation efficiency is thus defined as the reciprocal of the traversed path length.</p></sec><sec id="s4-4-3"><title>Diffusion efficiency</title><p>The Diffusion Efficiency (DE) model characterizes signaling through the lens of random walks. The computation of DE involves the utilization of the transition matrix, <inline-formula><mml:math id="inf87"><mml:mi>P</mml:mi></mml:math></inline-formula> within a Markov chain process unfolding on the connection weight matrix, <inline-formula><mml:math id="inf88"><mml:mi>W</mml:mi></mml:math></inline-formula>. Specifically, it considers the probability <inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that a simple random walker at node <inline-formula><mml:math id="inf90"><mml:mi>i</mml:mi></mml:math></inline-formula> will advance to node <inline-formula><mml:math id="inf91"><mml:mi>j</mml:mi></mml:math></inline-formula> and the mean first passage time, <inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which quantifies the expected number of intermediate regions visited in a random walk. To elaborate on the computation procedure, please refer to <xref ref-type="table" rid="table1">Table 1</xref>. Additionally, for a more in-depth theoretical understanding, consult references (<xref ref-type="bibr" rid="bib48">Goñi et al., 2013</xref>; <xref ref-type="bibr" rid="bib135">Zhou, 2003</xref>).</p></sec><sec id="s4-4-4"><title>Search information</title><p>Search information is a metric that evaluates the required amount of information to push a random walker towards the shortest path, thereby reflecting the accessibility of efficient communication routes under a diffusive model. Computing SI involves finding the above-mentioned shortest path, <inline-formula><mml:math id="inf93"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi mathvariant="normal">ω</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between two regions, and the probability, <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, that a random walker will accidentally traverse from region <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to region <inline-formula><mml:math id="inf96"><mml:mi>j</mml:mi></mml:math></inline-formula> following this path. This probability is computed using the transition matrix, <inline-formula><mml:math id="inf97"><mml:mi>P</mml:mi></mml:math></inline-formula> within a Markov chain process unfolding on the connection weight matrix. <xref ref-type="table" rid="table1">Table 1</xref> summarizes the computation procedure.</p></sec><sec id="s4-4-5"><title>Communicability and scaled communicability</title><p>The measure of communicability between nodes, <inline-formula><mml:math id="inf98"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, is expressed as the weighted sum of the overall pathways connecting them, where each walk’s contribution is weighted proportionally to the inverse of its length—signifying the number of connections traversed. In practice, before computing communicability, nonbinary connection weight matrices are commonly normalized. This normalization step is employed to diminish the impact of highly influential nodes with substantial strength. Refer to <xref ref-type="table" rid="table1">Table 1</xref> for the computation procedure and references (<xref ref-type="bibr" rid="bib36">Estrada and Hatano, 2008</xref>; <xref ref-type="bibr" rid="bib132">Zamora-López and Gilson, 2024</xref>) for additional theoretical details. We also reported the scaled communicability measure, which introduces the modulating parameter, <inline-formula><mml:math id="inf100"><mml:mi mathvariant="normal">α</mml:mi></mml:math></inline-formula> to control the decay rate.</p></sec><sec id="s4-4-6"><title>Linear attenuation model</title><p>The Linear attenuation model (LAM) follows the same line of reasoning as communicability, that information propagation can be represented as a weighted sum of all walks in the network. The difference lies in how walk-lengths are discounted. In contrast to communicability, where the sum of walks is estimated through the exponentiation of the adjacency matrix, Katz introduced an attenuation factor, <inline-formula><mml:math id="inf101"><mml:mi mathvariant="normal">α</mml:mi></mml:math></inline-formula>, to reduce the influence over every step monotonically. Katz proposed a closed-form expression, given by <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib63">Katz, 1953</xref>). It is important to note that the convergence condition in this context is <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , where <inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi mathvariant="normal">λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the spectral radius of the adjacency matrix, <inline-formula><mml:math id="inf105"><mml:mi>W</mml:mi></mml:math></inline-formula>. Prior to computing LAM, the adjacency matrix undergoes a normalization process, similar to the one applied in the computation of communicability.</p></sec><sec id="s4-4-7"><title>Spatial autoregressive model</title><p>Moreover, we incorporated the spatial autoregressive (SAR) model into our analysis. This model, characterized as a generic representation of diffuse processes on networks, is intricately linked to the distribution of paths within the network (<xref ref-type="bibr" rid="bib14">Betzel and Bassett, 2018</xref>; <xref ref-type="bibr" rid="bib79">Messé et al., 2014</xref>; <xref ref-type="bibr" rid="bib119">Tononi et al., 1994</xref>; <xref ref-type="bibr" rid="bib131">Zamora-López et al., 2016</xref>). In this model, the fluctuating signals at each node are interconnected through a system of structural equations, expressing each signal as a linear function of others. This interconnection is influenced by a global coupling strength factor denoted as <inline-formula><mml:math id="inf106"><mml:mi mathvariant="normal">α</mml:mi></mml:math></inline-formula>. Importantly, when the network experiences input noise, the SAR model predicts that each signal follows a multivariate normal distribution. This prediction is mathematically represented by the covariance matrix <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mover><mml:mi>W</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mover><mml:mi>W</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> The term <inline-formula><mml:math id="inf108"><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> denotes the normalized adjacency matrix, where each element <inline-formula><mml:math id="inf109"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is obtained by dividing the original connection strength, <inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by the sum of all connection strengths in the corresponding row of the matrix <inline-formula><mml:math id="inf111"><mml:mi>W</mml:mi></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf112"><mml:mi mathvariant="double-struck">I</mml:mi></mml:math></inline-formula> is the identity matrix and <inline-formula><mml:math id="inf113"><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula> stands for the transposed inverse matrix.</p></sec></sec><sec id="s4-5"><title>Null and simulated network models</title><p>We employed two simulated weighted networks and two null models as described below:</p><sec id="s4-5-1"><title>Erdős-Rényi model</title><p>An Erdős-Rényi random graph, denoted as <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, was generated with 100 nodes, where <inline-formula><mml:math id="inf115"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.35</mml:mn></mml:math></inline-formula> represents the probability that any two distinct nodes are connected by an edge. The graph was constructed by iterating over all possible pairs of nodes and connecting them with an edge with the probability <inline-formula><mml:math id="inf116"><mml:mi>p</mml:mi></mml:math></inline-formula>. This resulted in a binary adjacency matrix <inline-formula><mml:math id="inf117"><mml:mi>W</mml:mi></mml:math></inline-formula> where <inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is 1 if nodes <inline-formula><mml:math id="inf119"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mi>j</mml:mi></mml:math></inline-formula> are connected and 0 otherwise. After generating the binary structure of the graph, weights were assigned to the edges by sampling from a log-normal distribution. The weight <inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each edge in the graph was drawn from <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:mi mathvariant="normal">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>. The log-normal distribution was chosen for its property of providing a multiplicative effect, appropriate for modeling local dynamical systems coupled in a network.</p></sec><sec id="s4-5-2"><title>Barabási-Albert model</title><p>The Barabási-Albert model was used to generate synthetic scale-free networks through a preferential attachment mechanism. As with the Erdős-Rényi model, we generated a network with 100 nodes. Each new node added to the network creates <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> edges to existing nodes, with a preference for nodes that already have a higher degree, thus simulating the ‘rich get richer’ phenomenon and producing hubs in the network. In this Barabási-Albert model, the probability that a new node will be connected to a node <inline-formula><mml:math id="inf126"><mml:mi>i</mml:mi></mml:math></inline-formula> depends on the degree <inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, according to the rule:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the sum of degrees of all existing nodes at the time of attachment. Similar to the Erdős-Rényi model, the edges of the resulting binary network were weighted by sampling from a log-normal distribution with parameters <inline-formula><mml:math id="inf129"><mml:mi mathvariant="normal">μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:mi mathvariant="normal">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>. Moreover, to model weakly connected hubs, all hubs in the network were fully connected with weights assigned from a separate log-normal distribution characterized by both mean and standard deviation equal to 0.1. This simulates the scenario where hubs have additional connections among themselves with relatively smaller weights, representing the observed weaker links of the rich-club in the brain.</p></sec><sec id="s4-5-3"><title>Topology-conserving null model</title><p>To investigate the impact of connection strength on network dynamics while maintaining the underlying topology, a weight shuffling procedure was implemented. The adjacency matrix was kept unaltered to preserve the original connectivity pattern, while the weights were redistributed to randomize the strength of the connections. This process was defined as follows:</p><list list-type="simple" id="list1"><list-item><p>Given a weighted adjacency matrix, <inline-formula><mml:math id="inf131"><mml:mi>W</mml:mi></mml:math></inline-formula>, with fixed topology, the set of non-zero weights corresponding to the edges in <inline-formula><mml:math id="inf132"><mml:mi>W</mml:mi></mml:math></inline-formula> was permuted. The permutation was performed in such a manner that each edge received a new weight from the set, ensuring that the sum of the weights and the overall weight distribution remained unchanged. This shuffling procedure was iterated 10 times, yielding 10 distinct weighted networks with identical topologies but randomized weight configurations.</p></list-item></list></sec><sec id="s4-5-4"><title>Weight-conserving null model</title><p>To study the impact of topology, we used a degree-, weight-, and strength-preserving model introduced by <xref ref-type="bibr" rid="bib100">Rubinov and Sporns, 2011</xref> in which the connectivity was shuffled while the total degree and strength of the nodes were preserved. We chose the default parameters provided by the BCTpy toolbox, however, the correlation between the strength sequence of pre- and post-rewired network was <italic>r</italic>=0.97 that implies a robust rewiring. As per <xref ref-type="bibr" rid="bib100">Rubinov and Sporns, 2011</xref>, the rewiring algorithm consists of two steps:</p><list list-type="order" id="list2"><list-item><p>The network is randomized while maintaining each node’s degree using a connection-switching algorithm, here Maslov-Sneppen (<xref ref-type="bibr" rid="bib78">Maslov and Sneppen, 2002</xref>). In this process, connections are adjusted by switching pairs of edges in a way that preserves the sum of weights for the involved nodes.</p></list-item><list-item><p>Original network weights are then reassigned to the randomized network. This reassignment is done by first ranking all weights by magnitude and then associating them with the network’s connections to approximate the original distribution of weights. During this process, weights are iteratively matched and re-ranked until all connections in the new network closely resemble the original positive and negative strengths.</p></list-item></list></sec></sec><sec id="s4-6"><title>Multivariate statistical model</title><p>We conducted two sets of Lasso regularized multivariate regression models (<xref ref-type="fig" rid="fig6">Figure 6B and C</xref>). In one, we systematically explored the dynamics of feature importance in predicting the optimal signal propagation and using the other set, we investigated the number of steps needed to predict the optimal signal propagation.</p><p>For the first set, we constructed a feature matrix <inline-formula><mml:math id="inf133"><mml:mi>X</mml:mi></mml:math></inline-formula> encompassing all communication measures, topological and geodesic distance measures, FC, structural weights, and fiber length between regions.</p><p>A range of 50 <inline-formula><mml:math id="inf134"><mml:mi mathvariant="normal">α</mml:mi></mml:math></inline-formula> (see <xref ref-type="table" rid="table1">Table 1</xref>) values from 0 to 1, delineating the degree of spatial influence, was explored. For each <inline-formula><mml:math id="inf135"><mml:mi>α</mml:mi></mml:math></inline-formula>, the SAR and LAM models were computed and subsequently integrated into the feature matrix <inline-formula><mml:math id="inf136"><mml:mi>X</mml:mi></mml:math></inline-formula>. A total of 25 repetition runs were performed for each <inline-formula><mml:math id="inf137"><mml:mi>α</mml:mi></mml:math></inline-formula>, wherein the dataset was randomly partitioned into training and testing subsets of proportion 0.7–0.3 using a 10-folds cross validation approach. The training set was subjected to standard scaling before model fitting. The statistical model was assessed through the coefficient of determination <inline-formula><mml:math id="inf138"><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> . The absolute values of the regression coefficients obtained from the model were normalized to sum to 100, reflecting the relative contribution of each feature to the model. These contributions were recorded for each <inline-formula><mml:math id="inf139"><mml:mi>α</mml:mi></mml:math></inline-formula> and averaged over all trials to ascertain the consistent predictors.</p><p>For the second set, a range of 15 steps were generated by first raising the structural connectivity matrix to the power of 0–15 and then discounting the longer paths according to the optimal discount factor of the LAM model α. For each step, 100 repetitions were performed following the same cross-validation, evaluation, and feature importance ranking introduced above.</p></sec><sec id="s4-7"><title>Graph-theoretical measures</title><p>Several graph-theoretical measures were employed in this study that are summarized in <xref ref-type="table" rid="table2">Table 2</xref> and briefly explained here.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Graph-theoretical measures.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Types</th><th align="left" valign="bottom">Measures<sup><xref ref-type="table-fn" rid="table2fn1">*</xref></sup></th><th align="left" valign="bottom">References</th></tr></thead><tbody><tr><td align="left" valign="middle">Closeness Centrality</td><td align="left" valign="middle"><inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib89">Oldham et al., 2019</xref></td></tr><tr><td align="left" valign="middle">Eigenvector Centrality</td><td align="left" valign="middle"><inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>W</mml:mi><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib89">Oldham et al., 2019</xref></td></tr><tr><td align="left" valign="middle">Node Betweenness Centrality</td><td align="left" valign="middle"><inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib89">Oldham et al., 2019</xref>; <xref ref-type="bibr" rid="bib136">Zuo et al., 2012</xref></td></tr><tr><td align="left" valign="middle">Edge Betweenness Centrality</td><td align="left" valign="middle"><inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>≠</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib89">Oldham et al., 2019</xref>; <xref ref-type="bibr" rid="bib136">Zuo et al., 2012</xref></td></tr><tr><td align="left" valign="middle">Random Walk Centrality</td><td align="left" valign="middle"><inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>T</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib84">Newman, 2005</xref></td></tr><tr><td align="left" valign="middle">Average Controllability</td><td align="left" valign="middle"><inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>W</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>T</mml:mi><mml:mi>U</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>M</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:mo>∘</mml:mo><mml:mi>U</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>diag</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>diag</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mtext>M</mml:mtext><mml:mi>P</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib54">Gu et al., 2015</xref></td></tr><tr><td align="left" valign="middle">Modal Controllability</td><td align="left" valign="middle"><inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>W</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mi>T</mml:mi><mml:mi>U</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="middle"><xref ref-type="bibr" rid="bib54">Gu et al., 2015</xref></td></tr></tbody></table><table-wrap-foot><fn id="table2fn1"><label>*</label><p>Throughout the table, <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi><mml:mi>ϵ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the adjacency matrix corresponding to a given graph, <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with vertices (nodes) <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> signifies the connection strength from vertex i to vertex j Anatomical connections are indicated by <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for connected region pairs and for <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> unconnected pairs. <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the shortest path distance between s nodes and, t and <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the total number of shortest paths from node s to node t and <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of those paths that pass through node i.In edge betweenness centrality, <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> stands for the number of those paths that pass through a given edge, e. Further, v refers to the left eigenvector associated with the eigenvalue of λ maximum modulus. In computing controllability measures, we used Schur decomposition to find the unitary matrix, <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the upper triangular matrix, <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, to express the adjacency matrix, <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Further, <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes element-wise multiplication, * represents the conjugate transpose, and <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> extracts the diagonal elements.</p></fn></table-wrap-foot></table-wrap><sec id="s4-7-1"><title>Closeness centrality</title><p>Closeness centrality is a measure reflecting the average shortest path length from a given node to all other nodes in the network. It quantifies how ‘close’ a node is to all other nodes, which can indicate the node’s efficiency in spreading information through the network. As with the SPE, the shortest path distance was calculated using the Floyd-Warshall algorithm.</p></sec><sec id="s4-7-2"><title>Eigenvector centrality</title><p>Eigenvector centrality extends the concept of centrality by not only considering the number of connections a node has, but also the centrality of its neighbors (<xref ref-type="bibr" rid="bib89">Oldham et al., 2019</xref>; <xref ref-type="bibr" rid="bib136">Zuo et al., 2012</xref>). It assigns relative scores to all nodes in the network, based on the principle that connections to high-scoring nodes contribute more to the score of the node than equal connections to low-scoring nodes. As detailed in <xref ref-type="table" rid="table2">Table 2</xref>, this quantity at the site of <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-th node is calculated as the <inline-formula><mml:math id="inf162"><mml:mi>i</mml:mi></mml:math></inline-formula>-th component of the eigenvector of the adjacency matrix.</p></sec><sec id="s4-7-3"><title>Shortest path node and edge betweenness centrality</title><p>Shortest path betweenness centrality measures the extent to which a node lies on the shortest paths between other nodes in the network. It captures the influence of a node over the flow of information in the network by identifying nodes that frequently act as bridges along the shortest paths between other nodes (<xref ref-type="bibr" rid="bib44">Freeman, 1977</xref>). Subsequently, the shortest path <italic>edge</italic> betweenness centrality quantifies the number of times an edge acts as a bridge along the shortest path between two nodes. It reflects the importance of an edge in facilitating communication within the network. To elaborate on the computation of these metrics, please refer to <xref ref-type="table" rid="table2">Table 2</xref>.</p></sec><sec id="s4-7-4"><title>Random-walk node and edge betweenness centrality</title><p>Random walk betweenness centrality, also known as current flow betweenness centrality, is also a measure that quantifies the node’s role in facilitating information flow in the network (<xref ref-type="bibr" rid="bib84">Newman, 2005</xref>). However, unlike shortest path betweenness centrality, which only considers the shortest paths, random walk centrality is based on the probability that a random walk starting at a source node will pass through a given node before reaching the destination node. For detailed information on the computation of Random-walk Node and Edge Betweenness Centrality based on the current flow notion, refer to <xref ref-type="table" rid="table2">Table 2</xref>.</p></sec><sec id="s4-7-5"><title>Average and modal controllability</title><p>Average controllability measures the ability of a node to steer the system into many easy-to-reach states, given a discrete dynamical system. It is particularly useful in understanding the ease with which any state of the system can be reached from a given initial state. Similarly, modal controllability represents the ability of a node to steer the system into farther and hard-to-reach states. In computing the controllability measure, we used Schur decomposition to find the unitary matrix, <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the upper triangular matrix, <inline-formula><mml:math id="inf164"><mml:mi>T</mml:mi></mml:math></inline-formula>, to express the adjacency matrix, <inline-formula><mml:math id="inf165"><mml:mi>W</mml:mi></mml:math></inline-formula>. The computation procedure is summarized in <xref ref-type="table" rid="table2">Table 2</xref>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Validation, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Supervision, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-101780-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The simulation results are available at the link below: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10849223">https://zenodo.org/records/10849223</ext-link>. All connectomes are available as a part of the Netneurotools library: <ext-link ext-link-type="uri" xlink:href="https://github.com/netneurolab/netneurotools">https://github.com/netneurolab/netneurotools</ext-link> (copy archived at <xref ref-type="bibr" rid="bib10">Bazinet et al., 2025</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>A general framework for characterizing optimal communication in brain networks</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10849223</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The funding is gratefully acknowledged: KF: German Research Foundation (DFG)-SFB 936–178316478 A1; TRR169-A2; SPP 2041/GO 2888/2–2; and the Templeton World Charity Foundation, Inc under grant TWCF-2022–30510. FH: DFG TRR169-A2. SD: SFB 936–178316478 A1. AM: SFB 936–178316478 A1. BM: the Natural Sciences and Engineering Research Council of Canada (NSERC Discovery Grant RGPIN #017–04265). The Brain Canada Future Leaders Fund and the Canadian Institutes of Health Research (CIHR). CS: N/A; GZ: N/A; CH: SFB 936–178316478 A1; TRR169-A2; SFB 1461 /A4; SPP 2041/HI 1286/7–1, the Human Brain Project, EU (SGA2, SGA3).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdelnour</surname><given-names>F</given-names></name><name><surname>Voss</surname><given-names>HU</given-names></name><name><surname>Raj</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Network diffusion accurately models the relationship between structural and functional brain connectivity networks</article-title><source>NeuroImage</source><volume>90</volume><fpage>335</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.12.039</pub-id><pub-id pub-id-type="pmid">24384152</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human lesion studies in the 21st century</article-title><source>Neuron</source><volume>90</volume><fpage>1151</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.014</pub-id><pub-id pub-id-type="pmid">27311080</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Algaba</surname><given-names>E</given-names></name><name><surname>Fragnelli</surname><given-names>V</given-names></name><name><surname>Sánchez-Soriano</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019a</year><source>Handbook of the Shapley Value, Chapman</source><publisher-loc>Philadelphia, PA</publisher-loc><publisher-name>Chapman &amp; Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/9781351241410</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Algaba</surname><given-names>E</given-names></name><name><surname>Fragnelli</surname><given-names>V</given-names></name><name><surname>Sánchez-Soriano</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019b</year><chapter-title>The shapley value, a paradigm of fairness</chapter-title><person-group person-group-type="editor"><name><surname>Algaba</surname><given-names>E</given-names></name></person-group><source>Handbook of the Shapley Value</source><publisher-name>Chapman and Hall/CRC</publisher-name><fpage>17</fpage><lpage>29</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avena-Koenigsberger</surname><given-names>A</given-names></name><name><surname>Misic</surname><given-names>B</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Communication dynamics in complex brain networks</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>17</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.149</pub-id><pub-id pub-id-type="pmid">29238085</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ay</surname><given-names>N</given-names></name><name><surname>Polani</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Information flows in causal networks</article-title><source>Advances in Complex Systems</source><volume>11</volume><fpage>17</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1142/S0219525908001465</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ay</surname><given-names>N</given-names></name><name><surname>Polani</surname><given-names>D</given-names></name><name><surname>Virgo</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Information decomposition based on cooperative game theory</article-title><source>Kybernetika</source><volume>5</volume><fpage>979</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.14736/kyb-2020-5-0979</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Small-world brain networks revisited</article-title><source>The Neuroscientist</source><volume>23</volume><fpage>499</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1177/1073858416667720</pub-id><pub-id pub-id-type="pmid">27655008</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bazinet</surname><given-names>V</given-names></name><name><surname>Hansen</surname><given-names>JY</given-names></name><name><surname>Misic</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Towards a biologically annotated brain connectome</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>747</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00752-3</pub-id><pub-id pub-id-type="pmid">37848663</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bazinet</surname><given-names>V</given-names></name><name><surname>Markello</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>ZQ</given-names></name><name><surname>Shafiei</surname><given-names>G</given-names></name><name><surname>Hansen</surname><given-names>J</given-names></name><name><surname>Milisav</surname><given-names>F</given-names></name><name><surname>Dominguez</surname><given-names>EC</given-names></name><name><surname>Oudyk</surname><given-names>K</given-names></name><name><surname>Kiar</surname><given-names>G</given-names></name><name><surname>Gensollen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Netneurotools</data-title><version designator="swh:1:rev:02d0c5eb4819064f49b764f6df28e11170e13242">swh:1:rev:02d0c5eb4819064f49b764f6df28e11170e13242</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c5d5341b673d05934a40127baa33cb479e0d4d12;origin=https://github.com/netneurolab/netneurotools;visit=swh:1:snp:2be3374801d4b18c4e017f0564f883e6f8dfb7a3;anchor=swh:1:rev:02d0c5eb4819064f49b764f6df28e11170e13242">https://archive.softwareheritage.org/swh:1:dir:c5d5341b673d05934a40127baa33cb479e0d4d12;origin=https://github.com/netneurolab/netneurotools;visit=swh:1:snp:2be3374801d4b18c4e017f0564f883e6f8dfb7a3;anchor=swh:1:rev:02d0c5eb4819064f49b764f6df28e11170e13242</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bettinardi</surname><given-names>RG</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Karlaftis</surname><given-names>VM</given-names></name><name><surname>Van Hartevelt</surname><given-names>TJ</given-names></name><name><surname>Fernandes</surname><given-names>HM</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Zamora-López</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How structure sculpts function: Unveiling the contribution of anatomical connectivity to the brain’s spontaneous correlation structure</article-title><source>Chaos</source><volume>27</volume><elocation-id>047409</elocation-id><pub-id pub-id-type="doi">10.1063/1.4980099</pub-id><pub-id pub-id-type="pmid">28456160</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Avena-Koenigsberger</surname><given-names>A</given-names></name><name><surname>Goñi</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>de Reus</surname><given-names>MA</given-names></name><name><surname>Griffa</surname><given-names>A</given-names></name><name><surname>Vértes</surname><given-names>PE</given-names></name><name><surname>Mišic</surname><given-names>B</given-names></name><name><surname>Thiran</surname><given-names>JP</given-names></name><name><surname>Hagmann</surname><given-names>P</given-names></name><name><surname>van den Heuvel</surname><given-names>M</given-names></name><name><surname>Zuo</surname><given-names>XN</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Generative models of the human connectome</article-title><source>NeuroImage</source><volume>124</volume><fpage>1054</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.09.041</pub-id><pub-id pub-id-type="pmid">26427642</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Gu</surname><given-names>S</given-names></name><name><surname>Medaglia</surname><given-names>JD</given-names></name><name><surname>Pasqualetti</surname><given-names>F</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Optimally controlling the human connectome: the role of network topology</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>30770</elocation-id><pub-id pub-id-type="doi">10.1038/srep30770</pub-id><pub-id pub-id-type="pmid">27468904</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Specificity and robustness of long-distance connections in weighted, interareal connectomes</article-title><source>PNAS</source><volume>115</volume><fpage>E4880</fpage><lpage>E4889</lpage><pub-id pub-id-type="doi">10.1073/pnas.1720186115</pub-id><pub-id pub-id-type="pmid">29739890</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Griffa</surname><given-names>A</given-names></name><name><surname>Hagmann</surname><given-names>P</given-names></name><name><surname>Mišić</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distance-dependent consensus thresholds for generating group-representative structural brain networks</article-title><source>Network Neuroscience</source><volume>3</volume><fpage>475</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00075</pub-id><pub-id pub-id-type="pmid">30984903</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beul</surname><given-names>SF</given-names></name><name><surname>Goulas</surname><given-names>A</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Comprehensive computational modelling of the development of mammalian cortical connectivity underlying an architectonic type principle</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006550</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006550</pub-id><pub-id pub-id-type="pmid">30475798</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Binmore</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Game Theory: A Very Short Introduction</source><publisher-loc>London, England</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/actrade/9780199218462.001.0001</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic models of large-scale brain activity</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>340</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1038/nn.4497</pub-id><pub-id pub-id-type="pmid">28230845</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buehlmann</surname><given-names>A</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimal information transfer in the cortex through synchronization</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000934</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000934</pub-id><pub-id pub-id-type="pmid">20862355</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullmore</surname><given-names>E</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>186</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1038/nrn2575</pub-id><pub-id pub-id-type="pmid">19190637</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullmore</surname><given-names>E</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The economy of brain network organization</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>336</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1038/nrn3214</pub-id><pub-id pub-id-type="pmid">22498897</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cakan</surname><given-names>C</given-names></name><name><surname>Jajcay</surname><given-names>N</given-names></name><name><surname>Obermayer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>neurolib: a simulation framework for whole-brain neural mass modeling</article-title><source>Cognitive Computation</source><volume>15</volume><fpage>1132</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1007/s12559-021-09931-9</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>JM</given-names></name><name><surname>Davis</surname><given-names>TD</given-names></name><name><surname>Anderson</surname><given-names>DN</given-names></name><name><surname>Arain</surname><given-names>A</given-names></name><name><surname>Inman</surname><given-names>CS</given-names></name><name><surname>Smith</surname><given-names>EH</given-names></name><name><surname>Rolston</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Subsets of cortico-cortical evoked potentials propagate as traveling waves</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.27.534002</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhuri</surname><given-names>R</given-names></name><name><surname>Knoblauch</surname><given-names>K</given-names></name><name><surname>Gariel</surname><given-names>MA</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A large-scale circuit mechanism for hierarchical dynamical processing in the primate cortex</article-title><source>Neuron</source><volume>88</volume><fpage>419</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.008</pub-id><pub-id pub-id-type="pmid">26439530</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Zhou</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Trade-off between multiple constraints enables simultaneous formation of modules and hubs in neural systems</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002937</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002937</pub-id><pub-id pub-id-type="pmid">23505352</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Covert</surname><given-names>IC</given-names></name><name><surname>Lundberg</surname><given-names>SM</given-names></name><name><surname>Lee</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Algorithms to estimate Shapley value feature attributions</article-title><source>Nature Machine Intelligence</source><volume>5</volume><fpage>590</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/s42256-023-00657-x</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Rosen</surname><given-names>BQ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Dynamical differential covariance recovers directional network structure in multiscale neural systems</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2117234119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2117234119</pub-id><pub-id pub-id-type="pmid">35679342</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chessa</surname><given-names>M</given-names></name><name><surname>Hanaki</surname><given-names>N</given-names></name><name><surname>Lardon</surname><given-names>A</given-names></name><name><surname>Yamada</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>Cost of Complexity in Implementing the Shapley Value by Choosing a Proposer through a Bidding Procedure</source><publisher-name>Osaka University</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chklovskii</surname><given-names>DB</given-names></name><name><surname>Schikorski</surname><given-names>T</given-names></name><name><surname>Stevens</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Wiring optimization in cortical circuits</article-title><source>Neuron</source><volume>34</volume><fpage>341</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00679-7</pub-id><pub-id pub-id-type="pmid">11988166</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>NB</given-names></name><name><surname>Krebs</surname><given-names>JR</given-names></name><name><surname>West</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>An Introduction to Behavioural Ecology</source><publisher-loc>Chichester, England</publisher-loc><publisher-name>Wiley-Blackwell</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>Ritter</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The dynamics of resting fluctuations in the brain: metastability and its dynamical cortical core</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>3095</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-03073-5</pub-id><pub-id pub-id-type="pmid">28596608</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Arnatkeviciute</surname><given-names>A</given-names></name><name><surname>Oldham</surname><given-names>S</given-names></name><name><surname>Sabaroedin</surname><given-names>K</given-names></name><name><surname>Rogasch</surname><given-names>NC</given-names></name><name><surname>Aquino</surname><given-names>KM</given-names></name><name><surname>Fornito</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dynamical consequences of regional heterogeneity in the brain’s transcriptional landscape</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabf4752</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abf4752</pub-id><pub-id pub-id-type="pmid">34261652</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dugatkin</surname><given-names>LA</given-names></name><name><surname>Reeve</surname><given-names>HK</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Game Theory and Animal Behavior</source><publisher-loc>New York</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oso/9780195096927.001.0001</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ehrlich</surname><given-names>DA</given-names></name><name><surname>Schneider</surname><given-names>AC</given-names></name><name><surname>Priesemann</surname><given-names>V</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Makkeh</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Measure of the Complexity of Neural Representations Based on Partial Information Decomposition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2209.10438">https://arxiv.org/abs/2209.10438</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ercsey-Ravasz</surname><given-names>M</given-names></name><name><surname>Markov</surname><given-names>NT</given-names></name><name><surname>Lamy</surname><given-names>C</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Knoblauch</surname><given-names>K</given-names></name><name><surname>Toroczkai</surname><given-names>Z</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A predictive network model of cerebral cortical connectivity based on A distance rule</article-title><source>Neuron</source><volume>80</volume><fpage>184</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.036</pub-id><pub-id pub-id-type="pmid">24094111</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Estrada</surname><given-names>E</given-names></name><name><surname>Hatano</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Communicability in complex networks</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>77</volume><elocation-id>036111</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.77.036111</pub-id><pub-id pub-id-type="pmid">18517465</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>MSA: A compact python package for multiperturbation shapley value analysis</data-title><version designator="v0.0.2">v0.0.2</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5636435">https://doi.org/10.5281/zenodo.5636435</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name><name><surname>Hadaeghi</surname><given-names>F</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Causal Influences Decouple From Their Underlying Network Structure In Echo State Networks</article-title><conf-name>2022 International Joint Conference on Neural Networks (IJCNN)</conf-name><conf-loc>Padua, Italy</conf-loc><pub-id pub-id-type="doi">10.1109/IJCNN55064.2022.9892782</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Systematic perturbation of an artificial neural network: A step towards quantifying causal contributions in the brain</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1010250</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010250</pub-id><pub-id pub-id-type="pmid">35714139</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name><name><surname>Dixit</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024a</year><data-title>OI-and-cms</data-title><version designator="swh:1:rev:2bf93a371c21d811ba7da21449da6dfad589ebb7">swh:1:rev:2bf93a371c21d811ba7da21449da6dfad589ebb7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:af67b20ba927557373d4879d9499c5edb4dbdc90;origin=https://github.com/kuffmode/OI-and-CMs;visit=swh:1:snp:9b96977168f06570578a74c833610e2d86cbe383;anchor=swh:1:rev:2bf93a371c21d811ba7da21449da6dfad589ebb7">https://archive.softwareheritage.org/swh:1:dir:af67b20ba927557373d4879d9499c5edb4dbdc90;origin=https://github.com/kuffmode/OI-and-CMs;visit=swh:1:snp:9b96977168f06570578a74c833610e2d86cbe383;anchor=swh:1:rev:2bf93a371c21d811ba7da21449da6dfad589ebb7</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name><name><surname>Dixit</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024b</year><data-title>YANAT</data-title><version designator="swh:1:rev:f5f87dbaee5f7f6e06e4c188b1d1c78627ee9db7">swh:1:rev:f5f87dbaee5f7f6e06e4c188b1d1c78627ee9db7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7ceb8c5fbc9e36316d8853dd42fae747d0a4cde7;origin=https://github.com/kuffmode/YANAT;visit=swh:1:snp:f5a1e67085b011bf154a7f58a2eab9c871f9e090;anchor=swh:1:rev:f5f87dbaee5f7f6e06e4c188b1d1c78627ee9db7">https://archive.softwareheritage.org/swh:1:dir:7ceb8c5fbc9e36316d8853dd42fae747d0a4cde7;origin=https://github.com/kuffmode/YANAT;visit=swh:1:snp:f5a1e67085b011bf154a7f58a2eab9c871f9e090;anchor=swh:1:rev:f5f87dbaee5f7f6e06e4c188b1d1c78627ee9db7</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fakhar</surname><given-names>K</given-names></name><name><surname>Dixit</surname><given-names>S</given-names></name><name><surname>Hadaeghi</surname><given-names>F</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2024">2024c</year><article-title>Downstream network transformations dissociate neural activity from causal functional contributions</article-title><source>Scientific Reports</source><volume>14</volume><elocation-id>2103</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-52423-7</pub-id><pub-id pub-id-type="pmid">38267481</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernández Galán</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>On how network architecture determines the dominant patterns of spontaneous neural activity</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e2148</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0002148</pub-id><pub-id pub-id-type="pmid">18478091</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>A set of measures of centrality based on betweenness</article-title><source>Sociometry</source><volume>40</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.2307/3033543</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rhythms for cognition: communication through coherence</article-title><source>Neuron</source><volume>88</volume><fpage>220</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.034</pub-id><pub-id pub-id-type="pmid">26447583</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>GC</given-names></name><name><surname>Lesne</surname><given-names>A</given-names></name><name><surname>Hütt</surname><given-names>MT</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Building blocks of self-sustained activity in a simple deterministic model of excitable neural networks</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>50</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00050</pub-id><pub-id pub-id-type="pmid">22888317</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>R</given-names></name><name><surname>Lerman</surname><given-names>K</given-names></name><name><surname>Surachawala</surname><given-names>T</given-names></name><name><surname>Voevodski</surname><given-names>K</given-names></name><name><surname>Teng</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Non-conservative diffusion and its application to social network analysis</article-title><source>Journal of Complex Networks</source><volume>12</volume><elocation-id>cnae006</elocation-id><pub-id pub-id-type="doi">10.1093/comnet/cnae006</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goñi</surname><given-names>J</given-names></name><name><surname>Avena-Koenigsberger</surname><given-names>A</given-names></name><name><surname>Velez de Mendizabal</surname><given-names>N</given-names></name><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Exploring the morphospace of communication efficiency in complex networks</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e58070</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0058070</pub-id><pub-id pub-id-type="pmid">23505455</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goulas</surname><given-names>A</given-names></name><name><surname>Schaefer</surname><given-names>A</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The strength of weak connections in the macaque cortico-cortical network</article-title><source>Brain Structure and Function</source><volume>220</volume><fpage>2939</fpage><lpage>2951</lpage><pub-id pub-id-type="doi">10.1007/s00429-014-0836-3</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gould</surname><given-names>SJ</given-names></name><name><surname>Lewontin</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>The spandrels of San Marco and the Panglossian paradigm: a critique of the adaptationist programme</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>205</volume><fpage>581</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1098/rspb.1979.0086</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grayson</surname><given-names>DS</given-names></name><name><surname>Bliss-Moreau</surname><given-names>E</given-names></name><name><surname>Machado</surname><given-names>CJ</given-names></name><name><surname>Bennett</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>K</given-names></name><name><surname>Grant</surname><given-names>KA</given-names></name><name><surname>Fair</surname><given-names>DA</given-names></name><name><surname>Amaral</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The rhesus monkey connectome predicts disrupted functional networks resulting from pharmacogenetic inactivation of the amygdala</article-title><source>Neuron</source><volume>91</volume><fpage>453</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.005</pub-id><pub-id pub-id-type="pmid">27477019</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Griffa</surname><given-names>A</given-names></name><name><surname>Alemán-Gómez</surname><given-names>Y</given-names></name><name><surname>Hagmann</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Structural and functional connectome from 70 young healthy adults</data-title><version designator="v1">v1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.2872624">https://doi.org/10.5281/zenodo.2872624</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffa</surname><given-names>A</given-names></name><name><surname>Mach</surname><given-names>M</given-names></name><name><surname>Dedelley</surname><given-names>J</given-names></name><name><surname>Gutierrez-Barragan</surname><given-names>D</given-names></name><name><surname>Gozzi</surname><given-names>A</given-names></name><name><surname>Allali</surname><given-names>G</given-names></name><name><surname>Grandjean</surname><given-names>J</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name><name><surname>Amico</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Evidence for increased parallel information transmission in human brain networks compared to macaques and male mice</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>8216</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-43971-z</pub-id><pub-id pub-id-type="pmid">38081838</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>S</given-names></name><name><surname>Pasqualetti</surname><given-names>F</given-names></name><name><surname>Cieslak</surname><given-names>M</given-names></name><name><surname>Telesford</surname><given-names>QK</given-names></name><name><surname>Yu</surname><given-names>AB</given-names></name><name><surname>Kahn</surname><given-names>AE</given-names></name><name><surname>Medaglia</surname><given-names>JD</given-names></name><name><surname>Vettel</surname><given-names>JM</given-names></name><name><surname>Miller</surname><given-names>MB</given-names></name><name><surname>Grafton</surname><given-names>ST</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Controllability of structural brain networks</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8414</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9414</pub-id><pub-id pub-id-type="pmid">26423222</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gul</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Bargaining foundations of shapely value</article-title><source>Econometrica</source><volume>57</volume><elocation-id>81</elocation-id><pub-id pub-id-type="doi">10.2307/1912573</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulyás</surname><given-names>A</given-names></name><name><surname>Bíró</surname><given-names>JJ</given-names></name><name><surname>Kőrösi</surname><given-names>A</given-names></name><name><surname>Rétvári</surname><given-names>G</given-names></name><name><surname>Krioukov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Navigable networks as Nash equilibria of navigation games</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>7651</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms8651</pub-id><pub-id pub-id-type="pmid">26138277</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutknecht</surname><given-names>AJ</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Makkeh</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bits and pieces: understanding information decomposition from part-whole relationships and formal logic</article-title><source>Proceedings. Mathematical, Physical, and Engineering Sciences</source><volume>477</volume><elocation-id>20210110</elocation-id><pub-id pub-id-type="doi">10.1098/rspa.2021.0110</pub-id><pub-id pub-id-type="pmid">35197799</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hagberg</surname><given-names>A</given-names></name><name><surname>Swart</surname><given-names>PJ</given-names></name><name><surname>Schult</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Exploring network structure, dynamics, and function using NetworkX (no.LA-UR-08-05495; LA-UR-08-5495)</article-title><conf-name>Conference: Exploring network structure, dynamics, and function using NetworkX</conf-name><pub-id pub-id-type="doi">10.25080/TCWV9851</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harriger</surname><given-names>L</given-names></name><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rich club organization of macaque cerebral cortex and its role in network communication</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e46497</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0046497</pub-id><pub-id pub-id-type="pmid">23029538</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Vallines</surname><given-names>I</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of temporal receptive windows in human cortex</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>2539</fpage><lpage>2550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5487-07.2008</pub-id><pub-id pub-id-type="pmid">18322098</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joutsa</surname><given-names>J</given-names></name><name><surname>Lipsman</surname><given-names>N</given-names></name><name><surname>Horn</surname><given-names>A</given-names></name><name><surname>Cosgrove</surname><given-names>GR</given-names></name><name><surname>Fox</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The return of the lesion for localization and therapy</article-title><source>Brain</source><volume>146</volume><fpage>3146</fpage><lpage>3155</lpage><pub-id pub-id-type="doi">10.1093/brain/awad123</pub-id><pub-id pub-id-type="pmid">37040563</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>M</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Nonoptimal component placement, but short processing paths, due to long-distance projections in neural systems</article-title><source>PLOS Computational Biology</source><volume>2</volume><elocation-id>e95</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020095</pub-id><pub-id pub-id-type="pmid">16848638</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>A new status index derived from sociometric analysis</article-title><source>Psychometrika</source><volume>18</volume><fpage>39</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1007/BF02289026</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keinan</surname><given-names>A</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Meilijson</surname><given-names>I</given-names></name><name><surname>Ruppin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004a</year><article-title>Causal localization of neural function: the Shapley value method</article-title><source>Neurocomputing</source><volume>58–60</volume><fpage>215</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2004.01.046</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keinan</surname><given-names>A</given-names></name><name><surname>Sandbank</surname><given-names>B</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Meilijson</surname><given-names>I</given-names></name><name><surname>Ruppin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004b</year><article-title>Fair attribution of functional contribution in artificial and biological networks</article-title><source>Neural Computation</source><volume>16</volume><fpage>1887</fpage><lpage>1915</lpage><pub-id pub-id-type="doi">10.1162/0899766041336387</pub-id><pub-id pub-id-type="pmid">15265327</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>CJ</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Entz</surname><given-names>L</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name><name><surname>Kelly</surname><given-names>C</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Intrinsic functional architecture predicts electrically evoked responses in the human brain</article-title><source>PNAS</source><volume>108</volume><fpage>10308</fpage><lpage>10313</lpage><pub-id pub-id-type="doi">10.1073/pnas.1019750108</pub-id><pub-id pub-id-type="pmid">21636787</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Moon</surname><given-names>HS</given-names></name><name><surname>Vo</surname><given-names>TT</given-names></name><name><surname>Kim</surname><given-names>CH</given-names></name><name><surname>Im</surname><given-names>GH</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Choi</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>SG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Whole-brain mapping of effective connectivity by fMRI with cortex-wide patterned optogenetics</article-title><source>Neuron</source><volume>111</volume><fpage>1732</fpage><lpage>1747</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.002</pub-id><pub-id pub-id-type="pmid">37001524</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Laasch</surname><given-names>N</given-names></name><name><surname>Braun</surname><given-names>W</given-names></name><name><surname>Knoff</surname><given-names>L</given-names></name><name><surname>Bielecki</surname><given-names>J</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Estimating Effective Connectivity in Neural Networks: Comparison of Derivative-Based and Correlation-Based Methods</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.02.05.578871</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latora</surname><given-names>V</given-names></name><name><surname>Marchiori</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficient behavior of small-world networks</article-title><source>Physical Review Letters</source><volume>87</volume><elocation-id>198701</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.87.198701</pub-id><pub-id pub-id-type="pmid">11690461</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Communication in neuronal networks</article-title><source>Science</source><volume>301</volume><fpage>1870</fpage><lpage>1874</lpage><pub-id pub-id-type="doi">10.1126/science.1089662</pub-id><pub-id pub-id-type="pmid">14512617</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The cost of cortical computation</article-title><source>Current Biology</source><volume>13</volume><fpage>493</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(03)00135-0</pub-id><pub-id pub-id-type="pmid">12646132</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Unified Approach to Interpreting Model Predictions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.07874">https://arxiv.org/abs/1705.07874</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Mediano</surname><given-names>PAM</given-names></name><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Holland</surname><given-names>N</given-names></name><name><surname>Fryer</surname><given-names>TD</given-names></name><name><surname>O’Brien</surname><given-names>JT</given-names></name><name><surname>Rowe</surname><given-names>JB</given-names></name><name><surname>Menon</surname><given-names>DK</given-names></name><name><surname>Bor</surname><given-names>D</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A synergistic core for human brain evolution and cognition</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>771</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01070-0</pub-id><pub-id pub-id-type="pmid">35618951</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Mediano</surname><given-names>PAM</given-names></name><name><surname>Menon</surname><given-names>DK</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Information decomposition and the informational architecture of the brain</article-title><source>Trends in Cognitive Sciences</source><volume>28</volume><fpage>352</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2023.11.005</pub-id><pub-id pub-id-type="pmid">38199949</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynn</surname><given-names>CW</given-names></name><name><surname>Holmes</surname><given-names>CM</given-names></name><name><surname>Palmer</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Heavy-tailed neuronal connectivity arises from Hebbian self-organization</article-title><source>Nature Physics</source><volume>20</volume><fpage>484</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1038/s41567-023-02332-9</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malherbe</surname><given-names>C</given-names></name><name><surname>Cheng</surname><given-names>B</given-names></name><name><surname>Königsberg</surname><given-names>A</given-names></name><name><surname>Cho</surname><given-names>TH</given-names></name><name><surname>Ebinger</surname><given-names>M</given-names></name><name><surname>Endres</surname><given-names>M</given-names></name><name><surname>Fiebach</surname><given-names>JB</given-names></name><name><surname>Fiehler</surname><given-names>J</given-names></name><name><surname>Galinovic</surname><given-names>I</given-names></name><name><surname>Puig</surname><given-names>J</given-names></name><name><surname>Thijs</surname><given-names>V</given-names></name><name><surname>Lemmens</surname><given-names>R</given-names></name><name><surname>Muir</surname><given-names>KW</given-names></name><name><surname>Nighoghossian</surname><given-names>N</given-names></name><name><surname>Pedraza</surname><given-names>S</given-names></name><name><surname>Simonsen</surname><given-names>CZ</given-names></name><name><surname>Wouters</surname><given-names>A</given-names></name><name><surname>Gerloff</surname><given-names>C</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Thomalla</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Game-theoretical mapping of fundamental brain functions based on lesion deficits in acute stroke</article-title><source>Brain Communications</source><volume>3</volume><elocation-id>fcab204</elocation-id><pub-id pub-id-type="doi">10.1093/braincomms/fcab204</pub-id><pub-id pub-id-type="pmid">34585140</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>NT</given-names></name><name><surname>Ercsey-Ravasz</surname><given-names>M</given-names></name><name><surname>Lamy</surname><given-names>C</given-names></name><name><surname>Ribeiro Gomes</surname><given-names>AR</given-names></name><name><surname>Magrou</surname><given-names>L</given-names></name><name><surname>Misery</surname><given-names>P</given-names></name><name><surname>Giroud</surname><given-names>P</given-names></name><name><surname>Barone</surname><given-names>P</given-names></name><name><surname>Dehay</surname><given-names>C</given-names></name><name><surname>Toroczkai</surname><given-names>Z</given-names></name><name><surname>Knoblauch</surname><given-names>K</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The role of long-range connections on the specificity of the macaque interareal cortical network</article-title><source>PNAS</source><volume>110</volume><fpage>5187</fpage><lpage>5192</lpage><pub-id pub-id-type="doi">10.1073/pnas.1218972110</pub-id><pub-id pub-id-type="pmid">23479610</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maslov</surname><given-names>S</given-names></name><name><surname>Sneppen</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Specificity and stability in topology of protein networks</article-title><source>Science</source><volume>296</volume><fpage>910</fpage><lpage>913</lpage><pub-id pub-id-type="doi">10.1126/science.1065103</pub-id><pub-id pub-id-type="pmid">11988575</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messé</surname><given-names>A</given-names></name><name><surname>Rudrauf</surname><given-names>D</given-names></name><name><surname>Benali</surname><given-names>H</given-names></name><name><surname>Marrelec</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Relating structure and function in the human brain: relative contributions of anatomy, stationary dynamics, and non-stationarities</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003530</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003530</pub-id><pub-id pub-id-type="pmid">24651524</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messé</surname><given-names>A</given-names></name><name><surname>Hütt</surname><given-names>MT</given-names></name><name><surname>König</surname><given-names>P</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>A closer look at the apparent correlation of structural and functional connectivity in excitable neural networks</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>7870</elocation-id><pub-id pub-id-type="doi">10.1038/srep07870</pub-id><pub-id pub-id-type="pmid">25598302</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messé</surname><given-names>A</given-names></name><name><surname>Rudrauf</surname><given-names>D</given-names></name><name><surname>Giron</surname><given-names>A</given-names></name><name><surname>Marrelec</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Predicting functional connectivity from structural connectivity via computational models using MRI: an extensive comparison study</article-title><source>NeuroImage</source><volume>111</volume><fpage>65</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.02.001</pub-id><pub-id pub-id-type="pmid">25682944</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messé</surname><given-names>A</given-names></name><name><surname>Hollensteiner</surname><given-names>KJ</given-names></name><name><surname>Delettre</surname><given-names>C</given-names></name><name><surname>Dell-Brown</surname><given-names>L-A</given-names></name><name><surname>Pieper</surname><given-names>F</given-names></name><name><surname>Nentwig</surname><given-names>LJ</given-names></name><name><surname>Galindo-Leon</surname><given-names>EE</given-names></name><name><surname>Larrat</surname><given-names>B</given-names></name><name><surname>Mériaux</surname><given-names>S</given-names></name><name><surname>Mangin</surname><given-names>J-F</given-names></name><name><surname>Reillo</surname><given-names>I</given-names></name><name><surname>de Juan Romero</surname><given-names>C</given-names></name><name><surname>Borrell</surname><given-names>V</given-names></name><name><surname>Engler</surname><given-names>G</given-names></name><name><surname>Toro</surname><given-names>R</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Structural basis of envelope and phase intrinsic coupling modes in the cerebral cortex</article-title><source>NeuroImage</source><volume>276</volume><elocation-id>120212</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120212</pub-id><pub-id pub-id-type="pmid">37269959</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>L</given-names></name><name><surname>Chavane</surname><given-names>F</given-names></name><name><surname>Reynolds</surname><given-names>J</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical travelling waves: mechanisms and computational principles</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>255</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1038/nrn.2018.20</pub-id><pub-id pub-id-type="pmid">29563572</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newman</surname><given-names>MEJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A measure of betweenness centrality based on random walks</article-title><source>Social Networks</source><volume>27</volume><fpage>39</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.socnet.2004.11.009</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novelli</surname><given-names>L</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inferring network properties from time series using transfer entropy and mutual information: Validation of multivariate versus bivariate approaches</article-title><source>Network Neuroscience</source><volume>5</volume><fpage>373</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00178</pub-id><pub-id pub-id-type="pmid">34189370</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozari</surname><given-names>E</given-names></name><name><surname>Bertolero</surname><given-names>MA</given-names></name><name><surname>Stiso</surname><given-names>J</given-names></name><name><surname>Caciagli</surname><given-names>L</given-names></name><name><surname>Cornblath</surname><given-names>EJ</given-names></name><name><surname>He</surname><given-names>X</given-names></name><name><surname>Mahadevan</surname><given-names>AS</given-names></name><name><surname>Pappas</surname><given-names>GJ</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Macroscopic resting-state brain dynamics are best described by linear models</article-title><source>Nature Biomedical Engineering</source><volume>8</volume><fpage>68</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/s41551-023-01117-y</pub-id><pub-id pub-id-type="pmid">38082179</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Byrne</surname><given-names>J</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>How critical is brain criticality?</article-title><source>Trends in Neurosciences</source><volume>45</volume><fpage>820</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2022.08.007</pub-id><pub-id pub-id-type="pmid">36096888</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Winslow</surname><given-names>B</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Henry</surname><given-names>AM</given-names></name><name><surname>Mortrud</surname><given-names>MT</given-names></name><name><surname>Ouellette</surname><given-names>B</given-names></name><name><surname>Nguyen</surname><given-names>TN</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Slaughterbeck</surname><given-names>CR</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Nicholas</surname><given-names>E</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Joines</surname><given-names>KM</given-names></name><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Hawrylycz</surname><given-names>MJ</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Hohmann</surname><given-names>JG</given-names></name><name><surname>Wohnoutka</surname><given-names>P</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A mesoscale connectome of the mouse brain</article-title><source>Nature</source><volume>508</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1038/nature13186</pub-id><pub-id pub-id-type="pmid">24695228</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldham</surname><given-names>S</given-names></name><name><surname>Fulcher</surname><given-names>B</given-names></name><name><surname>Parkes</surname><given-names>L</given-names></name><name><surname>Arnatkevic Iūtė</surname><given-names>A</given-names></name><name><surname>Suo</surname><given-names>C</given-names></name><name><surname>Fornito</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Consistency and differences between centrality measures across distinct classes of networks</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0220061</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0220061</pub-id><pub-id pub-id-type="pmid">31348798</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padamsey</surname><given-names>Z</given-names></name><name><surname>Rochefort</surname><given-names>NL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Paying the brain’s energy bill</article-title><source>Current Opinion in Neurobiology</source><volume>78</volume><elocation-id>102668</elocation-id><pub-id pub-id-type="doi">10.1016/j.conb.2022.102668</pub-id><pub-id pub-id-type="pmid">36571958</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panda</surname><given-names>R</given-names></name><name><surname>López-González</surname><given-names>A</given-names></name><name><surname>Gilson</surname><given-names>M</given-names></name><name><surname>Gosseries</surname><given-names>O</given-names></name><name><surname>Thibaut</surname><given-names>A</given-names></name><name><surname>Frasso</surname><given-names>G</given-names></name><name><surname>Cecconi</surname><given-names>B</given-names></name><name><surname>Escrichs</surname><given-names>A</given-names></name><name><surname>Collaborators</surname><given-names>CSG</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name><name><surname>Zamora-López</surname><given-names>G</given-names></name><name><surname>Annen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Whole-brain analyses indicate the impairment of posterior integration and thalamo-frontotemporal broadcasting in disorders of consciousness</article-title><source>Human Brain Mapping</source><volume>44</volume><fpage>4352</fpage><lpage>4371</lpage><pub-id pub-id-type="doi">10.1002/hbm.26386</pub-id><pub-id pub-id-type="pmid">37254960</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Parkes</surname><given-names>L</given-names></name><name><surname>Kim</surname><given-names>JZ</given-names></name><name><surname>Stiso</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Nctpy: network control theory for python</data-title><version designator="1.0.0">1.0.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7383161">https://doi.org/10.5281/zenodo.7383161</ext-link></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-Castrillo</surname><given-names>D</given-names></name><name><surname>Wettstein</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Bidding for the surplus : a non-cooperative approach to the shapley value</article-title><source>Journal of Economic Theory</source><volume>100</volume><fpage>274</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1006/jeth.2000.2704</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Barnes</surname><given-names>KA</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title><source>NeuroImage</source><volume>59</volume><fpage>2142</fpage><lpage>2154</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id><pub-id pub-id-type="pmid">22019881</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pronold</surname><given-names>J</given-names></name><name><surname>van Meegen</surname><given-names>A</given-names></name><name><surname>Shimoura</surname><given-names>RO</given-names></name><name><surname>Vollenbröker</surname><given-names>H</given-names></name><name><surname>Senden</surname><given-names>M</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Bakker</surname><given-names>R</given-names></name><name><surname>van Albada</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Multi-scale spiking network model of human cerebral cortex</article-title><source>Cerebral Cortex</source><volume>34</volume><elocation-id>bhae409</elocation-id><pub-id pub-id-type="doi">10.1093/cercor/bhae409</pub-id><pub-id pub-id-type="pmid">39428578</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rabuffo</surname><given-names>G</given-names></name><name><surname>Lokossou</surname><given-names>HA</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Ziaee-Mehr</surname><given-names>A</given-names></name><name><surname>Hashemi</surname><given-names>M</given-names></name><name><surname>Quilichini</surname><given-names>PP</given-names></name><name><surname>Ghestem</surname><given-names>A</given-names></name><name><surname>Arab</surname><given-names>O</given-names></name><name><surname>Esclapez</surname><given-names>M</given-names></name><name><surname>Verma</surname><given-names>P</given-names></name><name><surname>Raj</surname><given-names>A</given-names></name><name><surname>Gozzi</surname><given-names>A</given-names></name><name><surname>Sorrentino</surname><given-names>P</given-names></name><name><surname>Chuang</surname><given-names>KH</given-names></name><name><surname>Perles-Barbacaru</surname><given-names>TA</given-names></name><name><surname>Viola</surname><given-names>A</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>Bernard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>On Global Brain Reconfiguration after Local Manipulations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.09.08.556815</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>LN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Causal selection and the pathway concept</article-title><source>Philosophy of Science</source><volume>85</volume><fpage>551</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1086/699022</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>The Shapley Value: Essays in Honor of Lloyd S. Shapley</source><publisher-loc>Cambridge, England</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511528446</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Complex network measures of brain connectivity: uses and interpretations</article-title><source>NeuroImage</source><volume>52</volume><fpage>1059</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.003</pub-id><pub-id pub-id-type="pmid">19819337</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Weight-conserving characterization of complex functional brain networks</article-title><source>NeuroImage</source><volume>56</volume><fpage>2068</fpage><lpage>2079</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.069</pub-id><pub-id pub-id-type="pmid">21459148</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Ypma</surname><given-names>RJF</given-names></name><name><surname>Watson</surname><given-names>C</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Wiring cost and topological participation of the mouse brain connectome</article-title><source>PNAS</source><volume>112</volume><fpage>10032</fpage><lpage>10037</lpage><pub-id pub-id-type="doi">10.1073/pnas.1420315112</pub-id><pub-id pub-id-type="pmid">26216962</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Constraints and spandrels of interareal connectomes</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13812</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13812</pub-id><pub-id pub-id-type="pmid">27924867</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Safavi</surname><given-names>S</given-names></name><name><surname>Chalk</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Levina</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Signatures of criticality in efficient coding networks</article-title><source>PNAS</source><volume>121</volume><elocation-id>e2302730121</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2302730121</pub-id><pub-id pub-id-type="pmid">39352933</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Samoylenko</surname><given-names>I</given-names></name><name><surname>Aleja</surname><given-names>D</given-names></name><name><surname>Primo</surname><given-names>E</given-names></name><name><surname>Alfaro-Bittner</surname><given-names>K</given-names></name><name><surname>Vasilyeva</surname><given-names>E</given-names></name><name><surname>Kovalenko</surname><given-names>K</given-names></name><name><surname>Musatov</surname><given-names>D</given-names></name><name><surname>Raigorodskii</surname><given-names>AM</given-names></name><name><surname>Criado</surname><given-names>R</given-names></name><name><surname>Romance</surname><given-names>M</given-names></name><name><surname>Papo</surname><given-names>D</given-names></name><name><surname>Perc</surname><given-names>M</given-names></name><name><surname>Barzel</surname><given-names>B</given-names></name><name><surname>Boccaletti</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Why Are There Six Degrees of Separation in a Social Network?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2211.09463">https://arxiv.org/abs/2211.09463</ext-link></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seguin</surname><given-names>C</given-names></name><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Navigation of brain networks</article-title><source>PNAS</source><volume>115</volume><fpage>6297</fpage><lpage>6302</lpage><pub-id pub-id-type="doi">10.1073/pnas.1801351115</pub-id><pub-id pub-id-type="pmid">29848631</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seguin</surname><given-names>C</given-names></name><name><surname>Razi</surname><given-names>A</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Inferring neural signalling directionality from undirected structural connectomes</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4289</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12201-w</pub-id><pub-id pub-id-type="pmid">31537787</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seguin</surname><given-names>C</given-names></name><name><surname>Tian</surname><given-names>Y</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Network communication models improve the behavioral and functional predictive utility of the human structural connectome</article-title><source>Network Neuroscience</source><volume>4</volume><fpage>980</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00161</pub-id><pub-id pub-id-type="pmid">33195945</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Seguin</surname><given-names>C</given-names></name><name><surname>Jedynak</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>O</given-names></name><name><surname>Mansour L</surname><given-names>S</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Communication Dynamics in the Human Connectome Shape the Cortex-Wide Propagation of Direct Electrical Stimulation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.07.05.498875</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Seguin</surname><given-names>C</given-names></name><name><surname>Mansour L</surname><given-names>S</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name><name><surname>Calamante</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Network Communication Models Narrow the Gap between the Modular Organization of Structural and Functional Brain Networks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.02.18.480871</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seguin</surname><given-names>C</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Brain network communication: concepts, models and applications</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>557</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00718-5</pub-id><pub-id pub-id-type="pmid">37438433</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Dold</surname><given-names>D</given-names></name><name><surname>Kungl</surname><given-names>AF</given-names></name><name><surname>Ellenberger</surname><given-names>B</given-names></name><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Petrovici</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A neuronal least-action principle for real-time learning in cortical circuits</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.25.534198</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Shafiei</surname><given-names>G</given-names></name><name><surname>Markello</surname><given-names>R</given-names></name><name><surname>Talpalaru</surname><given-names>T</given-names></name><name><surname>Makowski</surname><given-names>C</given-names></name><name><surname>Kirschner</surname><given-names>M</given-names></name><name><surname>Devenyi</surname><given-names>GA</given-names></name><name><surname>Hagmann</surname><given-names>P</given-names></name><name><surname>Cashman</surname><given-names>NR</given-names></name><name><surname>Lepage</surname><given-names>M</given-names></name><name><surname>Chakravarty</surname><given-names>MM</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>Mišić</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Consensus structural and functional connectome from 70 young healthy adults</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3067849">https://doi.org/10.5281/zenodo.3067849</ext-link></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shafiei</surname><given-names>G</given-names></name><name><surname>Markello</surname><given-names>RD</given-names></name><name><surname>Makowski</surname><given-names>C</given-names></name><name><surname>Talpalaru</surname><given-names>A</given-names></name><name><surname>Kirschner</surname><given-names>M</given-names></name><name><surname>Devenyi</surname><given-names>GA</given-names></name><name><surname>Guma</surname><given-names>E</given-names></name><name><surname>Hagmann</surname><given-names>P</given-names></name><name><surname>Cashman</surname><given-names>NR</given-names></name><name><surname>Lepage</surname><given-names>M</given-names></name><name><surname>Chakravarty</surname><given-names>MM</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>Mišić</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatial patterning of tissue volume loss in schizophrenia reflects brain network architecture</article-title><source>Biological Psychiatry</source><volume>87</volume><fpage>727</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2019.09.031</pub-id><pub-id pub-id-type="pmid">31837746</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shapley</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1997">1997</year><chapter-title>A value for n-person games. contributions to the theory of games</chapter-title><person-group person-group-type="editor"><name><surname>Kuhn</surname><given-names>HW</given-names></name></person-group><source>Classics in Game Theory</source><publisher-name>Princeton University Press</publisher-name><fpage>307</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1515/9781400829156-012</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siddiqi</surname><given-names>SH</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Fox</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Causal mapping of human brain function</article-title><source>Nature Reviews. Neuroscience</source><volume>23</volume><fpage>361</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/s41583-022-00583-8</pub-id><pub-id pub-id-type="pmid">35444305</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Chialvo</surname><given-names>DR</given-names></name><name><surname>Kaiser</surname><given-names>M</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Organization, development and function of complex brain networks</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>418</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.07.008</pub-id><pub-id pub-id-type="pmid">15350243</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Graph theory methods: applications in brain networks</article-title><source>Dialogues in Clinical Neuroscience</source><volume>20</volume><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.31887/DCNS.2018.20.2/osporns</pub-id><pub-id pub-id-type="pmid">30250388</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suárez</surname><given-names>LE</given-names></name><name><surname>Markello</surname><given-names>RD</given-names></name><name><surname>Betzel</surname><given-names>RF</given-names></name><name><surname>Misic</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Linking structure and function in macroscale brain networks</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>302</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.01.008</pub-id><pub-id pub-id-type="pmid">32160567</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>A measure for brain complexity: relating functional segregation and integration in the nervous system</article-title><source>PNAS</source><volume>91</volume><fpage>5033</fpage><lpage>5037</lpage><pub-id pub-id-type="doi">10.1073/pnas.91.11.5033</pub-id><pub-id pub-id-type="pmid">8197179</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Travers</surname><given-names>J</given-names></name><name><surname>Milgram</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1977">1977</year><chapter-title>An experimental study of the small world problem</chapter-title><person-group person-group-type="editor"><name><surname>Leinhardt</surname><given-names>S</given-names></name></person-group><source>Social Networks</source><publisher-name>Elsevier</publisher-name><fpage>179</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-442450-0.50018-3</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaidya</surname><given-names>AR</given-names></name><name><surname>Pujara</surname><given-names>MS</given-names></name><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Fellows</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Lesion studies in contemporary neuroscience</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>653</fpage><lpage>671</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.05.009</pub-id><pub-id pub-id-type="pmid">31279672</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Mandl</surname><given-names>RCW</given-names></name><name><surname>Stam</surname><given-names>CJ</given-names></name><name><surname>Kahn</surname><given-names>RS</given-names></name><name><surname>Hulshoff Pol</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Aberrant frontal and temporal complex network structure in schizophrenia: a graph theoretical analysis</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>15915</fpage><lpage>15926</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2874-10.2010</pub-id><pub-id pub-id-type="pmid">21106830</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Rich-club organization of the human connectome</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>15775</fpage><lpage>15786</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3539-11.2011</pub-id><pub-id pub-id-type="pmid">22049421</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Heuvel</surname><given-names>MP</given-names></name><name><surname>Kahn</surname><given-names>RS</given-names></name><name><surname>Goñi</surname><given-names>J</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>High-cost, high-capacity backbone for global brain communication</article-title><source>PNAS</source><volume>109</volume><fpage>11372</fpage><lpage>11377</lpage><pub-id pub-id-type="doi">10.1073/pnas.1203593109</pub-id><pub-id pub-id-type="pmid">22711833</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Uran</surname><given-names>C</given-names></name><name><surname>Spyropoulos</surname><given-names>G</given-names></name><name><surname>Onorato</surname><given-names>I</given-names></name><name><surname>Broggini</surname><given-names>AC</given-names></name><name><surname>Schneider</surname><given-names>M</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Principles of large-scale neural interactions</article-title><source>Neuron</source><volume>111</volume><fpage>987</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.015</pub-id><pub-id pub-id-type="pmid">37023720</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>IE</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The influence of wiring economy on nervous system evolution</article-title><source>Current Biology</source><volume>26</volume><fpage>R1101</fpage><lpage>R1108</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.053</pub-id><pub-id pub-id-type="pmid">27780051</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Woodward</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Making Things Happen: A Theory of Causal Explanation, Oxford Studies in the Philosophy of Science</source><publisher-loc>New York, NY</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/0195155270.001.0001</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>MP</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name><name><surname>Scannell</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>On imputing function to structure from the behavioural effects of brain lesions</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>355</volume><fpage>147</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1098/rstb.2000.0555</pub-id><pub-id pub-id-type="pmid">10703050</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zamora-López</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>C</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical hubs form a module for multisensory integration on top of the hierarchy of cortical networks</article-title><source>Frontiers in Neuroinformatics</source><volume>4</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.001.2010</pub-id><pub-id pub-id-type="pmid">20428515</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zamora-López</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>C</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Exploring brain function from anatomical connectivity</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><elocation-id>83</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2011.00083</pub-id><pub-id pub-id-type="pmid">21734863</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zamora-López</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Zhou</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional complexity emerging from anatomical constraints in the brain: the significance of network modularity and rich-clubs</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>38424</elocation-id><pub-id pub-id-type="doi">10.1038/srep38424</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zamora-López</surname><given-names>G</given-names></name><name><surname>Gilson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>An integrative dynamical perspective for graph theory and the analysis of complex networks</article-title><source>Chaos</source><volume>34</volume><elocation-id>041501</elocation-id><pub-id pub-id-type="doi">10.1063/5.0202241</pub-id><pub-id pub-id-type="pmid">38625080</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zavaglia</surname><given-names>M</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal functional contributions and interactions in the attention network of the brain: an objective multi-perturbation analysis</article-title><source>Brain Structure and Function</source><volume>221</volume><fpage>2553</fpage><lpage>2568</lpage><pub-id pub-id-type="doi">10.1007/s00429-015-1058-z</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zavaglia</surname><given-names>M</given-names></name><name><surname>Malherbe</surname><given-names>C</given-names></name><name><surname>Schlaadt</surname><given-names>S</given-names></name><name><surname>Nachev</surname><given-names>P</given-names></name><name><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Ground-Truth Validation of Uni- and Multivariate Lesion Inference Approaches</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.28.534534</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Network landscape from a Brownian particle’s perspective</article-title><source>Physical Review E</source><volume>67</volume><elocation-id>041908</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.67.041908</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuo</surname><given-names>XN</given-names></name><name><surname>Ehmke</surname><given-names>R</given-names></name><name><surname>Mennes</surname><given-names>M</given-names></name><name><surname>Imperati</surname><given-names>D</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Network centrality in the human functional connectome</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>1862</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr269</pub-id><pub-id pub-id-type="pmid">21968567</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101780.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schneider-Mizell</surname><given-names>Casey M</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Allen Institute for Brain Science</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The authors provide a <bold>compelling</bold> method for characterizing communication within brain networks. The study engages <bold>important</bold>, biologically pertinent, concerns related to the balance of dynamics and structure in assessing the focal points of brain communication. It will be of interest to researchers trying to dissect structure of complex interaction networks across scales, from cells to regions.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101780.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors provide a compelling method for characterizing communication within brain networks. The study engages important, biologically pertinent, concerns related to the balance of dynamics and structure in assessing the focal points of brain communication. The methods are clear, and seem broadly applicable, although they require some forethought about data and modeling choices.</p><p>Strengths:</p><p>The study is well-developed, providing overall clear exposition of relevant methods, as well as in-depth validation of the key network structural and dynamical assumptions. The questions and concerns raised in reading the text were always answered in time, with straightforward figures and supplemental materials.</p><p>Weaknesses:</p><p>In earlier drafts of the work, the narrative structure at times conflicts with the interpretability, however, this was greatly improved during revisions. The only remaining limitation for broad applicability lies in the full observability required in the current paradigm, however, the authors point at avenues for relaxing this assumption, which could be fruitful next steps for researchers aiming to deploy this work to EM or two-photon based datasets.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.101780.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fakhar</surname><given-names>Kayson</given-names></name><role specific-use="author">Author</role><aff><institution>MRC Cognition and Brain Sciences Unit</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Hadaeghi</surname><given-names>Fatemeh</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Computational Neuroscience</institution><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Seguin</surname><given-names>Caio</given-names></name><role specific-use="author">Author</role><aff><institution>The University of Melbourne</institution><addr-line><named-content content-type="city">Melbourne</named-content></addr-line><country>Australia</country></aff></contrib><contrib contrib-type="author"><name><surname>Dixit</surname><given-names>Shrey</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Computational Neuroscience</institution><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Messé</surname><given-names>Arnaud</given-names></name><role specific-use="author">Author</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Zamora-López</surname><given-names>Gorka</given-names></name><role specific-use="author">Author</role><aff><institution>Pompeu Fabra University</institution><addr-line><named-content content-type="city">Barcelona</named-content></addr-line><country>Spain</country></aff></contrib><contrib contrib-type="author"><name><surname>Misic</surname><given-names>Bratislav</given-names></name><role specific-use="author">Author</role><aff><institution>McGill University</institution><addr-line><named-content content-type="city">Montreal</named-content></addr-line><country>Canada</country></aff></contrib><contrib contrib-type="author"><name><surname>Hilgetag</surname><given-names>Claus</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Computational Neuroscience</institution><addr-line><named-content content-type="city">Hamburg</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public reviews):</bold></p><p>Summary:</p><p>In this study, Fakhar et al. use a game-theoretical framework to model interregional communication in the brain. They perform virtual lesioning using MSA to obtain a representation of the influence each node exerts on every other node, and then compare the optimal influence profiles of nodes across different communication models. Their results indicate that cortical regions within the brain's &quot;rich club&quot; are most influential.</p><p>Strengths:</p><p>Overall, the manuscript is well-written. Illustrative examples help to give the reader intuition for the approach and its implementation in this context. The analyses appear to be rigorously performed and appropriate null models are included.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The use of game theory to model brain dynamics relies on the assumption that brain regions are similar to agents optimizing their influence, and implies competition between regions. The model can be neatly formalized, but is there biological evidence that the brain optimizes signaling in this way? This could be explored further. Specifically, it would be beneficial if the authors could clarify what the agents (brain regions) are optimizing for at the level of neurobiology - is there evidence for a relationship between regional influence and metabolic demands? Identifying a neurobiological correlate at the same scale at which the authors are modeling neural dynamics would be most compelling.</p></disp-quote><p>This is a fundamental point, and we put together a new project to address it. The current work focuses on, firstly, rigorously formalizing a prevailing assumption that brain regions optimize communication, and then uncovering what are the characteristics of communication if this optimization is indeed taking place. Based on our findings, we suspect the mechanism of an optimal communication to be through broadcasting (compared to other modes explored in our work, e.g., the shortest-path signalling or diffusion). However, we recognize that our game-theoretical framework does not directly address “how” this mechanism is implemented. Thus, in our follow-up work, we are analyzing available datasets of signal propagation in the brain to see if communication dynamics there match the predictions of the game-theoretical setup. However, following your question, we extended our discussion to cover this point, cited five other works on this topic, and what, we think, could be the neurobiological mechanism of optimal signalling.</p><disp-quote content-type="editor-comment"><p>It is not entirely clear what Figure 6 is meant to contribute to the paper's main findings on communication. The transition to describing this Figure in line 317 is rather abrupt. The authors could more explicitly link these results to earlier analyses to make the rationale for this figure clearer. What motivated the authors' investigation into the persistence of the signal influence across steps?</p></disp-quote><p>Great question. Figure 6 in part follows Figure 5, which summarizes a key aspect of our work: Signals subside at every step but not exponentially (Figure 5), and they nearly fall apart after around 6 steps (Figure 6 A and B). Subplots A and B together suggest that although measures like communicability account for all possible pathways, the network uses a handful instead, presumably to balance signalling robustness versus the energetic cost of signalling. Subplot C, one of our main findings, then shows how one simple model is all needed to predict a large portion of optimal influence compared to other models and variables. In sum, Figure 5 focused on the decay dynamics while Figure 6 focused on the extent, in terms of steps, given that the decay is monotonic. Together, our motivation for this figure was to show how the right assumption about decay rate and dynamics can outperform other measures in predicting optimal communication.</p><disp-quote content-type="editor-comment"><p>The authors used resting-state fMRI data to generate functional connectivity matrices, which they used to inform their model of neural dynamics. If I understand correctly, their functional connectivity matrices represent correlations in neural activity across an entire fMRI scan computed for each individual and then averaged across individuals. This approach seems limited in its ability to capture neural dynamics across time. Modeling time series data or using a sliding window FC approach to capture changes across time might make more sense as a means of informing neural dynamics.</p></disp-quote><p>We agree with you on the fact that static fMRI is limited in capturing neural dynamics. However, we opted not to perform dynamic functional connectivity fitting just yet for a practical reason: Other communication models used here do not fit to any empirical data and provide a static view of the dynamics, comparable to the static functional connectivity. Since one of our goals was to compare different communication regimes, and the fact that fitting dynamics does not seem to substantially change the outcome if the end result is static (Figure 7), we decided to go with the poorer representation of neural data for this work. However, part of our follow-up project involves looking into the dynamics of influence over time and for that, we will fit our models to represent more realistic dynamics.</p><disp-quote content-type="editor-comment"><p>The authors evaluated their model using three different structural connectomes: one inferred from diffusion spectrum imaging in humans, one inferred from anterograde tract tracing in mice, and one inferred from retrograde tract-tracing in macaque. While the human connectome is presumably an undirected network, the mouse and macaque connectomes are directed. What bearing does experimentally inferred knowledge of directionality have on the derivation of optimal influence and its interpretation?</p></disp-quote><p>In terms of if directionality changes the interpretation of optimal influence, we think it sets limits for how much we can compare communication dynamics of these two types of networks. We think interpreting optimal communication in directed graphs needs to disentangle incoming influence from outgoing influence, e.g., analyzing “projector hubs/coordinators” and “receiver hubs/integrators” instead of putting both into a common class of hubs. Also, here we showed the extent of which a signal travels before it significantly degrades, having done so in an undirected graph. One of its implications for a directed graph is the possibility that some nodes can be unreachable from others, given the more restricted navigation. A possibility that we did not observe in the human connectome as all nodes could reach others, although with limited influence (see Figure 2. C). We did not explore these differences, as we used mice and macaque connectomes primarily to control for modality-specific confounds of DSI. However, our relatively poorer fit for directed networks (Supplementary Figure 2) motivated us to analyze how reciprocal connections shape dynamics and what impact do they have on networks’ function. Using the same connectomes as the current work, we addressed this question in a separate publication (Hadaeghi et al., 2024) and plan to extend both works by analyzing the signalling properties of directed networks.</p><disp-quote content-type="editor-comment"><p>It would be useful if the authors could assess the performance of the model for other datasets. Does the model reflect changes during task engagement or in disease states in which relative nodal influence would be expected to change? The model assumes optimality, but this assumption might be violated in disease states.</p></disp-quote><p>This is a wonderful idea that we initially had in mind for this work as well, but decided to dedicate a separate work on deviations in different tasks states, as well as disease states (mainly neurodegenerative disorders). We noticed the practical challenges of fitting large-scale models to task dynamics and harmonizing neuroimaging datasets of neurodegenerative disorders is beyond the scope of the current work. Unfortunately, this effort, although exciting and promising, is still pending as the corresponding author does not yet have the required expertise of neuroimaging processing pipelines.</p><disp-quote content-type="editor-comment"><p>The MSA approach is highly computationally intensive, which the authors touch on in the Discussion section. Would it be feasible to extend this approach to task or disease conditions, which might necessitate modeling multiple states or time points, or could adaptations be made that would make this possible?</p></disp-quote><p>Continuing our response from the previous point, yes, we think, in theory, the framework is applicable to both settings. Currently, our main point of concern is not the computational cost of the framework but the harmonization of the data, to ensure differences in results are not due to differences in preprocessing steps. However, assuming that all is taken care of, we believe a reasonable compute cluster should suffice by parallelizing the analytical pipeline over subjects. We acknowledge that the process would still be time-consuming, but besides the fitting process, we expect a modern high-performance CPU with about 32–64 threads to take up to 3 days analyzing one subject, given 100 brain regions or fewer. This performance then scales with the number of cluster nodes that can each work on one subject. We note that the analytical estimators such as SAR could be used instead, as it largely predicts the results from MSA. The limitations are then the lack of dynamics over time and potential estimation errors.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The authors provide a compelling method for characterizing communication within brain networks. The study engages important, biologically pertinent, concerns related to the balance of dynamics and structure in assessing the focal points of brain communication. The methods are clear and seem broadly applicable, however further clarity on this front is required.</p><p>Strengths:</p><p>The study is well-developed, providing an overall clear exposition of relevant methods, as well as in-depth validation of the key network structural and dynamical assumptions. The questions and concerns raised in reading the text were always answered in time, with straightforward figures and supplemental materials.</p></disp-quote><p>Thank you.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The narrative structure of the work at times conflicts with the interpretability. Specifically, in the current draft, the model details are discussed and validated in succession, leading to confusion. Introducing a &quot;base model&quot; and &quot;core datasets&quot; needed for this type of analysis would greatly benefit the interpretability of the manuscript, as well as its impact.</p></disp-quote><p>Following your suggestion, we modified the introduction to emphasize on the human connectome and the linear model as the main toolkit. We also added a paragraph explaining the datasets that can be used instead.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Essential Revisions (for the authors):</bold></p><p>(1) The method presents an important and well-validated method for linking structural and functional networks, but it was not clear precisely what the necessary data inputs were and what assumptions about the data mattered. To improve the clarity of the presentation for the reader, it would be beneficial to have an early and explicit description of the flow of the method - what exact kinds of datasets are needed and what decisions need to be made to perform the analysis. In addition, there were questions about how the use or interpretation of the method might change with different methods of measuring structure or function, which could be answered via an explicit discussion of the issue. For example, how do undirected fMRI correlation networks compare to directed tracer injection projection networks? Similarly, could this approach apply in cases like EM connectomics with linked functional imaging that do not have full observability in both modalities?</p></disp-quote><p>This is an important point that we missed addressing in detail in the original manuscript. Now we did so, by first adding a paragraph (lines 292-305, page 10) explaining the pipeline and how our framework handles different modeling choices, and then further discussing it in the Discussion (lines 733-748, page 28). Moreover, we adjusted Figure 1, by delineating two main steps of the pipeline. Briefly, we clarified that MSA is model-agnostic, meaning that, in principle, any model of neural dynamics can be used with it, from the most abstract to the most biologically detailed. Moreover, the approach extends to networks built on EM connectomics, tract-tracing, DTI, and other measures of anatomical connectivity. However, we realized that a key detail was not explicitly discussed (pointed to by Reviewer #2), that is, the fact that these models naturally need to be fitted to the empirical dataset, even though this fitting step appears not to be critical, as shown in Figure 7.</p><p>Lines 292-305:</p><p>“The MSA begins by defining a ‘game.’ To derive OSP, this game is formulated as a model of dynamics, such as a network of interacting nodes. These can range from abstract epidemic and excitable models (Garcia et al., 2012; Messé et al., 2015a) to detailed spiking neural networks (Pronold et al., 2023) and to mean-field models of the whole brain dynamics, as chosen here (see below). The model should ideally be fitted to reflect real data dynamics, after which MSA systematically lesions all nodes to derive the OSP. Put together, the framework is general and model-agnostic in the sense that it accommodates a wide range of network models built on different empirical datasets, from human neuroimaging and electrophysiology to invertebrate calcium imaging, and anything in between. In essence, the framework is not bound to specific modelling paradigms, allowing direct comparison among different models (e.g., see section Global Network Topology is More Influential Than Local Node Dynamics).”</p><p>Lines 733-740:</p><p>“As noted in the introduction, OI is model-agnostic, here, we leveraged this liberty to compare signaling under different models of local dynamics, primarily built upon undirected human connectome data. We also considered different modalities, e.g., tract tracing in Macaque (see Structural and Functional Connectomes under Materials and Methods) to confirm that the influence of weak connections is not inflated due to imaging limitations (Supplementary Figure 5. A). The game theoretical formulation of signaling allows for systematic comparison among many combinations of modeling choices and data sources.”</p><p>We then continued with addressing the issue of full observability. We clarified that in this work, full observability was assumed. However, the mathematical foundations of our method capture unobserved contributors/influencers as an extra term, similar to the additive error term of a linear regression model. To keep the paper as non-technical as possible, we omitted expanding the axioms and the proof of how this is achieved, and instead referred to previous papers introducing the framework.</p><p>Lines 740-748:</p><p>“Nonetheless, in this work, we assumed full observability, i.e., complete empirical knowledge of brain structure and function that is not necessarily practically given. Although a detailed investigation of this issue is needed, mathematical principles behind the method suggest that the framework can isolate the unobserved influences. In these cases, activity of the target node is decomposed such that the influence from the observed sources is precisely mapped, while the unobserved influences form an extra term, capturing anything that is left unaccounted for, see (Algaba et al., 2019b; Fakhar et al., 2024) for more technical details.”</p><disp-quote content-type="editor-comment"><p>(2) The value of the normative game theoretic approach was clear, but the neurobiological interpretation was less so. To better interpret the model and understand its range of applicability, it would be useful to have a discussion of the potential neurobiological correlates that were at the same level of resolution as the modeling itself. Would such an optimization still make sense in disease states that might also be of interest?</p></disp-quote><p>This is a brilliant question, which we decided to explore further in separate studies. Specifically, the link between optimal communication and brain disorders is a natural next step that we are pursuing. Here, we expanded our discussion with a few lines first explaining the roots of our main assumption, which is that neurons optimize information flow, among other goals. We then hypothesized that the biological mechanisms by which this goal is achieved include (based on our findings) adopting a broadcasting regime of signaling. We suspect that this mode of communication, operationalized on complex network topologies, is a trade-off between robust signaling and energy efficiency. Currently, we are planning practical steps to test this hypothesis.</p><p>Lines 943-962:</p><p>“Nonetheless, our framework is grounded in game theory where its fundamental assumption is that nodes aim at maximizing their influence over each other, given the existing constraints. This assumption is well explored using various theoretical frameworks (Buehlmann and Deco, 2010; Bullmore and Sporns, 2012; Chklovskii et al., 2002; Laughlin and Sejnowski, 2003; O’Byrne and Jerbi, 2022) and remains open to further empirical investigation. Here, we used game theory to mathematically formalize a theoretical optimum for communication in brain networks. Our findings then provide a possible mechanism for achieving this optimality through broadcasting. Based on our results, we speculate that, there exists an optimal broadcasting strength that balances robustness of the signal with its metabolic cost. This hypothesis is reminiscent of the concept of brain criticality, which suggests the brain to be positioned in a state in which the information propagates maximally and efficiently (O’Byrne and Jerbi, 2022; Safavi et al., 2024). Together, we suggest broadcasting to be the possible mechanism with which communication is optimized in brain networks, however, further research directions include investigating whether signaling within brain networks indeed aligns with a game-theoretic definition of optimality. Additionally, if it does, subsequent studies could then examine how deviations from optimal communication contribute to or result from various brain states or neurological and psychiatric disorders.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>I would recommend that the authors consider the following point in a revision, as well as the major weaknesses of the public review. Some aspects of Figure 1 could be clearer. What is being illustrated by the looping arrow to MSA? What is being represented in the matrices (labeling &quot;source&quot; and &quot;target&quot; on the matrix might enhance clarity)? Is R2 the metric used to assess the degree of similarity between communication models? These could be addressed by making small additions to the figure legend or to the figure itself.</p></disp-quote><p>Thank you for your constructive comment on Figure 1, which is arguably the most important figure in the manuscript. We adjusted the figure and its caption (see above) based on your suggestions. After doing so, we think the figure is now clearer regarding the pipeline used in this work.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>Overall, as stated in the public review and the short assessment, the manuscript is in a clearly mature state and brings an important method to link the fields of structural and functional brain networks.</p><p>Nevertheless, the paper would benefit from an early, and clear, discussion of the:</p><p>(1) components of the model, and assumptions of each, should be stated at the end of the introduction, or early in results. (2) datasets necessary to run the analysis.</p><p>The confusion arises from lines 130-131, stating &quot;In the present work (summarized in Figure 1), we used the human connectome, large-131 scale models of dynamics, and a game-theoretical perspective of signaling.&quot; This, to me, indicated that a structural connectivity map may be the only dataset required, as the dynamics model and game theory component are solely simulated. However, later, lines 214-216 state that the empirical functional connectivity is estimated from the structural connectivity, indicating that the method is only applied to cases where we have both.</p><p>Finally, Supplemental Figure 5 validates a number of metrics on different solely structural networks (which is a very necessary and well-done control). Similarly, while the dynamical model is discussed in depth, and beautifully shown that the specific choice of dynamical model does not directly impact the results, it would be helpful to clarify the dynamical model utilized in the early figures.</p></disp-quote><p>Thank you for pointing out a critical detail that we missed elaborating sufficiently early in the paper: the modelling step. Following your suggestions, we added a paragraph from line 292 to 305 (page 10) expanding on the modelling framework. We also explicitly divided the modelling step in Figure 1 and briefly clarified our modelling choices in the caption. Together, we emphasized the fact that our framework is generally model agnostic, which allows different models of dynamics to be plugged into various anatomical networks. We then clarified that, like in any modelling effort, one needs to first fit/optimize the model parameters to reproduce empirical data. In other words, we emphasized the fact that our framework relies on a computational model as its ‘game’ to infer how regions interact, and we fine-tuned our models to reproduce the empirical FC.</p><disp-quote content-type="editor-comment"><p>Again, this is not a critique of the methods, which are excellent, but the presentation. It would help readers, and even me, to have a clear indication of the model earlier. Further, it would help to discuss, both in the introduction and discussion, the datasets required for applying these methods more broadly. For instance, 2-photon recordings are discussed - would it be possible to apply this method then to EM connectomes with functional data recorded for them? In theory, it seems like yes, although the current datasets have 100% observability, whereas 2-photon imaging, or other local methods, will not have perfect overlap between structural and functional connectomes. Discussions like this, related to the assumptions of the model, the necessary datasets, and broader application directions beyond DSI, fMRI, and BOLD cases where the method was validated, would increase the impact and interpretability for a broad readership.</p></disp-quote><p>This is a valid point that we should have been more explicit about. The revised manuscript now contains a paragraph (lines 740-748) clarifying the fact that, throughout this work, we assumed full observability. We then briefly discuss, based on the mathematical principles of the framework, what we expect to happen in cases with partial observability. We then point at two references in which the details of a framework with partial observability are laid out, one containing mathematical proofs and the other using numerical simulations.</p><p>References:</p><p>Hadaeghi, F., Fakhar, K., &amp; Hilgetag, C. C. (2024). Controlling Reciprocity in Binary and Weighted Networks: A Novel Density-Conserving Approach (p. 2024.11.24.625064). bioRxiv. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2024.11.24.625064">https://doi.org/10.1101/2024.11.24.625064</ext-link></p></body></sub-article></article>