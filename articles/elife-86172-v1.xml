<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="article-commentary" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86172</article-id><article-id pub-id-type="doi">10.7554/eLife.86172</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Insight</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Developmental Biology</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Machine Learning</subject></subj-group></article-categories><title-group><article-title>Finding the right type of cell</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-84490"><name><surname>Scheffer</surname><given-names>Louis K</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3289-6564</contrib-id><email>schefferl@janelia.hhmi.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><bio><p><bold>Louis K Scheffer</bold> is in the Janelia Research Campus, HHMI, Ashburn, United States</p></bio></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013sk6x84</institution-id><institution>Janelia Research Campus, HHMI</institution></institution-wrap><addr-line><named-content content-type="city">Ashburn</named-content></addr-line><country>United States</country></aff></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e86172</elocation-id><permissions><copyright-statement>© 2023, Scheffer</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Scheffer</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86172-v1.pdf"/><related-article related-article-type="commentary-article" ext-link-type="doi" xlink:href="10.7554/eLife.80918" id="ra1"/><abstract><p>A new method allows researchers to automatically assign cells into different cell types and tissues, a step which is critical for understanding complex organisms.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>machine learning</kwd><kwd>morphology</kwd><kwd>representation learning</kwd><kwd>3D</kwd><kwd>cells</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>P. dumerilii</italic></kwd></kwd-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new method allows researchers to automatically assign cells into different cell types and tissues, a step which is critical for understanding complex organisms.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></article-meta></front><body><boxed-text><p><bold>Related research article</bold> Zinchenko V, Hugger J, Uhlmann V, Arendt D, Kreshuk A. 2023. MorphoFeatures for unsupervised exploration of cell types, tissues and organs in volume electron microscopy. <italic>eLife</italic> <bold>12</bold>:e80918. doi: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.7554/eLife.80918">10.7554/eLife.80918</ext-link>.</p></boxed-text><p>Since the advent of microscopy in the 17<sup>th</sup> century, it has become well established that organisms are divided into tissues made up of different types of cells, with cells of the same type typically performing the same role. This simplifies the task of understanding a biological system immensely, as there are many fewer cell types than individual cells (<xref ref-type="bibr" rid="bib4">Masland, 2001</xref>).</p><p>Categorizing tissues and cell types has always been done manually, usually by grouping cells that look the same based on their shape, internal structures and various other features. This is also true for images collected using modern day techniques, such as electron microscopy, which can provide three-dimensional reconstructions of tissue samples, or even entire small organisms less than a millimeter cube in size.</p><p>While electron microscopy images can be automatically subdivided or ‘segmented’ into individual cells, assigning each one to a cell type by hand is both difficult and time consuming; for example, in a recent project, it took several experts many months to categorize one half of the fruit fly brain (<xref ref-type="bibr" rid="bib6">Scheffer et al., 2020</xref>). The whole task becomes even more challenging if the object being studied is not a well-known model organism. Now, in eLife, Valentyna Zinchenko, Johannes Hugger, Virginie Uhlmann Detlev Arendt and Anna Kreshuk of the European Molecular Biology Laboratory report a new method that could simplify this process (<xref ref-type="bibr" rid="bib8">Zinchenko et al., 2023</xref>).</p><p>Zinchenko et al. based their program on a machine learning method called unsupervised, contrastive learning (<xref ref-type="bibr" rid="bib7">van den Oord et al., 2018</xref>). The program works by grouping cell types without having received prior examples of ‘human-classified’ cell types or features (i.e., it is unsupervised) and by finding features that maximize the difference (or contrast) between examples that should be grouped together and those that should not. The method requires many examples, both of cells that should be grouped together, and those that should not. For the positive examples (those that should be grouped together), Zinchenko et al. created synthetic copies of each existing cell with minor modifications, such as different rotations, reflections, and texture or shape variations. In this case, the original cell and the modified cell should be grouped together. For negative examples (those that are of different types), they picked pairs of cells at random from their sample. This will be wrong occasionally but it is sufficiently accurate to train their model while allowing unsupervised operation.</p><p>Machine learning was then applied to find features shared by the positive examples only. The system combined the learned features of each original cell into a vector that summarizes the cell’s shape and texture. The team found that cells belonging to the same type were close together within the space of the vector, which can be visualized and interpreted by existing dimension reduction techniques (<xref ref-type="bibr" rid="bib5">McInnes et al., 2018</xref>).</p><p>Zinchenko et al. then tested their model on a three-dimensional reconstruction of the annelid worm (<italic>Platynereis dumerilii</italic>) obtained through electron microscopy. Their computer model was able to match the different cell types and could identify subgroups of cells that could not be distinguished using human-specified features. Moreover, when compared to a gene expression map of the whole animal, the cells that had been classified as similar based on their features also shared similar genetic signatures, more so than cells that had previously been clustered using ‘human-designed’ features.</p><p>Next, they extended their classification method to consider both the shape and texture of each cell, and a combination of these features of all physically adjacent cells. Grouping these enhanced features revealed different tissues and organs within the animal. The classification system of the model strongly agreed with human results, but also found subtle tissue distinctions and rare features that had been overlooked by humans examining the same data set. For example, the analyses revealed a specific type of neuron in the midgut region of the worm, which had previously not been confirmed to be located in this area of the body.</p><p>The ‘unsupervised’ aspect of the method created by Zinchenko et al. is critical because it means the program does not require a full library of the relevant cell types (or a full list of the features that can distinguish between the cell types). Instead, the program learns these characteristics from the data itself (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This is particularly useful for systems where the cell types are not known. Moreover, it is not restricted to using cell features that humans deem important, such as the roundness of a cell or the presence of dark vesicles. This means that the model can often outperform humans and work without bias, as it is not told what to expect and is thus less likely to overlook rare or unexpected cell types.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Making cell and tissue classification an automatic process.</title><p>Three-dimensional reconstructions of organisms using electron microscopy harbor a multitude of scientific data about cell composition and structure. Zinchenko et al. created a machine learning system that can identify different features to distinguish one cell type from another. Since these features are learned, they do not necessarily correspond to human terms commonly used to describe the shape or texture of cells (such as rounded or speckled). The above images show cells with the best and worst match to two learned features, which can then be used to determine which aspect of the cell the feature corresponds to, such as texture (top) or shape (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86172-fig1-v1.tif"/><attrib>Image credit: Adapted from Figure 3, <xref ref-type="bibr" rid="bib8">Zinchenko et al., 2023</xref> (CC BY 4.0).</attrib></fig><p>Electron microscopy and related techniques provide an incredible level of detail, including the shape, location and structure of every cell. But analyzing this flood of data by hand is nearly impossible and automated techniques are desperately needed to unlock the potential of these findings (<xref ref-type="bibr" rid="bib1">Eberle and Zeidler, 2018</xref>). Significant progress has already been made in turning some tasks, such as cell segmentation and identifying synapses, into automatic processes, leaving cell and tissue identification as some of the most time-consuming manual steps (<xref ref-type="bibr" rid="bib3">Januszewski et al., 2018</xref>; <xref ref-type="bibr" rid="bib2">Huang et al., 2018</xref>). By helping to automate this step, Zinchenko et al. make a critical step in the journey of understanding these invaluable but intimidating data sets.</p></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eberle</surname><given-names>AL</given-names></name><name><surname>Zeidler</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multi-beam scanning electron microscopy for high-throughput imaging in connectomics research</article-title><source>Frontiers in Neuroanatomy</source><volume>12</volume><elocation-id>112</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2018.00112</pub-id><pub-id pub-id-type="pmid">30618653</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>GB</given-names></name><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fully-automatic synapse prediction and validation on a large data set</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><elocation-id>87</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00087</pub-id><pub-id pub-id-type="pmid">30420797</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Kornfeld</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>PH</given-names></name><name><surname>Pope</surname><given-names>A</given-names></name><name><surname>Blakely</surname><given-names>T</given-names></name><name><surname>Lindsey</surname><given-names>L</given-names></name><name><surname>Maitin-Shepard</surname><given-names>J</given-names></name><name><surname>Tyka</surname><given-names>M</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-precision automated reconstruction of neurons with flood-filling networks</article-title><source>Nature Methods</source><volume>15</volume><fpage>605</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0049-4</pub-id><pub-id pub-id-type="pmid">30013046</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masland</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The fundamental plan of the retina</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>877</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1038/nn0901-877</pub-id><pub-id pub-id-type="pmid">11528418</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Umap: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Xu</surname><given-names>CS</given-names></name><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Takemura</surname><given-names>SY</given-names></name><name><surname>Hayworth</surname><given-names>KJ</given-names></name><name><surname>Huang</surname><given-names>GB</given-names></name><name><surname>Shinomiya</surname><given-names>K</given-names></name><name><surname>Maitlin-Shepard</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Clements</surname><given-names>J</given-names></name><name><surname>Hubbard</surname><given-names>PM</given-names></name><name><surname>Katz</surname><given-names>WT</given-names></name><name><surname>Umayam</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Ackerman</surname><given-names>D</given-names></name><name><surname>Blakely</surname><given-names>T</given-names></name><name><surname>Bogovic</surname><given-names>J</given-names></name><name><surname>Dolafi</surname><given-names>T</given-names></name><name><surname>Kainmueller</surname><given-names>D</given-names></name><name><surname>Kawase</surname><given-names>T</given-names></name><name><surname>Khairy</surname><given-names>KA</given-names></name><name><surname>Leavitt</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>PH</given-names></name><name><surname>Lindsey</surname><given-names>L</given-names></name><name><surname>Neubarth</surname><given-names>N</given-names></name><name><surname>Olbris</surname><given-names>DJ</given-names></name><name><surname>Otsuna</surname><given-names>H</given-names></name><name><surname>Trautman</surname><given-names>ET</given-names></name><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Goldammer</surname><given-names>J</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Svirskas</surname><given-names>R</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Neace</surname><given-names>E</given-names></name><name><surname>Knecht</surname><given-names>CJ</given-names></name><name><surname>Alvarado</surname><given-names>CX</given-names></name><name><surname>Bailey</surname><given-names>DA</given-names></name><name><surname>Ballinger</surname><given-names>S</given-names></name><name><surname>Borycz</surname><given-names>JA</given-names></name><name><surname>Canino</surname><given-names>BS</given-names></name><name><surname>Cheatham</surname><given-names>N</given-names></name><name><surname>Cook</surname><given-names>M</given-names></name><name><surname>Dreher</surname><given-names>M</given-names></name><name><surname>Duclos</surname><given-names>O</given-names></name><name><surname>Eubanks</surname><given-names>B</given-names></name><name><surname>Fairbanks</surname><given-names>K</given-names></name><name><surname>Finley</surname><given-names>S</given-names></name><name><surname>Forknall</surname><given-names>N</given-names></name><name><surname>Francis</surname><given-names>A</given-names></name><name><surname>Hopkins</surname><given-names>GP</given-names></name><name><surname>Joyce</surname><given-names>EM</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Kirk</surname><given-names>NA</given-names></name><name><surname>Kovalyak</surname><given-names>J</given-names></name><name><surname>Lauchie</surname><given-names>SA</given-names></name><name><surname>Lohff</surname><given-names>A</given-names></name><name><surname>Maldonado</surname><given-names>C</given-names></name><name><surname>Manley</surname><given-names>EA</given-names></name><name><surname>McLin</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>C</given-names></name><name><surname>Ndama</surname><given-names>M</given-names></name><name><surname>Ogundeyi</surname><given-names>O</given-names></name><name><surname>Okeoma</surname><given-names>N</given-names></name><name><surname>Ordish</surname><given-names>C</given-names></name><name><surname>Padilla</surname><given-names>N</given-names></name><name><surname>Patrick</surname><given-names>CM</given-names></name><name><surname>Paterson</surname><given-names>T</given-names></name><name><surname>Phillips</surname><given-names>EE</given-names></name><name><surname>Phillips</surname><given-names>EM</given-names></name><name><surname>Rampally</surname><given-names>N</given-names></name><name><surname>Ribeiro</surname><given-names>C</given-names></name><name><surname>Robertson</surname><given-names>MK</given-names></name><name><surname>Rymer</surname><given-names>JT</given-names></name><name><surname>Ryan</surname><given-names>SM</given-names></name><name><surname>Sammons</surname><given-names>M</given-names></name><name><surname>Scott</surname><given-names>AK</given-names></name><name><surname>Scott</surname><given-names>AL</given-names></name><name><surname>Shinomiya</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>NL</given-names></name><name><surname>Sobeski</surname><given-names>MA</given-names></name><name><surname>Suleiman</surname><given-names>A</given-names></name><name><surname>Swift</surname><given-names>J</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Talebi</surname><given-names>I</given-names></name><name><surname>Tarnogorska</surname><given-names>D</given-names></name><name><surname>Tenshaw</surname><given-names>E</given-names></name><name><surname>Tokhi</surname><given-names>T</given-names></name><name><surname>Walsh</surname><given-names>JJ</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Horne</surname><given-names>JA</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Parekh</surname><given-names>R</given-names></name><name><surname>Rivlin</surname><given-names>PK</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name><name><surname>Ito</surname><given-names>K</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Hess</surname><given-names>HF</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A connectome and analysis of the adult <italic>Drosophila</italic> central brain</article-title><source>eLife</source><volume>9</volume><elocation-id>e57443</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57443</pub-id><pub-id pub-id-type="pmid">32880371</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van den Oord</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representation Learning with Contrastive Predictive Coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.03748">https://arxiv.org/abs/1807.03748</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zinchenko</surname><given-names>V</given-names></name><name><surname>Hugger</surname><given-names>J</given-names></name><name><surname>Uhlmann</surname><given-names>V</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>MorphoFeatures for unsupervised exploration of cell types, tissues and organs in volume electron microscopy</article-title><source>eLife</source><volume>12</volume><elocation-id>e80918</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80918</pub-id></element-citation></ref></ref-list></back></article>