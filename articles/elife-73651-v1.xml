<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">73651</article-id><article-id pub-id-type="doi">10.7554/eLife.73651</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Different computations over the same inputs produce selective behavior in algorithmic brain networks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-83207"><name><surname>Jaworska</surname><given-names>Katarzyna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6482-1498</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-264217"><name><surname>Yan</surname><given-names>Yuening</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4027-2687</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-254372"><name><surname>van Rijsbergen</surname><given-names>Nicola J</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-32067"><name><surname>Ince</surname><given-names>Robin AA</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8427-0507</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-32069"><name><surname>Schyns</surname><given-names>Philippe G</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8542-7489</contrib-id><email>philippe.schyns@glasgow.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vtgdb53</institution-id><institution>School of Psychology and Neuroscience, University of Glasgow</institution></institution-wrap><addr-line><named-content content-type="city">Glasgow</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/028ndzd53</institution-id><institution>Department of Psychology, Edge Hill University</institution></institution-wrap><addr-line><named-content content-type="city">Ormskirk</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>02</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e73651</elocation-id><history><date date-type="received" iso-8601-date="2021-09-06"><day>06</day><month>09</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-01-06"><day>06</day><month>01</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-02-05"><day>05</day><month>02</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.02.04.429372"/></event></pub-history><permissions><copyright-statement>© 2022, Jaworska et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Jaworska et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-73651-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-73651-figures-v1.pdf"/><abstract><p>A key challenge in neuroimaging remains to understand where, when, and now particularly <italic>how</italic> human brain networks compute over sensory inputs to achieve behavior. To study such dynamic algorithms from mass neural signals, we recorded the magnetoencephalographic (MEG) activity of participants who resolved the classic XOR, OR, and AND functions as overt behavioral tasks (N = 10 participants/task, N-of-1 replications). Each function requires a different computation over the same inputs to produce the task-specific behavioral outputs. In each task, we found that source-localized MEG activity progresses through four computational stages identified within individual participants: (1) initial contralateral representation of each visual input in occipital cortex, (2) a joint linearly combined representation of both inputs in midline occipital cortex and right fusiform gyrus, followed by (3) nonlinear task-dependent input integration in temporal-parietal cortex, and finally (4) behavioral response representation in postcentral gyrus. We demonstrate the specific dynamics of each computation at the level of individual sources. The spatiotemporal patterns of the first two computations are similar across the three tasks; the last two computations are task specific. Our results therefore reveal where, when, and how dynamic network algorithms perform different computations over the same inputs to produce different behaviors.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computation</kwd><kwd>categorization</kwd><kwd>task</kwd><kwd>representation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name><name><surname>Ince</surname><given-names>Robin AA</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014036</institution-id><institution>Multidisciplinary University Research Initiative</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name><name><surname>Ince</surname><given-names>Robin AA</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Four stages of linear and nonlinear computations in hierarchical dynamic brain networks progressively transform the same two visual inputs into the task-specific XOR, AND, and OR behavioral responses.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Extensive studies revealed that the primate visual system comprises the ventral and dorsal pathways, with specific anatomical and functional hierarchical organization (<xref ref-type="bibr" rid="bib34">Van Essen et al., 1992</xref>; <xref ref-type="bibr" rid="bib20">Kravitz et al., 2013</xref>). These pathways compute over the high-dimensional visual input, starting separately in each hemisphere with contralateral detection of simple, small features with small receptive fields, that are then hierarchically integrated into more complex, broader receptive field features (<xref ref-type="bibr" rid="bib4">Bugatus et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib19">Kay et al., 2015</xref>), leading to the integrated face, object, and scene features (<xref ref-type="bibr" rid="bib5">DiCarlo and Cox, 2007</xref>; <xref ref-type="bibr" rid="bib9">Grill-Spector and Weiner, 2014</xref>; <xref ref-type="bibr" rid="bib21">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Sigala and Logothetis, 2002</xref>) that are compared with memory to produce behavior (<xref ref-type="bibr" rid="bib39">Zhan et al., 2019b</xref>; <xref ref-type="bibr" rid="bib36">Wyart et al., 2012</xref>; <xref ref-type="bibr" rid="bib30">Ratcliff et al., 2009</xref>; <xref ref-type="bibr" rid="bib2">Alamia and VanRullen, 2019</xref>). This flow of information reverses when the same pathways predict the top-down input from memory (<xref ref-type="bibr" rid="bib7">Friston, 2008</xref>; <xref ref-type="bibr" rid="bib23">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Engel et al., 2001</xref>).</p><p>There is broad agreement that such a bidirectional hierarchical architecture supports much of the information processing that subtends everyday face, object, and scene recognition. However, despite considerable progress, we have yet to understand where, when, and how specific algorithmic computations in the pathways dynamically represent and transform the visual input into integrated features to produce behavior (<xref ref-type="bibr" rid="bib22">Kriegeskorte and Douglas, 2018</xref>; <xref ref-type="bibr" rid="bib26">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Wu et al., 2006</xref>) and vice versa, when reversing the flow in the hierarchy, to predict a cascade of features from complex to simpler ones. Furthermore, it is unclear that such an algorithmic understanding can be achieved with current analytical approaches to neuroimaging, even in simple tasks (<xref ref-type="bibr" rid="bib18">Jonas and Kording, 2017</xref>). Here, we achieved such systems-level algorithmic understanding with magnetoencephalographic (MEG) measurements, in the context of well-defined visual inputs and tasks.</p><p>We framed this broad problem using the classic logical functions XOR, AND, and OR, in which different algorithms are required to produce correct responses from the same input stimuli (see these input-output relationships in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>). XOR is famously a nonlinearly separable function, whereas AND or OR is linearly separable, implying nonlinear vs. linear transformations of the same inputs in the considered architectures (<xref ref-type="bibr" rid="bib25">Minsky and Papert, 2017</xref>; <xref ref-type="bibr" rid="bib31">Rumelhart et al., 1986</xref>; <xref ref-type="bibr" rid="bib8">Gidon et al., 2020</xref>) (see <xref ref-type="fig" rid="fig1">Figures 1A</xref> and <xref ref-type="fig" rid="fig2">2</xref>). We aimed to reverse engineer the different stages of linear and nonlinear computations in brain networks that implement the algorithms (<xref ref-type="bibr" rid="bib29">O’Reilly and Mars, 2011</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>XOR task, four stages of computation in a schematic network and in the brain.</title><p>(<bold>A</bold>) Hypotheses and schematic hierarchical brain network in the XOR task. Stimuli consisted of the image of a face wearing glasses, with dark (‘1’, ‘on’) and clear (‘0’, ‘off’) left and right lenses serving as inputs, for a total of four input classes for XOR behavioral decisions. (<bold>B</bold>) Four hierarchical stages of computations. Each colored curve shows the average (N = 10 participants) time course of the maximum across sources that: (1) linearly discriminates in its magnetoencephalographic (MEG) activity the ‘on’ vs. ‘off’ state of the left (Lin, blue) and right (Lin, orange) inputs (weighted distance pattern), (2) linearly discriminates both inputs (LinBoth, magenta) (weighted distance pattern), (3) nonlinearly integrates both inputs with the XOR task pattern (NonLin, green) (weighted XOR pattern metric) and (4) nonlinearly integrates both inputs with the XOR task pattern and with amplitude variations that relate to reaction time (RT) (mutual information, MI (MEG; RT)) (yellow). Colored brains localize the regions where these computations start (onset times for left and right) or peak (peak latencies for both, XOR and RT) (p&lt;0.05) familywise error rate corrected with a permutation test, (see <italic>Methods, Linear vs. Nonlinear Representations; Representation Patterns</italic>). Dots report the onset time of computation (1) and (2) and the peak time of computation (3) and (4) in each participant. See <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for individual participant replications of each computation in the same brain regions and time windows.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Clustering of computation stages.</title><p>(<bold>A</bold>) Number of computation stages. Results of k-means analysis, with k = 1...20, with sum of squared error of each k (plain curve) and distance of the plain curve from straightline (dashed curve). We selected the k with furthest distance (elbow method). At stage 0, there is no computation. At stage 1, LinLeft (blue) and LinRight (orange) dominate. At stage 2, LinBoth (magenta) dominates. At stages 3 and 4 the task-specific NonLin XOR, AND, and OR (green) dominate. Color intensity reflects the frequency of participants performing this computation in this stage and source. (<bold>B</bold>) Similarity of computation stages across XOR, AND, and OR. We computed pairwise similarities between the four computation stages of each task, as the percentage of sources that shared the same computation (either LinLeft, LinRight, BothLin, NonLin XOR, AND, or OR) across pairs of stages. Similarity is high between stages 1 and 2 across the three tasks (color-coded brains show the sources with intersecting computations across the three tasks), whereas the NonLin stages 3 and 4 are task specific. (<bold>C</bold>) Generalization of stages of computation in XOR from face to Gabor stimuli. Using the similarity metric again, we demonstrate the generalization of the three participants (<bold>P1, P2, P3</bold>) who resolved XOR with Gabor stimuli (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>), showing similarity of their stages 1 and 2 with those of the face stimuli, and similarity of stages 3 and 4 only with the XOR face stage.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Results for individual participants.</title><p>Gray curves correspond to the individual participants’ time courses of the individual source Lin (left and right), LinBoth, NonLin, and NonLin&amp;RT, in the XOR, AND, and OR tasks. Only significant (familywise error rates p=0.05) time points are shown, other time points are set to 0. The colored lines correspond to the data of one exemplar participant used in <xref ref-type="fig" rid="fig3">Figure 3</xref> (XOR) and 3 (AND and OR). Tick marks on each colored line correspond to the seven time-steps of the dynamic unfolding of each computation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Replication of four stages of computation with Gabor stimuli.</title><p>(<bold>A</bold>) Stimuli. In three participants, we presented two spatially separated Gabor and phase-opposed Gabor patches. Participants responded when the two patches had the same vs. different colors with different keyboard keys (an XOR task). Gabors were presented in four randomly chosen orientations, spanning in total 11 × 11 deg of visual angle (the inner edges of each Gabor were located ~5.7 deg of visual angle away from the centrally presented fixation cross). Gabor inputs were simultaneously presented on the screen for 150ms. (<bold>B</bold>) Four hierarchical stages of computations. Each plot shows the maximum within participant of source-level (1) linear discrimination of each Gabor input Lin (blue and orange), (2) linear discrimination of both input, Linboth (magenta), (3) nonlinear integration of both inputs, NonLin (green) with the XOR task pattern, and (4) with amplitude variations that relate to reaction time (RT) NonLin&amp;RT (yellow). (<bold>C</bold>) Colored brains localize the regions where these computations start. The brackets report the per participant onset times (for Lin, left and right) and peak latencies (for LinBoth, NonLin XOR, and XOR&amp;RT), and averaged across participants in the colored brains.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig1-figsupp3-v1.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-73651-fig1-video1.mp4"><label>Figure 1—video 1.</label><caption><title>Dynamic summary of the four stages of computation in the XOR task (data from the example XOR participant highlighted in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</title><p>The left brain animation reports the voxel-level computations performed at each stage. Specifically, stage 1: LinLeft (light blue), LinRight (orange); stage 2: LinBoth (magenta); stage 3: NonLin XOR (green); stage 4: NonLin XOR&amp;RT (yellow). The schematic network illustrates each stage as it unfolds over time, with left and right hemispheres represented as the left and right network nodes. Scatter plots represent each dynamic computation at each stage in the 2D space of magnetoencephalographic response of the source with highest metric for this computation. In the scatter, each color-coded dot represents the average source response to each one of the four classes of color-coded stimuli (see adjacent legend). The specific computation exerted on the same four classes of stimuli is provided in the discrimination line (or curve) in the legend. Note that the first two stages comprise only linear computations that represent the stimuli; the latter two comprise nonlinear computations that related to the specific task performed. <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>, and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref> show that only the latter two nonlinear stages change with task (XOR, AND, and OR).</p></caption></media></fig-group><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Stages of computations in AND and OR tasks.</title><p>Each colored curve averages (N = 10 participants) the time courses of the maximum across sources that: (1) linearly discriminates (weighted distance pattern) in its magnetoencephalographic activity the ‘on’ vs. ‘off’ state of the left (blue) and right (orange) inputs, (2) linearly discriminates both inputs (magenta), (3) nonlinearly integrates both inputs with the respective task pattern and (4) nonlinearly integrates both inputs with the respective task pattern and with amplitude variations that relates to reaction time (RT) (yellow). Colored brains localize the regions where these computations start (onset times for left and right) or peak (peak latencies for LinBoth, NonLin, and RT) p&lt;0.05 familywise error rates corrected with a permutation test, see <italic>Methods, Linear vs. Nonlinear Representations; Representation Patterns</italic>. Dots report the onset time of computation (1) and (2), and the peak time of computation (3) and (4) in each participant. See <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for individual participant replication of each computation in the same brain regions and time windows. Single source plots in each task develop the data of one typical observer, where the green and yellow computations (cf. color-coded sources in the glass brain) differ between AND, OR, and XOR (see <xref ref-type="fig" rid="fig3">Figure 3</xref> for caption).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Four stages of computations replicated with delayed stimulus presentation.</title><p>Using the same face stimuli as in the main experiment, we presented either the right or the left input as gray for the first 1000ms following stimulus onset. The other input was either clear or dark. After 1000ms, the gray input changed into either clear or dark. Participants waited until the gray input changed and then responded according to the logic gate task: XOR (N = 5, top), AND (N = 4, middle), and OR (N = 5, bottom) (two participants were excluded under the same criteria as the main experiment). Linear representation of the first input started around 80ms poststimulus (with a ~100ms peak; or around –900ms with respect to second lens presentation). The second input was linearly represented from ~100ms with a peak at ~120ms (blue, orange, and magenta time courses). Averaged time courses of the Lin and NonLin computations across individual participants (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) replicated the results of <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Four stages of computations replicated with delayed stimulus presentation: individual participants’ results.</title><p>Gray curves correspond to the individual participants’ time courses of the individual source Lin (left and right), LinBoth, NonLin, and NonLin&amp;RT, in the XOR, AND, and OR tasks, with 0ms corresponding to the time point when the second input is revealed (i.e. when the gray lens turns clear or dark, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The colored lines correspond to the mean across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig2-figsupp2-v1.tif"/></fig></fig-group><p>To do so, we simultaneously presented the inputs laterally within the visual field (a pair of sunglasses on a face, with dark ‘on’ vs. clear ‘off’ lenses representing the binary inputs, <xref ref-type="fig" rid="fig1">Figure 1A</xref>) so that occipital cortex initially represented each separately and contralaterally (i.e. in analogy to a network model that takes in two distinct inputs, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>, here with left vs. right input projected in right vs. left occipital cortex; see <italic>Methods, Stimuli</italic>). Our analyses of the ensuing computations in the networks of individual participants systematically revealed four distinct stages that represent and transform the same inputs to produce different task-specific behavior. The first two stages of linear computations similarly represent the two inputs across the XOR, AND, and OR tasks. The last two stages of nonlinear computations differently represent the same inputs in a task-dependent manner (N = 10 participants per task, each analyzed separately to provide an independent replication; we further replicated the key results in different participants, using opposite-phase Gabor patches and also with sequentially presented inputs) (<xref ref-type="bibr" rid="bib24">Little and Smith, 2018</xref>; <xref ref-type="bibr" rid="bib17">Ince et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Naselaris et al., 2021</xref>; <xref ref-type="bibr" rid="bib33">Smith and Little, 2018</xref>). A video (<xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>) visualizes the key results in one typical participant, with four stages of computation in the brain between stimulus and behavior schematized in a network model, and examples of the different color-coded source-level dynamic computations on the same inputs that comprise each stage. We advise watching (<xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>) stage by stage, to complement the presentation of the main results below.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Starting with behavior, <xref ref-type="table" rid="table1">Table 1</xref> shows that participants were both accurate and fast in all tasks, with no significant task differences on average accuracy and reaction times (RT), measured with independent sample t-tests. We reconstructed the dynamic neural representation of the inputs of each participant from concurrently measured, source modeled MEG activity (see <italic>Methods, MEG Data Acquisition, Source Reconstruction</italic>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Mean behavioral accuracy and median reaction times in XOR, AND, and OR, 95% percentile bootstrap confidence interval shown in brackets.</title><p>All pairwise comparisons, p&gt;0.05.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Accuracy</th><th align="left" valign="bottom">Reaction time</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>XOR</bold></td><td align="char" char="." valign="bottom">98.5% [97, 99]</td><td align="char" char="." valign="bottom">499ms [461, 557]</td></tr><tr><td align="left" valign="bottom"><bold>AND</bold></td><td align="left" valign="bottom">99% [98.6, 99.5]</td><td align="char" char="." valign="bottom">457ms [393, 505]</td></tr><tr><td align="left" valign="bottom"><bold>OR</bold></td><td align="char" char="." valign="bottom">99.3% [99.2, 99.5]</td><td align="char" char="." valign="bottom">490ms [439, 541]</td></tr></tbody></table></table-wrap><p>To simplify presentation, henceforth we use vector notation to denote the state of the two inputs and, for example, write left input ‘on’, right input ‘off’ as [1,0]. To preview the analysis and key results, for each source and every 4ms we fit linear models to explain the 2D MEG magnetic vector field activity in terms of the two presented binary inputs, with and without a nonlinear interaction term between them. The interaction term captures the nonlinear integration of the two inputs on this MEG source and time point—i.e., when source response to [1,1] differs from the sum of the responses to [1,0] and [0,1]. Additional metrics quantified how the 2D MEG responses match the response pattern expected in each task (see <italic>Methods, Representational Patterns</italic>). Our analyses reveal that individual MEG source responses reflect changing representations of the visual inputs in the brain, revealing four different stages of neural computations that lead to behavior in each task (see <italic>Methods, Linear vs. Nonlinear Representations, Representation Patterns</italic> and <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>).</p><sec id="s2-1"><title>Four systems-level stages of computation link stimulus to behavior</title><p>First, we performed a data-driven clustering analysis that delivered four stages of computation in the XOR, AND, and OR tasks (see <italic>Methods, Clustering the Stages of Computation</italic> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). <xref ref-type="fig" rid="fig1">Figure 1B</xref> (XOR) and <xref ref-type="fig" rid="fig2">Figure 2</xref> (AND and OR) show the time course of these four stages averaged across the 10 participants of each task (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> shows the results of each individual participant). Each task shows a similar dynamic unfolding: the first two stages represent and linearly discriminate the visual inputs; the third and fourth stages nonlinearly integrate them in a task-specific manner, revealing the solution of each task in the responses of individual MEG sources. The network model of <xref ref-type="fig" rid="fig1">Figure 1A</xref> schematizes these stages. Specifically, we show:</p><list list-type="order"><list-item><p><bold>Linear, contralateral discrimination of each input state separately (‘Lin’) in V1-4 regions with onset from ~60ms poststimulus</bold>, quantified as the product of and the LEFT/RIGHT representation pattern metric (see <italic>Methods, Representation Patterns</italic>) and the multivariate R2 of a linear model for each binary input (see <italic>Methods, Linear Representation</italic>), color coded in light blue for the left input, in orange for the right input.</p></list-item><list-item><p><bold>Linear discrimination of both inputs (‘LinBoth’) on the same occipital and ventral sources ~ 100ms poststimulus</bold>, quantified as the product of the BOTH representation pattern metric (see <italic>Methods, Representation Patterns</italic>) and the multivariate R2 of a linear model considering both inputs with no interaction (see <italic>Methods, Linear Representation</italic>), color coded in magenta.</p></list-item><list-item><p><bold>Nonlinear integration of both inputs (‘NonLin’) for task performance (XOR, AND, or OR) in temporal-parietal regions ~260ms</bold>, quantified as the product of the XOR representation pattern metric (see <italic>Methods, Representation Patterns</italic>) and the significant improvement in model fit with interaction term (see <italic>Methods, Nonlinear Representation</italic>), color coded in green.</p></list-item><list-item><p><bold>Nonlinear integration of both inputs together with response-related activity (‘NonLin&amp;RT’) in postcentral gyrus ~400ms</bold>, quantified as mutual information (MI) between the 2D MEG magnetic field and RT on the corresponding trial (see <italic>Methods, Information Theoretic Analyses</italic>), also thresholded by the product of the XOR pattern metric and the model interaction term, color coded in yellow.</p></list-item></list><p>In <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>, colored sources shown in glass brain localize the regions where each color-coded computation onsets or peaks (cf. dashed lines) in different time windows poststimulus. <xref ref-type="table" rid="table2">Table 2</xref> shows independent replications of each computation within these regions and time windows, in at least 9/10 participants. We also replicated each color-coded computation at the level of individual participants, using Gabor inputs in XOR (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>) and a sequential presentation of the inputs in XOR, AND, and OR (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref>). Finally, we show that the four stages of computations generalize stage by stage in the XOR from face stimuli to Gabor stimuli (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Number of individual participant replications of the four color-coded computations, within the same region and time window.</title><p>Lin, left and right occipital sources [74–117ms]; LinBoth, occipital midline and right fusiform gyrus [74–117ms]; NonLin, XOR: parietal [261–273ms], AND: temporal-parietal [246–304ms], OR: temporal-parietal [261–304ms]; RT, postcentral gyrus [386–398ms]. Bayesian population prevalence (<xref ref-type="bibr" rid="bib17">Ince et al., 2021</xref>) of 9/10 = 0.89 [0.61 0.99]; 10/10 = 1 [0.75 1] (MAP [95% HPDI]).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Lin</th><th align="left" valign="bottom">LinBoth</th><th align="left" valign="bottom">NonLin</th><th align="left" valign="bottom">NonLin&amp;RT</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>XOR</bold></td><td align="char" char="." valign="bottom">10/10</td><td align="char" char="." valign="bottom">10/10</td><td align="left" valign="bottom">9/10 (parietal)</td><td align="char" char="." valign="bottom">9/10</td></tr><tr><td align="left" valign="bottom"><bold>AND</bold></td><td align="char" char="." valign="bottom">10/10</td><td align="char" char="." valign="bottom">10/10</td><td align="left" valign="bottom">10/10</td><td align="char" char="." valign="bottom">9/10</td></tr><tr><td align="left" valign="bottom"><bold>OR</bold></td><td align="char" char="." valign="bottom">10/10</td><td align="char" char="." valign="bottom">10/10</td><td align="left" valign="bottom">10/10</td><td align="char" char="." valign="bottom">10/10</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>Detailed dynamic unfolding of each computation at single source level</title><p>We next detail the dynamic unfolding of each color-coded computation on single sources, using an exemplar XOR participant (highlighted with colored curves in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, XOR, also reported in <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>). The selected sources maximize the metric of each computation—i.e., Lin: onset; LinBoth, NonLin, NonLin&amp;RT: peak. The glass brain in <xref ref-type="fig" rid="fig3">Figure 3A</xref> locates the selected sources and color codes them by type of computation. The subpanels visualize the dynamic response trajectories of each source to the same four stimuli (representing the ‘on’ vs. ‘off’ combinatorics of the two inputs) over 72ms, with a 12ms timestep resolution (those indicated with triangle markers in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> and <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>). To preview the results, different source response trajectories to the same inputs detail the neural implementation of the different color-coded computations.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dynamic unfolding of different computations on the same inputs (e.g. XOR participant, see also <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref> ).</title><p>(<bold>A</bold>) Localized sources. Color-coded source localized in the top glass brain illustrates each color-coded computations in each scatter plot. Legend. Axes of the scatter plots represent the 2D source magnetic field response. Small dots are single-trial source responses; larger dots their averages for each color-coded stimulus class, dynamically reported over seven timesteps (cf. legend) corresponding to the seven triangular markers in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, XOR. Increasing dot sizes, saturations, and connecting lines denote increasing timesteps of the dynamic trajectory. (<bold>B</bold>) Right occipital. The light blue discrimination line indicates the linear (LinLeft) computation that this source represents at the seventh timestep (cf. adjacent scatter for the distribution of individual trials at this time). Inset vector diagram provides a geometric interpretation of the linear computations. Using stimulus [0,0] as the origin: blue arrow illustrates source response to stimulus [0,1] (blue disk); red arrow shows source responses to [1,0] (red disk); gray disk illustrates linear summation of these vectors (opaque lines); black disk is the observed mean response to stimulus [1,1]. (<bold>C</bold>) Left occipital. Same caption for orange LinRight computation. (<bold>D</bold>) Right midline occipital and (<bold>E</bold>) right fusiform gyrus. Same caption for magenta LinBoth computation. (<bold>F</bold>) Right parietal and (<bold>G</bold>) postcentral gyrus. Same caption for green XOR nonlinear computations. Green vector shows discrepant nonlinear observed response to stimulus [1,1] and linear sum of responses to [0,1] and [1,0].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Dynamics of relation of magnetoencephalographic (MEG) responses to reaction times (RTs).</title><p>In each task, for each participant, we computed the single-trial relationship between variations of MEG source amplitude at each time point and RT, computed as mutual information (<italic>MI) (&lt; RT; MEG<sub>t</sub>&gt;</italic>). Left plots show the average of significant MI (p&lt;0.05, corrected). Right plots show the number of participants with significant MI in each source and time point. In each task, stage 4 postcentral gyrus (and to some extent frontal regions) source activity relates to behavioral RTs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig3-figsupp1-v1.tif"/></fig></fig-group><p>To illustrate, we start with the light blue right occipital source (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Its Lin computation (see Legend and <xref ref-type="fig" rid="fig3">Figure 3B</xref>) develops over seven time-steps (from 2 to 74ms) to linearly discriminate the ‘on’ (dark) vs. ‘off’ (clear) state of the left input (see schematic in the bottom left quadrant). The plot shows how 2D source activity progressively separates left lens ‘on’ trials (red and gray stimuli, see blue line in schematic) from left lens ‘off’ trials (blue and black stimuli). The adjacent scatter (upper right quadrant) shows the source response to individual trials at 74ms (final plotted time point). The vector diagram (lower right quadrant) confirms that the linear addition of the vector responses [0, 1] and [1, 0] (gray point, sum of blue and red vector) is close to the actual source response to [1, 1] (black point). The left occipital source (orange, <xref ref-type="fig" rid="fig3">Figure 3A,C</xref>) reflects a similar unfolding for the linear discrimination of the right input ‘on’ vs. ‘off’ state. Again, the response is close to the linear sum of the responses to each individual input.</p><p>The second computation (LinBoth, magenta) that linearly and jointly represents the ‘on’ vs. ‘off’ state of both inputs takes two distinct forms. In midline occipital sources (<xref ref-type="fig" rid="fig3">Figure 3A,D</xref>), all four stimuli are equally spread out in the quadrants of the source response space (i.e. all inputs are equally discriminated). In contrast, the right fusiform gyrus source (<xref ref-type="fig" rid="fig3">Figure 3A,E</xref>) discriminates the [1,0] and [0,1] stimuli with an opposite activity, whereas the [1,1] (black dot) and [0,0] (gray dot) stimuli are less discriminated. The vector diagrams of the two LinBoth examples confirm that the joint response to [1,1] is indeed the sum of [1,0] and [0,1] responses. Interestingly, the two LinBoth discriminations illustrate a progression toward an XOR representation. The first LinBoth midline occipital source (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) discriminates equally each input in the quadrants of its 2D response. In contrast, the amplitude response of the LinBoth right fusiform gyrus source (<xref ref-type="fig" rid="fig3">Figure 3E</xref>) can linearly discriminate the XOR responses, but only if a nonlinear operation was added (i.e. drawing a circle that separates the two ‘same’ black and gray stimuli near the origin in the 2D source response space from the two ‘different’ blue and red stimuli). So, the right fusiform gyrus LinBoth stage likely represents an important intermediate step toward the computation of XOR. We will see next that the green computation adds the nonlinear computation.</p><p>The third and most critical computation that starts distinguishing the task (NonLin, green) occurs when the parietal source (<xref ref-type="fig" rid="fig3">Figure 3A, F</xref>) nonlinearly represents the XOR solution for behavior, with ‘same’ vs. ‘different’ stimuli discriminated at 254ms. Like LinBoth in right fusiform gyrus, this representation has black dot [1,1] and gray dot [0,0] responses close together, but with two crucial differences. First, the red and blue vectors (lower right quadrant) now point in the same direction, rather than in opposite directions, as happens in right Fusiform Gyrus LinBoth. Such different source-level responses to the same [1,0] and [0,1] stimuli likely reflect different activities of the neural populations in the regions where the two sources are located. In parietal source NonLin, responses to [1,0] and [0,1] stimuli are magnetic fields with the same dipole orientation, suggesting that the same pattern of cortical activity (i.e. the same neural population) responds to both stimuli. In right fusiform gyrus source LinBoth (<xref ref-type="fig" rid="fig3">Figure 3A, E</xref>), responses to [1,0] and [0,1] are magnetic fields with different dipole directions, suggesting that different neural populations within the region modeled by the source respond to each stimulus. Second, the representation of [1,1] (black dot) is now nonlinear (green vector), pointing away from the sum of the red and blue vectors of the individual inputs. Following this nonlinear transformation, the XOR outputs are now linearly decodable in the 2D MEG response space.</p><p>Finally, the fourth stage on a postcentral gyrus source (<xref ref-type="fig" rid="fig3">Figure 3A, G</xref>, NonLin&amp;RT, yellow) also nonlinearly represents the stimuli, also allowing linear readout of the XOR task outputs at 386ms. In addition, this source activity now relates trial by trial to behavioral RTs. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> shows that this last postcentral gyrus fourth stage (also in frontal regions) primarily relates to behavioral RTs.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows that the key differences in the AND and OR tasks are at the third and fourth stages, where the temporal-parietal (green) and postcentral gyrus (yellow) sources represent AND and OR solutions for task behavior. The earlier stages linearly represent the two inputs, with light blue and orange Lin, magenta LinBoth discriminating the four stimuli as in XOR participants (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> for individual replications, prevalence = 1 [0.75 1], MAP [96% HPDI], <xref ref-type="bibr" rid="bib17">Ince et al., 2021</xref>). In NonLin and NonLin&amp;RT stages, the representation is nonlinear and reflects the task (cf. vector diagrams inset, bottom right quadrant). In AND, the task output is linearly separable in the 2D MEG response space: the black [1,1] response is further from the other stimulus classes than they are from each other, see <italic>Methods, Representational Patterns</italic>. In OR, the task outputs are also linearly separable, with the gray [0,0] stimulus class represented apart from the other ones. This shows how these later post-200ms computational stages involve nonlinear task-specific stimulus representations in ventral (NonLin) and parietal (NonLin, NonLin&amp;RT) areas.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we addressed the challenge of understanding where, when, and how the brain dynamically implements different algorithmic computations over sensory inputs. We tightly controlled behavior using the simple logical functions XOR, OR, and AND as tasks that require different computations over the same tightly controlled binary inputs. Our analyses revealed, at the level of individual MEG sources, four main stages of computation that dynamically unfold from ~60 to 400 ms poststimulus. The first computation linearly discriminates the ‘on’ vs. ‘off’ state of each input in contral-lateral occipital cortex ~60ms poststimulus. This is followed by the linear discrimination of both inputs on occipital and ventral sources ~100ms, followed by the nonlinear integration of both inputs, revealing the XOR, AND, or OR task solution in 2D source response space in the parietal-temporal regions ~260ms, and finally the nonlinear integration with RT-related activity in postcentral gyrus ~400ms. These four stages are common to XOR, AND, and OR, with the main task-related changes occurring in the latter two nonlinear stages. Notably, we performed all statistical analyses leading to these results within each individual participant, controlling the familywise error rate (FWER) over all considered sources and time points. By treating each participant as an N-of-1 study, 10 participants per task provide 10 independent replications of the experiment. We replicated the four computational stages in at least 9/10 participants (and in two further replication experiments with similarly high prevalence), providing strong evidence that a majority of individuals in the population sampled and tested in the same way would show the same effects (<xref ref-type="bibr" rid="bib17">Ince et al., 2021</xref>).</p><sec id="s3-1"><title>Reverse engineering systems-level algorithms</title><p>Our systems-level approach aims to reverse engineer, from mass brain signals, the hierarchy of brain computations that represent and transform sensory inputs to produce behavior—i.e., the brain’s algorithm of the behavioral task. The four stages of computation that we systematically found in each individual participant and tasks meet the five key properties of an algorithm: (1) the inputs were specified as the four possible combinations of two binary inputs; (2) the output responses were also specified as the responses of the logical functions XOR, OR, and AND; (3) the algorithms were definite in each task, with a sequence of two characterized Lin and two NonLin computations that transformed the same inputs into the task-specific outputs; the algorithms were also (4) effective in the brain, in the sense that they only relied on brain resources, and (5) finite in processing time, producing behavior with ~450–500ms.</p><p>Note that reverse engineering a systems-level algorithm at the granularity of MEG brain measures does not preclude the existence of different compositions of algorithms at lower levels of granularity of individual neurons that together implement the higher-level algorithm (much like the lower granularity algorithms of machine language implement the higher-level algorithms of C++). Rather, such systems-level analysis provides constraints on where (the brain regions) and when (the specific time windows) specific computations take place, enabling targeted studies of the algorithmic ‘how’ across modalities and granularities of brain measures. <xref ref-type="bibr" rid="bib18">Jonas and Kording, 2017</xref> used a related systems-level approach to understand the hierarchy of computations of a microprocessor and concluded that there was risk that analytic approaches in neuroimaging could fall short of producing a meaningful algorithmic understanding of neural activity. We could do so here because we adhered to the main properties of an algorithm: our explicit behavioral tasks (i.e. XOR, OR, and AND) require an implementation of a specific computation on simple (i.e. fully characterized) binary inputs to achieve behavior. We could therefore trace the dynamic representations of the inputs into the 2D space of MEG activity to understand the stages of representation underlying the computation (i.e. Lin, LinBoth, and task-specific NonLin). Such descriptive models of an algorithm enable explicit testing of the different stages of the computation hierarchy. For example, by manipulating the timing or nature of the presented stimuli, or by targeting causal interventions (e.g. magnetic stimulation, or stimulus manipulations) at specific brain regions and peristimulus times.</p></sec><sec id="s3-2"><title>Generalization to naturalistic categorization tasks</title><p>Generalizing from our case study of the algorithms underlying the simple XOR, AND, and OR functions to more naturalistic face, object, and scene categorization tasks will incur many challenges that we can frame in the context of the properties of an algorithm detailed above.</p><p>A key challenge is that the task-relevant features of real-world faces, objects, and scenes, may be completely different for different behaviors and participants, effectively changing the inputs to the algorithm. Unfortunately, task- or participant-specific features are generally not considered in studies of neural representation, processing, and categorization. Their understanding remains a similar challenge for deep convolutional neural network research, including instances when these are used as models of the brain. Specifically, a key property of an algorithm is that we specify its inputs as precisely as possible. In real-world categorizations, this implies understanding which specific features of the complex images are task relevant for each particular participant performing each specific behavioral task. Furthermore, specification of the outputs is another key property of an algorithm. Passive viewing, or one-back tasks do not provide this specification. For example, from the same face, the feature of a smiling mouth feature will be used to overtly respond ‘happy’, but the forehead wrinkles to respond ‘45 years of age’; from the same car picture, its specific shape to respond ‘New Beetle’, but the front tyre shape to respond ‘flat tyre’; or the specific roof tiles to respond ‘Chrysler building’ but the density of buildings on the horizon to respond ‘city’; and so forth. Relatedly, experts vs. novices will use different features to classify the same pictures of the 35 different subtypes of sparrows that exist in North America. Such relative perceptual expertise and associated features generally characterize the relationship between visual cognition and outside world stimuli. Then, to infer the hierarchical stages of computation from the brain measures, we can start tracing the dynamic representation of these specific task-relevant input features, when we have formally characterized them, between stimulus onset and explicit output task behavior, as we did here. Different modalities or granularities of brain measures (e.g. M/EEG, 3/7T fMRI, NIRS vs. single electrodes (<xref ref-type="bibr" rid="bib8">Gidon et al., 2020</xref>) and electrode arrays) will likely provide complementary understandings (e.g. timing vs. location) of the computations in different brain regions. And when we finally have a model of the computation hierarchy (even initially a descriptive model), we can test its key properties.</p><p>To conclude, we reverse engineered four stages of dynamic algorithmic computations over the same sensory inputs that produce different behaviors in the brain. We could do so because we explicitly controlled the input features and the explicit tasks that each individual participant was instructed to resolve while we modeled their brain response with MEG source level activity. Therefore, our results and methodology pave the way to study algorithmic computations when the stimuli and tasks are more complex (e.g. face, object, and scene and their explicit categorizations) but well controlled (e.g. with generative systems rather than uncontrolled 2D images), as they are in any algorithm.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>We recruited 35 participants (all right handed; 24 women). All reported normal or corrected-to-normal vision and gave written informed consent to participate in the experiment and for their data to be anonymously published. We conducted the study according to the British Psychological Society ethics guidelines and was approved by the ethics committee at the College of Medical, Veterinary and Life Sciences, University of Glasgow.</p></sec><sec id="s4-2"><title>Stimuli</title><p>We synthesized an average face using a generative photorealistic 3D face model (<xref ref-type="bibr" rid="bib37">Yu et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Zhan et al., 2019a</xref>) to which we added glasses with an image editing program (Adobe Photoshop). Black and clear lenses produced four different input conditions corresponding to four classes of logical inputs: (1) both clear, in vector notation [0,0], (2) left clear/right dark, [0,1]; (3) left dark/right clear [1,0]; and (4) both dark, [1,1]. The edges of the left and the right lens were 0.5 deg of visual angle away from the centrally presented fixation cross.</p></sec><sec id="s4-3"><title>Task procedure</title><p>Each trial began with a central fixation cross displayed for a randomly chosen duration (between 500 and 1000ms), immediately followed by one of the four stimulus classes described above and displayed for 150ms. We instructed participants to maintain fixation on each trial, to pay attention to the left and the right lenses and to respond as quickly and accurately as possible by pressing one of two keys ascribed to each response choice with the index or middle fingers of their right hand. Responses were ‘same’ vs. ‘different’ in the XOR task; ‘both dark’ vs. ‘otherwise‘ in AND; or ‘at least one dark’ vs. ‘otherwise’ in OR. Participants were randomly allocated to one of the three tasks.</p><p>Stimuli were presented in blocks of 80 trials, with random intertrial interval of [800–1300ms] and randomized stimulus order in each block. Participants completed a total of 20–24 blocks split across 2–3 single day sessions, with short breaks between blocks. Each session lasted 2.5–3hr. We selected this recording time and number of trials based on sensitivity of MEG for within-participant whole brain corrected feature representation in previous experiments using face stimuli (<xref ref-type="bibr" rid="bib39">Zhan et al., 2019b</xref>; <xref ref-type="bibr" rid="bib15">Ince et al., 2015</xref>). We focus on within-participant inference in which each participant serves as an independent replication of the experiment. Inferring population prevalence of effects is less sensitive to the number of participants and more sensitive to the amount of data collected per participant (<xref ref-type="bibr" rid="bib17">Ince et al., 2021</xref>). Note that the 5–9hr of scanning time per participant we employ ( &gt;200hr scanning in total for the 30 participants in the main experiment) is far higher than typical standards in the field in which N = 30 participants might be scanned for 45min each (total scanning time~22hr).</p></sec><sec id="s4-4"><title>MEG data acquisition and preprocessing</title><p>We recorded the participants’ MEG activity using a 248-magnetometer whole-head system (MAGNES 3600 WH, 4D Neuroimaing) at a 1017 Hz sampling rate. We discarded each participant’s runs with more than 0.6 cm head movement measured with prerun vs. postrun head position recordings. Participants were excluded if the number of trials remaining after preprocessing (eye movement artifact rejection and rejecting runs for excessive head motion) was less than 700. We excluded five participants resulting in a final sample sizes of N = 30 (10 per task). Mean head movement (averaged across blocks) across participants was 0.3 cm (min = 0.12, max = 0.44).</p><p>We performed analyses with Fieldtrip (<xref ref-type="bibr" rid="bib28">Oostenveld et al., 2011</xref>) in MATLAB, according to recommended guidelines (<xref ref-type="bibr" rid="bib11">Gross et al., 2013</xref>). We high-pass filtered the data at 0.5 Hz (fifth order two-pass Butterworth IIR filter), filtered for line noise (notch filter in frequency space), and denoised via a Principtal Component Analysis (PCA) projection of the reference channels. We identified noisy channels, jumps, muscle, and other signal artifacts using a combination of automated techniques and visual inspection. The median number of trials for subsequent analyses was 1064 (min = 701, max = 1361).</p><p>Next, we epoched the data into trial windows ([−500 to 1000ms]) around stimulus onset, low-pass filtered the data at 45 Hz (third order two-pass Butterworth IIR filter), resampled to 256 Hz, and decomposed using Independent Component Analysis (ICA), separately for each participant. We identified and projected out of the data the ICA sources corresponding to heartbeat and eye blinks or movements (2–4 components per participant).</p></sec><sec id="s4-5"><title>Source reconstruction</title><p>For each participant, we coregistered their structural MRI scan with their head shape recorded on the first session and warped to standardized MNI coordinate space (<xref ref-type="bibr" rid="bib12">Gross, 2019</xref>). Using brain surfaces segmented from individual warped MRI, we then prepared a realistic single-shell head model. Next, we low-pass filtered the clean dataset at 40 Hz, re-epoched the data between –100 and 400ms around stimulus onset, demeaned using a prestimulus baseline, and computed covariance across the entire epoch. Using average sensor positions across good runs (i.e. where head movement was &lt;0.6 cm, see above), and a 6 mm uniform grid warped to standardized MNI space, we then computed the forward model, keeping the two orientations of MEG activity. We computed the Linearly Constrained Minimum Variance beamformer (<xref ref-type="bibr" rid="bib13">Hillebrand and Barnes, 2005</xref>) solution with parameters ‘lambda = 6%’ and ‘fixedori = no’. The resulting inverse filter applied to the sensor space MEG activity enabled reconstruction of the single-trial 2D MEG magnetic field vector (i.e. dipole with amplitude and direction) activity time courses on 12,773 grid points. Using a Talairaih-Daemon atlas, we excluded all cerebellar and noncortical sources and performed the statistical analyses on the remaining 5107 cortical grid points.</p></sec><sec id="s4-6"><title>Linear vs. nonlinear representations</title><sec id="s4-6-1"><title>Linear representation</title><p>Every 4ms time point between –100 and 400ms poststimulus, we computed independent multivariate linear regressions to model the dynamic representation of the state of each input (i.e. 0, clear lens vs. 1, dark lens) into the 2D MEG responses of each source. We computed three linear models covering each input separately (Left, L and right, R) and additively.<disp-formula id="equ1"><mml:math id="m1"><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi></mml:math></disp-formula></p><p>We fitted each model with ordinary least squares, resulting in beta coefficients for the intercept and slope. We quantified the fit in the 2D response space of the source with a multivariate R<sup>2</sup> that quantifies multivariate variance as the determinant of the covariance matrix:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo mathvariant="bold">,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the 2D source data, their mean, and model predictions respectively. This linear modeling produced a time course of R<sup>2</sup> values per source with 4ms resolution.</p><p>To control the FWER over all considered time points and sources, we computed a nonparametric statistical threshold with the method of maximum statistics (<xref ref-type="bibr" rid="bib10">Groppe et al., 2011</xref>). Specifically, on each of 100 permutations we randomly shuffled input state (‘on’ vs. ‘off’) across the experimental trials, repeated the linear modeling and R<sup>2</sup> computation explained above, and extracted the maximum R<sup>2</sup> across all sources and time points. This produced a distribution of 100 maximum R<sup>2</sup> values, of which we used the 95<sup>th</sup> percentile as statistical threshold (FWER p&lt;0.05).</p></sec><sec id="s4-6-2"><title>Nonlinear representation</title><p>A fourth model considered, for each source and time point the nonlinear interaction term between the left and Right inputs.<disp-formula id="equ5"><mml:math id="m5"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:mi>R</mml:mi></mml:math></disp-formula></p><p>A log-likelihood ratio (LLR) tested whether the added interaction term significantly improved model fit (p&lt;0.05), FDR corrected over time points and sources (<xref ref-type="bibr" rid="bib10">Groppe et al., 2011</xref>; <xref ref-type="bibr" rid="bib3">Benjamini and Yekutieli, 2001</xref>).</p></sec></sec><sec id="s4-7"><title>Representation patterns</title><p>Linear and nonlinear representations of the two inputs into 2D source activity could form a variety of different patterns. To ensure that these patterns corresponded to expectations (e.g. of an XOR solution), we applied two further computations at each source and time point. First, we computed the pairwise Mahalanobis distances as detailed below between the color-coded 2D distributions of single trial MEG activity in response to each input class (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). To do so, we averaged the covariance matrices of each pair of input conditions and multiplied the inverse average covariance by the difference of the condition means:<disp-formula id="equ7"><mml:math id="m7"><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>μ</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi> </mml:mi><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.2222222222222222em"/><mml:mi>*</mml:mi><mml:mspace width="0.2222222222222222em"/><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="0.2222222222222222em"/><mml:mi>*</mml:mi><mml:mspace width="0.2222222222222222em"/><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi> </mml:mi><mml:mi>μ</mml:mi><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>Then, we quantified the geometric relationships between the two-dimensional centroids of the source responses to each input class. We did so by combining the pairwise distances in the way that quantifies the expected representational pattern (see <xref ref-type="fig" rid="fig4">Figure 4</xref>, right):</p><list list-type="bullet"><list-item><p>left lens representation (LL): mean([d1, d3, d5, d6]) – mean([d2, d4]). This measure contrasts distances where the left lens state changes, with distances where the left lens state does not change.</p></list-item><list-item><p>right lens representation (RL): mean([d1, d2, d4, d6]) – mean([d3, d5]). As above, for the right lens.</p></list-item><list-item><p>both lenses representation (BL): mean(all) – std(all)</p></list-item><list-item><p>XOR representation: mean([d2, d3, d4, d5]) – mean([d1, d6]). Contrasts the distances between elements of the two different output classes with the distances between elements within each output class.</p></list-item><list-item><p>AND representation: mean([d4, d3, d6]) – mean([d1, d2, d5])</p></list-item><list-item><p>OR representation: mean([d6, d2, d5]) – mean([d4, d3, d1])</p></list-item></list><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Computation of representation patterns.</title><p>Note: the face stimulus was artificially synthesized and so does not belong to any real person.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-fig4-v1.tif"/></fig><p>We tested the statistical significance of each pattern with permutation testing, using again the method of maximum statistics. Over 25 permutations, at each source and time point, we randomly shuffled input ‘on’ and ‘off’, repeated the above distance calculations, computed the maximum differences, and used the 95<sup>th</sup> percentiles of these maxima as thresholds (FWER p&lt;0.05, corrected).</p><p>Finally, we weighted the significant XOR, AND, and OR representation patterns (see above) with the significant R<sup>2</sup> for linear representation patterns of the left or right input (see <italic>Methods, Linear Representation</italic>) or with the significant nonlinear LLR test statistic (see <italic>Methods, Nonlinear Representation</italic>).</p></sec><sec id="s4-8"><title>Localization of representation patterns</title><p>We quantified the temporal dynamics of the four stages of information processing in individual participants as follows. For early representation of the left and right lenses, we computed representation onsets as the first significant time point of R<sup>2</sup>. For early representation of both lenses, we computed its R<sup>2</sup> peak time. Finally, for XOR, AND, and OR nonlinear representations, we also selected the peak times of the respective representation task pattern distance measure. For each state, we then computed time stamps as median across observers and extracted the sourcewise representation averaged across participants at the corresponding time stamp. We plotted these sources back onto glass brains in <xref ref-type="fig" rid="fig1">Figure 1C</xref> using <italic>Nilearn</italic> (<xref ref-type="bibr" rid="bib1">Abraham et al., 2014</xref>).</p></sec><sec id="s4-9"><title>Clustering of the computation stages</title><p>To compute the number of computation stages that characterize the whole information processing, we applied a data-driven clustering analysis (k-means) on the 5107 × 102 (source × time points) space separately in XOR, AND, and OR tasks as follows:</p><sec id="s4-9-1"><title>Step 1</title><p>First, we transformed each participant’s MEG data into the main computation that each source performs at each time point, by assigning to each source the computation (i.e. LinLeft, LinRight, LinBoth, XOR, AND, or OR) with highest representation pattern score at this time point (relative to the participant’s distribution across all sources and time points). This produced a source × time matrix of strongest source-level computations for this participant. Examination of the data revealed strong regularities of the computations performed in different time windows (e.g. LinLeft early on), though each computation could be performed across slightly different sources of the same region across participants (e.g. right occipital cortex). Across participants, we therefore computed the modal computation at each source and time point, producing one group-level computation matrix per task that we then clustered over time, rather than over source × time, as we next explain.</p></sec><sec id="s4-9-2"><title>Step 2</title><p>In the task-specific computation matrix, we summed at each time point the number of sources that performed each computation (out of six, LinLeft, LinRight, LinBoth, XOR, AND, and OR). The resulting 6 (computations) × 102 time points matrix represented the total brain volume of each computation over time in the task.</p></sec><sec id="s4-9-3"><title>Step 3</title><p>We k-means clustered (k = 1–20, repeating 1000 times) each computation matrix from step 2, using the 102 time points as samples and selected <italic>k</italic> as the elbow of the within-cluster sums of point-to-centroid distances metric—i.e., as the furthest point from straight line between k = 1–20. In the XOR, AND, and OR tasks, different clusters therefore represent different stages of the full process over time, with different brain volumes of source-level computations (i.e. LinLeft, LinRight, LinBoth, XOR, AND and OR).</p><p><xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> shows that the XOR, AND, and OR tasks all had <italic>k</italic> = 5 as a good solution. First, a stage 0, before any computation starts, and then four distinct timed stages with different brain volumes of LinLeft, LinRight, LinBoth, XOR, AND, and OR computations. To visualize each stage, we used the onset time of stages 0, 1, and 2 and the central time point of stages 3 and 4. In each stage, we color coded at voxel level in the small brains the most frequent computation across participants (i.e. LinLeft, LinRight, LinBoth, XOR, AND, and OR).</p></sec></sec><sec id="s4-10"><title>Similarity of computation stages across tasks and stimuli</title><p>To test whether the XOR, AND, and OR tasks share stages of computation, we computed the percentage of sources that perform the same computations across any pair of stages. This produced a 12 (3 tasks × 4 stages)-by-12 similarity matrix that compares each stage in each task with all other stages in all other tasks. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref> reveals that stage 1 (LinLeft and LinRight, cyan and orange) and stage 2 (LinBoth, magenta) are similar across all tasks, whereas stages 3 and 4 (i.e. NonLin, green; NonLin &amp; RT, yellow) are specific to each task.</p><p>To test the generalization of computation stages between face and Gabor stimuli, we computed the similarity matrix between the group-level stages in XOR, AND, and OR tasks with faces and the individual participant’s (N = 3) stages in XOR with Gabor stimuli (computed as explained with steps 1–3 above, but here within participant). <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref> shows, for each Gabor participant, that their first two linear stages do indeed generalize to the first two linear stages of XOR, AND, and OR faces, whereas their third and fourth NonLinear stages only generalize to the third and fourth stages of XOR faces.</p></sec><sec id="s4-11"><title>Information theoretic analyses</title><p>We used information theory to quantify the association between RTs and MEG activity, as MI (&lt;RT; MEG<sub>t</sub>&gt;), splitting RTs into four equiprobable bins and using continuous MEG (on all sources and time points). To this end, we used Gaussian-Copula mutual information <xref ref-type="bibr" rid="bib16">Ince et al., 2017</xref> on all sources and time points. We assessed statistical significance with a permutation test (p&lt;0.05). <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> shows the source × time point average MI and its prevalence across 10 participants in each task.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Supervision, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Investigation, Methodology, Project administration, Supervision, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave written informed consent. We conducted the study according to the British Psychological Society ethics guidelines and was approved by the ethics committee at the College of Medical, Veterinary and Life Sciences, University of Glasgow.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-73651-transrepform1-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The analyzed data and custom code that support the findings of this study are deposited in Dryad: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.d7wm37q2x">https://doi.org/10.5061/dryad.d7wm37q2x</ext-link>. Any further information are available by request to the Lead Contact.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Jaworska</surname><given-names>K</given-names></name><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Nicola</surname><given-names>JVR</given-names></name><name><surname>Robin</surname><given-names>AAI</given-names></name><name><surname>Philippe</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>DifferentComputations_MEG_eLife</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.d7wm37q2x</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>PGS received support from the Wellcome Trust (Senior Investigator Award, UK; 107802) and the Multidisciplinary University Research Initiative/Engineering and Physical Sciences Research Council (USA, UK; 172046–01). RAAI was supported by the Wellcome Trust [214120/Z/18/Z]. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alamia</surname><given-names>A</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Alpha oscillations and traveling waves: Signatures of predictive coding?</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000487</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000487</pub-id><pub-id pub-id-type="pmid">31581198</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Yekutieli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The control of the false discovery rate in multiple testing under dependency</article-title><source>The Annals of Statistics</source><volume>29</volume><fpage>1165</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1214/aos/1013699998</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bugatus</surname><given-names>L</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task alters category representations in prefrontal but not high-level visual cortex</article-title><source>NeuroImage</source><volume>155</volume><fpage>437</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.062</pub-id><pub-id pub-id-type="pmid">28389381</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic predictions: oscillations and synchrony in top-down processing</article-title><source>Nature Reviews. Neuroscience</source><volume>2</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1038/35094565</pub-id><pub-id pub-id-type="pmid">11584308</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Hierarchical models in the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000211</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000211</pub-id><pub-id pub-id-type="pmid">18989391</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Zolnik</surname><given-names>TA</given-names></name><name><surname>Fidzinski</surname><given-names>P</given-names></name><name><surname>Bolduan</surname><given-names>F</given-names></name><name><surname>Papoutsi</surname><given-names>A</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Holtkamp</surname><given-names>M</given-names></name><name><surname>Vida</surname><given-names>I</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dendritic action potentials and computation in human layer 2/3 cortical neurons</article-title><source>Science</source><volume>367</volume><fpage>83</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.aax6239</pub-id><pub-id pub-id-type="pmid">31896716</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname><given-names>DM</given-names></name><name><surname>Urbach</surname><given-names>TP</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mass univariate analysis of event-related brain potentials/fields I: a critical tutorial review</article-title><source>Psychophysiology</source><volume>48</volume><fpage>1711</fpage><lpage>1725</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2011.01273.x</pub-id><pub-id pub-id-type="pmid">21895683</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Barnes</surname><given-names>GR</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Hillebrand</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Maess</surname><given-names>B</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Good practice for conducting and reporting MEG research</article-title><source>NeuroImage</source><volume>65</volume><fpage>349</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.001</pub-id><pub-id pub-id-type="pmid">23046981</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Magnetoencephalography in Cognitive Neuroscience: A Primer</article-title><source>Neuron</source><volume>104</volume><fpage>189</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.07.001</pub-id><pub-id pub-id-type="pmid">31647893</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillebrand</surname><given-names>A</given-names></name><name><surname>Barnes</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Beamformer analysis of MEG data</article-title><source>International Review of Neurobiology</source><volume>68</volume><fpage>149</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/S0074-7742(05)68006-3</pub-id><pub-id pub-id-type="pmid">16443013</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>van Rijsbergen</surname><given-names>NJ</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tracing the Flow of Perceptual Features in an Algorithmic Brain Network</article-title><source>Scientific Reports</source><volume>5</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1038/srep17681</pub-id><pub-id pub-id-type="pmid">26635299</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>1541</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1002/hbm.23471</pub-id><pub-id pub-id-type="pmid">27860095</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Paton</surname><given-names>AT</given-names></name><name><surname>Kay</surname><given-names>JW</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bayesian inference of population prevalence</article-title><source>eLife</source><volume>10</volume><elocation-id>e62461</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.62461</pub-id><pub-id pub-id-type="pmid">34612811</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonas</surname><given-names>E</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Could a Neuroscientist Understand a Microprocessor?</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005268</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005268</pub-id><pub-id pub-id-type="pmid">28081141</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention reduces spatial uncertainty in human ventral temporal cortex</article-title><source>Current Biology</source><volume>25</volume><fpage>595</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.12.050</pub-id><pub-id pub-id-type="pmid">25702580</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.011</pub-id><pub-id pub-id-type="pmid">23265839</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Douglas</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cognitive computational neuroscience</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1148</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0210-5</pub-id><pub-id pub-id-type="pmid">30127428</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linde-Domingo</surname><given-names>J</given-names></name><name><surname>Treder</surname><given-names>MS</given-names></name><name><surname>Kerrén</surname><given-names>C</given-names></name><name><surname>Wimber</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evidence that neural information flow is reversed between object perception and object reconstruction from memory</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>179</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08080-2</pub-id><pub-id pub-id-type="pmid">30643124</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Little</surname><given-names>DR</given-names></name><name><surname>Smith</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Replication is already mainstream: Lessons from small-N designs</article-title><source>The Behavioral and Brain Sciences</source><volume>41</volume><elocation-id>e141</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X18000766</pub-id><pub-id pub-id-type="pmid">31064541</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Minsky</surname><given-names>M</given-names></name><name><surname>Papert</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Perceptrons: An Introduction to Computational Geometry</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/11301.001.0001</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Encoding and decoding in fMRI</article-title><source>NeuroImage</source><volume>56</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id><pub-id pub-id-type="pmid">20691790</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extensive sampling for complete models of individual brains</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>45</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.12.008</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Computational neuroimaging: localising Greek letters? Comment on Forstmann et al</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><elocation-id>450</elocation-id><pub-id pub-id-type="doi">10.1016/j.tics.2011.07.012</pub-id><pub-id pub-id-type="pmid">21862381</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Philiastides</surname><given-names>MG</given-names></name><name><surname>Sajda</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Quality of evidence for perceptual decision making is indexed by trial-to-trial variability of the EEG</article-title><source>PNAS</source><volume>106</volume><fpage>6539</fpage><lpage>6544</lpage><pub-id pub-id-type="doi">10.1073/pnas.0812589106</pub-id><pub-id pub-id-type="pmid">19342495</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual categorization shapes feature selectivity in the primate temporal cortex</article-title><source>Nature</source><volume>415</volume><fpage>318</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1038/415318a</pub-id><pub-id pub-id-type="pmid">11797008</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>PL</given-names></name><name><surname>Little</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Small is beautiful: In defense of the small-N design</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>2083</fpage><lpage>2101</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1451-8</pub-id><pub-id pub-id-type="pmid">29557067</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name><name><surname>Felleman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Information processing in the primate visual system: an integrated systems perspective</article-title><source>Science</source><volume>255</volume><fpage>419</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1126/science.1734518</pub-id><pub-id pub-id-type="pmid">1734518</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>MC-K</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Complete functional characterization of sensory neurons by system identification</article-title><source>Annual Review of Neuroscience</source><volume>29</volume><fpage>477</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113024</pub-id><pub-id pub-id-type="pmid">16776594</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>de Gardelle</surname><given-names>V</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rhythmic fluctuations in evidence accumulation during decision making in the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>847</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.015</pub-id><pub-id pub-id-type="pmid">23177968</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Garrod</surname><given-names>OGB</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Perception-driven facial expression synthesis</article-title><source>Computers &amp; Graphics</source><volume>36</volume><fpage>152</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/j.cag.2011.12.002</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Garrod</surname><given-names>OGB</given-names></name><name><surname>van Rijsbergen</surname><given-names>N</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Modelling face memory reveals task-generalizable representations</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>817</fpage><lpage>826</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0625-3</pub-id><pub-id pub-id-type="pmid">31209368</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>van Rijsbergen</surname><given-names>N</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Dynamic Construction of Reduced Representations in the Brain for Perceptual Decision Behavior</article-title><source>Current Biology</source><volume>29</volume><fpage>319</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.049</pub-id><pub-id pub-id-type="pmid">30639108</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73651.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Huan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group></front-stub><body><p>How does the brain implement basic logical computations (AND, OR, XOR) regardless of stimulus types? This is one of the most fundamental questions in cognitive neuroscience. This MEG study, by combining interesting experimental paradigms and sophisticated signal analyses, demonstrates four serial neural components in different brain regions that correspond to four system-level computations, respectively.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73651.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Different computations over the same inputs produce selective behavior in algorithmic brain networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Chris Baker as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Testing the representational generalization across stimulus set. In principle, the logical computation should be independent of the stimuli and would engage similar four-step neural computation in the brain for many types of stimuli. Although the authors demonstrate similar decoding temporal course and patterns for grating and serially presented stimuli (in supplementary materials), it is important to perform a generalization analysis across different stimuli. Specifically, the classifier obtained for face stimuli in the main experiment could be used to decode computations performed over grating stimuli (data in supplementary materials), for example.</p><p>2) Clarifying the temporal relationship of the four components and why their alignment over time still supports the four-step conclusion. Moreover, the 3nd component also shows early onset activation. How to reconcile the results with the 4-step computation view? Please see details in the first two comments raised by Reviewer 2.</p><p>3) Adding analyses to test alternative computational steps, e.g., 3 system-level, 2 system-level for comparison, and clarifying why the proposed 4-step is the best model to characterize the whole process.</p><p>4) The behavioral relevance analysis was only performed on the fourth component. What about the first three components which should in principle be related to behavioral performance as well? More generally, is there any neural signature that is related to behavior for both linear and nonlinear calculations?</p><p>5) The authors should collect new data or at least address the possibility of the involvement of eye movement in the four levels of computation.</p><p><italic>Reviewer #1:</italic></p><p>This work by Jaworska et al. examined the spatiotemporal correlates of logical computations over the same stimuli in the human brain using MEG recordings. They revealed four neural components that occur in different brain regions and at varied latencies, corresponding to four system-level computations respectively.</p><p>Overall, it is an important study addressing the most fundamental question in cognitive neuroscience, that is, how, where and when do the basic logic computations (AND, OR, XOR) occur in the brain? The study used advanced analysis approaches to address the thorny question and the results are impressively clean and robust and have been replicated for different stimulus sets and conditions.</p><p>My major concern is the representational generalization across different stimuli. Specifically, the logical computation (AND, OR, XOR) in principle should be independent of the employed stimuli (face, gratings, etc.) and would thus engage similar neural computation in the brain for different types of stimuli. Therefore, it is important to confirm that the observed logical computation components could be generalized across different stimulus setting (representational generalization across stimuli).</p><p>1) If my understanding is correct, the logical computation (AND, OR, XOR) should be independent of the stimuli and therefore would engage similar four-step neural computation in the brain for other types of stimuli. Although the authors demonstrate similar decoding temporal course and patterns for grating and serially presented stimuli (in supplementary materials), I think it is important to perform a generalization analysis across different stimuli. Specifically, we would expect that the classifier obtained for face stimuli could be used to decode computations performed over grating stimuli, for example.</p><p>2) I am curious how could the authors make sure that the logical computation (based on the binary classification on four types of stimuli) are the genuine calculation in the brain, since it is completely defined in terms of the experimenter's terminology and the subjects might use different strategy. Have the authors considered other alternative computation over the same inputs? Or put in another way, how could the current results be incorporated into or account for previous findings?</p><p>3) The last component is interesting and shows behavioral correlates, which is important evidence. If my understanding is correct, that component only applies to nonlinear integration in combination with RT. What about the behaviorally related linear computation? Is there a general behaviorally related neural component that is related to both linear and nonlinear computation?</p><p><italic>Reviewer #2:</italic></p><p>How our brain dynamically represents and transforms the same visual input into integrated features to produce different behavior remains unclear. To study the dynamic algorithms from mass neural signals, the authors recorded the MEG activity of participants who resolved the classic XOR, OR, and AND behavioral tasks. Using linear and nonlinear representations, they found that source-localized MEG activity progresses through four systems-level computations identified within individual participants. The first two computations similarly represent the inputs across the three tasks; the last two differently represent the inputs in a task-dependent manner. The topic is interesting and timely. The study is elegantly designed and the data statistics are highly significant.</p><p>To study the dynamic algorithms from mass neural signals, the authors recorded the MEG activity of participants who resolved the classic XOR, OR, and AND behavioral tasks. Using linear and nonlinear representations, they found that source-localized MEG activity progresses through four systems-level computations identified within individual participants. The first two computations similarly represent the inputs across the three tasks; the last two differently represent the inputs in a task-dependent manner. The topic is interesting and timely. The study is elegantly designed and the data statistics are highly significant. I have some comments listed as below.</p><p>1. For each task, the authors proposed the 4 systems-level stages of computation link stimulus to behavior (the first two stages represent and linearly discriminate the visual inputs; the third and fourth stages nonlinearly integrate them in a task-specific manner), according to the different onsets from post-stimulus (~60, 100, 260, and 400ms for each stages, respectively). However, the time window of the first stage (Lin) is the same as the second stage (LinBoth), namely, 74-117ms. This was also reflected very well in the results from Figure 1B, the &quot;Lin&quot; and &quot;LinBoth&quot; computations have almost the same onsets and peaks, particularly, the peak of the blue line (first stage, Lin) looks like much latter than the magenta line (second stage, LinBoth). I am confused that how the authors distinguished these two stages when they had the same time window and why the authors proposed they were the two different stages rather than the same stage during their computations. More generally, the authors should offer more evidence for the 4 systems-level stages of computation, for example, whether it explained their results better than the 3 systems-level stages or 2 systems-level stages of computation.</p><p>2. Similar to the first point, for the results in Figure 3, there were several significant peak values in the very early time window (0-60ms) for the third stage (&quot;NonLin&quot;). However, the author only focused on the late time window (~260ms) for this stage. How to reconcile these early peaks (early effects) of the third (late) stage?</p><p>3. The behavior data and decoding performance lacked in the first three systems-level computations. Functional inferences based on hypothetical computations of MEG data are not convincing.</p><p>4. I would like to suggest the authors collect eye movement data to address whether eye movements are a possible confound for the difference between the four systems-level computations.</p><p>5. The legend descriptions are too long and confusing, like figure 2. I would like to suggest the authors label small figures in one figure to make the legend easier to understand by readers.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73651.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Testing the representational generalization across stimulus set. In principle, the logical computation should be independent of the stimuli and would engage similar four-step neural computation in the brain for many types of stimuli. Although the authors demonstrate similar decoding temporal course and patterns for grating and serially presented stimuli (in supplementary materials), it is important to perform a generalization analysis across different stimuli. Specifically, the classifier obtained for face stimuli in the main experiment could be used to decode computations performed over grating stimuli (data in supplementary materials), for example.</p></disp-quote><p>We framed this question in the context of the 4-stage clustering analysis by examining the generalization of the Gabor stimuli using the four-stage model derived from the face stimuli. Specifically, we also clustered the Stages of each Gabor XOR participant (N = 3) (see Figure 1—figure supplement 3), leading to four stages in each, and compared each stage within the similarity matrix of the four computation stages over face stimuli (see Figure 1—figure supplement 1). In each Gabor participant, their first two Linear stages are similar to XOR, AND and OR face participants, whereas their third and fourth NonLinear stages are similar only to the third and fourth stages of XOR face participants. This generalizes to Gabor stimuli the 4-stage computation model derived from face stimuli.</p><disp-quote content-type="editor-comment"><p>2) Clarifying the temporal relationship of the four components and why their alignment over time still supports the four-step conclusion. Moreover, the 3nd component also shows early onset activation. How to reconcile the results with the 4-step computation view? Please see details in the first two comments raised by Reviewer 2.</p></disp-quote><p>Clarification of Stages 1 and 2. The alignment in time of the first and second stages in the original manuscript is now clearly separated in each task in the revised figures (see the plots of the revised Figures 1 and 3 in the manuscript reported in a “before” and “after” <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). The next section explains the revised computations that led to these changes. We know from the anatomy of the visual system that information from a one side of the visual field will first be represented in early visual cortical areas in the contralateral hemisphere. We do indeed find this at Stage 1, but this first stage is very brief (~10-15 ms), because the bottom-up input is quickly propagated across hemispheres to higher visual areas. This duration is consistent with the latencies of cross-hemispheric transfer (~15 ms, Brown et al. 1994; Ipata et al., 1997) also found in our earlier work on cross hemisphere communication of specific visual features (Ince et al. 2016). The second stage that linearly represents both inputs onto the same MEG source in occipital and temporal-ventral cortex is longer (color-coded in purple, see Stage 2 in Figure 1—figure supplement 1 and (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>)).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>This image shows the changes to the main results presented in Figures 1 and 3 between the original and the revised manuscripts.</title><p>The color-coded curves present each computation. We can now better see the 4 distinct stages (cf. 4-stage image above), where the brief timing of Stage 1 (in orange and blue, LinLeft and LinRight computations) is now better decoupled from that of Stage 2 (in magenta, LinBoth). Individual participants peak time results for each computation (represented as colored dots) further demonstrates the decoupling into four stages.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-73651-sa2-fig1-v1.tif"/></fig><p>Brown WS, Larson EB, Jeeves MA (1994) Directional asymmetries in interhemispheric transmission time: Evidence from visual evoked potentials. <italic>Neuropsychologia</italic> 32:439–448.</p><p>Ipata A, Girelli M, Miniussi C, Marzi CA (1997) Interhemispheric transfer of visual information in humans: the role of different callosal channels. <italic>Arch Ital Biol</italic> 135:169–182.</p><p>Ince, A.A., Jaworska, K., Gross, J., Panzeri, S., van Rijsbergen, N., Rousselet, G. and Schyns, P.G. (2016). The deceptively simple N170 reflects network information processing mechanisms involving feature coding and transfer across hemispheres. <italic>Cerebral Cortex</italic>, 11, 4123-4135.</p><p>Early unexpected peaks. The early peaks of nonlinear computations in Stage 3 (particularly of the OR task) were indeed problematic. We went back to our pipeline, starting with the data of each participant. We noticed in a few participants that an anomaly occurred due to the potential for the task representational distance measure to be noisy, particularly when brain regions had weak representations of the inputs at specific time points. Though the Log-Likelihood Ratio measure of non-linear computation was significant, its effect size was extremely weak compared to the later effects, implying that the task distance was correspondingly less meaningful. We addressed this anomaly by multiplying the statistically significant R2 and LLR (i.e. the effect size for the evidence of the linear and nonlinear relationships) with the statistically significant distance metrics for each computation, to better combine the two measures (and avoid considering distance metrics with very weak effects). This did not change the substance of a 4-stage process but smoothed these anomalies in the plots, as now shown in the key panels of Figures 1 and 3 of the revised manuscript and in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>. An important side-effect of the better quantifications is the clearer expected separation of Stages 1 and 2 (including in individual participants data), and all stages are sharper in time. We updated all figure and figure supplements accordingly.</p><disp-quote content-type="editor-comment"><p>3) Adding analyses to test alternative computational steps, e.g., 3 system-level, 2 system-level for comparison, and clarifying why the proposed 4-step is the best model to characterize the whole process.</p></disp-quote><p>We thank you and the reviewers for raising these important points. In our methods we described the computations that led to 4 stages, but we did not explicitly test explicitly alternatives numbers of stages (2-, 3-, 4- and more). In our revised manuscript, we now explicitly test these alternative numbers of stages to characterize the whole process. We used a simple data-driven k-means clustering analysis over the measures of each computation across time, pooling all participants’ data and performed separately in the XOR, AND and OR tasks. This created a computation space, where clusters represent different stages of the full process over time, with different brain volumes of source-level computations (i.e. Lin left and right, LinBoth, NonLin XOR, AND and OR). In this space, the data clustered into 4 stages in each task (i.e. with k = 5, determined using the elbow method), with a first null Stage 0 of no computation followed by 4 stages, each characterized with brain volumes of sourcelevel computations (as shown in the new Figure 1—figure supplement 1). Using in each task the indexes to these clusters shows that each cluster does indeed capture a specific stage of the entire process that fits the 4-stage data presented in Figures 1 and 3 of the original paper. To conclude, 4stage is the better model of the entire process.</p><p>Furthermore, a similarity analysis of the 4 stages in each task (see Figure 1—figure supplement 1B) revealed the similarity of Stages 1 and 2 of linear computations across the XOR, AND and OR tasks (i.e. Stage 1: Lin left and right, cyan and orange; Stage 2: LinBoth), followed by dissimilar nonlinear computations (i.e. NonLin, green; NonLin and RT) to compute and XOR, AND and OR for task specific behaviors.</p><disp-quote content-type="editor-comment"><p>4) The behavioral relevance analysis was only performed on the fourth component. What about the first three components which should in principle be related to behavioral performance as well? More generally, is there any neural signature that is related to behavior for both linear and nonlinear calculations?</p></disp-quote><p>Thank for you for this comment. We had performed this full analysis but did not report it. It is now included in the manuscript (as Figure 2—figure supplement 1) and we can indeed confirm that the fouth Stage is the only one that relates to behavioral reaction time.</p><disp-quote content-type="editor-comment"><p>5) The authors should collect new data or at least address the possibility of the involvement of eye movement in the four levels of computation.</p></disp-quote><p>This is a difficult point to address for two main reasons: First, we did not collect eye movement data for all participants and so would need to rerun the entire experiment to address this comment. Second, there is no specific proposal for how eye movements could confound the differences between the four systems-level computation stages that we report. Here, we review the arguments that make such confounds unlikely. First, consider that our experimental conditions and task instructions were optimized to minimize eye movement artifacts. Participants were instructed to fixate the center of the screen and not move their eyes throughout the experiment, a fixation cross was presented at the center of the screen on each trial (for a random duration of 500-1000ms), the stimuli were presented for only 150 ms (the timing of only one potential fixation) and preprocessing of our MEG data removed all eye movements artifacts (we now amended the manuscript to make this explicit). Second, further consider that each stage is consistently located in specific brain regions, which are not primarily involved with fixation control (the frontal eye field is). Specifically, we located LinRight and LinLeft at left and right occipital cortex at Stage 1, LinBoth at Stage 2, starting ~15 ms later, in mid-line occipital cortex and right fusiform gyrus, and NonLin XOR, AND and OR task-relevant computations in temporal parietal, parietal and pre-motor cortex. Furthermore, we now show that Stages 1 and 2 are similar across all three tasks, with rapid sequencing (of ~10-15 ms) compatible with the timing of hemispheric transfer (Brown et al., 1994; Ince et al., 2016; Ipata et al., 1997), not with the timing of different fixations (each would take ~150 – 300 ms with face stimuli). The last two NonLin computations are different across the three tasks, but they occur between ~200-400 ms, when the stimulus is not on the screen anymore. In sum, as the two inputs are identical in all tasks, but disappear from the screen (at 150 ms), it is unclear how specific eye movements would confound Stages 3 and 4 (~200-400 ms post-stimulus). These two stages perform task-specific nonlinear integrations (of inputs absent from the screen at this time) over 76 ms in temporal, parietal and premotor cortex (and not in the frontal eye field). In sum, we deem it unlikely that different patterns of eye movement in each task could confound the stages of computation that we report.</p><p>Brown WS, Larson EB, Jeeves MA (1994) Directional asymmetries in interhemispheric transmission time: Evidence from visual evoked potentials. <italic>Neuropsychologia</italic> 32:439–448.</p><p>Ipata A, Girelli M, Miniussi C, Marzi CA (1997) Interhemispheric transfer of visual information in humans: the role of different callosal channels. <italic>Arch Ital Biol</italic> 135:169–182.</p><p>Ince, A.A., Jaworska, K., Gross, J., Panzeri, S., van Rijsbergen, N., Rousselet, G. and Schyns, P.G. (2016). The deceptively simple N170 reflects network information processing mechanisms involving feature coding and transfer across hemispheres. <italic>Cerebral Cortex</italic>, 11, 4123-4135.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>1) If my understanding is correct, the logical computation (AND, OR, XOR) should be independent of the stimuli and therefore would engage similar four-step neural computation in the brain for other types of stimuli. Although the authors demonstrate similar decoding temporal course and patterns for grating and serially presented stimuli (in supplementary materials), I think it is important to perform a generalization analysis across different stimuli. Specifically, we would expect that the classifier obtained for face stimuli could be used to decode computations performed over grating stimuli, for example.</p></disp-quote><p>We thank the reviewer for this comment. Our analyses are mainly within participant and so we do not have an explicit classifier for the group (and such classifier would likely require higher participant numbers). Thus, we addressed the question of generalization by considering explicitly the similarity of stages of computation across the XOR, AND and OR groups of participants we have, with an approach that also speaks to the generalization from face stimuli to other types of stimuli. See the fuller response in point 3 of the response to the Editors and the generalization data in Figure 1—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>2) I am curious how could the authors make sure that the logical computation (based on the binary classification on four types of stimuli) are the genuine calculation in the brain, since it is completely defined in terms of the experimenter's terminology and the subjects might use different strategy. Have the authors considered other alternative computation over the same inputs? Or put in another way, how could the current results be incorporated into or account for previous findings?</p></disp-quote><p>Here, we took the simplest computations that we could think of, documented in theory of computation as a well-known nonlinearity required to resolve XOR (in contrast to AND and OR). Each of these tasks are defined completely by a four element “truth table” and theory of computation consider them to be undecomposable computational primitives. We acknowledge the limitations of such simple tasks, but their advantage is that if the participants are correctly performing it, then we can be confident that somewhere in their brains the XOR computation (i.e. the rule specified in the truth table) is indeed being dynamically implemented. It is difficult to see how different strategies than nonlinear integration could be employed to perform XOR starting with two separate inputs. We did indeed ensure (cf. Stage 1) initial contra-lateral representation of each input (as predicted from neurophysiology and neuroanatomy) to enable a network model that takes in two separate inputs and must necessarily integrate them for behavior. From there on, we traced where and when MEG source activity linearly (Stage 2) and nonlinearly (Stages 3 and 4) represents the same two inputs on the way to behavior.</p><p>As we cannot access the ground-truth computations of the brain, all we can do is model them. With each task (XOR, AND and OR) and participant (N = 10 in each group), we now show similar Stages 1 and 2 of linear computations over the two inputs and divergent, task-related Stage 3 and Stage 4 computations over these same inputs (cf. Figure 1—figure supplement 1). Thus, within each task and across the tasks (and now also across stimuli), we are validating that the brain does indeed perform different computations over the same inputs, both at different stages of the process within task and across the final two stages across tasks.</p><disp-quote content-type="editor-comment"><p>3) The last component is interesting and shows behavioral correlates, which is important evidence. If my understanding is correct, that component only applies to nonlinear integration in combination with RT. What about the behaviorally related linear computation? Is there a general behaviorally related neural component that is related to both linear and nonlinear computation?</p></disp-quote><p>Thank for you for this comment. We had performed this analysis but did not report it. We have now included in the manuscript (as Figure 2—figure supplement 1) and can indeed confirm that the fourth component is the only one that relates to behavioral reaction time.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The study is elegantly designed and the data statistics are highly significant. I have some comments listed as below.</p><p>1. For each task, the authors proposed the 4 systems-level stages of computation link stimulus to behavior (the first two stages represent and linearly discriminate the visual inputs; the third and fourth stages nonlinearly integrate them in a task-specific manner), according to the different onsets from post-stimulus (~60, 100, 260, and 400ms for each stages, respectively). However, the time window of the first stage (Lin) is the same as the second stage (LinBoth), namely, 74-117ms. This was also reflected very well in the results from Figure 1B, the &quot;Lin&quot; and &quot;LinBoth&quot; computations have almost the same onsets and peaks, particularly, the peak of the blue line (first stage, Lin) looks like much latter than the magenta line (second stage, LinBoth). I am confused that how the authors distinguished these two stages when they had the same time window and why the authors proposed they were the two different stages rather than the same stage during their computations. More generally, the authors should offer more evidence for the 4 systems-level stages of computation, for example, whether it explained their results better than the 3 systems-level stages or 2 systems-level stages of computation.</p></disp-quote><p>We thank the reviewer for this comment, agree with them and have developed a new data-driven analysis that delivers the 4-stage account of the data, rather than fewer or more stages. We presented these results in the context of the overall response to the 5 main points of the Editors.</p><disp-quote content-type="editor-comment"><p>2. Similar to the first point, for the results in Figure 3, there were several significant peak values in the very early time window (0-60ms) for the third stage (&quot;NonLin&quot;). However, the author only focused on the late time window (~260ms) for this stage. How to reconcile these early peaks (early effects) of the third (late) stage?</p></disp-quote><p>We thank again the reviewer for this comment and for pointing to these peaks that caused confusion.</p><p>We explained how we addressed the peaks in the response to the Editors and provide a “Before and After” (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>) that handle these anomalies and revised Figures 1 and 3 accordingly. Note the clearer separation of the early stages (including individual participants data) and their increased sharpness in time.</p><disp-quote content-type="editor-comment"><p>3. The behavior data and decoding performance lacked in the first three systems-level computations. Functional inferences based on hypothetical computations of MEG data are not convincing.</p></disp-quote><p>Thank for you for this comment. We had performed this analysis but did not report it. We have now included in the manuscript (Figure 2—figure supplement) and can indeed confirm that the fourth Stage is the only one that relates to behavioral reaction time. Please see response to the Editors above for the figure showing the data.</p><disp-quote content-type="editor-comment"><p>4. I would like to suggest the authors collect eye movement data to address whether eye movements are a possible confound for the difference between the four systems-level computations.</p></disp-quote><p>See response to this point in response to the Editors’ comments.</p><disp-quote content-type="editor-comment"><p>5. The legend descriptions are too long and confusing, like figure 2. I would like to suggest the authors label small figures in one figure to make the legend easier to understand by readers.</p></disp-quote><p>We agree, have simplified the caption, and added a Supplementary video (Figure 1-video supplement 1) that presents the dynamics that Figure 2 illustrated statically. The figures and new video should make the specifics of the different dynamic computations over the same input a lot easier to understand.</p></body></sub-article></article>