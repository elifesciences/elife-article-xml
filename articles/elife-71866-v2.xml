<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">71866</article-id><article-id pub-id-type="doi">10.7554/eLife.71866</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Aberrant causal inference and presence of a compensatory mechanism in autism spectrum disorder</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-244598"><name><surname>Noel</surname><given-names>Jean-Paul</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5297-3363</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-245641"><name><surname>Shivkumar</surname><given-names>Sabyasachi</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-90367"><name><surname>Dokka</surname><given-names>Kalpana</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-232079"><name><surname>Haefner</surname><given-names>Ralf M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5031-0379</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-138210"><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9650-8962</contrib-id><email>da93@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York City</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>Brain and Cognitive Sciences, University of Rochester</institution></institution-wrap><addr-line><named-content content-type="city">Rochester</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Department of Neuroscience, Baylor College of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Yu</surname><given-names>Xiang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e71866</elocation-id><history><date date-type="received" iso-8601-date="2021-07-01"><day>01</day><month>07</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-05-15"><day>15</day><month>05</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Noel, Shivkumar, Dokka et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Noel, Shivkumar, Dokka et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-71866-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-71866-figures-v2.pdf"/><abstract><p>Autism spectrum disorder (ASD) is characterized by a panoply of social, communicative, and sensory anomalies. As such, a central goal of computational psychiatry is to ascribe the heterogenous phenotypes observed in ASD to a limited set of canonical computations that may have gone awry in the disorder. Here, we posit causal inference – the process of inferring a causal structure linking sensory signals to hidden world causes – as one such computation. We show that audio-visual integration is intact in ASD and in line with optimal models of cue combination, yet multisensory behavior is anomalous in ASD because this group operates under an internal model favoring integration (vs. segregation). Paradoxically, during explicit reports of common cause across spatial or temporal disparities, individuals with ASD were less and not more likely to report common cause, particularly at small cue disparities. Formal model fitting revealed differences in both the prior probability for common cause (p-common) and choice biases, which are dissociable in implicit but not explicit causal inference tasks. Together, this pattern of results suggests (i) different internal models in attributing world causes to sensory signals in ASD relative to neurotypical individuals given identical sensory cues, and (ii) the presence of an explicit compensatory mechanism in ASD, with these individuals putatively having learned to compensate for their bias to integrate in explicit reports.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>autism</kwd><kwd>inference</kwd><kwd>multisensory</kwd><kwd>perception</kwd><kwd>bayesian</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIH U19NS118246</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><name><surname>Haefner</surname><given-names>Ralf M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014370</institution-id><institution>Simons Foundation Autism Research Initiative</institution></institution-wrap></funding-source><award-id>396921</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Individuals within the autism spectrum disorder implicitly outweigh integration (rather than segregating) when performing causal inference and have developed an explicit compensatory mechanism as reflected in choice biases.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Autism spectrum disorder (ASD) is a heterogenous neurodevelopmental condition characterized by impairments across social, communicative, and sensory domains (<xref ref-type="bibr" rid="bib3">American Psychiatric Association, 2013</xref>; see also <xref ref-type="bibr" rid="bib46">Robertson and Baron-Cohen, 2017</xref> for a review focused on sensory processing in ASD). Given this vast heterogeneity, many <xref ref-type="bibr" rid="bib29">Lawson et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Lawson et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Lawson et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Lieder et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Noel et al., 2020</xref>; <xref ref-type="bibr" rid="bib39">Noel et al., 2021a</xref>, <xref ref-type="bibr" rid="bib40">Noel et al., 2021b</xref>; <xref ref-type="bibr" rid="bib54">Series, 2020</xref> have recently turned their attention to computational psychiatry to ascribe the diverse phenotypes within the disorder to a set of canonical computations that may have gone awry.</p><p>A strong yet unexplored candidate for such a computation is causal inference (<xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>). In causal inference, observers first make use of observations from their sensory milieu to deduce a putative causal structure – a set of relations between hidden (i.e. not directly observable) source(s) in the world and sensory signals (e.g. photons hitting your retina and air-compression waves impacting your cochlea). For instance, in the presence of auditory and visual speech signals, one may hypothesize a single speaker emitting both auditory and visual signals, or contrarily, the presence of two sources, e.g., a puppet mouthing (visual) and the unskillful ventriloquist emitting sounds (auditory). This internal model linking world sources to signals then impacts downstream processes. If signals are hypothesized to come from a common source, observers may combine these redundant signals to ameliorate the precision (<xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>) and accuracy (<xref ref-type="bibr" rid="bib43">Odegaard et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Dokka et al., 2015</xref>) of their estimates. In fact, an array of studies <xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib22">Hillis et al., 2002</xref>; <xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>; <xref ref-type="bibr" rid="bib25">Kersten et al., 2004</xref> have suggested that humans combine sensory signals weighted by their reliability. On the other hand, hypothesizing that a single source exists, when in fact multiple do, may lead to perceptual biases (as in the ventriloquist example).</p><p>It is well established that humans perform causal inference in solving a wide array of tasks, such as spatial localization (<xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib43">Odegaard et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib48">Rohe and Noppeney, 2016</xref>), orientation judgments (<xref ref-type="bibr" rid="bib57">van den Berg et al., 2012</xref>), oddity detection (<xref ref-type="bibr" rid="bib23">Hospedales and Vijayakumar, 2009</xref>), rate detection (<xref ref-type="bibr" rid="bib7">Cao et al., 2019</xref>), verticality estimation (<xref ref-type="bibr" rid="bib12">de Winkel et al., 2018</xref>), spatial constancy (<xref ref-type="bibr" rid="bib44">Perdreau et al., 2019</xref>), speech perception (<xref ref-type="bibr" rid="bib34">Magnotti et al., 2013</xref>), time-interval perception (<xref ref-type="bibr" rid="bib53">Sawai et al., 2012</xref>), and heading estimation (<xref ref-type="bibr" rid="bib1">Acerbi et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>), among others. As such, causal inference may be a canonical computation, ubiquitously guiding adaptive behavior and putatively underlying a wide array of (anomalous) phenotypes, as is observed in autism.</p><p>Indeed, the hypothesis that causal inference may be anomalous in ASD is supported by a multitude of tangential evidence, particularly within the study of multisensory perception. Namely, the claims that multisensory perception is anomalous in ASD are abundant and well established (see <xref ref-type="bibr" rid="bib5">Baum et al., 2015</xref> and <xref ref-type="bibr" rid="bib59">Wallace et al., 2020</xref>, for recent reviews), yet these studies tend to lack a strong computational backbone and have not explored whether these deficits truly lie in the ability to perform cue combination, or in the ability to deduce when cues ought to (vs. not) be combined. In this vein, we have demonstrated that optimal cue combination for visual and vestibular signals is intact in ASD (<xref ref-type="bibr" rid="bib65">Zaidel et al., 2015</xref>). In turn, the root of the multisensory deficits in ASD may not be in the integration process itself (see <xref ref-type="bibr" rid="bib38">Noel et al., 2020</xref>, for recent evidence suggesting intact integration over a protracted timescale in ASD), but in establishing an internal model suggesting when signals ought to be integrated vs. segregated – a process of causal inference.</p><p>Here we employ multiple audio-visual behavioral tasks to test the hypothesis that causal inference may be aberrant in ASD. These tasks separate cue integration from causal inference, consider both explicit and implicit causal inference tasks, and explore both the spatial and temporal domains. Importantly, we bridge across these experiments by estimating features of causal inference in ASD and control individuals via computational modeling. Finally, we entertain a set of alternative models beyond that of causal inference that could in principle account for differences in behavior between the ASD and control cohorts and highlight which parameters governing causal inference are formally dissociable in implicit vs. explicit tasks (these latter ones constituting a large share of the studies of perceptual abilities in ASD).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Intact audio-visual optimal cue integration</title><p>First, we probe whether individuals with ASD show a normal or impaired ability to optimally combine sensory cues across audio-visual pairings. To do so, individuals with ASD (n=31; mean ± S.E.M; 15.2±0.4 years; 5 females) and age-matched neurotypical controls (n=34, 16.1±0.4 years; 9 females) viewed a visual disk and/or heard an audio beep for 50 ms. The auditory tone and visual flash were synchronously presented either at the same location (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, left panel) or separated by a small spatial disparity ∆ = ±6° (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right panel). The disparity was small enough to escape perceptual awareness (see explicit reports below for corroboration). The auditory stimulus was always the same, making the auditory signals equally reliable across trials. The reliability of the visual cue was manipulated by varying the size of the visual stimulus (see Methods for detail). On each trial, subjects indicated if the stimulus appeared to the right or left from straight ahead.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Audio-visual optimal cue combination in autism spectrum disorder (ASD).</title><p>(<bold>A</bold>) Participants (neurotypical control or individual with ASD) viewed a visual disk and heard an auditory tone at different locations and with different small disparities (top = no disparity, bottom = small disparity). They had to indicate the location of the audio-visual event. (<bold>B</bold>) Rightward (from straight ahead) responses (y-axis) as a function of stimulus location (x-axis, positive = rightward) for an example, control subject. Color gradient (from darkest to lightest) indicates the reliability of the visual cue. (<bold>C</bold>) As (<bold>B</bold>), but for an example, ASD subject. (<bold>D</bold>) Discrimination thresholds in localizing audio (blue) or visual stimuli with different reliabilities (color gradient) for control (black) and ASD (red) subjects. Every point is an individual subject. A subset of six ASD subjects had very poor goodness of fit to a cumulative Gaussian (green) and were excluded from subsequent analyses. (<bold>E</bold>) Measured (x-axis) vs. predicted (y-axis) audio-visual discrimination threshold, as predicted by optimal cue integration. Black and red lines are the fit to all participants and reliabilities, respectively, for the control and ASD subjects. Two-dimensional error bars are the mean and 95% CI for each participant group and reliability condition. (<bold>F</bold>) Rightward response of an example control subject as a function of mean stimulus location (x-axis, auditory at +3 and visual –3 would result in mean stimulus location = 0) and disparity, the visual stimuli being either to the right (solid curve) or left (dashed) of the auditory stimuli. Color gradient shows the same gradient in reliability of the visual cue as in (<bold>B</bold>). (<bold>G</bold>) As (<bold>F</bold>), but for an example, ASD subject. (<bold>H</bold>) Measured (x-axis) vs. predicted (y-axis) visual weights, according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> (Methods). Convention follows that established in (<bold>E</bold>). Both control (black) and ASD (red) subjects dynamically adjust the weight attributed to each sensory modality according to their relative reliability.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig1-v2.tif"/></fig><p><xref ref-type="fig" rid="fig1">Figure 1B and C</xref>, respectively, shows the location discrimination of unisensory stimuli (audio in blue and visual according to a color gradient) for an example, control and ASD subject. Overall, subjects with ASD (6.83±0.68°) localized the visual stimulus as well as neurotypical subjects (6.30±0.49°, <xref ref-type="fig" rid="fig1">Figure 1D</xref>, no group effect, F[1, 57]=0.88, p=0.35, η2=0.01). As visual reliability decreased (lighter colors), the psychometric curves became flatter indicating larger spatial discrimination thresholds (high reliability: 1.10±0.07°, medium: 4.76±0.36°, low: 13.96±0.82°). This effect of visual reliability was equal across both subject groups (group × reliability interaction, F[2, 114]=0.11, p=0.89, η2&lt;0.01), with visual thresholds being equal in control and ASD across all reliability levels. Auditory discrimination seemed to highlight potentially two subgroups within the ASD cohort (blue vs. green). Auditory threshold estimation was not possible for 6 of the 31 subjects within the ASD group (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, green, R<sup>2</sup> value &lt;0.50), due to a lack of modulation in their reports as a function of cue location (excluding these 6 subjects, average R<sup>2</sup> neurotypical control = 0.95; average R<sup>2</sup> ASD = 0.96). Given that the central interest here is in interrogating audio-visual cue combination, and its agreement or disagreement with optimal models of cue combination, the rest of the analyses focuses on the 25 ASD subjects (and the control cohort) who were able to localize auditory tones. Auditory thresholds were similar across neurotypical controls and the ASD cohort where threshold estimation was possible (t<sub>57</sub>=–1.14, p=0.21, Cohen’s d=0.11).</p><p>The central hallmark of multisensory cue combination is the improvement in the precision of estimates (e.g. reduced discrimination thresholds) resulting from the integration of redundant signals. Optimal integration (<xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>) specifies exactly what ought to be the thresholds derived from integrating two cues, and thus we can compare measured and predicted audio-visual thresholds, according to optimal integration (see <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1; 2</xref> in <italic>Methods</italic>). <xref ref-type="fig" rid="fig1">Figure 1E</xref> demonstrates that indeed both control (gradients of black) and ASD (gradients of red) subjects combined cues in line with predictions from statistical optimality (control, slope = 0.93, 95% CI = [0.85–1.04]; ASD, slope = 0.94, 95% CI = [0.88–1.08]). These results generalize previous findings from <xref ref-type="bibr" rid="bib65">Zaidel et al., 2015</xref> and suggest that across sensory pairings (e.g. audio-visual here, visuo-vestibular in <xref ref-type="bibr" rid="bib65">Zaidel et al., 2015</xref>) statistically optimal integration of multisensory cues is intact in ASD.</p><p>A second characteristic of statistically optimal integration is the ability to dynamically alter the weight attributed to each sensory modality according to their relative reliability, i.e., decreasing the weight assigned to less reliable cues. <xref ref-type="fig" rid="fig1">Figure 1F and G</xref>, respectively, shows example psychometric functions for an example control and ASD individual when auditory and visual stimuli were separated by a small spatial disparity (Δ=±6°). Both show the same pattern. When the auditory stimulus was to the right of the visual stimulus (∆=6°, dashed curves), psychometric curves at high reliability (dark black and red symbols for control and ASD) were shifted to the right indicating a leftward bias, in the direction of the visual cue (see <italic>Methods</italic>). At low visual reliability, psychometric curves shifted to the left indicating a rightward bias, toward the auditory cue. That is, in line with predictions from optimal cue combination, psychometric curves shifted to indicate auditory or visual ‘dominance’, respectively, when auditory and visual cues were the most reliable. Analogous shifts of the psychometric functions were observed when the auditory stimulus was to the left of the visual stimulus (∆=−6°, solid curves). At the intermediary visual reliability – matching the reliability of auditory cues (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) – both stimuli influenced localization performance about equally. Such a shift from visual to auditory dominance as the visual cue reliability worsened was prevalent across ASD and control subjects. Importantly, measured and predicted visual weights according to optimal cue combination were well matched in control (<xref ref-type="fig" rid="fig1">Figure 1H</xref>, black, slope = 0.97, 95% CI = [0.92–1.02]) and ASD (<xref ref-type="fig" rid="fig1">Figure 1H</xref>, red, slope = 0.99, 95% CI = [0.93–1.05]) groups. Measured visual weights were also not different between groups at any reliability (F[2, 114]=1.11, p=0.33, η2=0.02). Thus, just as their neurotypical counterparts, ASD subjects dynamically reweighted auditory and visual cues on a trial-by-trial basis depending on their relative reliabilities. Together, this pattern of results suggests that individuals with ASD did not show impairments in integrating perceptually congruent (and near-congruent) auditory and visual stimuli.</p></sec><sec id="s2-2"><title>Impaired audio-visual causal inference</title><p>Having established that the process of integration is itself intact in ASD, we next queried implicit causal inference – the more general problem of establishing when cues ought to be integrated vs. segregated. Individuals with ASD (n=21, 17.32±0.57 years; 5 females) and age-matched neurotypical controls (n=15, 16.86±0.55 years; 7 females, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for overlap in cohorts across experiments) discriminated the location of an auditory tone (50 ms), while a visual disk was presented synchronously at varying spatial disparities. The stimuli were identical to those above but spanned a larger disparity range (∆=±3,±6,±12, and ±24°), including those large enough to be perceived as separate events (see explicit reports below). Subjects indicated if the auditory stimulus was located to the left or right of straight ahead, and as above, we fit psychometric curves to estimate perceptual biases. The addition of large audio-visual disparities fundamentally changes the nature of the experiment, where now observers must first ascertain an internal model, i.e., whether auditory and visual cues come from the same or separate world sources. As the disparity between cues increases, we first expect to see the emergence of perceptual biases – one cue influencing the localization of the other. However, as cue disparities continue to increase, we expect observers to switch worldviews, from a regime where cues are hypothesized to come from the same source, to one where cues are now hypothesized to come from separate sources. Thus, as cue disparities continue to increase, eventually the conflict between cues ought to be large enough that perceptual biases asymptote or decrease, given that the observer is operating under the correct internal model (<xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib47">Rohe and Noppeney, 2015</xref>; <xref ref-type="bibr" rid="bib48">Rohe and Noppeney, 2016</xref>; <xref ref-type="bibr" rid="bib49">Rohe et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Noel and Angelaki, 2022</xref>).</p><p>Overall, individuals with ASD showed a larger bias (i.e. absolute value of the mean of the cumulative Gaussian fit) in auditory localization than the control group (see <xref ref-type="fig" rid="fig2">Figure 2A and B</xref>, respectively, for control and ASD cohorts; F[1, 34]=5.44, p=0.025, η2=0.13). Further, how the bias varied with spatial disparity (∆) significantly differed between the groups (group × disparity interaction: F[7, 168]=3.50, p=0.002, η2=0.12). While the bias saturated at higher ∆ in neurotypical subjects, as expected under causal inference, the bias increased monotonically as ∆ increased in the ASD group. Thus, despite increasing spatial discrepancy, ASD subjects tended to integrate the cues, as if they nearly always utilized visual signals to localize the auditory cue and did not readily switch to a worldview where the auditory and visual cues did not come from the same world source. The effect of visual cue reliability was similar in both groups (group × reliability interaction, F[2, 168]=1.05, p=0.35, η2=0.01), indicating that the auditory bias decreased as visual cue reliability worsened in both groups.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Audio-visual causal inference.</title><p>Participants (black = control; ASD = red) localized auditory tones relative to straight ahead, in the presence of visual cues at different disparities of up to 24°. See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for overlap of subjects with <xref ref-type="fig" rid="fig1">Figure 1</xref>. (<bold>A</bold>) Auditory bias (y-axis, central point of the cumulative Gaussian, e.g. <xref ref-type="fig" rid="fig1">Figure 1B</xref>) as a function of spatial disparity (x-axis, relative location of the visual cue) and reliability of the visual cue (darker = more reliable) in control subjects. (<bold>B</bold>) As (<bold>A</bold>), but for individuals with ASD. (<bold>C</bold>) Coefficient of the linear fits (y-axis, larger value indicates quicker increase in bias with relative visual location) in control (black) and ASD (red), as a function of visual cue reliability (darker = more reliable). (<bold>D</bold>) Linear R<sup>2</sup> (x-axis) demonstrates that the linear fits account well for observed ASD data. On the other hand, adding a cubic term (y-axis, partial R<sup>2</sup>) improved fit to control data (at two reliabilities) but not ASD data. Error bars are ±1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Visual and auditory localization performance of participants in Experiment 2 (audio-visual implicit causal inference).</title><p>(<bold>A</bold>) Proportion of ‘rightward’ report as a function of stimulus location (in deg, x-axis, positive values indicate the right hemifield). Top row: all control subjects. From left to right: visual discrimination at high, medium, and low reliability, and finally auditory discrimination. Bottom row: as top row, for all autism spectrum disorder (ASD) subjects. (<bold>B</bold>) Discrimination thresholds for ASD and controls subjects, for low, medium, and high reliability visual stimuli, as well as auditory cues. Dots and error bars are the mean and S.E.M for each group and condition. (<bold>C</bold>) and (<bold>D</bold>) are, respectively, the bias and R-squared. Format follows that of (<bold>B</bold>). Results indicate that psychometric thresholds (all p&gt;0.09), bias (all p&gt;0.11), and goodness of fit (all p&gt;0.26) are not different across the ASD and control cohorts, across visual and auditory modalities, and across all reliabilities.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Heading discrimination during concurrent implied self-motion and object motion.</title><p>We very briefly present these results, as a conceptual replication and extension of the main audio-visual results, in order to highlight the statistical reliability and generalizability of the main result – an internal model most readily specifying integration in autism spectrum disorder (ASD). (<bold>A</bold>) Subjects are presented with a wide field of dots (background) that move as to imply self-motion, leftward or rightward, at different angles (white arrows). Simultaneously, an object (red clouds of dots) may move independently, at different speeds. Observers are asked to report on their own self-motion by button press (either leftward or rightward from straight-ahead), as well as to indicate if the object moved or not. (<bold>B</bold>) This task requires causal inference in attributing motion across our retinas to self-motion or object-motion, yet the generative model is distinct (c.f. orange arrow) from that reported in the main text (audio-visual). (<bold>C</bold>) Heading bias as a function of object motion speed. As predicted by causal inference, the control group (black), shows a bias peaking at intermediary cue disparities, which then dissipates as the observer infers two separate causes (see <xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>). Instead, the ASD group (dark red) shows a monotonically increasing bias, as if always integrating cues. (<bold>D</bold>) Linear fits account better for ASD than control data (x-axis), while adding a cubic component to the regression aids in explaining control but not ASD data. (<bold>E</bold>) Reports of the object being stationary, as a function of its speed in the world. As expected, reports of stationarity occur more often when the object moves at a slow speed. The peak report of stationarity in ASD is lower than that of control subjects (<bold>F</bold>), and the velocity range over which reports of stationarity are likely, is larger in ASD than control individuals (<bold>G</bold>). Dots are means, and error bars are ±1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig2-figsupp2-v2.tif"/></fig></fig-group><p>To more rigorously quantify how auditory localization depended on ∆, we fit a third-order regression model to the auditory bias as a function of ∆, independently for each subject and at each visual reliability (y=a<sub>0</sub>+a<sub>1</sub>∆+a<sub>2</sub>∆<sup>2</sup>+a<sub>3</sub>∆<sup>3</sup>; see <italic>Methods</italic>). As shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, across all visual reliabilities, the ASD group had a larger linear coefficient (a<sub>1</sub>, ANOVA: F[1, 34]=6.69, p=0.014, η2=0.16), again indicating a monotonic increase in bias with cue spatial disparity.</p><p>To better account for putative non-linear effects at large ∆ - those which ought to most clearly index a change from integration to segregation - we fit different regression models (i.e. null, linear, quadratic, and cubic) and estimated the added variance accounted by adding a cubic term (partial R<sup>2</sup>). This latter term may account for non-linear effects at large ∆, where the impact of visual stimuli on auditory localization may saturate or even decrease (a<sub>3</sub> being zero or negative) at large disparities. Results showed that not only the linear term accounted for more variance in the ASD data than controls (<xref ref-type="fig" rid="fig2">Figure 2D</xref> and x-axis, ANOVA: F[1, 34]=7.08, p=0.012, η2=0.17), but also the addition of a cubic term significantly improved fits in the control, but not ASD, group (<xref ref-type="fig" rid="fig2">Figure 2D</xref> and y-axis, partial R<sup>2</sup>, ANOVA: F[1, 34]=9.87, p=0.003, η2=0.22). Taken together, these results suggest that contrary to predictions from causal inference – where disparate cues should affect one another at small but not large disparities, i.e., only when they may reasonably index the same source – ASD subjects were not able to down-weight the impact of visual cues on auditory localization at large spatial disparities, resulting in larger errors in auditory localization.</p><p>To confirm that the larger biases observed within the ASD cohort were in fact due to these subjects using an incorrect internal model, and not a general impairment in cue localization, we compared unisensory visual and auditory localization thresholds and biases between experimental groups. From the 21 ASD and 15 control subjects who participated in the audio-visual causal inference experiment (Experiment 2), respectively, 15 and 14 of these also participated in Experiment 1 - performing an auditory and visual localization experiment with no disparity (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for further detail). <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> shows the psychometric functions (auditory localization and visual localization at three different reliability levels) for all subjects participating in Experiment 2. Psychometric thresholds (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>, all p&gt;0.09), bias (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>, all p&gt;0.11), and goodness of fit (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>, all p&gt;0.26) were not significantly different between the ASD and control cohorts, across visual and auditory modalities, and across all reliabilities.</p><p>Last, to further bolster the conclusion that individuals with ASD show anomalous implicit causal inference, we replicate the same effect in a very different experimental setup. Namely, subjects (n=17 controls, n=14 ASD, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>) performed a visual heading discrimination task requiring the attribution of optic flow signals to self-motion and/or object-motion (a causal inference task requiring the attribution of motion across the retina to multiple sources, self and/or object; see <xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>, <italic>Methods</italic>, and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref> for further detail). We describe the details in the <italic>Supplementary materials</italic> given that the task is not audio-visual and has a different generative model (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>). Importantly, however, the results demonstrate that while heading biases are present during intermediate self-velocity disparities and object-velocity disparities for controls and ASD subjects (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C, D</xref>), they disappear during large cue discrepancies in control subjects, but not ASD subjects. Just as in the audio-visual case (<xref ref-type="fig" rid="fig2">Figure 2</xref>)<bold>,</bold> ASD subjects do not readily change worldviews and move from integration to segregation as disparities increase (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C, D</xref>).</p><p>Together, these results suggest that in ASD the process of integrating information across modalities is normal (see <xref ref-type="bibr" rid="bib65">Zaidel et al., 2015</xref>) once a correct internal model of the causal structure of the world has been formed. However, the process of inferring this causal structure – the set of relations between hidden sources and sensory signals that may have given rise to the observed data – is anomalous. Namely, individuals with ASD seem to operate under the assumption that sensory cues ought to be integrated most of the time, even for large disparities. Next, we questioned if and how this deficit in causal inference expresses explicitly in overt reports.</p></sec><sec id="s2-3"><title>Decreased disparity-independent explicit report of common cause</title><p>Individuals with ASD (n=23; 16.14±0.51 years; 5 females) and age-matched neurotypical controls (n=24; 17.10±0.42 years; 7 females; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for overlap in cohorts with previous experiments) viewed a visual disk and heard an auditory tone presented synchronously (50 ms), but at different spatial disparities (same stimuli as above, disparity up to 24°). Participants indicated whether the auditory and visual cues originated from a common source, or from two separate sources (see <italic>Methods</italic> for instructions). In contrast to the localization experiments, where subjects localized the physical position of stimuli, here subjects were asked to explicitly report the relationship between the auditory and visual stimuli. See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the unisensory discrimination performance in participants who took part in both the cue integration experiment (Experiment 1) and the current explicit common cause report across spatial disparities. Auditory and visual localization thresholds (all p&gt;0.07), bias (all p&gt;0.15), and the goodness of fit (all p&gt;0.16) of these psychometric estimates were no different between the ASD and control cohort participating in this explicit causal inference judgment experiment.</p><p>As expected, most subjects reported a common source more frequently at smaller rather than larger ∆ (<xref ref-type="fig" rid="fig3">Figure 3</xref> F[8, 259]=94.86, p&lt;0.001, η2=0.74). Interestingly, while this pattern was true for all individual control subjects, eight of the individuals with ASD (i.e. ~⅓ of the cohort) did not modulate their explicit common cause reports as a function of spatial disparity, despite good auditory and visual localization (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). These subjects were not included in subsequent analyses. For lower visual reliability (<xref ref-type="fig" rid="fig3">Figure 3</xref>, from <bold>A-C</bold>), both groups reported common cause less frequently (F[2, 74]=10.68, p&lt;0.001, η2=0.22). A striking difference between experimental groups was the decreased likelihood of reporting common cause, across spatial disparities and visual cue reliabilities, in ASD relative to controls (<xref ref-type="fig" rid="fig3">Figure 3A–C</xref> shades of black vs. shades of red, F[1, 37]=11.6, p=0.002, η2=0.23). This pattern of results using an explicit causal inference task is opposite from that described for the implicit task of auditory localization, where individuals with ASD were more, and not less, likely to combine cues.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Explicit common cause reports across spatial (top) and temporal (bottom) disparities.</title><p>Proportion of common cause reports (y-axis) as a function of spatial disparity (x-axis) and visual cue reliability; high (<bold>A</bold>), medium (<bold>B</bold>), or low (<bold>C</bold>). The most striking characteristic is the reduced likelihood to report common cause, across any disparity or cue reliability. (<bold>D</bold>) Proportion of common cause reports (y-axis) as a function of temporal disparity. As indexed by many (e.g. <xref ref-type="bibr" rid="bib17">Feldman et al., 2018</xref>) individuals with autism spectrum disorder (ASD) show larger ‘temporal binding windows’; temporal extent over which they are likely to report common cause. However, these individuals are also less likely to report common cause, when auditory and visual stimuli are in very close temporal proximity (an effect sometimes reported, e.g., <xref ref-type="bibr" rid="bib37">Noel et al., 2018b</xref>, but many times neglected, given normalization from 0 to 1, to index binding windows; see e.g., <xref ref-type="bibr" rid="bib62">Woynaroski et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Dunham et al., 2020</xref>). See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for overlap of subjects with previous figures. Error bars are ±1 S.E.M.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Visual and auditory localization performance of participants in Experiment 3 (audio-visual explicit causal inference).</title><p><bold>A</bold>) Discrimination thresholds for autism spectrum disorder (ASD) and controls subjects, for low, medium, and high reliability visual stimuli, as well as auditory cues. Transparent dots are individual subjects, while opaque white circles and error bars are mean and S.E.M. (<bold>B</bold>) and (<bold>C</bold>) follow the formatting of (<bold>A</bold>), but respectively, show the bias and R-squared. Four ASD subjects had poor auditory localization performance (as seen in the threshold and R-squared subplots) yet even including these subjects did not differentiate between ASD and control cohorts in threshold, bias, or R-squared.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Reports of common cause as a function of spatial disparity.</title><p>All individual data is presented. Report of ‘common cause’ (y-axis) as a function of spatial disparity (x-axis) and visual reliability (color gradient). Data of all control subjects (first to fourth column) are modulated by disparity. 15/23 Autism spectrum disorder (ASD) subjects (top four rows of columns five to eight) showed a modulation in explicit reports of common cause by disparity. However, 8 (bottom two rows) did not.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Fitting a functional form to the explicit causal inference reports.</title><p>(<bold>A</bold>) We fit a functional model of the form a*N(X;mu,sd^2)+b where X is the spatial disparity. The model was fit for each reliability therefore with 12 parameters in total. The model predictions are shown in shades of magenta with the causal inference model predictions in gray. The causal inference model performs better than the functional form as quantified by Akaike Information Criterion (AIC) as indicated in the title. (<bold>B</bold>) The functional form in (<bold>A</bold>) was fit to control subjects and then the bias (parameter ‘b’ in the equation) was varied to best explain the autism spectrum disorder (ASD) subject responses. This model was outperformed (quantified by AIC) by the causal inference model, which was also fit to the control subjects with choice and inference parameters varied.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig3-figsupp3-v2.tif"/></fig></fig-group><p>These differences were quantified by fitting Gaussian functions to the proportion of common source reports as a function of ∆ (excluding the eight ASD subjects with no modulation in their reports; R<sup>2</sup> for this cohort &lt;0.5). The Gaussian fits (control: R<sup>2</sup>=0.89±0.02; ASD: R<sup>2</sup>=0.93±0.01) yield three parameters that characterize subjects’ behavior: (1) peak amplitude, which represents the maximum proportion of common source reports; (2) mean, which represents the ∆ at which subjects perceived a common source most frequently; and (3) width (SD), which represents the range of ∆ over which the participant was likely to perceive a common source. Both control and ASD participants perceived a common source most frequently at a ∆ close to 0°, and there was no group difference for this parameter (control = 0.30±1.33°; ASD = 0.48±1.9°; F[1, 37]&lt;0.01, p=0.92, η2&lt;0.01). Amplitude and width, however, differed between the two groups. The peak amplitude of the best-fit Gaussian was smaller for the ASD than the control group (control = 0.75±0.02; ASD = 0.62±0.05; F[1, 37]=8.44, p=0.0006, η2=0.18), quantifying the fact that the ASD group perceived a common source less frequently than control participants. The width of the Gaussian fit was smaller in the ASD compared to the control group (control = 30.21±2.10°; ASD = 22.35±3.14°; F[1, 37]=7.00, p=0.012, η2=0.15), suggesting that the range of spatial disparities at which ASD participants perceived a common source was significantly smaller than in controls. Note, this range is well beyond the 6° used in the maximum likelihood estimation experiment (~fourfold), thus corroborating that during the first experiment participants perceived auditory and visual cues as a single, multisensory cue.</p><p>To further substantiate these differences in the explicit report of common cause across ASD and neurotypical subjects, we next dissociated auditory and visual cues across time, as opposed to space. Twenty-one individuals with ASD (15.94±0.56 years; 5 females) and 13 age-matched neurotypical controls (16.3±0.47 years; 5 females, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>) viewed a visual disk and heard an auditory tone, either in synchrony (∆=0 ms) or over a wide range of asynchronies (from ±10 to ±700 ms; positive ∆ indicates visual led auditory stimulus). Subjects indicated if auditory and visual stimuli occurred synchronously or asynchronously.</p><p>Analogous to the case of spatial disparities, we fit reports of common cause (i.e. synchrony, in this case) to Gaussian functions. Just as for spatial disparities, the ASD group had smaller amplitudes (ASD = 0.83±0.04; control = 0.98±0.01; <xref ref-type="fig" rid="fig3">Figure 3D</xref>; t-test: t<sub>32</sub>=7.75, p&lt;0.001, Cohen’s d&gt;2), suggesting that at small ∆ individuals with ASD perceived the stimuli as originating from a common cause less frequently than control subjects did. Further, the ASD group exhibited larger Gaussian widths (control = 171.68±13.17; ASD = 363±55.63 ms; t-test: t<sub>32</sub>=2.61, p=0.01, Cohen’s d=0.9), reflecting more frequent reports of common cause at large temporal disparities. This second effect corroborates a multitude of reports demonstrating larger ‘temporal binding windows’ in ASD than control (see <xref ref-type="bibr" rid="bib17">Feldman et al., 2018</xref> for a meta-analysis of 53 studies). Overall, therefore, explicit reports of common cause across spatial and temporal disparities agree in suggesting a lower likelihood of inferring a common cause at small temporal disparities - including no disparity - in ASD relative to neurotypical controls (see e.g. <xref ref-type="bibr" rid="bib37">Noel et al., 2018b</xref>; <xref ref-type="bibr" rid="bib36">Noel et al., 2018a</xref>, for previous reports showing altered overall tendency to report common cause during temporal disparities in ASD, although these reports typically focus on the size of ‘binding windows’).</p><p>Correlational analyses between psychometric features distinguishing control and ASD individuals (i.e. linear and cubic terms accounting for auditory biases during large audio-visual spatial disparities, amplitude and width of explicit common cause reports during spatial and temporal disparities) and symptomatology measures, i.e., autism quotient (AQ; <xref ref-type="bibr" rid="bib4">Baron-Cohen et al., 2001</xref>) and social communication questionnaire (SCQ; <xref ref-type="bibr" rid="bib52">Rutter et al., 2003</xref>) demonstrated weak to no association. Of the 12 correlations attempted ([AQ + SCQ] × [amplitude + width] × [temporal + spatial] + [AQ + SCQ] × [linear + cubic terms]), the only significant relation (surviving Bonferroni-correction) was that between the width of the Gaussian function describing synchrony judgments as a function of temporal disparity and SCQ scores (Type II regression: <italic>r</italic>=0.52, p=0.002; see <xref ref-type="bibr" rid="bib55">Smith et al., 2017</xref> for a similar observation).</p></sec><sec id="s2-4"><title>Causal inference modeling suggests an increased prior probability for common cause in ASD</title><p>To bridge across experiments (i.e. implicit and explicit audio-visual spatial tasks) and provide a quantitative account of the switch between internal models (i.e. segregate vs. integrate) in ASD vs. controls, we fit subjects’ responses with a Bayesian causal inference model (<xref ref-type="fig" rid="fig4">Figure 4A</xref> and <xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>). The modeling effort is split in three steps.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Causal inference modeling of implicit and explicit spatial tasks.</title><p>(<bold>A</bold>) Generative models of the causal inference process in the two tasks (implicit task in left and explicit task in right). The subject makes noisy sensory measurements (<italic>X</italic>) of the veridical cue locations (<inline-formula><mml:math id="inf1"><mml:mi mathvariant="normal">ϵ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and combines them with their prior belief to obtain their percept (<italic>S</italic>). To do so optimally, the subject first must infer whether signals came from the same cause (<italic>C</italic>) and thereby determine if it is useful to combine the information from the two cues for inferring the trial category (<italic>D</italic>). The causal inference process is shared between the two tasks but the subject infers D<sub>imp</sub> (side of the tone) in the implicit task and D<sub>exp</sub> (number of causes for the sensory observations) in the explicit task. (<bold>B</bold>) Aggregate data (dots) and model fits (lines) in the implicit task (the visual reliability varies from high to low from left to right). The causal inference model is fit to the control aggregate subject and different set of parameters are varied to match the autism spectrum disorder (ASD) subject data (see main text). See <xref ref-type="fig" rid="fig4s12">Figure 4—figure supplement 12</xref> for a fit to the same data while (1) allowing all parameters free to vary, (2) allowing the same parameter as here to vary, but fitting to visual reliabilities separately, or (3) doing both (1) and (2). Of course, these result in better fits, but this is at the expense of interpretability in that they are inconsistent with the empirical data. (<bold>C</bold>) Same as (<bold>B</bold>) but fits are to the explicit spatial task. See <xref ref-type="fig" rid="fig4s13">Figure 4—figure supplement 13</xref> for the equivalent of <xref ref-type="fig" rid="fig4s12">Figure 4—figure supplement 12</xref>, for the implicit task. Data (dots) are slightly different from that in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> because in the previous figures data was first averaged within subjects, then psychometric functions were fit, and finally estimates of bias were averaged across subjects. Here, data is first aggregated across all subjects and then psychometric fits are done on the aggregate. Importantly, the difference between ASD and control subjects holds either way. Error bars are 68% CI (see <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> for additional detail regarding deriving CIs for the amalgamated subject). (<bold>D</bold>). ASD subjects have a higher p-common for the aggregate subject in the implicit task but seemingly compensate in the explicit task where they show a lower aggregate p-common and choice bias. (<bold>E</bold>). The causal inference model provides an equally good fit (quantified by explainable variance explained), a measure of goodness of fit appropriate for noisy, as opposed to noiseless data (<xref ref-type="bibr" rid="bib20">Haefner and Cumming, 2008</xref>) for control and ASD subjects. (<bold>F</bold>) Individual ASD (red) subjects have a higher p-common on average for the implicit task (in agreement with the aggregate subject) but (<bold>G</bold>) show no significant difference in the combined p-common and choice bias for the explicit task due to considerable heterogeneity across subjects. Subjects were included in the single-subject modeling effort if they had participated in Experiment 1 (and thus we had an estimate of their sensory encoding) in addition to the particular task of interest. That is, for panel (<bold>F</bold>), we included all participants taking part in Experiments 1 and 2. This included participants deemed poor in Experiment 1, given our attempt to account for participant’s behavior with the causal inference model. For panel (<bold>G</bold>), we included all participants taking part in Experiments 1 and 3. Individual subject error bars are 68% CI, while group-level error bars are 95% CI (see <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> for additional detail regarding statistical testing). CDF = cumulative density function.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Flowchart detailing steps in fitting procedure.</title><p>Top: Fit to aggregate data presented in <xref ref-type="fig" rid="fig4">Figure 4B,C and D</xref>. First, we aggregate the control data (1) and estimate posteriors for all parameters in order to best fit the data via slice sampling (2). Next, we attempt to account for the aggregate autism spectrum disorder (ASD) data while fixing the sensory parameters but varying choice and/or inference parameters (3). The empirical data suggests no difference in the sensory parameters between control and ASD (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), and including these parameters or allowing them to be free while the inference parameters are not, does not account well for the implicit data (<xref ref-type="fig" rid="fig4s7">Figure 4—figure supplement 7</xref>, Alternative <bold>A</bold> and<bold> B</bold>). Next, we estimate the posterior parameters for the ASD cohort (4, 5). Bottom: Fit to the single subject data presented in <xref ref-type="fig" rid="fig4">Figure 4F and G</xref>. For each participant data from Experiment 1 (allowing estimation of sensory parameters) and either Experiment 2 (implicit causal inference) or Experiment 3 (explicit causal inference in space) are aggregated (1). Then we fit all parameters in order to best account for the data (2) and estimate posteriors over these parameters by slice sampling (3).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Fit to aggregate data for the implicit causal inference task, allowing sensory uncertainty and choice parameters to vary but fixing the inference parameter p<sub>common</sub> (shown in pink).</title><p>This model performs worse (quantified by Akaike Information Criterion [AIC] where positive AIC indicates worse fit) as compared to the model in the main paper where p<sub>common</sub> and the choice parameters are allowed to vary (shown in red).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Data from a single, representative control subject.</title><p>(<bold>A</bold>) Auditory localization, proportion rightward (y-axis) as a function of auditory position vis-à-vis straight ahead (x-axis, positive values are rightward). (<bold>B</bold>) Visual localization across three reliabilities levels, from high reliability in dark, to low reliability in light gray. Follows the format from A. (<bold>C</bold>) Forced fusion task, where auditory and visual cues are presented congruently or with a slight disparity (±6°). (<bold>D</bold>) Explicit causal inference task, where participants report whether auditory and visual cues originated from the same source (y-axis), according to different spatial disparities (x-axis) and visual reliability levels (darker = more reliable). (<bold>F</bold>) Implicit causal inference. Auditory bias (y-axis, in degree) as a function of audio-visual disparity (x-axis, up to 24° ) and visual reliability (darker = more reliable). Errors bars are ±1 S.E. The parameters of the causal inference model are simultaneously fit to all tasks, considerable constraining the model estimates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Data from another single, representative control subject.</title><p>As <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>, for a second representative control subject. (<bold>A</bold>) Auditory localization, proportion rightward (y-axis) as a function of auditory position vis-à-vis straight ahead (x-axis, positive values are rightward). (<bold>B</bold>) Visual localization across three reliability levels, from high reliability in dark, to low reliability in light gray. Follows the format from A. (<bold>C</bold>) Forced fusion task, where auditory and visual cues are presented congruently or with a slight disparity (±6°). (<bold>D</bold>) Explicit causal inference task, where participants report whether auditory and visual cues originated from the same source (y-axis), according to different spatial disparities (x-axis) and visual reliability levels (darker = more reliable). (<bold>F</bold>) Implicit causal inference. Auditory bias (y-axis, in degree) as a function of audio-visual disparity (x-axis, up to 24°) and visual reliability (darker = more reliable). Errors bars are ±1 S.E. The parameters of the causal inference model are simultaneously fit to all tasks, considerable constraining the model estimates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp4-v2.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Data from a single, representative autism spectrum disorder (ASD) subject.</title><p>As <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplements 3</xref> and <xref ref-type="fig" rid="fig4s4">4</xref>, for a representative ASD subject. (<bold>A</bold>) Auditory localization, proportion rightward (y-axis) as a function of auditory position vis-à-vis straight ahead (x-axis, positive values are rightward). (<bold>B</bold>) Visual localization across three reliability levels, from high reliability in dark, to low reliability in light gray. Follows the format from A. (<bold>C</bold>) Forced fusion task, where auditory and visual cues are presented congruently or with a slight disparity (±6°). (<bold>D</bold>) Explicit causal inference task, where participants report whether auditory and visual cues originated from the same source (y-axis), according to different spatial disparities (x-axis) and visual reliability levels (darker = more reliable). (<bold>F</bold>) Implicit causal inference. Auditory bias (y-axis, in degree) as a function of audio-visual disparity (x-axis, up to 24°) and visual reliability (darker = more reliable). Errors bars are ±1 S.E. The parameters of the causal inference model are simultaneously fit to all tasks, considerable constraining the model estimates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp5-v2.tif"/></fig><fig id="fig4s6" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 6.</label><caption><title>Data from another single, representative autism spectrum disorder subject.</title><p>As <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>, for a second representative control subject. (<bold>A</bold>) Auditory localization, proportion rightward (y-axis) as a function of auditory position vis-à-vis straight ahead (x-axis, positive values are rightward). (<bold>B</bold>) Visual localization across three reliabilities levels, from high reliability in dark, to low reliability in light gray. Follows the format from A. (<bold>C</bold>) Forced fusion task, where auditory and visual cues are presented congruently or with a slight disparity (±6°). (<bold>D</bold>) Explicit causal inference task, where participants report whether auditory and visual cues originated from the same source (y-axis), according to different spatial disparities (x-axis) and visual reliability levels (darker = more reliable). (<bold>F</bold>) Implicit causal inference. Auditory bias (y-axis, in degree) as a function of audio-visual disparity (x-axis, up to 24°) and visual reliability (darker = more reliable). Errors bars are ±1 S.E. The parameters of the causal inference model are simultaneously fit to all tasks, considerable constraining the model estimates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp6-v2.tif"/></fig><fig id="fig4s7" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 7.</label><caption><title>Goodness of fit of alternative models for the implicit and explicit spatial causal inference task.</title><p>Four alternatives were tested: (<bold>A</bold>) a forced fusion model where C is set to 1. (<bold>B</bold>) A forced segregation model where C is set to 2. (<bold>C</bold>) A model where there is no lapse bias. (D1) An alternative where solely choice parameters are varied for implicit task: the lapse parameters and p<sub>choice</sub> (D2). For the explicit task, p<sub>choice</sub> cannot be separately constrained from p<sub>common</sub> so only the lapse parameters are varied. For the implicit task, two main observations must be noted. First, all models perform worse than that presented in the main text (y-axis is difference to the model presented in the main text, with higher Akaike Information Criterion [AIC] indicating a worse quality of fit). Second, Alternative A is substantially better than Alternative B, but still worse than the model in the main text. Thus, seemingly individuals with autism spectrum disorder do perform causal inference and not pure forced fusion, yet they overweight integration (vs. segregation) compared to their neurotypical counterparts. For the explicit task, all alternative models perform worse than the model in the main text where the choice and inference parameters were varied.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp7-v2.tif"/></fig><fig id="fig4s8" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 8.</label><caption><title>Illustration of the alternative models fits to implicit causal inference model data.</title><p>The rows show the four alternative models tested, while the columns show data in decreasing order of reliability. For each panel, the model presented in the main text is shown, in shades of black for the control subjects and shades of red for the autism spectrum disorder subjects. The alternative tested in each panel is shown in shades of magenta.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp8-v2.tif"/></fig><fig id="fig4s9" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 9.</label><caption><title>Illustration of the alternative models fits to explicit causal inference model data.</title><p>The rows show the four alternative models tested, while the columns show data in decreasing order of reliability. For each panel, the model presented in the main text is shown, in shades of black for the control subjects and shades of red for the autism spectrum disorder subjects. The alternative tested in each panel is shown in shades of magenta.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp9-v2.tif"/></fig><fig id="fig4s10" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 10.</label><caption><title>Causal inference modeling of temporal, simultaneity judgment task.</title><p>(<bold>A</bold>) Causal inference model for auditory-visual simultaneity judgement. This model is a simplified version from the full model in <xref ref-type="fig" rid="fig4">Figure 4A</xref> by measuring temporal judgments relative to a reference cue (the auditory cue in this case). (<bold>B</bold>) Model fits to aggregate subjects in the explicit temporal task. The causal inference model is fit to the control aggregate subject and different set of parameters are varied to match the autism spectrum disorder (ASD) subject data (similar to <xref ref-type="fig" rid="fig4">Figure 4</xref>). (<bold>C</bold>) Estimated p<sub>combined</sub> cannot be accurately constrained using the given data, as illustrated by the large error bars. (<bold>D</bold>) Scatter plots of samples from the joint posterior distribution between the p<sub>combined</sub> parameter and the lapse bias parameter. The two parameters are (negatively) correlated (<italic>r</italic>=−0.72) making them hard to separate. (<bold>E</bold>) Illustration of the correlation between the parameters in B. Fixing the p<sub>combined</sub> to a range of values between 0.3 and 0.7, we can estimate the lapses such that the model provides a qualitatively good prediction to the data, hence making it hard to constrain the p<sub>combined</sub> as seen in C. (<bold>F</bold>) The predicted responses from the causal inference model can be visualized as a difference between two cumulative Gaussian functions with a vertical scaling and offset determined by the lapse rate and lapse biases. The slope of the cumulative Gaussian is a function of the sensory uncertainty, whereas the separation is a function of p<sub>combined</sub>. By fixing the slope, offset, and scaling from the sensory uncertainties and lapses, the differences between the curves for different p<sub>combined</sub> are small, making it hard to separate using reasonable amount of empirical data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp10-v2.tif"/></fig><fig id="fig4s11" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 11.</label><caption><title>Lapse rate and lapse bias for aggregate and individual subjects during the implicit and explicit spatial tasks.</title><p>(<bold>A</bold>) Aggregate lapse rate and lapse bias for controls and autism spectrum disorder (ASD) (plotted similar to main text <xref ref-type="fig" rid="fig4">Figure 4D</xref>). We find that in both implicit and explicit causal inference tasks, ASD subjects have a higher lapse rate. ASD subjects also seem to have a larger deviation from a uniform lapse bias of 0.5. (<bold>B–D</bold>) Lapse rate and lapse bias fit to individual subjects. There is no significant difference in the lapse rate and lapse bias for both tasks due to considerable heterogeneity across subjects. Subject depictured in blue is the same as in <xref ref-type="fig" rid="fig4">Figure 4</xref>, main text.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp11-v2.tif"/></fig><fig id="fig4s12" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 12.</label><caption><title>Fit to aggregate data for the implicit causal inference task, given that all parameters are free to vary (<bold>A</bold>), the different visual reliabilities are fit separately (<bold>B</bold>) or both of the above (<bold>C</bold>).</title><p>The change in Akaike Information Criterions (AICs) is reported in the panel title, with all values being smaller than the baseline model, suggesting better fits. Shades of black and red are, respectively, the baseline model for control and autism spectrum disorder (ASD) subjects, while shades of magenta are the comparison model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp12-v2.tif"/></fig><fig id="fig4s13" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 13.</label><caption><title>Fit to aggregate data for the explicit causal inference task, given that all parameters are free to vary (<bold>A</bold>), the different visual reliabilities are fit separately (<bold>B</bold>) or both of the above (<bold>C</bold>).</title><p>The change in Akaike Information Criterions (AICs) is reported in the panel title, with all values being smaller than the baseline model, suggesting better fits. Shades of black and red are, respectively, the baseline model for control and autism spectrum disorder (ASD) subjects, while shades of magenta are the comparison model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-fig4-figsupp13-v2.tif"/></fig></fig-group><p>First, we fit aggregate data and attempt to discern which of the parameters that govern the causal inference process may globally differ between the ASD and control cohorts. The parameters of the causal inference model can be divided into three sets. First, sensory parameters: the visual and auditory sensory uncertainty (i.e. inverse of reliability), as well as visual and auditory priors (i.e. expectations) over the perceived auditory and visual locations (mean and variance of Gaussian priors). Second, choice parameters: choice bias (p<sub>choice</sub>), as well as lapse rate and bias. These latter two parameters are the frequency with which an observer may make a choice independent of the sensory evidence (lapse rate) and whether these stimuli-independent judgments are biased (lapse bias). Third, inference parameters: the prior probability of combination (p<sub>common</sub>; see <italic>Methods and</italic> <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>, <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> for further detail). In this first modeling step, we fit all parameters (see <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>) to best account for the aggregate control subject. Then, we test whether a difference in choice and inference parameters, but not the sensory ones, can explain the observed difference between the control and the aggregate ASD data. We do not vary the sensory parameters given that unisensory discrimination thresholds did not differ between experimental groups (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. See <italic>Methods</italic>, <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for technical detail regarding the model fitting procedure. Also see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> corroborating the fact that varying the inference parameter, as opposed to sensory uncertainty, results in better model fits). In a second step, we attempt not to globally differentiate between ASD and control cohorts, but to account for individual subject behavior. Thus, we fit single subject data and utilize the subject-specific measured sensory uncertainty to fit all parameters (i.e. sensory, choice, and inference). All subjects who completed the cue integration experiment (Experiment 1) – allowing for deriving auditory and visual localization thresholds – and either the implicit (Experiment 2) or explicit (Experiment 3) spatial causal inference task were included in this effort. This included ‘poor performers’ (six in Experiment 1 and eight in Experiment 3), given that the goal of this second modeling step was to account for individual subject behavior. Last, we perform model comparison between the causal inference model and a set of alternative accounts, also putatively differentiating the two experimental groups.</p><p><xref ref-type="fig" rid="fig4">Figure 4B and C</xref><bold>,</bold> respectively, shows the aggregate control and ASD data for the implicit and explicit causal inference task (with each panel showing different visual reliabilities). In the implicit task (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top panel), allowing only for a difference in the choice parameters (lapse rate, bias, and p<sub>choice</sub>; magenta) between the control and ASD cohorts, could only partially account for observed differences between these groups (explainable variance explained, EVE=0.91, see <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). Instead, differences between the control and ASD data could be better explained if the prior probability of combining cues, p<sub>common</sub>, was also significantly higher for ASD relative to control observers (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, p=4.5 × 10<sup>–7</sup>, EVE=0.97, ∆AIC between model varying only choice parameters vs. choice and inference parameters = 1 × 10<sub>3</sub>). This suggests the necessity to include p<sub>common</sub> as a factor globally differentiating between the neurotypical and ASD cohort.</p><p>For the explicit task, different lapse rates and biases between ASD and controls could also not explain their differing reports (as for the implicit task; EVE = 0.17). Differently from the implicit task, however, we cannot dissociate the prior probability of combination (i.e. p<sub>common</sub>) and choice biases, given that the report is on common cause (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, see <italic>Methods</italic> and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> for additional detail). Thus, we call the joint choice and inference parameter p<sub>combined</sub> (this one being a joint p<sub>common</sub> and p<sub>choice</sub>). Allowing for a lower p<sub>combined</sub> in ASD could better explain the observed differences between ASD and control explicit reports (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; EVE = 0.69, ∆AIC relative to a model solely varying lapse rate and bias = 1.3 × 10<sup>3</sup>). This is illustrated for the ASD aggregate subject relative to the aggregate control subject in <xref ref-type="fig" rid="fig4">Figure 4D</xref> (p=1.8 × 10<sup>–4</sup>). Under the assumption that an observer’s expectation for cues to come from the same cause (p<sub>common</sub>) is formed over a long timescale, and hence is the same across the implicit and explicit tasks, we can ascribe the differing pattern of results in the tasks (i.e. increased p<sub>common</sub> in ASD in the implicit task, yet a decreased p<sub>combined</sub> in the explicit task) to differences in the choice bias (i.e. the added component from p<sub>common</sub> to p<sub>combined</sub>). This bias may in fact reflect a compensatory strategy by ASD observers since we found their p<sub>common</sub> (uncorrupted by explicit choice biases) to be roughly three times as large as that of the aggregate control observer (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>Next, we fit the model to individual subject data (as opposed to the aggregate) and obtained full posterior estimates over all model parameters for individual observers. We fit the model jointly to unisensory and causal inference tasks, such that we can constrain the sensory parameters by the observed unisensory data (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The causal inference model provided a good and comparable fit for both ASD and control subjects (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) with the model explaining more than 80% of explainable variance in all but one subject (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, blue dot). <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplements 3</xref>–<xref ref-type="fig" rid="fig4s6">6</xref> show individual data for two representative control (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplements 3</xref> and <xref ref-type="fig" rid="fig4s4">4</xref>) and two ASD subjects (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplements 5</xref> and <xref ref-type="fig" rid="fig4s6">6</xref>), while highlighting all the data that constrained the model fits (audio localization, visual localization at three reliabilities, forced fusion task at three reliabilities, as well as implicit and explicit causal inference). Overall, both groups were heterogeneous (<xref ref-type="fig" rid="fig4">Figure 4F and G</xref>). Nonetheless, in agreement with the aggregate data, individuals with ASD had a higher prior probability of common cause than control subjects (<xref ref-type="fig" rid="fig4">Figure 4F</xref>) during the implicit task (p=0.02), where p<sub>common</sub> can be estimated independently from p<sub>choice</sub>. When estimating p<sub>combined</sub> (i.e. the combination of p<sub>common</sub> and p<sub>choice</sub>) for the explicit task (<xref ref-type="fig" rid="fig4">Figure 4G</xref>), the parameter estimates extracted from the individual fits suggested no difference between ASD and control subjects (p=0.26), although numerically the results are in line with the aggregate data, suggesting a lower p<sub>combined</sub> in ASD than control (see inter-subject variability in <xref ref-type="fig" rid="fig4">Figure 4F and G</xref>). Importantly, the aggregate and single subject fits concord in suggesting an explicit compensatory mechanism in individuals with ASD, given that p<sub>common</sub> is higher in ASD than control (when this parameter can be estimated in isolation) and a measure corrupted by explicit choice biases (i.e. p<sub>combined</sub>) is not. Individual subjects’ p<sub>common</sub> and p<sub>combined</sub> as estimated by the model did not correlate with ASD symptomatology, as measured by the AQ and SCQ (all p&gt;0.17). Exploration of the model parameters in the ‘poor performers’ did not suggest a systematic difference between these subjects and other vis-à-vis their causal inference parameters.</p><p>Last, we consider a set of alternative models that could in principle account for differences in behavior across the aggregate control and ASD cohorts. The first alternative (alternative A) was a forced fusion model where all parameters were fit to the ASD aggregate subject, but p<sub>common</sub> was fixed to a value of 1. Thus, under this account the ASD subject always combines the cues irrespective of the disparity between them. Alternative B was a no fusion model, the opposite to Alternative A, where now all parameters were fit to the ASD aggregate subject, but p<sub>common</sub> was fixed to a value of 2. Alternative C had a lapse rate but no lapse bias. Last, alternative D allowed only the choice parameters to vary between control and ASD, but no inference or sensory parameter. For the implicit task, lapse rate, bias, and p<sub>choice</sub> were allowed to vary. For the explicit task since p<sub>choice</sub> trades off with p<sub>common</sub>, only lapse rate and bias were allowed to vary.</p><p>We performed model comparison using AIC and <xref ref-type="fig" rid="fig4s7">Figure 4—figure supplement 7</xref> shows this metric for the ASD aggregate subject relative to the causal inference model where we vary choice and inference parameters (i.e. the model used in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Lower AIC indicates a better fit). <xref ref-type="fig" rid="fig4s8">Figure 4—figure supplement 8</xref> and <xref ref-type="fig" rid="fig4s9">Figure 4—figure supplement 9</xref> show the original (choice and inference) and alternative fits, respectively, to implicit and explicit spatial causal inference tasks. For the implicit task, varying sensory and choice parameters, as opposed to inference parameters, results in a worse quality fit. Interestingly, alternative A (forced fusion) is a considerably better model than alternative B (forced segregation). Together, this pattern of results suggests that choice and inference (and not choice and sensory) parameters distinguish between ASD and control subjects in the implicit causal inference task. Likewise, these results further corroborate the conclusion that ASD subjects favor an internal model where integration outweighs segregation (AIC alternative A&lt;AIC alternative B), yet there is not a complete lack of causal inference in ASD, given that alternative A is inferior to the model where p<sub>common</sub> is less than 1. In other words, individuals with ASD do perform causal inference, but they give more weight to integration (vs. segregation) compared to neurotypical subjects. For the explicit task, the alternative models considered performed worse than allowing the choice and inference parameters to vary (main model used in <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>For completeness, we fit the causal inference model to data from the simultaneity judgment task (see <xref ref-type="fig" rid="fig4s10">Figure 4—figure supplement 10</xref> and <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>), given that this task constitutes a large portion of the literature on multisensory impairments in ASD (see e.g. <xref ref-type="bibr" rid="bib17">Feldman et al., 2018</xref>). However, in this task, given its explicit nature, it is also not possible to dissociate p<sub>choice</sub> and p<sub>common</sub> (as for the explicit spatial task), and even more vexingly, given that reliabilities were not manipulated (as is typical in the study of multisensory temporal acuity, see <xref ref-type="bibr" rid="bib35">Nidiffer et al., 2016</xref>, for an exception), it is also difficult to dissociate the p<sub>choice</sub> from lapse parameters with a reasonable amount of data. We also explore the impact of lapse rates and biases and their differences across ASD and control subjects in <xref ref-type="fig" rid="fig4s11">Figure 4—figure supplement 11</xref>.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We presented individuals with ASD and neurotypical controls with audio-visual stimuli at different spatial or temporal disparities, and measured their unisensory spatial discrimination thresholds, their implicit ability to perform optimal cue combination, and their implicit and explicit tendency to deduce different causal structures across cue disparities. The results indicate no overall impairment in the ability to perform optimal multisensory cue integration (<xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>). These observations generalize a previous report (<xref ref-type="bibr" rid="bib65">Zaidel et al., 2015</xref>) and suggest that across domains (visuo-vestibular in <xref ref-type="bibr" rid="bib65">Zaidel et al., 2015</xref> audio-visual here), optimal cue combination is intact in ASD. Instead, we found that even at large spatial disparities, individuals with ASD use information from one sensory modality in localizing another. That is, in contrast to neurotypical controls, individuals with ASD behaved as if they were more likely to infer that cues come from the same rather the different sources. This suggests that the well-established anomalies in multisensory behavior in ASD - e.g., biases (see <xref ref-type="bibr" rid="bib5">Baum et al., 2015</xref> and <xref ref-type="bibr" rid="bib59">Wallace et al., 2020</xref>, for reviews) – may not be due to a dysfunctional process of multisensory integration per se, but one of impair causal inference.</p><p>The juxtaposition between an impaired ability for causal inference yet the presence of an intact ability for optimal cue combination may suggest a deficit in a specific kind of computation and point toward anomalies in particular kinds of neural motifs. Indeed, an additional algorithmic component in causal inference (<xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>) relative to optimal cue combination models (<xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>) is the presence of non-linear operations such as marginalization. This operation corresponds to ‘summing out’ nuisance variables, allows for non-linearities, and may be neurally implemented via divisive normalization (see <xref ref-type="bibr" rid="bib6">Beck et al., 2011</xref> for detail on marginalization and the relationship between this operation and divisive normalization). In fact, while not all proposed neural network models of causal inference rely on divisive normalization (see <xref ref-type="bibr" rid="bib11">Cuppini et al., 2017</xref>; <xref ref-type="bibr" rid="bib66">Zhang et al., 2019</xref> for networks performing causal inference without explicit marginalization), many do (e.g. <xref ref-type="bibr" rid="bib63">Yamashita et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Yu et al., 2016</xref>). Divisive normalization is a canonical neural motif (<xref ref-type="bibr" rid="bib8">Carandini and Heeger, 2011</xref>), i.e., thought to operate throughout the brain, wherein neural activity from a given unit is normalized by the joint output of a normalization neural pool. Thus, the broad anomalies observed in ASD may be underpinned by an alteration in a canonical computation, i.e., causal inference, which in turn is dependent on a canonical neural motif, i.e., divisive normalization. <xref ref-type="bibr" rid="bib50">Rosenberg et al., 2015</xref>, suggested that anomalies in divisive normalization – specifically a reduction in the amount of inhibition that occurs through divisive normalization – —can account for a host of perceptual anomalies in ASD, such as altered local vs. global processing (<xref ref-type="bibr" rid="bib21">Happé and Frith, 2006</xref>), altered visuo-spatial suppression (<xref ref-type="bibr" rid="bib18">Foss-Feig et al., 2013</xref>), and increased tunnel vision (<xref ref-type="bibr" rid="bib45">Robertson et al., 2013</xref>). This suggestion – from altered divisive normalization, to altered marginalization, and in turn altered causal inference and multisensory behavior – is well aligned with known physiology in ASD and ASD animal models showing decrease GABAergic signaling (<xref ref-type="bibr" rid="bib30">Lee et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2020</xref>), the comorbidity between ASD and seizure activity (<xref ref-type="bibr" rid="bib24">Jeste and Tuchman, 2015</xref>), and the hypothesis that ASD is rooted in an increased excitation-to-inhibition ratio (i.e. E/I imbalance; <xref ref-type="bibr" rid="bib51">Rubenstein and Merzenich, 2003</xref>).</p><p>A second major empirical finding is that individuals with ASD seem to explicitly report common cause less frequently than neurotypical controls. Here we demonstrate a reduced tendency to explicitly report common cause during small cue disparities, across both spatial and temporal disparities (also see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2E-G</xref> for corroborative evidence during a motion processing task). This has previously been observed within the temporal domain (<xref ref-type="bibr" rid="bib37">Noel et al., 2018b</xref>; <xref ref-type="bibr" rid="bib36">Noel et al., 2018a</xref>), yet frequently multisensory simultaneity judgments are normalized to peak at ‘1’ (e.g. <xref ref-type="bibr" rid="bib62">Woynaroski et al., 2013</xref>; <xref ref-type="bibr" rid="bib15">Dunham et al., 2020</xref>), obfuscating this effect. To the best of our knowledge, the reduced tendency to explicitly report common cause across spatial disparities in ASD has not been previously reported. Further, it is interesting to note that while ‘temporal binding windows’ were larger in ASD than control (see <xref ref-type="bibr" rid="bib17">Feldman et al., 2018</xref>), ‘spatial binding windows’ were smaller in ASD relative to control subjects. This pattern of results highlights that when studying explicit ‘binding windows’, it may not be sufficient to index temporal or spatial domains independently, but there could potentially be a trade-off. More importantly, the reduced tendency to overtly report common cause across spatial and temporal domains in ASD (even when implicitly they seem to integrate more, and not less often) is indicative of a choice bias that may have emerged as a compensatory mechanism to their increased implicit tendency to bind information across sensory modalities. This speculation is supported by formal model fitting, where the prior probability of combination (p-common) was larger at the (aggregate) population level in the ASD than the control subjects in implicit tasks (where p-common may be independently estimated), yet a combined measure of p-common and a choice bias (these not being dissociable in explicit tasks such as spatial or temporal common cause reports) that was reduced (in the aggregate) or not significantly different (in the individual subject data) between ASD and control individuals. The presence of this putative compensatory mechanism is important to note, particularly when a significant fraction of the characterization of (multi)sensory processing in ASD relies on explicit tasks. Further, this finding, highlights the importance in characterizing both implicit and explicit perceptual mechanisms – particularly when framed under a strong theoretical foundation (<xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>) and using model-based analyses (e.g. <xref ref-type="bibr" rid="bib29">Lawson et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Lieder et al., 2019</xref>) – given that explicit reports may not faithfully reflect subjects’ percepts.</p><p>Last, it is also interesting to speculate on how an increased prior probability of integrating cues, and the presence of a compensatory mechanism, may relate to ASD symptomatology. Here we did not observe any reliable correlation between symptomatology and either psychophysical measures or model parameter estimates. However, it must be acknowledged that while the overall number of participants across all experiments was relatively large (91 subjects in total), our sample sizes within each experiment were moderate (~20 subjects per group and experiment), perhaps explaining the lack of any correlation. Regardless, it is well established that beyond (multi)sensory anomalies (<xref ref-type="bibr" rid="bib5">Baum et al., 2015</xref>), individuals with ASD show inflexible and repetitive behaviors (<xref ref-type="bibr" rid="bib19">Geurts et al., 2009</xref>) and demonstrate ‘stereotypy’, self-stimulatory behaviors thought to relieve sensory-driven anxiety (<xref ref-type="bibr" rid="bib10">Cunningham and Schreibman, 2008</xref>). The finding that individuals with ASD do not change their worldview (i.e. from integration to segregation, even at large sensory disparities) may result in sensory anomalies and reflect the slow updating of expectations (<xref ref-type="bibr" rid="bib58">Vishne et al., 2021</xref>). Thus, anomalies in causal inference may have the potential of explaining seemingly disparate phenotypes in ASD – anomalous perception and repetitive behaviors. Similarly, we may conjecture that stereotypy is a physical manifestation of a compensatory mechanism, such as the one uncovered here. Stereotypy could result from attempting to align incoming sensory evidence with the (inflexible) expectations of what that sensory input ought to be.</p><p>In conclusion, by leveraging a computational framework (optimal cue combination and causal inference; <xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>) and systematically measuring perception at each step (i.e. unisensory, forced cue integration, and causal inference) across a range of audio-visual multisensory behaviors, we can ascribe anomalies in multisensory behavior to the process of inferring the causal structure linking sensory observations to their hidden causes. Of course, this anomaly results in perceptual biases (see the current results and <xref ref-type="bibr" rid="bib5">Baum et al., 2015</xref> for an extensive review), but the point is that these biases are driven by a canonical computation that has gone awry. Further, given the known E/I imbalance in ASD (<xref ref-type="bibr" rid="bib51">Rubenstein and Merzenich, 2003</xref>; <xref ref-type="bibr" rid="bib30">Lee et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2020</xref>) and the fact that causal inference may require marginalization but optimal cue combination does not (<xref ref-type="bibr" rid="bib6">Beck et al., 2011</xref>), we can speculatively suggest a bridge from neural instantiation to behavioral computation; E/I imbalance may disrupt divisive normalization (neural implementation), which leads to improper marginalization (algorithm) and thus altered causal inference (computation) and multisensory perception (biases in behavior) in ASD.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>A total of 91 adolescents (16.25±0.4 years; 20 females) took part (completely or partially) in a series of up to five behavioral experiments (four audio-visual and presented in the main text, in addition to a visual heading discrimination task presented in the <italic>Supplementary Materials</italic>). Forty-eight of these were neurotypical controls. Individuals in the control group (16.5±0.4 years; 13 females) had no diagnosis of ASD or any other developmental disorder or related medical diagnosis. These subjects were recruited by flyers posted throughout Houston. The other 43 participants (16.0±0.5 years; 7 females) were diagnosed as within ASD. The participants with ASD were recruited through several sources, including (1) the Simons Simplex Collection families, (2) flyers posted at Texas Children’s Hospital, (3) the Houston Autism Center, and (4) the clinical databases maintained by the Simons Foundation Autism Research Initiative (SFARI). All participants were screened at enrollment with SCQ (<xref ref-type="bibr" rid="bib52">Rutter et al., 2003</xref>) and/or the AQ (<xref ref-type="bibr" rid="bib4">Baron-Cohen et al., 2001</xref>) to afford (1) a measure of current ASD symptomatology and (2) rule out concerns for ASD in control subjects. There was no individual with ASD below the recommended SCQ cutoff, and only 2 (out of 47) control subjects above this cutoff (<xref ref-type="bibr" rid="bib56">Surén et al., 2019</xref>). Similarly, there was almost no overlap in ASD and control AQ scores (with only 3 out of 47 control individuals having a higher AQ score than the lowest of the individuals with ASD). All individuals with ASD were above the AQ cutoffs recommended by <xref ref-type="bibr" rid="bib61">Woodbury-Smith et al., 2005</xref> and <xref ref-type="bibr" rid="bib31">Lepage et al., 2009</xref> (respectively, cutoff scores of 22 and 26), but not by <xref ref-type="bibr" rid="bib4">Baron-Cohen et al., 2001</xref> (cutoff score of 36). Inclusion in the ASD group required that subjects have (1) a confirmed diagnosis of ASD according to the DSM-5 (<xref ref-type="bibr" rid="bib3">American Psychiatric Association, 2013</xref>) by part of a research-reliable clinical practitioner and (2) no history of seizure or other neurological disorders. A subset of the individuals with ASD were assessed by the Autism Diagnostic Observation Schedule (ADOS-2, <xref ref-type="bibr" rid="bib33">Lord et al., 2012</xref>), and no difference was observed in the AQ, SCQ, or psychometric estimates between individuals with ASD with and without the ADOS assessment (all p&gt;0.21). Similarly, the intelligence quotient (IQ) as estimated by the Wechsler Adult Intelligence Scale (WAIS) was available for a subset of the ASD participants (n=10, or 22% of the cohort), whose average score was 103±9 (S.E.M.), this being no different from the general population (which by definition has a mean of 100). All subjects had normal visual and auditory acuity, as characterized by parents’ and/or participants’ reports. For each of the five psychophysics experiments, we aimed at scheduling approximately 25–30 participants per group, in accord with sample sizes from previous similar reports (<xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Noel et al., 2018b</xref>). Data were not examined until after data collection was complete. The study was approved by the Institutional Review Board at the Baylor College of Medicine (protocol number H-29411) and written consent/assent was obtained.</p></sec><sec id="s4-2"><title>Experimental materials and procedures</title><sec id="s4-2-1"><title>Experiment 1: Audio-visual spatial localization; maximum-likelihood estimation (implicit)</title><p>Thirty-one ASD (age = 15.2±0.4 years) and 34 control (16.1±0.4 years) subjects participated in this experiment. As expected, the SCQ (ASD = 17.1±0.75; control = 4.8±0.5; t-test: t<sub>63</sub>=–13.31, p&lt;0.0001) and AQ scores (ASD = 31.2±1.7; control = 15.3±1.5; t<sub>41</sub>=–6.61, p&lt;0.0001) of the ASD group were significantly greater than that of the control group.</p><p>Subjects performed a spatial localization task of auditory, visual, or combined audio-visual stimuli. A custom-built setup comprising of (1) an array of speakers and (2) a video projection system delivered the auditory and visual stimuli, respectively. Seven speakers (TB-F Series; W2-852SH) spaced 3° apart were mounted on a wooden frame along a horizontal line. A video projector (Dell 2,400 MP) displayed images onto a black projection screen (60 × 35°) that was mounted over the speaker array. This arrangement allowed presentation of the visual stimulus precisely at the location of the auditory stimulus, or at different locations on the screen. The auditory stimulus was a simple tone at 1200 Hz. The visual stimulus was a white circular patch. Reliability of the visual stimulus was manipulated by varying the size of the visual patch such that reliability inversely varied with the patch size (<xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>). Three levels of visual reliability were tested: high (higher reliability of visual vs. auditory localization), medium (similar reliabilities of visual and auditory localization), and low (poorer reliability of visual vs. auditory localization). For high and low visual reliabilities, the patch diameter was fixed for all participants at 5 and 30°, respectively. For medium reliability, the patch diameter ranged from 15 to 25° across subjects. In all conditions (audio-only, visual-only, or combined audio-visual), the auditory and/or visual stimuli were presented for 50 ms (and synchronously in the case of combined stimuli). Stimuli were generated by custom MATLAB scripts employing the PsychToolBox (<xref ref-type="bibr" rid="bib26">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Noel et al., 2022</xref>).</p><p>Subjects were seated 1 m from the speaker-array with their chins supported on a chinrest and fixated a central cross. Subjects performed a single-interval, two-alternative-forced-choice spatial localization task. In each trial, they were presented with either an auditory, visual, or combined audio-visual stimulus (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). They indicated if the auditory and/or visual stimulus were located to the left or right of straight forward by button-press. The spatial locations of the stimuli were varied in steps around straight forward. In single-cue auditory and combined conditions, the auditory stimulus was presented at one of the seven locations: 0,±3,±6, and ±9° (positive sign indicates that the stimulus was presented to the right of the participant). By contrast, the visual stimulus could be presented at any location on the screen. Specifically, in the single-cue visual condition, the visual stimulus was presented at ±20, ±10, ±5, ±2.5, ±1.25, ±0.65, ±0.32, and 0°. In the combined condition, auditory and visual stimuli were either presented at the same spatial location (<xref ref-type="fig" rid="fig1">Figure 1</xref>, top panel; <italic>Δ</italic>=0°) or at different locations separated by a spatial disparity Δ=±6° (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, bottom panel; positive Δ indicates that the auditory stimulus was located to the right of the visual stimulus). For trials in which there was a spatial conflict, a mean stimulus location was defined. The auditory and visual stimuli were presented on either side of this mean stimulus location at an angular distance of Δ/2. For <italic>Δ</italic>=6°, the mean stimulus was located at –12, –9, –6, –3, 0, 3, and 6°. For Δ=–6°, the mean stimulus was located at –6, –3, 0, 3, 6, 9, and 12°. Each subject performed a total of 1680 trials (auditory condition = 7 stimulus locations × 15 repetitions; visual condition = 14 stimulus locations × 15 repetitions × 3 visual cue reliabilities; and combined auditory-visual condition = 7 stimulus locations × 3 reliabilities × 3 conflict angles × 15 repetitions). All conditions were interleaved.</p><p>For each subject, visual cue reliability, stimulus condition, and spatial disparity, psychometric functions were constructed by plotting the proportion of rightward responses as a function of stimulus location. These data were fit with a cumulative Gaussian function using <italic>psignifit</italic>, a MATLAB package that implements the maximum-likelihood method (<xref ref-type="bibr" rid="bib60">Wichmann and Hill, 2001</xref>). The psychometric function yields two parameters that characterize participants’ localization performance: bias and threshold. Bias (μ) is the stimulus value at which responses are equally split between rightward and leftward. A bias close to 0° indicates highly accurate localization. The threshold is given by the SD (σ) of the fitted cumulative Gaussian function. The smaller the threshold, the greater the precision of spatial localization. The bias and threshold values estimated from these psychometric functions were used to test the predictions of optimal cue integration. The psychometric fitting could not estimate auditory thresholds for six ASD subjects, whose report did not vary as a function of auditory stimuli location. These subjects were not included in the remaining analyses reported in the main text.</p><p>Based on unisensory localization, we may derive predictions for the combined case, given optimal cue combination by maximum-likelihood estimation (<xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref>; <xref ref-type="bibr" rid="bib22">Hillis et al., 2002</xref>; <xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>; <xref ref-type="bibr" rid="bib25">Kersten et al., 2004</xref>). First, assuming optimal cue combination, the threshold in the combined auditory-visual condition (σ<sub>com</sub>) should be equal to:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msqrt><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:msqrt></mml:math></disp-formula></p><p>with <italic>σ</italic><sub><italic>a</italic></sub> and <italic>σ</italic><sub><italic>v</italic></sub> being the thresholds in the unisensory auditory and visual localization, respectively. Second, the weight assigned to the visual cue in combined audio-visual stimuli (see <xref ref-type="bibr" rid="bib16">Ernst and Banks, 2002</xref> and <xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>, for detail) should vary with its reliability. Specifically, as visual cue reliability decreases, the visual weight will also decrease. The visual weight, <italic>w</italic><sub><italic>v</italic></sub>, is predicted to be:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>and in turn the auditory cue weight (<italic>w</italic><sub><italic>a</italic></sub>) is computed as 1 <italic>− w</italic><sub><italic>v</italic></sub>.</p></sec><sec id="s4-2-2"><title>Experiment 2: Audio spatial localization with disparate visual cues; causal inference (implicit)</title><p>Twenty-two ASD (age = 17.32±0.57 years) and 15 control (age = 16.86±0.55 years) subjects participated in this experiment. As expected, the SCQ (ASD = 16.42±1.12; control = 5.06±0.65; t-test: t<sub>35</sub>=7.84, p&lt;0.0001) and AQ scores (ASD = 31.95±1.76; control = 13.76±1.61; t<sub>35</sub>=7.21, p&lt;0.0001) of the ASD group were significantly greater than that of the control group.</p><p>The task and stimuli employed here were identical to the audio-visual localization experiment described above, except that a larger range of spatial disparities were employed. The disparity between cues (∆) could take one of nine values: 0, ±3, ±6, ±12, and ±24°. Each ∆ was presented 8 times at each of the 7 speaker locations, and at each visual cue reliability, resulting in a total of 1512 trials (9 spatial disparities × 7 speaker locations × 3 reliabilities × 8 repetitions). Subjects indicated if the auditory stimulus was located to the right or left of straight ahead. Subjects were informed that the flash and beep could appear at different physical locations. All conditions were interleaved, and subjects were required to take breaks and rest after each block.</p><p>For each subject, audio-visual disparity (∆), and visual cue reliability, psychometric functions were constructed by plotting the proportion of rightward responses as a function of the true auditory stimulus location. As for the audio-visual localization task described above, data were fitted with a cumulative Gaussian function. An auditory bias close to 0° indicates that the subject was able to discount the distracting influence of the visual cues and accurately localize the audio beep. Data from one ASD subject was excluded from this analysis as the subject was unable to perform the task even when auditory and visual stimuli were co-localized (∆=0°). In eight ASD subjects, psychometric functions could not fit into the data even at the highest disparity (∆ = ±24°) during high reliability, as subjects’ estimates were ‘captured’ by the visual cues. The remaining data from these subjects were included in the analyses.</p><p>As an initial quantification of localization estimates, and putative differences in audio-visual biases between the groups, a third-order regression model of the form: y = a<sub>0</sub> + a<sub>1</sub>∆ + a<sub>2</sub>∆<sup>2</sup> + a<sub>3</sub>∆ <xref ref-type="bibr" rid="bib3">American Psychiatric Association, 2013</xref> was fitted to the auditory bias as a function of ∆ and visual cue reliability. Coefficient a<sub>1</sub> represents how sensitive the bias is to changes in ∆ - larger a<sub>1</sub> indicates a greater change in the bias for a given change in ∆. Coefficient a<sub>2</sub> indicates if the dependence of bias on ∆ is uniform for positive and negative ∆ values. Importantly, coefficient a<sub>3</sub> generally represents how the bias changes at large ∆ values – negative a<sub>3</sub> indicates a saturation or a decrease in the bias at large ∆. If subjects perform causal inference (<xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>), we expect a saturation or even a return to no bias at large ∆. Furthermore, partial R<sup>2</sup> values associated with a<sub>1</sub>, a<sub>2</sub>, and a<sub>3</sub> describe the contribution of each term in explaining the total variance. ASD and control subjects’ data was well-explained by the third-order regression model (ASD: R<sup>2</sup>=0.93±0.04; control: R<sup>2</sup>=0.88±0.03). A mixed-effects ANOVA with group, ∆, and visual cue reliability as factors compared the bias, threshold, and parameters of the regression model for the ASD and control groups.</p></sec><sec id="s4-2-3"><title>Experiment 3: Audio-visual common source reports under spatial disparities (Explicit)</title><p>Twenty-three23 ASD (age = 16.14±0.51 years) and 24 control (age = 17.10±0.42 years) subjects participated in this experiment. Six other ASD subjects were screened for this experiment, but showed poor auditory localization (c.f. Experiment 1). The SCQ (ASD = 16.91±0.83; control = 5.04±0.47; t-test: t<sub>57</sub>=11.46, p&lt;0.0001) and AQ scores (ASD = 30.77±1.60; control = 15.18±1.60; t<sub>41</sub>=6.42, p&lt;0.0001) of the ASD group were significantly greater than that of the control group.</p><p>The auditory and visual stimuli presented in this task were identical to those employed in Experiment 2. Each ∆ was presented 7 times, at each of seven speaker locations, and at each visual cue reliability, resulting in a total of 1323 trials (9 spatial disparities × 7 speaker locations × 3 reliabilities × 7 repetitions). Subjects indicated via button-press if the auditory and visual cues originated from a common source or from different sources. The exact instructions were to “press the ‘same source’ key if auditory and visual signals come from the same source, and press the ‘different sources’ key if auditory and visual signals come from different sources.” All conditions were interleaved, and subjects were required to take breaks and rest after each block. Before the start of the main experiment, subjects participated in a practice block to familiarize themselves with the stimuli and response buttons. The response buttons (one for ‘same source’ and the other for ‘different sources’ were the left and right buttons of a standard computer mouse. Reports from eight ASD subjects did not vary with ∆, and thus their data was excluded from the main analyses).</p><p>For each subject, audio-visual disparity (∆), and visual cue reliability, the proportion of common source reports was calculated. A mixed-effects ANOVA with group as the between-subjects factor, along with ∆ and visual cue reliability as within-subjects factors compared the proportion of common source reports in 26 control and 25 ASD subjects.</p><p>Further, to quantify putative differences in how ASD and control subjects inferred the causal relationship between auditory and visual stimuli, Gaussian functions were fit to the proportion of common source reports as a function of ∆ (e.g. <xref ref-type="bibr" rid="bib47">Rohe and Noppeney, 2015</xref>). These fits yielded three parameters of interest: (1) amplitude (tendency to report common cause when maximal), (2) mean (spatial disparity at which auditory and visual cues are most likely considered to originate from a common cause), and (3) width (spatial disparity range over which subjects are likely to report common cause).</p></sec><sec id="s4-2-4"><title>Experiment 4: Audio-visual common source reports under temporal disparities (Explicit)</title><p>Twenty-one ASD (age = 15.94±0.56 years) and 19 control (age = 16.3±0.47 years) subjects participated in this task. As expected, ASD subjects had significantly higher SCQ (ASD: SCQ = 18.31±1; control: SCQ = 4.92±0.73; t-test: t<sub>32</sub>=–9.41, p&lt;0.0001) and AQ (ASD: AQ = 32.76±1.58; control: AQ = 14.58±1.15; t-test: t<sub>32</sub>=7.43, p&lt;0.0001) scores than the control subjects. Subjects viewed a flash and heard an audio beep (same stimuli as in Experiments 1, 2, and 3) presented centrally either at the same time or at different asynchronies. Twenty-three different temporal disparities (∆) were presented: 0, ±10, ±20, ±50, ±80, ±100, ±150, ±200, ±250, ±300, ±500, and ±700 ms (positive ∆s indicate that flash led the auditory stimulus). Subjects indicated if the flash and beep were synchronous (exact instruction: ‘appeared at the same time’) or asynchronous (‘appeared at different times’) via button press on a standard computer mouse. Each ∆ was presented 25 times in random order.</p><p>Proportion of synchronous reports at each ∆ was calculated. A Gaussian function was fit to the proportion of synchronous reports as a function of ∆ (ASD: R<sup>2</sup>=0.86±0.05; control: R<sup>2</sup>=0.94±0.01). The Gaussian fits yielded three parameters that characterized subjects’ performance: (1) amplitude (representing the maximum proportion of synchronous reports), (2) mean (representing the ∆ at which subjects maximally perceived the flash and beep to be synchronous), and (3) width (representing the range of ∆ within which subjects were likely to perceive the auditory and visual stimuli to co-occur in time).</p><p>A mixed-effects ANOVA with group as the between-subjects factor, and temporal disparity (∆) as a within-subjects factor compared the proportion of synchronous reports. Similarly, independent-samples t-tests compared the parameters of the Gaussian fits between the groups.</p></sec><sec id="s4-2-5"><title>Experiment 5: Visual heading discrimination during concurrent object motion</title><p>Fourteen ASD and 17 control subjects (ASD: 15.71±0.5 years; control: 16.3±0.6 years) participated in this task. The ASD group had significantly higher SCQ (ASD: 16.71±1.36; control: SCQ = 7.35±1.12; p&lt;0.0001) and AQ scores (ASD: AQ = 33.78±2.20; control = 11.79±2.35, p&lt;0.0001) than the control group. Details of the apparatus and experimental stimuli have been previously described (<xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>).</p><p>In brief, subjects viewed lateral movement of a multipart spherical object while presented with a 3D cloud of dots mimicking forward translation (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). The multipart object moved rightward or leftward within a fronto-parallel plane at five peak speeds: 0.07, 0.13, 0.8, 2.67, and 5.33 m/s. Implied self-motion consisted of a single interval, 1 s in duration, during which the motion stimulus followed a smooth Gaussian velocity profile (displacement = 13 cm; peak velocity = 0.26 m/s). Heading was varied in discrete steps around straight forward (0°), using the following set of values: 0, ±5, ±10, ±15, ±20, ±25, and ±45° (positive value indicates rightward heading). In one session, subjects indicated if they perceived the object to be stationary or moving in the world. In another session, subjects indicated if their perceived heading was to the right or left of straight ahead. In each session there were a total of 130 distinct stimulus conditions (2 object motion directions × 5 object motion speeds × 13 headings) and each condition was presented 7 times. All stimulus conditions were interleaved in each block of trials.</p><p>Heading discrimination performance was quantified by fitting psychometric curves for each object motion direction and speed (<xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>). These fits yielded parameters that characterize the accuracy and precision of heading perception: bias and threshold. For statistical analyses, the bias measured with leftward object motion was multiplied by –1, such that expected biases were all positive (<xref ref-type="bibr" rid="bib14">Dokka et al., 2019</xref>). To quantify the differences in the heading bias between groups, a third-order regression model of the form: y = b<sub>0</sub> + b<sub>1</sub>X + b<sub>2</sub>X2 + b<sub>3</sub>X3, where X is the sign consistent logarithm of object motion speed was fitted to the heading bias. We compared the linear (b<sub>1</sub>), quadratic (b<sub>2</sub>), and cubic (b<sub>3</sub>) coefficients along with their corresponding partial R<sup>2</sup> values between groups, similar to the analyses performed on the auditory bias in the audio-visual localization tasks.</p></sec></sec><sec id="s4-3"><title>Causal Inference Modeling</title><p>We modeled subject responses using a causal inference model (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) where the observer has to infer whether two sensory cues (auditory and visual) come from the same or separate causes(s), and use this information to either integrate or not information from these cues. In each trial, we assume that the subject’s observations of the auditory and visual location (denoted <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) are the experimenter defined veridical values (denoted by <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) corrupted by sensory noise with variances <inline-formula><mml:math id="inf6"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> ,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf8"><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></inline-formula> denotes the normal probability density function with mean μ and variance <inline-formula><mml:math id="inf9"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> . We assume that subjects have a good estimate of their sensory uncertainties (over lifelong learning) and hence the subject’s estimated likelihoods become,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext mathvariant="fraktur"> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext mathvariant="fraktur"> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote the inferred location of auditory and visual stimuli. The subject’s joint prior over the cue locations is parameterized as a product of three terms which reflect:</p><p>(a) <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> : the subject’s natural prior over the unisensory cue locations. For example, subjects may have a prior that sensory cue locations are more likely to occur closer to midline as compared to peripheral locations. We model this component of the prior as normal distributions where the mean and variance are unknown parameters fitted to the data.<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mi mathvariant="script">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>(b) <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> : the influence that the inferred cause (C) has on the knowledge of cue locations. In our causal inference model <inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is inferred as being equal to <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> if C=1 and independent if C=2.<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(c) <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> : the relationship between the inferred trial category (D) and the cue locations.</p><sec id="s4-3-1"><title>Implicit task</title><p>In the implicit discrimination task, where the trial category corresponds to the side of the auditory cue location relative to the midline, <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is positive if D<sub>imp</sub> = 1 and negative if D<sub>imp</sub> =-1.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mtext>=</mml:mtext><mml:mtext>1</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mtext>=</mml:mtext><mml:mtext>–</mml:mtext><mml:mtext>1</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where H(x) is the Heaviside function (H(x)=1 if x&gt;0 and 0 otherwise).</p><p>The product of <xref ref-type="disp-formula" rid="equ7 equ8 equ9">Equations 7–9</xref>, defines the probability over cue locations conditioned on C and D<sub>imp</sub> in the implicit task as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>∝</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>which can be succinctly written as<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>∝</mml:mo><mml:mi mathvariant="script"> </mml:mi><mml:mi mathvariant="script">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mi mathvariant="script">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo><mml:mi>δ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>We parameterize the observer’s priors over D<sub>imp</sub> and C as Bernoulli distributions with means <inline-formula><mml:math id="inf18"><mml:mtext>p</mml:mtext><mml:mtext>choice</mml:mtext><mml:mtext> </mml:mtext></mml:math></inline-formula> and p<sub>common</sub>.<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>;</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mi mathvariant="normal"> </mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>The posterior probability of the subject inferring the auditory cue to come from the right can be obtained by marginalizing over the observer’s belief whether the auditory and visual cue come from a single or from separate causes<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>We assume the subject makes their response by choosing the response that has the highest posterior probability. If <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is the subject response (1 for right and –1 for left), then<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mo>-</mml:mo><mml:mn>1,1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>implicit</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>imp</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-3-2"><title>Explicit task</title><p>We model the explicit task by assuming that the decision maker computes the belief over the trial category D<sub>exp</sub> using the inferred belief over C, but not exactly equating both (graphical model in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). This extends earlier approaches (<xref ref-type="bibr" rid="bib27">Körding et al., 2007</xref>) which equate trial category D<sub>exp</sub> with C, and additionally allows us to model task specific beliefs about the trial category. As we will show later, such a difference in beliefs between D<sub>exp</sub> and C is mathematically equivalent to the subject making their decision by comparing their belief over C to a criterion different from 0.5.</p><p>The subject’s knowledge about the relationship between the trial category and the inferred variable C is parameterized as <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> , as given by <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> and <xref ref-type="disp-formula" rid="equ17">Equation 17</xref><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>[</mml:mo><mml:mi>C</mml:mi><mml:mo>;</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow></mml:mfenced><mml:mo>]</mml:mo></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>[</mml:mo><mml:mi>C</mml:mi><mml:mo>;</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow></mml:mfenced><mml:mo>]</mml:mo></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> there is no relationship between trial category D and C (e.g. before learning the task), and thus the prior over C reduces to <inline-formula><mml:math id="inf22"><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:math></inline-formula>. On the other extreme, <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> corresponds to complete task-learning, where C and D<sub>exp</sub> are identical.</p><p>The prior probability of the subject’s belief over D<sub>exp</sub> in the explicit task is parameterized as a Bernoulli distribution with mean <inline-formula><mml:math id="inf24"><mml:mtext>p</mml:mtext><mml:mtext>choice</mml:mtext></mml:math></inline-formula> as given in <xref ref-type="disp-formula" rid="equ18">Equation 18</xref><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>We modeled subject’s belief about the sensory cue locations as the product of two terms: <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref> and <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>)<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with appropriate normalization constants obtained by integrating over all <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mtext>and </mml:mtext><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , we get<disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mrow><mml:mtext mathvariant="fraktur"> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mtext>=</mml:mtext><mml:mtext>1</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Our model makes choice <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> = 1 if<disp-formula id="equ22"><label>(21)</label><mml:math id="m22"><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which by Bayes rule reduces to,<disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>1-</mml:mtext><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where the likelihood over observations is evaluated by marginalizing across inferred sensory locations using the sensory likelihoods (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref> and <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>), i.e.,<disp-formula id="equ24"><label>(23)</label><mml:math id="m24"><mml:mi mathvariant="normal"> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>We can marginalize out C in <xref ref-type="disp-formula" rid="equ23">Equation 22</xref> to get<disp-formula id="equ25"><label>(24)</label><mml:math id="m25"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>By combining terms, <xref ref-type="disp-formula" rid="equ25">Equation 24</xref> can be simplified as<disp-formula id="equ26"><label>(25)</label><mml:math id="m26"><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>combined</mml:mtext></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>(1-</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>combined</mml:mtext></mml:mrow></mml:msub><mml:mtext>)</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>combined</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is a function of <inline-formula><mml:math id="inf30"><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:math></inline-formula> , <inline-formula><mml:math id="inf31"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> as given in <xref ref-type="disp-formula" rid="equ27">Equation 26</xref> which cannot be individually constrained.<disp-formula id="equ27"><label>(26)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We now show that a decision rule as given in <xref ref-type="disp-formula" rid="equ27">Equation 26</xref> is equivalent to a subject making their decision by comparing their inferred posterior <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> to a criterion t, i.e., <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> =1 if<disp-formula id="equ28"><label>(27)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Or equivalently<disp-formula id="equ29"><label>(28)</label><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula></p><p>which can be expanded using Bayes rule as given in <xref ref-type="disp-formula" rid="equ30">Equation 29</xref><disp-formula id="equ30"><label>(29)</label><mml:math id="m30"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Comparing <xref ref-type="disp-formula" rid="equ30">Equation 29</xref> to <xref ref-type="disp-formula" rid="equ26">Equation 25</xref>, we can relate terms to get<disp-formula id="equ31"><label>(30)</label><mml:math id="m31"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>combined</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mtext>p</mml:mtext><mml:mtext>common </mml:mtext><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where the criterion t is a function of <inline-formula><mml:math id="inf35"><mml:mtext>p</mml:mtext><mml:mtext>common</mml:mtext></mml:math></inline-formula>, <inline-formula><mml:math id="inf36"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>choice</mml:mtext></mml:mrow><mml:mrow><mml:mtext>explicit</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mtext>task</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> .</p><p>We provide further model derivation and fitting details in <italic>Supplementary Materials</italic>, <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>, <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>. We can also similarly derive the causal inference model for the simultaneity judgement by modeling the temporal percepts as Bayesian inference and replacing the spatial disparities with temporal disparities. Further details are provided in the <italic>Supplementary Materials,</italic> (<xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>).</p><p>Last, as a contrast to the causal inference model (and variants thereof, alternatives A–D presented in the main text), for explicit tasks we also fit a functional form, specified by a Gaussian (mean and SD as free parameters) plus an additive bias (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). We fit this model to the spatial common cause reports (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) of control subject. Then, we vary the additive bias, <italic>b</italic> (see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>), in attempting to account for the ASD data relative to the control. Both the fit to the control data, and to the ASD data relative to the control, were better accounted for by the causal inference model (which additionally is a principled one), than the functional form.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Visualization, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Investigation, Methodology, Project administration, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Project administration, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was approved by the Institutional Review Board at the Baylor College of Medicine (protocol number H-29411) and written consent/assent was obtained.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Control participants.</title><p>The table indexes each control participant by a unit ID, and indicates in which experiment did each participant take part in. Green indicates that the participant took part in the given experiment, while red indicates that they did not. Experiment 1 is the unisensory discrimination task and audio-visual cue combination that does not require causal inference (i.e. imperceptible disparities). Experiment 2 is the audio-visual implicit causal inference experiment. Experiment 3 is the explicit causal inference experiment with spatial disparities. Experiment 4 is the explicit causal inference experiment with temporal disparities. Experiment 5 is the visual heading discrimination task during concurrent object-motion.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-71866-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>ASD participants.</title><p>The table indexes each ASD participant by a unit ID, and indicates in which experiment did each participant take part in. Green indicates that the participant took part in the given experiment, while red indicates that they did not. An orange box indicates that the participant took part in the experiment, but their data was excluded in presentation of the empirical results (but not the modeling of individual subjects, as indicated in the main text). Experiment 1 is the unisensory discrimination task and audio-visual cue combination that does not require causal inference (i.e. imperceptible disparities). Experiment 2 is the audio-visual implicit causal inference experiment. Experiment 3 is the explicit causal inference experiment with spatial disparities. Experiment 4 is the explicit causal inference experiment with temporal disparities. Experiment 5 is the visual heading discrimination task during concurrent object-motion.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-71866-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Priors distributions over model parameters.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-71866-supp3-v2.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Causal inference model inference and fitting details.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-71866-supp4-v2.docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Causal inference modeling for simultaneity judgments.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-71866-supp5-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-71866-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and code are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/6xbzt">https://osf.io/6xbzt</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>S</given-names></name><name><surname>Dokka</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>ASD Causal Inference</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/6xbzt">6xbzt</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Jing Lin and Jian Chen for programming the experimental stimulus. This work was supported by NIH U19NS118246 (to RH and DEA), and by the Simons Foundation, SFARI Grant 396,921 and Grant 542949-SCGB (to DEA).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian comparison of explicit and implicit causal inference strategies in multisensory heading perception</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006110</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006110</pub-id><pub-id pub-id-type="pmid">30052625</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title><source>Current Biology</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id><pub-id pub-id-type="pmid">14761661</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><collab>American Psychiatric Association</collab></person-group><year iso-8601-date="2013">2013</year><source>Diagnostic and Statistical Manual of Mental Disorders</source><publisher-loc>Washington, DC)</publisher-loc><publisher-name>American Psychiatric Publishing</publisher-name><pub-id pub-id-type="doi">10.1176/appi.books.9780890425596</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname><given-names>S</given-names></name><name><surname>Wheelwright</surname><given-names>S</given-names></name><name><surname>Skinner</surname><given-names>R</given-names></name><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Clubley</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The autism-spectrum quotient (AQ): evidence from Asperger syndrome/high-functioning autism, males and females, scientists and mathematicians</article-title><source>Journal of Autism and Developmental Disorders</source><volume>31</volume><fpage>5</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1023/a:1005653411471</pub-id><pub-id pub-id-type="pmid">11439754</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baum</surname><given-names>SH</given-names></name><name><surname>Stevenson</surname><given-names>RA</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Behavioral, perceptual, and neural alterations in sensory and multisensory function in autism spectrum disorder</article-title><source>Progress in Neurobiology</source><volume>134</volume><fpage>140</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2015.09.007</pub-id><pub-id pub-id-type="pmid">26455789</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Marginalization in neural circuits with divisive normalization</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>15310</fpage><lpage>15319</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1706-11.2011</pub-id><pub-id pub-id-type="pmid">22031877</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal Inference in the Multisensory Brain</article-title><source>Neuron</source><volume>102</volume><fpage>1076</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.03.043</pub-id><pub-id pub-id-type="pmid">31047778</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Normalization as a canonical neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Deister</surname><given-names>CA</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>B</given-names></name><name><surname>Lynn-Jones</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>N</given-names></name><name><surname>Wells</surname><given-names>MF</given-names></name><name><surname>Liu</surname><given-names>R</given-names></name><name><surname>Goard</surname><given-names>MJ</given-names></name><name><surname>Dimidschstein</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>S</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Liao</surname><given-names>W</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Fishell</surname><given-names>G</given-names></name><name><surname>Moore</surname><given-names>CI</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dysfunction of cortical GABAergic neurons leads to sensory hyper-reactivity in a Shank3 mouse model of ASD</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>520</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0598-6</pub-id><pub-id pub-id-type="pmid">32123378</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>AB</given-names></name><name><surname>Schreibman</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Stereotypy in Autism: The Importance of Function</article-title><source>Research in Autism Spectrum Disorders</source><volume>2</volume><fpage>469</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.rasd.2007.09.006</pub-id><pub-id pub-id-type="pmid">19122856</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuppini</surname><given-names>C</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Magosso</surname><given-names>E</given-names></name><name><surname>Ursino</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A biologically inspired neurocomputational model for audiovisual integration and causal inference</article-title><source>The European Journal of Neuroscience</source><volume>46</volume><fpage>2481</fpage><lpage>2498</lpage><pub-id pub-id-type="doi">10.1111/ejn.13725</pub-id><pub-id pub-id-type="pmid">28949035</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Winkel</surname><given-names>KN</given-names></name><name><surname>Katliar</surname><given-names>M</given-names></name><name><surname>Diers</surname><given-names>D</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Causal Inference in the Perception of Verticality</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>5483</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-23838-w</pub-id><pub-id pub-id-type="pmid">29615728</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Multisensory Integration of Visual and Vestibular Signals Improves Heading Discrimination in the Presence of a Moving Object</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13599</fpage><lpage>13607</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2267-15.2015</pub-id><pub-id pub-id-type="pmid">26446214</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dokka</surname><given-names>K</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Jansen</surname><given-names>M</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference accounts for heading perception in the presence of object motion</article-title><source>PNAS</source><volume>116</volume><fpage>9060</fpage><lpage>9065</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820373116</pub-id><pub-id pub-id-type="pmid">30996126</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunham</surname><given-names>K</given-names></name><name><surname>Feldman</surname><given-names>JI</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Cassidy</surname><given-names>M</given-names></name><name><surname>Conrad</surname><given-names>JG</given-names></name><name><surname>Santapuram</surname><given-names>P</given-names></name><name><surname>Suzman</surname><given-names>E</given-names></name><name><surname>Tu</surname><given-names>A</given-names></name><name><surname>Butera</surname><given-names>I</given-names></name><name><surname>Simon</surname><given-names>DM</given-names></name><name><surname>Broderick</surname><given-names>N</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Lewkowicz</surname><given-names>D</given-names></name><name><surname>Woynaroski</surname><given-names>TG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stability of Variables Derived From Measures of Multisensory Function in Children With Autism Spectrum Disorder</article-title><source>American Journal on Intellectual and Developmental Disabilities</source><volume>125</volume><fpage>287</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1352/1944-7558-125.4.287</pub-id><pub-id pub-id-type="pmid">32609807</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>JI</given-names></name><name><surname>Dunham</surname><given-names>K</given-names></name><name><surname>Cassidy</surname><given-names>M</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Woynaroski</surname><given-names>TG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Audiovisual multisensory integration in individuals with autism spectrum disorder: A systematic review and meta-analysis</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>95</volume><fpage>220</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.09.020</pub-id><pub-id pub-id-type="pmid">30287245</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foss-Feig</surname><given-names>JH</given-names></name><name><surname>Tadin</surname><given-names>D</given-names></name><name><surname>Schauder</surname><given-names>KB</given-names></name><name><surname>Cascio</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A substantial and unexpected enhancement of motion perception in autism</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>8243</fpage><lpage>8249</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1608-12.2013</pub-id><pub-id pub-id-type="pmid">23658163</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geurts</surname><given-names>HM</given-names></name><name><surname>Corbett</surname><given-names>B</given-names></name><name><surname>Solomon</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The paradox of cognitive flexibility in autism</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>74</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.11.006</pub-id><pub-id pub-id-type="pmid">19138551</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Haefner</surname><given-names>RM</given-names></name><name><surname>Cumming</surname><given-names>BG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>An improved estimator of Variance Explained in the presence of noise</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The weak coherence account: detail-focused cognitive style in autism spectrum disorders</article-title><source>Journal of Autism and Developmental Disorders</source><volume>36</volume><fpage>5</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1007/s10803-005-0039-0</pub-id><pub-id pub-id-type="pmid">16450045</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillis</surname><given-names>JM</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Combining sensory information: mandatory fusion within, but not between, senses</article-title><source>Science (New York, N.Y.)</source><volume>298</volume><fpage>1627</fpage><lpage>1630</lpage><pub-id pub-id-type="doi">10.1126/science.1075396</pub-id><pub-id pub-id-type="pmid">12446912</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hospedales</surname><given-names>T</given-names></name><name><surname>Vijayakumar</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Multisensory oddity detection as bayesian inference</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e4205</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0004205</pub-id><pub-id pub-id-type="pmid">19145254</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeste</surname><given-names>SS</given-names></name><name><surname>Tuchman</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Autism Spectrum Disorder and Epilepsy: Two Sides of the Same Coin?</article-title><source>Journal of Child Neurology</source><volume>30</volume><fpage>1963</fpage><lpage>1971</lpage><pub-id pub-id-type="doi">10.1177/0883073815601501</pub-id><pub-id pub-id-type="pmid">26374786</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>Mamassian</surname><given-names>P</given-names></name><name><surname>Yuille</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Object perception as Bayesian inference</article-title><source>Annual Review of Psychology</source><volume>55</volume><fpage>271</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142005</pub-id><pub-id pub-id-type="pmid">14744217</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname><given-names>RP</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An aberrant precision account of autism</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>302</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00302</pub-id><pub-id pub-id-type="pmid">24860482</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname><given-names>RP</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adults with autism overestimate the volatility of the sensory environment</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1293</fpage><lpage>1299</lpage><pub-id pub-id-type="doi">10.1038/nn.4615</pub-id><pub-id pub-id-type="pmid">28758996</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Excitation/Inhibition Imbalance in Animal Models of Autism Spectrum Disorders</article-title><source>Biological Psychiatry</source><volume>81</volume><fpage>838</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2016.05.011</pub-id><pub-id pub-id-type="pmid">27450033</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lepage</surname><given-names>JF</given-names></name><name><surname>Lortie</surname><given-names>M</given-names></name><name><surname>Taschereau-Dumouchel</surname><given-names>V</given-names></name><name><surname>Théoret</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Validation of French-Canadian versions of the Empathy Quotient and Autism Spectrum Quotient</article-title><source>Canadian Journal of Behavioural Science / Revue Canadienne Des Sciences Du Comportement</source><volume>41</volume><fpage>272</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1037/a0016248</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname><given-names>I</given-names></name><name><surname>Adam</surname><given-names>V</given-names></name><name><surname>Frenkel</surname><given-names>O</given-names></name><name><surname>Jaffe-Dax</surname><given-names>S</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Ahissar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Perceptual bias reveals slow-updating in autism and fast-forgetting in dyslexia</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>256</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0308-9</pub-id><pub-id pub-id-type="pmid">30643299</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lord</surname><given-names>C</given-names></name><name><surname>Rutter</surname><given-names>M</given-names></name><name><surname>DiLavore</surname><given-names>PC</given-names></name><name><surname>Risi</surname><given-names>S</given-names></name><name><surname>Gotham</surname><given-names>K</given-names></name><name><surname>Bishop</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Autism Diagnostic Observation Schedule</source><publisher-loc>Torrance, CA</publisher-loc><publisher-name>Western Psychological Services</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Causal inference of asynchronous audiovisual speech</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>798</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00798</pub-id><pub-id pub-id-type="pmid">24294207</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Stevenson</surname><given-names>RA</given-names></name><name><surname>Krueger Fister</surname><given-names>J</given-names></name><name><surname>Barnett</surname><given-names>ZP</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Interactions between space and effectiveness in human multisensory performance</article-title><source>Neuropsychologia</source><volume>88</volume><fpage>83</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.01.031</pub-id><pub-id pub-id-type="pmid">26826522</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Lytle</surname><given-names>M</given-names></name><name><surname>Cascio</surname><given-names>C</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Disrupted integration of exteroceptive and interoceptive signaling in autism spectrum disorder</article-title><source>Autism Research</source><volume>11</volume><fpage>194</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1002/aur.1880</pub-id><pub-id pub-id-type="pmid">29030901</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Stevenson</surname><given-names>RA</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Atypical audiovisual temporal function in autism and schizophrenia: similar phenotype, different cause</article-title><source>The European Journal of Neuroscience</source><volume>47</volume><fpage>1230</fpage><lpage>1241</lpage><pub-id pub-id-type="doi">10.1111/ejn.13911</pub-id><pub-id pub-id-type="pmid">29575155</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Increased variability but intact integration during visual navigation in Autism Spectrum Disorder</article-title><source>PNAS</source><volume>117</volume><fpage>11158</fpage><lpage>11166</lpage><pub-id pub-id-type="doi">10.1073/pnas.2000216117</pub-id><pub-id pub-id-type="pmid">32358192</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Paredes</surname><given-names>R</given-names></name><name><surname>Terrebonne</surname><given-names>E</given-names></name><name><surname>Feldman</surname><given-names>JI</given-names></name><name><surname>Woynaroski</surname><given-names>T</given-names></name><name><surname>Cascio</surname><given-names>CJ</given-names></name><name><surname>Seriès</surname><given-names>P</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Inflexible Updating of the Self-Other Divide During a Social Context in Autism: Psychophysical, Electrophysiological, and Neural Network Modeling Evidence</article-title><source>Biological Psychiatry. Cognitive Neuroscience and Neuroimaging</source><fpage>00090</fpage><lpage>00092</lpage><pub-id pub-id-type="doi">10.1016/j.bpsc.2021.03.013</pub-id><pub-id pub-id-type="pmid">33845169</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Zhang</surname><given-names>LQ</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Individuals with autism spectrum disorder have altered visual encoding capacity</article-title><source>PLOS Biology</source><volume>19</volume><elocation-id>e3001215</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001215</pub-id><pub-id pub-id-type="pmid">33979326</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cognitive, Systems, and Computational Neurosciences of the Self in Motion</article-title><source>Annual Review of Psychology</source><volume>73</volume><fpage>103</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-021021-103038</pub-id><pub-id pub-id-type="pmid">34546803</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Noel</surname><given-names>JP</given-names></name><name><surname>Shivkumar</surname><given-names>SD</given-names></name><name><surname>Haefner</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>ASD Causal Inference</data-title><source>Open Science Framework</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/6xbzt">https://osf.io/6xbzt</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odegaard</surname><given-names>B</given-names></name><name><surname>Wozny</surname><given-names>DR</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Biases in Visual, Auditory, and Audiovisual Perception of Space</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004649</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004649</pub-id><pub-id pub-id-type="pmid">26646312</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdreau</surname><given-names>F</given-names></name><name><surname>Cooke</surname><given-names>JRH</given-names></name><name><surname>Koppen</surname><given-names>M</given-names></name><name><surname>Medendorp</surname><given-names>WP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causal inference for spatial constancy across whole body motion</article-title><source>Journal of Neurophysiology</source><volume>121</volume><fpage>269</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1152/jn.00473.2018</pub-id><pub-id pub-id-type="pmid">30461369</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>CE</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Freyberg</surname><given-names>J</given-names></name><name><surname>Baron-Cohen</surname><given-names>S</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tunnel vision: sharper gradient of spatial attention in autism</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>6776</fpage><lpage>6781</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5120-12.2013</pub-id><pub-id pub-id-type="pmid">23595736</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>CE</given-names></name><name><surname>Baron-Cohen</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sensory perception in autism</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.112</pub-id><pub-id pub-id-type="pmid">28951611</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical hierarchies perform Bayesian causal inference in multisensory perception</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002073</pub-id><pub-id pub-id-type="pmid">25710328</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct Computational Principles Govern Multisensory Integration in Primary Sensory and Association Cortices</article-title><source>Current Biology</source><volume>26</volume><fpage>509</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.056</pub-id><pub-id pub-id-type="pmid">26853368</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Ehlis</surname><given-names>AC</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural dynamics of hierarchical Bayesian causal inference in multisensory perception</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1907</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09664-2</pub-id><pub-id pub-id-type="pmid">31015423</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>A</given-names></name><name><surname>Patterson</surname><given-names>JS</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A computational perspective on autism</article-title><source>PNAS</source><volume>112</volume><fpage>9158</fpage><lpage>9165</lpage><pub-id pub-id-type="doi">10.1073/pnas.1510583112</pub-id><pub-id pub-id-type="pmid">26170299</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubenstein</surname><given-names>JLR</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Model of autism: increased ratio of excitation/inhibition in key neural systems</article-title><source>Genes, Brain, and Behavior</source><volume>2</volume><fpage>255</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1034/j.1601-183x.2003.00037.x</pub-id><pub-id pub-id-type="pmid">14606691</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rutter</surname><given-names>M</given-names></name><name><surname>Bailey</surname><given-names>A</given-names></name><name><surname>Lord</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>The Social Communication Questionnaire: Manual</source><publisher-name>Western Psychological Services</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawai</surname><given-names>KI</given-names></name><name><surname>Sato</surname><given-names>Y</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Auditory time-interval perception as causal inference on sound sources</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>524</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00524</pub-id><pub-id pub-id-type="pmid">23226136</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Series</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Computational Psychiatry</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>E</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Bennetto</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal synchrony and audiovisual integration of speech and object stimuli in autism</article-title><source>Research in Autism Spectrum Disorders</source><volume>39</volume><fpage>11</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.rasd.2017.04.001</pub-id><pub-id pub-id-type="pmid">30220908</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Surén</surname><given-names>P</given-names></name><name><surname>Saasen-Havdahl</surname><given-names>A</given-names></name><name><surname>Bresnahan</surname><given-names>M</given-names></name><name><surname>Hirtz</surname><given-names>D</given-names></name><name><surname>Hornig</surname><given-names>M</given-names></name><name><surname>Lord</surname><given-names>C</given-names></name><name><surname>Reichborn-Kjennerud</surname><given-names>T</given-names></name><name><surname>Schjølberg</surname><given-names>S</given-names></name><name><surname>Øyen</surname><given-names>AS</given-names></name><name><surname>Magnus</surname><given-names>P</given-names></name><name><surname>Susser</surname><given-names>E</given-names></name><name><surname>Lipkin</surname><given-names>WI</given-names></name><name><surname>Stoltenberg</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensitivity and specificity of early screening for autism</article-title><source>BJPsych Open</source><volume>5</volume><elocation-id>e41</elocation-id><pub-id pub-id-type="doi">10.1192/bjo.2019.34</pub-id><pub-id pub-id-type="pmid">31530312</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname><given-names>R</given-names></name><name><surname>Vogel</surname><given-names>M</given-names></name><name><surname>Josic</surname><given-names>K</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Optimal inference of sameness</article-title><source>PNAS</source><volume>109</volume><fpage>3178</fpage><lpage>3183</lpage><pub-id pub-id-type="doi">10.1073/pnas.1108790109</pub-id><pub-id pub-id-type="pmid">22315400</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vishne</surname><given-names>G</given-names></name><name><surname>Jacoby</surname><given-names>N</given-names></name><name><surname>Malinovitch</surname><given-names>T</given-names></name><name><surname>Epstein</surname><given-names>T</given-names></name><name><surname>Frenkel</surname><given-names>O</given-names></name><name><surname>Ahissar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Slow update of internal representations impedes synchronization in autism</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5439</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25740-y</pub-id><pub-id pub-id-type="pmid">34521851</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Woynaroski</surname><given-names>TG</given-names></name><name><surname>Stevenson</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multisensory Integration as a Window into Orderly and Disrupted Cognition and Communication</article-title><source>Annual Review of Psychology</source><volume>71</volume><fpage>193</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010419-051112</pub-id><pub-id pub-id-type="pmid">31518523</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wichmann</surname><given-names>FA</given-names></name><name><surname>Hill</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The psychometric function: I. Fitting, sampling, and goodness of fit</article-title><source>Perception &amp; Psychophysics</source><volume>63</volume><fpage>1293</fpage><lpage>1313</lpage><pub-id pub-id-type="doi">10.3758/bf03194544</pub-id><pub-id pub-id-type="pmid">11800458</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodbury-Smith</surname><given-names>MR</given-names></name><name><surname>Robinson</surname><given-names>J</given-names></name><name><surname>Wheelwright</surname><given-names>S</given-names></name><name><surname>Baron-Cohen</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Screening adults for Asperger Syndrome using the AQ: a preliminary study of its diagnostic validity in clinical practice</article-title><source>Journal of Autism and Developmental Disorders</source><volume>35</volume><fpage>331</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1007/s10803-005-3300-7</pub-id><pub-id pub-id-type="pmid">16119474</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woynaroski</surname><given-names>TG</given-names></name><name><surname>Kwakye</surname><given-names>LD</given-names></name><name><surname>Foss-Feig</surname><given-names>JH</given-names></name><name><surname>Stevenson</surname><given-names>RA</given-names></name><name><surname>Stone</surname><given-names>WL</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multisensory speech perception in children with autism spectrum disorders</article-title><source>Journal of Autism and Developmental Disorders</source><volume>43</volume><fpage>2891</fpage><lpage>2902</lpage><pub-id pub-id-type="doi">10.1007/s10803-013-1836-5</pub-id><pub-id pub-id-type="pmid">23624833</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamashita</surname><given-names>I</given-names></name><name><surname>Katahira</surname><given-names>K</given-names></name><name><surname>Igarashi</surname><given-names>Y</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name><name><surname>Okada</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Recurrent network for multisensory integration-identification of common sources of audiovisual stimuli</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>101</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00101</pub-id><pub-id pub-id-type="pmid">23898263</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>F</given-names></name><name><surname>Dong</surname><given-names>J</given-names></name><name><surname>Dai</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sampling-based causal inference in cue combination and its neural implementation</article-title><source>Neurocomputing</source><volume>175</volume><fpage>155</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2015.10.045</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidel</surname><given-names>A</given-names></name><name><surname>Goin-Kochel</surname><given-names>RP</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Self-motion perception in autism is compromised by visual noise but integrated optimally across multiple senses</article-title><source>PNAS</source><volume>112</volume><fpage>6461</fpage><lpage>6466</lpage><pub-id pub-id-type="doi">10.1073/pnas.1506582112</pub-id><pub-id pub-id-type="pmid">25941373</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits</article-title><source>Advances in Neural Information Processing Systems</source><volume>32</volume><fpage>3804</fpage><lpage>3813</lpage></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71866.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Xiang</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group></front-stub><body><p>Autism spectrum disorder is characterized by social, communicative and sensory anomalies. This study uses behavioral psychophysics experiments and computational modelling to interrogate how individuals with autism combine sensory cues in multisensory tasks. The results showed that individuals with autism were more likely to integrate cues, but less likely to report doing so, thus raising interesting questions regarding how individuals with autism perceive the world.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71866.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Yu</surname><given-names>Xiang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Beierholm</surname><given-names>Ulrik</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01v29qb04</institution-id><institution>Durham University</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Aberrant causal inference and presence of a compensatory mechanism in Autism Spectrum Disorder&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Richard Ivry as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Ulrik Beierholm (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The experiments report interesting results regarding audio-visual integration for spatial discriminations in both typical individuals and individuals with ASD. However, the conceptual framing (including the model) is one of several potential accounts of these data, and should be framed as such. Alternative accounts need to be presented and seriously discussed, and not just as an extension of the Discussion. The abstract and other parts of the manuscript also need to be adjusted accordingly.</p><p>2) Related to point 1. Prominent aspects of the data, including higher overall bias in autism in Figure 2, are not captured in the model in Figure 4. The dissociation between explicit and implicit is not convincing, and the stress on group differences puts an emphasis on small effects. Please revise model and/or Discussion to address these concerns.</p><p>3) Model fitting is not described sufficiently. How were the sensory parameters fitted? It seems that more than 20 parameters were fitted (Supp. File 1) for the aggregate subject through the slice sampling, is that correct? Was this also done for individual subjects? What was done to ensure convergence? Was any model comparison done? Please include a list or figure showing the different steps of the model fitting.</p><p>4) The model may be over-specified with both a lapse rate and a lapse bias. Please test a simpler model without lapse bias or explain why that was not done.</p><p>5) In experiments 3 and 4 please detail the specific instructions given. Specifically, were participants asked to press a button if they thought both cues come from the same source, or if they thought that the 2 cues come from 2 sources? Since there was not a default option (an &quot;I don't know option&quot;), it's important to know the default – determined by the way the question was phrased.</p><p>6) The participants in each experiment were not clearly described. Please provide more details about the task completion of participants, such as how many completed all four tasks, etc. A table would be helpful. Specifically, what were the performance scores in Experiment 1 – of the sub-group of participants of Experiment 2 – the question of whether the psychometric plots did not differ between ASD and controls participating in this study is crucial for estimating whether they were expected to have different magnitudes of bias (as they actually did). The authors did not address the question of the overall bias magnitude, only the values at the large disparities.</p><p>7) Please specify the criteria for the ASD diagnosis, DSM-5 or DSM-4? Are they classic autism or Asperger or PDD-NOS subjects? Were the gold standard ADOS ADIR performed to confirm the diagnosis? If not, the authors should acknowledge this as a limitation in Discussion.</p><p>8) More detailed research participant description is required. SCQ and AQ were performed for all participants. Were there ASD individuals below the cut-off of these two scales? or any TD participants above the cut-off? This information should be stated. The authors should consider excluding the ASD individuals below the cut-offs and TD individuals above the cut-offs from data analysis. Please provide more details about how the TD participants were recruited. IQ was available for a subset of the ASD participants: How many of them have IQ scores? IQ was measured using what test? Was the IQ measured for the TD group?</p><p>9) Please report effect sizes, e.g. eta<sup>2</sup> or Cohen's d.</p><p><italic>Reviewer #1:</italic></p><p>Using a series of cue combination tasks, the authors studied the causal inference of multisensory stimuli in people with ASD. The authors found the intact ability in optimal cue combination of participants with ASD but impairment in dissociating audio and visual stimuli when presented with wider spatial disparity. It suggested they persisted with a wrong integration model for causal inference. However, the individuals with ASD explicitly report the common cause of stimuli fewer than the controls. Through formal modeling, the authors found increased prior probability for the common cause in ASD. However, reporting the common cause in ASD is reduced in the explicit task, indicative of a compensatory mechanism via a choice bias.</p><p>In general, I think this study was well-designed and the results were interesting. The conclusions of this paper are mostly well supported by data. But I have a few questions that I would like to see the author’s address.</p><p>1. When comparing the temporal disparity task to the spatial task, the authors concluded that the overall reduced tendency to report common cause at any disparity and across spatial and temporal conflicts seemingly is the defining characteristic of ASD. However, in Figure 3D, it could tell that a higher proportion of common cause reporting in ASD when absolute temporal disparity became greater, which differed from the case of spatial task and from when the temporal disparity was narrower. Could the conclusion be too general? The authors should tone it down or give more discussion about the incongruence.</p><p>2. When fitting the model to individual subject data, the authors found comparable p<sub>combined</sub> for the explicit task between ASD and control subjects. This seemed to be contrasted to the result of aggregate data and behavioral results. Did the difference come from the fitting procedure? Did the significant decreased in p<sub>combined</sub> was because of the lack of consideration of subject heterogeneity? The authors could provide more explanation or discussion of it.</p><p>3. A related question is about the intuition behind the two steps of modeling fitting (i.e., to aggregate and individual data). What more could fitting models to aggregate or individual data provide to one another procedure? The authors should elaborate on it.</p><p>4. I would like to see the authors discuss more the interesting finding of a potential compensatory mechanism, particularly the meaning of it in terms of the possible relation to ASD symptoms. For example, how would the increased prior probability of common cause report and the compensatory choice bias contribute to the sensory abnormalities in ASD?</p><p>5. The participants in each experiment were not clearly introduced. The authors should provide more details about the task completion of participants, such as how many completed all four tasks, etc. And the data of how many participants who participated in both the implicit and explicit spatial task were included in modeling?</p><p>6. The authors could also conduct some correlational analyses between estimated model parameters and symptomatology measures, just as what they have done for psychometric features, to further investigate how autistic symptoms would affect the process of causal inference.</p><p>7. Since the data of the individuals with poor performance were also fitted (such as 8 of the individuals with ASD in Experiment 3), it is interesting to see if there is anything special or atypical in terms of their model parameters, even though their data were not included in behavioral analyses.</p><p>8. I suggest specifying the criteria for the ASD diagnosis, DSM-5? or DSM-4? or ICD-10? Are they classic autism or Asperger or PDD-NOS? Were the gold standard ADOS ADIR performed to confirm the diagnosis? If not, the authors should acknowledge this as the limitation in Discussion.</p><p>9. SCQ and AQ were performed to all participants. My question is: is there any ASD individuals below the cut-off of these two scales? or any TD participants above the cut-off. the authors should consider excluding the ASD individuals below the cut-offs and TD individuals above the cut-offs from the data analysis.</p><p>10. Please provide more details about how the TD participants were recruited?</p><p>11. IQ was available for a subset of the ASD participants: How many of them have IQ scores? Is there any particular reason that the other ASD participants did not have IQ scores? How the IQ was measured? using Wechesler or Raven's test? Was the IQ measured for the TD group?</p><p>12. The authors could provide direct comparisons of thresholds and visual weights between two groups in the result section of Experiment 1.</p><p>13. Errors bars in Figure 1E and 1H were not very obvious. The authors could consider using simpler markers, such as &quot;+&quot; (i.e., short lines) for simultaneously displaying horizontal and vertical error bars.</p><p>14. It should be &quot;As for the case of auditory disparities, …&quot; instead of &quot; As for the case of spatial disparities, …&quot; for the first sentence of the second paragraph after Figure 3.</p><p><italic>Reviewer #2:</italic></p><p>The paper consists of 4 interesting experiments examining multisensory processing in autism spectrum disorder. The first experiment shows that participants with ASD perform similar to controls in cross-model integration, a conceptual replication of earlier findings from this group. However, the subsequent experiments reveal some intriguing differences between the groups in terms of how they use explicit and implicit information in evaluating if auditory and visual information comes from a common source or distinct sources. The authors propose a model that aims to explain the seeming dissociation between explicit and implicit reports of the two groups. The strength of this work is that the experiments are very interesting and report interesting results regarding audio-visual integration for spatial discriminations in both typical individuals and people with ASD. The comparison between explicit and implicit reports is very interesting. In terms of weaknesses, the dissociation between explicit and implicit is not convincing, and the stress on group differences puts an emphasis on, at best, marginal effects, which the modelling does not explain. For example, an alternative account that is consistent with all the data presented is that there are individuals with ASD who are somewhat poorer auditory discriminators, resulting in the bias effects and broader disparities. These individuals would be less likely to commit to an explicit &quot;single source&quot; statement in line with their reduced auditory localization skills.</p><p>The dissociation between explicit and implicit is not convincing, and the stress on group differences puts an emphasis on, at best, marginal effects, which the modelling does not explain (the strongest linearity on ASD's curve in Figure 2 – is not captured in the modelling in Figure 4) For example, an alternative account that is consistent with all the data presented is that there are individuals with ASD who are somewhat poorer auditory discriminators and they impacted overall performance in Experiment 2, resulting in a larger bias effect, and also somewhat broader in disparities. These individuals would be less likely to commit to an explicit &quot;single source&quot; statement, which is quite committing, in line with their reduced auditory localization skills. The authors should at least address this alternative account, and present auditory discrimination curves of Experiment 2's participants.</p><p>The model does not account for the data point of individuals with autism being pulled by a reliable visual blob 24 degrees away, which was the main point in Figure 3.</p><p>Overall the authors ignore more prominent aspects of the data (e.g. higher overall bias in autism in Figure 2) for points they want to make (non linearity larger in autism than in controls).</p><p>Reliability – is a confusing term. The stimuli are reliably presented, but the information the perceivers derive regarding their position is less reliable when stimuli are small.</p><p>Figure 1f, g – I had difficulties understanding. I assume that the dashed lines should be to the right of the solid lines, which is the case for &quot;high-reliability&quot; blob, but why is it switched for the low reliability case? In both sample participants (f and g) and I wonder why the bias is larger (larger distance between dashed and matched solid plot, in both participants) for low versus intermediate size (reliability) blobs. If this is the actual result – it needs explanation.</p><p>Figure 2 – the main observation is that the bias in autism is larger. Perhaps this group difference stems from this group being somewhat poorer auditory spatial discriminators than their 15 age-matched controls in the experiment. If their auditory discrimination is poorer we would expect an overall larger bias, and perhaps also across a broader range of audio-visual disparities.</p><p>Importantly, this is probable account, since this is a smaller population than in Experiment 1 – and their discrimination thresholds are not addressed. Importantly – I could not figure out the overlap in participation across the various experiments. In experiment 1 matched performance was only obtained when 6 participants with ASD were excluded. In Experiment 3 (24 participants originally) – they also excluded a large subgroup, whose behavior was different. Here the group is initially small so variability across participants was not discussed.</p><p>The strongest point for the claim of too broad integration is the bottom left point – where high reliability blob has an effect that even increases when the visual blob is presented 24 degrees apart. This point is hard to reconcile (and is not reconciled by the model proposed in Figure 4 either). The authors should show that it is a reliable data point – perhaps by showing single subject data.</p><p>In experiments 3 and 4 the specific instructions are crucial – are participants asked to press a specific button if they are perceived as coming from the same source? Or press a button if they are perceived as coming from 2 separate sources. Here phrasing may have affected the decisions of individuals with autism. In order to dissociate between these 2 options it would have been nice to have a third option &quot;don't know&quot;. If participants with autism tend to say to be less decisive they would tend to commit to a single source. This account may be explained by being somewhat implicitly poorer localizers.</p><p>If you have discrimination functions of the specific subgroups that took part in Experiments 2-3 (since they all participated in Experiment 1 – right?) – please show them or report discrimination skills for these subgroups, since this is the relevant control-ASD matching.</p><p>Re modelling and Figure 4 – It is difficult to follow the model – perhaps label the model parameters in the diagram of Figure 4a.</p><p><italic>Reviewer #3:</italic></p><p>In this paper Noel et al., use a combination of psychophysical experiment and computational modeling to examine the differences in behaviour between participant on the Autism Spectrum Disorder and control participants when dealing with multi-sensory stimuli (e.g. audio-visual). It is well known that ASD subjects tend to differ in how they combine such stimuli, and it has previously been suggested that this may be due to a difference in the tendency to perform causal inference.</p><p>The study indeed finds that while ASD participants had similar ability to combine cues when unambiguously from the same source, they differed in the tendency to combine them when unclear if necessary to combine. In contrast when asked to explicitly indicate whether stimuli originated from the same source (and therefore should be combined) they tended to under report.</p><p>While the experiments are in themselves very standard, the paper relies on computational modeling to differentiate the possible behavioural effects, using advanced Bayesian statistical methods.</p><p>These results confirm existing ideas, and build on our understanding of ASD, while still leaving many questions unanswered. The results should be of interest to anyone studying ASD as well as any other developmental disorders, and perception in general.</p><p>I enjoyed reading this paper, although the model fitting procedure especially was not clear to me. How were the sensory parameters fitted? By my count more than 20 parameters were fitted (Supp. File 1) for the aggregate subject through the slice sampling, is that correct? Was this also done for individual subjects? I would be nervous about fitting that many parameters for individual subject data. What was done to ensure convergence?</p><p>Was any model comparison done? Might be better to include a list or figure showing the different steps of the model fitting.</p><p>I also worry that the model is over specified with both a lapse rate and a lapse bias. From my understanding the lapse rate specifies when subjects (through lack of concentration or otherwise) fail to take trial stimuli into account and therefore go with their prior. In other studies this prior may be identical to the prior over spatial range, or may be a uniform discrete distribution over the bottoms available for response.</p><p>Maybe the variables are constrained in ways that I did not understand, but with just a binary response (Left/Right) the model can largely incorporate any bias to a large set of possible parameter values of lapse rate and bias. I.e. that the model is over specified. That would also explain the wide range of values for the fitted parameters in Figure 3.</p><p>I think this should really be investigated before the results can be trusted.</p><p>Looking at Figure 4E and F makes me hesitant about trusting the results.</p><p>Authors also acknowledge that the lapse bias and P combined are too closely entwined to really be well separated in the explicit temporal experiment. Maybe for that reason it would also be useful to test a simpler model without lapse bias?</p><p>I find it mildly confusing that D refers to a Left/Right response in the implicit task, and Common/Separate in the explicit task. Maybe better to use separate symbols? D is fine for 'decision' but in places in the text it is instead referred to as 'trial category' which is vague. I also don't really think D is needed in the generative model in Figure 4 as it is not really causing the subsequent variables C or Sa.</p><p>Does <italic>eLife</italic> not require the reporting of effect sizes (e.g. eta<sup>2</sup> or Cohen's d)? It would be good to include these.</p><p>The plots in Figure 3 mostly look like shifts up for ASD relative to controls. The authors might want to fit a model with a positive bias, i.e.</p><p>a*N(mu,sd<sup>2</sup>)+b</p><p>may fit better (could do model comparison) and just show difference in b. This is just a suggestion though, but it may be cleaner for their argument.</p><p>In the Discussion, while divisive normalisation is one way to achieve the marginalisation needed for Bayesian causal inference, there are other ways to achieve it (Cuppino et al., 2017, Yamashita 2013, Yu et al., 2016, Zhang et al., 2019). It would be good to acknowledge this.</p><p>Eq 5 and 6, 38 are misleading. Likelihood is a function of Sa/Sv, so would be better to write as l(Sa)=N(Xa;Sa,Sv)</p><p>Eq 9: is D either 1 or 2? Or 1 or -1?</p><p>Detail: maybe use different symbols for lapse rate and lapse bias? I find λ and odell confusing. How about P<sub>lapse</sub> for the lapse rate to emphasise that it is a probability? P<sub>common</sub> is already a fitted variable that is also a probability of a Bernoulli distribution.</p><p>Page 5 (pages of the pdf):</p><p>“ …ASD did not show impairments in integrating perceptually congruent auditory and visual stimuli.”</p><p>– “ …ASD did not show impairments in integrating perceptually congruent (and near-congruent) auditory and visual stimuli.”</p><p>In experiment 2 there was a six degree discrepancy, so near-congruent seems appropriate.</p><p>Typos:</p><p>“We perform the integral in Eq. S5 for the implicit task by”: should this be Eq. 35?</p><p>References:</p><p>Cuppini, C., Shams, L., Magosso, E. and Ursino, M. A biologically inspired neurocomputational model for audiovisual integration and causal inference. Eur. J. Neurosci. 46, 2481-2498 (2017).</p><p>Yamashita, I., Katahira, K., Igarashi, Y., Okanoya, K. and Okada, M. Recurrent network for multisensory integration-identification of common sources of audiovisual stimuli. Front. Comput. Neurosci. 7, (2013).</p><p>Yu, Z., Chen, F., Dong, J. and Dai, Q. Sampling-based causal inference in cue combination and its neural implementation. Neurocomputing 175, 155-165 (2016).</p><p>Zhang, W., Wu, S., Doiron, B. and Lee, T. S. A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits. Adv. Neural Inf. Process. Syst. 32, 3804-3813 (2019).</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled “Aberrant causal inference and presence of a compensatory mechanism in Autism Spectrum Disorder” for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Barbara Shinn-Cunningham (Senior Editor) and a Reviewing Editor.</p><p>All reviewers agree that the manuscript has improved significantly during revision, but there are some remaining issues to be addressed, as noted below and described in detail in the individual reviews:</p><p>1. More detailed description of how statistical analysis was carried out, including clarifications/modifications as suggested by reviewer 1.</p><p>2. Rebalancing interpretation of the experimental and odelling results, as suggested by reviewer 2.</p><p><italic>Reviewer #1:</italic></p><p>The authors have addressed my recommendations and questions in much detail. Their changes have improved the quality of the manuscript as a result, illuminating the perceptual causal inference in ASD across different contexts. However, I believe there still are a couple of points that the authors can address to make the description of the results and the methods even clearer for publication.</p><p>1. Figure legends/captions of Figures 3 and 4 in the main texts lack detailed descriptions of the elements in the figures. For example, for Figures 3 and 4, what do those error bars represent? Standard errors or confidence intervals? In Figure 4B, are solid lines the model predictions and hollow points the observations? I believe this essential information would help readers better understand the figures.</p><p>2. The data points in Figure 2A-B and Figure 3A-C are slightly different from those in Figure 4B-C. For example, in Figure 2B, the audio bias of 24 deg disparity is weaker than that of 12 deg disparity for the high visual reliability condition (dark brown lines and points); however, in Figure 4B left panel, the audio bias of 24 deg disparity is even larger than that of 12 deg disparity. I assume that the data points depicted in Figure 4B-C are the aggregate data for modelling, in which the data of some participants were not included? I notice that the authors have included which participants were included in the single-subject modelling, but was the aggregate data the same as what was used for plotting Figures 2 and 3? I find it a bit confusing at first sight, perhaps the author could check it again and/or mention the related information in the caption or the main text?</p><p>3. From lines 451-453 of merged files (Instead, differences between […] relative to control observers.), did the author imply that the model where p<sub>common</sub> was freely estimated from the data was better, compared with the model where p<sub>common</sub> was fixed (I guess it’s the model in Figure 4 – supplement 2)? In other words, did the authors have two different models and conduct a model comparison here? If so, I think it’s better to provide model comparison results. The question also applies to the texts from lines 460-461. Also, what is DAIC? Is it the difference of AIC between the full model (that allows p<sub>common</sub>) and the restricted model (that fixes p<sub>common</sub> to a constant)? The authors should describe it somewhere in the main text.</p><p>4. The authors should be more specific about the tests they used to compare model parameters between groups and those correlational analyses. What type of tests did the authors use, parametric (i.e., Welch t-test, Pearson correlation) or non-parametric (i.e., Mann-Whitney, Spearman correlation, or permutation methods)? Particularly for the comparison of p<sub>combined</sub> (Figure 4G), would the result be different when a non-parametric test was used if the test used in the current revision was parametric? I suggest the authors take more robust approaches given that the distributions of the model parameters seemed not quite Gaussian.</p><p>5. What is α and ν in Equation 5 and 6, please define them in the text. Also, it would be better if the authors give a short introduction to the meaning of lapse rate, lapse bias, etc., when mentioning them for the first time. Given that many readers are not very familiar with computational modelling, they may not intuitively understand what these parameters represent.</p><p>6. The D in DAIC from line 462 is in another font.</p><p>7. I apologize in advance if it’s my mistake but I failed to find Supplementary Text 1 mentioned in lines 430, 451, and 459. Where could I find it?</p><p><italic>Reviewer #2:</italic></p><p>The authors have adequately addressed my comments.</p><p>The strong aspects of the results are better clarified, and the overlap between participants across experiments is also clear. Further, the authors do not make claims that are not directly supported experimentally.</p><p>The limitation of a somewhat small (&lt;20) number of participants per group in important experiments is still a drawback, given participants’ variability, particularly in the ASD group. Yet, I believe that the main results hold.</p><p>The strongest aspects of the study are the direct results, rather than the modelling:</p><p>Experiment 1: audio-visual integration is intact in ASD 2. Yet multisensory behavior is atypical (in the current experimental protocol) – ASD participants tend to favor source integration, as manifested by their cross-modal bias in localization even when visual and auditory signal are separable from a sensory perspective. Though both groups tend to over integrate, this is more salient and tend to span a broader distance in ASD. 3. Explicit reports have an opposite tendency – individuals with ASD were less likely to report a common cause for the two stimuli. Given the adequate direct measures of ASD cue integration with a small audio-visual distance (performance in Experiment 1) these results suggest a specific atypicality in cause attribution.</p><p>I also find the difference between spatial and temporal integration very interesting. Temporal and spatial groups differences in explicit attribution of a common source merits some additional discussion.</p><p>Personally, I think the contribution of the modelling part to the study is overstated in the paper, but I agree that is a personal perspective and need not be imposed on the authors.</p><p><italic>Reviewer #3:</italic></p><p>The authors have done a very good job including new alternative models, and improving the Description of the modelling (modelling my main points of scepticism). I am happy to recommend the paper for publication.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71866.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The experiments report interesting results regarding audio-visual integration for spatial discriminations in both typical individuals and individuals with ASD. However, the conceptual framing (including the model) is one of several potential accounts of these data, and should be framed as such. Alternative accounts need to be presented and seriously discussed, and not just as an extension of the Discussion. The abstract and other parts of the manuscript also need to be adjusted accordingly.</p></disp-quote><p>We thank the reviewers for their suggestion and agree that presenting alternative accounts will strengthen the manuscript. We have done this both empirically and via additional modelling.</p><p>Empirically, we check whether visual and/or auditory localization performance is equal across the control and ASD cohorts participating in the causal inference task. We now report the auditory and visual discrimination bias and thresholds for these participants (Figure 2 – supplement figure 1). The results show no difference between controls and individuals with ASD, suggesting that a potential baseline difference in sensory processing between these groups does not explain their differing behavior during causal inference.</p><p>In additional modelling, we have now included the following potential accounts (see Figure 4 – supplement figure 7, 8, and 9):</p><p>A. Forced fusion (all parameters are free to vary, except for C, which is fixed to 1).</p><p>B. Forced segregation (all parameters are free to vary, except for C, which is fixed to 2).</p><p>C. Uniform lapse bias (Lapse rate, p<sub>common</sub>, p<sub>choice</sub> are free to vary with a uniform lapse bias – unbiased model).</p><p>D. D1. Implicit task: Lapse rate, lapse bias, and p<sub>choice</sub> are free to vary, others are not.</p><p>D2. Explicit task: Since p<sub>choice</sub> trades off against p<sub>common</sub> for the explicit task, alternative D2 is similar to D1 (above), but only lapse rate and bias are free to vary.</p><p>In alternatives A and B, we fit the model to the ASD aggregate data. In models C and D, we first fit to the control aggregate subject, and then vary the specific parameters noted to fit to the ASD aggregate subject relative to the control. We report AICs for the ASD aggregate subject.</p><p>For the implicit task (most cleanly indexing p<sub>common</sub>), all alternative accounts perform worse than the model included in the main text (where lapse rate, lapse bias, p<sub>common</sub> and p<sub>choice</sub> are free to vary from control to ASD aggregate subjects; see Figure 4 – supplement figure 7). This suggests that how individuals with ASD infer causal relations, and not their sensory uncertainty, is the most parsimonious factor differentiating between ASD and control (having excluded alternatives A and B that allow for different sensory uncertainties while fixing the causal inference parameter). Further, the fact that alternative A (forced fusion) does better than alternative B (forced segregation) again suggests that individuals with ASD tend to overweight integration relative to segregation. The fact that the model in the main text performs better than Alternative A suggests that individuals with ASD do <italic>not always</italic> integrate, they do perform causal inference, but are biased toward integration over segregation compared to controls.</p><p>In response to this comment, we have modified the text to include the unisensory performance for participants in Experiment 2 (P6, “To confirm […] and across all reliabilities<italic>),</italic> as well as the alternative models (P10-11, “Lastly, we consider a set of alternative models […]” vary (main model used in Figure 4)). We have also included new supplementary figures:</p><p>– Figure 2 – supplement figure 1. Visual and auditory localization performance of participants in Experiment 2 (audio-visual implicit causal inference).</p><p>– Figure 4 – supplement figure 7. Goodness-of-fit of alternative models for the implicit and explicit spatial causal inference task.</p><p>– Figure 4 – supplement figure 8. Illustration of the alternative models fits to implicit causal inference model data.</p><p>– Figure 4 – supplement figure 9. Illustration of the alternative models fits to explicit causal inference model data.</p><p>Lastly, we have included tables specifying which participants took part in the different experiments (in order to examine unisensory performance only of those subjects performing the causal inference task):</p><p>– Supplement File 1. Control participants.</p><p>– Supplement File 2. ASD participants.</p><disp-quote content-type="editor-comment"><p>2) Related to point 1. Prominent aspects of the data, including higher overall bias in autism in Figure 2, are not captured in the model in Figure 4. The dissociation between explicit and implicit is not convincing, and the stress on group differences puts an emphasis on small effects. Please revise model and/or Discussion to address these concerns.</p></disp-quote><p>The reviewers are correct in pointing out that the model illustrated in Figure 4 (aggregate data) does not fully account for all aspects of the data. However, we must note that we present the aggregate fits to highlight what parameters could or could not in principle explain global differences between the ASD and control cohort. In other words, while the aggregate subjects highlight the common or differing patterns across the two cohorts (ASD and control), even if all subjects used a causal inference strategy, the aggregate subject need not have a pattern completely consistent with the predictions of a causal inference model. On the other hand, the individual subject fits are very good (Figure 4E), with all but one subject showing an explainable variance explained above 80%. To further illustrate the quality of these fits – particularly considering that all experiments are fit jointly! – in this revision we have included supplementary figures showing example control (Figure 4 – supplement figure 3, 4) and ASD (Figure 4 – supplement figure 5, 6) subjects. Again, we must reiterate that we do not fit individual models to account for a particular experiment, but instead attempt to account for a subject’s behavior as a whole, across experiments and sensory modalities.</p><p>In the original submission, to highlight global differences between the ASD and control cohorts in a principled manner, we fixed all the sensory parameters (given the results in Experiment 1 showing no unisensory difference across groups) and only allowed for flexibility in the choice and inference parameters. Our new analyses presented in Figure 2 – supplement figure 1, Figure 4 – supplement figure 7 (see above), and Figure 3 – supplement figure 1 (unisensory performance for subjects taking part in Experiment 3)<bold>,</bold> all concord in supporting the fact that there is no difference in sensory performance between the ASD and control cohorts. Nonetheless, we have now also tested this explicitly by testing an alternative model where the sensory uncertainty and choice parameters were allowed to vary from control to ASD (Figure 4 – supplement figure 2) but p<sub>common</sub> was fixed to the value of the aggregate control (similar to Alternatives A and B above, but where C is fixed to the aggregate control value, as opposed to C=1 or C=2). This model performed worse than that in the main text, where p<sub>common</sub> and choice parameters were allowed to vary. This indicates that a difference in p<sub>common</sub> explains better the difference between ASD and control subjects as compared to differences in sensory uncertainty. We also point out that for the individual subjects, sensory uncertainties were fit along with the inference parameters, and we found a significant difference in p<sub>common</sub> for ASD and control subjects. We found no difference in the estimated sensory uncertainty.</p><p>Similarly (and going beyond prior studies) here we fit data from all visual reliabilities jointly, which considerably constrains the model. In Figure 4 – supplement figure 12 (implicit task) and Figure 4 – supplement figure 13 (explicit task) we now demonstrate that either allowing all parameters to vary freely (Panel A), fitting all visual reliabilities separately (Panel B), or doing both of these together (Panel C), would have resulted in better aggregate fits. Even though these alternative models do not concord with our data showing no difference in sensory performance between ASD subjects and controls, these models yield similar results to those reported in the main text, with p<sub>common</sub> being numerically higher in ASD vs. control subjects, further demonstrating that this (our central) result is highly robust.</p><p>Lastly, the reviewers suggest that the group effect is small. To ascertain whether a difference in implicit causal inference between ASD and controls is a robust and replicable effect, and to probe its generalizability beyond audio-visual localization, we have now conducted a conceptual replication and extension. We follow the protocols from Dokka et al., 2019 (<italic>PNAS</italic>), where observers see a pattern of optic flow indicating translation slightly leftward or rightward. Further, they see an independent object, moving at different speeds. Subjects are asked to report their heading (leftward or rightward), and whether the object was stationary or moving. This is a causal inference task, given that if the object is perceived as stationary (hence part of the optic flow), it’s movement on our retinas ought to influence heading perception, but not if the object is perceived to be moving. In Figure 2 – supplement figure 2 (new addition to the revision) we demonstrate that just as for the audio-visual case, control subjects are biased by the independently moving object when this one has a slow speed in the world (similar to the optic flow), but not a fast speed. On the other hand, individuals with ASD seem to readily integrate the independent object into their own heading discrimination, even at large disparities. This is an important conceptual replication of the audio-visual experiment, and thus, even if the effect may be small, it appears to be a reliable one, and a domain general effect.</p><p>In response to this comment, we have modified the text to describe the methods (P17, section “Experiment 5: Visual heading discrimination during concurrent object motion”) and results (P6, “Lastly, to further bolster […] segregation (Figure S2C, D).”) of the replication and extension to the implicit causal inference experiment. We have also further described the goodness-of-fit of the aggregate and individual subject fits (P10, “The causal inference model […] as well as implicit and explicit causal inference”).</p><p>The following supplementary figures (and appropriate text and figure captions) have been included to (i) illustrate the additional implicit causal inference experiment (Figure 2 – supplement figures 1 and 2), and (ii) explicitly show single subject fits (Figure 4 – supplement figures 3-6) as well as alternative fits (e.g., fitting all visual reliabilities separately, or fitting sensory uncertainty but not p-common, Figure 4 – supplement figures 2, 12, and 13):</p><p>– Figure 2 – supplement figure 2. Heading discrimination during concurrent implied self-motion and object motion (Dokka et al., 2019).</p><p>– Figure 3 – supplement figure 1. Visual and auditory localization performance of participants in Experiment 3 (audio-visual explicit causal inference).</p><p>– Figure 4 – supplement figure 2. Fit to aggregate data for the implicit causal inference task, allowing sensory uncertainty and choice parameters to vary but fixing the inference parameter p<sub>common.</sub></p><p>– Figure 4 – supplement figure 3. Data and fit for a single, representative control subject</p><p>– Figure 4 – supplement figure 4. Data and fit for another single, representative control subject</p><p>– Figure 4 – supplement figure 5. Data and fit for a single, representative ASD subject</p><p>– Figure 4 – supplement figure 6. Data and fit for another single, representative ASD subject.</p><p>– Figure 4 – supplement figure 12. Fit to aggregate data for the implicit causal inference task, given that all parameters are free to vary (A), the different visual reliabilities are fit separately (B) or both of the above (C).</p><p>– Figure 4 – supplement figure 13. Fit to aggregate data for the odellin causal inference task, given that all parameters are free to vary (A), the different visual reliabilities are fit separately (B) or both of the above (C).</p><disp-quote content-type="editor-comment"><p>3) Model fitting is not described sufficiently. How were the sensory parameters fitted? It seems that more than 20 parameters were fitted (Supp. File 1) for the aggregate subject through the slice sampling, is that correct? Was this also done for individual subjects? What was done to ensure convergence? Was any model comparison done? Please include a list or figure showing the different steps of the model fitting.</p></disp-quote><p>We thank the reviewers for their question and apologize this was not clearer in the original submission.</p><p>Indeed, each subject had 20 parameters characterizing their responses, yet they were constrained by a large dataset, including four types of tasks (unisensory discrimination, multisensory discrimination, implicit causal inference, and explicit causal inference) and three different visual reliabilities. Further, the prior over the parameters reduces the effective degrees of freedom, such that several parameters have their values tied together (unless constrained to be a specific value based on the empirical data) – equivalent to hierarchical modelling. We have chosen this approach to allow for differences in parameters (e.g., lapse) across experiments that were conducted on different days, but a-priori assuming they are the same. Similarly, when we fit the model to a single (or a subset of) experiment(s), then the parameters specific to the experiment are inferred to be the same as the prior (maximum of prior for the MAP estimate and the prior distribution for the inferred posterior). Therefore, for the aggregate control subject we are effectively fitting 12 parameters for the implicit task and 11 parameters for the explicit task (subset relevant for each task). Moreover, since the model was fit to multiple reliabilities, the prior parameters that were shared across reliabilities were constrained by data from all three reliabilities. For the ASD aggregate subject, either only the choice parameters were varied, or the choice and p<sub>common</sub> were varied relative to the aggregate control.</p><p>We followed a similar procedure for the individual subject fits, but now utilizing their respective empirical data to constrain the parameters. The prior over the parameters is shared as is typical for hierarchical odelling. For the individual subjects, we infer full posteriors over all parameters using slice sampling. The convergence of the sampling was checked by the potential scale factor reduction <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> across chains. Therefore, when we look at the marginal distribution over p<sub>common</sub> or p<sub>combined</sub>, we marginalize out the other parameters as given by Equation R1. This implicitly performs model averaging by considering different models parameterized by different values of the parameters weighed by their posterior.</p><p><inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>interest</mml:mtext></mml:mrow></mml:msub><mml:mrow/><mml:mo>|</mml:mo><mml:mrow/><mml:mtext>data</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>interest</mml:mtext></mml:mrow></mml:msub><mml:mrow/><mml:mo>|</mml:mo><mml:mrow/><mml:mtext>data,</mml:mtext><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>other</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>other</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>data</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>(Equation R1)</p><p>In response to the reviewers’ comment, we have summarized the model fitting procedure, both in text (P9-10, “To bridge […] the two experimental groups”) and as a flowchart in the supplementary materials (Figure 4 – supplement figure 1)<italic>.</italic></p><disp-quote content-type="editor-comment"><p>4) The model may be over-specified with both a lapse rate and a lapse bias. Please test a simpler model without lapse bias or explain why that was not done.</p></disp-quote><p>Allowing for a lapse rate and a lapse bias separately is mathematically equivalent to allowing for two different lapse parameters, each corresponding to a different choice – as is commonly done (e.g., Schütt et al., 2016). We define a prior over lapse bias which is weakly informative, peaking at 0.5. This is the traditional assumption about lapse bias (i.e., unbiased). Therefore, under such a definition, we infer a lapse bias that is different from the commonly assumed 0.5 value if and only if the data has support under such a model. However, as per the reviewers’ suggestion, we have now also tested an alternative (Alternative C in response to Question #1) where only the lapse rate and p<sub>common</sub> were allowed to vary. As alluded to above, this model had a higher AIC (poorer fit) as compared to also allowing for a lapse bias. For individual subjects, the p<sub>common</sub> recovered when considering this simpler model was also higher for ASD as compared to control subjects (p=0.008). Similarly, the p<sub>combined</sub> was not different between the two populations. Thus, the results are consistent with the model allowing for a lapse bias, as presented in the main paper.</p><disp-quote content-type="editor-comment"><p>5) In experiments 3 and 4 please detail the specific instructions given. Specifically, were participants asked to press a button if they thought both cues come from the same source, or if they thought that the 2 cues come from 2 sources? Since there was not a default option (an &quot;I don't know option&quot;), it's important to know the default – determined by the way the question was phrased.</p></disp-quote><p>We thank the reviewers for this question and apologize that we had not provided enough information regarding the instructions. The short answer is that participants were asked to press one button for a given report (e.g., “common cause”) and another for the opposite report (e.g., “separate cause”). Thus, there was no imbalance in the effort required to give one response vs. the other – i.e., there was no default. This important information has now been included in the text (P16, “The exact instructions were […] a standard computer mouse”. And, P17, “Subjects indicated […] standard computer mouse”).</p><disp-quote content-type="editor-comment"><p>6) The participants in each experiment were not clearly described. Please provide more details about the task completion of participants, such as how many completed all four tasks, etc. A table would be helpful. Specifically, what were the performance scores in Experiment 1 – of the sub-group of participants of Experiment 2 – the question of whether the psychometric plots did not differ between ASD and controls participating in this study is crucial for estimating whether they were expected to have different magnitudes of bias (as they actually did). The authors did not address the question of the overall bias magnitude, only the values at the large disparities.</p></disp-quote><p>We have now included in the manuscript Supplement File 1 and Supplement File 2, listing exactly which participants took part in the different experiments (see above). We have also made explicit in the text the inclusion criteria for data in the modeling effort. As suggested by the reviewers, most (but not all, ~80%) participants in Experiments 2 and 3 also took part in Experiment 1, and thus we could detail their visual and auditory localization performance. As we show in Figure 2- supplement figure 1 and Figure 3 – supplement figure 1 (novel additions to the text) there was no difference between the control and ASD sub-groups that participated in Experiments 2 and 3 in terms of their unisensory localization performance. Thus, we can attribute their anomalies in implicit and explicit audio-visual causal inference to the latter computation.</p><p>We also wish to highlight the considerable effort it represents to recruit ~40 ASD and ~40 control adolescents to participate in a series of up to 5 experiments (4 in the original manuscript and an additional one added in revisions). Each task took about 60 to 90 minutes to complete and were completed on different days to avoid fatigue. Given that most participants could not transport themselves, on many occasions appointments were missed, rescheduled, and canceled given the caretakers availability. Still, we collected a total of 220 sessions worth of psychophysical data, which represents a significant contribution (particularly when these tasks are informed by a computational approach as is the case here).</p><disp-quote content-type="editor-comment"><p>7) Please specify the criteria for the ASD diagnosis, DSM-5 or DSM-4? Are they classic autism or Asperger or PDD-NOS subjects? Were the gold standard ADOS ADIR performed to confirm the diagnosis? If not, the authors should acknowledge this as a limitation in Discussion.</p></disp-quote><p>The individuals with ASD were diagnosed according to the DSM-5 (which does not distinguish anymore between classic autism and Aspergers). A subset of these subjects (depending on how they were recruited) also counted with an ADOS assessment. There was no statistical difference in AQ, SCQ, or any of the psychometric estimates between ASD individuals with and without ADOS assessment (all p &gt; 0.21). This suggests that all individuals categorized as within the autism spectrum were appropriately diagnosed. We have amended the text to include this information (P14, “Inclusion in the ASD group […] assessment (all p &gt; 0.21).)</p><disp-quote content-type="editor-comment"><p>8) More detailed research participant description is required. SCQ and AQ were performed for all participants. Were there ASD individuals below the cut-off of these two scales? or any TD participants above the cut-off? This information should be stated. The authors should consider excluding the ASD individuals below the cut-offs and TD individuals above the cut-offs from data analysis. Please provide more details about how the TD participants were recruited. IQ was available for a subset of the ASD participants: How many of them have IQ scores? IQ was measured using what test? Was the IQ measured for the TD group?</p></disp-quote><p>There was no individual with ASD below the recommended SCQ cutoff. There were 2 (out of 47, or 4%, i.e., below the false positive rate) control subjects above the recommended cutoff for the SCQ. The AQ is more complicated, given that different cutoffs have been proposed. Most importantly, there were only 3 control subjects with a higher AQ score than the lowest AQ score among all individuals with ASD. All individuals with ASD had AQ scores above the cutoff proposed by 2 out of 3 published recommendations. The text has been modified to include this information (P14, “There was no individual with ASD below […] by Baron-Cohen et al., 2001, cutoff score of 36”).</p><p>Excluding the control subjects above the SCQ cutoff / overlapping in AQ score with the ASD subject with the lowest score, did not change the statistical interpretation of any of the reported effects.</p><p>The IQ was not measured for the TD group, but we know the mean of the population (by construction) is 100. Thus, we can assume the mean of the TD group was about this value. We only had access to a subset of IQ scores in the ASD cohort (n = 10), because certain clinical providers assessed this metric, while others did not (and thus whether we had an IQ score or not depended on how the subject was recruited). IQ was measured by the Wechsler Adult Intelligence Scale (which is also appropriate for adolescents). The text has been amended to include this information (P14, “Similarly, the Intelligence Quotient […] <italic>mean of 100”</italic>).</p><disp-quote content-type="editor-comment"><p>9) Please report effect sizes, e.g. eta<sup>2</sup> or Cohen's d.</p></disp-quote><p>Thank you for the suggestion. We now report them throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>Using a series of cue combination tasks, the authors studied the causal inference of multisensory stimuli in people with ASD. The authors found the intact ability in optimal cue combination of participants with ASD but impairment in dissociating audio and visual stimuli when presented with wider spatial disparity. It suggested they persisted with a wrong integration model for causal inference. However, the individuals with ASD explicitly report the common cause of stimuli fewer than the controls. Through formal modeling, the authors found increased prior probability for the common cause in ASD. However, reporting the common cause in ASD is reduced in the explicit task, indicative of a compensatory mechanism via a choice bias.</p><p>In general, I think this study was well-designed and the results were interesting. The conclusions of this paper are mostly well supported by data. But I have a few questions that I would like to see the author’s address.</p><p>1. When comparing the temporal disparity task to the spatial task, the authors concluded that the overall reduced tendency to report common cause at any disparity and across spatial and temporal conflicts seemingly is the defining characteristic of ASD. However, in Figure 3D, it could tell that a higher proportion of common cause reporting in ASD when absolute temporal disparity became greater, which differed from the case of spatial task and from when the temporal disparity was narrower. Could the conclusion be too general? The authors should tone it down or give more discussion about the incongruence.</p></disp-quote><p>We thank the reviewer for their suggestion. We have now eliminated all reference to the difference in amplitude being the “defining characteristic”. Instead, we weigh equally the fact that individuals with ASD have both a shallower amplitude at small temporal disparities, and a larger width in the Gaussian describing reports of common cause as a function of temporal disparity. The result section has been modified to reflect this change (P9, “As for the case of spatial disparities […] “binding windows”).</p><p>In the Discussion section we also eliminated reference to the difference in amplitude as being the “defining characteristic” differentiating ASD from control individuals. We do, however, discuss this finding given that it was present in both the spatial and temporal task (also the visual heading discrimination task included during these revisions) and it is less often acknowledged (relative to the difference in “temporal binding windows”, e.g., Feldman et al., 2018).</p><p>Importantly, our modeling strongly implicates the difference in p<sub>common</sub> as the key difference between ASD subjects and controls in the context of a causal inference task.</p><disp-quote content-type="editor-comment"><p>2. When fitting the model to individual subject data, the authors found comparable p<sub>combined</sub> for the explicit task between ASD and control subjects. This seemed to be contrasted to the result of aggregate data and behavioral results. Did the difference come from the fitting procedure? Did the significant decreased in p<sub>combined</sub> was because of the lack of consideration of subject heterogeneity? The authors could provide more explanation or discussion of it.</p></disp-quote><p>The fitting procedure is slightly different, given that in one case we are using aggregate data, while in the other we are fitting to the individual subjects and thus we can leverage knowledge of the subject-specific sensory parameters.</p><p>The aggregate subject fitting was performed to highlight what parameters could or could not explain the global differences between ASD and control subjects. To do so, we considered a restricted model class where we assumed matched sensory parameters between ASD and control subjects (based on the findings from Experiment 1, and now Figure 2 – supplement figure 1 and Figure 2 – supplement figure 1). This approach increased the explanatory power of the model by restricting ourselves to a specific model class. This is similar to a bias-variance trade-off whereby introducing a bias (assumption of model family), we can reduce the variance in parameter estimates. Another reason for the aggregate subject having a higher explanatory power is that the aggregate subject has more data (scaled by number of subjects) which also increases the certainty of the estimates. This modelling approach, on the other hand, cannot capture subject heterogeneity, as pointed out by the reviewer. Hence, we also perform the single subject fits.</p><p>The individual subject data was fit across experiments and on a per subject level. We allowed for a more flexible model class given that we could estimate also sensory uncertainty parameters. This approach results in better fits (see response to questions above) but increases the heterogeneity in individual parameter estimates.</p><p>Overall, we do not think the results are contradictory. First, because slightly different approaches to model fitting were taken. These different approaches maximize the information we may gain, with the aggregate fits attempting to explain global differences between groups and the single subject fits trying to account for individual difference in the observed behavior. Second, there is no conceptual contradiction while there being strong differences in statistical power. In fact, the results are numerically congruent (between aggregate and individual subject). The individual subject data does not differ significantly for P<sub>combined</sub>, but a lack of a statistical difference is not evidence in favor of the null hypothesis. Regardless, even in the individual subject data, the presence of a difference between ASD and control in p<sub>common</sub>, and the lack thereof for P<sub>combined</sub>, indicates a compensatory mechanism since the implicit task clearly demonstrates that ASD individuals are different from controls, but the explicit task, by virtue of being sensitive to compensatory strategies, is not.</p><p>We have amended the text to clarify the fitting procedures, and why multiple approaches were taken (P9, “First, we fit aggregate data […] putatively differentiating the two experimental groups”. And, P10, “Overall, both groups were heterogeneous […] Figure 4F and G”). Further, we have added a few sentences in the results addressing the question as to whether aggregate and single subject data fits yielded contrasting results (P10, “Importantly, the aggregate and single subject fits concord in suggesting an explicit compensatory mechanism in individuals with ASD, given that p<sub>common</sub> is higher in ASD than control (when this parameter can be estimated in isolation) and a measure corrupted by explicit choice biases (i.e., p<sub>combined</sub>) is not.”).</p><disp-quote content-type="editor-comment"><p>3. A related question is about the intuition behind the two steps of modeling fitting (i.e., to aggregate and individual data). What more could fitting models to aggregate or individual data provide to one another procedure? The authors should elaborate on it.</p></disp-quote><p>By inferring the posterior over all model parameters given the data for each individual subject given the data, we are extracting all information there is about each subject, including any individual differences, whether within, or across groups. By repeating this analysis on our aggregate subject, constructed separately for ASD subjects and controls, we now extract information about group-level differences between ASD subjects and controls – the information we are primarily interested in. Additionally, performing the analysis on a group level has the advantage of being able to incorporate knowledge from Experiment 1 that is only available on a group level, namely that ASD subjects and controls have comparable unisensory thresholds – information that by its nature is impossible to use for constraining individual subject fits.</p><p>For the aggregate subject analysis, we first combined the data across subjects resulting in a larger dataset. Second, we restricted the model family by assuming that the sensory parameters were the same in the ASD and control population (given empirical observations in Experiment 1) and only the choice and inference parameters were allowed to vary. Of course, in the individual subject data it does not make sense to use estimates from a population (vs. the individual estimates). In turn, the individual subject fits are more flexible and make fewer assumptions, but this results in higher uncertainty in the conclusions, which are drawn from a limited amount of data.</p><p>By performing <italic>both</italic> these model fitting procedures, we ensure that we can extract as much information as possible while still ensuring that the conclusions reached are not dependent on assumptions made.</p><p>We amended the results (P9, “First, we fit aggregate data […] differentiating the two experimental groups”) to explain why we undertook two modeling approaches and how we interpret the results.</p><disp-quote content-type="editor-comment"><p>4.I would like to see the authors discuss more the interesting finding of a potential compensatory mechanism, particularly the meaning of it in terms of the possible relation to ASD symptoms. For example, how would the increased prior probability of common cause report and the compensatory choice bias contribute to the sensory abnormalities in ASD?</p></disp-quote><p>We thank the reviewer for this suggestion and agree that we could strengthen the clinical impact of this work by discussing a potential relation between the current findings and ASD symptomatology. However, we must also point out that we did attempt correlational analyses between the psychometric effects and two clinical scales (AQ and SCQ). These did not show any reliable correlation, and thus the discussion relating the current findings with symptomatology is speculative (P14, “It is also interesting to speculate […] sensory input ought to be”).</p><p>At the same time, a key insight of our analysis is the dissociation between the sensory percept and the behavioral report due to the compensatory bias in ASD subjects. An ASD subject’s sensory percept is determined by p<sub>common</sub>, and therefore differs significantly from controls. The compensatory bias implies an ability (whether conscious or not) by ASD subjects to compensate for this differing percept when responding in the experiment. A key implication of our discovery of this compensatory bias is the fact that the data from explicit tasks cannot be taken at “face value” since they are affected by any compensatory strategy, while the data from implicit tasks doesn’t suffer from this shortcoming.</p><disp-quote content-type="editor-comment"><p>5. The participants in each experiment were not clearly introduced. The authors should provide more details about the task completion of participants, such as how many completed all four tasks, etc. And the data of how many participants who participated in both the implicit and explicit spatial task were included in modeling?</p></disp-quote><p>We have included Supplement File 1 (controls) and Supplement File 2 (ASD) detailing in which experiment or experiments did each subject take part in (see above). Subjects were included in the modeling if they had participated in Experiment 1 (and thus we had an estimate of their sensory encoding) in addition to the particular task of interest. That is, for Figure 4F, we included all participants taking part in Experiments 1 and 2. This included participants deemed poor in Experiment 1, given our attempt to account for participant’s behavior with the causal inference model. For Figure 4G, we included all participants taking part in Experiment 1 and 3.</p><p>In addition to Supplement File 1 and Supplement File 2, we have also amended the text (P9, “In a second step […] individual subject behavior.”) to specify the inclusion criteria for the modeling. Similarly, we have amended the caption of Figure 4 to explicitly state which participants were included in the single subject modeling (P13, “Subjects were included […] in Experiment 1 and 3”).</p><disp-quote content-type="editor-comment"><p>6. The authors could also conduct some correlational analyses between estimated model parameters and symptomatology measures, just as what they have done for psychometric features, to further investigate how autistic symptoms would affect the process of causal inference.</p></disp-quote><p>We thank the reviewer for this important suggestion. We attempted correlating the estimated p<sub>common</sub> and p<sub>combined</sub> for each subject with their AQ and SCQ measures. None of these correlations (4 in total) was significant. We have amended the text to include this information (P10, “Individual subjects’ p<sub>common</sub> and p<sub>combined</sub> as estimated by the model did not correlate with ASD symptomatology, as measured by the AQ and SCQ (all p &gt; 0.17)”).</p><disp-quote content-type="editor-comment"><p>7. Since the data of the individuals with poor performance were also fitted (such as 8 of the individuals with ASD in Experiment 3), it is interesting to see if there is anything special or atypical in terms of their model parameters, even though their data were not included in behavioral analyses.</p></disp-quote><p>We thank the reviewer for this suggestion and agree this is interesting and important information. We looked at the parameters for the ASD subjects who had performed both Experiments 1 and 3, as shown in Figure 4G and Supplement File 2. We z-scored each parameter and looked for outliers, as well as for systematic differences between the ‘good’ (black) and ‘poor’ (red) ASD performers. We classified the parameters as outliers if their absolute z-score exceeded 2. We observed a few outliers (both “good” and “poor” performers) in a parameter or two, but no systematic differences between these sub-groups (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). We have added this information in the text (P10, “Exploration of the model parameters […] causal inference parameters”).</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Z-score of all parameters for all ASD subjects, both included in the main text (black) and not (poor performers, in red).</title><p>Dashed lines are +/- 2 standard deviations. There are a few outliers, both poor (red) and good (black) performers, but overall there is no categorical difference between sub-groups.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-71866-sa2-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>8. I suggest specifying the criteria for the ASD diagnosis, DSM-5? or DSM-4? or ICD-10? Are they classic autism or Asperger or PDD-NOS? Were the gold standard ADOS ADIR performed to confirm the diagnosis? If not, the authors should acknowledge this as the limitation in Discussion.</p></disp-quote><p>This has been addressed in Question 7 of the “Essential Revisions.”</p><disp-quote content-type="editor-comment"><p>9. SCQ and AQ were performed to all participants. My question is: is there any ASD individuals below the cut-off of these two scales? or any TD participants above the cut-off. the authors should consider excluding the ASD individuals below the cut-offs and TD individuals above the cut-offs from the data analysis.</p></disp-quote><p>This has been addressed in Question 8 of the “Essential Revisions.”</p><disp-quote content-type="editor-comment"><p>10. Please provide more details about how the TD participants were recruited?</p></disp-quote><p>We have now included this information (P13, “These subjects were recruited by flyers posted throughout Houston”). Importantly, we must note that we did screen for and exclude siblings of individuals with ASD.</p><disp-quote content-type="editor-comment"><p>11. IQ was available for a subset of the ASD participants: How many of them have IQ scores? Is there any particular reason that the other ASD participants did not have IQ scores? How the IQ was measured? using Wechesler or Raven's test? Was the IQ measured for the TD group?</p></disp-quote><p>This has been addressed in Question 8 of the “Essential Revisions.”</p><disp-quote content-type="editor-comment"><p>12. The authors could provide direct comparisons of thresholds and visual weights between two groups in the result section of Experiment 1.</p></disp-quote><p>We have expanded the Results section of Experiment 1 to more explicitly make the comparisons suggested by the reviewer (P4, “Overall, subjects with ASD […] with visual thresholds being equal in control and ASD across all reliability levels” and P4, “Measured visual weights were also not different between groups at any reliability (F(2, 114) = 1.11, p = 0.33)”).</p><disp-quote content-type="editor-comment"><p>13. Errors bars in Figure 1E and 1H were not very obvious. The authors could consider using simpler markers, such as &quot;+&quot; (i.e., short lines) for simultaneously displaying horizontal and vertical error bars.</p></disp-quote><p>We thank the reviewer for this suggestion. These figures have been modified, displaying simultaneously horizontal and vertical error bars. For these to be visible, instead of plotting standard error of the mean (SEM), we now plot 95% Confidence Intervals (CIs). We have also rendered the individual subject data (scatter plot) transparent, as to emphasize the error bars. The figure caption has been modified to reflect the change in illustration.</p><disp-quote content-type="editor-comment"><p>14. It should be &quot;As for the case of auditory disparities, …&quot; instead of &quot; As for the case of spatial disparities, …&quot; for the first sentence of the second paragraph after Figure 3.</p></disp-quote><p>This paragraph describes the explicit common cause reports during temporal disparities. The sentence highlighted by the reviewer is attempting to convey that we performed the same analyses as for spatial disparities (above). To reduce the chance of misunderstandings we now start this sentence “Analogous to the case of spatial disparities…”(P9).</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The paper consists of 4 interesting experiments examining multisensory processing in autism spectrum disorder. The first experiment shows that participants with ASD perform similar to controls in cross-model integration, a conceptual replication of earlier findings from this group. However, the subsequent experiments reveal some intriguing differences between the groups in terms of how they use explicit and implicit information in evaluating if auditory and visual information comes from a common source or distinct sources. The authors propose a model that aims to explain the seeming dissociation between explicit and implicit reports of the two groups. The strength of this work is that the experiments are very interesting and report interesting results regarding audio-visual integration for spatial discriminations in both typical individuals and people with ASD. The comparison between explicit and implicit reports is very interesting. In terms of weaknesses, the dissociation between explicit and implicit is not convincing, and the stress on group differences puts an emphasis on, at best, marginal effects, which the modelling does not explain. For example, an alternative account that is consistent with all the data presented is that there are individuals with ASD who are somewhat poorer auditory discriminators, resulting in the bias effects and broader disparities. These individuals would be less likely to commit to an explicit &quot;single source&quot; statement in line with their reduced auditory localization skills.</p><p>The dissociation between explicit and implicit is not convincing, and the stress on group differences puts an emphasis on, at best, marginal effects, which the modelling does not explain (the strongest linearity on ASD's curve in Figure 2 – is not captured in the modelling in Figure 4) For example, an alternative account that is consistent with all the data presented is that there are individuals with ASD who are somewhat poorer auditory discriminators and they impacted overall performance in Experiment 2, resulting in a larger bias effect, and also somewhat broader in disparities. These individuals would be less likely to commit to an explicit &quot;single source&quot; statement, which is quite committing, in line with their reduced auditory localization skills. The authors should at least address this alternative account, and present auditory discrimination curves of Experiment 2's participants.</p></disp-quote><p>We thank the reviewer for this very interesting set of comments and for proposing an alternative account.</p><p>In Figure S1 (novel addition during the revision) we plot the visual and auditory psychometric functions for all participants in Experiment 2 that also participated in Experiment 1 (panel A). 80% of participants in Experiment 2 also participated in Experiment 1, and we have confirmed that eliminating the 20% of subjects who did not participate in Experiment 1 does not change the conceptual results from Experiment 2. More importantly, and directly addressing the reviewer’s alternative explanation, as shown in panels B, C, and D, the thresholds, biases, and r-squared values were no different between the ASD and control cohorts. In turn, we can rule out the reviewer's alternative account. In addition to adding Figure 2 – supplement figure 1 in supplementary materials, we have modified the main text to reflect this analysis (P6, “To confirm that the larger […] across visual and auditory modalities, and for all reliabilities”).</p><p>The reviewer also suggests that the difference in causal inference between ASD and control subjects may be marginal. To ascertain whether the effect reported (i.e., individuals with ASD showing anomalous causal inference) is a robust one, we performed a conceptual replication and extension, as detailed in reply to Question #2 of the “Essential Revisions.” The results were replicated, suggesting that individuals with ASD outweigh integration relative to segregation when performing causal inference independent of sensory domain (see Figure 2 – supplement figure 2).</p><p>On the modeling front, we acknowledge that for the aggregate subject, the model is not able to completely capture all aspects of the data. However, our goal with the aggregate subject fitting was to understand what parameters could explain the overall difference between ASD and controls without losing interpretability (for example, we know from Experiment 1 that their sensory uncertainty is not different at a population level). Thus, we constrained the model such that only the choice and the causal inference parameters were allowed to vary. Further, it is not surprising that the aggregate data deviates somewhat from a causal inference model, since we combined the data from multiple subjects (with their own idiosyncrasies) in a largely model-agnostic way so as not to bias our results toward the causal inference model. On the other hand, the single subject fits are good, as we highlight in the reply to Question #2 of the “Essential Revisions” and in Figure 4 – supplement figures 3-6. Further, and most importantly, we have now also tested models where the choice and sensory uncertainty parameters are free to vary, while keeping p<sub>common</sub> fixed (Alternatives A and B in Question #2, above, as well as <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). These models perform worse in accounting for implicit and explicit causal inference in ASD than the model where sensory uncertainty is fixed and p<sub>common</sub> is allowed to vary, as quantified by AIC (see Figure 4 – supplement figure 7 – where C is fixed to 1 or 2, and Figure 4 – supplement figure 2, where C is fixed to the value obtained in control subjects).</p><disp-quote content-type="editor-comment"><p>The model does not account for the data point of individuals with autism being pulled by a reliable visual blob 24 degrees away, which was the main point in Figure 3.</p></disp-quote><p>We acknowledge that the aggregate subject model does not account for the data points at extreme disparities, and attribute this to two reasons.</p><p>First, the aggregate model for the ASD subjects is very constrained such that only the choice and causal inference parameters are allowed to vary relative to the control. Further, it is fit to all three visual reliabilities simultaneously. We used such a constraint since we wanted to drive the intuition about what parameters may drive the effect between ASD and control subjects. We show (see above, reply to Question #2 in “Essential Revisions”) that by removing this constraint we can capture the aggregate data better (Figure 4 – supplement figure 12, 13), but this leads to a loss of interpretability (parameters trading-off, and differences in sensory uncertainty not being supported by the empirical observations).</p><p>Second, the inability to capture the leftmost point arises from an asymmetry in the bias between the right-most and left-most disparities. This is due to combining data across subjects and is not explainable by a causal inference model (whose purview is the individual subject). In the original submission we summarized the quality of the individual subject fits using explainable variance explained (EVE). In Figure 4E you can see that the individual fits are in fact very good, explaining 80% of the EVE. In this revision we have now also included illustrations of the single subject fits (Figure 4 – supplement 3-6). Again, we must highlight that these fits are across all tasks and reliabilities, and not fine-tuned to account for responses in a single experiment.</p><p>Overall, we point out that our model for the ASD subjects deviates from the data in the direction of controls, i.e., is conservative. The raw data suggests that we are more likely to be underestimating than overestimating p<sub>common</sub> for ASD subjects, and hence the difference to controls, further strengthening our overall conclusion.</p><disp-quote content-type="editor-comment"><p>Overall the authors ignore more prominent aspects of the data (e.g. higher overall bias in autism in Figure 2) for points they want to make (non linearity larger in autism than in controls).</p></disp-quote><p>We thank the reviewer for this comment and have modified the text to more clearly emphasize the overall differences in bias (P6, “Overall, individuals with ASD showed a larger bias (i.e., absolute value of the mean of the cumulative Gaussian fit) in auditory localization than the control group (see Figure 2A and Figure 2B, respectively, for control and ASD cohorts; F(1, 34) = 5.44, p = 0.002)”.<italic>)</italic></p><p>However, we must highlight that the differences in bias exist at particular cue disparities (see Figure 2 and Figure 2 – supplement figure 1 and 2) and our goal here was to attribute well known differences in multisensory behavior (i.e., biases) to an underlying computation. Systematic biases occur when observers operate under an incorrect internal model. Thus, there being differences in biases is expected (at particular cue disparities) under causal inference. Further, notice that there is no difference in bias when no internal model is required, as in Experiment 1 (Figure 1 and Figure 2 – supplement figure 1 and Figure 3 – supplement figure 1). We do not ignore aspects of the data to make points we want to make, but instead focus on elements of the data that inform our understanding of the underlying computation (the biases being due to using incorrect internal models).</p><disp-quote content-type="editor-comment"><p>Reliability – is a confusing term. The stimuli are reliably presented, but the information the perceivers derive regarding their position is less reliable when stimuli are small.</p></disp-quote><p>We thank the reviewer for highlighting that “reliability” can be a confusing term. We now use this term to refer to the reliability of the information in the stimulus, or the reliability of the visual or auditory cue to avoid potential misunderstandings.</p><disp-quote content-type="editor-comment"><p>Figure 1f, g – I had difficulties understanding. I assume that the dashed lines should be to the right of the solid lines, which is the case for &quot;high-reliability&quot; blob, but why is it switched for the low reliability case? In both sample participants (f and g) and I wonder why the bias is larger (larger distance between dashed and matched solid plot, in both participants) for low versus intermediate size (reliability) blobs. If this is the actual result – it needs explanation.</p></disp-quote><p>We apologize, this was not clear in the text. Auditory thresholds were equal to visual thresholds at the intermediary reliability (shown in Figure 1D). At the high-reliability setting, visual thresholds were smaller than auditory ones. And in the low-reliability condition, visual thresholds were higher than auditory (Figure 1D). Thus, if participants are integrating cues in line with optimal cue combination, in the case of visual stimuli being highly reliable, participants' reports should be ‘pulled’ by visual location. Instead, in the low reliability condition, participants’ reports should be ‘pulled’ by the auditory location, given that it’s the most reliable one. At the intermediary reliability, both cues should influence the final report about equally. The example participants depicted in Figure 1F and 1G behave as predicted. The x-axis in the original version of the manuscript “stimulus location” was a misnomer, and likely the origin of the confusion. We apologize, this should have been “mean stimulus location” (given that it takes into account the relative location of the auditory and visual stimulus). Thus, as expected, when the reliabilities of the stimuli match (i.e., intermediary visual reliability), the dashed and solid line should be at the same location, and their slope should be maximal close to when the mean stimulus location (x-axis) is equal to zero. Instead, when visual reliability is high, the dashed curve should be to the right of the solid curve (indicating visual capture). When visual reliability is low, the dashed curve should be to the left of the solid curve (indicating auditory capture).</p><p>We have corrected the label of the x-axis in Figure 1F and 1G, and modified the text for clarity.</p><disp-quote content-type="editor-comment"><p>Figure 2 – the main observation is that the bias in autism is larger. Perhaps this group difference stems from this group being somewhat poorer auditory spatial discriminators than their 15 age-matched controls in the experiment. If their auditory discrimination is poorer we would expect an overall larger bias, and perhaps also across a broader range of audio-visual disparities.</p></disp-quote><p>There was no difference in visual or auditory discrimination performance among the subjects in Experiment 2, see Figure 2 – supplementary figure 1 and reply to comments above.</p><disp-quote content-type="editor-comment"><p>Importantly, this is probable account, since this is a smaller population than in Experiment 1 – and their discrimination thresholds are not addressed. Importantly – I could not figure out the overlap in participation across the various experiments. In experiment 1 matched performance was only obtained when 6 participants with ASD were excluded. In Experiment 3 (24 participants originally) – they also excluded a large subgroup, whose behavior was different. Here the group is initially small so variability across participants was not discussed.</p></disp-quote><p>We apologize for not providing this information in the initial manuscript. We have now added Supplementary File 1 (controls) and Supplementary File 2 (individuals with ASD) to detail which participants took part in the different experiments (see response to Reviewer #1 and “Essential Revisions”). As mentioned above, there was no difference in unisensory performance among participants taking part in Experiment 2.</p><disp-quote content-type="editor-comment"><p>The strongest point for the claim of too broad integration is the bottom left point – where high reliability blob has an effect that even increases when the visual blob is presented 24 degrees apart. This point is hard to reconcile (and is not reconciled by the model proposed in Figure 4 either). The authors should show that it is a reliable data point – perhaps by showing single subject data.</p></disp-quote><p>First, we would like to point out that while we agree that this one data point is particularly compelling in a qualitative way, our conclusions would hold even in its absence. Also, as we have addressed above, individual fits are good (see Figure 4E) and we have now included example single subject data; 2 per experimental cohort (Figure 4 – supplementary figures 3-6). The aggregate data fits are (1) very constrained (fitting to multiple tasks and reliabilities while solely varying choice and inference parameters) and (2) could be improved at the expense of losing interpretability (see above). Most importantly (3) the quality of the aggregate data fits speaks to inter-subject heterogeneity, and not the ability of the causal inference model to account for individual responses (which is quantified in Figure 4E and illustrated in Figure 4 – supplementary figures 3-6). The point to the lower left, 24 degrees in Figure 2B, is reliable, as shown by the S.E.M.</p><p>In addition to the individual subject fits, we have now also included a completely new experiment (heading discrimination during object-motion) in the supplementary materials (Figure 2 – supplementary figure 2), demonstrating again an impairment of causal inference in ASD (see replies to “Essential Revisions” above).</p><disp-quote content-type="editor-comment"><p>In experiments 3 and 4 the specific instructions are crucial – are participants asked to press a specific button if they are perceived as coming from the same source? Or press a button if they are perceived as coming from 2 separate sources. Here phrasing may have affected the decisions of individuals with autism. In order to dissociate between these 2 options it would have been nice to have a third option &quot;don't know&quot;. If participants with autism tend to say to be less decisive they would tend to commit to a single source. This account may be explained by being somewhat implicitly poorer localizers.</p></disp-quote><p>We thank the reviewer for this question and agree that in the future it may be interesting to allow for a third – non-committal – option. We have added to the text the explicit instructions that were given (see reply to Question #5 in the “Essential Revisions”). We do not believe that the specific phrasing drove the explicit effects we report, given that the phrasing was different for the spatial (Experiment 3) and temporal task (Experiment 4), while their reduced tendency to report common cause was shared across experiments. Further, if the phrasing does play a strong role (something we cannot be sure of in this experiment), and a stronger role in ASD relative to controls, this would simply provide an alternative explanation for the categorical bias that we found in the explicit task (i.e., a compensatory mechanism), while leaving our conclusions about the differences in p<sub>common</sub> unchanged. It would therefore add and not detract or oppose the current findings.</p><disp-quote content-type="editor-comment"><p>If you have discrimination functions of the specific subgroups that took part in Experiments 2-3 (since they all participated in Experiment 1 – right?) – please show them or report discrimination skills for these subgroups, since this is the relevant control-ASD matching.</p></disp-quote><p>We thank the reviewer for highlighting that this important control was missing. The discrimination functions for the subgroup participating in Experiment 2 are including as Figure 2 – supplementary figure 1, while these functions for the subgroup participating in Experiment 3 are included as Figure 3 – supplementary figure 1. The cohort of control and ASD participants taking part in Experiments 2 and 3 were no different from each other with regard to visual or auditory localization performance (as is also true of Experiment 1). This important information has been added to the text (P6, “To confirm that the larger biases […] and for all reliabilities”. And, P7, “See Figure S3 for the unisensory discrimination performance […] in this explicit causal inference judgment experiment”).</p><disp-quote content-type="editor-comment"><p>Re modelling and Figure 4 – It is difficult to follow the model – perhaps label the model parameters in the diagram of Figure 4a.</p></disp-quote><p>We thank the reviewer for their suggestion. We have updated the model figure to increase clarity. We have separated the generative models for the implicit and explicit task, and have included the parameter associated with each step of the generative model.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>In this paper Noel et al., use a combination of psychophysical experiment and computational modeling to examine the differences in behaviour between participant on the Autism Spectrum Disorder and control participants when dealing with multi-sensory stimuli (e.g. audio-visual). It is well known that ASD subjects tend to differ in how they combine such stimuli, and it has previously been suggested that this may be due to a difference in the tendency to perform causal inference.</p><p>The study indeed finds that while ASD participants had similar ability to combine cues when unambiguously from the same source, they differed in the tendency to combine them when unclear if necessary to combine. In contrast when asked to explicitly indicate whether stimuli originated from the same source (and therefore should be combined) they tended to under report.</p><p>While the experiments are in themselves very standard, the paper relies on computational modeling to differentiate the possible behavioural effects, using advanced Bayesian statistical methods.</p><p>These results confirm existing ideas, and build on our understanding of ASD, while still leaving many questions unanswered. The results should be of interest to anyone studying ASD as well as any other developmental disorders, and perception in general.</p><p>I enjoyed reading this paper, although the model fitting procedure especially was not clear to me. How were the sensory parameters fitted? By my count more than 20 parameters were fitted (Supp. File 1) for the aggregate subject through the slice sampling, is that correct? Was this also done for individual subjects? I would be nervous about fitting that many parameters for individual subject data. What was done to ensure convergence?</p></disp-quote><p>We thank the reviewer for his question. First of all, we want to emphasize that we perform full Bayesian inference over all model parameters given the empirical data. What this means is that we are computing a joint distribution over all parameters that captures all of our knowledge about these parameters given the subject responses (we do this by slice sampling but that is just a technical detail). Importantly, if some parameters are not constrained by our data, then the posteriors over them will be very wide, in the extreme case simply corresponding to the prior distributions that reflect our knowledge about them in the absence of any new data. Or if there is a degeneracy such that e.g., only the sum of two parameters is constrained, but not each of them individually, then this will also manifest itself in very wide individual error bars. Importantly, when reporting our estimates and confidence intervals about the parameters that we do care about, e.g. p<sub>common</sub>, we account for the uncertainty in all the other parameters.</p><p>Regarding the number of parameters, yes, each subject had 20 parameters characterizing their responses. However, this was across four tasks (unisensory and small disparities discrimination, multisensory discrimination, implicit causal inference, and explicit causal inference) and three visual reliability levels. Further, the prior over the parameters reduces the effective degrees of freedom, such that several parameters have their values tied together (unless the observed experimental data provides evidence to the contrary). We have chosen this approach to allow for variance in parameters (e.g., lapse parameters) across experiments that were conducted on different days, but a-priori assuming they are the same. Therefore, for the aggregate subject, when we fit the control data, we are effectively fitting 12 parameters for the implicit task and 11 parameters for the explicit task (subset relevant for each task). Also, since the model was fit to multiple reliabilities, the prior parameters that were shared across reliabilities were constrained by data from all three reliabilities. For the ASD aggregate subject, either only the choice parameters were varied, or the choice parameters and p<sub>common</sub> were varied (relative to the aggregate control subject). We followed a similar procedure for the fit to individual subjects, but additionally used subject specific data from the different experiments to constrain parameters (notably Experiment 1 and estimates of sensory uncertainty).</p><p>We have now summarized the model fitting procedure as a flowchart in the supplementary (Figure 4 – supplementary figure 1).</p><disp-quote content-type="editor-comment"><p>Was any model comparison done? Might be better to include a list or figure showing the different steps of the model fitting.</p></disp-quote><p>In this revision we have considered a number of different models that could explain the difference between the aggregate control and ASD subject. These alternative models are:</p><p>A. Forced fusion (all parameters are free, except for C, which is fixed to 1).</p><p>B. Forced segregation (all parameters are free, except for C, which is fixed to 2).</p><p>C. Lapse rate, p<sub>common</sub>, p<sub>choice</sub> are free with uniform lapse bias.</p><p>D. D1) Implicit task: Lapse rate and bias and p<sub>choice</sub> are free</p><p>D2) Explicit task: Since p<sub>choice</sub> trades off against p<sub>common</sub> for the explicit task only lapse rate and bias are free</p><p>We quantify the goodness of fit by AIC and contrast these alternative models to that presented in the main text (Figure 4 – supplementary figure 7). These models all perform worse than the one where p<sub>common</sub>, but not sensory uncertainties are allowed to vary across the control and ASD cohorts. See reply to Question #1 of the “Essential Revisions” and the main text (P9, 10) for further detail.</p><disp-quote content-type="editor-comment"><p>I also worry that the model is over specified with both a lapse rate and a lapse bias. From my understanding the lapse rate specifies when subjects (through lack of concentration or otherwise) fail to take trial stimuli into account and therefore go with their prior. In other studies this prior may be identical to the prior over spatial range, or may be a uniform discrete distribution over the bottoms available for response.</p><p>Maybe the variables are constrained in ways that I did not understand, but with just a binary response (Left/Right) the model can largely incorporate any bias to a large set of possible parameter values of lapse rate and bias. I.e. that the model is over specified. That would also explain the wide range of values for the fitted parameters in Figure 3.</p><p>I think this should really be investigated before the results can be trusted.</p><p>Looking at Figure 4E and F makes me hesitant about trusting the results.</p><p>Authors also acknowledge that the lapse bias and P combined are too closely entwined to really be well separated in the explicit temporal experiment. Maybe for that reason it would also be useful to test a simpler model without lapse bias?</p></disp-quote><p>Our prior over the lapse bias (i.e., the bias in the response given a lapse) peaks at 0.5. Thus, the model implicitly assumes no lapse bias, unless supported by the data. To further confirm that the empirical results do support the presence of a lapse bias (and thus require it in modeling), we have now fit a model with a uniform lapse bias for the aggregate subject (Alternative C in Response #1 to “Essential Revisions”). This model had a worse AIC than that allowing for a lapse bias (see Figure 4 – supplementary figure 7), suggesting the data supports the presence of a lapse bias. Regardless, the observation that individuals with ASD have a larger p<sub>common</sub> than controls holds for both models with and without a lapse bias.</p><p>Similarly, we obtained better model fits to individual subject data while allowing for the possibility of lapse biases. Only 6 subjects had a smaller AIC for the model without (vs. with) lapse bias. For those subjects, the estimated p<sub>common</sub> was not statistically different from the model with the lapse bias (p=0.68). Therefore, while using a model without a lapse bias does not change our conceptual results regarding p<sub>common</sub>, we believe that incorporating a lapse bias and then marginalizing out any potential contribution from it allows us to better estimate the actual contribution of the variables of interest.</p><p>We understand the reviewer’s concern that the model may be over-specified, particularly when fitting to a limited dataset (i.e., individual subjects). However, as described above, this is not a problem since we perform full Bayesian inference over all model parameters while fitting data from 4 tasks and 3 different reliabilities, simultaneously. Further, any under-constraint in the model would manifest as correlated posteriors, and large uncertainties in the parameter estimates. While the heterogeneity across subjects in Figure 4F and G is high, the confidence intervals over the subject specific parameters (the error bars around individual dots) is not, indicating that this subject-to-subject variability reflects actual differences between people.</p><disp-quote content-type="editor-comment"><p>I find it mildly confusing that D refers to a Left/Right response in the implicit task, and Common/Separate in the explicit task. Maybe better to use separate symbols? D is fine for 'decision' but in places in the text it is instead referred to as 'trial category' which is vague. I also don't really think D is needed in the generative model in Figure 4 as it is not really causing the subsequent variables C or Sa.</p></disp-quote><p>We want to clarify that D is not the response but the category of the trial. We model the subject as using a common perceptual framework across tasks: observers generate beliefs about the locations (s) that generated their observations (o). This belief is related to a belief over the trial category (D) that generated the observations in the trial. The response, which we refer to as R, is generally whichever belief is greatest (excluding lapses). This response minimizes the expected loss according to Bayesian decision theory. The reviewer is right that the response does not fit into the generative model, but the trial category does, since the experimenter has to infer the trial category in every trial.</p><disp-quote content-type="editor-comment"><p>Does eLife not require the reporting of effect sizes (e.g. eta<sup>2</sup> or Cohen's d)? It would be good to include these.</p></disp-quote><p>We thank the reviewer for this suggestion. Effect sizes have now been added throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>The plots in Figure 3 mostly look like shifts up for ASD relative to controls. The authors might want to fit a model with a positive bias, i.e.</p><p>a*N(mu,sd<sup>2</sup>)+b</p><p>may fit better (could do model comparison) and just show difference in b. This is just a suggestion though, but it may be cleaner for their argument.</p></disp-quote><p>We thank the reviewer for his suggestion and have attempted this modelling approach. We have added a new supplementary figure (Figure 3 – supplementary figure 3), showing that this model performs worse than the causal inference in terms of AIC. We have amended the text to reference this attempt (P22, Lastly, as a contrast to […] than the functional form).</p><disp-quote content-type="editor-comment"><p>In the Discussion, while divisive normalisation is one way to achieve the marginalisation needed for Bayesian causal inference, there are other ways to achieve it (Cuppino et al., 2017, Yamashita 2013, Yu et al., 2016, Zhang et al., 2019). It would be good to acknowledge this.</p></disp-quote><p>The reviewer is entirely correct and pointing toward prior work implementing neural networks of causal inference is extremely relevant. We have reviewed the reports cited by the reviewer; two of them make explicit reference to normalization in their modeling efforts, while the others do not (Zhang et al., 2019, for example, relying on the finding of “congruent” and “incongruent” cells). We have amended the discussion to acknowledge that divisive normalization is only one of many possible ways of achieve marginalization (P12, “The juxtaposition between […] Yamashita et al., 2013; Yu et al., 2016).</p><disp-quote content-type="editor-comment"><p>Equation 5 and 6, 38 are misleading. Likelihood is a function of Sa/Sv, so would be better to write as l(Sa)=N(Xa;Sa,Sv)</p></disp-quote><p>We thank the reviewer for their suggestion. We have added the likelihood function definition as suggested by the reviewer.</p><disp-quote content-type="editor-comment"><p>Equation 9: is D either 1 or 2? Or 1 or -1?</p></disp-quote><p>D is 1 or -1 in the implicit task where it refers to the side on which the tone is inferred, and D is 1 or 2 in the explicit task, where it refers to the number of causes inferred for the observations. For clarity, we have now separated the two using D<sub>imp</sub> and D<sub>exp</sub> where D<sub>imp</sub> is -1 or 1 and D<sub>exp</sub> is 1 or 2.</p><disp-quote content-type="editor-comment"><p>Detail: maybe use different symbols for lapse rate and lapse bias? I find λ and lambdar confusing. How about P<sub>lapse</sub> for the lapse rate to emphasise that it is a probability? P<sub>common</sub> is already a fitted variable that is also a probability of a Bernoulli distribution</p></disp-quote><p>As suggested by the reviewer we have replaced λ and λ_r with p<sub>lapse rate</sub> and p<sub>lapse bias.</sub></p><disp-quote content-type="editor-comment"><p>Page 5 (pages of the pdf):</p><p>&quot; …ASD did not show impairments in integrating perceptually congruent auditory and visual stimuli.&quot;</p><p>– &quot; …ASD did not show impairments in integrating perceptually congruent (and near-congruent) auditory and visual stimuli.&quot;</p><p>In experiment 2 there was a six degree discrepancy, so near-congruent seems appropriate.</p></disp-quote><p>The text has been amended as suggested by the reviewer.</p><disp-quote content-type="editor-comment"><p>Typos:</p><p>&quot;We perform the integral in Equation S5 for the implicit task by&quot;: should this be Equation 35?</p></disp-quote><p>Indeed, in the original version of the manuscript this should have been Equation 35. In the current version of the manuscript we have moved this section to the supplementary materials.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>1. Figure legends/captions of Figures 3 and 4 in the main texts lack detailed descriptions of the elements in the figures. For example, for Figures 3 and 4, what do those error bars represent? Standard errors or confidence intervals? In Figure 4B, are solid lines the model predictions and hollow points the observations? I believe this essential information would help readers better understand the figures.</p></disp-quote><p>We thank the reviewer for highlighting that we had missed this important information. In Figure 3, indeed, error bars represent standard error of the mean (SEM). In Figure 4B and C, error bars are 68% confidence intervals. Similarly, in Figure 4F and G, the individual subject-level error bars are 68% confidence intervals (equivalent to standard deviations under Gaussianity). In Supplementary File 4, we elaborate on how we obtain confidence intervals for individual (or amalgamated) subjects:</p><p>“We also obtained full posteriors over model parameters for the individual subjects by jointly fitting the model to all experiments with weakly informative priors. We modeled each observer population with a hierarchical model where an individual observer’s parameters are independent draws from the population parameter, parameterized as a Gaussian, i.e. <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>subject</mml:mtext></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mtext>population</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>population</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are the parameters of the causal inference model. We also approximated the posterior over the subject parameters as Gaussians which allowed us to analytically combine the individual posteriors to a combined Gaussian population posterior for further hypothesis testing and obtaining confidence intervals (CI). For the individual subject estimates, we plot 68%CI. We obtained the p-value for the difference between ASD and control subjects parameters using two methods: (a) Normal approximated analytical p-value which assumes that the sample variance is equal to the population variance, (b) Welch t-test which relaxes the above assumption. Both methods gave comparable p-values and we conservatively considered the higher p-value for significance testing.”</p><p>In Figure 4F and G group-level error bars are +/- 95% confidence intervals and hence non-overlapping bars indicate a statistical significance at p &lt; 0.05. As the reviewer inferred, the “dots” in Figure 4B and C are data, and the lines are fits to this data. We have modified the figure captions (Figure 3 and 4) to include all this important information.</p><disp-quote content-type="editor-comment"><p>2. The data points in Figure 2A-B and Figure 3A-C are slightly different from those in Figure 4B-C. For example, in Figure 2B, the audio bias of 24 deg disparity is weaker than that of 12 deg disparity for the high visual reliability condition (dark brown lines and points); however, in Figure 4B left panel, the audio bias of 24 deg disparity is even larger than that of 12 deg disparity. I assume that the data points depicted in Figure 4B-C are the aggregate data for modeling, in which the data of some participants were not included? I notice that the authors have included which participants were included in the single-subject modeling, but was the aggregate data the same as what was used for plotting Figures 2 and 3? I find it a bit confusing at first sight, perhaps the author could check it again and/or mention the related information in the caption or the main text?</p></disp-quote><p>We thank the reviewer for highlighting that the difference in data between Figures 2/3 and Figure 4 can be confusing. The difference is due to the fact that while in Figures 2/3 data is averaged within subjects, then psychometric fits are performed at the single subject level (e.g., Figure 1B, C, F, G), and finally psychometric estimates are averaged across subjects, in Figure 4 data is amalgamated and averaged across all subjects, and a single psychometric fit is performed. We have added this information in the figure caption to Figure 4.</p><disp-quote content-type="editor-comment"><p>3. From lines 451-453 of merged files (Instead, differences between […] relative to control observers.), did the author imply that the model where p<sub>common</sub> was freely estimated from the data was better, compared with the model where p<sub>common</sub> was fixed (I guess it's the model in Figure 4 – supplement 2)? In other words, did the authors have two different models and conduct a model comparison here? If so, I think it's better to provide model comparison results. The question also applies to the texts from lines 460-461. Also, what is DAIC? Is it the difference of AIC between the full model (that allows p<sub>common</sub>) and the restricted model (that fixes p<sub>common</sub> to a constant)? The authors should describe it somewhere in the main text.</p></disp-quote><p>Indeed, DAIC stood for the difference in AIC between two models. We have changed this nomenclature to ∆AIC, given the use of “∆” for difference throughout the text and in Figures 4 – supplement 2, 7, 12, and 13.</p><p>The models we compared in the main text were:</p><p>– For the implicit task, a model where only choice parameters (choice bias + lapse rate + lapse bias) were free to vary vs. a model where both choice and inference (p<sub>common</sub>) were free to vary.</p><p>– For the explicit task, a model where only lapses (rate and bias) were free to vary (given the impossibility to distinguish the choice bias from p<sub>common</sub>) vs. a model where both lapses and “p<sub>combined</sub>” were free to vary.</p><p>This is indeed close to what we present in Figure 4 – supplement 2, with the exception that we additionally add the sensory uncertainty in the supplement (which is not added in the main text given the empirical results demonstrating no difference between groups).</p><p>We have modified the text in the following manner to make this clearer:</p><p>“In the implicit task (Figure 4B, top panel), allowing only for a difference in the choice parameters (lapse rate, bias, and p<sub>choice</sub>; magenta) between the control and ASD cohorts, could only partially account for observed differences between these groups (explainable variance explained, E.V.E = 0.91, see Supplementary File 4). Instead, differences between the control and ASD data could be better explained if the prior probability of combining cues, p<sub>common</sub>, was also significantly higher for ASD relative to control observers (Figure 4D, p = 4.5x10<sup>-7</sup>, E.V.E = 0.97, ∆AIC between model varying only choice parameters vs. choice and inference parameters = 1x10<sup>3</sup>). This suggests the necessity to include p<sub>common</sub> as a factor globally differentiating between the neurotypical and ASD cohort.”</p><p>And:</p><p>“For the explicit task, different lapse rates and biases between ASD and controls could also not explain their differing reports (as for the implicit task; EVE = 0.17). Differently from the implicit task, however, we cannot dissociate the prior probability of combination (i.e., p<sub>common</sub>) and choice biases, given that the report is on common cause (Figure 4A, see Methods and Supplementary File 4 for additional detail). Thus, we call the joint choice and inference parameter p<sub>combined</sub> (this one being a joint p<sub>common</sub> and p<sub>choice</sub>). Allowing for a lower p<sub>combined</sub> in ASD could better explain the observed differences between ASD and control explicit reports (Figure 4C; EVE = 0.69, ∆AIC relative to a model solely varying lapse rate and bias = 1.3x10<sup>3</sup>). This is illustrated for the ASD aggregate subject relative to the aggregate control subject in Figure 4D (p = 1.8x10<sup>-4</sup>)”</p><disp-quote content-type="editor-comment"><p>4. The authors should be more specific about the tests they used to compare model parameters between groups and those correlational analyses. What type of tests did the authors use, parametric (i.e., Welch t-test, Pearson correlation) or non-parametric (i.e., Mann-Whitney, Spearman correlation, or permutation methods)? Particularly for the comparison of p<sub>combined</sub> (Figure 4G), would the result be different when a non-parametric test was used if the test used in the current revision was parametric? I suggest the authors take more robust approaches given that the distributions of the model parameters seemed not quite Gaussian.</p></disp-quote><p>Regarding Figure 4G, as we indicate in Supplementary File 4, we conducted both parametric and non- parametric t-tests. We then conservatively considered the higher p-value. The relevant piece of text is:</p><p>We obtained the p-value for the difference between ASD and control subjects parameters using two methods: (a) Normal approximated analytical p-value which assumes that the sample variance is equal to the population variance, (b) Welch t-test which relaxes the above assumption. Both methods gave comparable p-values and we conservatively considered the higher p-value for significance testing.</p><p>For the correlations, we performed Type II regression (indicated in the text). This approach appropriately considers that both measures being correlated are noisy estimates, and thus each of the two variables regressed are first transformed to have a mean of zero and a standard deviation of one (Ricker, 1973).</p><disp-quote content-type="editor-comment"><p>5. What is α and ν in Equation 5 and 6, please define them in the text. Also, it would be better if the authors give a short introduction to the meaning of lapse rate, lapse bias, etc., when mentioning them for the first time. Given that many readers are not very familiar with computational modeling, they may not intuitively understand what these parameters represent.</p></disp-quote><p>We have modified the text in order to introduce α and ν, as well as give a short introduction to the meaning of lapse rate, lapse bias, and prior. The text has been modified in the following manner:</p><p>We assume that subjects have a good estimate of their sensory uncertainties (over lifelong learning) and hence the subject’s estimated likelihoods become,</p><p><inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow/><mml:mo>|</mml:mo><mml:mrow/><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo mathvariant="script">=</mml:mo><mml:mtext mathvariant="script"> </mml:mtext><mml:mi mathvariant="script">N</mml:mi><mml:mo mathvariant="script" stretchy="false">(</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>(Equation 5)</p><p><inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow/><mml:mo>|</mml:mo><mml:mrow/><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo mathvariant="script">=</mml:mo><mml:mtext mathvariant="script"> </mml:mtext><mml:mi mathvariant="script">N</mml:mi><mml:mo mathvariant="script" stretchy="false">(</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>(Equation 6)</p><p>where <inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the inferred location of auditory and visual stimuli.</p><p>And:</p><p>First, sensory parameters: the visual and auditory sensory uncertainty (i.e., inverse of reliability), as well as visual and auditory priors (i.e., expectations) over the perceived auditory and visual locations (mean and variance of Gaussian priors). Second, choice parameters: choice bias (p<sub>choice</sub>), as well as lapse rate and bias. These latter two parameters are the frequency with which an observer may make a choice independent of the sensory evidence (lapse rate) and whether these stimuli-independent judgments are biased (lapse bias). Third, inference parameters: the prior probability of combination (p<sub>common</sub>; see Methods and Supplementary Files 3 and 4 for further detail).</p><disp-quote content-type="editor-comment"><p>6. The D in DAIC from line 462 is in another font.</p></disp-quote><p>We thank the reviewer for noticing this typo. As indicated above, we have changed this nomenclature to “∆AIC”.</p><disp-quote content-type="editor-comment"><p>7. I apologize in advance if it's my mistake but I failed to find Supplementary Text 1 mentioned in lines 430, 451, and 459. Where could I find it?</p></disp-quote><p>This is our mistake, we apologize. Supplementary Text 1 is now Supplementary File 4.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>The authors have adequately addressed my comments.</p><p>The strong aspects of the results are better clarified, and the overlap between participants across experiments is also clear. Further, the authors do not make claims that are not directly supported experimentally.</p><p>The limitation of a somewhat small (&lt;20) number of participants per group in important experiments is still a drawback, given participants' variability, particularly in the ASD group. Yet, I believe that the main results hold.</p></disp-quote><p>We thank the reviewer for helping us strengthen and clarify the results in this manuscript. We agree that within which experiment, the sample sizes were of moderate size. We have amended the discussion to acknowledge this limitation:</p><p>However, it must be acknowledged that while the overall number of participants across all experiments was relatively large (91 subjects in total), our sample sizes within each experiment were moderate (~20 subjects per group and experiment), perhaps explaining the lack of any correlation.</p><disp-quote content-type="editor-comment"><p>The strongest aspects of the study are the direct results, rather than the modelling:</p><p>Experiment 1: audio-visual integration is intact in ASD 2. yet multisensory behavior is atypical (in the current experimental protocol) – ASD participants tend to favor source integration, as manifested by their cross-modal bias in localization even when visual and auditory signal are separable from a sensory perspective. Though both groups tend to over integrate, this is more salient and tend to span a broader distance in ASD. 3. Explicit reports have an opposite tendency – individuals with ASD were less likely to report a common cause for the two stimuli. Given the adequate direct measures of ASD cue integration with a small audio-visual distance (performance in Experiment 1) these results suggest a specific atypicality in cause attribution.</p><p>I also find the difference between spatial and temporal integration very interesting. Temporal and spatial groups differences in explicit attribution of a common source merits some additional discussion.</p></disp-quote><p>We agree. We have expanded the discussion in the following manner:</p><p>“This has previously been observed within the temporal domain (Noel et al., 2018a, b), yet frequently multisensory simultaneity judgments are normalized to peak at ‘1’ (e.g., Woynaroski et al., 2013; Dunham et al., 2020), obfuscating this effect. To the best of our knowledge, the reduced tendency to explicitly report common cause across spatial disparities in ASD has not been previously reported. Further, it is interesting to note that while “temporal binding windows” were larger in ASD than control (see Feldman et al., 2018), “spatial binding windows” were smaller in ASD relative to control subjects. This pattern of results highlights that when studying explicit “binding windows”, it may not be sufficient to index temporal or spatial domains independently, but there could potentially be a trade-off.”</p></body></sub-article></article>