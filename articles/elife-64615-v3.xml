<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64615</article-id><article-id pub-id-type="doi">10.7554/eLife.64615</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Modelling the neural code in large populations of correlated neurons</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-213779"><name><surname>Sokoloski</surname><given-names>Sacha</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4166-1772</contrib-id><email>sacha.sokoloski@mailbox.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-216931"><name><surname>Aschner</surname><given-names>Amir</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-172989"><name><surname>Coen-Cagli</surname><given-names>Ruben</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2052-5894</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Systems and Computational Biology, Albert Einstein College of Medicine</institution><addr-line><named-content content-type="city">Bronx</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Institute for Ophthalmic Research, University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Dominick P. Purpura Department of Neuroscience, Albert Einstein College of Medicine</institution><addr-line><named-content content-type="city">Bronx</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name><role>Reviewing Editor</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>10</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e64615</elocation-id><history><date date-type="received" iso-8601-date="2020-11-05"><day>05</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-10-01"><day>01</day><month>10</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Sokoloski et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Sokoloski et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64615-v3.pdf"/><abstract><p>Neurons respond selectively to stimuli, and thereby define a code that associates stimuli with population response patterns. Certain correlations within population responses (noise correlations) significantly impact the information content of the code, especially in large populations. Understanding the neural code thus necessitates response models that quantify the coding properties of modelled populations, while fitting large-scale neural recordings and capturing noise correlations. In this paper, we propose a class of response model based on mixture models and exponential families. We show how to fit our models with expectation-maximization, and that they capture diverse variability and covariability in recordings of macaque primary visual cortex. We also show how they facilitate accurate Bayesian decoding, provide a closed-form expression for the Fisher information, and are compatible with theories of probabilistic population coding. Our framework could allow researchers to quantitatively validate the predictions of neural coding theories against both large-scale neural recordings and cognitive performance.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>primary visual cortex</kwd><kwd>neural coding</kwd><kwd>bayesian modelling</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY030578</award-id><principal-award-recipient><name><surname>Coen-Cagli</surname><given-names>Ruben</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY02826</award-id><principal-award-recipient><name><surname>Sokoloski</surname><given-names>Sacha</given-names></name><name><surname>Aschner</surname><given-names>Amir</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY016774</award-id><principal-award-recipient><name><surname>Aschner</surname><given-names>Amir</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The proposed techniques enable researchers to disentangle the statistical features of neural population responses, and rigorously quantify how these features carry information about stimuli and experimental variables.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A foundational idea in sensory neuroscience is that the activity of neural populations constitutes a ‘neural code’ for representing stimuli (<xref ref-type="bibr" rid="bib19">Dayan and Abbott, 2005</xref>; <xref ref-type="bibr" rid="bib20">Doya, 2007</xref>): the activity pattern of a population in response to a sensory stimulus encodes information about that stimulus, and downstream neurons decode, process, and re-encode this information in their own responses. Sequences of such neural populations implement the elementary functions that drive perception, cognition, and behaviour (<xref ref-type="bibr" rid="bib60">Pitkow and Angelaki, 2017</xref>). Therefore, by studying the encoding and decoding of population responses, researchers may investigate how information is processed along neural circuits, and how this processing influences perception and behaviour (<xref ref-type="bibr" rid="bib91">Wei and Stocker, 2015</xref>; <xref ref-type="bibr" rid="bib56">Panzeri et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Kriegeskorte and Douglas, 2018</xref>).</p><p>Given a true statistical model of how a neural population responds to (encodes information about) stimuli, Bayes’ rule can transform the encoding model into an optimal decoder of stimulus information (<xref ref-type="bibr" rid="bib97">Zemel et al., 1998</xref>; <xref ref-type="bibr" rid="bib59">Pillow et al., 2011</xref>). However, when validated as Bayesian decoders, statistical models of neural encoding are often outperformed by models trained to decode stimulus-information directly, indicating that the encoding models miss key statistics of the neural code (<xref ref-type="bibr" rid="bib28">Graf et al., 2011</xref>; <xref ref-type="bibr" rid="bib90">Walker et al., 2020</xref>). In particular, the correlations between neurons’ responses to repeated presentations of a given stimulus (noise correlations), and how these noise correlations are modulated by stimuli, can strongly impact coding in neural circuits (<xref ref-type="bibr" rid="bib99">Zohary et al., 1994</xref>; <xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>; <xref ref-type="bibr" rid="bib81">Sompolinsky et al., 2001</xref>; <xref ref-type="bibr" rid="bib23">Ecker et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Kohn et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Schneidman, 2016</xref>), especially in large populations of neurons (<xref ref-type="bibr" rid="bib53">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Montijn et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bartolo et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Kafashan et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Rumyantsev et al., 2020</xref>).</p><p>Statistically validating theories of population coding in large neural circuits thus depends on encoding models that support accurate Bayesian decoding, effectively capture noise-correlations, and efficiently fit large-scale neural recordings. There are at least two classes of neural recordings for which established models have facilitated such analyses. Firstly, for recordings of binary spike-counts, pairwise-maximum entropy models (<xref ref-type="bibr" rid="bib68">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib41">Lyamzin et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Granot-Atedgi et al., 2013</xref>; <xref ref-type="bibr" rid="bib85">Tkačik et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Meshulam et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Maoz et al., 2020</xref>) have been used to investigate the structure of the retinal code (<xref ref-type="bibr" rid="bib26">Ganmor et al., 2015</xref>; <xref ref-type="bibr" rid="bib86">Tkačik et al., 2015</xref>). Secondly, when modelling dynamic spike-train recordings, generalized linear models (GLMs) have proven effective at modelling spatio-temporal features of information processing in the retina and cortex (<xref ref-type="bibr" rid="bib58">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">Park et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Runyan et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Ruda et al., 2020</xref>).</p><p>Nevertheless, many theories of neural coding are formulated in terms of unbounded spike-counts (<xref ref-type="bibr" rid="bib42">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib8">Beck et al., 2011a</xref>; <xref ref-type="bibr" rid="bib25">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib45">Makin et al., 2015</xref>; <xref ref-type="bibr" rid="bib95">Yerxa et al., 2020</xref>), rather than the binary spike-counts of pairwise maximum entropy models. Furthermore, neural correlations are often low-dimensional (<xref ref-type="bibr" rid="bib3">Arieli et al., 1996</xref>; <xref ref-type="bibr" rid="bib22">Ecker et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Goris et al., 2014</xref>; <xref ref-type="bibr" rid="bib62">Rabinowitz et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Okun et al., 2015</xref>; <xref ref-type="bibr" rid="bib70">Semedo et al., 2019</xref>), in contrast with the correlations that result from the fully connected, recurrent structure of standard GLMs. Although there are extensions of the GLM approach that capture shared-variability (<xref ref-type="bibr" rid="bib88">Vidne et al., 2012</xref>; <xref ref-type="bibr" rid="bib2">Archer et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Zhao and Park, 2017</xref>), they seem unable to support exact Bayesian decoding. Similarly, methods such as factor analysis that model unbounded spike-counts as continuous variables have proven highly effective at modelling neural correlations in large-scale recordings (<xref ref-type="bibr" rid="bib96">Yu et al., 2009</xref>; <xref ref-type="bibr" rid="bib18">Cunningham and Yu, 2014</xref>; <xref ref-type="bibr" rid="bib22">Ecker et al., 2014</xref>; <xref ref-type="bibr" rid="bib70">Semedo et al., 2019</xref>), yet it is also unknown if they can support accurate Bayesian decoding.</p><p>Towards modelling spike-count responses and accurate Bayesian decoding in large populations of correlated neurons, we develop a class of encoding model based on finite mixtures of Poisson distributions. Within neuroscience, Poisson mixtures are widely applied to modelling the spike-count distributions of individual neurons (<xref ref-type="bibr" rid="bib93">Wiener and Richmond, 2003</xref>; <xref ref-type="bibr" rid="bib72">Shidara et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Goris et al., 2014</xref>; <xref ref-type="bibr" rid="bib84">Taouali et al., 2016</xref>). Outside of neuroscience, mixtures of multivariate Poisson distributions are an established model of multivariate count distributions that effectively capture correlations in count data (<xref ref-type="bibr" rid="bib37">Karlis and Meligkotsidou, 2007</xref>; <xref ref-type="bibr" rid="bib33">Inouye et al., 2017</xref>).</p><p>Building on the theory of exponential family distributions (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>; <xref ref-type="bibr" rid="bib44">Macke et al., 2011b</xref>), our model extends previous mixture models of multivariate count data in two ways. Firstly, we develop a tractable extension of Poisson mixtures that captures both over- and under-dispersed response variability (i.e. where the response variance is larger or smaller than the mean, respectively) based on Conway-Maxwell Poisson distributions (<xref ref-type="bibr" rid="bib73">Shmueli et al., 2005</xref>; <xref ref-type="bibr" rid="bib82">Stevenson, 2016</xref>). Secondly, we introduce an explicit dependence of the model on a stimulus variable, which allows the model to accurately capture changes in response statistics (including noise correlations) across stimuli. Importantly, the resulting encoding model affords closed-form expressions for both its Fisher information and probability density function, and thereby a rigorous quantification of the coding properties of a modelled neural population (<xref ref-type="bibr" rid="bib19">Dayan and Abbott, 2005</xref>). Moreover, the model learns low-dimensional representations of stimulus-driven neural activity, and we show how it captures a fundamental property of population codes known as information-limiting correlations (<xref ref-type="bibr" rid="bib53">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Montijn et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bartolo et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Kafashan et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Rumyantsev et al., 2020</xref>).</p><p>We apply our mixture model framework to both synthetic data and recordings from macaque primary visual cortex (V1), and demonstrate that it effectively models responses of populations of hundreds of neurons, captures noise correlations, and supports accurate Bayesian decoding. Moreover, we show how our model is compatible with the theory of probabilistic population coding (<xref ref-type="bibr" rid="bib97">Zemel et al., 1998</xref>; <xref ref-type="bibr" rid="bib61">Pouget et al., 2013</xref>), and could thus be used to study the theoretical coding properties of neural circuits, such as their efficiency (<xref ref-type="bibr" rid="bib25">Ganguli and Simoncelli, 2014</xref>), linearity (<xref ref-type="bibr" rid="bib42">Ma et al., 2006</xref>), or information content (<xref ref-type="bibr" rid="bib53">Moreno-Bote et al., 2014</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>A critical part of our theoretical approach is based on expressing models of interest in exponential family form. An exponential family distribution <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over some data <inline-formula><mml:math id="inf2"><mml:mi>n</mml:mi></mml:math></inline-formula> (in our case, neural responses) is defined by the proportionality relation <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are the so-called natural parameters, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a vector-valued function of the data called the sufficient statistic, and <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a scalar-valued, non-negative function called the base measure (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>). The exponential family form allows us to modify and extend existing models in a simple and flexible manner, and derive analytical results about the coding properties of our models. We demonstrate our approach with applications to both synthetic data, and experimental data recorded in V1 of anaesthetized and awake macaques viewing drifting grating stimuli at different orientations (for details see Materials and methods).</p><sec id="s2-1"><title>Extended Poisson mixture models capture spike-count variability and covariability</title><p>Our first goal is to define a class of models of stimulus-independent, neural population activity, that model neural activity directly as spike-counts, and that accurately capture single-neuron variability and pairwise covariability. We base our models on Poisson distributions, as they are widely applied to modelling the trial-to-trial distribution of the number of spikes generated by a neuron (<xref ref-type="bibr" rid="bib19">Dayan and Abbott, 2005</xref>; <xref ref-type="bibr" rid="bib43">Macke et al., 2011a</xref>). We will also generalize our Poisson-based models with the theory of Conway-Maxwell (CoM) Poisson distributions (<xref ref-type="bibr" rid="bib83">Sur et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Stevenson, 2016</xref>; <xref ref-type="bibr" rid="bib14">Chanialidis et al., 2018</xref>). The two-parameter CoM-Poisson model contains the one-parameter Poisson model as a special case, however, whereas the Poisson model always has a Fano factor (FF; the variance divided by the mean) of 1, the CoM-Poisson model can exhibit both over- (FF&gt;1) and under-dispersion (FF&lt;1), and thus capture the broader range of Fano factors observed in cortex (<xref ref-type="bibr" rid="bib82">Stevenson, 2016</xref>).</p><p>The other key ingredient in our modelling approach are mixtures of Poisson distributions, which have been used to model complex spike-count distributions in cortex, and also allow for over-dispersion (<xref ref-type="bibr" rid="bib72">Shidara et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Goris et al., 2014</xref>; <xref ref-type="bibr" rid="bib84">Taouali et al., 2016</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). In our case, we mix multiple, independent Poisson distributions in parallel, as such models can capture covariability in count data as well (see <xref ref-type="bibr" rid="bib37">Karlis and Meligkotsidou, 2007</xref> for a more general formulation of multivariate Poisson mixtures than what we consider here). To construct such a model, we begin with a product of independent Poisson distributions (IP distribution), one per neuron. We then mix a finite number of component IP models, to arrive at a multivariate spike-count, finite mixture model (see Materials and methods). Importantly, although each component of this mixture is an IP distribution, randomly switching between components induces correlations between the neurons (<xref ref-type="fig" rid="fig1">Figure 1B,C</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Poisson mixtures and Conway-Maxwell extensions exhibit spike-count correlations, and over- and under-disperson.</title><p>(<bold>A</bold>) A Poisson mixture distribution (red), defined as the weighted sum of three component Poisson distributions (black; scaled by their weights). FF denotes the Fano Factor (variance over mean) of the mixture. (<bold>B</bold>, <bold>C</bold>) The average spike-count (rate) of the first and second neurons for each of 13 components (black dots) of a bivariate IP mixture, and 68% confidence ellipses for the spike-count covariance of the mixture (red lines; see <xref ref-type="disp-formula" rid="equ6 equ7">Equations 6 and 7</xref>). The spike-count correlation of each mixture is denoted by <inline-formula><mml:math id="inf7"><mml:mi>r</mml:mi></mml:math></inline-formula>. (<bold>D</bold>) Same model as <bold>A</bold>, except we shift the distribution by increasing the baseline rate of the components. (<bold>E</bold>, <bold>F</bold>) Same model as <bold>A</bold>, except we use an additional baseline parameter based on Conway-Maxwell Poisson distributions to concentrate (<bold>E</bold>) or disperse (<bold>F</bold>) the mixture distribution and its components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-fig1-v3.tif"/></fig><p>IP mixtures can in fact model arbitrary covariability between neurons (see Materials and methods, <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>); however, they are still limited because the model neurons in an IP mixture are always over-dispersed. To overcome this, it is helpful to consider factor analysis (FA), which is widely applied to modelling neural population responses (<xref ref-type="bibr" rid="bib18">Cunningham and Yu, 2014</xref>). IP mixtures are similar to FA, in that FA represents the covariance matrix of neural responses as the sum of a diagonal matrix that helps capture individual variance, and a low-rank matrix that captures covariance (see <xref ref-type="bibr" rid="bib10">Bishop, 2006</xref>), and FA and IP mixtures can be fine-tuned to capture covariance arbitrarily well. However, whereas FA has distinct parameters for representing means and diagonal variances, the means and variances in an IP mixture are coupled through shared parameters (see Materials and methods, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). Our strategy will thus be to break this coupling between means and variances by granting IP mixtures an additional set of parameters based on the theory of CoM-Poisson distributions.</p><p>To do so, we first show how to express an IP mixture as the marginal distribution of an exponential family distribution. Note that an IP mixture with <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> components may be expressed as a latent variable model over spike-count vectors <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and latent component-indices <inline-formula><mml:math id="inf10"><mml:mi>k</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf11"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In this formulation we denote the <inline-formula><mml:math id="inf12"><mml:mi>k</mml:mi></mml:math></inline-formula>th component distribution by <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the probability of realizing (switching to) the <inline-formula><mml:math id="inf14"><mml:mi>k</mml:mi></mml:math></inline-formula> th component by <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The mixture model over spike-counts <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is then expressed as the marginal distribution <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, of the joint distribution <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Under mild regularity assumptions (see Materials and methods), we may reparameterize this joint distribution in exponential family form as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the vectors <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and matrix <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the natural parameters of <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the Kronecker delta vector defined by <inline-formula><mml:math id="inf24"><mml:mrow><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>, and 0 otherwise.</p><p>This representation affords an intuitive interpretation. In general, the natural parameters of an IP distribution are the logarithms of the average spike-counts (firing rates), and the natural parameters of the first component distribution <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of an IP mixture are simply <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The natural parameters of the <inline-formula><mml:math id="inf28"><mml:mi>k</mml:mi></mml:math></inline-formula> th component for <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> are then the sum of the ‘baseline’ parameters <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and column <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> from the matrix of parameters <inline-formula><mml:math id="inf32"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, Materials and methods). Because the dimension of the baseline parameters <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is much smaller than the total number of parameters in a given mixture, the baseline parameters provide a relatively low-dimensional means of affecting all the component distributions of the given mixture, as well as the probability distribution over indices <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; see Materials and methods, <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> for how the index-probabilities <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depend on <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>We next extend <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> with the theory of CoM-Poisson distributions, and define the latent variable exponential family<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐥𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector of log-factorials of the individual spike-counts, and <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are a set of natural parameters derived from CoM-Poisson distributions (see Materials and methods). Based on this construction, each component <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a product of independent CoM-Poisson distributions, and when <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we recover an IP mixture defined by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> with parameters <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf43"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The first component of this model <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has parameters <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and as with the IP mixture, the parameters <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are translated by column <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> when <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. However, the parameters <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are never translated, and remain the same for each component distribution (<xref ref-type="disp-formula" rid="equ16">Equation 16</xref>, Materials and methods, and see <xref ref-type="disp-formula" rid="equ15">Equation 15</xref> for formulae for the index-probabilities <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). We refer to models defined by <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> as CoM-based (CB) mixtures, and <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> as CB parameters.</p><p>Due to the addition of the CB parameters <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, a CB mixture breaks the coupling between the spike-count means and variances that is present in the simpler IP mixture (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>, Materials and methods). In <xref ref-type="fig" rid="fig1">Figure 1D–F</xref>, we demonstrate how changing the parameters of a CB mixture can concentrate or disperse both the mixture distribution and its components, and that a CB mixture can indeed exhibit under-dispersion.</p><p>To validate our mixture models, we tested if they capture variability and covariability of V1 population responses to repeated presentations of a grating stimulus with fixed orientation (<inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>43</mml:mn></mml:mrow></mml:math></inline-formula> neurons and <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>355</mml:mn></mml:mrow></mml:math></inline-formula> repetitions of 150 ms duration in one awake macaque; <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> of duration 70 ms in one anaesthetized macaque). We fit our mixtures to the complete datasets with expectation-maximization (EM, a standard choice for training finite mixture models [<xref ref-type="bibr" rid="bib49">McLachlan et al., 2019</xref>] see Materials and methods). The CB mixture accurately captured single-neuron variability (<xref ref-type="fig" rid="fig2">Figure 2A–B</xref>, red symbols), including both cases of over-dispersion and under-dispersion. On the other hand, the simpler IP mixture (<xref ref-type="fig" rid="fig2">Figure 2A–B</xref>, blue symbols) cannot accommodate under-dispersion due to its mathematical limits, and demonstrated limited ability to model over-dispersion due to the coupling between the mean and variance (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>CoM-based parameters help Poisson mixtures capture individual variability in V1 responses to a single stimulus.</title><p>We compare Independent Poisson (IP) mixtures (<xref ref-type="disp-formula" rid="equ1">Equation 1)</xref> and CoM-Based (CB) mixtures (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) on neural population responses to stimulus orientation <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>20</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> in V1 of awake (<inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>43</mml:mn></mml:mrow></mml:math></inline-formula> neurons and <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>355</mml:mn></mml:mrow></mml:math></inline-formula> trials) and anaesthetized (<inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) macaques; both mixtures are defined with <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> components for both data sets (see Materials and methods for training algorithms). <bold>A,B:</bold> Empirical Fano factors of the awake (<bold>A</bold>) and anaesthetized data (<bold>B</bold>), comparing IP (blue) and CB mixtures (red). <bold>C,D:</bold> Histogram of the CB parameters <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the CB mixture fits to the awake (<bold>C</bold>) and anaesthetized (<bold>D</bold>) data. Values of <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> denote under-dispersed mixture components, values <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi/><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> denote over-dispersed components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-fig2-v3.tif"/></fig><p>To understand how the CB parameters allow the CB mixture to overcome the limits of the IP mixture, we plot a histogram of the CB parameters <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> for both fits (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>). If the CB parameter of a given CoM-Poisson distribution is <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi/><mml:mo>&lt;</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi/><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, or <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, then the CoM-Poisson distribution is under-dispersed, over-dispersed, or Poisson-distributed, respectively. When a CB mixture is fit to the awake data (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), we see that it learns a range of values for the CB parameters around −1, to accommodate the variety of Fano factors observed in the awake data (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). On the anaesthetized data, even though IP mixtures can capture over-dispersion, the IP mixture underestimates the dispersion of neurons due to the coupling between the mean and variance (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The CB mixture thus uses the CB parameters to further disperse its model neurons (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>In contrast with individual variability, we found that both mixture models were flexible enough to qualitatively capture pairwise noise correlation structure in both awake and anaesthetized animals (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>), and that the distributions of modelled neural correlations were broadly similar when compared to the data (<xref ref-type="fig" rid="fig3">Figure 3C–D</xref>). In Appendix 1, we rigorously compare IP mixtures, CB mixtures, and FA on our datasets, and show that although FA is better than our mixture models at capturing second-order statistics in training data, IP mixtures and CB mixtures achieve comparable predictive performance as FA when evaluated on held-out data.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>IP and CB mixtures effectively capture pairwise covariability in V1 responses to a single stimulus.</title><p>Here we analyze the pairwise statistics of the same models from <xref ref-type="fig" rid="fig2">Figure 2</xref>. (<bold>A, B</bold>) Empirical correlation matrix (upper right triangles) of awake (<bold>A</bold>) and anaesthetized data (<bold>B</bold>), compared to the correlation matrix of the corresponding IP mixtures (lower left triangles). (<bold>C</bold>, <bold>D</bold>) Noise correlations highlighted in <bold>A</bold> and <bold>B</bold>, respectively. (<bold>E</bold>, <bold>F</bold>) Highlighted noise correlations for CB mixture fit. (<bold>G</bold>,<bold>H</bold>) Histogram of empirical noise correlations, and model correlations from IP and CB mixtures.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-fig3-v3.tif"/></fig></sec><sec id="s2-2"><title>Extended Poisson mixture models capture stimulus-dependent response statistics</title><p>So far, we have introduced the exponential family theory of IP and CB mixtures, and shown how they capture response variability and covariability for a fixed stimulus. To allow us to study stimulus encoding and decoding, we further extend our mixtures by inducing a dependency of the model parameters on a stimulus. When there are a finite number of stimulus conditions and sufficient data, we may define a stimulus-dependent model with a lookup table, and fit it by fitting a distinct model at each stimulus condition. However, this is inefficient when the amount of data at each stimulus-condition is limited and the stimulus-dependent statistics have structure that is shared across conditions. A notable feature of the exponential family parameterizations in <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> is that the baseline parameters influence both the index probabilities and all the component distributions of the model. This suggests that by restricting stimulus-dependence to the baseline parameters, we might model rich stimulus-dependent response structure, while bounding the complexity of the model.</p><p>In general, we refer to any finite mixture with stimulus-dependent parameters as a conditional mixture (CM), and depending on whether the CM is based on <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> and, we refer to it as an IP- or CB-CM, respectively. Although there are many ways we might induce stimulus-dependence, in this paper we consider two forms of CM: (i) a maximal CM, which we implement as a lookup table, such that all the parameters in <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> and depend on the stimulus, and (ii) a minimal CM, for which we restrict stimulus-dependence to the baseline parameters <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This results in the CB-CM<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf73"><mml:mi>x</mml:mi></mml:math></inline-formula> is the stimulus, and <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are the stimulus-dependent baseline parameters, and we recover a minimal, IP-CM by setting <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The IP-CM again affords an intuitive interpretation: The first component of an IP-CM <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has stimulus-dependent natural parameters <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and thus the stimulus-dependent firing rate, or tuning curve, of the <inline-formula><mml:math id="inf78"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron given <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf82"><mml:mi>i</mml:mi></mml:math></inline-formula> th element of <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The natural parameters of the <inline-formula><mml:math id="inf84"><mml:mi>k</mml:mi></mml:math></inline-formula> th component for <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> are then the sum of <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and column <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf88"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. As such, given <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the tuning curve of the <inline-formula><mml:math id="inf90"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a ‘gain-modulated’ version of <inline-formula><mml:math id="inf92"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the gain <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the exponential function of element <inline-formula><mml:math id="inf94"><mml:mi>i</mml:mi></mml:math></inline-formula> of column <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf96"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, Materials and methods). For a CB-CM this interpretation no longer holds exactly, but still serves as an approximate description of the behaviour of its components (see <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> and the accompanying discussions).</p><p>Towards understanding the expressive power of CMs, we study a minimal, CB-CM with <inline-formula><mml:math id="inf97"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> neurons, <inline-formula><mml:math id="inf98"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mixture components, and randomly chosen parameters (see Materials and methods). Moreover, we assume that the stimulus is periodic (e.g. the orientation of a grating), and that the tuning curves of the component distributions <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> have a von Mises shape, which is a widely applied model of neural tuning to periodic stimuli (<xref ref-type="bibr" rid="bib31">Herz et al., 2017</xref>). We may achieve such a shape by defining the stimulus-dependent baseline parameters as <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf102"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are parameters, and <inline-formula><mml:math id="inf103"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐯𝐦</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows that the tuning curves of the CB-CM neurons are approximately bell-shaped, yet many also exhibit significant deviations.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Expectation-maximization recovers a ground truth CoM-based, conditional mixture (CB-CM).</title><p>We compare a ground truth, CB-CM with 20 neurons, five mixture components, von Mises-tuned components, and randomized parameters to a learned CB-CM fit to 2000 samples from the ground truth CB-CM. <bold>A,B:</bold> Tuning curves of the ground-truth CB-CM (<bold>A</bold>) and learned CB-CM (<bold>B</bold>). Three tuning curves are highlighted for effect. <bold>C,D:</bold> The orientation-dependent index probabilities of the ground truth CB-CM (<bold>C</bold>) and learned CB-CM (<bold>D</bold>), where colour indicates component index. Dashed lines indicate example stimulus-orientations used in <bold>E</bold>, <bold>F</bold>, and <bold>G</bold>. (<bold>E</bold>, <bold>F</bold>) The correlation matrix of the ground truth CB-CM (upper right), compared to the correlation matrix of the learned CB-CM (lower left) at stimulus orientations <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>85</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (<bold>E</bold>) and <inline-formula><mml:math id="inf105"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>110</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (<bold>F</bold>). (<bold>G</bold>) The FFs of the ground-truth CB-CM compared to the learned CB-CM at orientations <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>85</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (blue circles) and <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>110</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (red triangles).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-fig4-v3.tif"/></fig><p>We also study if CMs can be effectively fit to datasets comparable to those obtained in typical neurophysiology experiments. We generated 200 responses from the CB-CM described above — the ground truth CB-CM — to each of 10 orientations spread evenly over the half-circle, for a total of 2000 stimulus-response sample points. We then used this data to fit a CB-CM with the same number of components. Toward this aim, we derive an approximate EM algorithm to optimize model parameters (see Materials and methods). <xref ref-type="fig" rid="fig4">Figure 4B</xref> shows that the tuning curves of the learned CB-CM are nearly indistinguishable from those of the ground truth CB-CM (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, coefficient of determination <inline-formula><mml:math id="inf108"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.998</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>To reveal the orientation-dependent latent structure of the model, in <xref ref-type="fig" rid="fig4">Figure 4C</xref> we plot the index probability <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for every <inline-formula><mml:math id="inf110"><mml:mi>k</mml:mi></mml:math></inline-formula> as a function of the orientation <inline-formula><mml:math id="inf111"><mml:mi>x</mml:mi></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig4">Figure 4D</xref> we show that the orientation-dependent index probabilities of the learned CB-CM qualitatively match the true index probabilities in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. We also note that although the learned CB-CM does not correctly identify the indices themselves, this has no effect on the performance of the CB-CM.</p><p>The orientation-dependent index-probabilities provide a high-level picture of how the complexity and structure of model correlations varies with the orientation. The vertical dashed lines in <xref ref-type="fig" rid="fig4">Figure 4C–D</xref> denote two orientations that yield substantially different index probabilities <inline-formula><mml:math id="inf112"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. When a large number of index-probabilities are non-zero, the correlation-matrices of the CB-CM can exhibit complex correlations with both negative and positive values (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). However, when one index dominates, the correlation structure largely disappears (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). In <xref ref-type="fig" rid="fig4">Figure 4G</xref> we show that the FFs also depend on stimulus orientation. Lastly, we find that both the FF and the correlation-matrices of the learned CB-CM are nearly indistinguishable from the ground-truth CB-CM (<xref ref-type="fig" rid="fig4">Figure 4E–G</xref>).</p><p>In summary, our analyses show that minimal CB-CMs can express complex, stimulus-dependent response statistics, and that we can recover the structure of a ground truth CB-CM from realistic amounts of synthetic data with EM. In the following sections, we rigorously evaluate the performance of CMs on our awake and anaesthetized datasets.</p></sec><sec id="s2-3"><title>Conditional mixtures effectively model neural responses in macaque V1</title><p>A variety of models may be defined within the CM framework delineated by <xref ref-type="disp-formula" rid="equ1 equ2 equ3">Equations 1, 2 and 3</xref>. Towards understanding how effectively CMs can model real data, we compare different variants by their cross-validated log-likelihood on both our awake and anaesthetized datasets; this is the same data used in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> but now including all stimulus-conditions. We consider both IP and CB variants of each of the following conditional mixtures: (i) maximal CMs where we learn a distinct mixture for each of <inline-formula><mml:math id="inf113"><mml:msub><mml:mi>d</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:math></inline-formula> stimulus conditions, (ii) minimal CMs with von Mises-tuned components, and (iii) minimal CMs with <italic>discrete</italic>-tuned components given by <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf115"><mml:mi mathvariant="bold-italic">𝜹</mml:mi></mml:math></inline-formula> is the Kronecker delta vector with <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> elements, and <inline-formula><mml:math id="inf117"><mml:mi>x</mml:mi></mml:math></inline-formula> is the index of the stimulus. In contrast with the von Mises CM, the discrete CM makes no assumptions about the form of component tuning. In <xref ref-type="table" rid="table1">Table 1</xref> we detail the number of parameters for all forms of CM.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameter counts of CM models.</title><p>First row is number of parameters in IP models, second row is number of additional parameters in CB extensions of IP models, as a function of number of stimuli <inline-formula><mml:math id="inf118"><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></inline-formula>, neurons <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula>, and mixture components <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4">Model parameter formulae</th></tr><tr><th/><th>Maximal</th><th>Von mises</th><th>Discrete</th></tr></thead><tbody><tr><td>Num. Params</td><td><inline-formula><mml:math id="inf121"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf122"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Add. CB Params</td><td><inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf125"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf126"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>To provide an interpretable measure of the relative performance of each CM variant, we define the ‘information gain’ as the difference between the estimated log-likelihood (base <inline-formula><mml:math id="inf127"><mml:mi>e</mml:mi></mml:math></inline-formula>) of the given CM and the log-likelihood of a von Mises-tuned, independent Poisson model, which is a standard model of uncorrelated neural responses to oriented stimuli (<xref ref-type="bibr" rid="bib31">Herz et al., 2017</xref>). We then evaluate the predictive performance of our models with 10-fold cross-validation of the information gain.</p><p><xref ref-type="table" rid="table2">Table 2</xref> shows that the CM variants considered achieve comparable performance, and perform substantially better than the independent Poisson lower bound on both the awake and anaesthetized data. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows that a performance peak emerges smoothly as the model complexity (number of parameters) is increased. In all cases, the CB models outperform their IP counterparts, and typically with fewer parameters. The discrete CB-CMs achieve high performance on both datasets. In contrast, von Mises CMs perform well on the anaesthetized data but more poorly on the awake data, and maximal CMs exhibit the opposite trend. Nevertheless, von Mises CMs solve a more difficult statistical problem as they also interpolate between stimulus conditions, and so may still prove relevant even where performance is limited. On the other hand, even though maximal CMs achieve high performance, they simply do so by replicating the high performance of stimulus-independent mixtures (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>) at each stimulus condition, and require more parameters than minimal CMs to maximize performance.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Finding the optimal number of parameters for CMs to model neural responses in macaque V1.</title><p>10-fold cross-validation of the information gain given awake V1 data (<bold>A</bold>) and anaesthetized V1 data (<bold>B</bold>), as a function of the number of model parameters, for multiple forms of CM: maximal CMs (green); minimal CMs with von Mises component tuning (blue); minimal CMs with discrete component tuning (purple); and for each case we consider either IP (dashed lines) or CB (solid lines) variants. Standard errors of the information gain are not depicted to avoid visual clutter, however they are approximately independent of the number of model parameters, and match the values indicated in <xref ref-type="table" rid="table2">Table 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-fig5-v3.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Conditional mixtures models of neural responses in macaque V1 capture significant information about higher-order statistics.</title><p>We apply 10-fold cross-validation to estimate the mean and standard error of the information gain (model log-likelihood -log-likelihood of a non-mixed, independent Poisson model in nats/trial) on held-out data, from either awake (sample size <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>168</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, from <inline-formula><mml:math id="inf129"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>43</mml:mn></mml:mrow></mml:math></inline-formula> neurons, over <inline-formula><mml:math id="inf130"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> orientations) or anaesthetized (<inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf132"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf133"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula>) macaque V1. We compare maximal CMs, minimal CMs with von Mises-tuned components, and minimal CMs with discrete-tuned components, and for each case we consider either IP or CB variants. For each variant, we indicate the number of CM components <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> and the corresponding number of model parameters required to achieve peak information gain (cross-validated). For reference, the non-mixed, independent Poisson models use 129 and 210 parameters for the awake and anaesthetized data, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="7">Encoding performance</th></tr><tr><th/><th colspan="3">V1 awake data</th><th colspan="3">V1 anaesthetized data</th></tr></thead><tbody><tr><td>CM Variant</td><td>Inf. Gain (<inline-formula><mml:math id="inf135"><mml:mfrac><mml:mtext>Nats</mml:mtext><mml:mtext>Trial</mml:mtext></mml:mfrac></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf136"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula></td><td># Params.</td><td>Inf. Gain (<inline-formula><mml:math id="inf137"><mml:mfrac><mml:mtext>Nats</mml:mtext><mml:mtext>Trial</mml:mtext></mml:mfrac></mml:math></inline-formula>)</td><td><inline-formula><mml:math id="inf138"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula></td><td># Params.</td></tr><tr><td>Maximal IP</td><td><inline-formula><mml:math id="inf139"><mml:mrow><mml:mn>2.30</mml:mn><mml:mo>±</mml:mo><mml:mn>0.32</mml:mn></mml:mrow></mml:math></inline-formula></td><td>5</td><td>1971</td><td><inline-formula><mml:math id="inf140"><mml:mrow><mml:mn>8.77</mml:mn><mml:mo>±</mml:mo><mml:mn>0.71</mml:mn></mml:mrow></mml:math></inline-formula></td><td>8</td><td>5103</td></tr><tr><td>Maximal CB</td><td><inline-formula><mml:math id="inf141"><mml:mrow><mml:mn>2.44</mml:mn><mml:mo>±</mml:mo><mml:mn>0.35</mml:mn></mml:mrow></mml:math></inline-formula></td><td>5</td><td>2358</td><td><inline-formula><mml:math id="inf142"><mml:mrow><mml:mn>9.42</mml:mn><mml:mo>±</mml:mo><mml:mn>0.70</mml:mn></mml:mrow></mml:math></inline-formula></td><td>7</td><td>5094</td></tr><tr><td>Von Mises IP</td><td><inline-formula><mml:math id="inf143"><mml:mrow><mml:mn>2.01</mml:mn><mml:mo>±</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:math></inline-formula></td><td>45</td><td>2065</td><td><inline-formula><mml:math id="inf144"><mml:mrow><mml:mn>8.97</mml:mn><mml:mo>±</mml:mo><mml:mn>0.70</mml:mn></mml:mrow></mml:math></inline-formula></td><td>40</td><td>2979</td></tr><tr><td>Von Mises CB</td><td><inline-formula><mml:math id="inf145"><mml:mrow><mml:mn>2.10</mml:mn><mml:mo>±</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula></td><td>40</td><td>1888</td><td><inline-formula><mml:math id="inf146"><mml:mrow><mml:mn>9.38</mml:mn><mml:mo>±</mml:mo><mml:mn>0.69</mml:mn></mml:mrow></mml:math></inline-formula></td><td>35</td><td>2694</td></tr><tr><td>Discrete IP</td><td><inline-formula><mml:math id="inf147"><mml:mrow><mml:mn>2.25</mml:mn><mml:mo>±</mml:mo><mml:mn>0.28</mml:mn></mml:mrow></mml:math></inline-formula></td><td>40</td><td>2103</td><td><inline-formula><mml:math id="inf148"><mml:mrow><mml:mn>9.17</mml:mn><mml:mo>±</mml:mo><mml:mn>0.70</mml:mn></mml:mrow></mml:math></inline-formula></td><td>35</td><td>3044</td></tr><tr><td>Discrete CB</td><td><inline-formula><mml:math id="inf149"><mml:mrow><mml:mn>2.35</mml:mn><mml:mo>±</mml:mo><mml:mn>0.29</mml:mn></mml:mrow></mml:math></inline-formula></td><td>30</td><td>1706</td><td><inline-formula><mml:math id="inf150"><mml:mrow><mml:mn>9.53</mml:mn><mml:mo>±</mml:mo><mml:mn>0.68</mml:mn></mml:mrow></mml:math></inline-formula></td><td>30</td><td>2689</td></tr><tr><td>Non-mixed IP</td><td>0</td><td>1</td><td>129</td><td>0</td><td>1</td><td>210</td></tr></tbody></table></table-wrap></sec><sec id="s2-4"><title>Conditional mixtures facilitate accurate and efficient decoding of neural responses</title><p>To demonstrate that CMs model the neural code, we must show that CMs not only capture the features of neural responses, but that these features also encode stimulus-information. Given an encoding model <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a response from the model <inline-formula><mml:math id="inf152"><mml:mi mathvariant="bold">𝐧</mml:mi></mml:math></inline-formula>, we may optimally decode the information in the response about the stimulus <inline-formula><mml:math id="inf153"><mml:mi>x</mml:mi></mml:math></inline-formula> by applying Bayes’ rule <inline-formula><mml:math id="inf154"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the posterior distribution (the decoded information), and <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents our prior assumptions about the stimulus (<xref ref-type="bibr" rid="bib97">Zemel et al., 1998</xref>). When we do not know the true encoding model, and rather fit a statistical model to stimulus-response data, using the statistical model for Bayesian decoding and analyzing its performance can tell us how well it captures the features of the neural code.</p><p>We analyze the performance of Bayesian decoders based on CMs by quantifying their decoding performance, and comparing the results to other common approaches to decoding. We evaluate decoding performance with the 10-fold cross-validation log-posterior probability <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (base <inline-formula><mml:math id="inf158"><mml:mi>e</mml:mi></mml:math></inline-formula>) of the true stimulus value <inline-formula><mml:math id="inf159"><mml:mi>x</mml:mi></mml:math></inline-formula>, for both our awake and anaesthetized V1 datasets. With regard to choosing the number of components <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>, we analyze the decoding performance of CMs that achieved the best <italic>encoding</italic> performance based as indicated in <xref ref-type="table" rid="table2">Table 2</xref> and depicted <xref ref-type="fig" rid="fig5">Figure 5</xref>. We do this to demonstrate how well a single model can simultaneously perform at both encoding and decoding, instead of applying distinct procedures for selecting CMs based on decoding performance (see Materials and methods for a summary of trade-offs when choosing <inline-formula><mml:math id="inf161"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>).</p><p>In our comparisons we focus on minimal, discrete CMs as overall they achieved high performance on both datasets (<xref ref-type="fig" rid="fig5">Figure 5</xref>). To characterize the importance of neural correlations to Bayesian decoding, we compare our CMs to the decoding performance of independent Poisson models with discrete tuning (Non-mixed IP). To characterize the optimality of our Bayesian decoders, we also evaluate the performance of linear multiclass decoders (Linear), as well nonlinear multiclass decoders defined as artificial neural networks (ANNs) with two hidden layers and a cross-validated number of hidden units (for details on the training and model selection procedure, see Materials and methods).</p><p><xref ref-type="table" rid="table3">Table 3</xref> shows that on the awake data, the performance of the CMs is statistically indistinguishable from the ANN, and the CMs and the ANN significantly exceed the performance of both the Linear and Non-mixed IP models. On the anaesthetized data, the minimal CM approaches the performance of the ANN, and the minimal CMs and ANN models again exceed the performance of the Non-mixed IP and Linear models. Yet in this case, the Linear model is much more competitive, whereas the Non-mixed IP model performs very poorly, possibly because of the larger magnitude of noise correlations in this data. In Appendix 2, we also report that a Bayesian decoder based on a factor analysis (FA) encoding model performed inconsistently, and poorly relative to CMs, as it would occasionally assign numerically 0 probability to the true stimulus, and thus score an average log-posterior of negative infinity. In Appendix 2, we present preliminary evidence that this is because CMs capture higher order structure that FA cannot.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>CMs support high-performance decoding of neural responses in macaque V1.</title><p>We apply 10-fold cross-validation to estimate the mean and standard error of the average log-posteriors <inline-formula><mml:math id="inf162"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on held-out data, from either awake or anaesthetized macaque V1. We compare discrete, minimal, CB-CM (CB-CM) and IP-CM (IP-CM); an independent Poisson model with discrete tuning (Non-mixed IP); a multiclass linear decoder (Linear); and a multiclass nonlinear decoder defined as an artificial neural network with two hidden layers (Artificial NN). The number of CM components <inline-formula><mml:math id="inf163"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> was chosen to achieve peak information gain in <xref ref-type="fig" rid="fig5">Figure 5</xref>. The number of ANN hidden units was chosen based on peak cross-validation performance. In all cases we also indicate the number of model parameters required to achieve the indicated performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5">Decoding performance</th></tr><tr><th/><th colspan="2">V1 awake data</th><th colspan="2">V1 anaesthetized data</th></tr></thead><tbody><tr><td/><td>Average Log-Post.</td><td>Num. Params.</td><td>Average Log-Post.</td><td>Num. Params.</td></tr><tr><td>IP-CM</td><td><inline-formula><mml:math id="inf164"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.207</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.039</mml:mn></mml:mrow></mml:math></inline-formula></td><td>2103</td><td><inline-formula><mml:math id="inf165"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.448</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.026</mml:mn></mml:mrow></mml:math></inline-formula></td><td>3044</td></tr><tr><td>CB-CM</td><td><inline-formula><mml:math id="inf166"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.206</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.043</mml:mn></mml:mrow></mml:math></inline-formula></td><td>1706</td><td><inline-formula><mml:math id="inf167"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.441</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.023</mml:mn></mml:mrow></mml:math></inline-formula></td><td>2689</td></tr><tr><td>Non-mixed IP</td><td><inline-formula><mml:math id="inf168"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.272</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.067</mml:mn></mml:mrow></mml:math></inline-formula></td><td>387</td><td><inline-formula><mml:math id="inf169"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.967</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.071</mml:mn></mml:mrow></mml:math></inline-formula></td><td>630</td></tr><tr><td>Linear</td><td><inline-formula><mml:math id="inf170"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.256</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.053</mml:mn></mml:mrow></mml:math></inline-formula></td><td>352</td><td><inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.457</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.019</mml:mn></mml:mrow></mml:math></inline-formula></td><td>568</td></tr><tr><td>Artificial NN</td><td><inline-formula><mml:math id="inf172"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.200</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.032</mml:mn></mml:mrow></mml:math></inline-formula></td><td>527,108</td><td><inline-formula><mml:math id="inf173"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.426</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.015</mml:mn></mml:mrow></mml:math></inline-formula></td><td>408,008</td></tr></tbody></table></table-wrap><p>On both the awake and anaesthetized data the ANN requires two orders of magnitude more parameters than the CMs to achieve its performance gains. In addition, the CB-CM achieves marginally better performance with fewer parameters than the IP-CM, indicating that although modelling individual variability is not essential for effective Bayesian decoding, doing so still results in a more parsimonious model of the neural code. In Appendix 3, we report a sample complexity analysis of CM encoding and decoding performance. We found that whereas our anaesthetized V1 dataset (sample size <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) was large enough to saturate the performance of our models, a larger awake V1 dataset (<inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>168</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) could yield further improvements to decoding performance.</p><p>We also consider widely used alternative measures of decoding performance, namely the Fisher information (FI), which is an upper bound on the average precision (inverse variance) of the posterior (<xref ref-type="bibr" rid="bib12">Brunel and Nadal, 1998</xref>), as well as the linear Fisher information (LFI), which is a linear approximation of the FI (<xref ref-type="bibr" rid="bib71">Seriès et al., 2004</xref>) corresponding to the accuracy of the optimal, unbiased linear decoder of the stimulus (<xref ref-type="bibr" rid="bib35">Kanitscheider et al., 2015a</xref>). The FI is especially helpful when the posterior cannot be evaluated directly (such as when it is continuous), and is widely adopted in theoretical (<xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>; <xref ref-type="bibr" rid="bib9">Beck et al., 2011b</xref>; <xref ref-type="bibr" rid="bib22">Ecker et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Kohn et al., 2016</xref>) and experimental (<xref ref-type="bibr" rid="bib21">Ecker et al., 2011</xref>; <xref ref-type="bibr" rid="bib34">Kafashan et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Rumyantsev et al., 2020</xref>) studies of neural coding. As with other models based on exponential family theory (<xref ref-type="bibr" rid="bib42">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib9">Beck et al., 2011b</xref>; <xref ref-type="bibr" rid="bib23">Ecker et al., 2016</xref>), the FI of a minimal CM may be expressed in closed-form, and is equal to its LFI (see Materials and methods), and therefore minimal CMs can be used to study FI analytically and obtain model-based estimates of FI from data.</p><p>To study how well CMs capture FI, we defined 40 random subpopulations of <inline-formula><mml:math id="inf176"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> neurons from both our V1 datasets, fit von Mises IP-CMs to the responses of each subpopulation, and used these learned models as ground-truth populations. We then generated 50 responses at each of 10 evenly spaced orientations from each ground truth IP-CM, for a total of <inline-formula><mml:math id="inf177"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> responses per ground-truth model. We then fit a new IP-CM to each set of 500 responses, and compared the FI of the re-fit CM to the FI of the ground-truth CM at 50 evenly spaced orientations. Pooled over all populations and orientations, the relative error of the estimated FI was <inline-formula><mml:math id="inf178"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>12.8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mn>18.6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the awake data and <inline-formula><mml:math id="inf179"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>9.1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mn>22.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the anaesthetized data, suggesting that IP-CMs can recover and even interpolate approximate FIs of ground-truth populations from modest amounts of data.</p><p>To summarize, CMs support accurate Bayesian decoding in awake and anaesthetized macaque V1 recordings, and are competitive with nonlinear decoders with two orders of magnitude more parameters. Moreover, CMs afford closed-form expressions of FI and can interpolate good estimates of FI from modest amounts of data, and thereby support analyses of neural data based on this widely applied theoretical tool.</p></sec><sec id="s2-5"><title>Constrained conditional mixtures support linear probabilistic population coding</title><p>Having shown that minimal CMs can both capture the statistics of neural encoding and facilitate accurate Bayesian decoding, we now aim to show how they relate to an influential theory of neural coding known as probabilistic population codes (PPCs), which describes how neural circuits process information in terms of encoding and Bayesian decoding (<xref ref-type="bibr" rid="bib97">Zemel et al., 1998</xref>). In particular, linear probabilistic population codes (LPPCs) are PPCs with a restricted encoding model, that explain numerous features of neural coding in the brain (<xref ref-type="bibr" rid="bib42">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib7">Beck et al., 2008</xref>; <xref ref-type="bibr" rid="bib8">Beck et al., 2011a</xref>).</p><p>In general, an exponential family of distributions that depend on some stimulus <inline-formula><mml:math id="inf180"><mml:mi>x</mml:mi></mml:math></inline-formula> may be expressed as <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf182"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> is the sufficient statistic, μ is the base measure, and <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is known as the log-partition function (in <xref ref-type="disp-formula" rid="equ1 equ2 equ3">Equations 1-3</xref> we used the proportionality symbol ∝ to avoid writing the log-partition functions explicitly). A PPC is an LPPC when its encoding model is in the so-called exponential family with linear sufficient statistics (EFLSS), which has the form <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for some functions <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib8">Beck et al., 2011a</xref>). If we equate the two expressions <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> we see that an EFLSS is a stimulus-dependent exponential family that satisfies two constraints: that the sufficient statistic <inline-formula><mml:math id="inf188"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:mrow></mml:math></inline-formula> is linear, and that the log-partition function <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> does not depend on the stimulus, so that <inline-formula><mml:math id="inf190"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>As presented, the EFLSS is a mathematical model that does not have fittable parameters. We wish to express CMs as a form of EFLSS in order to show how a fittable model could be compatible with LPPC theory. If we return to the general expression for a minimal CM (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) and assume that the log-partition function is given by the constant α, then we may write<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that the given CM is in the EFLSS. Observe that this equation only holds due to the specific structure of minimal CMs: if the parameters <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, or <inline-formula><mml:math id="inf194"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> would depend on the stimulus, then it would not be possible to absorb them into the function <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Ultimately, this equivalence between constrained CMs and EFLSSs allows LPPC theory to be applied to constrained CMs, and provides theorists working on neural coding with an effective statistical tool that can help validate their hypotheses.</p></sec><sec id="s2-6"><title>Minimal conditional mixtures capture information-limiting correlations</title><p>Our last aim is to demonstrate that CMs can approximately represent a central phenomenon in neural coding known as information-limiting correlations, which are neural correlations that fundamentally limit stimulus-information in neural circuits (<xref ref-type="bibr" rid="bib53">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Montijn et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bartolo et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Kafashan et al., 2021</xref>; <xref ref-type="bibr" rid="bib65">Rumyantsev et al., 2020</xref>). To illustrate this, we generate population responses with limited information, and then fit an IP-CM to these responses and study the learned latent representation. In particular, we consider a source population of 200 independent Poisson neurons <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with homogeneous, von Mises tuning curves responding to a noisy stimulus-orientation <inline-formula><mml:math id="inf197"><mml:mi>s</mml:mi></mml:math></inline-formula>, where the noise <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> follows a von Mises distribution centred at the true stimulus-orientation <inline-formula><mml:math id="inf199"><mml:mi>x</mml:mi></mml:math></inline-formula> (see Materials and methods). In <xref ref-type="fig" rid="fig6">Figure 6A</xref> we show that, as expected, the average FI in the source population about the noisy orientation <inline-formula><mml:math id="inf200"><mml:mi>s</mml:mi></mml:math></inline-formula> grows linearly with the size of randomized subpopulations, although the FI about the true orientation <inline-formula><mml:math id="inf201"><mml:mi>x</mml:mi></mml:math></inline-formula> is theoretically bounded by the precision (inverse variance) of the sensory noise.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>CMs can capture information-limiting correlations in data.</title><p>We consider a von Mises-tuned, independent Poisson source model (green) with <inline-formula><mml:math id="inf202"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> neurons, and an information-limited, IP-CM (purple) with <inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> components, fit to 10,000 responses of the source-model to stimuli obscured by von Mises noise. In <bold>B-F</bold> we consider a stimulus-orientation <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> (blue line). (<bold>A</bold>) The average (lines) and standard deviation (filled area) of the FI over orientations, for the source (green) and information-limited (purple) models, as a function of random subpopulations, starting with ten neurons, and gradually reintroducing missing neurons. Dashed black line indicates the theoretical upper bound. (<bold>B</bold>) The sum of the firing rates of the modulated IP-CM for all indices <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (lines) as a function of orientation, with three modulated IP-CMs highlighted (red, yellow, and orange lines) corresponding to the highlighted indices in <bold>C</bold>. (<bold>C</bold>) The index-probability curves (lines) of the IP-CM for indices <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and the intersection (red, yellow, and orange circles) of the stimulus with three curves (orange, yellow, and orange lines). (<bold>D-F</bold>) Three responses from the yellow (<bold>D</bold>; yellow points), red (<bold>E</bold>; red points), and orange modulated IP-CMs (<bold>F</bold>; orange points) indicated in <bold>C</bold>. For each response we plot the posterior based on the source model (green line) and the information-limited model (purple line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-fig6-v3.tif"/></fig><p>Even though the neurons in the source model are uncorrelated, sensory noise ensures that the encoding model <inline-formula><mml:math id="inf207"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">𝑑</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> contains information-limiting correlations that bound the FI about <inline-formula><mml:math id="inf208"><mml:mi>x</mml:mi></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib53">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Kanitscheider et al., 2015b</xref>). Information-limiting correlations can be small and difficult to capture, and to understand how CMs learn in the presence of information-limiting noise correlations, we fit a von Mises IP-CM <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> mixture components to <inline-formula><mml:math id="inf211"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> responses from the information-limited model <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="fig6">Figure 6A</xref> (purple) shows that the FI of the learned CM <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> appears to saturate near the precision of the sensory noise, indicating that the learned CM approximates the information-limiting correlations present in <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To understand how the learned CM approximates these information-limiting correlations, we study the relation between the latent structure of the model and how it generates population activity. For an IP-CM, the orientation-dependent index-probabilities may be expressed as <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf216"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the tuning curve of the <inline-formula><mml:math id="inf217"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron under component <inline-formula><mml:math id="inf218"><mml:mi>k</mml:mi></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig6">Figure 6B</xref>, we plot the sum of the tuning curves <inline-formula><mml:math id="inf219"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for each component <inline-formula><mml:math id="inf220"><mml:mi>k</mml:mi></mml:math></inline-formula> as a function of orientation, and we see that each component concentrates the tuning of the population around a particular orientation. This encourages the probability of each component to also concentrate around a particular orientation, and in <xref ref-type="fig" rid="fig6">Figure 6C</xref> we see that, given the true orientation <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, there are three components with probabilities substantially greater than 0.</p><p>Because there are essentially three components that are relevant to the responses of the IP-CM to the true orientation <inline-formula><mml:math id="inf222"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, generating a response from the CM approximately reduces to generating a response from one of the three possible component IP distributions. In <xref ref-type="fig" rid="fig6">Figure 6D–F</xref>, we depict a response to <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> from each of the three component IP distributions, as well as the optimal posterior based on the learned IP-CM (purple lines), and a suboptimal posterior based on the source model (i.e. ignoring noise correlations; green lines). We observe that the trial-to-trial variability of the learned IP-CM results in random shifts of the peak neural activity away from the true orientation, thus limiting information. Furthermore, when the response of the population is concentrated at the true orientation (<xref ref-type="fig" rid="fig6">Figure 6E</xref>), the suboptimal posterior assigns a high probability to the true orientation, whereas when the responses are biased away from the true orientation (<xref ref-type="fig" rid="fig6">Figure 6D and F</xref>) the suboptimal posterior assigns nearly 0 probability to the true orientation. This is in contrast to the optimal posterior, which always assigns a significant probability to the true orientation.</p><p>In summary, CMs can effectively approximate information-limiting correlations, and the simple latent structure of CMs could help reveal the presence of information-limiting correlations in data.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We introduced a latent variable exponential family formulation of Poisson mixtures. We showed how this formulation allows us to effectively extend Poisson mixtures both to capture sub-Poisson variability, and to incorporate stimulus dependence using conditional mixtures. Our analyses and simulations showed that these conditional mixtures (CMs) can be fit efficiently and recover ground truth models in synthetic data, capture a wide range of V1 response statistics in real data, and can be easily inverted to obtain accurate Bayesian decoding that is competitive with nonlinear decoders, while using orders of magnitude less parameters. In addition, we illustrated how the latent structure of CMs can represent a fundamental feature of the neural code, namely information-limiting correlations.</p><p>Our framework is particularly relevant for probabilistic theories of neural coding based on the theory of exponential families (<xref ref-type="bibr" rid="bib6">Beck et al., 2007</xref>), which include theories that address the linearity of Bayesian inference in neural circuits (<xref ref-type="bibr" rid="bib42">Ma et al., 2006</xref>), the role of phenomena such as divisive normalization in neural computation (<xref ref-type="bibr" rid="bib8">Beck et al., 2011a</xref>), Bayesian inference about dynamic stimuli (<xref ref-type="bibr" rid="bib45">Makin et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Sokoloski, 2017</xref>), and the metabolic efficiency of neural coding (<xref ref-type="bibr" rid="bib25">Ganguli and Simoncelli, 2014</xref>; <xref ref-type="bibr" rid="bib95">Yerxa et al., 2020</xref>). These theories have proven difficult to validate quantitatively with neural data due to a lack of statistical models which are both compatible with their exponential family formulation (see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>), and can model correlated activity in recordings of large neural populations. Our work suggests that CMs can overcome these difficulties, and help connect this rich mathematical theory of neural coding with the state-of-the-art in parallel recording technologies.</p><p>CMs are not limited to modelling neural responses to stimuli and can model how arbitrary experimental variables modulate neural variability and covariability. Examples of experimental variables that have measurable effects on neural covariability include the spatial and temporal context around a stimulus (<xref ref-type="bibr" rid="bib77">Snyder et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Snow et al., 2016</xref>; <xref ref-type="bibr" rid="bib76">Snow et al., 2017</xref>; <xref ref-type="bibr" rid="bib24">Festa et al., 2020</xref>), as well as task-variables and the attentional state of the animal (<xref ref-type="bibr" rid="bib15">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib51">Mitchell et al., 2009</xref>; <xref ref-type="bibr" rid="bib64">Ruff and Cohen, 2014</xref>; <xref ref-type="bibr" rid="bib48">Maunsell, 2015</xref>; <xref ref-type="bibr" rid="bib62">Rabinowitz et al., 2015</xref>; <xref ref-type="bibr" rid="bib87">Verhoef and Maunsell, 2017</xref>; <xref ref-type="bibr" rid="bib11">Bondy et al., 2018</xref>). Each of these variables could be incorporated into a CM by either replacing the stimulus-variable in our equations with the variable of interest, or combining it with the stimulus-variable to construct a CM with multivariate dependence. This would allow researchers to explore how the stimulus and the experimental variables mutually interact to shape variability and covariability in large populations of neurons.</p><p>To understand how this variability and covariability effects neural coding, latent variable models such as CMs are often applied to extract interpretable features of the neural code from data (<xref ref-type="bibr" rid="bib92">Whiteway and Butts, 2019</xref>). The latent states of a CM provide a soft classification of neural activity, and we may apply CMs to model how an experimental variable modulates the class membership of population activity. In the studies on experimental variables listed above, models of neural activity yielded predictions of perceptual and behavioural performance. Because CMs support Bayesian decoding, a CM can also make predictions about how a particular class of neurons is likely to modulate perception and behaviour, and we may then test these predictions with experimental interventions on the neurons themselves (<xref ref-type="bibr" rid="bib56">Panzeri et al., 2017</xref>). In this manner, we believe CMs could form a critical part of a rigorous, Bayesian framework for ‘cracking the neural code’ in large populations of neurons.</p><p>Outside of the framework of mixture models, there are broader possibilities for designing conditional, latent-variable models which have the minimal, exponential family structure of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, yet for which the latent variable is not a finite index. We make use of finite mixture models in this paper primarily because mixture models are analytically tractable, even when mixing Poisson distributions. In contrast, models with Gaussian latent variables are analytically tractable when the observations are also Gaussian, but not in general. Nevertheless, if the relevant formulae and computations can be effectively approximated, then many of the advantages of CMs could be preserved even when using continuous latent variables. For example, if the expectation step in our EM algorithm does not have a closed-form expression, it might be possible to approximate it with contrastive divergence (<xref ref-type="bibr" rid="bib32">Hinton, 2002</xref>).</p><p>In our applications, we considered one-dimensional stimuli and implemented the stimulus-dependence of the CM parameters with linearly parameterized functions. Nevertheless, this stimulus dependence can be implemented by arbitrary parametric functions of high-dimensional variables such as deep neural networks, and CMs can also incorporate history-dependence via recurrent connectivity (see Appendix 4). As such, CMs have the potential to integrate encoding models of higher cortical areas (<xref ref-type="bibr" rid="bib94">Yamins et al., 2014</xref>) with models of the temporal features of the neural code (<xref ref-type="bibr" rid="bib58">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">Park et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Runyan et al., 2017</xref>), towards analyzing the neural code in dynamic, correlated neural populations in higher cortex. Finally, outside of neuroscience, high-dimensional count data exists in many fields such as corpus linguistics and genomics (<xref ref-type="bibr" rid="bib33">Inouye et al., 2017</xref>), and researchers who aim to understand how this data depends on history or additional variables could benefit from our techniques.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Notation</title><p>We use capital, bold letters (e.g. <inline-formula><mml:math id="inf224"><mml:mi mathvariant="bold">𝚯</mml:mi></mml:math></inline-formula>) to indicate matrices; small, bold letters (e.g. <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) to indicate vectors; and regular letters (e.g. <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) to indicate scalars. We use subscript capital letters to indicate the role of a given variable, so that, in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> for example, <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the natural parameters that bias the index-probabilities, <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the baseline natural parameters of the neural firing rates, and <inline-formula><mml:math id="inf229"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the matrix of parameters through which the indices and rates interact.</p><p>We denote the <inline-formula><mml:math id="inf230"><mml:mi>i</mml:mi></mml:math></inline-formula> th element of a vector <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, or e.g. of the vector <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We denote the <inline-formula><mml:math id="inf235"><mml:mi>i</mml:mi></mml:math></inline-formula> th row or <inline-formula><mml:math id="inf236"><mml:mi>j</mml:mi></mml:math></inline-formula> th column of <inline-formula><mml:math id="inf237"><mml:mi mathvariant="bold">𝚯</mml:mi></mml:math></inline-formula> by <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively, and always state whether we are considering a row or column of the given matrix. When referring to the <inline-formula><mml:math id="inf240"><mml:mi>j</mml:mi></mml:math></inline-formula>th element of a vector <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> indexed by <inline-formula><mml:math id="inf242"><mml:mi>i</mml:mi></mml:math></inline-formula>, we write <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, when indexing data points from a sample, or parameters that are tied to individual data points, we use parenthesized, superscript letters, e.g. <inline-formula><mml:math id="inf244"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, or <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2"><title>Poisson mixtures and their moments</title><p>The following derivations were presented in a more general form in <xref ref-type="bibr" rid="bib37">Karlis and Meligkotsidou, 2007</xref>, but we present the simpler case here for completeness. A Poisson distribution has the form <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf247"><mml:mi>n</mml:mi></mml:math></inline-formula> is the count and λ is the rate (in our case, spike count and firing rate, respectively). We may use a Poisson model to define a distribution over <inline-formula><mml:math id="inf248"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> spike counts <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by supposing that the neurons generate spikes independently of one another, leading to the independent Poisson model <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with firing rates <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, if we consider the <inline-formula><mml:math id="inf252"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> rate vectors <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf254"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> weights <inline-formula><mml:math id="inf255"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf256"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf257"><mml:mi>k</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf258"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we then define a mixture of Poisson distributions as a latent variable model <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf261"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>The mean <inline-formula><mml:math id="inf262"><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the <inline-formula><mml:math id="inf263"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron of a mixture of independent Poisson distributions is<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The variance <inline-formula><mml:math id="inf264"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf265"><mml:mi>i</mml:mi></mml:math></inline-formula> is<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf266"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the variance of the <inline-formula><mml:math id="inf267"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron under the <inline-formula><mml:math id="inf268"><mml:mi>k</mml:mi></mml:math></inline-formula> th component distribution, that is the variance of <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and where <inline-formula><mml:math id="inf270"><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf271"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> both follow from the fact that a distribution’s variance equals the difference between its second moment and squared first moment.</p><p>The covariance <inline-formula><mml:math id="inf272"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> between spike-counts <italic>n</italic><sub><italic>i</italic></sub> and <italic>n</italic><sub><italic>j</italic></sub> for <inline-formula><mml:math id="inf273"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> is then<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Observe that if <inline-formula><mml:math id="inf274"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="inf275"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is simply the sample covariance between <inline-formula><mml:math id="inf276"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf277"><mml:mi>j</mml:mi></mml:math></inline-formula>, where the sample is composed of the rate components of the <inline-formula><mml:math id="inf278"><mml:mi>i</mml:mi></mml:math></inline-formula> th and <inline-formula><mml:math id="inf279"><mml:mi>j</mml:mi></mml:math></inline-formula> th neurons. <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> thus implies that Poisson mixtures can model arbitrary covariances. Nevertheless, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> shows that the variance of individual neurons is restricted to being larger than their means.</p></sec><sec id="s4-3"><title>Exponential family mixture models</title><p>In this section, we show that the latent variable form for Poisson mixtures we introduced above is a member of the class of models known as exponential families. An exponential family distribution <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over some data <inline-formula><mml:math id="inf281"><mml:mi>x</mml:mi></mml:math></inline-formula> has the form <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are the so-called natural parameters, <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector-valued function of the data called the sufficient statistic, <inline-formula><mml:math id="inf285"><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a scalar-valued function called the base measure, and <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo>∫</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the log-partition function (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>). In the context of Poisson mixture models, we note that an independent Poisson model <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is an exponential family, with natural parameters <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> given by <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, base measure <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and sufficient statistic <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and log-partition function <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Moreover, the distribution of component indices <inline-formula><mml:math id="inf293"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (also known as a categorical distribution) also has an exponential family form, with natural parameters <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf295"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, sufficient statistic <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, base measure <inline-formula><mml:math id="inf297"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and log-partition function <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that in both cases, the exponential parameters are well-defined only if the rates and weights are strictly greater than 0 — in practice, however, this is not a significant limitation.</p><p>We claim that the joint distribution of a multivariate Poisson mixture model <inline-formula><mml:math id="inf299"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be reparameterized in the exponential family form<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the log-partition function of <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To show this, we show how to express the natural parameters <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as (invertible) functions of the component rate vectors <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the weights <inline-formula><mml:math id="inf305"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>. In particular, we set<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf306"><mml:mi>log</mml:mi></mml:math></inline-formula> is applied element-wise. Then, for <inline-formula><mml:math id="inf307"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we set the <inline-formula><mml:math id="inf308"><mml:mi>k</mml:mi></mml:math></inline-formula> th row <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and the <inline-formula><mml:math id="inf311"><mml:mi>k</mml:mi></mml:math></inline-formula> th element of <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This reparameterization may then be checked by substituting <xref ref-type="disp-formula" rid="equ9 equ10 equ11">Equations 9, 10, and 11</xref> into <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> to recover the joint distribution of the mixture model <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; for a more explicit derivation see <xref ref-type="bibr" rid="bib79">Sokoloski, 2019</xref>.</p><p>The equation for <inline-formula><mml:math id="inf314"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> ensures that the index-probabilities are given by<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Consequently, the component distributions in exponential family form are given by<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Observe that <inline-formula><mml:math id="inf315"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a multivariate Poisson distribution with parameters <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, so that for <inline-formula><mml:math id="inf317"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the parameters are the sum of <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and row <inline-formula><mml:math id="inf319"><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Because the exponential family parameters are the logarithms of the firing rates of <inline-formula><mml:math id="inf321"><mml:mi mathvariant="bold">𝐧</mml:mi></mml:math></inline-formula>, each row of <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> modulates the firing rates of <inline-formula><mml:math id="inf323"><mml:mi mathvariant="bold">𝐧</mml:mi></mml:math></inline-formula> multiplicatively. When <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> depends on a stimulus and we consider the component distributions <inline-formula><mml:math id="inf325"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, each row of <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> then scales the tuning curves of the baseline population (i.e. <inline-formula><mml:math id="inf327"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf328"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>); in the neuroscience literature, such scaling factors are typically referred to as gain modulations.</p><p>The exponential family form has many advantages. However, it has a less intuitive relationship with the statistics of the model such as the mean and covariance. The most straightforward method to compute these statistics given a model in exponential family form is to first reparameterize it in terms of the weights and component rates, and then evaluate <xref ref-type="disp-formula" rid="equ5 equ6 equ7">Equations 5, 6, and 7</xref>.</p></sec><sec id="s4-4"><title>CoM-Poisson distributions and their mixtures</title><p>Conway-Maxwell (CoM) Poisson distributions decouple the location and shape of count distributions (<xref ref-type="bibr" rid="bib73">Shmueli et al., 2005</xref>; <xref ref-type="bibr" rid="bib82">Stevenson, 2016</xref>; <xref ref-type="bibr" rid="bib14">Chanialidis et al., 2018</xref>). A CoM Poisson model has the form <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The floor function <inline-formula><mml:math id="inf330"><mml:mrow><mml:mo>⌊</mml:mo><mml:mi>λ</mml:mi><mml:mo>⌋</mml:mo></mml:mrow></mml:math></inline-formula> of the location parameter λ is the mode of the given distribution. With regards to the shape parameter ν, <inline-formula><mml:math id="inf331"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a Poisson distribution with rate λ when <inline-formula><mml:math id="inf332"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and is under- or over-dispersed when <inline-formula><mml:math id="inf333"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf334"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, respectively. A CoM-Poisson model <inline-formula><mml:math id="inf335"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is also an exponential family, with natural parameters <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ν</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, sufficient statistic <inline-formula><mml:math id="inf337"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and base measure <inline-formula><mml:math id="inf338"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The log-partition function does not have a closed-form expression, but it can be effectively approximated by truncating the series <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib73">Shmueli et al., 2005</xref>). More generally, when we consider a product of independent CoM-Poisson distributions, we denote its log-partition function by <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are the parameters of the <inline-formula><mml:math id="inf342"><mml:mi>i</mml:mi></mml:math></inline-formula> th CoM-Poisson distribution. In this case we can also approximate the log-partition function <inline-formula><mml:math id="inf343"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></inline-formula> by truncating the <inline-formula><mml:math id="inf344"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> constituent series <inline-formula><mml:math id="inf345"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>!</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> in parallel.</p><p>We define a multivariate CoM-based (CB) mixture as<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf346"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐥𝐟</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector of log-factorials of the individual spike-counts, and <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the log-partition function. This form ensures that the index-probabilities satisfy<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and consequently that each component distribution <inline-formula><mml:math id="inf348"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a product of independent CoM Poisson distributions given by<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Observe that, whereas the parameters <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf350"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depend on the index <inline-formula><mml:math id="inf351"><mml:mi>k</mml:mi></mml:math></inline-formula>, the parameters <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf353"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are independent of the index and act exclusively as biases. Therefore, realizing different indices <inline-formula><mml:math id="inf354"><mml:mi>k</mml:mi></mml:math></inline-formula> has the effect increasing or decreasing the location parameters, and thus the modes of the corresponding CoM-Poisson distributions. As such, although the different components of a CB mixture are not simply rescaled versions of the first component <inline-formula><mml:math id="inf355"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, in practice they behave approximately in this manner.</p><p>The moments of a CoM-Poisson distribution are not available in closed-form, yet they can also be effectively approximated through truncation. We begin by computing approximate means <inline-formula><mml:math id="inf356"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and variances <inline-formula><mml:math id="inf357"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of <inline-formula><mml:math id="inf358"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through truncation, and then the mean of <italic>n</italic><sub><italic>i</italic></sub> is <inline-formula><mml:math id="inf359"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and its variance is<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf360"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Similarly to <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, the covariance <inline-formula><mml:math id="inf361"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between <italic>n</italic><sub><italic>i</italic></sub> and <italic>n</italic><sub><italic>j</italic></sub> is <inline-formula><mml:math id="inf362"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>By comparing <xref ref-type="disp-formula" rid="equ6 equ17">Equations 6 and 17</xref>, we see that the CB mixture may address the limitations on the variances <inline-formula><mml:math id="inf363"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the IP mixture by setting the average variance <inline-formula><mml:math id="inf364"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the components in <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> to be small, while holding the value of the means <inline-formula><mml:math id="inf365"><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> fixed, and ensuring that the means of the components <inline-formula><mml:math id="inf366"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> cover a wide range of values to achieve the desired values of <inline-formula><mml:math id="inf367"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf368"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Solving the parameters of a CB mixture for a desired covariance matrix is unfortunately not possible since we lack closed-form expressions for the means and variances. Nevertheless, we may justify the effectiveness of the CB strategy by considering the approximations of the components means and variances <inline-formula><mml:math id="inf369"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf370"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≈</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></inline-formula>, which hold when neither <inline-formula><mml:math id="inf371"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf372"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are too small (<xref ref-type="bibr" rid="bib14">Chanialidis et al., 2018</xref>). Based on these approximations, observe that when <inline-formula><mml:math id="inf373"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is large, <inline-formula><mml:math id="inf374"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is small, whereas <inline-formula><mml:math id="inf375"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is more or less unaffected. Therefore, in the regime where these approximations hold, a small value for <inline-formula><mml:math id="inf376"><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> can be achieved by reducing the parameters <inline-formula><mml:math id="inf377"><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, without significantly restricting the values of <inline-formula><mml:math id="inf378"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf379"><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-5"><title>Fisher information of a minimal CM</title><p>The Fisher information (FI) of an encoding model <inline-formula><mml:math id="inf380"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf381"><mml:mi>x</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf382"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∂</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib16">Cover and Thomas, 2006</xref>). With regard to the FI of a minimal CM,<disp-formula id="equ18"><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> follows from the chain rule and properties of the log-partition function (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>). Therefore<disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf384"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝚺</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the covariance matrix of <inline-formula><mml:math id="inf385"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Moreover, because <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>), the FI of a minimal CM may also be expressed as <inline-formula><mml:math id="inf387"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo>∂</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝝁</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝚺</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:msub><mml:mo>∂</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝝁</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which is the linear Fisher information (<xref ref-type="bibr" rid="bib9">Beck et al., 2011b</xref>).</p><p>Note that when calculating the FI or other quantities based on the covariance matrix, IP-CMs have the advantage that their covariance matrices tend to have large diagonal elements and are thus inherently well-conditioned. Because decoding performance is not significantly different between IP- and CB-CMs (see <xref ref-type="table" rid="table3">Table 3</xref>), IP-CMs may be preferable when well-conditioned covariance matrices are critical. Nevertheless, the covariance matrices of CB mixtures can be made well-conditioned by applying standard techniques.</p></sec><sec id="s4-6"><title>Expectation-maximization for CMs</title><p>Expectation-maximization (EM) is an algorithm that maximizes the likelihood of a latent variable model given data by iterating two steps: generating model-based expectations of the latent variables, and maximizing the complete log-likelihood of the model given the data and latent expectations. Although the maximization step optimizes the <italic>complete</italic> log-likelihood, each iteration of EM is guaranteed to not decrease the <italic>data</italic> log-likelihood as well (<xref ref-type="bibr" rid="bib54">Neal and Hinton, 1998</xref>).</p><p>EM is arguably the most widely applied algorithm for fitting finite mixture models (<xref ref-type="bibr" rid="bib49">McLachlan et al., 2019</xref>). As a form of latent variable exponential family, the expectation step for a finite mixture model reduces to computing average sufficient statistics, and the maximization step is a convex optimization problem (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>). In general, the average sufficient statistics, or mean parameters, correspond to (are dual to) the natural parameters of an exponential family, and where we denote natural parameters with θ, we denote their corresponding mean parameters with η.</p><p>Suppose we are given a dataset <inline-formula><mml:math id="inf388"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of neural spike-counts, and a CB mixture with natural parameters <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf392"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>). The expectation step for this model reduces to computing the data-dependent mean parameters <inline-formula><mml:math id="inf393"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> given by<disp-formula id="equ20"><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>for all <inline-formula><mml:math id="inf394"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The mean parameters <inline-formula><mml:math id="inf395"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are the averages of the sufficient statistic <inline-formula><mml:math id="inf396"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜹</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> under the distribution <inline-formula><mml:math id="inf397"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and are what we use to complete the log-likelihood since we do not observe <inline-formula><mml:math id="inf398"><mml:mi>k</mml:mi></mml:math></inline-formula>.</p><p>Given <inline-formula><mml:math id="inf399"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, the maximization step of a CB mixture thus reduces to maximizing the complete log-likelihood <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where we substitute <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> into the place of <inline-formula><mml:math id="inf402"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>, such that<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mstyle><mml:mspace linebreak="newline"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This objective may be maximized in closed-form for an IP mixture (<xref ref-type="bibr" rid="bib37">Karlis and Meligkotsidou, 2007</xref>), but this is not the case when the model has CoM-Poisson shape parameters or depends on the stimulus. Nevertheless, solving the resulting maximization step is still a convex optimization problem (<xref ref-type="bibr" rid="bib89">Wainwright and Jordan, 2008</xref>), and may be approximately solved with gradient ascent. Doing so requires that we first compute the mean parameters <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf404"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>N</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf405"><mml:msub><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf406"><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that are dual to <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf410"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively.</p><p>We compute the mean parameters by evaluating<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>†</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>†</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>†</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf411"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf412"><mml:mi>k</mml:mi></mml:math></inline-formula> th element of <inline-formula><mml:math id="inf413"><mml:msub><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf414"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf415"><mml:mi>j</mml:mi></mml:math></inline-formula> th element of <inline-formula><mml:math id="inf416"><mml:msub><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf417"><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> is the <inline-formula><mml:math id="inf418"><mml:mi>j</mml:mi></mml:math></inline-formula> th element of <inline-formula><mml:math id="inf419"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>N</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf420"><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id="inf421"><mml:mi>j</mml:mi></mml:math></inline-formula> th element of the <inline-formula><mml:math id="inf422"><mml:mi>k</mml:mi></mml:math></inline-formula> th column of <inline-formula><mml:math id="inf423"><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Note as well that we truncate the series <inline-formula><mml:math id="inf424"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mpadded width="+5pt"><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf425"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo rspace="7.5pt">!</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to approximate <inline-formula><mml:math id="inf426"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf427"><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula>. Given these mean parameters, we may then express the gradients of <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where ⊗ is the outer product operator, and where the second term in each equation follows from the fact that the derivative of <inline-formula><mml:math id="inf429"><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, or <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> yields the dual parameters <inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf435"><mml:msubsup><mml:mi mathvariant="bold-italic">𝜼</mml:mi><mml:mi>N</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf436"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf437"><mml:msub><mml:mi mathvariant="bold">𝐇</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. By ascending the gradients of <inline-formula><mml:math id="inf438"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msubsup><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> until convergence, we approximate a single iteration of the EM algorithm for a CB mixture.</p><p>Finally, if our dataset <inline-formula><mml:math id="inf439"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> includes stimuli <inline-formula><mml:math id="inf440"><mml:mi>x</mml:mi></mml:math></inline-formula>, and the parameters <inline-formula><mml:math id="inf441"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> depend on the stimulus, then the gradients of the parameters of <inline-formula><mml:math id="inf442"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> must also be computed. For a von Mises CM where <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the gradients are given by<disp-formula id="equ24"><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the output of <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf446"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>. Although in this paper we restrict our applications to Von Mises or discrete tuning curves for one-dimensional stimuli, this formalism can be readily extended to the case where the baseline parameters <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are a generic nonlinear function of the stimulus, represented by a deep neural network. Then, the gradients of the parameters of <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> can be computed through backpropagation, and <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the error that must be backpropagated through the network to compute the gradients.</p><p>If we ignore stimulus dependence, the single most computationally intensive operation in each gradient ascent step is the computation of the outer product when evaluating <inline-formula><mml:math id="inf450"><mml:mrow><mml:msub><mml:mo>∂</mml:mo><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which has a time complexity of <inline-formula><mml:math id="inf451"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As such, the training algorithm scales linearly in the number of neurons, and CMs could realistically be applied to populations of tens to hundreds of thousands of neurons. That being said, larger values of <inline-formula><mml:math id="inf452"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> will typically be required to maximize performance in larger populations, and fitting the model to larger populations typically requires larger datasets and more EM iterations.</p></sec><sec id="s4-7"><title>CM initialization and training procedures</title><p>To fit a CM to a dataset <inline-formula><mml:math id="inf453"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we first initialize the CM and then optimize its parameters with our previously described EM algorithm. Naturally, initialization depends on exactly which form of CM we consider, but in general we first initialize the baseline parameters <inline-formula><mml:math id="inf454"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, then add the categorical parameters <inline-formula><mml:math id="inf455"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and mixture component parameters <inline-formula><mml:math id="inf456"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. When training CB-CMs we always first train an IP-CM, and so the initialization procedure remains the same for IP and CB models.</p><p>To initialize a von Mises CM with <inline-formula><mml:math id="inf457"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> neurons, we first fit <inline-formula><mml:math id="inf458"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> independent, von Mises-tuned neurons by maximizing the log-likelihood <inline-formula><mml:math id="inf459"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is a convex optimization problem and so can be easily solved by gradient ascent, in particular by following the gradients<disp-formula id="equ25"><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>⊗</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>to convergence. For both discrete and maximal CMs, where there are <inline-formula><mml:math id="inf461"><mml:msub><mml:mi>d</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:math></inline-formula> distinct stimuli, we initialize <inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by computing the average rate vector at each stimulus-condition and creating a lookup table for these rate vectors. Formally, where <italic>x</italic><sub><italic>l</italic></sub> is the <inline-formula><mml:math id="inf463"><mml:mi>l</mml:mi></mml:math></inline-formula> th stimulus value for <inline-formula><mml:math id="inf464"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>l</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we may express the <inline-formula><mml:math id="inf465"><mml:mi>l</mml:mi></mml:math></inline-formula> th rate vector as <inline-formula><mml:math id="inf466"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝝀</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msubsup></mml:mstyle><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf467"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is one when <inline-formula><mml:math id="inf468"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and 0 otherwise. We then construct a lookup table for these rate vectors in exponential family form by setting <inline-formula><mml:math id="inf469"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and by setting the <inline-formula><mml:math id="inf470"><mml:mi>l</mml:mi></mml:math></inline-formula> th row <inline-formula><mml:math id="inf471"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In general, we initialize the parameters <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by sampling the weights <inline-formula><mml:math id="inf475"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> of a categorical distribution from a Dirichlet distribution with a constant concentration of 2, and converting the weights into the natural parameters of a categorical distribution <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For discrete and maximal CMs, we initialize the modulations <inline-formula><mml:math id="inf477"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by generating each element of <inline-formula><mml:math id="inf478"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from a uniform distribution over the range <inline-formula><mml:math id="inf479"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>0.0001</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. For von Mises CMs we initialize each row <inline-formula><mml:math id="inf480"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf481"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as shifted sinusoidal functions of the preferred stimuli of the independent von Mises neurons. That is, given <inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf483"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we compute the preferred stimulus of the <inline-formula><mml:math id="inf484"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron given by <inline-formula><mml:math id="inf485"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf486"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf487"><mml:mi>i</mml:mi></mml:math></inline-formula> th row of <inline-formula><mml:math id="inf488"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We then set the <inline-formula><mml:math id="inf489"><mml:mi>i</mml:mi></mml:math></inline-formula> th element <inline-formula><mml:math id="inf490"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf491"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mn>360</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Initializing von Mises CMs in this way ensures that each modulation has a unique peak as a function of preferred stimuli, which helps differentiate the modulations from each other, and in our experience improves training speed.</p><p>With regard to training, the expectation step in our EM algorithm may be computed directly, and so the only challenge is solving the maximization step. Although the optimal solution strategy depends on the details of the model and data in question, in the context of this paper we settled on a strategy that is sufficient for all simulations we perform. For each model we perform a total of <inline-formula><mml:math id="inf493"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> EM iterations, and for each maximization step we take <inline-formula><mml:math id="inf494"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> gradient ascent steps with the Adam gradient ascent algorithm (<xref ref-type="bibr" rid="bib38">Kingma and Ba, 2014</xref>) with the default momentum parameters (see <xref ref-type="bibr" rid="bib38">Kingma and Ba, 2014</xref>). We restart the Adam algorithm at each iteration of EM and gradually reduce the learning rate. Where <inline-formula><mml:math id="inf495"><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf496"><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0005</mml:mn></mml:mrow></mml:math></inline-formula> are the initial and final learning rates, we set the learning rate <inline-formula><mml:math id="inf497"><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> at EM iteration <inline-formula><mml:math id="inf498"><mml:mi>t</mml:mi></mml:math></inline-formula> to<disp-formula id="equ26"><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where we assume <inline-formula><mml:math id="inf499"><mml:mi>t</mml:mi></mml:math></inline-formula> starts at 0 and ends at <inline-formula><mml:math id="inf500"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Because we must evaluate large numbers of truncated series when working with CB-CMs, training times are typically one to two orders of magnitude greater. To minimize training time of CB-CMs over the <inline-formula><mml:math id="inf501"><mml:msub><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> EM iterations, we therefore first train a IP-CM for <inline-formula><mml:math id="inf502"><mml:mrow><mml:mn>0.8</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> iterations. We then equate the parameters <inline-formula><mml:math id="inf503"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf505"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the IP-CM (see <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>) with a CB-CM (see <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>) and set <inline-formula><mml:math id="inf506"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which ensures that resulting CB model has the same density function <inline-formula><mml:math id="inf507"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the original IP model. We then train the CB-CM for <inline-formula><mml:math id="inf508"><mml:mrow><mml:mn>0.2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> iterations. We found this strategy results in practically no performance loss, while greatly reducing training time.</p></sec><sec id="s4-8"><title>Strategies for choosing the CM form and latent structure</title><p>There are a few choices with regards to the form of the model than one must make when applying a CM: The form of the dependence, whether or not to use the CoM-based (CB) extension, and the number of components <inline-formula><mml:math id="inf509"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>. The form of the dependence is very open-ended, yet should be fairly clear from the problem context: one should use a minimal model if one wishes to make use of its mathematical features, and otherwise a maximal model may provide better performance. If one wishes to interpolate between stimulus conditions, or the number of stimulus-conditions in the data is high, then a continuous stimulus-dependence model (e.g. von Mises tuning curves) should be used, otherwise discrete tuning curves may provide better performance. Finally, if one wishes to model correlations in a complex neural circuit, one may use for example a deep neural network, and induce correlations in the output layer with the theory of CMs.</p><p>Similarly, CB-CMs have clear advantages for modelling individual variability, and as we show in Appendix 2, this includes higher-order variability. Nevertheless, from the perspective of decoding performance, IP-CMs and CB-CMs perform more-or-less equally well, and training CB-CMs is more computationally intensive. As such, IP-CMs may often be the better choice.</p><p>The number of components <inline-formula><mml:math id="inf510"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> provides a fine-grained method of adjusting model performance. If the goal is to maximize predictive encoding performance, then the standard way to do this is to choose a <inline-formula><mml:math id="inf511"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> that maximizes the cross-validated log-likelihood, as we demonstrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Nevertheless, one may rather aim to maximize decoding performance, in which case maximizing the cross-validated log-posterior may be a more appropriate objective. In both cases, for very large populations of neurons, choosing a <inline-formula><mml:math id="inf512"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> solely to maximize performance may be prohibitively, computationally expensive. As demonstrated in <xref ref-type="fig" rid="fig5">Figure 5</xref> and Appendix 3, a small <inline-formula><mml:math id="inf513"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> can achieve a large fraction of the performance gain of the optimal <inline-formula><mml:math id="inf514"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>, and choosing a modest <inline-formula><mml:math id="inf515"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> that achieves qualitatively acceptable performance may prove to be the most productive strategy.</p></sec><sec id="s4-9"><title>CM parameter selection for simulations</title><p>In the section Extended Poisson mixture models capture stimulus-dependent response statistics and the section Conditional mixtures facilitate accurate and efficient decoding of neural responses we considered minimal CB-CMs with randomized parameters <inline-formula><mml:math id="inf516"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf518"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf519"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which for simplicity we refer to as models 1 and 2, respectively. We construct randomized CMs piece by piece, in a similar fashion to our initialization procedure.</p><p>Firstly, where <inline-formula><mml:math id="inf520"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> is the number of neurons, we tile their preferred stimuli <inline-formula><mml:math id="inf521"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> over the circle such that <inline-formula><mml:math id="inf522"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>i</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. We then generate the concentration <inline-formula><mml:math id="inf523"><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and gain <inline-formula><mml:math id="inf524"><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the <inline-formula><mml:math id="inf525"><mml:mi>i</mml:mi></mml:math></inline-formula> th neuron by sampling from normal distributions in log-space, such that <inline-formula><mml:math id="inf526"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf527"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, for von Mises baseline parameters <inline-formula><mml:math id="inf528"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we set each row <inline-formula><mml:math id="inf529"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf530"><mml:msub><mml:mi mathvariant="bold">𝚯</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf531"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and each element <inline-formula><mml:math id="inf532"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf533"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf534"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf535"><mml:msub><mml:mi>ψ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:math></inline-formula> is the logarithm of the modified Bessel function of order 0, which is the log-partition function of the von Mises distribution.</p><p>We then set <inline-formula><mml:math id="inf536"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and generated each element <inline-formula><mml:math id="inf537"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of the modulation matrix <inline-formula><mml:math id="inf538"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the same matter as the gains, such that <inline-formula><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, to generate random CB parameters we generate each element <inline-formula><mml:math id="inf540"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf541"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> from a uniform distribution, such that <inline-formula><mml:math id="inf542"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1.5</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>0.8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Model two entails two more steps. Firstly, when sampling from larger populations of neurons, single modulations often dominate the model activity around certain stimulus values. To suppress this we consider the natural parameters <inline-formula><mml:math id="inf543"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ15">Equation 15</xref>), and compute the maximum value of these natural parameters over the range of stimuli <inline-formula><mml:math id="inf545"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We then set each element <inline-formula><mml:math id="inf546"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the parameters <inline-formula><mml:math id="inf547"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of the CM to <inline-formula><mml:math id="inf548"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>K</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf549"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>K</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:msubsup><mml:mfrac><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, which helps ensure that multiple modulations are active at any given <inline-formula><mml:math id="inf550"><mml:mi>x</mml:mi></mml:math></inline-formula>. Finally, since model two is a discrete CM, we replace the von Mises baseline parameters with discrete baseline parameters, by evaluating <inline-formula><mml:math id="inf551"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> at each of the <inline-formula><mml:math id="inf552"><mml:msub><mml:mi>d</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:math></inline-formula> valid stimulus-conditions, and assemble the resulting collection of natural parameters into a lookup table in the manner we described in our initialization procedures.</p></sec><sec id="s4-10"><title>Decoding models</title><p>When constructing a Bayesian decoder for discrete stimuli, we first estimate the prior <inline-formula><mml:math id="inf553"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by computing the relative frequency of stimulus presentations in the training data. For the given encoding model, we then evaluate <inline-formula><mml:math id="inf554"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at each stimulus condition, and then compute the posterior <inline-formula><mml:math id="inf555"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi mathvariant="bold">𝐧</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> by brute-force normalization of <inline-formula><mml:math id="inf556"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. When training the encoding model used for our Bayesian encoders, we only trained them to maximize encoding performance as previously described, and not to maximize decoding performance.</p><p>We considered two decoding models, namely the linear network and the artificial neural network (ANN) with sigmoid activation functions. In both cases, the input of the network was a neural response vector, and the output the natural parameters <inline-formula><mml:math id="inf557"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of a categorical distribution. The form of the linear network was <inline-formula><mml:math id="inf558"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and is otherwise fully determined by the structure of the data. For the ANN on the other hand, we had to choose both the number of hidden layers, and the number of neurons per hidden layer. We cross-validated the performance of both 1 and 2 hidden layer models, over a range of sizes from 100 to 2000 neurons. We found the performance of the networks with two hidden layers generally exceeded that of those with one hidden layer, and that 700 and 600 hidden neurons was optimal for the awake and anaesthetized networks, respectively.</p><p>Given a dataset <inline-formula><mml:math id="inf559"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we optimized the linear network and the ANN by maximizing <inline-formula><mml:math id="inf560"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐧</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> via stochastic gradient ascent. We again used the Adam optimizer with default momentum parameters, and used a fixed learning rate of 0.0003, and randomly divided the dataset into minibatches of 500 data points. We also used early stopping, where for each fold of our 10-fold cross-validation simulation, we partitioned the dataset into 80% training data, 10% test data, and 10% validation data, and stopped the simulation when performance on the test data declined from epoch to epoch.</p></sec><sec id="s4-11"><title>Experimental design</title><p>Throughout this paper, we demonstrate our methods on two sets of parallel response recordings in macaque primary visual cortex (V1). The stimuli were drifting full contrast gratings at nine distinct orientations spread evenly over the half-circle from 0° to 180° (2° diameter, two cycles per degree, 2.5 Hz drift rate). Stimuli were generated with custom software (EXPO by P. Lennie) and displayed on a cathode ray tube monitor (Hewlett Packard p1230; 1024 × 768 pixels, with ~ 40 cd/m<sup>2</sup> mean luminance and 100 Hz frame rate) viewed at a distance of 110 cm (for anaesthetized dataset) or 60 cm (for awake dataset). Grating orientations were randomly interleaved, each presented for 70 ms (for anaesthetized dataset) or 150 ms (for awake dataset), separated by a uniform gray screen (blank stimulus) for the same duration. Stimuli were centered in the aggregate spatial receptive field of the recorded units.</p><p>Neural activity from the superficial layers of V1 was recorded from a 96 channel microelectrode array (400 μm spacing, 1 mm length). (400 μm spacing, 1 mm length). Standard methods for waveform extraction and pre-processing were applied (see <xref ref-type="bibr" rid="bib4">Aschner et al., 2018</xref>). We computed spike counts in a fixed window with length equal to the stimulus duration, shifted by 50 ms after stimulus onset to account for onset latency. We excluded from further analyses all neurons that were not driven by any stimulus above baseline + 3std.</p><p>In the first dataset, the monkey was awake and performed a fixation task. Methods and protocols are as described in <xref ref-type="bibr" rid="bib24">Festa et al., 2020</xref>. There were <inline-formula><mml:math id="inf561"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>168</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> trials of the responses of <inline-formula><mml:math id="inf562"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>43</mml:mn></mml:mrow></mml:math></inline-formula> neurons in the dataset. We refer to this dataset as the awake V1 dataset.</p><p>In the second dataset, the monkey was anaesthetized and there were <inline-formula><mml:math id="inf563"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> trials of the responses of <inline-formula><mml:math id="inf564"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula> neurons; we refer to this dataset as the anaesthetized V1 dataset. The protocol and general methods employed for the anaesthetized experiment have been described previously (<xref ref-type="bibr" rid="bib74">Smith and Kohn, 2008</xref>).</p><p>All procedures were approved by the Institutional Animal Care and Use Committee of the Albert Einstein College of Medicine, and were in compliance with the guidelines set forth in the National Institutes of Health Guide for the Care and Use of Laboratory Animals under protocols 20180308 and 20180309 for the awake and anaesthetized macaque recordings, respectively.</p></sec><sec id="s4-12"><title>Code</title><p>All code used to run the simulations and generate the figures, as well as the awake and anaesthetized datasets, are available at the Git repository <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sacha-sokoloski/neural-mixtures">https://gitlab.com/sacha-sokoloski/neural-mixtures</ext-link> (<xref ref-type="bibr" rid="bib80">Sokoloski, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:5494d7bd39671f27c7b6c81beaba847303255073;origin=https://gitlab.com/sacha-sokoloski/neural-mixtures;visit=swh:1:snp:7fafbc2dfb95f1baf5ae819b786edf89461ad581;anchor=swh:1:rev:8e82799f8934c47961ea02c5b7c25bd952abb961">swh:1:rev:8e82799f8934c47961ea02c5b7c25bd952abb961</ext-link>). Instructions are provided for installation, and scripts are provided that may be run on alternative datasets with a similar structure to what we have considered in this manuscript without modifying the code.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank all the members of the labs of Ruben Coen-Cagli and Adam Kohn for their regular feedback and support.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Writing - review and editing, Data Collection</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were approved by the Institutional Animal Care and Use Committee of the Albert Einstein College of Medicine, and were in compliance with the guidelines set forth in the National Institutes of Health Guide for the Care and Use of Laboratory Animals under protocols 20180308 and 20180309 for the awake and anaesthetized macaque recordings, respectively.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-64615-transrepform-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data used in this study is available at the Git repository (<ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sacha-sokoloski/neural-mixtures">https://gitlab.com/sacha-sokoloski/neural-mixtures</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:8e82799f8934c47961ea02c5b7c25bd952abb961">https://archive.softwareheritage.org/swh:1:rev:8e82799f8934c47961ea02c5b7c25bd952abb961</ext-link>). This includes experimental data used for model validation, and code for running analyses and simulations.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of correlated variability on the accuracy of a population code</article-title><source>Neural Computation</source><volume>11</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1162/089976699300016827</pub-id><pub-id pub-id-type="pmid">9950724</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Archer</surname> <given-names>EW</given-names></name><name><surname>Koster</surname> <given-names>U</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Low-Dimensional Models of Neural Population Activity in Sensory Cortical Circuits</chapter-title><person-group person-group-type="editor"><name><surname>Ghahramani</surname> <given-names>Z</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name><name><surname>Cortes</surname> <given-names>C</given-names></name><name><surname>Lawrence</surname> <given-names>N. D</given-names></name><name><surname>Weinberger</surname> <given-names>K. Q</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>343</fpage><lpage>351</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arieli</surname> <given-names>A</given-names></name><name><surname>Sterkin</surname> <given-names>A</given-names></name><name><surname>Grinvald</surname> <given-names>A</given-names></name><name><surname>Aertsen</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dynamics of ongoing activity: explanation of the large variability in evoked cortical responses</article-title><source>Science</source><volume>273</volume><fpage>1868</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1126/science.273.5283.1868</pub-id><pub-id pub-id-type="pmid">8791593</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aschner</surname> <given-names>A</given-names></name><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Landy</surname> <given-names>MS</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Temporal contingencies determine whether adaptation strengthens or weakens normalization</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>10129</fpage><lpage>10142</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1131-18.2018</pub-id><pub-id pub-id-type="pmid">30291205</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolo</surname> <given-names>R</given-names></name><name><surname>Saunders</surname> <given-names>RC</given-names></name><name><surname>Mitz</surname> <given-names>AR</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Information-Limiting correlations in large neural populations</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>1668</fpage><lpage>1678</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2072-19.2019</pub-id><pub-id pub-id-type="pmid">31941667</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>J</given-names></name><name><surname>Wj</surname> <given-names>M</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>Computational Neuroscience: Theoretical Insights into Brain Function</chapter-title><person-group person-group-type="editor"><name><surname>Cisek</surname> <given-names>P</given-names></name><name><surname>Drew</surname> <given-names>T</given-names></name><name><surname>Kalaska</surname> <given-names>J. F</given-names></name></person-group><source>Progress in Brain Research</source><publisher-name> Elsevier</publisher-name><fpage>509</fpage><lpage>519</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Hanks</surname> <given-names>T</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Roitman</surname> <given-names>J</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Probabilistic population codes for bayesian decision making</article-title><source>Neuron</source><volume>60</volume><fpage>1142</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.021</pub-id><pub-id pub-id-type="pmid">19109917</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Marginalization in neural circuits with divisive normalization</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>15310</fpage><lpage>15319</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1706-11.2011</pub-id><pub-id pub-id-type="pmid">22031877</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname> <given-names>J</given-names></name><name><surname>Bejjanki</surname> <given-names>VR</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Insights from a simple expression for linear fisher information in a recurrently connected population of spiking neurons</article-title><source>Neural Computation</source><volume>23</volume><fpage>1484</fpage><lpage>1502</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00125</pub-id><pub-id pub-id-type="pmid">21395435</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bondy</surname> <given-names>AG</given-names></name><name><surname>Haefner</surname> <given-names>RM</given-names></name><name><surname>Cumming</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Feedback determines the structure of correlated variability in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>598</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0089-1</pub-id><pub-id pub-id-type="pmid">29483663</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Nadal</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Mutual information, Fisher information, and population coding</article-title><source>Neural Computation</source><volume>10</volume><fpage>1731</fpage><lpage>1757</lpage><pub-id pub-id-type="doi">10.1162/089976698300017115</pub-id><pub-id pub-id-type="pmid">9744895</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cain</surname> <given-names>MK</given-names></name><name><surname>Zhang</surname> <given-names>Z</given-names></name><name><surname>Yuan</surname> <given-names>KH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Univariate and multivariate skewness and kurtosis for measuring nonnormality: prevalence, influence and estimation</article-title><source>Behavior Research Methods</source><volume>49</volume><fpage>1716</fpage><lpage>1735</lpage><pub-id pub-id-type="doi">10.3758/s13428-016-0814-1</pub-id><pub-id pub-id-type="pmid">27752968</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanialidis</surname> <given-names>C</given-names></name><name><surname>Evers</surname> <given-names>L</given-names></name><name><surname>Neocleous</surname> <given-names>T</given-names></name><name><surname>Nobile</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient bayesian inference for COM-Poisson regression models</article-title><source>Statistics and Computing</source><volume>28</volume><fpage>595</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1007/s11222-017-9750-x</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1594</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1038/nn.2439</pub-id><pub-id pub-id-type="pmid">19915566</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname> <given-names>TM</given-names></name><name><surname>Thomas</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Elements of Information Theory</source><edition>Secound ed</edition><publisher-name>Wiley-Interscience</publisher-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowley</surname> <given-names>BR</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stimulus-Driven population activity patterns in macaque primary visual cortex</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005185</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005185</pub-id><pub-id pub-id-type="pmid">27935935</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1500</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1038/nn.3776</pub-id><pub-id pub-id-type="pmid">25151264</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source><publisher-name>Massachusetts Institute of Technology Press</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Doya</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Bayesian Brain: Probabilistic Approaches to Neural Coding</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname> <given-names>AS</given-names></name><name><surname>Berens</surname> <given-names>P</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The effect of noise correlations in populations of diversely tuned neurons</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>14272</fpage><lpage>14283</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2539-11.2011</pub-id><pub-id pub-id-type="pmid">21976512</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname> <given-names>AS</given-names></name><name><surname>Berens</surname> <given-names>P</given-names></name><name><surname>Cotton</surname> <given-names>RJ</given-names></name><name><surname>Subramaniyan</surname> <given-names>M</given-names></name><name><surname>Denfield</surname> <given-names>GH</given-names></name><name><surname>Cadwell</surname> <given-names>CR</given-names></name><name><surname>Smirnakis</surname> <given-names>SM</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>State dependence of noise correlations in macaque primary visual cortex</article-title><source>Neuron</source><volume>82</volume><fpage>235</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.02.006</pub-id><pub-id pub-id-type="pmid">24698278</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname> <given-names>AS</given-names></name><name><surname>Denfield</surname> <given-names>GH</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>On the structure of neuronal population activity under fluctuations in attentional state</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1775</fpage><lpage>1789</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2044-15.2016</pub-id><pub-id pub-id-type="pmid">26843656</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Festa</surname> <given-names>D</given-names></name><name><surname>Aschner</surname> <given-names>A</given-names></name><name><surname>Davila</surname> <given-names>A</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neuronal variability reflects probabilistic inference tuned to natural image statistics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.06.17.142182</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname> <given-names>D</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Efficient sensory encoding and bayesian inference with heterogeneous neural populations</article-title><source>Neural Computation</source><volume>26</volume><fpage>2103</fpage><lpage>2134</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id><pub-id pub-id-type="pmid">25058702</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganmor</surname> <given-names>E</given-names></name><name><surname>Segev</surname> <given-names>R</given-names></name><name><surname>Schneidman</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A thesaurus for a neural population code</article-title><source>eLife</source><volume>4</volume><elocation-id>e06134</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06134</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goris</surname> <given-names>RL</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Partitioning neuronal variability</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>858</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1038/nn.3711</pub-id><pub-id pub-id-type="pmid">24777419</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graf</surname> <given-names>AB</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Decoding the activity of neuronal populations in macaque primary visual cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>239</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1038/nn.2733</pub-id><pub-id pub-id-type="pmid">21217762</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granot-Atedgi</surname> <given-names>E</given-names></name><name><surname>Tkačik</surname> <given-names>G</given-names></name><name><surname>Segev</surname> <given-names>R</given-names></name><name><surname>Schneidman</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stimulus-dependent maximum entropy models of neural population codes</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002922</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002922</pub-id><pub-id pub-id-type="pmid">23516339</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grün</surname> <given-names>B</given-names></name><name><surname>Leisch</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><chapter-title>Finite Mixtures of Generalized Linear Regression Models</chapter-title><person-group person-group-type="editor"><name><surname>Shalabh</surname> <given-names>HC</given-names></name></person-group><source>Recent Advances in Linear Models and Related Areas: Essays in Honour of Helge Toutenburg Heidelberg</source><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>205</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1007/978-3-7908-2064-5</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herz</surname> <given-names>AV</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Stemmler</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Periodic population codes: from a single circular variable to higher dimensions, multiple nested scales, and conceptual spaces</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>99</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.07.005</pub-id><pub-id pub-id-type="pmid">28888183</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Training products of experts by minimizing contrastive divergence</article-title><source>Neural Computation</source><volume>14</volume><fpage>1771</fpage><lpage>1800</lpage><pub-id pub-id-type="doi">10.1162/089976602760128018</pub-id><pub-id pub-id-type="pmid">12180402</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inouye</surname> <given-names>D</given-names></name><name><surname>Yang</surname> <given-names>E</given-names></name><name><surname>Allen</surname> <given-names>G</given-names></name><name><surname>Ravikumar</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A review of multivariate distributions for count data derived from the poisson distribution</article-title><source>WIREs Computational Statistics</source><volume>9</volume><elocation-id>e1398</elocation-id><pub-id pub-id-type="doi">10.1002/wics.1398</pub-id><pub-id pub-id-type="pmid">28983398</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kafashan</surname> <given-names>M</given-names></name><name><surname>Jaffe</surname> <given-names>AW</given-names></name><name><surname>Chettih</surname> <given-names>SN</given-names></name><name><surname>Nogueira</surname> <given-names>R</given-names></name><name><surname>Arandia-Romero</surname> <given-names>I</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Drugowitsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Scaling of sensory information in large neural populations shows signatures of information-limiting correlations</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>473</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-20722-y</pub-id><pub-id pub-id-type="pmid">33473113</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanitscheider</surname> <given-names>I</given-names></name><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Measuring Fisher information accurately in correlated neural populations</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004218</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004218</pub-id><pub-id pub-id-type="pmid">26030735</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanitscheider</surname> <given-names>I</given-names></name><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Origin of information-limiting noise correlations</article-title><source>PNAS</source><volume>112</volume><fpage>E6973</fpage><lpage>E6982</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508738112</pub-id><pub-id pub-id-type="pmid">26621747</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlis</surname> <given-names>D</given-names></name><name><surname>Meligkotsidou</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Finite mixtures of multivariate poisson distributions with application</article-title><source>Journal of Statistical Planning and Inference</source><volume>137</volume><fpage>1942</fpage><lpage>1960</lpage><pub-id pub-id-type="doi">10.1016/j.jspi.2006.07.001</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>D</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name><name><surname>Kanitscheider</surname> <given-names>I</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correlations and neuronal population information</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>237</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013851</pub-id><pub-id pub-id-type="pmid">27145916</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Douglas</surname> <given-names>PK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cognitive computational neuroscience</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1148</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0210-5</pub-id><pub-id pub-id-type="pmid">30127428</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyamzin</surname> <given-names>DR</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Lesica</surname> <given-names>NA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modeling population spike trains with specified Time-Varying spike rates, Trial-to-Trial variability, and pairwise signal and noise correlations</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>144</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00144</pub-id><pub-id pub-id-type="pmid">21152346</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Buesing</surname> <given-names>L</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Bm</surname> <given-names>Y</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011a</year><chapter-title>Empirical Models of Spiking in Neural Populations</chapter-title><person-group person-group-type="editor"><name><surname>Shawe-Taylor</surname> <given-names>J</given-names></name><name><surname>Zemel</surname> <given-names>R. S</given-names></name><name><surname>Bartlett</surname> <given-names>P. L</given-names></name><name><surname>Pereira</surname> <given-names>F</given-names></name><name><surname>Weinberger</surname> <given-names>K. Q</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>1350</fpage><lpage>1358</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2011">2011b</year><chapter-title>How Biased Are Maximum Entropy Models?</chapter-title><person-group person-group-type="editor"><name><surname>Shawe-Taylor</surname> <given-names>J</given-names></name><name><surname>Zemel</surname> <given-names>R. S</given-names></name><name><surname>Bartlett</surname> <given-names>P. L</given-names></name><name><surname>Pereira</surname> <given-names>F</given-names></name><name><surname>Weinberger</surname> <given-names>K. Q</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>2034</fpage><lpage>2042</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname> <given-names>JG</given-names></name><name><surname>Dichter</surname> <given-names>BK</given-names></name><name><surname>Sabes</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning to estimate dynamical state with probabilistic population codes</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004554</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004554</pub-id><pub-id pub-id-type="pmid">26540152</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maoz</surname> <given-names>O</given-names></name><name><surname>Tkačik</surname> <given-names>G</given-names></name><name><surname>Esteki</surname> <given-names>MS</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Schneidman</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning probabilistic neural representations with randomly connected circuits</article-title><source>PNAS</source><volume>117</volume><fpage>25066</fpage><lpage>25073</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912804117</pub-id><pub-id pub-id-type="pmid">32948691</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mardia</surname> <given-names>KV</given-names></name><name><surname>El-atoum</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Bayesian inference for the von Mises-Fisher distribution</article-title><source>Biometrika</source><volume>63</volume><fpage>203</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1093/biomet/63.1.203</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname> <given-names>JHR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal mechanisms of visual attention</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>373</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035431</pub-id><pub-id pub-id-type="pmid">28532368</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLachlan</surname> <given-names>GJ</given-names></name><name><surname>Lee</surname> <given-names>SX</given-names></name><name><surname>Rathnayake</surname> <given-names>SI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Finite mixture models</article-title><source>Annual Review of Statistics and Its Application</source><volume>6</volume><fpage>355</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1146/annurev-statistics-031017-100325</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meshulam</surname> <given-names>L</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Collective behavior of place and Non-place neurons in the hippocampal network</article-title><source>Neuron</source><volume>96</volume><fpage>1178</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.10.027</pub-id><pub-id pub-id-type="pmid">29154129</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname> <given-names>JF</given-names></name><name><surname>Sundberg</surname> <given-names>KA</given-names></name><name><surname>Reynolds</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spatial attention decorrelates intrinsic activity fluctuations in macaque area V4</article-title><source>Neuron</source><volume>63</volume><fpage>879</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.013</pub-id><pub-id pub-id-type="pmid">19778515</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Montijn</surname> <given-names>JS</given-names></name><name><surname>Liu</surname> <given-names>RG</given-names></name><name><surname>Aschner</surname> <given-names>A</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Strong Information-Limiting correlations in early visual areas</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/842724</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Beck</surname> <given-names>J</given-names></name><name><surname>Kanitscheider</surname> <given-names>I</given-names></name><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Latham</surname> <given-names>P</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neal</surname> <given-names>RM</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>A View of the EM Algorithm That Justifies Incremental, Sparse, and Other Variants</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-94-011-5014-9_12</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>N</given-names></name><name><surname>Cossell</surname> <given-names>L</given-names></name><name><surname>Iacaruso</surname> <given-names>MF</given-names></name><name><surname>Ko</surname> <given-names>H</given-names></name><name><surname>Barthó</surname> <given-names>P</given-names></name><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Hofer</surname> <given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Diverse coupling of neurons to populations in sensory cortex</article-title><source>Nature</source><volume>521</volume><fpage>511</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1038/nature14273</pub-id><pub-id pub-id-type="pmid">25849776</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Piasini</surname> <given-names>E</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Fellin</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cracking the neural code for sensory perception by combining statistics, intervention, and behavior</article-title><source>Neuron</source><volume>93</volume><fpage>491</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.036</pub-id><pub-id pub-id-type="pmid">28182905</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>IM</given-names></name><name><surname>Meister</surname> <given-names>ML</given-names></name><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding and decoding in parietal cortex during sensorimotor decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1395</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1038/nn.3800</pub-id><pub-id pub-id-type="pmid">25174005</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Ahmadian</surname> <given-names>Y</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Model-based decoding, information estimation, and change-point detection techniques for multineuron spike trains</article-title><source>Neural Computation</source><volume>23</volume><fpage>1</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00058</pub-id><pub-id pub-id-type="pmid">20964538</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitkow</surname> <given-names>X</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inference in the brain: statistics flowing in redundant population codes</article-title><source>Neuron</source><volume>94</volume><fpage>943</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.028</pub-id><pub-id pub-id-type="pmid">28595050</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probabilistic brains: knowns and unknowns</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1170</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1038/nn.3495</pub-id><pub-id pub-id-type="pmid">23955561</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname> <given-names>NC</given-names></name><name><surname>Goris</surname> <given-names>RL</given-names></name><name><surname>Cohen</surname> <given-names>M</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention stabilizes the shared gain of V4 populations</article-title><source>eLife</source><volume>4</volume><elocation-id>e08998</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08998</pub-id><pub-id pub-id-type="pmid">26523390</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruda</surname> <given-names>K</given-names></name><name><surname>Zylberberg</surname> <given-names>J</given-names></name><name><surname>Field</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ignoring correlated activity causes a failure of retinal population codes</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4605</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18436-2</pub-id><pub-id pub-id-type="pmid">32929073</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Cohen</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attention can either increase or decrease spike count correlations in visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1591</fpage><lpage>1597</lpage><pub-id pub-id-type="doi">10.1038/nn.3835</pub-id><pub-id pub-id-type="pmid">25306550</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumyantsev</surname> <given-names>OI</given-names></name><name><surname>Lecoq</surname> <given-names>JA</given-names></name><name><surname>Hernandez</surname> <given-names>O</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Savall</surname> <given-names>J</given-names></name><name><surname>Chrapkiewicz</surname> <given-names>R</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fundamental bounds on the fidelity of sensory cortical coding</article-title><source>Nature</source><volume>580</volume><fpage>100</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2130-2</pub-id><pub-id pub-id-type="pmid">32238928</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runyan</surname> <given-names>CA</given-names></name><name><surname>Piasini</surname> <given-names>E</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Distinct timescales of population coding across cortex</article-title><source>Nature</source><volume>548</volume><fpage>92</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1038/nature23020</pub-id><pub-id pub-id-type="pmid">28723889</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Gilja</surname> <given-names>V</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Afshar</surname> <given-names>A</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Factor-analysis methods for higher-performance neural prostheses</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>1315</fpage><lpage>1330</lpage><pub-id pub-id-type="doi">10.1152/jn.00097.2009</pub-id><pub-id pub-id-type="pmid">19297518</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname> <given-names>E</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Segev</surname> <given-names>R</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title><source>Nature</source><volume>440</volume><fpage>1007</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1038/nature04701</pub-id><pub-id pub-id-type="pmid">16625187</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Towards the design principles of neural population codes</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>133</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.03.001</pub-id><pub-id pub-id-type="pmid">27016639</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semedo</surname> <given-names>JD</given-names></name><name><surname>Zandvakili</surname> <given-names>A</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical areas interact through a communication subspace</article-title><source>Neuron</source><volume>102</volume><fpage>249</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.026</pub-id><pub-id pub-id-type="pmid">30770252</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname> <given-names>P</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1038/nn1321</pub-id><pub-id pub-id-type="pmid">15452579</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shidara</surname> <given-names>M</given-names></name><name><surname>Mizuhiki</surname> <given-names>T</given-names></name><name><surname>Richmond</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuronal firing in anterior cingulate neurons changes modes across trials in single states of multitrial reward schedules</article-title><source>Experimental Brain Research</source><volume>163</volume><fpage>242</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2232-y</pub-id><pub-id pub-id-type="pmid">15912371</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shmueli</surname> <given-names>G</given-names></name><name><surname>Minka</surname> <given-names>TP</given-names></name><name><surname>Kadane</surname> <given-names>JB</given-names></name><name><surname>Borle</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A useful distribution for fitting discrete data: revival of the Conway–Maxwell–Poisson distribution</article-title><source>Journal of the Royal Statistical Society: Series C</source><volume>54</volume><fpage>127</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9876.2005.00474.x</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatial and temporal scales of neuronal correlation in primary visual cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>12591</fpage><lpage>12603</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2929-08.2008</pub-id><pub-id pub-id-type="pmid">19036953</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snow</surname> <given-names>M</given-names></name><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Specificity and timescales of cortical adaptation as inferences about natural movie statistics</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/16.13.1</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snow</surname> <given-names>M</given-names></name><name><surname>Coen-Cagli</surname> <given-names>R</given-names></name><name><surname>Schwartz</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adaptation in the visual cortex: a case for probing neuronal populations with natural stimuli</article-title><source>F1000Research</source><volume>6</volume><elocation-id>1246</elocation-id><pub-id pub-id-type="doi">10.12688/f1000research.11154.1</pub-id><pub-id pub-id-type="pmid">29034079</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname> <given-names>AC</given-names></name><name><surname>Morais</surname> <given-names>MJ</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Correlations in V1 are reduced by stimulation outside the receptive field</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>11222</fpage><lpage>11227</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0762-14.2014</pub-id><pub-id pub-id-type="pmid">25143603</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokoloski</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Implementing a Bayes filter in a neural circuit: the case of unknown stimulus dynamics</article-title><source>Neural Computation</source><volume>29</volume><fpage>2450</fpage><lpage>2490</lpage><pub-id pub-id-type="doi">10.1162/neco_a_00991</pub-id><pub-id pub-id-type="pmid">28599113</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sokoloski</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Implementing Bayesian Inference with Neural Networks</source><publisher-loc>Dissertation</publisher-loc><publisher-name>University of Leipzig</publisher-name></element-citation></ref><ref id="bib80"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sokoloski</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>Neural-Mixtures</source><ext-link ext-link-type="uri" xlink:href="https://gitlab.com/sacha-sokoloski/neural-mixtures">https://gitlab.com/sacha-sokoloski/neural-mixtures</ext-link></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Yoon</surname> <given-names>H</given-names></name><name><surname>Kang</surname> <given-names>K</given-names></name><name><surname>Shamir</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Population coding in neuronal systems with correlated noise</article-title><source>Physical Review E</source><volume>64</volume><elocation-id>051904</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.64.051904</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname> <given-names>IH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Flexible models for spike count data with both over- and under- dispersion</article-title><source>Journal of Computational Neuroscience</source><volume>41</volume><fpage>29</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1007/s10827-016-0603-y</pub-id><pub-id pub-id-type="pmid">27008191</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sur</surname> <given-names>P</given-names></name><name><surname>Shmueli</surname> <given-names>G</given-names></name><name><surname>Bose</surname> <given-names>S</given-names></name><name><surname>Dubey</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Modeling bimodal discrete data using Conway-Maxwell-Poisson mixture models</article-title><source>Journal of Business &amp; Economic Statistics</source><volume>33</volume><fpage>352</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1080/07350015.2014.949343</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taouali</surname> <given-names>W</given-names></name><name><surname>Benvenuti</surname> <given-names>G</given-names></name><name><surname>Wallisch</surname> <given-names>P</given-names></name><name><surname>Chavane</surname> <given-names>F</given-names></name><name><surname>Perrinet</surname> <given-names>LU</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Testing the odds of inherent vs. observed overdispersion in neural spike counts</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>434</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1152/jn.00194.2015</pub-id><pub-id pub-id-type="pmid">26445864</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname> <given-names>G</given-names></name><name><surname>Marre</surname> <given-names>O</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Amodei</surname> <given-names>D</given-names></name><name><surname>Berry II</surname> <given-names>MJ</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The simplest maximum entropy model for collective behavior in a neural network</article-title><source>Journal of Statistical Mechanics: Theory and Experiment</source><volume>2013</volume><elocation-id>P03011</elocation-id><pub-id pub-id-type="doi">10.1088/1742-5468/2013/03/P03011</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname> <given-names>G</given-names></name><name><surname>Mora</surname> <given-names>T</given-names></name><name><surname>Marre</surname> <given-names>O</given-names></name><name><surname>Amodei</surname> <given-names>D</given-names></name><name><surname>Palmer</surname> <given-names>SE</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Thermodynamics and signatures of criticality in a network of neurons</article-title><source>PNAS</source><volume>112</volume><fpage>11508</fpage><lpage>11513</lpage><pub-id pub-id-type="doi">10.1073/pnas.1514188112</pub-id><pub-id pub-id-type="pmid">26330611</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhoef</surname> <given-names>BE</given-names></name><name><surname>Maunsell</surname> <given-names>JHR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention-related changes in correlated neuronal activity arise from normalization mechanisms</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>969</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1038/nn.4572</pub-id><pub-id pub-id-type="pmid">28553943</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidne</surname> <given-names>M</given-names></name><name><surname>Ahmadian</surname> <given-names>Y</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Kulkarni</surname> <given-names>J</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>E</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Modeling the impact of common noise inputs on the network activity of retinal ganglion cells</article-title><source>Journal of Computational Neuroscience</source><volume>33</volume><fpage>97</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1007/s10827-011-0376-2</pub-id><pub-id pub-id-type="pmid">22203465</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname> <given-names>MJ</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Graphical models, exponential families, and variational inference</article-title><source>Foundations and Trends</source><volume>1</volume><fpage>1</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1561/2200000001</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname> <given-names>EY</given-names></name><name><surname>Cotton</surname> <given-names>RJ</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A neural basis of probabilistic computation in visual cortex</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>122</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0554-5</pub-id><pub-id pub-id-type="pmid">31873286</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>XX</given-names></name><name><surname>Stocker</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A bayesian observer model constrained by efficient coding can explain 'anti-Bayesian' percepts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1038/nn.4105</pub-id><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiteway</surname> <given-names>MR</given-names></name><name><surname>Butts</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The quest for interpretable models of neural population activity</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>86</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.07.004</pub-id><pub-id pub-id-type="pmid">31426024</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiener</surname> <given-names>MC</given-names></name><name><surname>Richmond</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Decoding spike trains instant by instant using order statistics and the mixture-of-Poissons model</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>2394</fpage><lpage>2406</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-06-02394.2003</pub-id><pub-id pub-id-type="pmid">12657699</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Seibert</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yerxa</surname> <given-names>TE</given-names></name><name><surname>Kee</surname> <given-names>E</given-names></name><name><surname>DeWeese</surname> <given-names>MR</given-names></name><name><surname>Cooper</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient sensory coding of multidimensional stimuli</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008146</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008146</pub-id><pub-id pub-id-type="pmid">32970679</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaussian-Process factor analysis for Low-Dimensional Single-Trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id><pub-id pub-id-type="pmid">19357332</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zemel</surname> <given-names>RS</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Probabilistic interpretation of population codes</article-title><source>Neural Computation</source><volume>10</volume><fpage>403</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1162/089976698300017818</pub-id><pub-id pub-id-type="pmid">9472488</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>Y</given-names></name><name><surname>Park</surname> <given-names>IM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variational latent gaussian process for recovering Single-Trial dynamics from population spike trains</article-title><source>Neural Computation</source><volume>29</volume><fpage>1293</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00953</pub-id><pub-id pub-id-type="pmid">28333587</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zohary</surname> <given-names>E</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title><source>Nature</source><volume>370</volume><fpage>140</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1038/370140a0</pub-id><pub-id pub-id-type="pmid">8022482</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Comparing conditional mixtures with factor analysis</title><p>In Conditional mixtures effectively model neural responses in macaque V1, we assess encoding performance with the cross-validated, average log-likelihood of the given conditional mixture (CM) on the given dataset. However, in some cases, one might only be concerned with how well a model captures particular statistics of a dataset. In particular, response models based on Gaussian distributions treat spike-counts as continuous values, and assign positive probability to both negative and non-integer values. Although their log-likelihood performance consequently tends to suffer relative to spike-count models, they can still prove highly effective at capturing the mean and covariance of data.</p><p>Here, we compare CMs with factor analysis (FA), which is widely applied to modelling neural responses (<xref ref-type="bibr" rid="bib67">Santhanam et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Cowley et al., 2016</xref>; <xref ref-type="bibr" rid="bib70">Semedo et al., 2019</xref>). FAs model data as Gaussian distributed, and have a latent structure that facilitates both interpretability and predictive performance. The easiest way to design an encoding model based on FA is with a simple lookup-table, so that we fit an independent FA model at each stimulus condition. This is also how we define maximal CMs, and so to keep our comparison straightforward we compare FA encoding models with maximal CMs. In particular, we compare FA to both independent Poisson (IP) and CoM-Based (CB) maximal CMs on how well they capture response statistics on the two datasets from the article (anaesthetized and awake macaque V1). In general, we trained the CMs with expectation-maximization (EM) as described in Materials and methods, and the FA model with standard EM.</p><p>In <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A and B</xref>, we depict scatter plots that compare the data noise correlations from our awake and anaesthetized datasets at the stimulus orientation <inline-formula><mml:math id="inf565"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>40</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, to the noise correlations learned by CB and IP mixtures, and FA trained on the complete datasets. Each model was defined with <inline-formula><mml:math id="inf566"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> latent states/dimensions. We also state the coefficient of determination <inline-formula><mml:math id="inf567"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for each model, and see that although all models perform comparably on the anaesthetized data, FA has a clear advantage over the mixture models on the awake data. To see if this performance advantage holds on held-out data, in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C and D</xref> we depict the results of 10-fold cross-validation of the coefficient of determination <inline-formula><mml:math id="inf568"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> between the data noise correlations and the various model noise correlations over all nine stimulus orientations, as a function of the number of latant states/dimensions <inline-formula><mml:math id="inf569"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>. We see that the predictive performance advantage of FA on the awake data is small, and that CB-CMs exceed the performance of FA on anaesthetized data. At the same time, FA achieves peak performance on both datasets with a smaller number of parameters. Nevertheless, FA is designed precisely to capture second-order correlations, and that our mixture models achieve comparable performance speaks favourably to the overall strengths of the mixture model approach.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Mixture models capture spike-count correlations.</title><p>(<bold>A</bold>, <bold>B</bold>) Scatter plots of the data noise correlations versus the noise correlations modelled by IP (blue) and CB (red) mixtures, and FA (purple), in both the awake (<bold>A</bold>) and anaesthetized (<bold>B</bold>) datasets at orientation <inline-formula><mml:math id="inf570"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>40</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Each point represent a pair of neurons (<inline-formula><mml:math id="inf571"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>43</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf572"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula> neurons in the awake and anaesthetized datasets, respectively). (<bold>C</bold>, <bold>D</bold>) We evaluate the noise correlation <inline-formula><mml:math id="inf573"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> over all stimulus-orientations with 10-fold cross-validation. We plot the average (lines) and standard error (error bars) of the cross-validated noise correlation <inline-formula><mml:math id="inf574"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> as a function of number of latent states/dimensions <inline-formula><mml:math id="inf575"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>, in both the awake (<bold>C</bold>) and anaesthetized (<bold>D</bold>) datasets. We also indicate the peak performance achieved for each model, and requisite number of latent states/dimensions <inline-formula><mml:math id="inf576"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-app1-fig1-v3.tif"/></fig><p>In <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>, we depict scatter plots between the data Fano factors (FFs) and learned FFs of our models at stimulus orientation <inline-formula><mml:math id="inf577"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>40</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, and find that both the CB mixture and FA almost perfectly capture the data FFs. In <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2C–D</xref>, we see that the CB mixture and FA also achieve good cross-validated <inline-formula><mml:math id="inf578"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> scores on FFs. Unsurprisingly, however, the IP mixture struggles to effectively capture FFs.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Mixture models capture spike-count Fano factors.</title><p>We repeat the analyses from Appendix References Figure 7 on Fano factors (FFs). (<bold>A</bold>, <bold>B</bold>) Scatter plots of the data FFs versus the FFs modelled by IP (blue) and CB mixtures (red), and FA (purple) in both the awake (<bold>A</bold>) and anaesthetized (<bold>B</bold>) datasets at orientation <inline-formula><mml:math id="inf579"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>40</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>, <bold>D</bold>) As a function of the number of latent states/dimensions, we plot the average (lines) and standard error (bars) of the cross-validated <inline-formula><mml:math id="inf580"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> between the data and modelled Fano factors over all stimulus orientations, in both the awake (<bold>C</bold>) and anaesthetized (<bold>D</bold>) datasets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-app1-fig2-v3.tif"/></fig></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s9" sec-type="appendix"><title>Higher order moments and neural decoding</title><p>FA-based encoding models are highly effective at capturing the first- and second-order statistics of neural responses, yet in our simulations we found that Bayesian decoders based on FA encoding models perform poorly when compared to the other decoding models considered in Conditional mixtures facilitate accurate and efficient decoding of neural responses. There we evaluate decoding performance by fitting a candidate model to training data, and computing the mean and standard error of the log-posterior at the true stimulus on held-out data. On the awake data FA scores <inline-formula><mml:math id="inf581"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.246</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.066</mml:mn></mml:mrow></mml:math></inline-formula>, which is comparable to an optimal linear decoder, yet still significantly worse than a nonlinear decoder, or a Bayesian decoder based on a CM. On the anaesthetized data FA scored <inline-formula><mml:math id="inf582"><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, as it would occasionally assign numerically 0 posterior probability to the true stimulus; when we filtered out <inline-formula><mml:math id="inf583"><mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> values from the average, the FA encoder still only achieved performance of <inline-formula><mml:math id="inf584"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2.21</mml:mn></mml:mrow><mml:mo>±</mml:mo><mml:mn>0.31</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Normal distributions — and the FA observable distribution by extension — are essentially defined by their first- and second-order statistics, which suggests that there are higher-order statistics that are important for decoding that FA cannot capture. The third and fourth order statistics known as the skewness and excess kurtosis measure the asymmetry and heavy-tailedness of a given distribution, respectively. Normal distributions have a skewness and excess kurtosis of 0. Here, we study how well this normality assumption is reflected in our neural recordings, and how well our mixture models capture these higher order statistics.</p><p>In <xref ref-type="fig" rid="fig1">Figure 1</xref>, we present scatter plots of the empirical skewness and kurtosis of all the neurons in our datasets at orientation <inline-formula><mml:math id="inf585"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>40</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, and the model skewness and kurtosis learned by our mixture models. Exactly quantifying the non-normality of higher order moments in multivariate distributions is a complicated and evolving subject (<xref ref-type="bibr" rid="bib47">Mardia and El-atoum, 1976</xref>; <xref ref-type="bibr" rid="bib13">Cain et al., 2017</xref>), nevertheless in <xref ref-type="fig" rid="fig1">Figure 1</xref> the empirical skewness and kurtosis of the recorded neurons appear to qualitatively deviate from zero. On the awake data, both the CB and IP mixture achieve high <inline-formula><mml:math id="inf586"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> when compared with the data skewness (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) and kurtosis (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), although the CB mixtures achieves notably better performance. On the anaesthetized data (<xref ref-type="fig" rid="fig1">Figure 1C and D</xref>), the CB mixture continues to achieve a high <inline-formula><mml:math id="inf587"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, but the IP mixture performs extremely poorly; although the disparity in <inline-formula><mml:math id="inf588"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> is not immediately apparent in the scatter plots, this is because some of the model skewness of the IP mixtures are outside the plot bounds.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>CoM-based mixtures capture data skewness and kurtosis.</title><p>(<bold>A</bold>, <bold>B</bold>) Scatter plots of the data skewness (<bold>A</bold>) and kurtosis (<bold>B</bold>) versus the skewness and kurtosis modelled by IP mixtures (blue) and CB mixtures (red). The skewness and kurtosis of 1 of 43 neurons modelled by the IP mixture were outside the bounds of each scatter plot. C,D: Same as A-B but on the anaesthetized data; the skewness of 11 of 43, and kurtosis of 12 of 43 neurons modelled by the IP mixture were outside the bounds of each scatter plot.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-app2-fig1-v3.tif"/></fig><p>When fit to the complete datasets, and averaged over all stimulus conditions, the CB mixtures achieved a skewness <inline-formula><mml:math id="inf589"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> average and standard error of <inline-formula><mml:math id="inf590"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.87</mml:mn><mml:mo>±</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf591"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.94</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and a kurtosis <inline-formula><mml:math id="inf592"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> average and standard error of <inline-formula><mml:math id="inf593"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.61</mml:mn><mml:mo>±</mml:mo><mml:mn>0.20</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf594"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.82</mml:mn><mml:mo>±</mml:mo><mml:mn>0.13</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> on the awake and anaesthetized data, respectively; in contrast, the presence of outliers caused the average scores for the IP mixture to be dramatically negative in all cases. These results suggest that the CoM-based parameters of the CB mixture provide important degrees of freedom for capturing individual variability. That being said, when we cross-validated the <inline-formula><mml:math id="inf595"><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> performance on the higher-order moments, the results were not significantly higher than 0 for the CB mixture, and as such, accurately estimating higher order moments requires larger datasets than what we have considered here.</p><p>Nevertheless, in spite of the inability to capture predictive performance on these moments, we speculate that when combined across higher order moments and cross-moments, the ability of mixture models to capture higher order structure in the data is necessary for maximizing decoding performance, and that these moments might play an important role in neural coding. As the complexity of neural datasets increases, a careful study of such higher-order statistics would become both feasible and warranted, and our mixture model approach could prove to be a useful tool in such work.</p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s10" sec-type="appendix"><title>Sample complexity of conditional mixtures</title><p>To develop a sense of the sample complexity of CMs, we repeated the cross-validation simulations with discrete CMs on subsets of our two datasets (see Conditional mixtures effectively model neural responses in macaque V1 and Conditional mixtures facilitate accurate and efficient decoding of neural responses). In particular, we ran a 10-fold cross-validation simulation on a single subsample of 25%, 50%, 75%, and 100%, of each of our datasets. On our anaesthetized dataset this occasionally resulted in some neurons recording 0 spikes in a giving training set, which tends to cause our training algorithm to diverge, and so we filtered out neurons with low firing rates, leaving 50 neurons in our anaesthetized dataset.</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Sample complexity of discrete CMs.</title><p>10-fold cross-validation of discrete CMs with 3, 10, 20, and 40 components, on subsamples of 25%, 50%, 75%, and 100% of the awake and anaesthetized datasets, with <inline-formula><mml:math id="inf596"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>43</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf597"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> neurons, respectively. <italic>Left column</italic>: Cross-validated average log-likelihood of the models given test data (i.e. encoding performance). <italic>Right column</italic>: Cross-validated average log-posterior (i.e. decoding performance). Error bars represent standard error. In all panels, we added an offset on the abscissa for better visualization of the error bars.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64615-app3-fig1-v3.tif"/></fig><p>We present the results of our simulations in <xref ref-type="fig" rid="fig1">Figure 1</xref>. In the left column (<xref ref-type="fig" rid="fig1">Figure 1A–D</xref>), we present the cross-validated log-likelihood of the vanilla and CoM-models, on the awake and anaesthetized data, respectively, and we see that, as we would expect, models with fewer components maximize their performance on smaller datasets. Even with large amounts of data, however, the performance difference between models with more than 10 components is nearly statistically indistinguishable. In the right column (<xref ref-type="fig" rid="fig1">Figure 1E–H</xref>), we present the cross-validated log-posterior performance of the models, and the results mirror those of the log-likelihood simulations, except the benefits of larger models becomes even more marginal.</p></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s11" sec-type="appendix"><title>Conditional mixtures and generalized linear models</title><p>CMs are closely related to generalized linear models (GLMs), which are widely applied in neuroscience. The application of GLMs to modelling stimulus-driven spike-trains was pioneered in <xref ref-type="bibr" rid="bib58">Pillow et al., 2008</xref>, in which the authors develop a Poisson encoding model <inline-formula><mml:math id="inf598"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf599"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> is the number of recorded neurons, <italic>n</italic><sub><italic>t</italic></sub> is the spike-count of the modelled neuron in timebin <inline-formula><mml:math id="inf600"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf601"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> is the stimulus (here the stimulus is an image and represented as a vector), and where each <inline-formula><mml:math id="inf602"><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the spike-count history of the <inline-formula><mml:math id="inf603"><mml:mi>i</mml:mi></mml:math></inline-formula> th recorded neuron up to time <inline-formula><mml:math id="inf604"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The log-rate <inline-formula><mml:math id="inf605"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of the modelled neuron at time <inline-formula><mml:math id="inf606"><mml:mi>t</mml:mi></mml:math></inline-formula> depends linearly on the stimulus and the spike-history, and is given by<disp-formula id="equ27"><label>(18)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>𝐱</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>𝐦</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>𝐦</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo>⋅</mml:mo><mml:mi>𝐤</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:msub><mml:mi>𝐡</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>𝐦</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf607"><mml:mi mathvariant="bold">𝐤</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf608"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are vectors; in <xref ref-type="bibr" rid="bib58">Pillow et al., 2008</xref> both <inline-formula><mml:math id="inf609"><mml:mi mathvariant="bold">𝐤</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf610"><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are represented by basis functions with a manageable number of fittable parameters.</p><p>This model may be trivially combined with a CM in order to extend the GLM formulation with a latent source of shared-variability that affords analytic expressions for various quantities of interest. The definition of a CB-CM is <inline-formula><mml:math id="inf611"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">l</mml:mi><mml:mi mathvariant="bold">f</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and we may simply replace the variable <inline-formula><mml:math id="inf612"><mml:mi>x</mml:mi></mml:math></inline-formula> with all the independent variables in the GLM formulation, namely <inline-formula><mml:math id="inf613"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf614"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐦</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, and define the baseline log-firing rates <inline-formula><mml:math id="inf615"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf616"><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></inline-formula> copies of the function defined by <xref ref-type="disp-formula" rid="equ27">Equation 18</xref>, each with its own independent parameters. In principle, the expectation-maximization framework we have presented for training CMs can be directly applied to a model with this structure. That being said, choosing the right parameterization of <inline-formula><mml:math id="inf617"><mml:mi mathvariant="bold">𝐤</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf618"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐡</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> would pose a unique challenge in a combined GLM-CM model, and whether such a model would be practically useful is an empirical question that is beyond the scope of this paper.</p><p>Here, we also clarify that although a CM is closely related to both mixture models and GLMs, it should not be confused with the models known as ‘mixtures of GLMs’ (<xref ref-type="bibr" rid="bib30">Grün and Leisch, 2008</xref>). A mixture of GLMs has the form <inline-formula><mml:math id="inf619"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf620"><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:math></inline-formula> is the number of GLMs to be mixed, <italic>w</italic><sub><italic>k</italic></sub> are the weight parameters, and <inline-formula><mml:math id="inf621"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a GLM with parameters <inline-formula><mml:math id="inf622"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This model differs from a CM in many subtle ways, and the easiest to note is that the weights <italic>w</italic><sub><italic>k</italic></sub> do not depend on the stimulus <inline-formula><mml:math id="inf623"><mml:mi>x</mml:mi></mml:math></inline-formula> as they do in a CM, which, as shown in <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig6">6</xref> of the main paper, is critical to how CMs represent data.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64615.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name><role>Reviewing Editor</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Harris</surname><given-names>Kenneth D</given-names></name><role>Reviewer</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.11.05.369827">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.11.05.369827v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper introduces a new framework for modeling correlated neural population spike responses using multivariate mixtures of Poisson or Conway-Maxwell-Poisson distributions. It describes an algorithm for fitting the model to data using Expectation Maximization (EM), a formula for Fisher information, and a Bayesian decoder that is competitive with other more computationally demanding decoding methods such as artificial neural networks. The authors apply this model to V1 data from awake and anesthetized monkeys, and show that it captures the variability (eg., Fano Factor) and co-variability of population responses better than Poisson models. Finally, the paper shows how the latent variables of the model can provide insight into the structure of population codes. The resulting framework represents a powerful advance for modeling the correlated variability in neural population responses, and promises to be a useful new tool for analyzing large-scale neural recordings. The paper will be of interest to computational neuroscientists studying neural coding, and to system neuroscientists who use descriptive models to characterize the stimulus tuning of correlated spiking activity recorded from large neural populations.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Modelling the neural code in large populations of correlated neurons&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Kenneth D Harris (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>1) Clarify abstract (and optionally title): see comments from R1.</p><p>2) Address R1 comments about claims &quot;C1&quot; and &quot;C2&quot;.</p><p>3) Compare performance of the proposed models to existing models that capture stimulus encoding by large populations of correlated neurons (see comments from R2).</p><p>4) Clarify how users should decide how many mixture components to add. (R3 comment 4).</p><p>5) Provide publicly released code.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1 – A main suggestion is to rewrite the title and abstract as I outlined in Weakness #1 in the public comments.</p><p>2 – Address 2.1-2.3 (under weaknesses), in the public comments or eliminate/tone down the claims C1-C3, in the abstract, intro, discussion or elsewhere.</p><p>3 – I found the discussion around Figure 5 hard to follow. To improve this, I suggest that authors provide (already in the Results section) simplified/readable formulas (valid for the &quot;vanilla&quot; Poisson case which is used in this section) for the stimulus dependence of index probabilities (IP) which depends on the mean population response according to</p><p>p(k|x) ∝expθ<sub>K</sub>(k) + r(x|k)</p><p>where r(x|k) is the average total population spike count in the mixture component k, which I believe is given by the partition function ψ<sub>N</sub>.</p><p>4 – As they point out, in their minimal model, the tuning curves (mean response) of neurons are scaled (&quot;gain modulated&quot;) versions of the &quot;baseline&quot;-component's tuning curves. Again, I think it would help to write a formula for this in the Results section, and connect the scale factor and the baseline tuning curve with the θ<sub>NK</sub> and θ<sub>N</sub>(x) components, respectively.</p><p>5 – I don't think the simple scaling (see previous comment) relationship actually holds in the non-Vanilla CoM case, but they do claim that. If so, the text in lines 189-192 (especially &quot;scaled&quot; in line 191) should be corrected.</p><p>6 – As they derive in the Methods part, the population covariance matrix of the CPM has the same form as the covariance matrix in factor analysis: a diagonal &quot;private noise&quot; matrix + low-rank &quot;shared-noise&quot; matrix. I think it would be valuable to point this out and write the corresponding formula in the Results section e.g. around Figure 2. Also point out what happens to the diagonal term in the vanilla vs. CoM cases.</p><p>7 – The ground truth exercise of Figure 3 is valuable, but I think more valuable than showing how the model fits one example would be to give an idea of the &quot;sample complexity&quot;: give an idea of goodness of fit vs. number of trials in the dataset. (At least clarify in the caption how many trials per stimulus conditions were used.)</p><p>8 – Not sure what the exercise described in lines 310-321 shows. Given that the gound truth model is within the fit model family, isn't it given (by classical asymptotic statistics results) that for large enough data the likelihood and therefore the posterior should converge the true posterior?</p><p>So is the result really surprising given that the dataset seems pretty large (d_T=10000)?</p><p>Again the more relevant thing would be: what is the minimal amount of data needed to find a good posterior approximation… or as a simpler version: how would it do for typical neural dataset sizes (# trials). (c.f. the previous comment).</p><p>9 – lines 359-371 – especially line 365-6: The reasoning here (that the shown results establish the information-limiting nature of the noise) are not really complete. Technically, &quot;Information-limiting&quot; means that the Fisher info is not extensive, i.e. does not scale linearly with population size. So they have to argue that the &quot;random shifts&quot; (discussed in line 365) will not go to zero as. Population size goes to infinity.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Line 103: Some more introduction to CoM Poisson distributions would be nice. Why are these better than the negative binomial, which is analytically more tractable? Presumably because they can handle underdispersion? Neural data is usually overdispersed, but does the extra dispersion introduced by a mixture model mean one needs to use underdispersed components for the mixture components?</p><p>Line 119: &quot;express multivariate Poisson mixtures in an exponential family form&quot;. This is misleading: it sounds like you have expressed the marginal distribution of the mixture model in exponential form, which I believe is impossible. In fact, you are expressing each component distribution in exponential form.</p><p>Line 150: &quot;vanilla mixtures&quot;. Why not call them Poisson mixtures? That's what they are.</p><p>Line 159: &quot;optimized model parameters as described in Materials and methods&quot;. You mean you used the EM algorithm derived in Materials and methods? Say so explicitly.</p><p>Figure 2. Is this cross-validated? From the text it seems not, so no wonder the CoM model, with more parameters, fits better. Also, why does the vanilla model ever produce FFs that are too low? Can't it just add more mixture components to increase dispersion?</p><p>Line 180: the &quot;CPM&quot; sounds like a mixture of generalized linear models. If so, &quot;mixture of GLMs&quot; would be more familiar terminology for most readers.</p><p>Line 193-227: it is not clear what we really learn from this. If it just is a validation that the EM algorithm can work on simulated ground truth, then shouldn't that go first, before the application to real data? Also comparing to a less sophisticated model would help show the benefits of this one.</p><p>Table 1: please state how many cells are in both data sets.</p><p>Line 238: &quot;log likelihood&quot;. Please specify if this is to base 2 or base e; also give a unit in table 1 (e.g. bits/trial).</p><p>Figure 5: it would be nice to see this applied to real data.</p><p>Line 466: do you mean ψN = ∑i θ<sub>N,i</sub>? The log partition functions should add, right?</p><p>Equation 12: is there a denominator of ∏<sub>i</sub> n<sub>i</sub>! missing?</p><p>Line 573: how much time does the gradient ascent take? Is it going to be a problem for recordings with large numbers of neurons?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64615.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>1) Clarify abstract (and optionally title): see comments from R1.</p></disp-quote><p>We rewrote the abstract to be more concrete, and state more clearly the nature of the model, and our exact contributions.</p><disp-quote content-type="editor-comment"><p>2) Address R1 comments about claims &quot;C1&quot; and &quot;C2&quot;.</p></disp-quote><p>C1 concerned the exact relationship between our work and the theory of Probabilistic Population Codes. We clarified this relationship with a mathematical result and a new section in the manuscript (Constrained conditional mixtures support linear probabilistic population coding).</p><p>C2 concerned how we (over) described the “interpretability&quot; of the latent variables in our application of conditional mixtures to synthetic data with information-limiting correlations. We reformulated relevant text to emphasize that the model in this case is descriptive and not mechanistic.</p><disp-quote content-type="editor-comment"><p>3) Compare performance of the proposed models to existing models that capture stimulus encoding by large populations of correlated neurons (see comments from R2).</p></disp-quote><p>We've expanded the article with an appendix, where we provided a thorough comparison of our model with factor analysis, and highlight these results where relevant in the main article. We focused our comparison on factor analysis since it has been applied to modelling unbounded, trial-to-trial spike-counts, and in our introduction we attempted to better explain this is the form of data that our model should be applied to. In the Introduction we have also included references to other models indicated by R2 (lines 44{51), and clarified how they differ from ours.</p><disp-quote content-type="editor-comment"><p>4) Clarify how users should decide how many mixture components to add. (R3 comment 4).</p></disp-quote><p>We added a subsection in the methods section (Strategies for choosing the CM form and latent structure) which presents strategies and suggestions for choosing the latent variable structure/number of mixture components when applying conditional mixtures.</p><disp-quote content-type="editor-comment"><p>5) Provide publicly released code.</p></disp-quote><p>Our code is now available at the Git repository: https://gitlab.com/sacha-sokoloski/</p><p>neural-mixtures, and at that page we have provided an overview of the library and instructions for running the code (we point to this page at lines 796-800). The code provides several scripts with which we generated the various figures in the manuscript, and we would be willing to write e.g. Python bindings if interested parties would desire more fine-grained control of the code and libraries.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1 – A main suggestion is to rewrite the title and abstract as I outlined in Weakness #1 in the public comments.</p></disp-quote><p>See online response to public review.</p><disp-quote content-type="editor-comment"><p>2 – Address 2.1-2.3 (under weaknesses), in the public comments or eliminate/tone down the claims C1-C3, in the abstract, intro, discussion or elsewhere.</p></disp-quote><p>See online response to public review.</p><disp-quote content-type="editor-comment"><p>3 – I found the discussion around Figure 5 hard to follow. To improve this, I suggest that authors provide (already in the Results section) simplified/readable formulas (valid for the &quot;vanilla&quot; Poisson case which is used in this section) for the stimulus dependence of index probabilities (IP) which depends on the mean population response according to</p></disp-quote><p>p(k|x) ∝expθ<sub>K</sub>(k) + r(x|k)</p><disp-quote content-type="editor-comment"><p>where r(x|k) is the average total population spike count in the mixture component k, which I believe is given by the partition function ψ<sub>N</sub>.</p></disp-quote><p>We've added this formula (lines 415-416) and reworked this section (see the paragraphs from lines 413-434).</p><disp-quote content-type="editor-comment"><p>4 – As they point out, in their minimal model, the tuning curves (mean response) of neurons are scaled (&quot;gain modulated&quot;) versions of the &quot;baseline&quot;-component's tuning curves. Again, I think it would help to write a formula for this in the Results section, and connect the scale factor and the baseline tuning curve with the θ<sub>NK</sub> and θ<sub>N</sub>(x)components, respectively.</p></disp-quote><p>We now provide more explicit formulae that connect gain modulation with the exponential family parameters (lines 221-227).</p><disp-quote content-type="editor-comment"><p>5 – I don't think the simple scaling (see previous comment) relationship actually holds in the non-Vanilla CoM case, but they do claim that. If so, the text in lines 189-192 (especially &quot;scaled&quot; in line 191) should be corrected.</p></disp-quote><p>You are correct, and we've clarified the language in the corresponding section (lines 227-229).</p><disp-quote content-type="editor-comment"><p>6 – As they derive in the Methods part, the population covariance matrix of the CPM has the same form as the covariance matrix in factor analysis: a diagonal &quot;private noise&quot; matrix + low-rank &quot;shared-noise&quot; matrix. I think it would be valuable to point this out and write the corresponding formula in the Results section e.g. around Figure 2. Also point out what happens to the diagonal term in the vanilla vs. CoM cases.</p></disp-quote><p>This is very insightful, thank you. We've incorporated this comparison into the development and introduction of mixture our models (see lines 126-137, and 170-172). We've resisted adding the explicit formulas for the variances/covariances of the various models, as we found that it required introducing too much notation, however we hope we conveyed your insight effectively in words.</p><disp-quote content-type="editor-comment"><p>7 – The ground truth exercise of Figure 3 is valuable, but I think more valuable than showing how the model fits one example would be to give an idea of the &quot;sample complexity&quot;: give an idea of goodness of fit vs. number of trials in the dataset. (At least clarify in the caption how many trials per stimulus conditions were used.)</p></disp-quote><p>We agree that this is important to establish. In Appendix 3 we repeated our cross-validated simulations for estimating the predictive log-likelihood and log-posterior on subsets of our two datasets, i.e. by subsampling different fractions of trials, to better understand the sample complexity of these two quantities.</p><disp-quote content-type="editor-comment"><p>8 – Not sure what the exercise described in lines 310-321 shows. Given that the gound truth model is within the fit model family, isn't it given (by classical asymptotic statistics results) that for large enough data the likelihood and therefore the posterior should converge the true posterior?</p><p>So is the result really surprising given that the dataset seems pretty large (d_T=10000)?</p><p>Again the more relevant thing would be: what is the minimal amount of data needed to find a good posterior approximation… or as a simpler version: how would it do for typical neural dataset sizes (# trials). (c.f. the previous comment).</p></disp-quote><p>Again we agree. We have removed this short paragraph and instead provide a sample complexity analysis in Appendix 3.</p><disp-quote content-type="editor-comment"><p>9 – lines 359-371 – especially line 365-6: The reasoning here (that the shown results establish the information-limiting nature of the noise) are not really complete. Technically, &quot;Information-limiting&quot; means that the Fisher info is not extensive, i.e. does not scale linearly with population size. So they have to argue that the &quot;random shifts&quot; (discussed in line 365) will not go to zero as. Population size goes to infinity.</p></disp-quote><p>This is a fair point, and we've reworded parts of this section to emphasize that our model learns to approximate information-limiting correlations, which we hope clarifies the difference. (See again the section starting at line 393, and in particular lines 392-393, and 435-436).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Line 103: Some more introduction to CoM Poisson distributions would be nice. Why are these better than the negative binomial, which is analytically more tractable? Presumably because they can handle underdispersion? Neural data is usually overdispersed, but does the extra dispersion introduced by a mixture model mean one needs to use underdispersed components for the mixture components?</p></disp-quote><p>To address these questions, first, we added a bit more introduction to the CoM-Poisson distributions (lines 108-114), and indeed the reason we use them is to allow our components do exhibit under-dispersion, as described in Stevenson et al., 2016, Journal of Computational Neuroscience.</p><p>Secondly, as you say, it is indeed helpful to to use CoM-Poisson distributions even when the data is primarily over-dispersed, because mixture models can make the statistics “too&quot; over-dispersed. A specific example of this effect is illustrated in our applications to anaesthetized data (Figure 2B). We explained this more concretely when we show the improvements that result from our CoM-based model, and show how the CoM-Based parameters facilitate this in Figures 2C-D.</p><disp-quote content-type="editor-comment"><p>Line 119: &quot;express multivariate Poisson mixtures in an exponential family form&quot;. This is misleading: it sounds like you have expressed the marginal distribution of the mixture model in exponential form, which I believe is impossible. In fact, you are expressing each component distribution in exponential form.</p></disp-quote><p>Indeed, thanks for pointing this out. We rewrote the text to properly explain that mixtures are marginals of an exponential family joint distribution (lines 138{139).</p><disp-quote content-type="editor-comment"><p>Line 150: &quot;vanilla mixtures&quot;. Why not call them Poisson mixtures? That's what they are.</p></disp-quote><p>Indeed, we realize now that our naming conventions could lead to confusion, and we have retooled the names and acronyms in the paper. We now refer to Independent Poisson (IP) and CoM-Based (CB) models, and for Conditional Mixtures we write CM, so we have IP and CB mixtures, and then IP-CMs and CB-CMs.</p><disp-quote content-type="editor-comment"><p>Line 159: &quot;optimized model parameters as described in Materials and methods&quot;. You mean you used the EM algorithm derived in Materials and methods? Say so explicitly.</p></disp-quote><p>Changed (lines 178-179).</p><disp-quote content-type="editor-comment"><p>Figure 2. Is this cross-validated? From the text it seems not, so no wonder the CoM model, with more parameters, fits better. Also, why does the vanilla model ever produce FFs that are too low? Can't it just add more mixture components to increase dispersion?</p></disp-quote><p>The analyses around Figure 2 are not cross-validated. We highlight that better in the revised text (lines 178-179).</p><p>The reviewer is correct that it's not a surprise that the CoM model, with more parameters, fits better. Our goal with this exercise instead is to illustrate how it fits better. Indeed, intuitively, one would expect that the vanilla mixture should capture anaesthetized data well, since it is primarily overdispersed, yet we find that this is not necessarily the case.</p><p>In the revised manuscript we have emphasized and explained this counter-intuitive result with the histograms of the CoM parameters around Figure 2 (which has now been split into two figures) and related text. In short, the means and variances of a Poisson mixture are coupled through shared parameters, and the CoM-based parameters break this coupling, and in particular allow the model to capture over-dispersion directly without comprising its estimate of the mean. This correction highlights the strength and importance of incorporating CoM-Poisson distributions into our models.</p><disp-quote content-type="editor-comment"><p>Line 180: the &quot;CPM&quot; sounds like a mixture of generalized linear models. If so, &quot;mixture of GLMs&quot; would be more familiar terminology for most readers.</p></disp-quote><p>This is a logical conclusion but in fact mixtures of GLMs are different from our models. In Appendix 4 material we include an explanation of the relationship between these two models.</p><disp-quote content-type="editor-comment"><p>Line 193-227: it is not clear what we really learn from this. If it just is a validation that the EM algorithm can work on simulated ground truth, then shouldn't that go first, before the application to real data? Also comparing to a less sophisticated model would help show the benefits of this one.</p></disp-quote><p>This is a fair point. In some sense this is just a validation of the EM algorithm, however, in the first set of results around figures 2-3, before Line 193-227 (now Lines 230-263), we aren't considering stimuli, and the EM algorithm for stimulus-dependence, requires a further extension.</p><p>The paper is centred around two results - incorporating CoM-based parameters, and adding stimulus-dependence - and in some sense it's arbitrary which we start with. We went with the former, and we hope it's okay to leave the structure as is.</p><p>As for the comparison with less sophisticated models, in our cross-validation of the log-likelihood and log-posterior under various models, we do consider an independent Poisson model and a Linear decoder. In the text we have attempted to better explain that this is a “sanity check&quot; and not a rigorous performance analysis of the model, which comes in later sections (lines 263-264).</p><disp-quote content-type="editor-comment"><p>Table 1: please state how many cells are in both data sets.</p></disp-quote><p>Done (Now Table 2).</p><disp-quote content-type="editor-comment"><p>Line 238: &quot;log likelihood&quot;. Please specify if this is to base 2 or base e; also give a unit in table 1 (e.g. bits/trial).</p></disp-quote><p>Added and added (see Table 2, and line 276).</p><disp-quote content-type="editor-comment"><p>Figure 5: it would be nice to see this applied to real data.</p></disp-quote><p>We agree, but, as for the application on other datasets, we believe that a proper analysis of the latent structure of information-limiting correlations in real data deserves a dedicated and extensive study.</p><disp-quote content-type="editor-comment"><p>Line 466: do you mean ψN = ∑i θ<sub>N,i</sub>? The log partition functions should add, right?</p></disp-quote><p>Fixed, thank you (Line 542).</p><disp-quote content-type="editor-comment"><p>Equation 12: is there a denominator of ∏<sub>i</sub> n<sub>i</sub>! missing?</p></disp-quote><p>Fixed, thank you (Now Equation 13).</p><disp-quote content-type="editor-comment"><p>Line 573: how much time does the gradient ascent take? Is it going to be a problem for recordings with large numbers of neurons?</p></disp-quote><p>The model scales quite well. The largest computation in the descent step is an outer product computation which is O(k * n), where k is the number of mixture components, n is the number of neurons. Nevertheless, there are subtleties (in particular around minibatch sizes and number of stimuli). We believe the best place for this information is in our gitlab repository, and so we have included it there.</p><p>For reference, we have successfully applied the model in synthetic experiments to populations of thousands of neurons, with training times on the order of hours. For tens of thousands there might be a need for refinement of our code, perhaps including GPU computation.</p></body></sub-article></article>