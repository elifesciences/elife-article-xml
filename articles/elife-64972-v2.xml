<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64972</article-id><article-id pub-id-type="doi">10.7554/eLife.64972</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Anticipation of temporally structured events in the brain</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-215264"><name><surname>Lee</surname><given-names>Caroline S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7769-8799</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-175733"><name><surname>Aly</surname><given-names>Mariam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4033-6134</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-152764"><name><surname>Baldassano</surname><given-names>Christopher</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3540-5019</contrib-id><email>c.baldassano@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Columbia University, Department of Psychology</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Dartmouth College, Department of Psychological and Brain Sciences</institution><addr-line><named-content content-type="city">Hanover</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>22</day><month>04</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e64972</elocation-id><history><date date-type="received" iso-8601-date="2020-11-17"><day>17</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-04-21"><day>21</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Lee et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Lee et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64972-v2.pdf"/><abstract><p>Learning about temporal structure is adaptive because it enables the generation of expectations. We examined how the brain uses experience in structured environments to anticipate upcoming events. During fMRI (functional magnetic resonance imaging), individuals watched a 90 s movie clip six times. Using a hidden Markov model applied to searchlights across the whole brain, we identified temporal shifts between activity patterns evoked by the first vs. repeated viewings of the movie clip. In many regions throughout the cortex, neural activity patterns for repeated viewings shifted to precede those of initial viewing by up to 15 s. This anticipation varied hierarchically in a posterior (less anticipation) to anterior (more anticipation) fashion. We also identified specific regions in which the timing of the brain’s event boundaries was related to those of human-labeled event boundaries, with the timing of this relationship shifting on repeated viewings. With repeated viewing, the brain’s event boundaries came to precede human-annotated boundaries by 1–4 s on average. Together, these results demonstrate a hierarchy of anticipatory signals in the human brain and link them to subjective experiences of events.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Anticipating future events is essential. It allows individuals to plan and prepare what they will do seconds, minutes, or hours in the future. But how the brain can predict future events in both the short-term and long-term is not yet clear. Researchers know that the brain processes images or other sensory information in stages. For example, visual features are processed from lines to shapes to objects, and eventually scenes. This staged approach allows the brain to create representations of many parts of the world simultaneously.</p><p>A similar hierarchy may be at play in anticipation. Different parts of the brain may track what is happening now, and what could happen in the next few seconds and minutes. This would provide a way for the brain to forecast upcoming events in the immediate, near, and more distant future at the same time.</p><p>Now, Lee et al. show that the regions in the back of the brain anticipate the immediate future, while longer-term predictions are made in brain regions near the front. In the experiments, study participants watched a 90-second clip of the movie ‘The Grand Budapest Hotel’ six times while undergoing functional magnetic resonance imaging (fMRI). Then, Lee et al. used computer modeling to compare the brain activity captured by fMRI during successive viewings. This allowed the researchers to watch participants’ brain activity moment-by-moment.</p><p>As the participants repeatedly watched the movie clip, their brains began to anticipate what was coming next. Regions near the back of the brain like the visual cortex anticipated events in the next 1 to 4 seconds. Areas in the middle of the brain anticipated 5 to 8 seconds in the future. The front of brain anticipated 8 to 15 seconds into the future. Lee et al. show that many parts of the brain work together to predict the near and more distant future. More research is needed to understand how this information translates into actions. Learning more may help scientists understand how diseases or injuries affect people’s ability to plan and respond to future events.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fMRI</kwd><kwd>movie</kwd><kwd>anticipation</kwd><kwd>memory</kwd><kwd>timescales</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The brain contains a hierarchy of anticipatory signals, with more anterior regions showing anticipation that reaches further into the future.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A primary function of the brain is to adaptively use past experience to generate expectations about events that are likely to occur in the future (<xref ref-type="bibr" rid="bib11">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib19">Friston, 2005</xref>). Indeed, anticipation and prediction are ubiquitous in the brain, spanning systems that support sensation, action, memory, motivation, and language (<xref ref-type="bibr" rid="bib15">den Ouden et al., 2010</xref>). For example, the visual system takes advantage of the world’s relative stability over space and time to anticipate upcoming input (<xref ref-type="bibr" rid="bib14">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Summerfield and Egner, 2009</xref>). The majority of studies examining anticipatory signals, however, have tested anticipation based on memory for relatively simple associations between pairs of discrete stimuli, such as auditory tones, lines, dots, oriented gratings, or abstract objects (e.g., <xref ref-type="bibr" rid="bib2">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Gavornik and Bear, 2014</xref>; <xref ref-type="bibr" rid="bib22">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Kok et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Kok and Turk-Browne, 2018</xref>). These studies have found anticipatory signals about a single upcoming stimulus in a variety of brain regions, from perceptual regions (<xref ref-type="bibr" rid="bib26">Kok et al., 2012</xref>) to the medial temporal lobe (<xref ref-type="bibr" rid="bib22">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Kok and Turk-Browne, 2018</xref>). How does the brain use repeated experience in naturalistic environments to anticipate upcoming sequences of events that extend farther into the future?</p><p>Prior work has shown that the brain integrates information about the recent past over a hierarchy of timescales (<xref ref-type="bibr" rid="bib3">Aly et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Hasson et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Kurby and Zacks, 2008</xref>). Lower-order areas primarily represent the current moment, whereas higher-order areas are sensitive to information from many seconds or even minutes into the past. Higher-order regions with longer timescales play a critical role in organizing perceptual input into semantically meaningful schematic representations (<xref ref-type="bibr" rid="bib5">Baldassano et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Baldassano et al., 2018</xref>). What is less clear is whether this hierarchy also exists in a prospective direction: as we move from lower-order perceptual systems into higher-order areas, do these regions exhibit different timescales of anticipation into the future? We previously found that higher-order regions did exhibit anticipatory signals when individuals had prior knowledge of the general structure of a narrative (<xref ref-type="bibr" rid="bib5">Baldassano et al., 2017</xref>). But these individuals only had knowledge of information at relatively long timescales (e.g., the general sequence of events, and not moment-by-moment perceptual features), so we were unable to assess whether they could generate expectations across the timescale hierarchy.</p><p>Here, we examine how the brain anticipates event boundaries in familiar sequences of actions. We used a naturalistic narrative stimulus (a movie), in which regularities are present at multiple timescales. For example, upon second viewing of a movie, one can anticipate the next action to be taken in a given scene, the next character to appear, the next location that is visited, and the last scene of the movie. The presence of predictability at multiple timescales in the same stimulus enables us to identify varying timescales of anticipation in the brain that co-exist simultaneously. We hypothesized that the timescale of anticipation in the brain would vary continuously, with progressively higher-order regions (e.g., prefrontal cortex) anticipating events that are further in the future compared to lower-order regions (e.g., visual cortex).</p><p>To test this, we examined brain activity with functional magnetic resonance imaging (fMRI) while individuals watched a 90 s clip from the movie <italic>The Grand Budapest Hotel</italic> six times. To uncover anticipation in the brain, we used a searchlight approach in which, for each region of interest, we fit a hidden Markov model (HMM) to identify temporal shifts between multivariate activity patterns (functionally hyperaligned across individuals using the shared response model [SRM]) evoked by the first viewing of the movie clip compared to repeated viewings (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This model assumes that the brain’s response to a structured narrative stimulus consists of a sequence of distinct, stable activity patterns that correspond to event structure in the narrative (<xref ref-type="bibr" rid="bib5">Baldassano et al., 2017</xref>). We could then identify, on a timepoint-by-timepoint basis, the extent to which viewers were activating event-specific brain activity patterns earlier in subsequent viewings of the movie, by drawing on their prior experience. Because the HMM infers a probability distribution over states, it is able to detect subtle shifts between viewings; activity patterns may reflect a combination of current and upcoming events, and the degree of anticipation can vary throughout the clip.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Computing varying timescales of anticipatory signals by examining temporal shifts in events across multiple viewings of a movie.</title><p>(<bold>a</bold>) Given the voxel by time pattern of responses evoked by the movie clip on each viewing (darker colors indicate higher levels of activity), our goal is to model all viewings as a series of transitions through a shared sequence of event patterns. (<bold>b–c</bold>) By fitting a hidden Markov model (HMM) jointly to all viewings, we can identify this shared sequence of event patterns, as well as a probabilistic estimate of event transitions. Regions with anticipatory representations are those in which event transitions occur earlier in time for repeated viewings of a stimulus compared to the initial viewing, indicated by an upward shift on the plot of the expected value of the event at each timepoint. (<bold>d</bold>) Taking the temporal derivative of the event timecourse plot in (<bold>c</bold>) produces a measure of the strength of event shifts at each moment in time, allowing for comparison with event boundary annotations from human observers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig1-v2.tif"/></fig><p>We also compared the brain’s event boundaries (identified by the HMM) to subjective event boundary annotations made by a separate group of participants. This allowed us to test how the relationship between the brain’s events and subjective event boundaries changes with repeated viewings. Together, this approach allowed us to characterize the nature of hierarchical anticipatory signals in the brain and link them to behavioral measures of event perception.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Timescales of anticipation in the brain</title><p>To identify anticipatory signals in the brain, we examined TR-by-TR brain activity patterns during each of the six viewings of the movie clip. For each spherical searchlight within the brain volume, we fit an HMM jointly to all repetitions, to identify a sequence of event patterns common to all viewings and the timing of spatial pattern changes for each viewing. At each timepoint for each viewing, the HMM produced a probability distribution that describes the mixture of event patterns active at that timepoint. Computing the expected value of this distribution provides an index of how the brain transitions through event patterns on each viewing, allowing us to identify how this timing shifts within each region of the brain.</p><p>Our analysis revealed temporal shifts in event patterns in many brain regions, including lateral occipital cortex, angular and supramarginal gyri, lateral and anterior temporal lobe, lateral and medial prefrontal cortex (mPFC), and insular cortex (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The magnitude of this shift varied along a posterior-to-anterior temporal hierarchy (Spearman’s rho = 0.58, p=0.0030), with the most anterior regions in the temporal pole and prefrontal cortex showing shifts of up to 15 s on subsequent viewings compared to the first viewing. This hierarchy persisted even when computed on the unthresholded anticipation map including voxels that did not meet the threshold for statistical significance (Spearman’s rho = 0.42, p=0.0028; see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). There were no significant correlations with the left-to-right axis (rho = 0.06, p=0.41 for thresholded map; rho = 0.12, p=0.29 for unthresholded map) or the inferior-to-superior axis (rho = 0.07, p=0.28 for thresholded map; rho = −0.11, p=0.73 for unthresholded map). We obtained a similar map when comparing the first viewing to just the sixth viewing alone (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Timescales of anticipation vary across the cortical hierarchy.</title><p>(<bold>a</bold>) Multiple regions exhibited shifts in event timing between initial and repeated viewings, with event transitions shifting earlier in time with subsequent viewings. Across the brain, anticipation timescales varied from a few seconds to 15 s, with the longest timescale anticipatory signals in prefrontal cortex and the temporal pole. Anticipation followed a posterior-to-anterior hierarchy, with progressively anterior areas generating anticipatory signals that reach further into the future (Spearman’s rho = 0.58, p=0.0030). Statistical thresholding was conducted via a permutation test, with correction for false discovery rate (FDR), q&lt;0.05. (<bold>b</bold>) Event by time plots for three sample regions from (<bold>a</bold>), selected post hoc for illustration. Because the HMM produces a probability distribution across states at each timepoint, which can reflect a combination of current and upcoming event representations, we plot the expected value of the event assignments at each timepoint. The upward shift from the first viewing to subsequent viewings indexes the amount of anticipation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Unthresholded statistical map of anticipation timescales.</title><p>This figure shows the same analysis as in <xref ref-type="fig" rid="fig2">Figure 2a</xref>, but without statistical thresholding. The posterior-to-anterior hierarchy of anticipation persists in this unthresholded map (Spearman’s rho = 0.42, p=0.0028).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Timescales of anticipation when the first viewing is compared to the last viewing.</title><p>This figure shows the same analysis as that in <xref ref-type="fig" rid="fig2">Figure 2</xref>, except that the first viewing is compared to the last (6th) viewing only.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Timescales of anticipation as a function of the optimal number of events.</title><p>(<bold>a</bold>) This map depicts the optimal number of events in each brain region that shows statistically significant anticipation (i.e., masked by regions shown in <xref ref-type="fig" rid="fig2">Figure 2a</xref>). The optimal number of events was determined by maximizing the log-likelihood of the HMM on the first movie viewing. Purple indicates the fastest timescale (optimally fit by eight events, average event duration = 11 s) and yellow indicates the slowest timescale (optimally fit by two events, average event duration = 45 s). (<bold>b</bold>) Anticipation (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) is shown binned by the optimal event timescale. Anticipation is further reaching in regions with longer timescales (Spearman’s rho = 0.319, p=0.00031).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Cross-correlation analysis of anticipation.</title><p>Cross-correlations were conducted between the activity timecourse of a given region for the first movie viewing and the average of the subsequent movie viewings. The regions depicted showed statistically significant shifts in activity, such that the peak correlation occurred when the subsequent viewing activity was shifted earlier in time. Statistical thresholding was conducted via a permutation test, with correction for false discovery rate (FDR), q&lt;0.05. Colors depict the temporal shift of the peak in the cross-correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig2-figsupp4-v2.tif"/></fig></fig-group><p>We compared how this hierarchy of anticipation timescales related to the intrinsic processing timescales in each region during the initial viewing of the movie clip. Identifying the optimal number of HMM events for each searchlight, we observed a timescale hierarchy similar to that described in previous work, with faster timescales in sensory regions and slower timescales in more anterior regions (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3a</xref>). Regions with longer intrinsic timescales also showed a greater degree of anticipation with repeated viewing (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3b</xref>).</p><p>We also compared these results to those obtained by using a simple cross-correlation approach, testing for a fixed temporal offset between the responses to initial and repeated viewing. This approach did detect significant anticipation in some anterior regions, but was much less sensitive than the more flexible HMM fits, especially in posterior regions (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p></sec><sec id="s2-2"><title>Relationship with human-annotated events</title><p>Our data-driven method for identifying event structure in fMRI data does not make use of information about the content of the stimulus, leaving open the question of how the HMM-identified event boundaries correspond to subjective event transitions in the movie. One possibility is that the brain’s event boundaries could <italic>start</italic> well-aligned with event boundaries in the movie and then shift earlier (indicating anticipation of upcoming stimulus content). Alternatively, they may initially lag behind stimulus boundaries (reflecting a delayed response time on initial viewing) and then shift to become <italic>better</italic> aligned with movie scene transitions on repeated viewings. Finally, both patterns may exist simultaneously in the brain, but in different brain regions.</p><p>We asked human raters to identify event transitions in the stimulus, labeling each ‘meaningful segment’ of activity (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To generate a hypothesis about the strength and timing of event shifts in the fMRI data, we convolved the distribution of boundary annotations with a hemodynamic response function (HRF) as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. We then explored alignment between these human-annotated event boundaries and the event boundaries extracted from the brain response to each viewing, as shown in <xref ref-type="fig" rid="fig1">Figure 1d</xref>. In each searchlight, we cross-correlated the brain-derived boundary timecourse with the event annotation timecourse to find the temporal offset that maximized this correlation.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>An example of event annotations from <italic>The Grand Budapest Hotel</italic>.</title><p>Dotted lines demarcate events and phrases between the lines are brief titles given by one participant to describe each event. (Frames in this figure have been blurred to comply with copyright restrictions, but all participants were presented with the original unblurred version.)</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig3-v2.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Construction of behavioral boundary timecourse from human annotations.</title><p>The number of boundary annotations at each second of the movie clip (in gray) was convolved with a hemodynamic response function (HRF) to produce a continuous measure of boundary strength (black line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig4-v2.tif"/></fig><p>We found three clusters in the middle temporal gyrus (MTG), fusiform gyrus (FG), and superior temporal sulcus (STS) in which the optimal lag for the repeated viewings was significantly earlier than for the initial viewing, indicating that the relationship between the brain-derived HMM event boundaries and the human-annotated boundaries was changing with repeated viewings (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The HMM boundaries on the first viewing were significantly later than the annotated boundaries in FG and STS, while the optimal lag did not significantly differ from 0 in MTG (95% confidence intervals for the optimal lag, in seconds: MTG = [−0.27, 2.86]; FG = [0.14, 1.99]; STS = [1.48, 8.53]). The HMM boundaries on repeated viewings were significantly earlier than the annotated boundaries in all three regions (95% confidence intervals for the average optimal lag, in seconds: MTG = [−4.06, –1.83]; FG = [−1.56, –0.26]; STS = [−3.06, –1.69]).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Correlations between the brain’s event transitions and human-annotated event boundaries.</title><p>Cross-correlation plots show the correlation between the brain’s hidden Markov model (HMM) event boundaries and annotated event boundaries as the timecourses are shifted with respect to one another. The correlation at 0 lag indicates the similarity between the brain’s event boundaries and annotated event boundaries when the timecourses are aligned. Negative lags show the correlations when the human-annotated event timecourse is shifted earlier in time, and positive lags show the correlation when the human-annotated event timecourse is shifted later in time. Peaks in the cross-correlation plot indicate the lag that produced the highest correlation between the brain’s event boundaries and annotated event boundaries. On initial viewing, the HMM event boundaries for the fusiform gyrus and superior temporal sulcus lagged significantly behind the annotated event boundaries, while the timing of the peak correlation for the middle temporal gyrus did not significantly differ from 0 lag. On subsequent viewings, the HMM event boundaries in all three regions shifted to be significantly earlier than the initial viewing, with the timing of the peak correlation significantly preceding 0 lag.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-64972-fig5-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We investigated whether the brain contains a hierarchy of anticipatory signals during passive viewing of a naturalistic movie. We found that regions throughout the brain exhibit anticipation of upcoming events in audiovisual stimuli, with activity patterns shifting earlier in time as participants repeatedly watched the same movie clip. This anticipation occurred at varying timescales along the cortical hierarchy. Anticipation in higher-order, more anterior regions reached further into the future than that in lower-order, more posterior regions. Furthermore, in a subset of these regions, the coupling between event representations and human-annotated events shifted with learning: event boundaries in the brain came to reliably precede subjective event boundaries in the movie.</p><sec id="s3-1"><title>Regions with anticipatory representations</title><p>One region showing long-timescale anticipatory signals was the bilateral anterior insula. This region has been linked to anticipation of diverse categories of positive and negative outcomes (<xref ref-type="bibr" rid="bib32">Liu et al., 2011</xref>), including outcomes that will be experienced by other people (<xref ref-type="bibr" rid="bib37">Singer et al., 2009</xref>). The movie stimulus used in our experiment depicts an interview in which the protagonist is initially judged to have ‘zero’ experience but then ends up impressing the interviewer, allowing for anticipation of this unexpected social outcome only on repeat viewings. Other regions showing long timescales of anticipation include the medial prefrontal cortex (mPFC), which tracks high-level narrative schemas (<xref ref-type="bibr" rid="bib6">Baldassano et al., 2018</xref>) and has been proposed to play a general role in event prediction (<xref ref-type="bibr" rid="bib1">Alexander and Brown, 2014</xref>), and lateral prefrontal cortex, including the inferior frontal gyrus, which processes structured sequences across multiple domains (<xref ref-type="bibr" rid="bib39">Uddén and Bahlmann, 2012</xref>).</p><p>We also observed shorter-timescale anticipation throughout lateral occipital and ventral temporal cortex, which, though primarily thought to process bottom-up visual information, also exhibits event-specific patterns during recall (<xref ref-type="bibr" rid="bib10">Chen et al., 2017</xref>). A top-down memory-driven signal could be responsible for driving anticipatory activation in these regions during repeated movie viewing (<xref ref-type="bibr" rid="bib18">Finnie et al., 2021</xref>). Future work incorporating eye-tracking measurements could determine whether anticipatory eye movements can account for the temporal shifts in these regions, or if this anticipation is separate from the representation of the current retinal input.</p><p>We did not observe widespread anticipatory signals in primary sensory areas, although some prior fMRI studies have been able to observe such signals in early regions such as V1 (<xref ref-type="bibr" rid="bib2">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Kok et al., 2012</xref>). One possibility is that the rich, ongoing sensory input dominated relatively small anticipatory signals in these regions. Paradigms involving periods without any sensory input (e.g., occasionally removing the audiovisual movie from the screen during repeated viewings) may be necessary to detect these subtle signals. Alternatively, ultra-fast fMRI sequences (<xref ref-type="bibr" rid="bib16">Ekman et al., 2017</xref>) or alternative imaging modalities (discussed below) may be required to track anticipation at a subsecond scale.</p></sec><sec id="s3-2"><title>Relationship to previous studies of timescale hierarchies</title><p>Previous work has identified cumulatively longer timescales up the cortical hierarchy but has primarily focused on representations of the past. <xref ref-type="bibr" rid="bib30">Lerner et al., 2011</xref> demonstrated hierarchical cortical dynamics in participants who listened to variants of a 7 min narrative that was scrambled at different timescales (e.g., paragraphs, sentences, or words). Response reliability, measured as the correlation in BOLD activity timecourses across individuals, varied based on the timescale of scrambling, with higher-level brain regions responding consistently to only the more-intact narrative conditions. This led to the idea that higher-order brain regions contain larger ‘temporal receptive windows’ than lower-order areas, in that their activity at a given moment is influenced by relatively more of the past. Likewise, using intracranial EEG (iEEG), <xref ref-type="bibr" rid="bib23">Honey et al., 2012</xref> observed progressively longer temporal receptive windows in successive stages of the cortical hierarchy in participants who watched intact and scrambled versions of the movie <italic>Dog Day Afternoon</italic>. These findings can be described by the process memory framework (<xref ref-type="bibr" rid="bib21">Hasson et al., 2015</xref>), where hierarchical memory timescales process, represent, and support longer and longer units of information. We found that this hierarchy also exists in the prospective direction, with the degree of anticipatory temporal shifts increasing from posterior-to-anterior regions of the brain. Furthermore, regions with longer intrinsic processing timescales showed further-reaching anticipation. These results extend the process memory framework, suggesting that the timescales in these regions are relevant not only for online processing and memory, but also for future anticipation or simulation.</p><p>Although prior work has uncovered anticipatory and predictive coding in the brain, most studies have examined fixed, shorter timescales of anticipation. Moreover, these shorter timescales have often been studied using simple, non-narrative stimuli such as objects moving across the screen, short visual sequences, and visual pattern completion tasks (<xref ref-type="bibr" rid="bib2">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Gavornik and Bear, 2014</xref>; <xref ref-type="bibr" rid="bib22">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Kok et al., 2012</xref>). Some studies have used dynamic movie stimuli, but anticipation was measured via correlations between initial and repeated viewing of a movie at a constant fixed lag of 2 s (<xref ref-type="bibr" rid="bib36">Richardson and Saxe, 2020</xref>). Such an approach is not well suited to capturing dynamic levels of anticipation within and across brain regions.</p><p>Research investigating longer timescales of anticipation, such as learning future state representations in a maze task, examined single timescales up to 30 s ahead in OFC-VMPFC regions (<xref ref-type="bibr" rid="bib17">Elliott Wimmer and Büchel, 2019</xref>). Some studies that use narrative stimuli have examined specially constructed texts in order to manipulate predictions about upcoming sentences; for example, work by <xref ref-type="bibr" rid="bib24">Kandylaki et al., 2016</xref> demonstrated that predictive processing of referents in narratives can be modulated by voice (passive vs. active) and causality (high vs. low). Our results show that in a naturalistic setting, in which structure exists at many timescales, anticipation at multiple levels can occur in parallel across different brain regions. We found anticipation up to approximately 15 s into the future with our 90 s stimulus, but future work with stimuli of longer duration could uncover even longer timescales of anticipation, on the scale of minutes. Simultaneously maintaining expectations at varying timescales could allow for flexible behaviors, because different timescales of anticipation may be helpful for a variety of tasks and actions. Taking action to avoid immediate harm or danger would require shorter timescales of prediction, whereas cultivating social relationships demands predictions on longer timescales.</p><p>These results are consistent with those of <xref ref-type="bibr" rid="bib5">Baldassano et al., 2017</xref>, in which some participants listening to an audio narrative had advance knowledge of the high-level events of the story (because they had previously watched a movie version of the narrative). Using a similar HMM approach as in this paper, the authors observed shifts in event boundaries in higher-level regions including angular gyrus, posterior medial cortex, and mPFC. In the current study, however, participants were repeatedly exposed to an identical movie stimulus, allowing them to generate expectations at a broad range of timescales, including the timescales of fast-changing low-level visual features. This novel approach allowed us to observe for the first time that anticipation occurs in both low- and high-level regions, with shorter-timescale anticipation in visual occipital regions and the furthest-reaching anticipatory signals in prefrontal cortex.</p><p>Our model detects anticipation as temporal shifts in events, and though timepoints can reflect ‘mixed’ event assignments, it assumes that the underlying event patterns themselves (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) are constant. This view of anticipation is complementary to other theories of predictive representations, in which event patterns themselves should change over time to incorporate future information. One example is the ‘successor representation’ model from the field of reinforcement learning, which describes a representation in which each state (here, event representation) comes to include features of future events, weighted by their likelihood of occurring and their distance into the future (<xref ref-type="bibr" rid="bib13">Dayan, 1993</xref>). Successor representations can also be constructed at multiple scales (by changing the relative weighting of events near vs. far in the future). Such multi-scale representations are useful for goal-directed prediction that require multiple stages of planning (<xref ref-type="bibr" rid="bib35">Momennejad and Howard, 2018</xref>; <xref ref-type="bibr" rid="bib8">Brunec and Momennejad, 2019</xref>). Future work could explore how these two different theories could be integrated to model both mixing of event patterns and temporal shifts in the activation of these event patterns.</p></sec><sec id="s3-3"><title>Anticipation in other neuroimaging modalities</title><p>The current fMRI study is complementary to investigations of memory replay and anticipation that use MEG and iEEG. In an MEG study, <xref ref-type="bibr" rid="bib33">Michelmann et al., 2019</xref> found fast, compressed replay of encoded events during recall, with the speed of replay varying across the event. Furthermore, an iEEG investigation found anticipatory signals in auditory cortex when individuals listened to the same story twice (<xref ref-type="bibr" rid="bib34">Michelmann et al., 2020</xref>). In another MEG study, <xref ref-type="bibr" rid="bib40">Wimmer et al., 2020</xref> found compressed replay of previously encoded information. Replay was forward when participants were remembering what came after an event, and backward when participants were remembering what came before an event. The forward replay observed in the Wimmer et al. study may be similar to the anticipatory signals observed in the current study, although there was no explicit demand on memory retrieval in our paradigm. Thus, one possibility is that the anticipatory signals observed in MEG or iEEG are the same as those we observe in fMRI, except that they are necessarily sluggish and smoothed in time when measured via a hemodynamic response. This possibility is supported by fMRI work showing evidence for compressed anticipatory signals, albeit at a slower timescale relative to MEG (<xref ref-type="bibr" rid="bib16">Ekman et al., 2017</xref>).</p><p>An alternative possibility is that the anticipatory signals measured in our study are fundamentally different from those captured via MEG or iEEG. That could explain why we failed to find widespread anticipatory signals in primary visual or primary auditory cortex: the anticipatory signals in those regions might have been too fast to be captured with fMRI, particularly when competing with incoming, dynamic perceptual input. Future studies that obtain fMRI and MEG or iEEG in participants watching the same movie would be informative in that regard. It is possible that fMRI may be particularly well suited for capturing relatively slow anticipation of stable events, as opposed to faster anticipatory signals relating to fast sub-events. Nevertheless, advances in fMRI analyses may allow the detection of very fast replay or anticipation, closing the gap between these methods and allowing more direct comparisons (<xref ref-type="bibr" rid="bib41">Wittkuhn and Schuck, 2021</xref>).</p></sec><sec id="s3-4"><title>Future directions and conclusions</title><p>One limitation of the current work is the reliance on one movie clip. Movie clips of different durations might yield different results. For example, it is an open question whether the duration of anticipation scales with the length of the movie and playback speed or if the amount of anticipation is fixed (<xref ref-type="bibr" rid="bib31">Lerner et al., 2014</xref>; <xref ref-type="bibr" rid="bib7">Baumgarten et al., 2021</xref>). Furthermore, the content of the movie and how frequently event boundaries occur may change anticipation amounts. That said, anticipatory signals in naturalistic stimuli have been observed across multiple studies that use different movies and auditorily presented stories (e.g., <xref ref-type="bibr" rid="bib5">Baldassano et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Michelmann et al., 2020</xref>; also see <xref ref-type="bibr" rid="bib33">Michelmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Elliott Wimmer and Büchel, 2019</xref>; <xref ref-type="bibr" rid="bib40">Wimmer et al., 2020</xref>). Thus, it is likely that anticipatory hierarchies will also replicate across different stimuli. There may nevertheless be important differences across stimuli. For example, the specific regions that are involved in anticipation may vary depending on what the most salient features of a movie or narrative are (e.g., particular emotional states, actions, conversations, or perceptual information).</p><p>The detection of varying timescales of anticipation in the brain can be applied to multiple domains and modalities of memory research. Future work could explore even shorter timescales using other neuroimaging modalities, or longer timescales using longer movies or narratives from TV series that span multiple episodes. Furthermore, the impact of top-down goals on the hierarchy of anticipation timescales could be explored by using different tasks that require different levels of anticipation, such as anticipating camera angle changes vs. location changes. Brain stimulation studies or studies of patients with brain lesions could also explore the extent to which anticipation in lower-level regions relies on feedback from higher-level regions (<xref ref-type="bibr" rid="bib4">Auksztulewicz and Friston, 2016</xref>; <xref ref-type="bibr" rid="bib25">Kiebel et al., 2008</xref>).</p><p>The increased use of naturalistic, dynamic stimuli in neuroscience, and the development of methods to analyze the resulting data, has opened many avenues for research exploring flexible, future-oriented behavior. Our results and analysis approach provide a new framework for studying how anticipatory signals are distributed throughout the cortex, modulated by prior memory, and adaptive for improving comprehension and behavior.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Grand Budapest Hotel dataset</title><p>We used data collected by <xref ref-type="bibr" rid="bib3">Aly et al., 2018</xref>. Thirty individuals (12 men, age: M = 23.0 years, SD = 4.2; education: M = 15.3 years, SD = 3.2; all right-handed) watched movie clips from <italic>The Grand Budapest Hotel</italic> while undergoing fMRI. None of the participants reported previously seeing this movie. We analyzed data from the <italic>Intact</italic> condition, during which participants watched a continuous 90 s clip from the movie in its original temporal order. This clip was watched six times, interspersed with other video clips that are not considered here. This <italic>Intact</italic> clip depicts an interview scene between the protagonist and his future employer inside of the Grand Budapest Hotel. Stimuli and data are available on OpenNeuro: <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds001545/versions/1.1.1">https://openneuro.org/datasets/ds001545/versions/1.1.1</ext-link>.</p><p>Data were acquired on a 3T Siemens Prisma scanner with a 64-channel head/neck coil using a multiband echo planar imaging (EPI) sequence (repetition time = 1.5 s; echo time = 39 ms; flip angle = 50°; acceleration factor = 4; shift = 3; voxel size = 2.0 mm iso). T1-weighted structural images (whole-brain high-resolution; 1.0 mm iso) were acquired with an MPRAGE sequence. Field maps (40 oblique axial slices; 3 mm iso) were collected to aid registration. The fMRI scan took place over three experimental runs, each of which contained two presentations of the <italic>Intact</italic> movie clip (as well as other movie clips not considered here).</p><p>The first three EPI volumes of each run were discarded to allow for T1 equilibration. Data preprocessing was carried out in FSL, and included brain extraction, motion correction, high-pass filtering (max period = 140 s), spatial smoothing (3 mm FWHM Gaussian kernel), and registration to standard Montreal Neurological Institute (MNI) space. After preprocessing, the functional images for each run were divided into volumes that corresponded to each of the video clips presented within that run, and only the two <italic>Intact</italic> clips within each run are considered further. Finally, each voxel’s timecourse was z-scored to have zero mean and unit variance.</p></sec><sec id="s4-2"><title>Event annotations by human observers</title><p>Fourteen individuals (nine men) were asked to mark event boundaries corresponding to the same 90 s <italic>Intact</italic> clip from <italic>The Grand Budapest Hotel</italic> as shown to the fMRI participants. Each participant was asked to pause the clip at the end of a meaningful segment and to record the time and a brief title corresponding to the segment (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Specifically, they were given the following instructions: <italic>The movie clip can be divided into meaningful segments. Record the times denoting when you feel like a meaningful segment has ended. Pause the clip at the end of the segment, write down the time in the spreadsheet, and provide a short, descriptive title. Try to record segments with as few viewings of the movie clip as possible; afterward, record the number of times you viewed the clip.</italic> Although participants were allowed to watch the clip multiple times, they were instructed to minimize and report the number of viewings needed to complete the task. No participant reported watching the clip more than three times.</p></sec><sec id="s4-3"><title>Detecting anticipatory signals using an event segmentation model</title><p>Group-averaged fMRI data were fit with the event segmentation model described by <xref ref-type="bibr" rid="bib5">Baldassano et al., 2017</xref>. This HMM assumes that (1) events are a sequence of discrete states, (2) each event is represented in the brain by a unique spatial activity pattern, and (3) all viewings of the movie evoke the same sequence of activity patterns in the same order (though possibly with different timings). We fit the HMM jointly to all six viewings. This fitting procedure involved simultaneously estimating a sequence of event activity patterns that were shared across viewings, and estimating the probability of belonging to each of these events for every timepoint in all six datasets. The model was fit with seven events; this number was chosen to match the approximate timescale of the semantic events in the narrative, matching the mean number of events annotated by human observers (mean = 6.5).</p><p>After fitting the HMM, we obtain an event by timepoint matrix for each viewing, giving the probability that each timepoint belongs to each event. Note that because this assignment of timepoints to events is probabilistic, it is possible for the HMM to detect that the pattern of voxel activity at a timepoint reflects a mixture of multiple event patterns. This allows us to track subtle changes in the timecourse of how the brain is transitioning between events. We took the expectation over events at each timepoint, yielding curves showing the average event label at each timepoint for each viewing. To compute shifts in time between the first viewing and the average of repeated viewings, the area under the curve (AUC) was computed for each viewing. We then computed the amount of anticipation as the average AUC for repeated viewing (viewings 2–6) minus the AUC for the first viewing. In a supplementary analysis, we compared the first viewing to the last viewing only. To convert to seconds, we divide by the vertical extent of the graph (number of events minus 1) and multiplied by the repetition time (1.5 s). We then performed a one-tailed statistical test (described below) to determine whether this difference was significantly positive, indicating earlier event transitions with repeated viewing. Not only does this approach provide a way of quantifying anticipation, it gives us a trajectory of the most likely event at any given timepoint, as well as the onset and duration of each event.</p><p>We obtained whole-brain results using a searchlight analysis. We generated spherical searchlights spaced evenly throughout the MNI volume (radius = 5 voxels; stride = 5 voxels). We retained only the searchlights with at least 20 voxels which were inside a standard MNI brain mask and for which at least 15 participants had valid data for all viewings. We then used the SRM (<xref ref-type="bibr" rid="bib9">Chen et al., 2015</xref>) to functionally hyperalign all participants into shared 10-dimensional space (jointly fitting the alignment across all six viewings) and averaged their responses together. This produced a 10 feature by 60 timepoint data matrix for each of the six viewings, which was input to the HMM analysis described above. After running the analysis in all searchlights, the anticipation in each voxel was computed as the average anticipation of all searchlights that included that voxel.</p><p>To assess statistical significance, we utilized a permutation-based null hypothesis testing approach. We constructed null datasets by randomly shuffling each participant’s six responses to the six presentations of the movie clip. The full analysis pipeline (including hyperalignment) was run 100 times, once on the real (unpermuted) dataset and 99 times on null (permuted) datasets, with each analysis producing a map of anticipation across all voxels. A one-tailed p-value was obtained in each voxel by fitting a normal distribution to the null anticipation values, and then finding the fraction of this distribution that exceeded the real result in this voxel (i.e., showed more anticipation than in our unpermuted dataset). Voxels were determined significant (q&lt;0.05) after applying the Benjamini-Hochberg FDR correction, as implemented in AFNI (<xref ref-type="bibr" rid="bib12">Cox, 1996</xref>).</p><p>To determine if anticipation systematically varied across the cortex in the hypothesized posterior-to-anterior direction, we calculated the Spearman’s correlation between the Y-coordinate of each significant (q&lt;0.05) voxel (indexing the position of that voxel along the anterior/posterior axis) and the mean amount of anticipation in that voxel. To obtain a p-value, the observed correlation was compared to a null distribution in which the Spearman’s correlation was computed with the null anticipation values from the permutation analysis described above, in which the order of the viewings was randomly scrambled for each participant. For comparison, the correlation was also computed for the X (left-right) and Z (inferior-superior) axes. This analysis was repeated on unthresholded anticipation maps, to examine if this hierarchy remained even when including regions whose anticipation amounts did not reach statistical significance.</p><p>To relate the timescales of anticipation to the intrinsic timescales of brain regions during the first viewing, we fit the HMM on the first viewing alone, varying the number of events from 2 to 10. The HMM was trained on the average response from half of the participants (fitting the sequence of activity patterns for the events and the event variance) and the log-likelihood of the model was then measured on the average response in the other half of the participants. The training and testing sets were then swapped, and the log-likelihoods from both directions were averaged together. Hyperalignment was not used during this fitting process, to ensure that the training and testing sets remained independent. The number of events that yielded the largest log-likelihood was identified as the optimal number of events for that searchlight. The optimal number of events was then compared to the anticipation timescale in that region (from the main analysis), using Spearman’s correlation.</p><p>For comparison, we also ran a searchlight looking for anticipatory effects using a non-HMM cross-correlation approach. Within each searchlight, we obtained an average timecourse across all voxels and correlated the response to the first viewing with the average response to repeated viewings at differing lags. Using the same quadratic-fit approach for identifying the optimal lag described below, we tested whether the repeated-viewing timecourse was significantly ahead of the initial-viewing timecourse (relative to a null distribution in which the viewing order was shuffled within each subject). The p-values obtained were then corrected for FDR.</p></sec><sec id="s4-4"><title>Comparison of event boundaries in brain regions to annotations</title><p>We compared the event boundaries identified by the HMM within each searchlight to the event boundaries annotated by human observers. To obtain an event boundary timecourse from the annotations, we convolved the number of annotations (across all raters) at each second with the HRF (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Separately, we generated a continuous measure of HMM ‘boundary-ness’ at each timepoint by taking the derivative of the expected value of the event assignment for each timepoint, as illustrated in <xref ref-type="fig" rid="fig1">Figure 1d</xref>. Moments with high boundary strength indicate moments in which the brain pattern was rapidly switching between event patterns. We cross-correlated the HMM boundary strength timecourse for each viewing with the annotated event boundary timecourse, shifting the annotated timecourse forward and backward to determine the optimal temporal offset (with the highest correlation). We measured the timing of the peak correlation by identifying the local maximum in correlation closest to 0 lag, then fitting a quadratic function to the maximum correlation lag and its two neighboring lags and recording the location of the peak of this quadratic fit. This produced a continuous estimate of the optimal lag for each viewing. We measured the amount of shift between the optimal lag for the first viewing and the average of the optimal lags for repeated viewings, and obtained a p-value by comparing to the null distribution over maps with permuted viewing orders (as in the main analysis), then performed an FDR correction.</p><p>We identified three gray matter clusters significant at q&lt;0.05. To statistically assess whether the optimal lags differed from 0 in the three searchlights maximally overlapping these three clusters, we repeated the cross-correlation analysis in 100 bootstrap samples, in which we resampled from the raters who generated the annotated event boundaries. We obtained 95% bootstrap confidence intervals for maximally correlated lag on the first viewing and for the average of the maximally correlated lags on repeated viewings.</p></sec><sec id="s4-5"><title>Code and resource availability</title><p>Data preprocessing scripts and python code to reproduce all the results in this paper are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/dpmlab/Anticipation-of-temporally-structured-events">https://github.com/dpmlab/Anticipation-of-temporally-structured-events</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b4206124101d0eb41a414054ab42c61042674408;origin=https://github.com/dpmlab/Anticipation-of-temporally-structured-events;visit=swh:1:snp:e7d3b704ea175d657be758cda2c4071d7605bbfa;anchor=swh:1:rev:8fbd488c04d47148f9a53048de5d05a90e1c1663">swh:1:rev:8fbd488c04d47148f9a53048de5d05a90e1c1663</ext-link>). Results in MNI space can be viewed at <ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:9584">https://identifiers.org/neurovault.collection:9584</ext-link>.</p></sec></sec></body><back><ack id="ack"> <title>Acknowledgements</title><p>We thank the Aly and Baldassano labs for their feedback and support during this project, Janice Chen for helpful conversations about prediction hierarchies, and our three reviewers for proposing many useful improvements to the analyses.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-64972-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>We used a publicly-available dataset, from <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds001545/versions/1.1.1">https://openneuro.org/datasets/ds001545/versions/1.1.1</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Aly</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Learning Naturalistic Temporal Structure in the Posterior Medial Network</data-title><source>OpenNeuro</source><pub-id assigning-authority="other" pub-id-type="doi">10.18112/openneuro.ds001545.v1.1.1</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname> <given-names>WH</given-names></name><name><surname>Brown</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A general role for medial prefrontal cortex in event prediction</article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00069</pub-id><pub-id pub-id-type="pmid">25071539</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Schwiedrzik</surname> <given-names>CM</given-names></name><name><surname>Kohler</surname> <given-names>A</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus predictability reduces responses in primary visual cortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>2960</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3730-10.2010</pub-id><pub-id pub-id-type="pmid">20181593</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aly</surname> <given-names>M</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning naturalistic temporal structure in the posterior medial network</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1345</fpage><lpage>1365</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01308</pub-id><pub-id pub-id-type="pmid">30004848</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Repetition suppression and its contextual determinants in predictive coding</article-title><source>Cortex</source><volume>80</volume><fpage>125</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.11.024</pub-id><pub-id pub-id-type="pmid">26861557</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Zadbood</surname> <given-names>A</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Discovering event structure in continuous narrative perception and memory</article-title><source>Neuron</source><volume>95</volume><fpage>709</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.041</pub-id><pub-id pub-id-type="pmid">28772125</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representation of Real-World event schemas during narrative perception</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9689</fpage><lpage>9699</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0251-18.2018</pub-id><pub-id pub-id-type="pmid">30249790</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumgarten</surname> <given-names>TJ</given-names></name><name><surname>Maniscalco</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>JL</given-names></name><name><surname>Flounders</surname> <given-names>MW</given-names></name><name><surname>Abry</surname> <given-names>P</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural integration underlying naturalistic prediction flexibly adapts to varying sensory input rate</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2643</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22632-z</pub-id><pub-id pub-id-type="pmid">33976118</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brunec</surname> <given-names>IK</given-names></name><name><surname>Momennejad</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictive representations in hippocampal and prefrontal hierarchies</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/786434</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>P-H</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Yeshurun</surname> <given-names>Y</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Ramadge</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Reduced-Dimension fMRI shared response model</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Leong</surname> <given-names>YC</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name><name><surname>Yong</surname> <given-names>CH</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Shared memories reveal shared structure in neural activity across individuals</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>115</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nn.4450</pub-id><pub-id pub-id-type="pmid">27918531</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? predictive brains, situated agents, and the future of cognitive science</article-title><source>Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Improving generalization for temporal difference learning: the successor representation</article-title><source>Neural Computation</source><volume>5</volume><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Heilbron</surname> <given-names>M</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>den Ouden</surname> <given-names>HE</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Roiser</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Striatal prediction error modulates cortical coupling</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3210</fpage><lpage>3219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4458-09.2010</pub-id><pub-id pub-id-type="pmid">20203180</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname> <given-names>M</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time-compressed preplay of anticipated events in human primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15276</pub-id><pub-id pub-id-type="pmid">28534870</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott Wimmer</surname> <given-names>G</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning of distant state predictions by the orbitofrontal cortex in humans</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2554</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10597-z</pub-id><pub-id pub-id-type="pmid">31186425</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Finnie</surname> <given-names>PSB</given-names></name><name><surname>Komorowski</surname> <given-names>RW</given-names></name><name><surname>Bear</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The spatiotemporal organization of experience dictates hippocampal involvement in primary visual cortical plasticity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.01.433430</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id><pub-id pub-id-type="pmid">15937014</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavornik</surname> <given-names>JP</given-names></name><name><surname>Bear</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learned spatiotemporal sequence recognition and prediction in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>732</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/nn.3683</pub-id><pub-id pub-id-type="pmid">24657967</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Chen</surname> <given-names>J</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical process memory: memory as an integral component of information processing</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>304</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.04.006</pub-id><pub-id pub-id-type="pmid">25980649</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindy</surname> <given-names>NC</given-names></name><name><surname>Ng</surname> <given-names>FY</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Linking pattern completion in the Hippocampus to predictive coding in visual cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>665</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1038/nn.4284</pub-id><pub-id pub-id-type="pmid">27065363</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honey</surname> <given-names>CJ</given-names></name><name><surname>Thesen</surname> <given-names>T</given-names></name><name><surname>Donner</surname> <given-names>TH</given-names></name><name><surname>Silbert</surname> <given-names>LJ</given-names></name><name><surname>Carlson</surname> <given-names>CE</given-names></name><name><surname>Devinsky</surname> <given-names>O</given-names></name><name><surname>Doyle</surname> <given-names>WK</given-names></name><name><surname>Rubin</surname> <given-names>N</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow cortical dynamics and the accumulation of information over long timescales</article-title><source>Neuron</source><volume>76</volume><fpage>423</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.011</pub-id><pub-id pub-id-type="pmid">23083743</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kandylaki</surname> <given-names>KD</given-names></name><name><surname>Nagels</surname> <given-names>A</given-names></name><name><surname>Tune</surname> <given-names>S</given-names></name><name><surname>Kircher</surname> <given-names>T</given-names></name><name><surname>Wiese</surname> <given-names>R</given-names></name><name><surname>Schlesewsky</surname> <given-names>M</given-names></name><name><surname>Bornkessel-Schlesewsky</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Predicting &quot;When&quot; in Discourse Engages the Human Dorsal Auditory Stream: An fMRI Study Using Naturalistic Stories</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>12180</fpage><lpage>12191</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4100-15.2016</pub-id><pub-id pub-id-type="pmid">27903727</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname> <given-names>SJ</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A hierarchy of time-scales and the brain</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id><pub-id pub-id-type="pmid">19008936</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Failing</surname> <given-names>MF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Prior expectations evoke stimulus templates in the primary visual cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>1546</fpage><lpage>1554</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00562</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Associative prediction of visual shape in the Hippocampus</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>6888</fpage><lpage>6899</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0163-18.2018</pub-id><pub-id pub-id-type="pmid">29986875</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurby</surname> <given-names>CA</given-names></name><name><surname>Zacks</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Segmentation in the perception and memory of events</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>72</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.11.004</pub-id><pub-id pub-id-type="pmid">18178125</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname> <given-names>Y</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name><name><surname>Silbert</surname> <given-names>LJ</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>2906</fpage><lpage>2915</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3684-10.2011</pub-id><pub-id pub-id-type="pmid">21414912</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname> <given-names>Y</given-names></name><name><surname>Honey</surname> <given-names>CJ</given-names></name><name><surname>Katkov</surname> <given-names>M</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal scaling of neural responses to compressed and dilated natural speech</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>2433</fpage><lpage>2444</lpage><pub-id pub-id-type="doi">10.1152/jn.00497.2013</pub-id><pub-id pub-id-type="pmid">24647432</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Hairston</surname> <given-names>J</given-names></name><name><surname>Schrier</surname> <given-names>M</given-names></name><name><surname>Fan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Common and distinct networks underlying reward Valence and processing stages: a meta-analysis of functional neuroimaging studies</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>35</volume><fpage>1219</fpage><lpage>1236</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2010.12.012</pub-id><pub-id pub-id-type="pmid">21185861</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michelmann</surname> <given-names>S</given-names></name><name><surname>Staresina</surname> <given-names>BP</given-names></name><name><surname>Bowman</surname> <given-names>H</given-names></name><name><surname>Hanslmayr</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speed of time-compressed forward replay flexibly changes in human episodic memory</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>143</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0491-4</pub-id><pub-id pub-id-type="pmid">30944439</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Michelmann</surname> <given-names>S</given-names></name><name><surname>Price</surname> <given-names>AR</given-names></name><name><surname>Aubrey</surname> <given-names>B</given-names></name><name><surname>Doyle</surname> <given-names>WK</given-names></name><name><surname>Friedman</surname> <given-names>D</given-names></name><name><surname>Dugan</surname> <given-names>PC</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Moment-by-moment tracking of naturalistic learning and its underlying hippocampo-cortical interactions</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.09.416438</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Momennejad</surname> <given-names>I</given-names></name><name><surname>Howard</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predicting the future with multi-scale successor representations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/449470</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname> <given-names>H</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Development of predictive responses in theory of mind brain regions</article-title><source>Developmental Science</source><volume>23</volume><elocation-id>e12863</elocation-id><pub-id pub-id-type="doi">10.1111/desc.12863</pub-id><pub-id pub-id-type="pmid">31125472</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname> <given-names>T</given-names></name><name><surname>Critchley</surname> <given-names>HD</given-names></name><name><surname>Preuschoff</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A common role of insula in feelings, empathy and uncertainty</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.05.001</pub-id><pub-id pub-id-type="pmid">19643659</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Egner</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Expectation (and attention) in visual cognition</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>403</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id><pub-id pub-id-type="pmid">19716752</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uddén</surname> <given-names>J</given-names></name><name><surname>Bahlmann</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A rostro-caudal gradient of structured sequence processing in the left inferior frontal gyrus</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>367</volume><fpage>2023</fpage><lpage>2032</lpage><pub-id pub-id-type="doi">10.1098/rstb.2012.0009</pub-id><pub-id pub-id-type="pmid">22688637</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Vehar</surname> <given-names>N</given-names></name><name><surname>Behrens</surname> <given-names>TEJ</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Episodic memory retrieval success is associated with rapid replay of episode content</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0649-z</pub-id><pub-id pub-id-type="pmid">32514135</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittkuhn</surname> <given-names>L</given-names></name><name><surname>Schuck</surname> <given-names>NW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dynamics of fMRI patterns reflect sub-second activation sequences and reveal replay in human visual cortex</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1795</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-21970-2</pub-id><pub-id pub-id-type="pmid">33741933</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64972.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Geerligs</surname><given-names>Linda</given-names> </name><role>Reviewer</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.10.14.338145">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.10.14.338145v3">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This study uses innovative fMRI analysis methods to show how the brain predicts the future. It provides compelling evidence for anticipatory neural activity during repeated viewing of a movie clip, finding that different brain regions anticipate events to different degrees, mirroring the temporal integration windows of these brain regions.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Anticipation of temporally structured events in the brain&quot; for consideration by eLife. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Linda Geerligs (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>1. Provide more statistical support for differences between brain regions and the anterior-posterior hierarchy</p><p>2. Analyze repeated viewings separately, also to account for differences in reliability</p><p>3. Reconsider the event correlation analysis (see several specific suggestions below)</p><p>4. Consider optimizing analyses to reduce noise (and false negatives)</p><p><italic>Reviewer #1 (Recommendations for the authors (required)):</italic></p><p>– Were the correlations in Figure 6 corrected for multiple comparisons?</p><p>– The introduction mentions that &quot;The majority of studies examining anticipatory signals, however, have tested only one-step associations&quot;. Arguably, the current results might also reveal one-step associations, but with the step duration being longer in some regions than others (in line with previous findings of different temporal integration windows). Please discuss this possibility or clarify what you mean with one-step associations.</p><p>– The results are based on one 90-s movie segment with 7 events, mostly involving humans. To what extent may results be specific to this short segment and these specific events (e.g., anticipation of theory of mind or emotions)?</p><p>– To get a sense of the false-positive rate, it would be informative to see the same map (Figure 2) but testing for the opposite temporal direction, as Supplementary file.</p><p>– As Supp Figure 2, please also show unthresholded maps (cf Figure 3 of Baldassano, 2017) plotting both positive and negative anticipation. This would give a fuller insight into the data, also in regions that didn't cross the threshold.</p><p>– In the table in Figure 6, one correlation of 0.52 is not indicated as significant (cluster 7). Similarly, the difference (0.29) is also not indicated as significant. Is this correct?</p><p><italic>Reviewer #2 (Recommendations for the authors (required)):</italic></p><p>Below I will mention concrete suggestions for improvement related to the points in the public recommendation.</p><p>1. I would suggest repeating all analyses using the estimated real/optimal number of events in each brain region, rather than the number that was based on behavioural annotations.</p><p>2. Repeating the analyses with hyper-aligned data should reduce the amount of noise in the group-averaged data.</p><p>3. Using only viewing 6 as the repeated viewing condition may improve the detection of anticipatory signals in early sensory areas. Looking at how the amount of anticipation changes across all viewings would add an interesting new dimension to the results presented in the paper.</p><p>4. An alternative approach to this analysis is to vary the HRF delay for the annotated events and investigate which delay shows the optimal correlation. This approach would provide additional evidence for the estimated amount of anticipation shown in figure 2.</p><p>5. Rather than grouping voxels based on the identified cluster, I would suggest either sticking to the original searchlight definitions or grouping searchlights based on the similarity of their event boundaries.</p><p>6. A discussion about this issue may be a valuable addition to the Discussion section.</p><p><italic>Reviewer #3 (Recommendations for the authors (required)):</italic></p><p>1. The discussion is relatively quite long.</p><p>2. It seems like the brain maps in Figure 6 should be added to Figure 2, or their own figure, before the annotation correlation-related results in Figure 5 and the table in Figure 6. As presented, it is confusing and not initially clear why there are clusters with no significant correlation results – that the annotation analysis presented is independent from the analysis that identified the clusters.</p><p>3. In Figure 5, the temporal language is unclear. 'Backward' and 'forward' here are confusing descriptors, e.g. backward can be behind the current position (earlier in time) or pushed 'back' later in time. On a different note about this figure, it should have brain labels in the text of the correlation plots, the same cluster numbering added as in Figure 6, and panel letters.</p><p>4. The original report in Aly et al., (2018) notes that no participants had previously viewed the movie that the clips were taken from. I may have missed it, but it would be helpful to repeat that information here.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64972.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>1. Provide more statistical support for differences between brain regions and the anterior-posterior hierarchy</p></disp-quote><p>Thank you for raising this important concern. We briefly outline our major changes here, and describe our changes in more detail below in response to individual reviewer comments. First, we related anticipation amounts to the position of brain regions along the anterior/posterior axis, and indeed found that anticipation amounts progressively increase from posterior to anterior parts of the brain (Spearman’s rho = 0.58, p = 0.0030). Second, this systematicity exists even when the analysis was done on an unthresholded statistical map (Spearman’s rho = 0.42, p = 0.0028; Figure 2—figure supplement 1). Finally, we explored whether brain regions with faster vs slower activity dynamics (i.e., more vs fewer events during the initial viewing of the movie) showed differences in anticipation amounts. We found that regions that integrate information over more of the past (i.e., show fewer, longer events) show more anticipation (Spearman’s rho = 0.319, p = 0.00031; Figure 2 – Figure supplement 3). This is consistent with our hypothesis that the <italic>retrospective</italic> temporal hierarchy observed in prior studies (Hasson et al., 2008; Hasson et al., 2015; Lerner et al., 2011) is directly related to the anticipatory hierarchy that we describe here.</p><disp-quote content-type="editor-comment"><p>2. Analyze repeated viewings separately, also to account for differences in reliability</p></disp-quote><p>We implemented the changes recommended by the viewers. First, we entered each viewing separately into the HMM analyses and then averaged the amount of anticipation across repetitions 2-6 (rather than averaging the timecourses of the repetitions before entering them into the HMM). This is now our main analysis, and each viewing is now shown as a separate line in our example time by event plots (Figure 2B). We also compared the first viewing to the last (6th) viewing alone, which yielded a similar result (Figure 2—figure supplement 2). Finally, when relating the brain’s event representations to human-annotated events (Figure 5), we examine the brain data for each viewing separately and present the data for all six movie presentations. Our main conclusions remain unchanged when taking these approaches.</p><disp-quote content-type="editor-comment"><p>3. Reconsider the event correlation analysis (see several specific suggestions below)</p></disp-quote><p>We agree that the most important test for this analysis is whether there is a systematic shift, across movie repetitions, in the timing of the peak cross-correlation between the brain’s event transitions and human-annotated event boundaries. To test this, we conducted a new analysis in which we measured the timing of the peak cross-correlation between HMM-derived event transitions in the brain and the human-annotated event boundaries, separately for each of the six movie viewings. In other words, we found the amount of shift in the brain’s event transitions that led to the maximum correlation with the timing of the human-annotated event boundaries. We then compared the timing of the correlation peak for the first movie viewing to the timing of the mean peak across viewings 2-6, and found regions of the brain where the peak shifted to be earlier with subsequent movie viewings. This was done as a whole-brain analysis with FDR correction. We include a figure (Figure 5) showing the data for the three searchlights that corresponded to clusters that met the q &lt;.05 FDR criterion.</p><p>The preceding analysis looked for regions for which the timing of the peak cross-correlation between the brain’s events and human-annotated events shifted earlier over movie repetitions, but did not test for the absolute location of that peak correlation (relative to zero lag between the HMM events and annotated events). Do the brain’s event transitions occur before annotated event transitions, after, or are they aligned? And how does this change over movie repetitions? We examined this question in the three clusters that emerged from the analysis in the preceding paragraph. We found that for the initial viewing, the brain’s event transitions lagged behind human-annotated event boundaries for two of the three clusters, whereas for the last cluster, the brain’s transitions and subjective event boundaries were aligned. For repeated viewings, the timing of the peak correlations shifted such that the brain’s representations of an event transition reliably preceded the occurrence of the human-annotated event boundary, for all three clusters (Figure 5).</p><p>Together, these results confirm that, in some regions, the best alignment between the brain’s event transitions and human-annotated event boundaries shifts over movie repetitions such that the brain’s event transitions start to occur earlier over repetitions. In particular, the brain’s events shift to precede subjective event boundaries.</p><disp-quote content-type="editor-comment"><p>4. Consider optimizing analyses to reduce noise (and false negatives)</p></disp-quote><p>We implemented many changes to this end, which we will briefly describe here and describe in more detail in response to individual reviewer comments. First, we hyperaligned the brain data of individual participants before conducting our anticipation analyses, using the &quot;Shared Response Model&quot; (SRM) hyperalignment approach (Chen et al., 2015). Hyperalignment projects features (e.g., voxels) from individual brains into a common high-dimensional space, in which features across individuals share functional properties as opposed to anatomical locations. This approach increases the sensitivity of analyses, such as our HMM approach, that depend on across-brain similarities because traditional anatomical alignment approaches do not accommodate idiosyncrasies in fine-grained functional topographies across individuals. This approach uncovered anticipation in more widespread regions compared to our initial (anatomically aligned) analyses. The hyperaligned analyses now replace our prior analyses. Second, due to reviewer concerns about whether we were properly controlling the rate of false positives in our maps, we replaced our bootstrapping-based approach with a permutation-based approach. Rather than resampling participants to produce confidence intervals on our results, we permuted the order of the viewings to generate null maps and then computed p values by comparing our results to these null results. This approach yielded similar p values to our original bootstrapping approach, verifying that we are appropriately controlling our false positive rate. We then applied a False Discovery Rate (FDR) correction as before, to account for multiple comparisons across voxels. Third, we now share and analyze unthresholded maps of anticipation in the brain. As noted above, the anticipation hierarchy persists even when the analysis is conducted on an anticipation map that was not corrected for statistical significance.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors (required)):</p><p>– Were the correlations in Figure 6 corrected for multiple comparisons?</p></disp-quote><p>In our initial manuscript, those correlations were not corrected for multiple comparisons. In our revision, we have replaced our initial analysis examining the relationship between the brain’s event transitions and human-annotated event boundaries with a new analysis. This new analyses tests for shifts in the relationship between brain and human-annotated event boundaries over movie viewings in a searchlight across the whole cortex (as described in Essential Revision # 3; see Figure 5). This new analysis is FDR-corrected at q &lt; 0.05.</p><disp-quote content-type="editor-comment"><p>– The introduction mentions that &quot;The majority of studies examining anticipatory signals, however, have tested only one-step associations&quot;. Arguably, the current results might also reveal one-step associations, but with the step duration being longer in some regions than others (in line with previous findings of different temporal integration windows). Please discuss this possibility or clarify what you mean with one-step associations.</p></disp-quote><p>Thank you for raising this issue. We have clarified our language, to state that most studies have used discrete items as stimuli and looked for anticipation of the single item that was coming up next:</p><p>“The majority of studies examining anticipatory signals, however, have tested anticipation based on memory for relatively simple associations between pairs of discrete stimuli, such as auditory tones , lines, dots, oriented gratings, or abstract objects (e.g., Alink, Schwiedrzik, Kohler, Singer, and Muckli, 2010; Gavornik and Bear, 2014; Hindy, Ng, and Turk-Browne, 2016; Kok, Jehee, and de Lange, 2012; Kok, Failing, and de Lange, 2014; Kok and Turk-Browne, 2018). These studies have found anticipatory signals about a single upcoming stimulus in a variety of brain regions, from perceptual regions (Kok et al., 2012, 2014) to the medial temporal lobe (Hindy et al., 2016; Kok and Turk-Browne, 2018).” (p.2)</p><disp-quote content-type="editor-comment"><p>– The results are based on one 90-s movie segment with 7 events, mostly involving humans. To what extent may results be specific to this short segment and these specific events (e.g., anticipation of theory of mind or emotions)?</p></disp-quote><p>Thank you for raising this point, which we agree is important. We now mention this limitation in the Discussion:</p><p>“One limitation of the current work is the reliance on one movie clip. Movie clips of different durations might yield different results. For example, it is an open question whether the duration of anticipation scales with the length of the movie or if the amount of anticipation is fixed (c.f., Lerner, Honey, Katkov, and Hasson, 2014). Furthermore, the content of the movie and how frequently event boundaries occur may change anticipation amounts. That said, anticipatory signals in naturalistic stimuli have been observed across multiple studies that use different movies and auditorily presented stories (e.g., Baldassano et al., 2017; Michelmann et al., 2020; also see Michelmann et al., 2019; Wimmer and Büchel, 2019; Wimmer et al., 2020). Thus, it is likely that anticipatory hierarchies will also replicate across different stimuli. There may nevertheless be important differences across stimuli. For example, the specific regions that are involved in anticipation may vary depending on what the most salient features of a movie or narrative are (e.g., particular emotional states, actions, conversations, or perceptual information).” (p.10)</p><disp-quote content-type="editor-comment"><p>– To get a sense of the false-positive rate, it would be informative to see the same map (Figure 2) but testing for the opposite temporal direction, as Supplementary file.</p></disp-quote><p>We agree that it is important to get a sense of the false-positive rate. We considered this proposed approach, but we are not sure if the opposite temporal direction provides a measure of the false positive rate. Instead, the opposite temporal direction — on repeated viewings, brain areas lag behind initial viewing — could be an interesting phenomenon in its own right. That could reflect, for example, holding on to the past for longer amounts of time in order to better integrate information with what is coming up next.</p><p>We therefore opted to use permutation tests to get a more direct measure of the false positive rate. For these permutation tests, we randomly shuffled movie viewings within each participant before conducting the anticipation analysis. This random shuffling was done 99 items, allowing us to obtain a null distribution of anticipation for each searchlight. This null distribution was used to calculate a p-value for each searchlight by computing the z-score of our result relative to the null distribution and then obtaining a p-value from a Normal survival function. The p-value map was then FDR-corrected with q &lt; 0.05.</p><p>Although we did not statistically test for temporal shifts in the opposite direction (with activity shifting later on repeated viewings), the unthresholded map of positive and negative anticipation can be viewed at https://identifiers.org/neurovault.collection:9584, and shows very few regions with negative values of anticipation.</p><disp-quote content-type="editor-comment"><p>– As Supp Figure 2, please also show unthresholded maps (cf Figure 3 of Baldassano, 2017) plotting both positive and negative anticipation. This would give a fuller insight into the data, also in regions that didn't cross the threshold.</p></disp-quote><p>We agree this is useful, and have included the unthresholded map as Figure 2—figure supplement 1. As noted earlier, the posterior-to-anterior hierarchy of anticipation is also present in this unthresholded map.</p><disp-quote content-type="editor-comment"><p>– In the table in Figure 6, one correlation of 0.52 is not indicated as significant (cluster 7). Similarly, the difference (0.29) is also not indicated as significant. Is this correct?</p></disp-quote><p>That was actually correct; that value failed to reach statistical significance because of very high variance. That said, that analysis (and table) has since been replaced (see Essential Revision #3).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors (required)):</p><p>Below I will mention concrete suggestions for improvement related to the points in the public recommendation.</p><p>1. I would suggest repeating all analyses using the estimated real/optimal number of events in each brain region, rather than the number that was based on behavioural annotations.</p></disp-quote><p>Please see our response under point #2 in the public recommendation of this reviewer. Briefly, we found that the optimal number of events (defined on the first viewing) did differ across the cortex in a way that was correlated with the degree of anticipation. We chose to use a fixed number of events in the anticipation analyses in the manuscript to avoid a potential confound between the number of events used in the HMM and the degree of anticipation detected.</p><p>That said, we nevertheless ran this proposed analysis, in which the number of events used in the anticipation analysis was set to the optimal value based on the first viewing in each region, and obtained the following result (thresholded at q&lt;0.05):</p><p>Although noisier than our main analysis, this result does replicate the general posterior-to-anterior topography of anticipation. Again, we chose not to include this in the manuscript due to our concerns that this analysis could produce an artificial relationship between event timescales during initial viewing and degree of anticipation. We hope that our compromise — relating optimal event numbers to anticipation amounts in our main analysis — is a satisfactory approach given the difficulties in interpreting this proposed analysis.</p><disp-quote content-type="editor-comment"><p>2. Repeating the analyses with hyper-aligned data should reduce the amount of noise in the group-averaged data.</p></disp-quote><p>This useful suggestion has been implemented. All the analyses were repeated after hyper-alignment. The same pattern of results emerged, but anticipatory signals are now generally more widespread and robust.</p><disp-quote content-type="editor-comment"><p>3. Using only viewing 6 as the repeated viewing condition may improve the detection of anticipatory signals in early sensory areas. Looking at how the amount of anticipation changes across all viewings would add an interesting new dimension to the results presented in the paper.</p></disp-quote><p>Thank you for these recommendations. We now statistically compare the first viewing to the last viewing and show those results as Figure 2—figure supplement 2. These data exhibit the same overall pattern as the first viewing compared to all subsequent viewings. We also plot data for each viewing separately in the event by time plots in Figure 2. Finally, we show the relationship between the brain’s event boundaries and human-annotated event boundaries for each viewing separately in Figure 5. Visual inspection of the latter two figures shows that anticipation generally increases with subsequent movie viewings.</p><disp-quote content-type="editor-comment"><p>4. An alternative approach to this analysis is to vary the HRF delay for the annotated events and investigate which delay shows the optimal correlation. This approach would provide additional evidence for the estimated amount of anticipation shown in figure 2.</p></disp-quote><p>Thank you for this suggestion. If we are interpreting it correctly, this is functionally equivalent to what we did. In particular, a reduced HRF delay ( i.e., an HRF that is shifted earlier in time) is analogous to shifting the convolved timecourse earlier in time. Likewise, an increased HRF delay (i.e., an HRF that is shifted later in time) is analogous to shifting the convolved timecourse later in time. However, please let us know if we misinterpreted your comment and should consider a different alternative. For example, if the suggestion is to change the delay between the HRF onset and its peak, that would indeed yield different results. However, such an analysis would have to be done carefully so that it remains biologically plausible.</p><disp-quote content-type="editor-comment"><p>5. Rather than grouping voxels based on the identified cluster, I would suggest either sticking to the original searchlight definitions or grouping searchlights based on the similarity of their event boundaries.</p></disp-quote><p>All analyses are now conducted with the same whole-brain searchlight approach, without any post-hoc grouping or clustering.</p><disp-quote content-type="editor-comment"><p>6. A discussion about this issue may be a valuable addition to the Discussion section.</p></disp-quote><p>We have clarified that the HMM produces a probability distribution across states (events) at each time-point, i.e., an activity pattern at any given time-point can reflect a mixture of current and upcoming events.</p><p>This is clarified in the caption to Figure 1:</p><p>“By fitting a Hidden Markov Model (HMM) jointly to all viewings, we can identify this shared sequence of event patterns, as well as a probabilistic estimate of event transitions. Regions with anticipatory representations are those in which event transitions occur earlier in time for repeated viewings of a stimulus compared to the initial viewing , indicated by an upward shift on the plot of the expected value of the event at each timepoint.” (p.3)</p><p>And in the caption to Figure 2:</p><p>“Because the HMM produces a probability distribution across states at each timepoint, which can reflect a combination of current and upcoming event representations, we plot the expected value of the event assignments at each timepoint.” (p.5)</p><p>And also in the Methods:</p><p>“After fitting the HMM, we obtain an event by time-point matrix for each viewing , giving the probability that each timepoint belongs to each event. Note that, because this assignment of timepoints to events is probabilistic, it is possible for the HMM to detect that the pattern of voxel activity at a timepoint reflects a mixture of multiple event patterns, allowing us to track subtle changes in the timecourse of how the brain is transitioning between events.” (p.13)</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors (required)):</p><p>1. The discussion is relatively quite long.</p></disp-quote><p>We apologize for that! We tried to condense when possible, but it was unfortunately difficult given reviewer requests to relate our findings to other relevant work and clarify the implications of our results. We have now added subsection headings to better organize the Discussion.</p><disp-quote content-type="editor-comment"><p>2. It seems like the brain maps in Figure 6 should be added to Figure 2, or their own figure, before the annotation correlation-related results in Figure 5 and the table in Figure 6. As presented, it is confusing and not initially clear why there are clusters with no significant correlation results – that the annotation analysis presented is independent from the analysis that identified the clusters.</p></disp-quote><p>We agree that running the annotation correlation analysis on post-hoc clusters led to some confusion. In the revised version of the manuscript, the (new) annotation correlation analysis is conducted as a separate whole-brain searchlight analysis, and the significant clusters are shown in Figure 5.</p><disp-quote content-type="editor-comment"><p>3. In Figure 5, the temporal language is unclear. 'Backward' and 'forward' here are confusing descriptors, e.g. backward can be behind the current position (earlier in time) or pushed 'back' later in time. On a different note about this figure, it should have brain labels in the text of the correlation plots, the same cluster numbering added as in Figure 6, and panel letters.</p></disp-quote><p>Thank you for pointing this out. We changed that sentence to the following:</p><p>“Negative lags show the correlations when the human-annotated event timecourse is shifted earlier in time, and positive lags show the correlation when the human-annotated event timecourse is shifted later in time.” (p.7)</p><p>With respect to Figure 5, the old figure has been replaced with one depicting the analyses in which we look for significant <italic>shifts</italic> in the peak cross-correlation between the brain’s event boundaries and human-annotated event boundaries. The new figure has the clusters labeled clearly with their names, and we no longer use cluster numbers (see discussion with reviewer #1, with respect to functionally heterogeneous clusters).</p><disp-quote content-type="editor-comment"><p>4. The original report in Aly et al., (2018) notes that no participants had previously viewed the movie that the clips were taken from. I may have missed it, but it would be helpful to repeat that information here.</p></disp-quote><p>This is a good point, and we have added this clarification to the Methods:</p><p>“None of the participants reported previously seeing this movie.” (p.12)</p></body></sub-article></article>