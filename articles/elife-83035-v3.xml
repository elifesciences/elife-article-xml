<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83035</article-id><article-id pub-id-type="doi">10.7554/eLife.83035</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural population dynamics of computing with synaptic modulations</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-264368"><name><surname>Aitken</surname><given-names>Kyle</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0207-5885</contrib-id><email>kyle.aitken@alleninstitute.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-94457"><name><surname>Mihalas</surname><given-names>Stefan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2629-7100</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cpe7c52</institution-id><institution>Allen Institute, MindScope Program</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mongillo</surname><given-names>Gianluigi</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f82e368</institution-id><institution>Université Paris Descartes</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e83035</elocation-id><history><date date-type="received" iso-8601-date="2022-08-27"><day>27</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-02-22"><day>22</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-06-30"><day>30</day><month>06</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.06.27.497776"/></event></pub-history><permissions><copyright-statement>© 2023, Aitken and Mihalas</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Aitken and Mihalas</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83035-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83035-figures-v3.pdf"/><abstract><p>In addition to long-timescale rewiring, synapses in the brain are subject to significant modulation that occurs at faster timescales that endow the brain with additional means of processing information. Despite this, models of the brain like recurrent neural networks (RNNs) often have their weights frozen after training, relying on an internal state stored in neuronal activity to hold task-relevant information. In this work, we study the computational potential and resulting dynamics of a network that relies solely on synapse modulation during inference to process task-relevant information, the multi-plasticity network (MPN). Since the MPN has no recurrent connections, this allows us to study the computational capabilities and dynamical behavior contributed by synapses modulations alone. The generality of the MPN allows for our results to apply to synaptic modulation mechanisms ranging from short-term synaptic plasticity (STSP) to slower modulations such as spike-time dependent plasticity (STDP). We thoroughly examine the neural population dynamics of the MPN trained on integration-based tasks and compare it to known RNN dynamics, finding the two to have fundamentally different attractor structure. We find said differences in dynamics allow the MPN to outperform its RNN counterparts on several neuroscience-relevant tests. Training the MPN across a battery of neuroscience tasks, we find its computational capabilities in such settings is comparable to networks that compute with recurrent connections. Altogether, we believe this work demonstrates the computational possibilities of computing with synaptic modulations and highlights important motifs of these computations so that they can be identified in brain-like systems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>neural population dynamics</kwd><kwd>synapse dynamics</kwd><kwd>recurrent neural networks</kwd><kwd>synaptic plasticity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RF1DA055669</award-id><principal-award-recipient><name><surname>Mihalas</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EB029813</award-id><principal-award-recipient><name><surname>Mihalas</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100017008</institution-id><institution>Allen Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Aitken</surname><given-names>Kyle</given-names></name><name><surname>Mihalas</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Synaptic modulations alone imbue networks with computational capabilities comparable to recurrent connections on several neuroscience-relevant tasks, which manifest in fundamentally different neuronal dynamics.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The brain’s synapses constantly change in response to information under several distinct biological mechanisms (<xref ref-type="bibr" rid="bib37">Love, 2003</xref>; <xref ref-type="bibr" rid="bib26">Hebb, 2005</xref>; <xref ref-type="bibr" rid="bib5">Bailey and Kandel, 1993</xref>; <xref ref-type="bibr" rid="bib47">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib13">Bi and Poo, 1998</xref>; <xref ref-type="bibr" rid="bib64">Stevens and Wang, 1995</xref>; <xref ref-type="bibr" rid="bib46">Markram and Tsodyks, 1996</xref>). These changes can serve significantly different purposes and occur at drastically different timescales. Such mechanisms include synaptic rewiring, which modifies the topology of connections between neurons in our brain and can be as fast as minutes to hours. Rewiring is assumed to be the basis of long-term memory that can last a lifetime (<xref ref-type="bibr" rid="bib5">Bailey and Kandel, 1993</xref>). At faster timescales, individual synapses can have their strength modified (<xref ref-type="bibr" rid="bib47">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib13">Bi and Poo, 1998</xref>; <xref ref-type="bibr" rid="bib64">Stevens and Wang, 1995</xref>; <xref ref-type="bibr" rid="bib46">Markram and Tsodyks, 1996</xref>). These changes can occur over a spectrum of timescales and can be intrinsically transient (<xref ref-type="bibr" rid="bib64">Stevens and Wang, 1995</xref>; <xref ref-type="bibr" rid="bib46">Markram and Tsodyks, 1996</xref>). Though such mechanisms may not immediately lead to structural changes, they are thought to be vital to the brain’s function. For example, short-term synaptic plasticity (STSP) can affect synaptic strength on timescales less than a second, with such effects mainly presynaptic-dependent (<xref ref-type="bibr" rid="bib64">Stevens and Wang, 1995</xref>; <xref ref-type="bibr" rid="bib69">Tsodyks and Markram, 1997</xref>). At slower timescales, long-term potentiation (LTP) can have effects over minutes to hours or longer, with the early phase being dependent on local signals and the late phase including a more complex dependence on protein synthesis (<xref ref-type="bibr" rid="bib7">Baltaci et al., 2019</xref>). Also on the slower end, spike-time-dependent plasticity (STDP) adjusts the strengths of connections based on the relative timing of pre- and postsynaptic spikes (<xref ref-type="bibr" rid="bib47">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib13">Bi and Poo, 1998</xref>; <xref ref-type="bibr" rid="bib51">McFarlan et al., 2023</xref>).</p><p>In this work, we investigate a new type of artificial neural network (ANN) that uses biologically motivated synaptic modulations to process short-term sequential information. The <italic>multi-plasticity network</italic> (MPN) learns using two complementary plasticity mechanisms: (1) long-term synaptic rewiring via standard supervised ANN training and (2) simple synaptic modulations that operate at faster timescales. Unlike many other neural network models with synaptic dynamics (<xref ref-type="bibr" rid="bib70">Tsodyks et al., 1998</xref>; <xref ref-type="bibr" rid="bib53">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Lundqvist et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Barak and Tsodyks, 2014</xref>; <xref ref-type="bibr" rid="bib54">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib6">Ballintyn et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Masse et al., 2019</xref>), <italic>the MPN has no recurrent synaptic connections</italic>, and thus can only rely on modulations of synaptic strengths to pass short-term information across time. Although both recurrent connections and synaptic modulation are present in the brain, it can be difficult to isolate how each of these affects temporal computation. The MPN thus allows for an in-depth study of the computational power of synaptic modulation alone and how the dynamics behind said computations may differ from networks that rely on recurrence. Having established how modulations alone compute, we believe it will be easier to disentangle synaptic computations from brain-like networks that may compute using a combination of recurrent connections, synaptic dynamics, neuronal dynamics, etc.</p><p>Biologically, the modulations in the MPN represent a general synapse-specific change of strength on shorter timescales than the structural changes, the latter of which are represented by weight adjustment via backpropagation. We separately consider two forms of modulation mechanisms, one of which is dependent on <italic>both</italic> the pre- and postsynaptic firing rates and a second that only depends on presynaptic rates. The first of these rules is primarily envisioned as coming from associative forms of plasticity that depend on both pre- and postsynaptic neuron activity (<xref ref-type="bibr" rid="bib47">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib13">Bi and Poo, 1998</xref>; <xref ref-type="bibr" rid="bib51">McFarlan et al., 2023</xref>). Meanwhile, the second type of modulation models presynaptic-dependent STSP (<xref ref-type="bibr" rid="bib53">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib76">Zucker and Regehr, 2002</xref>). While both these mechanisms can arise from distinct biological mechanisms and can span timescales of many orders of magnitude, the MPN uses simplified dynamics to keep the effects of synaptic modulations and our subsequent results as general as possible. It is important to note that in the MPN, as in the brain, the mechanisms that represent synaptic modulations and rewiring are not independent of one another – changes in one affect the operation of the other and vice versa.</p><p>To understand the role of synaptic modulations in computing and how they can change neuronal dynamics, throughout this work we contrast the MPN with recurrent neural networks (RNNs), whose synapses/weights remain fixed after a training period. RNNs store temporal, task-relevant information in transient internal neural activity using recurrent connections and have found widespread success in modeling parts of our brain (<xref ref-type="bibr" rid="bib17">Cannon et al., 1983</xref>; <xref ref-type="bibr" rid="bib11">Ben-Yishai et al., 1995</xref>; <xref ref-type="bibr" rid="bib61">Seung, 1996</xref>; <xref ref-type="bibr" rid="bib75">Zhang, 1996</xref>; <xref ref-type="bibr" rid="bib23">Ermentrout, 1998</xref>; <xref ref-type="bibr" rid="bib67">Stringer et al., 2002</xref>; <xref ref-type="bibr" rid="bib73">Xie et al., 2002</xref>; <xref ref-type="bibr" rid="bib25">Fuhs and Touretzky, 2006</xref>; <xref ref-type="bibr" rid="bib14">Burak and Fiete, 2009</xref>). Although RNNs model the brain’s significant recurrent connections, the weights in these networks neglect the role transient synaptic dynamics can have in adjusting synaptic strengths and processing information.</p><p>Considerable progress has been made in analyzing brain-like RNNs as population-level dynamical systems, a framework known as <italic>neural population dynamics</italic> (<xref ref-type="bibr" rid="bib72">Vyas et al., 2020</xref>). Such studies have revealed a striking universality of the underlying computational scaffold across different types of RNNs and tasks (<xref ref-type="bibr" rid="bib43">Maheswaranathan et al., 2019b</xref>). To elucidate how computation through synaptic modulations affect neural population behavior, we thoroughly characterize the MPN’s low-dimensional behavior in the neural population dynamics framework (<xref ref-type="bibr" rid="bib72">Vyas et al., 2020</xref>). Using a novel approach of analyzing the synapse population behavior, we find the MPN computes using completely different dynamics than its RNN counterparts. We then explore the potential benefits behind its distinct dynamics on several neuroscience-relevant tasks.</p><sec id="s1-1"><title>Contributions</title><p>The primary contributions and findings of this work are as follows:</p><list list-type="bullet"><list-item><p>We elucidate the neural population dynamics of the MPN trained on integration-based tasks and show it operates with qualitatively different dynamics and attractor structure than RNNs. We support this with analytical approximations of said dynamics.</p></list-item><list-item><p>We show how the MPN’s synaptic modulations allow it to store and update information in its state space using a task-independent, single point-like attractor, with dynamics slower than task-relevant timescales.</p></list-item><list-item><p>Despite its simple attractor structure, for integration-based tasks, we show the MPN performs at level comparable or exceeding RNNs on several neuroscience-relevant measures.</p></list-item><list-item><p>The MPN is shown to have dynamics that make it a more effective reservoir, less susceptible to catastrophic forgetting, and more flexible to taking in new information than RNN counterparts.</p></list-item><list-item><p>We show the MPN is capable of learning more complex tasks, including contextual integration, continuous integration, and 19 neuroscience tasks in the NeuroGym package (<xref ref-type="bibr" rid="bib52">Molano-Mazon et al., 2022</xref>). For a subset of tasks, we elucidate the changes in dynamics that allow the network to solve them.</p></list-item></list></sec><sec id="s1-2"><title>Related work</title><p>Networks with synaptic dynamics have been investigated previously (<xref ref-type="bibr" rid="bib70">Tsodyks et al., 1998</xref>; <xref ref-type="bibr" rid="bib53">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib68">Sugase-Miyamoto et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Lundqvist et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Barak and Tsodyks, 2014</xref>; <xref ref-type="bibr" rid="bib54">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib6">Ballintyn et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Masse et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Hu et al., 2021</xref>; <xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>; <xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Rodriguez et al., 2022</xref>). As we mention above, many of these works investigate networks with both synaptic dynamics and recurrence (<xref ref-type="bibr" rid="bib70">Tsodyks et al., 1998</xref>; <xref ref-type="bibr" rid="bib53">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Lundqvist et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Barak and Tsodyks, 2014</xref>; <xref ref-type="bibr" rid="bib54">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib6">Ballintyn et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Masse et al., 2019</xref>), whereas here we are interested in investigating the computational capabilities and dynamical behavior of computing with synapse modulations alone. Unlike previous works that examine computation solely through synaptic changes, the MPN’s modulations occur at all times and do not require a special signal to activate their change (<xref ref-type="bibr" rid="bib68">Sugase-Miyamoto et al., 2008</xref>). The networks examined in this work are most similar to the recently introduced ‘HebbFF’ (<xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>) and ‘STPN’ (<xref ref-type="bibr" rid="bib60">Rodriguez et al., 2022</xref>) that also examine computation through continuously updated synaptic modulations. Our work differs from these studies in that we focus on elucidating the neural population dynamics of such networks, contrasting them to known RNN dynamics, and show why this difference in dynamics may be beneficial in certain neuroscience-relevant settings. Additionally, the MPN uses a multiplicative modulation mechanism rather than the additive modulation of these two works, which in some settings we investigate yields significant performance differences. The exact form of the synaptic modulation updates were originally inspired by ‘fast weights’ used in machine learning for flexible learning (<xref ref-type="bibr" rid="bib4">Ba et al., 2016</xref>). However, in the MPN, both plasticity rules apply to the same weights rather than different ones, making it more biologically realistic.</p><p>This work largely focuses on understanding computation through a <italic>neural population dynamics</italic>-like analysis (<xref ref-type="bibr" rid="bib72">Vyas et al., 2020</xref>). In particular, we focus on the dynamics of networks trained on integration-based tasks, that have previously been studied in RNNs (<xref ref-type="bibr" rid="bib43">Maheswaranathan et al., 2019b</xref>; <xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref>; <xref ref-type="bibr" rid="bib44">Maheswaranathan and Sussillo, 2020</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). These studies have demonstrated a degree of universality of the underlying computational structure across different types of tasks and RNNs (<xref ref-type="bibr" rid="bib43">Maheswaranathan et al., 2019b</xref>). Due to the MPN’s dynamic weights, its operation is fundamentally different than said recurrent networks.</p></sec><sec id="s1-3"><title>Setup</title><p>Throughout this work, we primarily investigate the dynamics of the MPN on tasks that require an integration of information over time. To correctly respond to said task, the network is required to both store and update its internal state as well as compare several distinct items in its memory. All tasks in this work consist of a discrete sequence of vector inputs, <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the tasks we consider presently, at time <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the network is queried by a ‘go signal’ for an output, for which the correct response can depend on information from the entire input sequence. Throughout this paper, we denote vectors using lowercase bold letters, matrices by uppercase bold letters, and scalars using standard (not-bold) letters. The input, hidden, and output layers of the networks we study have <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons, respectively.</p></sec><sec id="s1-4"><title>Multi-plasticity network</title><p>The <italic>multi-plasticity network (MPN</italic>) is an artificial neural network consisting of input, hidden, and output layers of neurons. It is identical to a fully-connected, two-layer, feedforward network (<xref ref-type="fig" rid="fig1">Figure 1</xref>, middle), with one major exception: the weights connecting the input and hidden layer are modified by the time-dependent <italic>synapse modulation (SM) matrix</italic>, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1</xref>, left). The expression for the hidden layer activity at time step <inline-formula><mml:math id="inf8"><mml:mi mathsize="90%">t</mml:mi></mml:math></inline-formula> is<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> weight matrix representing the network’s synaptic strengths that is fixed after training, ‘<inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⊙</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>’ denotes element-wise multiplication of the two matrices (the Hadamard product), and the <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is applied element-wise. For each synaptic weight in <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, a corresponding element of <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> multiplicatively modulates its strength. Note if <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the first term vanishes, so the <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are unmodified and the network simply functions as a fully connected feedforward network.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Two neural network computational mechanisms: synaptic modulations and recurrence.</title><p>Throughout this figure, neurons are represented as white circles, the black lines between neurons represent regular feedforward weights that are modified during training through gradient descent/backpropagation. From bottom to top are the input, hidden, and output layers, respectively. (Middle) A two-layer, fully connected, feedforward neural network. (Left) Schematic of the MPN. Here, the pink and black lines (between the input and hidden layer) represent weights that are modified by <italic>both</italic> backpropagation (during training) and the synapse modulation matrix (during an input sequence), see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. (Right) Schematic of the Vanilla RNN. In addition to regular feedforward weights between layers, the RNN has (fully connected) weights between its hidden layer from one time step to the next, see <xref ref-type="disp-formula" rid="equ4">Equation 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig1-v3.tif"/></fig><p>What allows the MPN to store and manipulate information as the input sequence is passed to the network is how the SM matrix, <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, changes over time. Throughout this work, we consider two distinct modulation update rules. The primary rule we investigate is dependent upon <italic>both</italic> the pre- and postsynaptic firing rates. An alternative update rule only depends upon the presynaptic firing rate. Respectively, the SM matrix updated for these two cases takes the form (<xref ref-type="bibr" rid="bib26">Hebb, 2005</xref>; <xref ref-type="bibr" rid="bib4">Ba et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>),<disp-formula id="equ2"><label>(2a)</label><mml:math id="m2"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mspace linebreak="newline"/></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(2b)</label><mml:math id="m3"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mtext>pre. only:</mml:mtext><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are parameters learned during training and 1 is the n-dimensional vector of all 1s. We allow for <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>η</mml:mi><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, so the size and sign of the modulations can be optimized during training. Additionally, <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so the SM matrix exponentially decays at each time step, asymptotically returning to its <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> baseline. For both rules, we define <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at the start of each input sequence. Since the SM matrix is updated and passed forward at each time step, we will often refer to <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the <italic>state</italic> of said networks.</p><p>To distinguish networks with these two modulation rules, we will refer to networks with the presynaptic only rule as <italic>MPNpre</italic>, while we reserve <italic>MPN</italic> for networks with the pre- and postsynatpic update that we primarily investigate. For brevity, and since almost all results for the MPN generalize to the simplified update rule of the MPNpre, the main text will foremost focus on results for the MPN. Results for the MPNpre are discussed only briefly or given in the supplement.</p><p>As mentioned in the introduction, from a biological perspective the MPN’s modulations represent a general associative plasticity such as STDP, whereas the presynaptic-dependent modulations of the MPNpre can represent STSP. The decay induced by <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the return to baseline of the aforementioned processes, which all occur at a relatively slow speed to their onset (<xref ref-type="bibr" rid="bib12">Bertram et al., 1996</xref>; <xref ref-type="bibr" rid="bib76">Zucker and Regehr, 2002</xref>). To ensure the eventual decay of such modulations, unless otherwise stated, throughout this work we further limit <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Additionally, we observe no major performance or dynamics difference for positive or negative <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, so we do not distinguish the two throughout this work (Methods). We emphasize that the modulation mechanisms of the MPN and MPNpre could represent biological processes that occur at significantly different timescales, so although we train them on identical tasks the tasks themselves are assumed to occur at timescales that match the modulation mechanism of the corresponding network. Note that the modulation mechanisms are not independent of weight adjustment from backpropagation. Since the SM matrix is active during training, the network’s weights that are being adjusted by backpropgation (see below) are experiencing modulations, and said modulations factor into how the weights are adjusted.</p><p>Lastly, the output of the MPN and MPNpre at time <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is determined by a fully-connected readout matrix, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is an <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> weight matrix adjusted during training. Throughout this work, we will view said readout matrix as <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> distinct <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional readout vectors, that is one for each output neuron.</p></sec><sec id="s1-5"><title>Recurrent neural networks</title><p>As discussed in the introduction, throughout this work we will compare the learned dynamics and performance of the MPN to artificial RNNs. The hidden layer activity for the simplest recurrent neural network, the <italic>Vanilla RNN</italic>, is<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the recurrent weights, an <inline-formula><mml:math id="inf38"><mml:mi mathsize="90%">n</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf39"><mml:mi mathsize="90%">n</mml:mi></mml:math></inline-formula> matrix that updates the hidden neurons from one time step to the next (<xref ref-type="fig" rid="fig1">Figure 1</xref>, right). We also consider a more sophisticated RNN structure, the <italic>gated recurrent unit (GRU</italic>), that has additional gates to more precisely control the recurrent update of its hidden neurons (see Methods 5.2). In both these RNNs, information is stored and updated via the hidden neuron activity, so we will often refer to <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the RNNs’ <italic>hidden state</italic> or just its <italic>state</italic>. The output of the RNNs is determined through a trained readout matrix in the same manner as the MPN above, i.e. <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s1-6"><title>Training</title><p>The weights of the MPN, MPNpre, and RNNs will be trained using gradient descent/backpropagation through time, specifically ADAM (<xref ref-type="bibr" rid="bib31">Kingma and Ba, 2014</xref>). All network weights are subject to L1 regularization to encourage sparse solutions (Methods 5.2). Cross-entropy loss is used as a measure of performance during training. Gaussian noise is added to all inputs of the networks we investigate.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Network dynamics on a simple integration task</title><sec id="s2-1-1"><title>Simple integration task</title><p>We begin our investigation of the MPN’s dynamics by training it on a simple N-class (Through most of this work, the number of neurons in the output layer of our networks will always be equal to the number of classes in the task, so we use <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to denote both unless otherwise stated). integration task, inspired by previous works on RNN integration-dynamics (<xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). In this task, the network will need to determine for which of the <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> classes the input sequence contains the most evidence (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Each stimulus input, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, can correspond to a discrete unit of evidence for one of the <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> classes. We also allow inputs that are evidence for none of the classes. The final input, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, will always be a special ‘go signal’ input that tells the network an output is expected. The network’s output should be an integration of evidence over the entire input sequence, with an output activity that is largest from the neuron that corresponds to the class with the maximal accumulated evidence. (We omit sequences with two or more classes tied for the most evidence. See Methods 5.1 for additional details). Prior to adding noise, each possible input, including the go signal, is mapped to a random binary vector (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). We will also investigate the effect of inserting a <italic>delay period</italic> between the stimulus period and the go signal, during which no input is passed to the network, other than noise (<xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Schematic of simple integration task.</title><p>(<bold>a</bold>) Example sequence of the two-class integration task where each box represents an input. Here and throughout this work, distinct classes are represented by different colors. In this case, red and blue. The red/blue boxes represent evidence for their respective classes, while the grey box represents an input that is evidence for neither class. At the end of the sequence is the ‘go signal’ that lets the network know an output is expected. The correct response for the sequence is the class with the most evidence; in the example shown, the red class. (<bold>b</bold>) Each possible input is mapped to a (normalized) random binary vector. (<bold>c</bold>) The integration task can be modified by the insertion of a ‘delay period’ between the stimulus period and the go signal. During the delay period, the network receives no input (other than noise).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig2-v3.tif"/></fig><p>We find the MPN (and MPNpre) is capable of learning the above integration task to near perfect accuracy across a wide range of class counts, sequence lengths, and delay lengths. It is the goal of this section to illuminate the dynamics behind the trained MPN that allow it to solve such a task and compare them to more familiar RNN dynamics. Here, in the main text, we will explicitly explore the dynamics of a two-class integration task, generalizations to <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> classes are straightforward and are discussed in the Methods 5.4. We will start by considering the simplest case of integration without a delay period, revisiting the effects of delay afterwards.</p><p>Before we dive into the dynamics of the MPN, we give a quick recap of the known RNN dynamics on integration-based tasks.</p></sec><sec id="s2-1-2"><title>Review of RNN integration: attractor dynamics encodes accumulated evidence</title><p>Several studies, both on natural and artificial neural networks, have discovered that networks with recurrent connections develop attractor dynamics to solve integration-based tasks (<xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref>; <xref ref-type="bibr" rid="bib44">Maheswaranathan and Sussillo, 2020</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). Here, we specifically review the behavior of artificial RNNs on the aforementioned N-class integration tasks that share many qualitative features with experimental observations of natural neural networks. Note also the structure/dimensionality of the dynamics can depend on correlations between the various classes (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>), in this work we only investigate the case where the various classes are uncorrelated.</p><p>RNNs are capable of learning to solve the simple integration task at near-perfect accuracy and their dynamics are qualitatively the same across several architectures (<xref ref-type="bibr" rid="bib43">Maheswaranathan et al., 2019b</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). Discerning the network’s behavior by looking at individual hidden neuron activity can be difficult (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), and so it is useful to turn to a population-level analysis of the dynamics. When the number of hidden neurons is much larger than number of integration classes (<inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≫</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), the population activity of the trained RNN primarily exists in a low-dimensional subspace of approximate dimension <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). This is due to recurrent dynamics that create a task-dependent attractor manifold of approximate dimension <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the hidden activity often operates close to said attractor. (See Methods 5.4 for a more in-depth review of these results including how approximate dimensionality is determined). In the two-class case, the RNN will operate close to a finite length line attractor. The low-dimensionality of hidden activity allows for an intuitive visualization of the dynamics using a two-dimensional PCA projection (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). From the sample trajectories, we see the network’s hidden activity starts slightly offset from the line attractor before quickly falling towards its center. As evidence for one class over the other builds, <italic>the hidden activity encodes accumulated evidence by moving along the one-dimensional attractor</italic> (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). The two readout vectors are roughly aligned with the two ends of the line, so the further the final hidden activity, <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, is toward one side of the attractor, the higher that class’s corresponding output and thus the RNN correctly identifies the class with the most evidence. For later reference, we note that the hidden activity of the trained RNN is not highly dependent upon the input of the present time step (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), but instead it is the <italic>change</italic> in the hidden activity from one time step to the next, <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mrow><mml:mi mathsize="90%">t</mml:mi><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, that are highly input-dependent (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, inset). For the Vanilla RNN (GRU), we find <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.53</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.88</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) of the hidden activity variance to be explained by the accumulated evidence and only <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.19</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.29</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) to be explained by the present input to the network (mean±s.e., Methods 5.4).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Two-class integration: comparison of multi-plasticity network and RNN dynamics.</title><p>(<bold>a-c</bold>) Vanilla RNN hidden neuron dynamics, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> <italic>for GRU</italic>. (<bold>a</bold>) Hidden layer neural activity, <inline-formula><mml:math id="inf57"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, for four sample neurons of the RNN as a function of sequence time (in units of sequence index). The shaded grey region represents the stimulus period during which information should be integrated across time and the thin purple-shaded region representing the response to the go signal. (<bold>b</bold>) Hidden neuron activity, collected over 1000 input sequences, projected into their top two PCA components, colored by relative accumulated evidence between red/blue classes at time <inline-formula><mml:math id="inf58"><mml:mi>t</mml:mi></mml:math></inline-formula> (Methods 5.5). Also shown are PCA projections of sample trajectories (thin lines, colored by class), the red/blue class readout vector (thick lines), and the initial state (black square). (<bold>c</bold>) Same as (<bold>b</bold>), with <inline-formula><mml:math id="inf59"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> now colored by input at the present time step, <inline-formula><mml:math id="inf60"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> (four possibilities, see inset). The inset shows the PCA projection of <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as a function of the present input, <inline-formula><mml:math id="inf62"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, with the dark lines showing the average for each of the four inputs. [d-f] <italic>MPN hidden neuron dynamics, see</italic> <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> <italic>for MPNpre</italic>. (<bold>d</bold>) Same as (<bold>a</bold>). (<bold>e</bold>) Same as (<bold>b</bold>). (<bold>f</bold>) Same as (<bold>c</bold>), except for inset. The inset now shows the alignment of each input-cluster with the readout vectors (Methods 5.5). [g-h] <italic>MPN synaptic modulation dynamics</italic>. (<bold>g</bold>) Same as (<bold>b</bold>), but instead of hidden neuron activity, the PCA projection of the SM matrices, <inline-formula><mml:math id="inf63"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, collected over 1000 input sequences. Final <inline-formula><mml:math id="inf64"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are colored slightly darker for clarity. (<bold>h</bold>) Same as (<bold>g</bold>), with a different <inline-formula><mml:math id="inf65"><mml:mi>y</mml:mi></mml:math></inline-formula>-axis. The inset is the same as that shown in (<bold>b</bold>), but for <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Two-class integration: additional network dynamics.</title><p>(<bold>a-b</bold>) GRU dynamics. (<bold>a</bold>) GRU hidden activity, projected into their top two PCA components, colored by relative evidence between red/blue classes (Methods). Also shown are PCA projections of sample trajectories (thin lines), the red/blue class readout vector (thick lines), and initial state (black square). (<bold>b</bold>) Same as (<bold>a</bold>), colored by input label. The inset shows a scatter of <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as a function of the current input, <inline-formula><mml:math id="inf68"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, with the dark line showing the average for each input. (<bold>c-d</bold>) MPN go signal projections. (<bold>c</bold>) The go signal projection, see <xref ref-type="disp-formula" rid="equ17">Equation 14</xref>, of the SM matrix activity shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The resulting projections are low-dimensional and thus projected onto their top two PC directions. The plot shown colors the projections by accumulated evidence. (<bold>d</bold>) Same as (<bold>c</bold>), but the projections are colored by present input. Inset shows go signal projections of <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, also colored by present input. (<bold>e-h</bold>) MPNpre dynamics. (<bold>e</bold>) Same as (<bold>a</bold>), but for the MPN hidden activity. Note the <inline-formula><mml:math id="inf70"><mml:mi>y</mml:mi></mml:math></inline-formula> scale has been enhanced relative to <inline-formula><mml:math id="inf71"><mml:mi>x</mml:mi></mml:math></inline-formula> scale to show additional detail. (<bold>f</bold>) For the MPN hidden activity, the same plot as (<bold>b</bold>) except for the inset. The inset now shows the alignment of each input cluster with the readout vectors (Methods). See Sec. 5.3 for an explanation of the qualitatively different readout alignment relative to the MPN. (<bold>g</bold>) The flattened SM matrix values, <inline-formula><mml:math id="inf72"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, projected into their top two PCA components, colored by relative evidence between red/blue classes (Methods). Also shown are PCA projections of sample trajectories (thin lines) and initial state (black square). (<bold>h</bold>) Same as (<bold>g</bold>), with a different <inline-formula><mml:math id="inf73"><mml:mi>y</mml:mi></mml:math></inline-formula>-axis. (<bold>i-l</bold>) ReLU MPN dynamics. (<bold>i</bold>) Same as (<bold>e</bold>), for ReLU MPN. (<bold>j</bold>) Same as (<bold>f</bold>). (<bold>k</bold>) Same as (<bold>g</bold>). (<bold>l</bold>) Same as (<bold>k</bold>), with a different <inline-formula><mml:math id="inf74"><mml:mi>y</mml:mi></mml:math></inline-formula>-axis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig3-figsupp1-v3.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Two-class integration: additive dynamics and modulation bounds.</title><p>(<bold>a-d</bold>) Additive MPN dynamics. (<bold>a</bold>) Additive MPN hidden activity, projected into their top two PCA components, colored by relative evidence between red/blue classes (Methods). Also shown are PCA projections of sample trajectories (thin lines), the red/blue class readout vector (thick lines), and initial state (black square). (<bold>b</bold>) Same as (<bold>a</bold>), colored by input label. The inset now shows the alignment of each input cluster with the readout vectors (Methods). (<bold>c</bold>) The flattened SM matrix values, <inline-formula><mml:math id="inf75"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, projected into their top two PCA components, colored by relative evidence between red/blue classes (Methods). Also shown are PCA projections of sample trajectories (thin lines) and initial state (black square). (<bold>d</bold>) Same as (<bold>c</bold>), with a different <inline-formula><mml:math id="inf76"><mml:mi>y</mml:mi></mml:math></inline-formula>-axis. [e-j] <italic>Effects of bounded modulations</italic>. (<bold>e</bold>) Size of the SM matrix as a function of sequence time for MPNs with no modulation bounds (red), <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula> (blue), and <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> (green). Here, size is the Frobenius norm of the <inline-formula><mml:math id="inf79"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, averaged across 10 trials. (<bold>f</bold>) Same as (<bold>e</bold>), but the size of the hidden activity, as measured by L2 norm of the hidden activity. (<bold>g</bold>) Histogram of the size of <inline-formula><mml:math id="inf80"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> for the same three modulation bound setups shown in (<bold>e</bold>). The dotted line shows the mean across all trials for each setup. (<bold>h</bold>) For a network with strict bounds on modulations (<inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>), same plot as (<bold>a</bold>). (<bold>i</bold>) Same as (<bold>c</bold>). (<bold>j</bold>) Same as (<bold>i</bold>), with a different <inline-formula><mml:math id="inf82"><mml:mi>y</mml:mi></mml:math></inline-formula>-axis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig3-figsupp2-v3.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>3-class integration: MPN, Vanilla RNN, and GRU dynamics.</title><p>Plots are generalizations of those shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> for 3-class integration instead of 2-class. [a-d] <italic>Vanilla RNN and GRU</italic> <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>N</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">3</mml:mn></mml:mrow></mml:math></inline-formula> <italic>dynamics</italic>. (<bold>a</bold>) Hidden activity of the Vanilla RNN, colored by sequence label, projected into its PCA space. Also shown are sample trajectories (thin lines), readout vectors (thick lines), and initial activity (black square). (<bold>b</bold>) Same as (<bold>a</bold>), but <inline-formula><mml:math id="inf84"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is colored by present input, <inline-formula><mml:math id="inf85"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>. Inset shows change in hidden state <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, also colored by present input. The dark lines show the average change in hidden activity for each corresponding input. (<bold>c</bold>) Same as (<bold>a</bold>), for the GRU. (<bold>d</bold>) Same as (<bold>b</bold>), for the GRU. [e-h] <italic>MPN</italic> <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>N</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">3</mml:mn></mml:mrow></mml:math></inline-formula> <italic>dynamics</italic>. (<bold>e</bold>) Same as (<bold>a</bold>), for the MPN. (<bold>f</bold>) Same as (<bold>b</bold>), for the MPN. (<bold>g</bold>) SM matrix activity, <inline-formula><mml:math id="inf88"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> colored by sequence label, plotted in its PCA space. Also shown are sample trajectories (thin lines) and initial value, <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (black square). (<bold>h</bold>) Same as (<bold>g</bold>), with a different PC shown along the <inline-formula><mml:math id="inf90"><mml:mi>x</mml:mi></mml:math></inline-formula>-axis. The inset shows the change in the SM matrix, <inline-formula><mml:math id="inf91"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, colored by the present input <inline-formula><mml:math id="inf92"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>. The dark lines show the average change in the SM matrix for each corresponding input.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig3-figsupp3-v3.tif"/></fig></fig-group></sec><sec id="s2-1-3"><title>MPN hidden activity encodes inputs, not so much accumulated evidence</title><p>We now turn to analyzing the hidden activity of the trained MPNs in the same manner that was done for the RNNs. The MPN trained on a two-class integration task appears to have significantly more sporadic activity in the individual components of <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). We again find the hidden neuron activity to be low-dimensional, with approximate dimension <inline-formula><mml:math id="inf94"><mml:mrow><mml:mn mathsize="90%">2.07</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.12</mml:mn></mml:mrow></mml:math></inline-formula> (mean±s.e.), lending it to informative visualization using a PCA projection (Methods 5.4). Unlike the RNN, we observe the hidden neuron activity to be separated into several distinct clusters (<xref ref-type="fig" rid="fig3">Figure 3e</xref>). Exemplar input sequences cause <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to rapidly transition between said clusters. Coloring the <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by the sequence input at the present time step, we see the different inputs are what divide the hidden activity into distinct clusters, that we hence call <italic>input-clusters</italic> (<xref ref-type="fig" rid="fig3">Figure 3f</xref>). That is, the hidden neuron activity is largely dependent upon the most recent input to the network, rather than the accumulated evidence as we saw for the RNN. However, within each input-cluster, we also see a variation in <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from accumulated evidence (<xref ref-type="fig" rid="fig3">Figure 3e</xref>). For the MPN (MPNpre), we now find only <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.21</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf99"><mml:mrow><mml:mn mathsize="90%">0.16</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.05</mml:mn></mml:mrow></mml:math></inline-formula>) of the hidden activity variance to be explained by accumulated evidence and <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.87</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf101"><mml:mrow><mml:mn mathsize="90%">0.80</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.03</mml:mn></mml:mrow></mml:math></inline-formula>) to be explained by the present input to the network (mean±s.e., Methods 5.4). (The MPNpre dynamics are largely the same of what we discuss here, see Sec. 5.4.2 for further discussion).</p><p>With the hidden neuron activity primarily dependent upon the current input to the network, one may wonder how the MPN ultimately outputs information dependent upon the entire sequence to solve the task. Like the other possible inputs to the network, the go signal has its own distinct input-cluster within which the hidden activities vary by accumulated evidence. Amongst all input-clusters, we find the readout vectors are highly aligned with the evidence variation within the go cluster (<xref ref-type="fig" rid="fig3">Figure 3f</xref>, inset). The readouts are then primed to distinguish accumulated evidence immediately following a go signal, as required by the task. (This idea leads to another intuitive visualization of the MPN behavior by asking what <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> would look like at any given time step if the most recent input is the go signal (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1e and f</xref>)).</p></sec><sec id="s2-1-4"><title>MPNs encode accumulated evidence in the synapse modulations (<inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>)</title><p>Although the hidden neuron behavior is useful for comparison to the RNN and to understand what we might observe from neural recordings, information in the MPN is passed from one step to the next solely through the SM matrix (<xref ref-type="disp-formula" rid="equ2">Equation 2a</xref>) and so it is also insightful to understand its dynamics. Flattening each <inline-formula><mml:math id="inf104"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> matrix, we can investigate the population dynamics in a manner identical to the hidden activity.</p><p>Once again, we find the variation of the SM matrix to be low-dimensional meaning we can visualize the evolution of its elements in its PCA space (<xref ref-type="fig" rid="fig3">Figure 3g and h</xref>). From exemplar sequences, we see that <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> appears to evolve in time along a particular direction as the input sequence is passed (<xref ref-type="fig" rid="fig3">Figure 3g</xref>). Perpendicular to this direction, we see a distinct separation of <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values by accumulated evidence, very similar to what was seen for the RNN hidden activity (<xref ref-type="fig" rid="fig3">Figure 3h</xref>). Also like the RNN, the distinct evidence inputs tend to cause a change in the state in opposite directions (<xref ref-type="fig" rid="fig3">Figure 3h</xref>, inset). We also note the input that provides no evidence for either class and the go signal both cause sizable changes in the SM matrix.</p><p>Thus, since the state of the MPN is stored in the SM matrix, we see its behavior is much more similar to the dynamics of the hidden neuron activity of the RNN: each of the states tracks the accumulated evidence of the input sequence. Information about the relative evidence for each class is stored in the position of the state in its state space, and this information is continuously updated as new inputs are passed to the network by moving around said state space. Even so, the fact that the size of the MPN state seems to grow with time (a fact we confirm two paragraphs below) and has large deflections even for inputs that provide no evidence for either class make it stand apart from the dynamics of the RNN’s state.</p></sec><sec id="s2-1-5"><title>The MPN and RNNs have distinct long-time behaviors</title><p>Given that <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> appears to get progressively larger as the sequence is read in (<xref ref-type="fig" rid="fig3">Figure 3e</xref>), one might wonder if there is a limit to its growth. More broadly, this brings up the question of what sort of long-time behavior the MPN has, including any attractor structure. A full understanding of attractor dynamics is useful for characterizing dynamical systems. Attractors are often defined as the set of states toward which the network eventually flows asymptotically in time. As mentioned earlier, it is known that RNNs form low-dimensional, task-dependent attractor manifolds in their hidden activity space for integration-based tasks (<xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). However, where the activity of a network flows to is dependent upon what input is being passed to the network at that time. For example, the network may flow to a different location under (1) additional stimulus input versus (2) no input. We will investigate these two specific flows for the MPN and compare them to RNNs.</p><p>We will be specifically interested in the flow of <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the MPN’s state, since from the previous section it is clear its dynamics are the closest analog of an RNN’s state, <inline-formula><mml:math id="inf109"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula>. We train both the MPN and RNNs on a <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> integration task and then monitor the behavior of their states for stimuli lengths ranging from 10 to 200 steps. As might be expected from the dynamics in the previous section, we do indeed observe that <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’s components grow increasingly large with longer stimuli (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). Meanwhile, the RNN’s state appears to remain constrained to its line attractor even for the longest of input sequences (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). To quantitatively confirm these observations, we look at the magnitude of the network’s final state (normalized by number of components) as a function of the stimulus length (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, Methods 5.5). As expected, since the RNNs operate close to an attractor, the final state magnitude does not increase considerably despite passing stimuli 10 times longer than what was observed in training. In contrast, the magnitude of the MPN state can change by several orders of magnitude, but its growth is highly dependent on the value of its <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameter that controls the rate of exponential decay (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). (Although for any <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> the modulations of the MPN will eventually stop growing, the ability of the SM matrix to grow unbounded is certainly biologically unrealistic. We introduced explicitly bounded modulations to see if this changed either the computational capabilities or behavior of the MPN, finding that both remained largely unchanged (Methods 5.4.5, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>)). As <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> grows in magnitude so does the size of its decay and eventually this decay will be large enough to cancel out the growth of <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from additional stimuli input. For smaller <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> this decay is larger and thus occurs at shorter sequence lengths. Despite this saturation of state size, the accuracy of the MPN does not decrease significantly with longer sequence lengths (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). These results also demonstrate the MPN (and RNNs) are capable of generalizing to both shorter and longer sequence lengths, despite being trained at a fixed sequence length.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Two-class integration: long-time behavior of multi-plasticity network and RNNs.</title><p>[<bold>a-d</bold>] <italic>Flow of states under stimulus</italic>. (<bold>a</bold>) The MPN state, <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, collected over 100 input sequences, colored by the stimulus length <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, for <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> (dark blue) to 200 (yellow) time steps, with <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="inf121"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are projected onto their PCA space. The blue/red colored <inline-formula><mml:math id="inf122"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are for <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> and are the same as those in <xref ref-type="fig" rid="fig3">Figure 3f</xref>. (<bold>b</bold>) Same as (<bold>a</bold>), for the Vanilla RNN states, <inline-formula><mml:math id="inf124"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, plotted in their PCA space. (<bold>c</bold>) For various sequences lengths, <inline-formula><mml:math id="inf125"><mml:mi>T</mml:mi></mml:math></inline-formula>, normalized magnitude of final states. Networks trained on a sequence length of <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (dotted line). (<bold>d</bold>) Accuracy of the networks shown in (<bold>c</bold>) as a function of sequence length. [e-h] <italic>Flow of states under zero input (delay input</italic>). (<bold>e</bold>) MPN states, <inline-formula><mml:math id="inf127"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, collected over 100 input sequences, colored by the delay length, 10 (dark blue) to 100 (yellow) time steps. The blue/red colored <inline-formula><mml:math id="inf128"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are for zero delay. (<bold>f</bold>) Same as (<bold>e</bold>), for GRU states. (<bold>g</bold>) For various delay lengths, magnitude of final state. Networks trained with a delay of 20 (dotted line). (<bold>h</bold>) Accuracy of the networks shown in (<bold>i</bold>) as a function of delay length.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig4-v3.tif"/></fig></sec><sec id="s2-1-6"><title>The MPN has a single, point-like attractor that its state uniformly decays toward</title><p>Another important behavior that is relevant to the operation of these networks is how they behave under no stimulus input. Such inputs occur if we add a delay period to the simple integration task, so we now turn to analyzing the MPN trained on a integration-delay task (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). We again train MPNs with varying <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and RNNs, this time on a task with a delay length of 20 time steps. (Vanilla RNNs trained on this task perform poorly due to vanishing gradients, so we omit them for this analysis. It is possible to train them by bootstrapping their training by gradually increasing the delay period of the task). During the delay period, the state of the MPNs decay over time (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). Once again, since the RNNs operates close to its a line attractor manifolds, its state changes little over the delay period, other than flowing more towards the ends of the line (<xref ref-type="fig" rid="fig4">Figure 4f</xref>). Again, we quantify this behavior by monitoring the normalized final state magnitude as a function of the delay length (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). We see the decay in the MPN’s state is fastest for networks with smaller <inline-formula><mml:math id="inf130"><mml:mi mathsize="90%">λ</mml:mi></mml:math></inline-formula> and that the network with <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> has no such decay.</p><p>Perhaps obvious in hindsight, the MPN’s state will simply decay toward <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> under no input. As such, for <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <italic>the MPN has an attractor at</italic> <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> , due to the exponential decay of its state built into its update expression. Since the evidence is stored in the magnitude of certain components of the SM matrix, a uniform decay across all elements maintains their relative size and does not decrease the MPN’s accuracy for shorter delay lengths (<xref ref-type="fig" rid="fig4">Figure 4h</xref>). However, eventually the decay decreases the information stored in <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> enough that early inputs to the network will be indistinguishable from input noise, causing the accuracy of the network to plummet as well. The RNN, since it operates close to an attractor, has no appreciable decay in its final state over longer delays (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). Still, even RNN attractors are subject to drift along the attractor manifold and we do eventually see a dip in accuracy as well (<xref ref-type="fig" rid="fig4">Figure 4h</xref>).</p></sec><sec id="s2-1-7"><title>MPNs are ‘activity silent’ during a delay period</title><p>We have seen the MPN’s state decays during a delay period, here we investigate what we would observe in its hidden neurons during said period. Since the MPN’s hidden activity primarily depends on the present input, during the delay period when no input is passed to the network (other than noise), we expect the activity to be significantly smaller (<xref ref-type="bibr" rid="bib68">Sugase-Miyamoto et al., 2008</xref>). Indeed, at the start of the delay period, we see a few of the MPN’s hidden neuron activities quickly drops in magnitude, before spiking back up after receiving the go signal (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Meanwhile, at the onset of the delay, the RNN’s hidden layer neurons quickly approach finite asymptotic values that remain fairly persistent throughout the delay period (although see <xref ref-type="bibr" rid="bib54">Orhan and Ma, 2019</xref> for factors that can affect this result). The aforementioned behaviors are also seen by taking the average activity magnitude across the entire population of hidden neurons in each of these networks (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref>). Reduced activity during delay periods has been observed in working memory experiments and models and is sometimes referred to as an ‘activity-silent’ storage of information (<xref ref-type="bibr" rid="bib8">Barak et al., 2010</xref>; <xref ref-type="bibr" rid="bib66">Stokes, 2015</xref>; <xref ref-type="bibr" rid="bib40">Lundqvist et al., 2018</xref>). It contrasts with the ‘persistent activity’ exhibited by RNNs that has been argued to be more metabolically expensive (<xref ref-type="bibr" rid="bib3">Attwell and Laughlin, 2001</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Integration with delay and analytical predictions.</title><p>[<bold>a-b</bold>] <italic>MPN and RNN hidden activity behavior during a integration-delay task</italic>. The left shaded grey region represents the stimulus period; followed by the delay period with a white background; and finally the thin purple-shaded region representing the response to the go signal. (<bold>a</bold>) Sample hidden neuron activity for the MPN and RNN. (<bold>b</bold>) Decoding accuracy on the hidden neuron activity of the MPN and RNN as a function of time (Methods 5.5). Dotted line represents chance accuracy. [<bold>c-d</bold>] <italic>Analytical MPN approximations</italic>. (<bold>c</bold>) Exemplar hidden neuron (teal) and SM matrix (pink) activity as a function of time (solid lines) and their analytical predictions (dotted lines) from <xref ref-type="disp-formula" rid="equ5 equ6">Equations 4 and 5</xref>. Shading is the same as (<bold>a</bold>) and (<bold>b</bold>). (<bold>d</bold>) Overall accuracy of of theoretical predictions as a function of the size of <inline-formula><mml:math id="inf136"><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mtext>inp</mml:mtext></mml:msub></mml:math></inline-formula> with either <inline-formula><mml:math id="inf137"><mml:mi>n</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf138"><mml:mi>d</mml:mi></mml:math></inline-formula> fixed (Methods 5.5).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>More on delay dynamics of the MPN and RNN.</title><p>[<bold>a-c</bold>] <italic>Various measures as a function of time for networks trained with different delay inputs magnitudes. Note</italic> <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>delay</mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> <italic>throughout the main text</italic>. (<bold>a</bold>) Mean activity magnitude. (<bold>b</bold>) Decoding accuracy. (<bold>c</bold>) Normalized time variance of hidden layer neurons for the MPN and GRU networks (Methods). Variation is measured over a rolling window of previous 5 sequence inputs, so data for first four time steps is omitted. Region of grey-to-white gradient represents transition between stimulus and delay periods, where time window captures data from both periods. (<bold>d</bold>) Error of theoretical approximations, same as <xref ref-type="fig" rid="fig5">Figure 5d</xref>, now as a function of sequence time. [e-g] <italic>Cross time decoding accuracy for the GRU and MPN. Color scale is the same for all plots</italic>. (<bold>e</bold>) Decoding accuracy on the hidden neuron activity for a GRU. The <inline-formula><mml:math id="inf140"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:mi>y</mml:mi></mml:math></inline-formula> axes show the testing and training times, respectively. For this plot, <inline-formula><mml:math id="inf142"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>delay</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.0</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>f</bold>) Same as (<bold>e</bold>), for the MPN. (<bold>g</bold>) Same as (<bold>f</bold>), but now <inline-formula><mml:math id="inf143"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>delay</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig5-figsupp1-v3.tif"/></fig></fig-group><p>To further quantify the degree of variation in the output information stored in the hidden neuron activity of each of these networks as a function of <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we train a decoder on the <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (Methods 5.5; <xref ref-type="bibr" rid="bib48">Masse et al., 2019</xref>). Confirming that the MPN has significant variability during its stimulus and delay periods, we see the MPN’s decoding accuracy drops to almost chance levels at the onset of the delay period before jumping back to near-perfect accuracy after the go signal (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). Since the part of the hidden activity that tracks accumulated evidence is small, during this time period said activity is washed out by noise, leading to a decoding accuracy at chance levels. Meanwhile, the RNN’s hidden neuron activity leads to a steady decoding accuracy throughout the entire delay period, since the RNN’s state just snaps to the nearby attractor, the position along which encodes the accumulated evidence. Additionally, the RNN’s trained decoders maintain high accuracy when used at different sequence times, whereas the cross-time accuracy of the MPN fluctuates significantly more (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1e, f and g</xref>). The increased time-specificity of activity in the MPN has been observed in working memory experiments (<xref ref-type="bibr" rid="bib65">Stokes et al., 2013</xref>). (Additional measures of neuron variability and activity silence are shown in the supplement (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>)).</p></sec><sec id="s2-1-8"><title>Analytical confirmation of MPN dynamics</title><p>It is possible to analytically approximate the behavior of the MPN’s <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at a given time step. Details of the derivation of these approximations is given in Methods 5.3. Briefly, the approximation relies on neglecting quantities that are made small with an increasing number of neurons in either the input or hidden layer. The net effect of this is that <italic>synaptic modulations are small</italic> and thus can be neglected at leading-order approximations. Explicitly, the approximations are given by<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mtext>leading</mml:mtext></mml:munder><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>η</mml:mi><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mtext>sub-leading</mml:mtext></mml:munder><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where, in the first expression, we have indicated the terms that are the leading and sub-leading contributions. These approximation do quite well in predicting the element-wise evolution of <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, although are notably bad at predicting the hidden activity during a delay period where it is driven by only noise (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). We quantify how good the approximations do across the entire test set and see that they improve with increasing input and hidden layer size (<xref ref-type="fig" rid="fig5">Figure 5d</xref>).</p><p>These simplified analytical expressions allow us to understand features we’ve qualitatively observed in the dynamics of the MPN. Starting with the expression for <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we see the leading-order contributions comes from the term <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is solely dependent upon the current input, i.e. <italic>not</italic> the sequence history. Comparing to the exact expression, <xref ref-type="disp-formula" rid="equ2">Equation 2a</xref>, the leading-order approximation is equivalent to taking <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, so <italic>at leading-order the MPN just behaves like a feedforward network</italic>. This explains why we see the input-dependent clustering in the <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> dynamics: a feedforward network’s activity is only dependent on its current input. Meanwhile, the sub-leading term depends on all previous inputs (<inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), which is why the individual input-clusters vary slightly by accumulated evidence.</p><p>From the approximation for <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we see its update from one time step is solely dependent upon the current input as well. Without the <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> term, <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> simply acts as an accumulator that counts the number of times a given input has been passed to the network – exactly what is needed in an integration-based task. In practice, with <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the contribution of the earlier inputs of the network to <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> will slowly decay.</p><p>The MPNpre modulation updates are significantly simpler to analyze since they do not depend on the hidden activity, and thus the recurrent dependence of <inline-formula><mml:math id="inf160"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> on previous inputs is much more straightforward to evaluate. The equivalent expressions of <xref ref-type="disp-formula" rid="equ5 equ6">Equations 4 and 5</xref> for the MPNpre are given in Sec. 5.3.</p></sec></sec><sec id="s2-2"><title>Capacity, robustness, and flexibility</title><p>Having established an understanding of how the dynamics of the MPN compares to RNNs when trained on a simple integration-delay task, we now investigate how their different operating mechanisms affect their performance in various settings relevant to neuroscience.</p><sec id="s2-2-1"><title>MPNs have comparable integration capacity to GRUs and outperform Vanilla RNNs</title><p>Given their distinct state storage systems, it is unclear how the capacity of the MPN compares to RNNs on integration-based tasks. In RNNs, the capacity to store information has been linked to the number of synapses/parameters and the size of their state space (<xref ref-type="bibr" rid="bib20">Collins et al., 2016</xref>). For example, since we know that RNNs tend to use an approximate <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> -dimensional attractor manifold in their hidden activity space to solve an N-class task (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>), one might expect to see a drop in accuracy for <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>&gt;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the number of hidden neurons. To investigate state storage capacity in the MPN and RNNs, we limit the number of adjustable parameters/synaptic connections by making the number of neurons in each layer small, specifically taking the number of input and hidden neurons to be <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively.</p><p>We observe the MPN and GRU (and MPNpre) are capable of training to accuracies well above chance, even for <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≥</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> classes, while the Vanilla RNN’s accuracy quickly plummets beyond <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6a</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a</xref>). The size of the MPN state scales as <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and indeed we see the the accuaracies receive a small bump, becoming more comparable to that of the GRU, when the input dimension is increased from <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to 40. (For <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and fixed <inline-formula><mml:math id="inf171"><mml:mi mathsize="90%">d</mml:mi></mml:math></inline-formula>, the number of trainable parameters in the MPN is smaller than that of the RNNs, see <xref ref-type="table" rid="table1">Table 1</xref>. With <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the MPN has a number of trainable parameters more comparable to that of the <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> GRU (125 and 126, respectively)).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Capacity, robustness, and flexibility of the MPN.</title><p>[<bold>a-d</bold>] Accuracy of the MPN and RNNs as a function of several measures that make the integration task more difficult, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> <italic>for MPNpre results</italic>. (<bold>a</bold>) For very small networks (<inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>), number of classes in integration task (<inline-formula><mml:math id="inf176"><mml:mi>N</mml:mi></mml:math></inline-formula>) with fixed sequence length, <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (Methods 5.5). (<bold>b</bold>) Also for very small networks, length of integration task (<inline-formula><mml:math id="inf178"><mml:mi>T</mml:mi></mml:math></inline-formula>) with fixed number of classes, <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>c</bold>) Ratio of signal and noise magnitudes of the input (Methods 5.5). Networks were trained at a ratio of 10 (dotted black line), the dotted grey line represents a ratio of 1.0. (<bold>d</bold>) Networks trained with all parameters frozen at initialization values except for readout matrix (and <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula>). [<bold>e-f</bold>] <italic>Flexibility to learn new tasks</italic>. (<bold>e</bold>) Accuracy on a two-class integration-delay task pre- and post-training on a novel two-class integration-delay task. Thick lines/dots show averages, raw trial data is scattered behind. (<bold>f</bold>) Hidden activity, colored by sequence label, for a GRU trained on a two-class integration task (red/blue classes) when a novel class (green) is introduced <italic>without</italic> additional training. Activity collected over 1000 input sequences, plotted in their PCA space. Also shown are readouts, initial state, and sample trajectory. (<bold>g</bold>) Same as (<bold>f</bold>), for the MPN state, <inline-formula><mml:math id="inf182"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>(<bold>h</bold>) Decoding accuracy of the final states of the MPN and GRU when a novel class introduced, again without training. Dotted line represents accuracy that would be achieved for perfect classification of only the 2 familiar classes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig6-v3.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Additional performance comparisons, including additive MPN.</title><p>Results here were already partially shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. [a-d] <italic>Accuracy of various networks, as a function of several measures that make the integration task more difficult</italic>. (<bold>a</bold>) For very small networks (<inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, unless otherwise stated), number of classes in integration task (<inline-formula><mml:math id="inf185"><mml:mi>N</mml:mi></mml:math></inline-formula>) with fixed sequence length, <inline-formula><mml:math id="inf186"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) Also for very small networks, length of integration task (<inline-formula><mml:math id="inf187"><mml:mi>T</mml:mi></mml:math></inline-formula>) with fixed number of classes, <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>c</bold>) Ratio of signal to noise magnitudes of the input. Networks were trained at a ratio of 10 (dotted black line), the dotted grey line represents a ratio of 1.0. (<bold>d</bold>) Networks trained with all parameters frozen at initialization values except for readout matrix (<inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>e</bold>) Accuracy on a two-class integration with delay task pre- and post-training on a novel two-class integration task. Thick lines/dots show averages, raw trial data is scattered behind. *The MPNpre has a significantly more difficult time achieving the accuracy threshold on this integration task with a delay period, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> details for additional discussion. (<bold>f</bold>) Performance of the MPN and MPNpre for <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> across 19 supervised learning NeuroGym tasks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig6-figsupp1-v3.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameter and operation counts for various networks.</title><p>The number of neurons in the input, hidden, and output layers are <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. Note these counts do not include parameters of the readout layer, since said layer contributes the same number of parameters for each network (<inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with and without a bias) and are for fixed initial states.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Network</th><th align="left" valign="bottom">Trainable Parameters</th><th align="left" valign="bottom">State update operations</th></tr></thead><tbody><tr><td align="left" valign="bottom">2-layer fully connected</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">MPN</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">MPNpre</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Vanilla RNN</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">GRU</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>3</mml:mn><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mi>n</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">LSTM</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>4</mml:mn><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mi>n</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>A second way we can test for integration information capacity is to increase the length of the input sequences in the task. Across the board, both the MPN, MPNpre, and RNNs are capable of learning input sequences up to length <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> at high accuracy (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>). Although differences are of only a few percent, the MPN is capable of learning to integrate relatively long sequences at a level greater than Vanilla RNNs, but not quite as good as GRUs.</p></sec><sec id="s2-2-2"><title>MPNs can operate at high input noise and minimal training</title><p>To test the robustness of the MPN’s dynamics we make the integration task harder in two different ways. First, we add increasingly more noise to the inputs to the network. Even for networks trained with a relatively small amount of noise, we find both the MPN and RNNs are capable of achieving near-perfect accuracy on the two-class task up to when noise is a comparable magnitude to the signal (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Continuing to increase the size of the noise, we see all networks eventually fall to chance accuracy, as expected. Notably, the RNNs maintain higher accuracy for a slightly smaller ratio of signal to noise magnitude. This might be expected given the RNN’s operation close to attractor manifolds, which are known to be robust to perturbations such as noisy inputs (<xref ref-type="bibr" rid="bib72">Vyas et al., 2020</xref>).</p><p>Second, we aim to understand if the MPN’s intrinsic dynamics at initialization allow it to perform integration with a minimal adjustment of weights. We test this by freezing all internal parameters at their initialization values and only training the MPN’s readout layer. It is well known that RNNs with a large number of hidden layer neurons have varied enough dynamics at initialization that simply adjusting the readout layer allows them to accomplish a wide variety of tasks (<xref ref-type="bibr" rid="bib41">Maass et al., 2002</xref>; <xref ref-type="bibr" rid="bib30">Jaeger and Haas, 2004</xref>; <xref ref-type="bibr" rid="bib38">Lukoševičius and Jaeger, 2009</xref>). Such settings are of interest to the neuroscience community since the varied underlying dynamics in random networks allow for wide variety of responses, matching the observed versatility of certain areas of the brain, for example the neocortex (<xref ref-type="bibr" rid="bib41">Maass et al., 2002</xref>). Since the <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameters play especially important roles in the MPN, we fix them at modest values, namely <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (We do not see a significant difference in accuracy for <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is an anti-Hebbian update rule). For two different layer sizes, we find all networks are capable of training in this setup, but the MPN consistently outperforms both RNNs (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). Notably, even the MPN with significantly less neurons across its input/hidden layers outperforms the RNNs. These results suggest that the intrinsic computational structure built into the MPN from its update expressions allow for it to be particularly good at integration tasks, even with randomized synaptic connections.</p></sec><sec id="s2-2-3"><title>MPNs are flexible to taking in new information</title><p>The flexibility of a network to learn several tasks at once is an important feature in both natural and artificial neural networks (<xref ref-type="bibr" rid="bib24">French, 1999</xref>; <xref ref-type="bibr" rid="bib50">McCloskey and Cohen, 1989</xref>; <xref ref-type="bibr" rid="bib49">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib33">Kumaran et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Ratcliff, 1990</xref>). It is well-known that artificial neural networks can suffer from large drops in accuracy when learning tasks sequentially. This effect has been termed <italic>catastrophic forgetting</italic>. For example, although it is known artificial RNNs are capable of learning many neuroscience-related tasks at once, this is not possible without interleaving the training of said tasks or modifying the training and/or network with continual-learning techniques (<xref ref-type="bibr" rid="bib74">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Duncker et al., 2020</xref>). Given the minimal training needed for an MPN to learn to integrate, as well as its task-independent attractor structure, here we test if said flexibility also extends to a sequential learning setting.</p><p>To test this, we train the MPN and GRU on a two-class integration-delay task until a certain accuracy threshold is met and then train on a <italic>different</italic> two-class integration-delay task until the accuracy threshold is met on the novel data. Afterwards, we see how much the accuracy of each network on the original two-class task falls. (Significant work has been done to preserve networks from such pitfalls by, for example, modifying the training order (<xref ref-type="bibr" rid="bib59">Robins, 1995</xref>) or weight updates (<xref ref-type="bibr" rid="bib32">Kirkpatrick et al., 2017</xref>). Here, we do not implement any such methods, we are simply interesting in how the different operating mechanisms cause the networks to behave ‘out of the box’). We find that the MPN loses significantly less accuracy than the GRU when trained on the new task (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). Intuitively, an RNN might be able to use the same integration manifold for both integration tasks, since they each require the same capacity for the storage of information. The state space dimensionality of the MPN and GRU do not change significantly pre- and post-training on the novel data (Methods 5.5). However, we find the line attractors reorient in state space before and after training on the second task, on average shifting by <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>23</mml:mn><mml:mo>±</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> degrees (mean±s.e.). Since the MPN has a task-agnostic attractor structure, it does not change in the presence of new data.</p><p>To understand the difference of how these two networks adapt to new information in more detail, we investigate how the MPN and RNN dynamics treat a novel input, for example how networks trained on a two-class task behave when suddenly introduced to a novel class. For the RNN, the novel inputs to the network do not cause the state to deviate far from the attractor (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). The attractors that make the RNNs so robust to noise are their shortcoming when it comes to processing new information, since anything it hasn’t seen before is simply snapped into the attractor space. Meanwhile for the MPN, the minimal attractor structure means new inputs have no problem deflecting the SM matrix in a distinct direction from previous inputs (<xref ref-type="fig" rid="fig6">Figure 6g</xref>). To quantify the observed separability of the novel class we train a decoder to determine the information about the output contained in the final state of each network (still with no training on the novel class). The MPN’s states have near-perfect separability for the novel class even before training, <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.998</mml:mn><mml:mo>±</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> accuracy, while the GRU has more trouble separating the new information, <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.812</mml:mn><mml:mo>±</mml:mo><mml:mn>0.009</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> accuracy (mean±s.e., <xref ref-type="fig" rid="fig6">Figure 6h</xref>). Hence, out of the box, the MPN is primed to take in new information.</p></sec></sec><sec id="s2-3"><title>Additional tasks</title><p>Given the simplicity of the MPN, it may be called into question if such a setup is capable of learning anything beyond the simple integration-delay tasks we have presented thus far. Additionally, if it is capable of learning other tasks, how its dynamics may change in such settings is also of interest. To address these questions, in this section we train and analyze the MPNs on additional integration tasks studied in neuroscience, many of which require the network to learn more nuanced behavior such as context. Additionally, dynamics of networks trained on prospective and true-anti contextual tasks are shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> (Methods 5.1).</p><sec id="s2-3-1"><title>Retrospective contextual integration</title><p>Integration with context is a well-known task in the neuroscience literature (<xref ref-type="bibr" rid="bib45">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Panichello and Buschman, 2021</xref>). In this setup, two independent two-class integration-delay <italic>subtasks</italic> are passed to the network at once (inputs are concatenated) and the network must provide the correct label to only one of the subtasks based on the contextual input it receives (i.e. report subtask 1 or 2). Here, we discussing the retrospective case, where the context comes <italic>after</italic> the stimuli of the subtasks, in the middle of the delay period (<xref ref-type="fig" rid="fig7">Figure 7a</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>MPN dynamics on additional tasks.</title><p>[a-d] <italic>Retrospective context task</italic>. (<bold>a</bold>) Schematic of example sequence. (<bold>b</bold>) MPN states during the initial stimulus period, projected into their (flattened) PCA space, colored by label and subtask (subtask 1: red/blue, subtask 2: green/yellow). Two example trajectories are shown. (<bold>c</bold>) Same as (<bold>b</bold>), for states during the time period where the network is passed context (5 time steps). States at end of time period colored darker for clarity. (<bold>d</bold>) The readout difference of the MPN as a function of sequence time (Methods 5.5). More positive values correspond to the MPN better distinguishing the two classes for the subtask. Solid lines correspond to when the subtask is chosen by the context, dotted lines to when the subtask is not chosen by the context. Grey, yellow, and white background correspond to stimulus, context, and delay periods, respectively. [e-h] <italic>Continuous integration task</italic>. (<bold>e</bold>) Schematic of example sequence. (<bold>f</bold>) Hidden neuron activity, colored by sequence label, projected into their PCA space. Thick lines are readout vectors of corresponding class. (<bold>g</bold>) Normalized input (black) and example hidden neuron (green) activities, as a function of time (Methods 5.5). (<bold>h</bold>) MPN states, colored by sequence label, projected into their PCA space. Two example trajectories are shown, black square is the initial state. [i-j] <italic>NeuroGym tasks</italic> (<xref ref-type="bibr" rid="bib24">French, 1999</xref>). (<bold>i</bold>) The beginning of an example sequence of the Go No-Go task. The grey/white shading behind the sequence represents distinct stimulus/delay/decision periods within the example sequence. (<bold>j</bold>) Performance of the various networks we investigate across 19 supervised learning NeuroGym tasks (Methods 5.5).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig7-v3.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>More integration tasks, supplemental figures.</title><p>[a-c] <italic>True-anti context task</italic>. (<bold>a</bold>) Schematic of example sequence. The context vector, which is passed as an input to the network (added to the regular stimulus inputs), is passed during the entire input sequence. An ‘anti-’ context tells the network to report the class with the <italic>least</italic> evidence. A ‘true’ context tells the network to solve the task as usual, reporting the class with the most evidence. (<bold>b</bold>) PC projection of the SM matrix for the entire input sequence, colored by both class label and context. The darker colors represent the SM matrix for ‘anti-’ context cases. (<bold>c</bold>) Final hidden activity, colored by both class label and context. Also shown are readouts. [d-g] <italic>Prospective context task</italic>. (<bold>d</bold>) Schematic of example sequence. (<bold>e</bold>) MPN states during the initial context period, projected into their (flattened) PCA space, colored by label and subtask (subtask 1: red/blue, subtask 2: green/yellow). Two example trajectories are shown. (<bold>f</bold>) Same as (<bold>f</bold>), for states during the time period where the network is passed stimulus information. States at end of time period colored darker for clarity. (<bold>g</bold>) The readout difference of the MPN as a function of sequence time (Methods 5.5). More positive values correspond to the MPN better distinguishing the two classes for the subtask. Solid lines correspond to when the subtask is chosen by the context, dotted lines to when the subtask is not chosen by the context. Grey, yellow, and white background correspond to stimulus, context, and delay periods, respectively. [h-i] <italic>Readout alignment with subtask evidence variation as a function of time</italic> (Methods). (<bold>h</bold>) Retrospective context case. Solid lines correspond to relevant subtask, dotted line to irrelevant subtask. Notably, the alignment of the readouts and variation plummets for the irrelevant task when the context is passed to the network. (<bold>i</bold>) Prospective context. Alignment is only shown after evidence is passed to the network because subtask evidence variation is ill-defined prior to evidence stimuli. (<bold>j</bold>) Closeup of the hidden activity for the stimulus period (prior to the go signal) for the continuous integration task shown in <xref ref-type="fig" rid="fig2">Figure 2j</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83035-fig7-figsupp1-v3.tif"/></fig></fig-group><p>We find the MPN is easily capable of learning this task and again achieves near-perfect accuracy, on average 98.4%. To understand how the MPN is capable of processing context, we can investigate how its dynamics change from that of the simple integration task analyzed previously. Once again turning to the state of MPN to see what information it encodes, during the time-period where the subtask stimuli are being input we see <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> holds information about both the integration subtasks simultaneously (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). That is, we see the same continuum that encodes the relative evidence between the two classes that we saw in the single task case earlier (<xref ref-type="fig" rid="fig3">Figure 3f</xref>), but for both of the integration subtasks. Furthermore, these two one-dimensional continua lie in distinct subspaces, allowing a single location in the two-dimensional space to encode the relative evidences of both subtasks at once. This make sense from the perspective that the network does not yet know which information is relevant to the output, so must encode information from each subtask prior to seeing the context.</p><p>When the context is finally passed to the network, we see its state space becomes increasingly separated into two clusters that correspond to whether the context is asking to report the label from subtask 1 or 2 (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). Separating the states into distinct regions of state space allows for them to be processed differently when converted to hidden and output activity, and we now show this is how the MPN solves the task. We can quantify the difference the separation induced by contextual input produces by looking at the how each states gets converted into an output and how this changes with context. We define a subtask’s <italic>readout difference</italic> such that more positive values means the two classes belonging to the subtask are easier to distinguish from one another via the readout vectors, that is what is needed to solve the subtask (Methods 5.5). Prior to the context being passed, we see the readout difference increases for both subtasks as evidence is accumulated (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). As soon as the contextual input is passed, the readout difference for the subtask that is now irrelevant (the one that doesn’t match the context, dotted line) immediately plummets, while that of the relevant subtask (the one that matches the context, solid line) increases (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). (An alternative way to quantify this difference is to compare each subtask’s direction of evidence variation (<xref ref-type="fig" rid="fig7">Figure 7b</xref>) in hidden activity to that of the readout directions. We find that, after context is passed to the network, the now-irrelevant subtask’s evidence variation becomes close to perpendicular to the readouts, meaning their difference is almost irrelevant to the output neurons (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1d and e</xref>)). After a final delay period, the go signal is passed to the network and these two separate clusters of state space are readout distinctly, allowing the network to output the label of the appropriate subtask.</p></sec><sec id="s2-3-2"><title>Continuous integration</title><p>Thus far, we have investigated integration tasks where evidence comes in discrete chunks, but often evidence from stimuli can take on continuous values (<xref ref-type="bibr" rid="bib45">Mante et al., 2013</xref>). In this task, the network receives continuous inputs and must determine if the input was drawn from a distribution with positive or negative mean (<xref ref-type="fig" rid="fig7">Figure 7e</xref>). Evidence is again passed to the network through a random binary vector, but the vector is multiplied by the continuous signal. The MPN is once again able to achieve near perfect accuracy on this task.</p><p>Since all evidence is scalar multiples of a random input vector, the hidden neuron activity prior to the go signal now exists in a single cluster as opposed to the distinct input-clusters we saw for the discrete case (<xref ref-type="fig" rid="fig7">Figure 7f</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1j</xref>). The go signal again has its own separate cluster, within which the hidden state varies with the total evidence of the sequence and is well-aligned with the readout vectors (<xref ref-type="fig" rid="fig7">Figure 7f</xref>). Although the dynamics of this task may look more similar to the line-attractors, we again note that the hidden neuron activity largely tracks input activity rather than accumulated evidence, unlike an RNN (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). The accumulated evidence is still stored in the SM matrix, <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the low-dimensional space, the state moves along a line to track the relative evidence between the two classes, before jumping to a separate cluster when the go signal is passed (<xref ref-type="fig" rid="fig7">Figure 7h</xref>). Again note this is distinct from line attractor dynamics of the RNN, since in the absence of stimulus during a delay period, <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> will still exponentially decay back towards its baseline value at <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s2-3-3"><title>NeuroGym tasks</title><p>NeuroGym is a collection of tasks with a common interface that allows for rapid training and assessment of networks across many neuroscience-relevant computations (<xref ref-type="bibr" rid="bib52">Molano-Mazon et al., 2022</xref>). We train the MPN, MPNpre, and RNNs on 19 different supervised learning NeuroGym tasks. Whereas the tasks we have considered thus far only require the networks output task-relevant information after a go signal, the NeuroGym tasks require the network to give a specific output at all sequence time steps (<xref ref-type="fig" rid="fig7">Figure 7i</xref>). Since several of the NeuroGym tasks we investigate have very low-dimensional input information (the majority with <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>≤</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), we pass pass all inputs through a fixed random matrix and added bias before passing them into the relevant networks (Methods 5.1). Across all 19 tasks we investigate, we find that the MPN performs at levels comparable to the GRU and VanillaRNN (<xref ref-type="fig" rid="fig7">Figure 7j</xref>). Despite its simplified modulation mechanism, the MPNpre also achieves comparable performance on the majority of tasks. Since we compare networks with the same number of input, hidden, and output neurons, we again note that the MPN and MPNpre have significantly fewer parameters than their RNN counterparts (see <xref ref-type="table" rid="table1">Table 1</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we have thoroughly explored the trained integration dynamics of the MPN, a network with multiple forms of plasticity. It has connections between neurons that are effectively a product between two terms: (1) the <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix, trained in a supervised manner with backpropagation and assumed to be constant during input sequences and (2) the synaptic modulation matrix, <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which has faster dynamics and evolves in an unsupervised manner. We analyzed MPNs without recurrent connections so that they have to rely solely on synaptic dynamics for the storage and updating of short-timescale information. Unlike an RNN, we have found the dynamics of the hidden neurons in the MPN primarily track the present input to the network, and only at subleading-order do we see them encode accumulated evidence. This makes sense from the point of view that the hidden neurons have two roles in the MPN: (1) they connect directly to the readouts and must hold information about the entire sequence for the eventual output, but also (2) they play a role in encoding the input information to update the <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix. We also investigated the MPNpre, a network whose modulations depend only on presynaptic activity. Despite this simplified update mechanism, we find the MPNpre is often able to perform computations at the level of the MPN and operates using qualitatively similar dynamics.</p><p>The synaptic modulations, contained in the SM matrix, <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, encode the accumulated evidence of input sequence through time. Hence, the synaptic modulations play the role of the state of the MPN, similar to the role the hidden activity, <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, plays in the RNN. Additionally, we find the MPN’s state space has a fundamentally different attractor structure than that of the RNN: the uniform exponential decay in <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> imbues the state space with a single point-like attractor at <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Said attractor structure is task-independent, which significantly contrasts the manifold-like, task-dependent attractors of RNNs. Despite its simplistic attractor structure, the MPN can still hold accumulated evidence over time since its state decays slowly and uniformly, maintaining the relative encoding of information (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Although they have relatively simple state dynamics, across many neuroscience-relevant tests, we found the MPN and MPNpre were capable of performing at comparable levels to the VanillaRNN and GRU, sometimes even outperforming their recurrent counterparts. The exception to this was noise robustness, where the RNNs’ attractor structure allows them to outperform the MPN in a relatively small window of signal to noise ratio. However, the MPN’s integration dynamics that rely on its minimal attractor structure allowed it to outperform RNNs in both minimal training and sequential-learning settings. Altogether, we find such performance surprising given the simplicity of the MPN and MPNpre. Unlike the highly designed architecture of the GRU, these modulation-based networks operate using relatively simple biological mechanisms, with no more architectural design than the simplest feedforward neural networks.</p><p>While this study focuses on theoretical models with a general synapse-specific change of strength on shorter timescales than the structural changes implemented via backpropagation, we can hypothesize potential mechanistic implementations for the MPN and MPNpre. The MPNpre’s presynaptic activity dependent modulations can be mapped to STSP, which is largely dependent on availability and probability of release of transmitter vesicles (<xref ref-type="bibr" rid="bib69">Tsodyks and Markram, 1997</xref>). As an example, the associative modulations and backpropagation present in MPN could model the distinct plasticity mechanisms underlying early and late phases of LTP/LTD (<xref ref-type="bibr" rid="bib7">Baltaci et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Becker and Tetzlaff, 2021</xref>). The early phase of the LTP can be induced by rapid and transient Calmodulin-dependent Kinase II (CaMKII) activation, sometimes occurring within <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s (<xref ref-type="bibr" rid="bib27">Herring and Nicoll, 2016</xref>) and which can last for <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> min (<xref ref-type="bibr" rid="bib34">Lee et al., 2009</xref>). CaMKII is necessary and sufficient for early LTP induction (<xref ref-type="bibr" rid="bib62">Silva et al., 1992</xref>; <xref ref-type="bibr" rid="bib57">Pettit et al., 1994</xref>; <xref ref-type="bibr" rid="bib36">Lledo et al., 1995</xref>) and is known to be activated by Ca2<sup>+</sup> influx via N-methyl-D-aspartate receptors (NMDAR), which open primarily when the pre- and postsynaptic neurons activate in quick succession (<xref ref-type="bibr" rid="bib27">Herring and Nicoll, 2016</xref>). This results in insertions of AMPA receptors in the synapses within 2 min resulting in changes in postsynaptic currents (<xref ref-type="bibr" rid="bib56">Patterson et al., 2010</xref>). These fast transitions that occur on scales of minutes can stabilize and have been observed to decay back to baseline within a few hours (<xref ref-type="bibr" rid="bib10">Becker and Tetzlaff, 2021</xref>). Altogether, these associative modulations leading to AMPA receptor exocytosis that can be sensitive to stimulus changes on the order of minutes can be represented by the within-sequence changes in modulations of the MPN. The AMPA receptor decay back to baseline corresponds to a decay of the modulations (via the <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> term) corresponding to hours. Subsequently, based on more complex mechanisms that involve new protein expression and can affect the structure, late phase LTP can stabilize changes over much longer timescales (<xref ref-type="bibr" rid="bib7">Baltaci et al., 2019</xref>). If these mechanisms bring in additional factors that can be used in credit assignment across the same synapses affected by early phase LTP, they would map to the slow synaptic changes represented by backpropagation in the MPN’s weight adjustment (<xref ref-type="bibr" rid="bib35">Lillicrap et al., 2020</xref>).</p><p>The simplicity of the MPN and MPNpre leaves plenty of room for architectural modifications to either better match onto biology or improve performance. Foremost among such modifications is to combine recurrence and dynamic synapses into a single network. The MPN/MPNpre and Vanilla RNN are subnetworks of this architecture, and thus we already know this network could exhibit either of their dynamics or some hybrid of the two. In particular, if training is incentivized to find a solution with sparse connections or minimal activity, the MPN that computes with no recurrent connections and activity silence could be the preferred solution. The generality of the MPN/MPNpre’s synaptic dynamics easily allows for the addition of such dynamic weights to any ANN layer, including recurrent layers (<xref ref-type="bibr" rid="bib54">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib6">Ballintyn et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Burnham et al., 2021</xref>). Finally, adding synaptic dynamics that vary with neuron or individual synapses would also be straightforward: the scalar <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameters that are uniform across all neurons can be replaced by a vector or matrix equivalents that can be unique for each pre- or postsynaptic neuron or synapse (<xref ref-type="bibr" rid="bib60">Rodriguez et al., 2022</xref>). A non-uniform decay from vector/matrix-like <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> would allow the MPN have a more nuanced attractor structure in its state space. Indeed, the fact that the SM matrix decays to zero resembles the fading memory of reservoir computing models and there it has been shown multiple timescales facilitate computation (<xref ref-type="bibr" rid="bib21">de Sá et al., 2007</xref>).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>As in the main text, throughout this section we take the number of neurons in the input, hidden, and output layers to be be <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. We continue to use uppercase bold letters for matrices and lowercase bold letters for vectors. We use <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to index the hidden neurons and <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to index the input neurons. For components of matrices and vectors, we use the same non-bolded letter, e.g. <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>J</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><sec id="s4-1"><title>Supporting code</title><p>Code for this work can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/kaitken17/mpn">https://github.com/kaitken17/mpn</ext-link> (copy archived at <xref ref-type="bibr" rid="bib2">Aitken and Mihalas, 2023</xref>). We include a demonstration of how to implement a <italic>multi-plasticity layer</italic>, which allows one to incorporate the synaptic modulations used in this work into any fully-connected ANN layer. This allows one to easily generalize the MPN to, say, deeper networks or networks with multi-plastic recurrent connections.</p></sec><sec id="s4-2"><title>Tasks</title><sec id="s4-2-1"><title>Simple integration task</title><p>The simple integration task is used throughout this work to establish a baseline for how MPNs and RNNs learn to perform integration. It is inspired by previous work on how RNNs learn to perform integration in natural language processing tasks, where it has been shown they generate attractor manifolds of a particular shape and dimensionality (<xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref>; <xref ref-type="bibr" rid="bib44">Maheswaranathan and Sussillo, 2020</xref>; <xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>).</p><p>The N-class integration task requires the network to integration evidence from multiple classes over time and determine the class with the most evidence (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Each example from the task consists of a sequence of <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> input vectors, <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, passed to the network one after another. We draw possible inputs at a given time step from a bank of <italic>stimulus inputs</italic>, <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>null</mml:mtext><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, ‘<inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’ corresponds to one unit of evidence for the mth class. The ‘null’ input provides evidence for none of the classes. Each sequence ends in an ‘go signal’ input, letting the network know an output is expected at that time step. All the stimulus inputs have a one-to-one mapping to a distinct random binary vector that has an expected magnitude of 1 (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, see below for details). Finally, each example has an integer label from the set <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, corresponding to the class with the most evidence. The network has correctly learned the task if its largest output component at time <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the one that matches each example’s label.</p><p>The input sequence examples are randomly generated as follows. For a given sequence of an N-class task, the amount evidence for each class in the entire sequence can be represented as an N-dimensional <italic>evidence vector</italic>. The mth element of this vector is the number of <inline-formula><mml:math id="inf252"><mml:msub><mml:mtext mathsize="90%">evid</mml:mtext><mml:mi mathsize="90%">m</mml:mi></mml:msub></mml:math></inline-formula> in the given sequence. For example, the three-class sequence of length <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, ‘<inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, null, <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, go’ has an evidence vector <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The sequences are randomly generated by drawing them from a uniform distribution <italic>over possible evidence vectors</italic>. That is, for a given <inline-formula><mml:math id="inf259"><mml:mi mathsize="90%">N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf260"><mml:mi mathsize="90%">T</mml:mi></mml:math></inline-formula>, we enumerate all possible evidence vectors and draw uniformly over said set. Sequences that have two or more classes tied for the most evidence are eliminated. Then, for a given evidence vector, we draw uniformly over sequences that could have generated said vector (Note that simply drawing uniformly over the bank of possible inputs significantly biases the inputs away from sequence with more extreme evidence differences. This method is still biased toward evidence vectors with small relative differences, but much less so than the aforementioned method). Unless otherwise stated, we generally consider the case of <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> throughout this work.</p><p>To generate the random binary vectors that map to each possible stimulus input, each of their elements is independently drawn uniformly from the set <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msqrt><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The binary vectors are normalized by <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> so they have an expected magnitude of 1,<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes L2-normalization. Note the expected dot product between two such vectors is<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Often, we will take the element-wise (Hadamard) product between two input vectors, the expected magnitude of the resulting vector is<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where we have used the fact that the only nonzero element of the element-wise product occurs when the elements are both <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. The fact that this product scales as <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will be useful for analytical approximations later on.</p><p>Notably, since this task only requires a network to determine the class with the <italic>most</italic> evidence (rather than the absolute amount of evidence for each class), we claim this task can be solved by keeping track of <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> relative evidence values. For example, in a two-class integration task, at a minimum the network needs to keep track of a single number representing the relative evidence between the two classes. For a three-class task, the network could keep track of the relative evidence between the first and second, as well as the second and third (from which, the relative evidence between the first and third could be determined). This generalizes to <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for an N-class integration class.(The range of numbers the network needs to keep track of also scales with the length of the sequence, <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For instance, in a two-class integration task, the relative difference of evidence for the two classes, that is evidence for class one minus evidence for class two, are all integers in the range <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></sec><sec id="s4-2-2"><title>Simple integration task with delay</title><p>A modified version of the simple integration task outlined above involves adding a delay period between the last of the stimulus inputs and the go signal (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). We denote the length of the delay period by <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. During the delay period, the sequence inputs (without noise) are simply the zero vector, <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Unless otherwise stated, we consider the case of <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for this task. We briefly explore the effects of training networks on this task where the delay input has a small nonzero magnitude, see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p></sec><sec id="s4-2-3"><title>Contextual integration task (retrospective and prospective)</title><p>For the retrospective context task, we test the network’s ability to hold onto multiple pieces of information and then distinguish between said information from a contextual clue (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). The prospective integration task is the same but has the contextual cue precede the stimulus sequence (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1d</xref>). Specifically, we simultaneously pass the network <italic>two</italic> <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> simple integration tasks with delay by concatenating their inputs together. The label of the entire sequence is the label of one of the two integration subtasks, determined by the context which is randomly chosen uniformly over the two possibilities (e.g. subtask 1 or subtask 2) for each sequence. Note for each of the subtasks, labels take on the values <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so there are still only two possible output labels.</p><p>As with above, each task has its various inputs mapped to random binary vectors and the full concatenated input has an expected magnitude of 1. We specifically considered the case where the input size was <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so each subtask has 25-dimensional input vectors. The full sequence length was <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>stimulus</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>19</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>context</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. For both the retrospective and prospective setups, context was passed with 10 delay time steps before it, and 5 delay time steps after it.</p></sec><sec id="s4-2-4"><title>Continuous integration task</title><p>The continuous integration tests the networks ability to integrate over a continuous values, rather than the discrete values used in the pure integration task above (<xref ref-type="fig" rid="fig7">Figure 7e</xref>).</p><p>The continuous input is randomly generated by first determining the mean value, µ, for a given example by drawing uniformly over the range <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.1875</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1875</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For a sequence length of <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, at each time step the continuous signal is drawn from the distribution <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>750</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each example has a binary label corresponding to whether µ is positive or negative. Numerical values were chosen such that the continuous integration has a similar difficulty to the integration task investigated in <xref ref-type="bibr" rid="bib45">Mante et al., 2013</xref>. The continuous signal is then multiplied by some random binary vector. Unlike the previous tasks, since the continuous input can be negative, the input values to the network can be negative as well. The go signal is still some random binary vector. We specifically consider the case of <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-5"><title>True-anti contextual integration task</title><p>The true-anti contextual integration task is the same as the simple <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> integration task, except the correct label to a given example may be the class with the <italic>least</italic> evidence, determined by a contextual clue. The contextual clue is uniformly drawn from the set <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mtext>true</mml:mtext><mml:mo>,</mml:mo><mml:mtext>anti</mml:mtext><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each possible clue again one-to-one maps to a random binary vector that is <italic>added</italic> to the random binary vectors of the normal stimulus input at all time steps (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1a</xref>). We specifically consider the case of <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Note this task is not discussed in detail in the main text, instead the details are shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. We find both the MPN and GRU are easily able to learn to solve this task.</p></sec><sec id="s4-2-6"><title>NeuroGym tasks</title><p>We also train our networks on 19 of the supervised learning tasks of the NeuroGym package (<xref ref-type="bibr" rid="bib52">Molano-Mazon et al., 2022</xref>). As mentioned in the main text, unlike the other tasks considered in this work, the NeuroGym tasks require the network to output responses at all time steps of a sequence rather than following a go signal. Additionally, sequences of the NeuroGym tasks often contain several different trials of a given task, meaning the networks do not automatically have their states reset at the beginning of each distinct task trial. All NeuroGym tasks use default parameters and a sequence length of <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. See (<xref ref-type="bibr" rid="bib52">Molano-Mazon et al., 2022</xref>) for details about each of the tasks we consider.</p><p>We pass all inputs of the NeuroGym tasks through a fixed random linear layer. This is because some tasks require the networks to respond during delay periods represented by zero input and with zero input into MPN and MPNpre they cannot give a modulation-dependent response. Specifically, for a NeuroGym input size <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, we generate the random matrix <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and random vector <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> using Xavier initialization (see below). All NeuroGym inputs are passed through this matrix to yield the inputs to the network via<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The values of <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>rand</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are fixed during training and are redrawn for each separate initialization. We use <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> across all NeuroGym tasks.</p></sec></sec><sec id="s4-3"><title>Networks</title><p>In this section, we give a more thorough description for the various networks consider in this work. The number of adjustable parameters in the networks as well as the scaling of the number of state update operations they require are given in <xref ref-type="table" rid="table1">Table 1</xref>. Unless otherwise stated, throughout this work we take networks to have input and hidden neuron counts of <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">50</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf298"><mml:mrow><mml:mi mathsize="90%">n</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">100</mml:mn></mml:mrow></mml:math></inline-formula>, respectively. In this setting, the MPN has roughly 1/3 the number of trainable parameters as the Vanilla RNN and 1/9 the number of the GRU.</p><sec id="s4-3-1"><title>Multi-plasticity network</title><p>The multi-plasticity networks used in this work can be thought of as a generalization of a two-layer, fully-connected, feedforward network, given by<disp-formula id="equ11"><label>(10a)</label><mml:math id="m11"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><label>(10b)</label><mml:math id="m12"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the weights connecting the input and hidden layer, <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the weights connecting the hidden and output layer, and <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the hidden activities. The function <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is some activation function, usually non-linear, which is applied element-wise. Weights <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are adjusted during training (see below for details).</p><p>The primary difference between the MPN and the above network is that the weights between the input and hidden layers are modified by a time-dependent synaptic modulation (SM) matrix, <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. That is, <xref ref-type="disp-formula" rid="equ11">Equation 10a</xref> is replaced by<disp-formula id="equ13"><label>(11a)</label><mml:math id="m13"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(11b)</label><mml:math id="m14"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the same dimensions as <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> has 1 for all its elements. Here, ‘<inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⊙</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>’ represent an element-wise multiplication of the two matrices (the Hadamard product). In the main text, we set <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The former condition we take this to represent the lack of a background signal, and does not change the qualitative results of the main text. In practice, no significant difference in dynamics from the sigmoid activation function was observed, and the <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>tanh</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was chosen to better match the computational structure of the RNNs and existing literature on their dynamics. In <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1e</xref>–<xref ref-type="fig" rid="fig3s3">h–3</xref>, we show the dynamics are qualitatively the same for <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e. the ReLU activation function.</p><p>The SM matrix, <inline-formula><mml:math id="inf316"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula>, serves as an internal state of the network, intialized at <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. As mentioned in the main text, throughout this work we consider two types of SM matrix updates intended to broadly model synaptic modulations that occur over distinct timescales. The primary mechanism we investigate uses both the pre- and postsynaptic firing rates,<disp-formula id="equ15"><label>(12)</label><mml:math id="m15"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are parameters that can be adjusted during training. The alternative modulation update is only dependent upon the presynaptic firing rate,<disp-formula id="equ16"><label>(13)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the all 1s vector. Here, the <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> factor ensures the <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> vector in the outer product has <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> magnitude. As mentioned in the main text, we refer to networks with the synaptic modulation update of <xref ref-type="disp-formula" rid="equ16">Equation 13</xref> as the <italic>MPNpre</italic>, reserving just <italic>MPN</italic> to refer to the more general associative update of <xref ref-type="disp-formula" rid="equ15">Equation 12</xref>.</p><p>Notably, unlike the weight modification from backpropagation, the SM matrix weight modification is <italic>local</italic>. That is, the information to update a given synapse/weight only comes from nodes to which the weight is directly connected. Note the convention we have chosen for the time labels in these update expressions means, in order for <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to be well defined, we must specify an initial state for <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which throughout this work we take to be <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. It is possible to train <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as an additional set of parameters for the network. In practice, we don’t observe significant qualitative differences in the dynamics when this is done.</p><p>We let <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, while <inline-formula><mml:math id="inf329"><mml:mi mathsize="90%">λ</mml:mi></mml:math></inline-formula> must obey <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi><mml:mo>≤</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a hyperparameter of the MPN. Throughout this work we choose <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> unless otherwise noted. This is to enforce a level of biological realism, where the SM matrix exponentially decays. In practice, during training we find that <inline-formula><mml:math id="inf333"><mml:mi mathsize="90%">λ</mml:mi></mml:math></inline-formula> comes close to saturating <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note for <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, in the usual settings where we train for 20 or 40 sequence steps, this means by the time the go signal is passed the first input perturbations decay to roughly 0.35 and 0.13 of their initial values, respectively. As mentioned in the main text, we observe no major difference in performance or dynamics for networks that learn positive or negative <inline-formula><mml:math id="inf336"><mml:mi mathsize="90%">η</mml:mi></mml:math></inline-formula> (see, for example, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1f</xref>). Although (<xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>) finds performance advantages for <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we note that our network is different in that it is more symmetric with respect to signs flips (the only asymmetry is introduced by the input signal). In particular, (<xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>) uses sigmoid activations that limit firing rates to be positive whereas here we use hyperbolic tangent.</p><p>The output of the network is given by the expression <xref ref-type="disp-formula" rid="equ12">Equation 10b</xref>. For ease of interpretation of the mapping between hidden activity and the network output, throughout the main text we take <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Once more, dynamics and quantiative results do not differ significantly from this change.</p><p>To emphasize its functional dependence on only the current input and the synaptic modulations, the MPN can be written in the generic form <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similarly, the hidden layer activity is given by the generic expression <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which makes it fundamentally different than the RNNs’ generic expression, <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, see below for additional details.</p><p>To understand the dynamics of the MPN, it is occasionally useful to see what the network’s hidden activity would look like <italic>if</italic> the go signal were passed to it at the current time step. This is done by translating any given state <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> into its corresponding post-go-signal hidden activity by doing what we call a <italic>go signal projection</italic>, defined by<disp-formula id="equ17"><label>(14)</label><mml:math id="m17"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mtext>{GO}</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>≡</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mtext>{GO}</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mtext>{GO}</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mtext>{GO}</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the go signal input. That is, this defines a mapping from any MPN state to a hidden activity. An example of this projection is shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c, d</xref>, corresponding to the MPN trained on the <inline-formula><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> simple integration task discussed in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><sec id="s4-3-1-1"><title>Additive multi-plasticity network</title><p>An alternative model for the MPN, where the SM matrix additively modifies the input weights, is given by <xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref><disp-formula id="equ18"><label>(15)</label><mml:math id="m18"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where once more <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Other than the above expression for hidden neuron activity, this network is identical to the MPN with element-wise multiplication above. Tests on the performance of the additive MPN compared to the multiplicative MPN used throughout this work found the additive model to generally perform worse than its multiplicative counterpart (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). (We note that the additive MPN setup used here is slightly different than the ‘HebbFF’ used in <xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>. Like the MPN used throughout this work we add no bias to the hidden or output layers, use a <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>tanh</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> activation function, initialize parameters (including <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) using Xavier initialization, limit <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and train with L1 regularization. We also note the SM matrix is denoted by <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="bibr" rid="bib71">Tyulmankov et al., 2022</xref>). The dynamics of this network are also found to be low-dimensional and are shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1i-l</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>. An in-depth analysis of its dynamics is outside the scope of this work.</p></sec><sec id="s4-3-1-2"><title>Postsynatpic <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> updates</title><p>Since throughout this work we consider a modulation updates that are dependent upon either <italic>both</italic> the pre- and postsynaptic firing rates or only the presynaptic firing rate, for completeness here we also consider the SM matrix update that is only postsynaptic dependent. Specifically, we consider an update of the form,<disp-formula id="equ19"><label>(16)</label><mml:math id="m19"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the d-dimensional vectors of all 1’s. That is, the explicit dependence on the pre- or postsynaptic firing rates of <xref ref-type="disp-formula" rid="equ15">Equation 12</xref> is once again just replaced by a uniform vector. The restriction of information does not appear to change the ability of the network to learn significantly, for only postsynaptic dependence we find the network achieve <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>98.8</mml:mn><mml:mo>±</mml:mo><mml:mn>1.2</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> accuracy, compared to a baselines of <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>99.7</mml:mn><mml:mo>±</mml:mo><mml:mn>0.6</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>98.0</mml:mn><mml:mo>±</mml:mo><mml:mn>1.3</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for networks with the associative or presynaptic update rule (mean±s.e., across 10 initializations).</p></sec></sec></sec><sec id="s4-4"><title>Recurrent neural networks</title><p>Throughout this work, we will compare the MPN to RNNs that are common artificial neural networks used in both the neuroscience and machine learning communities. The simplest version of the RNN is the Vanilla RNN, which can again be thought of as a simple modification to the two-layer feedforward network, <xref ref-type="disp-formula" rid="equ11">Equation 10a</xref>, <xref ref-type="disp-formula" rid="equ12">Equation 10b</xref>. Now the hidden layer neurons serve as an additional input to the next layer,<disp-formula id="equ20"><label>(17a)</label><mml:math id="m20"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(17b)</label><mml:math id="m21"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are weights trained in the same manner as those in <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note with this convention for <inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to be well defined we must specify the initial state <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which we always take to be <inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similar to the MPNs above, <inline-formula><mml:math id="inf362"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub></mml:math></inline-formula> could be trained as an additional parameter, but we simply fix it in this work. Additionally, like the MPN, throughout this work we take <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Vanilla RNNs suffer from the vanishing/exploding gradient problem and more sophisticated units have been introduced to remedy these problems. Perhaps one of the two most famous generalizations of the Vanilla RNN is the Gated Recurrent Unit (GRU) (<xref ref-type="bibr" rid="bib19">Cho et al., 2014</xref>). The GRU introduces several gates to the Vanilla RNN’s hidden activity update expressions. These gates control how the hidden activity of the network is changed from one time step to the next via <italic>additive</italic> updates, allowing information to persist for longer timescales (<xref ref-type="bibr" rid="bib28">Hochreiter and Schmidhuber, 1997</xref>; <xref ref-type="bibr" rid="bib4">Ba et al., 2016</xref>). The explicit expressions for the GRU are given by<disp-formula id="equ22"><label>(18a)</label><mml:math id="m22"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mtext>u</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow><mml:mrow><mml:mtext>u</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>u </mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(18b)</label><mml:math id="m23"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mtext>r</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow><mml:mrow><mml:mtext>r</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>r </mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ24"><label>(18c)</label><mml:math id="m24"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp </mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(18d)</label><mml:math id="m25"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf365"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">u</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf366"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">r</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> are the update and reset gate vectors, respectively. The hidden activity is translated to an output through a readout layer, identical to <xref ref-type="disp-formula" rid="equ21">Equation 17b</xref>.</p><p>Both the above RNNs can be written in the generic form <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. That is, despite its complicated update expressions, the GRUs hidden state at a given time step is still only a function of its current input and previous hidden state. We again note the similarity and differences of this expression to the updates expression for the MPN, <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-5"><title>Training</title><p>All networks are trained using standard backpropagation (through time), as implemented through the PyTorch package. Networks are trained using ADAM with default parameters (<xref ref-type="bibr" rid="bib31">Kingma and Ba, 2014</xref>) and a constant learning rate, set to <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. All trained parameters of the network are subject to L1 regularization, and unless otherwise stated the coefficient of said regularization is <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. In all training settings, Gaussian noise of expected magnitude 0.1 is added to the inputs of the network. Throughout this work, training was conducted for a minimum number of time steps and then stopped under three possible conditions: (1) a (rolling average) validation accuracy was met, (2) the (rolling average) validation loss became saturated, or (3) the maximum training time was reached. See below for details on when each of these thresholds were used. Gradients were clipped to 10 to avoid gradient explosion. All weight matrices are initialized using Xavier initialization, e.g. for <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> each element is drawn from a uniform distribution over <inline-formula><mml:math id="inf372"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mi mathsize="90%">γ</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">γ</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>6</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For the MPN, <inline-formula><mml:math id="inf374"><mml:mi mathsize="90%">η</mml:mi></mml:math></inline-formula> is similarly drawn from a uniform distribution between <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt><mml:mo>,</mml:mo><mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is initialized to its maximum value <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, as generally we find it to approach its maximal value during training anyway.</p></sec><sec id="s4-6"><title>Theoretical analysis</title><sec id="s4-6-1"><title>MPN analysis</title><p>Here we give additional details of the theoretical approximations for MPN dynamics. First note that we can write a given <inline-formula><mml:math id="inf377"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> in terms previous inputs, hidden activity, and its initial state<disp-formula id="equ26"> <label>(19)</label><mml:math id="m26"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where, in reaching the final line, we have assumed that <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>In the settings we train the MPNs in this work, <italic>we find the effect of the modulation matrix on the hidden activity to be small compared to the unmodulated weights</italic>. That is, in general, the contributions to the hidden activity in <xref ref-type="disp-formula" rid="equ13">Equation 11a</xref> obey<disp-formula id="equ27"><label>(20)</label><mml:math id="m27"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≪</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Thus, the leading-order contributions to the dynamics of both the hidden activity and the SM matrix come from expanding in terms of the number of modulations and neglecting terms with compounded modulation contributions. In Appendix A, we show explicitly why this occurs in the setup we consider in this work. (In brief, writing out the exact expression for <inline-formula><mml:math id="inf379"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula>, several terms of the form <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> appear. By definition of our input vectors, the non-zero components of the vector <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, see <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>. These components are small compared to the size of terms of a stand-alone input <inline-formula><mml:math id="inf383"><mml:mi mathsize="90%" mathvariant="bold">x</mml:mi></mml:math></inline-formula> (i.e. without the Hadamard product), whose nonzero-components are of size <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>d</mml:mi></mml:mfrac></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, any term with <inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is small compared to a term with just <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). Given the relative size of terms discussed above, the leading-order hidden activity can then be approximated by<disp-formula id="equ28"><label>(21)</label><mml:math id="m28"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>which is just the expression of the hidden activity in an unmodulated feedforward network. Plugging this approximation into the update expression for <inline-formula><mml:math id="inf387"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> above, we arrive at an approximation for the modulation matrix<disp-formula id="equ29"><label>(22)</label><mml:math id="m29"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≈</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Plugging in <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, this matches the approximation used in <xref ref-type="disp-formula" rid="equ6">Equation 5</xref> of the main text (again see Appendix A for additional details).</p><p>Notably, the leading-order expression for <inline-formula><mml:math id="inf390"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> above does not capture the variation due to accumulated evidence we see in the hidden activity. Although subleading, such variation is important for the MPN to solve the integration task. To get a more accurate approximation, we keep terms of subleading order, i.e. terms that arise from a single application of the SM matrix. In this case we arrive at the approximation for hidden activity used in the main text,<disp-formula id="equ30"><label>(23)</label><mml:math id="m30"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Once more, details of how one arrives at this expression are given in Appendix A. One can of course continue expanding in terms of the modulation matrix to get increasingly accurate and complicated expressions. Notably, from the above expression, one can see that in order for the SM matrix to have a nonzero contribution requires the input of the current and previous time steps to have a nonzero Hadamard product. This is especially important for the final hidden activity, which in this work we take to be the go signal, otherwise all subleading contributions to the hidden activity are zero and the network cannot train. This would occur if one used one-hot inputs for all distinct inputs to the network.</p></sec><sec id="s4-6-2"><title>MPNpre analysis</title><p>The lack of hidden activity dependence significantly simplifies the expressions for the MPNpre. In particular, <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in terms previous inputs is simply given by<disp-formula id="equ31"><label>(24)</label><mml:math id="m31"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mi>η</mml:mi><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where once again we have assumed that <inline-formula><mml:math id="inf392"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note this is the same expression as the MPN, where <inline-formula><mml:math id="inf393"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> has been replaced by <inline-formula><mml:math id="inf394"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, the lack of hidden activity dependence in this expression means the recurrent expression for <inline-formula><mml:math id="inf395"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> is no longer dependent upon previous hidden activities. As such, the <italic>exact</italic> expression for the hidden activity in terms of previous inputs is given by<disp-formula id="equ32"><label>(25)</label><mml:math id="m32"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>η</mml:mi><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This is the same as the expression for the MPN above, <xref ref-type="disp-formula" rid="equ30">Equation 23</xref>, except there is now no need to approximate the explicit <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> dependence.</p></sec><sec id="s4-6-3"><title>Review of RNN analysis</title><p>Since we have observed RNNs tend to operate in close vicinity to attractors that in our case are slow/fixed points, we are motivated to approximate their behavior using a linear expansion (<xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref>). If we expand the generic RNN expression, <inline-formula><mml:math id="inf397"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, to linear order about the arbitrary location <inline-formula><mml:math id="inf398"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have<disp-formula id="equ33"><label>(26)</label><mml:math id="m33"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where we have defined the Jacobian matrices<disp-formula id="equ34"><label>(27)</label><mml:math id="m34"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>If we take <inline-formula><mml:math id="inf399"><mml:msup><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mtext mathsize="90%">e</mml:mtext></mml:msup></mml:math></inline-formula> to be a slow/fixed point under the input <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have that <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Inserting this into the above expression, we can approximate the effect of a given input at time <inline-formula><mml:math id="inf402"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">x</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> on the hidden activity <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus an approximation of the state at time <inline-formula><mml:math id="inf405"><mml:mi mathsize="90%">t</mml:mi></mml:math></inline-formula>, assuming <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, is given by <xref ref-type="bibr" rid="bib42">Maheswaranathan et al., 2019a</xref><disp-formula id="equ35"><label>(28)</label><mml:math id="m35"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Using SVD, we can always write <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf408"><mml:mrow><mml:mi mathsize="90%" mathvariant="bold">R</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msup><mml:mi mathsize="90%" mathvariant="bold">L</mml:mi><mml:mrow><mml:mo mathsize="90%" stretchy="false">-</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then this reduces to<disp-formula id="equ36"><label>(29)</label><mml:math id="m36"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where in the second line <inline-formula><mml:math id="inf410"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the columns of <bold>R</bold> and <bold>L</bold>, respectively. From this expression it is straightforward to understand the impact each input will have on a given hidden state in terms of its projection onto the eigenmodes. We see that an eigenmode’s contribution will disappear/explode over time if <inline-formula><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf413"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. In practice, trained networks tend to have one eigenvalue <inline-formula><mml:math id="inf414"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for each integration dimension (e.g. a single eigenvalue at <inline-formula><mml:math id="inf415"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for a line attractor) (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). So long as there are integrator modes, we see all inputs have the ability to contribute to a given <inline-formula><mml:math id="inf416"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> on roughly equal footing. This is fundamentally different from the operation of the MPN discussed above.</p></sec></sec><sec id="s4-7"><title>Network dynamics, dimensional reduction, and modulation bounds</title><sec id="s4-7-1"><title>Determining dimensionality, PCA projection, and explained variance</title><p>Throughout this work, we use PCA to visualized the dynamics. This is always done by passing some test set to the network of size <inline-formula><mml:math id="inf417"><mml:mi mathsize="90%">m</mml:mi></mml:math></inline-formula>, and collecting all activity vectors/matrices (e.g. hidden activity or SM matrices) over the entire test set and some time period, <inline-formula><mml:math id="inf418"><mml:mi mathsize="90%">S</mml:mi></mml:math></inline-formula>. Unless otherwise stated, this time period is over the entire input sequence, that is <inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and thus this yields on order <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> vectors/matrices. Let <inline-formula><mml:math id="inf421"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the size of the space said vectors or matrices live in (e.g. <inline-formula><mml:math id="inf422"><mml:mrow><mml:mi mathsize="90%">D</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mi mathsize="90%">n</mml:mi></mml:mrow></mml:math></inline-formula> for hidden activity, <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for SM matrices). PCA is then performed over this set, which yields some set of PCA vectors, <inline-formula><mml:math id="inf424"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">w</mml:mi><mml:mi mathsize="90%">α</mml:mi></mml:msub></mml:math></inline-formula>, and their associated ratio of variance explained, <inline-formula><mml:math id="inf425"><mml:msub><mml:mi mathsize="90%">v</mml:mi><mml:mi mathsize="90%">α</mml:mi></mml:msub></mml:math></inline-formula>, for <inline-formula><mml:math id="inf426"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext>min</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The dimensionality of activity is determined by calculating the <italic>participation ratio</italic> on the <inline-formula><mml:math id="inf427"><mml:msub><mml:mi mathsize="90%">v</mml:mi><mml:mi mathsize="90%">α</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>),<disp-formula id="equ37"><label>(30)</label><mml:math id="m37"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mtext>PR</mml:mtext><mml:mo>≡</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Except for the smallest networks considered in this work, we always find <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>PR</mml:mtext><mml:mo>≪</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which we take to mean the activity operates in a low-dimensional space.</p><p>To determine the hidden activity’s proportion of variance explained by either the accumulated evidence or the present input, we associate each hidden activity vector, <inline-formula><mml:math id="inf429"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, with the relative score difference between the two classes and present input, <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, at the given time step. We then use scikit-learn’s LinearRegression class with default parameters to fit the hidden activity over an entire test to the accumulated evidence and present input individually. Note for the former, which consists of four possible input categories, we use three indicator functions with the reference case chosen to be the ‘null’ input. A linear fit was chosen for simplicity and the authors felt it was appropriate given the fact the outputs of the network are also a linear function of the hidden activity. The reported proportion of variance explained correspond to the <inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> of the individual fits, averaged over 10 separate initializations.</p></sec><sec id="s4-7-2"><title>MPNpre dynamics</title><p>For the MPNpre, using the update rule that is only dependent upon the presynaptic firing rates does not considerably change the dynamics that we observed for the MPN’s associative learning rule. The hidden activity is again separated into distinct input clusters that each vary with accumulated evidence (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1e and f</xref>). The SM matrix activity grows as the sequence is read in and also encodes the accumulated evidence of the phrase (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1g and h</xref>).</p><p>Comparing the hidden activity expressions for the MPN and MPNpre, <xref ref-type="disp-formula" rid="equ30 equ32">Equations 23 and 25</xref>, we can explicitly see a manifestation of the simpler modulation updates: the MPN’s modulations that are dependent upon both the pre- and postsynatpic activity means the modulations are modified via a more complex dependence on previous inputs. Indeed, the MPNpre has significantly less control over the form of its modulations, the only part of the update rule that can be modified through training is <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Meanwhile, the MPN can modify <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> which in turn affects the <inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and thus can have significantly more nuanced modulation updates. We see this explicitly when comparing the accumulated evidence variation of the input clusters in the two networks. Comparing the insets of <xref ref-type="fig" rid="fig3">Figure 3f</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1g</xref>, we see the alignment of all four input clusters of the MPNpre are high rather than just the go signal cluster for the MPN. Averaging several initializations of the networks, the MPN’s readout alignment for words that are not the go signal is <inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.16</mml:mn><mml:mo>±</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> whereas for the MPNpre it is <inline-formula><mml:math id="inf436"><mml:mrow><mml:mn mathsize="90%">0.79</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.08</mml:mn></mml:mrow></mml:math></inline-formula> (mean±s.e.). Note this also means the individual input clusters are significantly more aligned with one another than in the case of the MPN. These effects are a direct result of the additional term that modifies the hidden state dependence in <xref ref-type="disp-formula" rid="equ30">Equation 23</xref> that is absent in <xref ref-type="disp-formula" rid="equ32">Equation 25</xref>.</p></sec><sec id="s4-7-3"><title>Generalization of MPN dynamics for <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula></title><p>In the main text, the dynamics of the MPN was discussed for the case of an <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> integration task. Here we expand on how said dynamics generalize for <inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Throughout this discussion, we always assume the number of classes is significantly smaller than the hidden activity dimension, i.e. <inline-formula><mml:math id="inf440"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≪</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Like the <inline-formula><mml:math id="inf441"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> case, for <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> we observe the hidden activity to continue to be low-dimensional and separated into distinct input-clusters (dependent upon the most recent input to the network). <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> shows a visualization of the dynamics for the MPN trained on an <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> task. Within each input-cluster, the hidden activity continues to vary with accumulated evidence. However, since keeping track of the relative scores between classes now requires the network to hold more than a single number, the variance of accumulated evidence within each input-cluster takes on a more complex structure. For example, in the <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> case, we see each input-cluster is roughly triangle-like, with the hidden activity belonging to the 3 possible output labels clustering toward each of the corners (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3e</xref>). Similar to the RNN dynamics (see below), we expect accumulated evidence for an N-class task to take on the approximate shape of an N - 1-simplex within each input-cluster (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>).</p><p>Once again, the SM matrix continues to monotonically evolve along a particular direction with sequence index (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3g</xref>). We also find the SM matrix again encodes accumulated evidence of the distinct classes (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3h</xref>). That is, each distinct input deflects the SM matrix in a distinct direction, allowing said matrix to encode all previous inputs to the MPN up to that point.</p><p>As mentioned in the main text, we limit our analysis of dynamics to the case where the separate classes of the task are uncorrelated. Similar to RNNs, we expect the dynamics to qualitatively change in cases where classes are non-uniformly correlated with one another (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>).</p></sec><sec id="s4-7-4"><title>Review of RNN dynamics</title><p>Recall that to solve the N-class simple integration task, at a minimum a network needs to keep track of <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> relative evidence values (see Sec. 5.1 above). In general, so long as <inline-formula><mml:math id="inf446"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≪</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, after training the RNNs’ hidden activity lies in a low-dimensional subspace that has the approximate shape of an <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> regular simplex (which has dimension <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib1">Aitken et al., 2020</xref>). By moving around in said subspace, the RNN encodes the <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> relative evidence values needed to solve the task. If instead the network needed to keep track of the absolute evidence for each class, we would expect the network’s hidden activity to lie in an <inline-formula><mml:math id="inf450"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional subspace. For example, in the <inline-formula><mml:math id="inf451"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> case, the relative evidence is a single number, so we only see a one-dimensional line attractor (a one-simplex). Beyond two-class, the attractor becomes higher-dimensional, for example <inline-formula><mml:math id="inf452"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> yields a two-dimensional triangular attractor (a two-simplex) because the network needs to keep track of the relative evidence of two distinct pairs of classes, from which it also encodes the relative evidence of the final pair of classes (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p></sec><sec id="s4-7-5"><title>Modulation bounds</title><p>To remove the biologically implausibility of arbitrary large modulations, we briefly investigate the effects of bounding the size of the <inline-formula><mml:math id="inf453"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix explicitly. Note bounds on the size of the modulations are usually implicitly implemented by restricting the magnitude of <inline-formula><mml:math id="inf454"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf455"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the explicit bounds, all elements of the matrix are restricted to <inline-formula><mml:math id="inf456"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the size of the modulation bounds. Note that due to the multiplicative modulation expression we use in this work, this means that the total range a given synaptic connection can be modulated depends on the size of the corresponding element of <inline-formula><mml:math id="inf458"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For example, if <inline-formula><mml:math id="inf459"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, then the total synaptic strength can be anywhere in the range from zero to double its value at the start of the sequence, since<disp-formula id="equ38"><label>(31)</label><mml:math id="m38"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>inp</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>inp</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>inp</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Biologically, STDP that potentiates synapses up to ∼200% of their original values and depresses by up to ∼30% at timescales of 10s of minutes has been seen in a wide variety of studies in various parts of the brain (<xref ref-type="bibr" rid="bib63">Sjöström and Häusser, 2006</xref>; <xref ref-type="bibr" rid="bib51">McFarlan et al., 2023</xref>). Other specific studies sometimes find values that are a bit more extreme, for example depression as low as 20% in <xref ref-type="bibr" rid="bib18">Cho et al., 2000</xref>. Over shorter timescales, STSP has been shown to facilitate to a similar relative magnitude, and seems to be capable of almost complete depression (<xref ref-type="bibr" rid="bib16">Campagnola et al., 2022</xref>).</p><p>We notice very little to no significant drop in the network’s ability to do integration-based tasks when the aforementioned biologically realistic bounds on modulations are introduced. For a two-class integration task the unbounded network achieves <inline-formula><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>99.8</mml:mn><mml:mo>±</mml:mo><mml:mn>0.4</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> accuracy, while the for <inline-formula><mml:math id="inf461"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and 0.1 we observe <inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>99.7</mml:mn><mml:mo>±</mml:mo><mml:mn>0.9</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf463"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>96.7</mml:mn><mml:mo>±</mml:mo><mml:mn>2.1</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively (mean±s.e.). Note the latter of these is quite prohibitive, only allowing the synapses to change by 10% of their original value. As expected, the bounds on the modulation matrix result in an overall smaller modulations (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2e</xref>). As a result of the smaller modulation range, we also observe that the hidden activity is also smaller (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2f</xref>) and that the network’s learned <inline-formula><mml:math id="inf464"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>η</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is smaller for more restrictive modulation bounds (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2g</xref>). We hypothesize that the smaller hidden activity and learning rates help facilitate the discrimination of modulation differences when they are restricted to smaller values.</p><p>Even for the very restrictive case of <inline-formula><mml:math id="inf465"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we observe qualitatively similar hidden activity dynamics to the unbounded case discussed in the main text. That is, the hidden activity is separated into distinct input clusters that each encode accumulated evidence at subleading order (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2h</xref>). Meanwhile, the SM matrix activity again grows in time as the sequence is read in (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2i</xref>) and also encodes the accumulated evidence along a perpendicular axis (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2j</xref>). Note the small size of both the hidden activity and SM matrix activity relative to their unbounded counterparts in the main text (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></sec></sec><sec id="s4-8"><title>Figure details</title><sec id="s4-8-1"><title><xref ref-type="fig" rid="fig1">Figure 1</xref> details</title><p>Explicit expressions for the MPN, fully connected network, and (Vanilla) RNN are given above in Sec. 5.2. The details of the backpropagation modified weights are given in Sec. 5.2.3. The details of the modulated synapses/weights are given in <xref ref-type="disp-formula" rid="equ2">Equation 2a</xref>.</p></sec><sec id="s4-8-2"><title><xref ref-type="fig" rid="fig2">Figure 2</xref> details</title><p>Details of the simple integration task shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> can be found in Sec. 5.1 above.</p></sec><sec id="s4-8-3"><title><xref ref-type="fig" rid="fig3">Figure 3</xref> details</title><p>All subplots in this figure are for networks trained on a two-class integration task with <inline-formula><mml:math id="inf466"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and no delay period. For this task, there are only four possible inputs to the networks, ‘<inline-formula><mml:math id="inf467"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’, ‘<inline-formula><mml:math id="inf468"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’, ‘null’, or ‘go’. Networks are trained until an accuracy thershold of 98% is reached, with a minimum of 2000 training batches.</p><p><xref ref-type="fig" rid="fig3">Figure 3a and d</xref> show the activity over the input sequence for a few randomly chosen hidden neurons of a single test example. The single test example is randomly chosen from the set of examples that have at least half the of the maximum possible accumulated evidence difference. A large collection of plots in this work are projected into PCA space as a means of visualizing the low-dimensional dynamics. To generate the plot in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, all hidden neuron activity, <inline-formula><mml:math id="inf469"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf470"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were collected over a test batch of size 1000. Across this entire set of hidden activity, PCA was performed (with a centering of per-feature means). The projection of the hidden neuron activities onto their first two components is plotted in <xref ref-type="fig" rid="fig3">Figure 3b</xref>. The hidden activity is colored by the relative evidence of the two classes, that is total evidence for the red class minus evidence for the blue class, at the associated time-step. In addition to the hidden activity, two example trajectories and the two readout vectors are projected onto the hidden neuron PCA space as well. <xref ref-type="fig" rid="fig3">Figure 3c</xref> contains the same hidden activity projection, but now the hidden states are colored by the most recent input passed to the network, that is whether the input corresponded to ‘<inline-formula><mml:math id="inf471"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’ (red), ‘<inline-formula><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>evid</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>’ (blue), ‘null’ (grey), or ‘go’ (purple). The inset was computed by calculating the change in the hidden activity, <inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, induced by the input <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, again projected into the hidden activity PCA space. Once again, these changes in hidden activity are colored by the corresponding input. Additionally, for each possible input, the set of all hidden activity changes was averaged together to form a mean change in activity, and this was also plotted in the inset as a darker line.</p><p>For the MPN hidden activity, <xref ref-type="fig" rid="fig3">Figure 3e and f</xref> were generated in an identical manner to those in <xref ref-type="fig" rid="fig3">Figure 3b and c</xref> (with the exception of the inset). Note there is no initial hidden activity for the MPN, just an initial SM matrix. The inset of <xref ref-type="fig" rid="fig3">Figure 3f</xref> was computed by separating all hidden activities into distinct groups by their most recent input. Then, for each group of hidden activities, PCA was performed on the subset. The ‘RO alignment’ plotted in the inset is the average across all readout vectors of the cosine angle magnitude between the readout vector, <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the top PC direction, <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="inf477"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The MPN state (<inline-formula><mml:math id="inf478"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) activity shown in <xref ref-type="fig" rid="fig3">Figure 3g and h</xref> is plotted in an analogous manner to the hidden activity for the previous subplots in this figure. That is, we collect all <inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf480"><mml:mrow><mml:mi mathsize="90%">t</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">0</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%" mathvariant="normal">…</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> over the test set, and perform PCA on the <italic>flattened</italic> matrices. We then project the flattened states onto said PCA space and color them in the same manner as the hidden activity in <xref ref-type="fig" rid="fig3">Figure 3b and e</xref> (relative evidence of the two classes). <xref ref-type="fig" rid="fig3">Figure 3g and h</xref> differ only in that they show two different PCA directions along the <inline-formula><mml:math id="inf481"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-axis. The final SM matrix for each input sequence was colored in a slightly darker shade (but still colored by relative evidence) for clarity. We note that the time-evolution direction is not always aligned with the second PC direction as it is in this plot – in our tests it was also often aligned with the first PC direction or diagonally oriented in the PC1-PC2 plane. The inset of <xref ref-type="fig" rid="fig3">Figure 3h</xref> is completely analogous to that in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, but for the MPN state rather than the RNN state. That is, it shows the change in SM matrix, <inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, colored by the present input, <inline-formula><mml:math id="inf483"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-8-4"><title><xref ref-type="fig" rid="fig4">Figure 4</xref> details</title><p>The networks that generated the data in the top row of this figure were all trained on an <inline-formula><mml:math id="inf484"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf485"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> integration task until they reached an average validation accuracy of 98%, with a minimum training size of 2000 batches. MPNs were trained at a variety of <inline-formula><mml:math id="inf486"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values, but in practice we found the MPNs always saturated said value at its maximum so <inline-formula><mml:math id="inf487"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. After training, the networks were then passed longer sequences (up to <inline-formula><mml:math id="inf488"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and their states (i.e. <inline-formula><mml:math id="inf489"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for the RNNs and <inline-formula><mml:math id="inf490"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for the MPNs) were tracked as a function of <inline-formula><mml:math id="inf491"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note the same MPN was used to generate <xref ref-type="fig" rid="fig4">Figure 4a</xref> as that used to generate the dynamics plots in <xref ref-type="fig" rid="fig3">Figure 3[d–f]</xref>. Additionally, the same Vanilla RNN was used to generate <xref ref-type="fig" rid="fig4">Figure 4b</xref> as that used to generate the dynamics plots in <xref ref-type="fig" rid="fig3">Figure 3[a–c]</xref>. All data in <xref ref-type="fig" rid="fig4">Figure 4c and d</xref> were averaged over 10 different network initializations. The normalized final state for the MPN/MPNpre and the RNNs are given by<disp-formula id="equ39"><label>(32)</label><mml:math id="m39"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Note the final state of the MPN/MPNpre is chosen to be <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> because <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is not used in the calculation of the final output.</p><p>For the delay task shown in the bottom row, we train the networks on an <inline-formula><mml:math id="inf494"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> integration-delay task with <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and a total sequence length of <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. For all networks other than the MPNpre, training is stopped when the networks reach an accuracy threshold of 98% with a minimum training size of 2000 batches. We found for the given sizes of <inline-formula><mml:math id="inf497"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the MPNpre sometimes could not reach the aforementioned accuracy threshold and thus they were trained to a threshold of 95% instead. <xref ref-type="fig" rid="fig4">Figure 4e and f</xref> are generated analogously to <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>, with <inline-formula><mml:math id="inf498"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> varied from 10 to 100. The red and blue points shown in <xref ref-type="fig" rid="fig4">Figure 4e and f</xref> correspond to the state projections immediately preceding the onset of the delay. Note <xref ref-type="fig" rid="fig4">Figure 4f</xref> shows the state space of a GRU while <xref ref-type="fig" rid="fig4">Figure 4b</xref> shows that of a Vanilla RNN, since the latter was not able to train on the integration-delay task. <xref ref-type="fig" rid="fig4">Figure 4g and h</xref> are generated analogously to <xref ref-type="fig" rid="fig4">Figure 4c and d</xref>, using the same definition of normalized final states.</p></sec><sec id="s4-8-5"><title><xref ref-type="fig" rid="fig5">Figure 5</xref> details</title><p>The networks shown in <xref ref-type="fig" rid="fig5">Figure 5a and b</xref> were trained on an <inline-formula><mml:math id="inf499"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> integration task, with <inline-formula><mml:math id="inf500"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf501"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>delay</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Sample neurons in <xref ref-type="fig" rid="fig5">Figure 5a</xref> are randomly chosen from a single randomly chosen test example. To calculate the decoding accuracy in <xref ref-type="fig" rid="fig5">Figure 5b and a</xref> linear SVC was trained on the hidden activity of each of the networks, with each hidden activity labeled by its corresponding input sequence’s label. A linear SVC was chosen because the readout vectors also implement flat decision boundaries (although they are piece-wise in the case of the readout vectors). The linear SVC was implemented using scikit-learn’s LinearSVC class with default settings except for number of iterations was increased to 100,000 and balanced class weights. Since the number of classes in this example was <inline-formula><mml:math id="inf502"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the multi-class strategy was all-vs.-one. Additionally, 10 fold cross-validation was used and the results shown are averaged over folds.</p><p><xref ref-type="fig" rid="fig5">Figure 5c</xref> contains the activity as a function of time of a few randomly chosen components of the hidden activity and SM matrix of an MPN from a randomly chosen test example. The theoretical predictions come from <xref ref-type="disp-formula" rid="equ5 equ6">Equations 4 and 5</xref>. In <xref ref-type="fig" rid="fig3">Figure 3d</xref>, to quantify the error in the theoretical approximation we use the following expressions<disp-formula id="equ40"> <label>(33)</label><mml:math id="m40"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mtext>error</mml:mtext><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>theory</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mtext>error</mml:mtext><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>theory</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf503"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the L2-normalization of the correspond vector or matrix (the Frobenius norm) and <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>theory</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf505"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>theory</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are the approximations in <xref ref-type="disp-formula" rid="equ5 equ6">Equations 4 and 5</xref>, respectively. See the derivation leading up to <xref ref-type="disp-formula" rid="equ29 equ30">Equations 22 and 23</xref> to see details of these approximations.</p></sec><sec id="s4-8-6"><title><xref ref-type="fig" rid="fig6">Figure 6</xref> details</title><p>Network capacity in <xref ref-type="fig" rid="fig6">Figure 6a</xref> was measured by training networks on tasks with an increasingly large number of classes, <inline-formula><mml:math id="inf506"><mml:mi mathsize="90%">N</mml:mi></mml:math></inline-formula>. A sequence length of <inline-formula><mml:math id="inf507"><mml:mrow><mml:mi mathsize="90%">T</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">20</mml:mn></mml:mrow></mml:math></inline-formula> was still used for all <inline-formula><mml:math id="inf508"><mml:mi mathsize="90%">N</mml:mi></mml:math></inline-formula>. Training was conducted until the rolling average validation loss plateaued. The rolling average validation loss is the average of the last 10 measured validation losses. Since validation loss is only computed 10 batches, this represents an average over the previous 100 batches of training. In particular, if the current rolling average validation loss was larger than the rolling validation loss measured 100 batches prior, the network’s training was determined to have plateaued. Occasionally this metric may have caused a network to end training early, hence for the aggregate measure we plotted the median accuracy over 10 separate initializations. In <xref ref-type="fig" rid="fig6">Figure 6b</xref>, network capacity as a function of sequence length, <inline-formula><mml:math id="inf509"><mml:mi mathsize="90%">T</mml:mi></mml:math></inline-formula>, was computed in an identical manner. A class size of <inline-formula><mml:math id="inf510"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula> was used for all <inline-formula><mml:math id="inf511"><mml:mi mathsize="90%">T</mml:mi></mml:math></inline-formula>.</p><p>For <xref ref-type="fig" rid="fig6">Figure 6c</xref>, we trained networks at a fixed ratio of (expected) input noise and signal magnitude. Specifically, if the input without noise is <inline-formula><mml:math id="inf512"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the noise is <inline-formula><mml:math id="inf513"><mml:mi mathsize="90%" mathvariant="bold">N</mml:mi></mml:math></inline-formula>, then<disp-formula id="equ41"><label>(34)</label><mml:math id="m41"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>α</mml:mi><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf514"><mml:mrow><mml:mi mathsize="90%">α</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">10</mml:mn></mml:mrow></mml:math></inline-formula> was chosen for the initial training. We then systematically scanned over <inline-formula><mml:math id="inf515"><mml:mi mathsize="90%">α</mml:mi></mml:math></inline-formula> values and measured accuracy (without retraining on different <inline-formula><mml:math id="inf516"><mml:mi mathsize="90%">α</mml:mi></mml:math></inline-formula>). It was observed that training the networks at lower <inline-formula><mml:math id="inf517"><mml:mi mathsize="90%">α</mml:mi></mml:math></inline-formula> did not change the results significantly.</p><p>For the echo-state setup in <xref ref-type="fig" rid="fig6">Figure 6d</xref>, all weights except for the readout matrix, <inline-formula><mml:math id="inf518"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">W</mml:mi><mml:mtext mathsize="90%">RO</mml:mtext></mml:msub></mml:math></inline-formula>, were frozen at their initialization values during training. Since in the MPNs, <inline-formula><mml:math id="inf519"><mml:mi mathsize="90%">λ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf520"><mml:mi mathsize="90%">η</mml:mi></mml:math></inline-formula> play especially important roles in their operation, we initialized <inline-formula><mml:math id="inf521"><mml:mrow><mml:mi mathsize="90%">λ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:msup><mml:mi mathsize="90%">λ</mml:mi><mml:mtext mathsize="90%">max</mml:mtext></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0.95</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf522"><mml:mrow><mml:mi mathsize="90%">η</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">1.0</mml:mn></mml:mrow></mml:math></inline-formula>. Training was performed until the validation loss plateaued, see above.</p><p><xref ref-type="fig" rid="fig6">Figure 6e</xref> was generated by first training the networks on an <inline-formula><mml:math id="inf523"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula> integration-delay task until an accuracy threshold, measured by the rolling average validation accuracy (over the last 10 measured validation runs), achieves an accuracy of 97% or higher. Then, the network was trained on a new <inline-formula><mml:math id="inf524"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula> integration-delay task. Afterwards, we test how much the accuracy on the original <inline-formula><mml:math id="inf525"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula> task fell. We take <inline-formula><mml:math id="inf526"><mml:mrow><mml:mi mathsize="90%">T</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">40</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf527"><mml:mrow><mml:msub><mml:mi mathsize="90%">T</mml:mi><mml:mtext mathsize="90%">delay</mml:mtext></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">20</mml:mn></mml:mrow></mml:math></inline-formula> for both tasks. The new task uses the same random vectors for the ‘null’ and go signal inputs, but generates new random vectors for the two types of evidence. The label set of the new task is the same as the old task and the readouts must be adjusted to compensate. That is, we use the same readouts for both tasks, so the readouts may get adjusted during the second task. This is chosen to simulate the bottleneck of sparse strong connections between areas of the brain. After the network achieves the same accuracy threshold on the new task, its accuracy on the original task is reported. In this setup, we reduce the networks’ batch size to 1 so we the training can be monitored down to individual examples and also reduced the L1 regularization coefficient to 10<sup>-6</sup>. Additionally, raw data in <xref ref-type="fig" rid="fig6">Figure 6e</xref> shows an average over 20 training examples, and some accuracies (for the GRU in particular) are too low to be seen on the plot.</p><p>For the MPN, we find the change in dimensionality of the state space (post-training minus pre-training) to be <inline-formula><mml:math id="inf528"><mml:mrow><mml:mn mathsize="90%">0.08</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.04</mml:mn></mml:mrow></mml:math></inline-formula> (mean±s.e.). Similarly, for the GRU, we find the change to be <inline-formula><mml:math id="inf529"><mml:mrow><mml:mn mathsize="90%">0.19</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.08</mml:mn></mml:mrow></mml:math></inline-formula> (mean±s.e.). To find the change in GRU line attractor angle, we take the first PC direction of the hidden states to be a good measure of the direction of the line attractor. For both pre- and post-training, we continue to find the dimensionality of the GRU state space to be small, <inline-formula><mml:math id="inf530"><mml:mrow><mml:mn mathsize="90%">1.52</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.10</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf531"><mml:mrow><mml:mn mathsize="90%">1.71</mml:mn><mml:mo mathsize="90%" stretchy="false">±</mml:mo><mml:mn mathsize="90%">0.13</mml:mn></mml:mrow></mml:math></inline-formula>, respectively (mean±s.e.).</p><p>The PC plots in <xref ref-type="fig" rid="fig6">Figure 6f and g</xref> are generated as per usual for the states of the respective networks (including the novel states). Looking along higher PC directions, we did not observe a large degree of separation between the new class and previous classes in the RNN.</p><p>The decoding accuracy in <xref ref-type="fig" rid="fig6">Figure 6h</xref> was calculating by training a linear SVC on only the final state activity of the MPN and GRU. The sequence labels were used to label the states. Otherwise, the setup for the linear SVC was identical to that used in <xref ref-type="fig" rid="fig5">Figure 5b</xref>, see above. Since the MPN and GRU have the same <inline-formula><mml:math id="inf532"><mml:mrow><mml:mi mathsize="90%">n</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">100</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">50</mml:mn></mml:mrow></mml:math></inline-formula>, this means the state of the MPN has 50 times more components than the GRU. Thus, to ensure the SVCs are being trained on equal footing for the two networks, we dimensionally reduce the (flattened) MPN states using PCA so that said states have the same number of components as the GRU state (100 in this case). Note that in practice, both networks operate their states in significantly smaller subspaces than the number of components they have, and said subspaces are of comparable dimension for the two types of networks (but not the same, and this may contribute to the MPN’s higher accuracy).</p></sec><sec id="s4-8-7"><title><xref ref-type="fig" rid="fig7">Figure 7</xref> details</title><p>Details on the tasks shown in <xref ref-type="fig" rid="fig7">Figure 7a, e and i</xref> can be found in Sec. 5.1 above. Accuracy on the retrospective task was found by training 10 different initializations on said task with <inline-formula><mml:math id="inf534"><mml:mrow><mml:mi mathsize="90%">T</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">40</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf535"><mml:mrow><mml:msub><mml:mi mathsize="90%">T</mml:mi><mml:mtext mathsize="90%">delay</mml:mtext></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">20</mml:mn></mml:mrow></mml:math></inline-formula> until the average validation loss saturated (with a minimum training time of 2000 batches). In figures for the retrospective integration, we color the example sequences by both their final label as when as their context, yielding <inline-formula><mml:math id="inf536"><mml:mrow><mml:mrow><mml:mn mathsize="90%">2</mml:mn><mml:mo mathsize="90%" stretchy="false">×</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">4</mml:mn></mml:mrow></mml:math></inline-formula> possible colorings.</p><p>For <xref ref-type="fig" rid="fig7">Figure 7[b–d]</xref>, data is shown for a single network, trained to an accuracy threshold of 98%. In <xref ref-type="fig" rid="fig7">Figure 7d</xref>, we define the output difference as follows. To determine the potential output at any given time-step, we perform the go signal projection of the states <inline-formula><mml:math id="inf537"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula>, see <xref ref-type="disp-formula" rid="equ17">Equation 14</xref>, and then further pass this through the readouts,<disp-formula id="equ42"><label>(35)</label><mml:math id="m42"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>RO</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For an entire test set, we then group each sequence of outputs into the <inline-formula><mml:math id="inf538"><mml:mrow><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">×</mml:mo><mml:mn mathsize="90%">2</mml:mn><mml:mo mathsize="90%" stretchy="false">×</mml:mo><mml:mi mathsize="90%">N</mml:mi></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathsize="90%">N</mml:mi><mml:mn mathsize="90%">2</mml:mn></mml:msup></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">8</mml:mn></mml:mrow></mml:math></inline-formula> distinct possible combinations of label, subtask, and context. We then compute the average output sequence for each of these eight groupings, to arrive at eight sequences that represent the average output in each situation. At risk of abusing the number of subscripts a variable can take, let <inline-formula><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> be this average output sequence for label <inline-formula><mml:math id="inf540"><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">l</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, subtask <inline-formula><mml:math id="inf541"><mml:mrow><mml:mi mathsize="90%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and context <inline-formula><mml:math id="inf542"><mml:mrow><mml:mi mathsize="90%">c</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> at a given time <inline-formula><mml:math id="inf543"><mml:mi mathsize="90%">t</mml:mi></mml:math></inline-formula>. For <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula>, each <inline-formula><mml:math id="inf545"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a two-dimensional vector. The <italic>readout difference</italic> at a given time <inline-formula><mml:math id="inf546"><mml:mi mathsize="90%">t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf547"><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">y</mml:mi><mml:mrow><mml:mi mathsize="90%">t</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, is then defined to be<disp-formula id="equ43"><label>(36)</label><mml:math id="m43"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>GO</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Intuitively, this is the average over the output vector-component difference of the two possible labels. For each of these quantities, the network should have learned that this difference should be positive if the sequence is to be labeled correctly (e.g. when the task has label <inline-formula><mml:math id="inf548"><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">l</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula>, the second component of <inline-formula><mml:math id="inf549"><mml:mi mathsize="90%" mathvariant="bold">y</mml:mi></mml:math></inline-formula> should be larger than the first so <inline-formula><mml:math id="inf550"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). The more positive a given <inline-formula><mml:math id="inf551"><mml:mrow><mml:mi mathsize="90%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathsize="90%">y</mml:mi><mml:mrow><mml:mi mathsize="90%">t</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the better the network is distinguishing sequences that belong to the individual labels at the given time step. The four lines in <xref ref-type="fig" rid="fig7">Figure 7d</xref> are generated by plotting all four combinations of <inline-formula><mml:math id="inf552"><mml:mrow><mml:mi mathsize="90%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf553"><mml:mrow><mml:mi mathsize="90%">c</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="90%">1</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, where the solid lines correspond to <inline-formula><mml:math id="inf554"><mml:mrow><mml:mi mathsize="90%">τ</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mi mathsize="90%">c</mml:mi></mml:mrow></mml:math></inline-formula> and the colors correspond to the <inline-formula><mml:math id="inf555"><mml:mi mathsize="90%">τ</mml:mi></mml:math></inline-formula> label.</p><p><xref ref-type="fig" rid="fig7">Figure 7g</xref> was generated by randomly choosing an example sequence and a single input and hidden neuron values. Each sequence was then normalized to have zero mean and a maximum magnitude of 1 for better comparison.</p><p><xref ref-type="fig" rid="fig7">Figure 7j</xref> shows the average accuracy of five separate network initializations for each type of network. All networks were trained on a batch size of 32 for a minimum of 2000 iterations and with hidden biases. Training was ended when either the network achieved a (rolling) accuracy of 99% on the task or the (rolling) validation accuracy saturated. All NeuroGym <xref ref-type="bibr" rid="bib52">Molano-Mazon et al., 2022</xref> used a sequence length of 100 steps and were generated with default parameters. Since the NeuroGym tasks require outputs at all sequence time steps, we compute accuracies across the entire sequence. Additionally, since the NeuroGym tasks are 100 sequence steps long, we set <inline-formula><mml:math id="inf556"><mml:mrow><mml:msup><mml:mi mathsize="90%">λ</mml:mi><mml:mtext mathsize="90%">max</mml:mtext></mml:msup><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0.99</mml:mn></mml:mrow></mml:math></inline-formula> for the MPN and MPNpre.</p></sec><sec id="s4-8-8"><title><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> details</title><p>All subplots in this figure were generated in an analogous manner to those in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><p><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a and b</xref> are the same as <xref ref-type="fig" rid="fig3">Figure 3b and c</xref>, except the network is a GRU instead of a Vanilla RNN. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c and d</xref> are the go signal projection, see <xref ref-type="disp-formula" rid="equ17">Equation 14</xref>, of the <inline-formula><mml:math id="inf557"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> states shown in <xref ref-type="fig" rid="fig3">Figure 3g and h</xref>. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c</xref> has the states colored by accumulated evidence, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1d</xref> has the states colored by present input. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1e-h</xref> are the same as those in <xref ref-type="fig" rid="fig3">Figure 3[e–h]</xref>, but for the MPNpre. That is, the SM matrix is updated using only the presynaptic dependent expression in <xref ref-type="disp-formula" rid="equ16">Equation 13</xref> instead of <xref ref-type="disp-formula" rid="equ15">Equation 12</xref>. Finally, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1i-l</xref> are also the same as those in <xref ref-type="fig" rid="fig3">Figure 3[e–h]</xref>, except the MPN has <inline-formula><mml:math id="inf558"><mml:mrow><mml:mrow><mml:mi mathsize="90%">ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo mathsize="90%" stretchy="false">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mtext mathsize="90%">max</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn mathsize="90%">0</mml:mn><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mo mathsize="90%" stretchy="false">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, that is the ReLU activation function.</p></sec><sec id="s4-8-9"><title><xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> details</title><p><xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a-d</xref> are the same as those in <xref ref-type="fig" rid="fig3">Figure 3[e–h]</xref>, but for the additive MPN of <xref ref-type="disp-formula" rid="equ18">Equation 15</xref>. See Methods 5.4.5 for a discussion of the results of introducing bounded modulations. The accuracy results and <xref ref-type="fig" rid="fig3s2 fig3s3">Figure 3—figure supplements 2e-g</xref> were averaged across 10 separate initializations. Plots <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2h, i, j</xref> are generated in an analogous manner to those in <xref ref-type="fig" rid="fig3">Figure 3e, g and h</xref>.</p></sec><sec id="s4-8-10"><title><xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> details</title><p>All subplots in this figure were generated in an analogous manner to those in <xref ref-type="fig" rid="fig3">Figure 3</xref>, but for the case of <inline-formula><mml:math id="inf559"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">3</mml:mn></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf560"><mml:mrow><mml:mi mathsize="90%">N</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">2</mml:mn></mml:mrow></mml:math></inline-formula>. Hidden activity and SM matrix activity that were previously colored by accumulated evidence are now colored by example label instead.</p></sec><sec id="s4-8-11"><title><xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> details</title><p>In <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1[a-c]</xref>, we plot several measures of the MPN and GRU trained on the same integration-delay explored in <xref ref-type="fig" rid="fig5">Figure 5</xref>. For the MPN, we vary the magnitude of the delay input, which throughout the main text was set to 0. The mean hidden activity of <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref> is given by <inline-formula><mml:math id="inf561"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="90%">h</mml:mi><mml:mo mathsize="90%" stretchy="false">¯</mml:mo></mml:mover><mml:mi mathsize="90%">t</mml:mi></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mn mathsize="90%">1</mml:mn><mml:mi mathsize="90%">n</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" mathsize="90%" stretchy="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow><mml:mi mathsize="90%">n</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi mathsize="90%">h</mml:mi><mml:mrow><mml:mi mathsize="90%">i</mml:mi><mml:mo mathsize="90%" stretchy="false">,</mml:mo><mml:mi mathsize="90%">t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b</xref>, we again plot the decoding accuracy trained on the hidden activity, see description of <xref ref-type="fig" rid="fig5">Figure 5b</xref> above. Notably, decoding accuracy during the delay period increases slightly with larger delay input. In <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c</xref>, we plot the time-variation over a rolling time window, normalized relative to the size of activity. The normalized time variability is computed over a <inline-formula><mml:math id="inf562"><mml:mi mathsize="90%">τ</mml:mi></mml:math></inline-formula>-time step rolling window. Specifically, for each neuron it is given by<disp-formula id="equ44"><label>(37)</label><mml:math id="m44"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The plot is then created by averaging this quantity over both batches and neurons for all time steps <inline-formula><mml:math id="inf563"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf564"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We see the MPN not only has significantly higher time-variation than the GRU during the delay period but also the stimulus period that preceded it as well.</p><p><xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref> shows the accuracy of analytical approximations, see description of <xref ref-type="fig" rid="fig5">Figure 5d</xref> above, this time as a function of sequence time. The accuracy of the <inline-formula><mml:math id="inf565"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> approximation gets worse as a function of time due to the compounding effects of the approximation over many time steps.</p><p><xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1e-g</xref> show the decoding accuracy as a function of decoder train and test time (all use the same color scale shown on the far right). The only difference from the calculation of decoding accuracy from <xref ref-type="fig" rid="fig5">Figure 5b</xref> is that the train and test times can be different in this setup. <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1e</xref> shows the data for a GRU, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1f</xref> for the MPN with zero delay input magnitude, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1g</xref> for the MPN with a delay magnitude of 0.05.</p></sec><sec id="s4-8-12"><title><xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> details</title><p>For plots <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a-e</xref>, the MPNpre and additive MPN was trained in an identical setting to the MPN and RNNs of <xref ref-type="fig" rid="fig6">Figure 6[a–e]</xref>. For the noise plot, the additive MPN was not able to achieve the same accuracy thresholds of the other networks, so training was capped at 16,000 batches. A similar iteration threshold was used for catastrophic forgetting as the networks also occasionally struggled to learn the delay task for certain initializations. For the catastrophic forgetting tasks, the MPNpre had considerable difficulty achieving the 97% accuracy threshold and in some cases reached the maximum training step count (100,000 training samples) without reaching this accuracy threshold. This appears to be an issue with credit assignment in such setups, since it barely achieves better than chance accuracy. In such cases, we omitted these results from the the average behavior, since all other network results were conditioned on reaching the accuracy threshold on both tasks. The MPNpre’s worse behavior relative to the MPN on integration tasks with delays was also observed <xref ref-type="fig" rid="fig6">Figure 6h</xref>. We leave an investigation into such effects for future work. <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1j</xref> was generated analogously to <xref ref-type="fig" rid="fig7">Figure 7j</xref> across 10 initializations of the MPN and MPNpre.</p></sec><sec id="s4-8-13"><title><xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> details</title><p>Details of the true-anti context task can be found in Sec. 5.1 above. In <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1b and c</xref>, we color the SM matrix/hidden activity by both label and context, with the darker colors corresponding to the ‘anti-’ context. <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1b</xref> is the PC projection of the SM matrix over all sequence time steps. <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1c</xref> is the PC projection of only the final hidden activity.</p><p>The prospective task integration task shown in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1[d-g]</xref> is identical to the retrospective one of the main text and shown in <xref ref-type="fig" rid="fig7">Figure 7[a–d]</xref>, except that the context comes <italic>before</italic> the evidence sequence (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1d</xref>). Again looking to the MPN’s <inline-formula><mml:math id="inf566"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> dynamics, we see the context quickly separates the evidence-less states into two separate clusters (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1e</xref>). Soon after, once evidence starts being passed to the network, the clusters remain distinct and evidence for the individual integration subtasks is accumulated (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1f</xref>). Once again quantifying the information contained in <inline-formula><mml:math id="inf567"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> via the readout difference, we see the subtasks that match the context have their readout difference grow quickly (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1g</xref>). Meanwhile, since the context has already been processed by the network, the evidence needed to solve the irrelevant subtask is stored in a way that is unimportant for the readouts.</p><p><xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1h and i</xref> quantifies how the accumulated evidence direction aligns with the readout vectors for both the retrospective and prospective contextual integration tasks. The accumulated evidence direction is computed for each subtask and context combination, so four lines are shown in each subplot. At each time step, for each possible subtask/context combination, the SM matrix states are sorted by the amount of accumulated evidence they encode (which takes on a finite number of integer values). All states that have the same accumulated evidence are averaged together, yielding a set of average SM matrix states for all possible accumulated evidence scores at that time step and subtask/context combination. For a given subtask/context combination, the average SM states are mapped to hidden activity using the go signal projection, <xref ref-type="disp-formula" rid="equ17">Equation 14</xref>, then said hidden activities are fit using PCA, with the top PC direction taken to be the direction of accumulated evidence variation in the hidden activity space. The readout alignment is then the average cosine angle of the top PC direction with the two readout vectors (taking the maximum of the two possible directions PC1 could point to eliminate ambiguity). Thus, higher readout aligns mean the direction of accumulated evidence variation for the subtask is more aligned with the readout vectors, indicating that the readout vectors are better at distinguishing evidence for the particular subtask. Notably, once context is passed, the irrelevant subtask’s accumulated evidence variation becomes close to perpendicular to the readouts, since its accumulated evidence is no longer relevant for the task. Finally, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1i</xref> shows the readout alignment only for time steps after stimulus inputs are passed to the network, since the accumulated evidence is always zero prior to said inputs being passed to the network and thus the accumulated evidence variational direction is ill-defined.</p><p><xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1j</xref> simply shows a closeup of the continuous integration hidden activity shown in <xref ref-type="fig" rid="fig7">Figure 7j</xref>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83035-mdarchecklist1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modeling code is available on GitHub at: <ext-link ext-link-type="uri" xlink:href="https://github.com/kaitken17/mpn">https://github.com/kaitken17/mpn</ext-link> (copy archived at (<xref ref-type="bibr" rid="bib2">Aitken and Mihalas, 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Kayvon Daie, Lukasz Kusmierz, Niru Maheswaranathan, Aaron Milstein, Danil Tyulmankov for feedback on this paper. Stefan Mihalas has been in part supported by NIH grants RF1DA055669 and R01EB029813. We also wish to thank the Allen Institute for Brain Science founder, Paul G Allen, for his vision, encouragement, and support.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>K</given-names></name><name><surname>Ramasesh</surname><given-names>VV</given-names></name><name><surname>Garg</surname><given-names>A</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Geometry of Integration in Text Classification Rnns</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.15114">https://arxiv.org/abs/2010.15114</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>K</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Supporting code for multi-plasticity networks</data-title><version designator="swh:1:rev:6353f7f482eb181bbdce7deaae70e4be6972f35e">swh:1:rev:6353f7f482eb181bbdce7deaae70e4be6972f35e</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7007c96f5ccdb4f494d8a3e24538c873ccf9b91a;origin=https://github.com/kaitken17/mpn;visit=swh:1:snp:0a26db931702c72f0aed8b4f4f4bd4fb6a2756da;anchor=swh:1:rev:6353f7f482eb181bbdce7deaae70e4be6972f35e">https://archive.softwareheritage.org/swh:1:dir:7007c96f5ccdb4f494d8a3e24538c873ccf9b91a;origin=https://github.com/kaitken17/mpn;visit=swh:1:snp:0a26db931702c72f0aed8b4f4f4bd4fb6a2756da;anchor=swh:1:rev:6353f7f482eb181bbdce7deaae70e4be6972f35e</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname><given-names>D</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>Journal of Cerebral Blood Flow and Metabolism</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id><pub-id pub-id-type="pmid">11598490</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ba</surname><given-names>J</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Ionescu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using Fast Weights to Attend to the Recent Past</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1610.06258">https://arxiv.org/abs/1610.06258</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bailey</surname><given-names>CH</given-names></name><name><surname>Kandel</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Structural changes accompanying memory storage</article-title><source>Annual Review of Physiology</source><volume>55</volume><fpage>397</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1146/annurev.ph.55.030193.002145</pub-id><pub-id pub-id-type="pmid">8466181</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballintyn</surname><given-names>B</given-names></name><name><surname>Shlaer</surname><given-names>B</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spatiotemporal discrimination in attractor networks with short-term synaptic plasticity</article-title><source>Journal of Computational Neuroscience</source><volume>46</volume><fpage>279</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1007/s10827-019-00717-5</pub-id><pub-id pub-id-type="pmid">31134433</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baltaci</surname><given-names>SB</given-names></name><name><surname>Mogulkoc</surname><given-names>R</given-names></name><name><surname>Baltaci</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Molecular mechanisms of early and late LTP</article-title><source>Neurochemical Research</source><volume>44</volume><fpage>281</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1007/s11064-018-2695-4</pub-id><pub-id pub-id-type="pmid">30523578</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuronal population coding of parametric working memory</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>9424</fpage><lpage>9430</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1875-10.2010</pub-id><pub-id pub-id-type="pmid">20631171</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Working models of working memory</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>20</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.10.008</pub-id><pub-id pub-id-type="pmid">24709596</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>MFP</given-names></name><name><surname>Tetzlaff</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The biophysical basis underlying the maintenance of early phase long-term potentiation</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008813</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008813</pub-id><pub-id pub-id-type="pmid">33750943</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname><given-names>R</given-names></name><name><surname>Bar-Or</surname><given-names>RL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Theory of orientation tuning in visual cortex</article-title><source>PNAS</source><volume>92</volume><fpage>3844</fpage><lpage>3848</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id><pub-id pub-id-type="pmid">7731993</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertram</surname><given-names>R</given-names></name><name><surname>Sherman</surname><given-names>A</given-names></name><name><surname>Stanley</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Single-domain/bound calcium hypothesis of transmitter release and facilitation</article-title><source>Journal of Neurophysiology</source><volume>75</volume><fpage>1919</fpage><lpage>1931</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.75.5.1919</pub-id><pub-id pub-id-type="pmid">8734591</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>GQ</given-names></name><name><surname>Poo</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>10464</fpage><lpage>10472</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-24-10464.1998</pub-id><pub-id pub-id-type="pmid">9852584</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id><pub-id pub-id-type="pmid">19229307</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Burnham</surname><given-names>D</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning to Predict in Networks with Heterogeneous and Dynamic Synapses</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.05.18.444107</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campagnola</surname><given-names>L</given-names></name><name><surname>Seeman</surname><given-names>SC</given-names></name><name><surname>Chartrand</surname><given-names>T</given-names></name><name><surname>Kim</surname><given-names>L</given-names></name><name><surname>Hoggarth</surname><given-names>A</given-names></name><name><surname>Gamlin</surname><given-names>C</given-names></name><name><surname>Ito</surname><given-names>S</given-names></name><name><surname>Trinh</surname><given-names>J</given-names></name><name><surname>Davoudian</surname><given-names>P</given-names></name><name><surname>Radaelli</surname><given-names>C</given-names></name><name><surname>Kim</surname><given-names>M-H</given-names></name><name><surname>Hage</surname><given-names>T</given-names></name><name><surname>Braun</surname><given-names>T</given-names></name><name><surname>Alfiler</surname><given-names>L</given-names></name><name><surname>Andrade</surname><given-names>J</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Dalley</surname><given-names>R</given-names></name><name><surname>Henry</surname><given-names>A</given-names></name><name><surname>Kebede</surname><given-names>S</given-names></name><name><surname>Alice</surname><given-names>M</given-names></name><name><surname>Sandman</surname><given-names>D</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Teeter</surname><given-names>C</given-names></name><name><surname>Daigle</surname><given-names>TL</given-names></name><name><surname>Berry</surname><given-names>K</given-names></name><name><surname>Dotson</surname><given-names>N</given-names></name><name><surname>Enstrom</surname><given-names>R</given-names></name><name><surname>Gorham</surname><given-names>M</given-names></name><name><surname>Hupp</surname><given-names>M</given-names></name><name><surname>Dingman Lee</surname><given-names>S</given-names></name><name><surname>Ngo</surname><given-names>K</given-names></name><name><surname>Nicovich</surname><given-names>PR</given-names></name><name><surname>Potekhina</surname><given-names>L</given-names></name><name><surname>Ransford</surname><given-names>S</given-names></name><name><surname>Gary</surname><given-names>A</given-names></name><name><surname>Goldy</surname><given-names>J</given-names></name><name><surname>McMillen</surname><given-names>D</given-names></name><name><surname>Pham</surname><given-names>T</given-names></name><name><surname>Tieu</surname><given-names>M</given-names></name><name><surname>Siverts</surname><given-names>L</given-names></name><name><surname>Walker</surname><given-names>M</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Schroedter</surname><given-names>M</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Cobb</surname><given-names>C</given-names></name><name><surname>Ellenbogen</surname><given-names>R</given-names></name><name><surname>Gwinn</surname><given-names>RP</given-names></name><name><surname>Keene</surname><given-names>CD</given-names></name><name><surname>Ko</surname><given-names>AL</given-names></name><name><surname>Ojemann</surname><given-names>JG</given-names></name><name><surname>Silbergeld</surname><given-names>DL</given-names></name><name><surname>Carey</surname><given-names>D</given-names></name><name><surname>Casper</surname><given-names>T</given-names></name><name><surname>Crichton</surname><given-names>K</given-names></name><name><surname>Clark</surname><given-names>M</given-names></name><name><surname>Dee</surname><given-names>N</given-names></name><name><surname>Ellingwood</surname><given-names>L</given-names></name><name><surname>Gloe</surname><given-names>J</given-names></name><name><surname>Kroll</surname><given-names>M</given-names></name><name><surname>Sulc</surname><given-names>J</given-names></name><name><surname>Tung</surname><given-names>H</given-names></name><name><surname>Wadhwani</surname><given-names>K</given-names></name><name><surname>Brouner</surname><given-names>K</given-names></name><name><surname>Egdorf</surname><given-names>T</given-names></name><name><surname>Maxwell</surname><given-names>M</given-names></name><name><surname>McGraw</surname><given-names>M</given-names></name><name><surname>Pom</surname><given-names>CA</given-names></name><name><surname>Ruiz</surname><given-names>A</given-names></name><name><surname>Bomben</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Hejazinia</surname><given-names>N</given-names></name><name><surname>Shi</surname><given-names>S</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Phillips</surname><given-names>J</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Esposito</surname><given-names>L</given-names></name><name><surname>D’Orazi</surname><given-names>FD</given-names></name><name><surname>Sunkin</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Sorensen</surname><given-names>S</given-names></name><name><surname>Lein</surname><given-names>E</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Murphy</surname><given-names>G</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Local connectivity and synaptic dynamics in mouse and human neocortex</article-title><source>Science</source><volume>375</volume><elocation-id>eabj5861</elocation-id><pub-id pub-id-type="doi">10.1126/science.abj5861</pub-id><pub-id pub-id-type="pmid">35271334</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>SC</given-names></name><name><surname>Robinson</surname><given-names>DA</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>A proposed neural network for the integrator of the oculomotor system</article-title><source>Biol Cybern</source><volume>49</volume><fpage>127</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1007/BF00320393</pub-id><pub-id pub-id-type="pmid">6661444</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Kemp</surname><given-names>N</given-names></name><name><surname>Noel</surname><given-names>J</given-names></name><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Brown</surname><given-names>MW</given-names></name><name><surname>Bashir</surname><given-names>ZI</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A new form of long-term depression in the perirhinal cortex</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>150</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1038/72093</pub-id><pub-id pub-id-type="pmid">10649570</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>van Merrienboer</surname><given-names>B</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Bougares</surname><given-names>F</given-names></name><name><surname>Schwenk</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</article-title><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP</conf-name><pub-id pub-id-type="doi">10.3115/v1/D14-1179</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Capacity and Trainability in Recurrent Neural Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1611.09913">https://arxiv.org/abs/1611.09913</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>de Sá</surname><given-names>JM</given-names></name><name><surname>Alexandre</surname><given-names>LA</given-names></name><name><surname>Duch</surname><given-names>W</given-names></name><name><surname>Mandic</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Artificial Neural Networks – Icann 2007</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-540-74690-4</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duncker</surname><given-names>L</given-names></name><name><surname>Driscoll</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Organizing recurrent network dynamics by task-computation to enable continual learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>14387</fpage><lpage>14397</lpage></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ermentrout</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural networks as spatio-temporal pattern-forming systems</article-title><source>Reports on Progress in Physics</source><volume>61</volume><fpage>353</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1088/0034-4885/61/4/002</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Catastrophic forgetting in connectionist networks</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>128</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(99)01294-2</pub-id><pub-id pub-id-type="pmid">10322466</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhs</surname><given-names>MC</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A spin glass model of path integration in rat medial entorhinal cortex</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>4266</fpage><lpage>4276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4353-05.2006</pub-id><pub-id pub-id-type="pmid">16624947</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>The Organization of Behavior</source><publisher-name>Psychology Press</publisher-name><pub-id pub-id-type="doi">10.4324/9781410612403</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herring</surname><given-names>BE</given-names></name><name><surname>Nicoll</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Long-Term potentiation: from CaMKII to AMPA receptor trafficking</article-title><source>Annual Review of Physiology</source><volume>78</volume><fpage>351</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1146/annurev-physiol-021014-071753</pub-id><pub-id pub-id-type="pmid">26863325</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>B</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ollerenshaw</surname><given-names>DR</given-names></name><name><surname>Shang</surname><given-names>J</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Manavi</surname><given-names>S</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Adaptation supports short-term memory in a visual change detection task</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009246</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009246</pub-id><pub-id pub-id-type="pmid">34534203</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>H</given-names></name><name><surname>Haas</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication</article-title><source>Science</source><volume>304</volume><fpage>78</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1126/science.1091277</pub-id><pub-id pub-id-type="pmid">15064413</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A method for stochastic optimization</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>J</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Rabinowitz</surname><given-names>N</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Desjardins</surname><given-names>G</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Milan</surname><given-names>K</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Ramalho</surname><given-names>T</given-names></name><name><surname>Grabska-Barwinska</surname><given-names>A</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hadsell</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Overcoming catastrophic forgetting in neural networks</article-title><source>PNAS</source><volume>114</volume><fpage>3521</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1073/pnas.1611835114</pub-id><pub-id pub-id-type="pmid">28292907</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What learning systems do intelligent agents need? complementary learning systems theory updated</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>512</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.05.004</pub-id><pub-id pub-id-type="pmid">27315762</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SJR</given-names></name><name><surname>Escobedo-Lozoya</surname><given-names>Y</given-names></name><name><surname>Szatmari</surname><given-names>EM</given-names></name><name><surname>Yasuda</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Activation of CaMKII in single dendritic spines during long-term potentiation</article-title><source>Nature</source><volume>458</volume><fpage>299</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1038/nature07842</pub-id><pub-id pub-id-type="pmid">19295602</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Marris</surname><given-names>L</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Backpropagation and the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>335</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id><pub-id pub-id-type="pmid">32303713</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lledo</surname><given-names>PM</given-names></name><name><surname>Hjelmstad</surname><given-names>GO</given-names></name><name><surname>Mukherji</surname><given-names>S</given-names></name><name><surname>Soderling</surname><given-names>TR</given-names></name><name><surname>Malenka</surname><given-names>RC</given-names></name><name><surname>Nicoll</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Calcium/calmodulin-dependent kinase II and long-term potentiation enhance synaptic transmission by the same mechanism</article-title><source>PNAS</source><volume>92</volume><fpage>11175</fpage><lpage>11179</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.24.11175</pub-id><pub-id pub-id-type="pmid">7479960</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Love</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>DEGENERATION and regeneration in the nervous system</article-title><source>Brain</source><volume>126</volume><fpage>1009</fpage><lpage>1011</lpage><pub-id pub-id-type="doi">10.1093/brain/awg078</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukoševičius</surname><given-names>M</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Computer Science Review</source><volume>3</volume><fpage>127</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Herman</surname><given-names>P</given-names></name><name><surname>Lansner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Theta and gamma power increases and alpha/beta power decreases with memory load in an attractor network model</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>3008</fpage><lpage>3020</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00029</pub-id><pub-id pub-id-type="pmid">21452933</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Herman</surname><given-names>P</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Working memory: delay activity, Yes! persistent activity? maybe not</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7013</fpage><lpage>7019</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2485-17.2018</pub-id><pub-id pub-id-type="pmid">30089640</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-Time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics</article-title><source>Advances in Neural Information Processing Systems</source><volume>32</volume><fpage>15696</fpage><lpage>15705</lpage><pub-id pub-id-type="pmid">32782423</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Williams</surname><given-names>A</given-names></name><name><surname>Golub</surname><given-names>M</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Universality and Individuality in Neural Dynamics across Large Populations of Recurrent Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.08549">https://arxiv.org/abs/1907.08549</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How Recurrent Networks Implement Contextual Processing in Sentiment Analysis</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/2004.08013.pdf">https://arxiv.org/pdf/2004.08013.pdf</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-Dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Redistribution of synaptic efficacy between neocortical pyramidal neurons</article-title><source>Nature</source><volume>382</volume><fpage>807</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1038/382807a0</pub-id><pub-id pub-id-type="pmid">8752273</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Lübke</surname><given-names>J</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APS and EPSPs</article-title><source>Science</source><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Circuit mechanisms for the maintenance and manipulation of information in working memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0414-3</pub-id><pub-id pub-id-type="pmid">31182866</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCloskey</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Catastrophic interference in connectionist networks: the sequential learning problem</article-title><source>Psychology of Learning and Motivation</source><volume>24</volume><fpage>109</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/S0079-7421(08)60536-8</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McFarlan</surname><given-names>AR</given-names></name><name><surname>Chou</surname><given-names>CYC</given-names></name><name><surname>Watanabe</surname><given-names>A</given-names></name><name><surname>Cherepacha</surname><given-names>N</given-names></name><name><surname>Haddad</surname><given-names>M</given-names></name><name><surname>Owens</surname><given-names>H</given-names></name><name><surname>Sjöström</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The plasticitome of cortical interneurons</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>80</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/s41583-022-00663-9</pub-id><pub-id pub-id-type="pmid">36585520</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Molano-Mazon</surname><given-names>M</given-names></name><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Pastor-Ciurana</surname><given-names>J</given-names></name><name><surname>Fradera</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>R-Y</given-names></name><name><surname>Forest</surname><given-names>J</given-names></name><name><surname>del Pozo Lerida</surname><given-names>J</given-names></name><name><surname>Ji-An</surname><given-names>L</given-names></name><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>NeuroGym: An Open Resource for Developing and Sharing Neuroscience Tasks</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/aqc9n</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname><given-names>G</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orhan</surname><given-names>AE</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A diverse range of factors affect the nature of neural representations underlying short-term memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>275</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0314-y</pub-id><pub-id pub-id-type="pmid">30664767</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shared mechanisms underlie the control of working memory and attention</article-title><source>Nature</source><volume>592</volume><fpage>601</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id><pub-id pub-id-type="pmid">33790467</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>MA</given-names></name><name><surname>Szatmari</surname><given-names>EM</given-names></name><name><surname>Yasuda</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ampa receptors are exocytosed in stimulated spines and adjacent dendrites in a ras-erk-dependent manner during long-term potentiation</article-title><source>PNAS</source><volume>107</volume><fpage>15951</fpage><lpage>15956</lpage><pub-id pub-id-type="doi">10.1073/pnas.0913875107</pub-id><pub-id pub-id-type="pmid">20733080</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pettit</surname><given-names>DL</given-names></name><name><surname>Perlman</surname><given-names>S</given-names></name><name><surname>Malinow</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Potentiated transmission and prevention of further LTP by increased CaMKII activity in postsynaptic hippocampal slice neurons</article-title><source>Science</source><volume>266</volume><fpage>1881</fpage><lpage>1885</lpage><pub-id pub-id-type="doi">10.1126/science.7997883</pub-id><pub-id pub-id-type="pmid">7997883</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions</article-title><source>Psychological Review</source><volume>97</volume><fpage>285</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.97.2.285</pub-id><pub-id pub-id-type="pmid">2186426</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robins</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Catastrophic forgetting, rehearsal and pseudorehearsal</article-title><source>Connection Science</source><volume>7</volume><fpage>123</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1080/09540099550039318</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rodriguez</surname><given-names>HG</given-names></name><name><surname>Guo</surname><given-names>Q</given-names></name><name><surname>Moraitis</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Short-term plasticity neurons learning to learn and forget</article-title><conf-name>In International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>How the brain keeps the eyes still</article-title><source>PNAS</source><volume>93</volume><fpage>13339</fpage><lpage>13344</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.23.13339</pub-id><pub-id pub-id-type="pmid">8917592</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname><given-names>AJ</given-names></name><name><surname>Stevens</surname><given-names>CF</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Deficient hippocampal long-term potentiation in alpha-calcium-calmodulin kinase II mutant mice</article-title><source>Science</source><volume>257</volume><fpage>201</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1126/science.1378648</pub-id><pub-id pub-id-type="pmid">1378648</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cooperative switch determines the sign of synaptic plasticity in distal dendrites of neocortical pyramidal neurons</article-title><source>Neuron</source><volume>51</volume><fpage>227</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.017</pub-id><pub-id pub-id-type="pmid">16846857</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>CF</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Facilitation and depression at single central synapses</article-title><source>Neuron</source><volume>14</volume><fpage>795</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1016/0896-6273(95)90223-6</pub-id><pub-id pub-id-type="pmid">7718241</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>“ Activity-silent ” working memory in prefrontal cortex: a dynamic coding framework</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>394</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id><pub-id pub-id-type="pmid">26051384</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>SM</given-names></name><name><surname>Trappenberg</surname><given-names>TP</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>de Araujo</surname><given-names>IET</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Self-organizing continuous attractor networks and path integration: one-dimensional models of head direction cells</article-title><source>Network</source><volume>13</volume><fpage>217</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1080/net.13.2.217.242</pub-id><pub-id pub-id-type="pmid">12061421</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugase-Miyamoto</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Wiener</surname><given-names>MC</given-names></name><name><surname>Optican</surname><given-names>LM</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Short-term memory trace in rapidly adapting synapses of inferior temporal cortex</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000073</pub-id><pub-id pub-id-type="pmid">18464917</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>MV</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability</article-title><source>PNAS</source><volume>94</volume><fpage>719</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.2.719</pub-id><pub-id pub-id-type="pmid">9012851</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Pawelzik</surname><given-names>K</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural networks with dynamic synapses</article-title><source>Neural Computation</source><volume>10</volume><fpage>821</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1162/089976698300017502</pub-id><pub-id pub-id-type="pmid">9573407</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyulmankov</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Meta-learning synaptic plasticity and memory addressing for continual familiarity detection</article-title><source>Neuron</source><volume>110</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.11.009</pub-id><pub-id pub-id-type="pmid">34861149</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname><given-names>S</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computation through neural population dynamics</article-title><source>Annual Review of Neuroscience</source><volume>43</volume><fpage>249</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-092619-094115</pub-id><pub-id pub-id-type="pmid">32640928</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Double-Ring network model of the head-direction system</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>66</volume><elocation-id>041902</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.66.041902</pub-id><pub-id pub-id-type="pmid">12443230</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0310-2</pub-id><pub-id pub-id-type="pmid">30643294</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-06-02112.1996</pub-id><pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zucker</surname><given-names>RS</given-names></name><name><surname>Regehr</surname><given-names>WG</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Short-Term synaptic plasticity</article-title><source>Annual Review of Physiology</source><volume>64</volume><fpage>355</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1146/annurev.physiol.64.092501.114547</pub-id><pub-id pub-id-type="pmid">11826273</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Analytical Approximations</title><sec sec-type="appendix" id="s8-1"><title>Hadamard identity</title><p>To start, note the following Hadamard identity for matrices composed of outer products. Below, we often have multiplications of the form <inline-formula><mml:math id="inf568"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Writing out the elements of this product explicitly, we have<disp-formula id="equ45"><label>(38)</label><mml:math id="m45"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>and thus, in matrix notation we have<disp-formula id="equ46"><label>(39)</label><mml:math id="m46"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In words: we can effectively ‘move the <inline-formula><mml:math id="inf569"><mml:mi mathsize="90%" mathvariant="bold">x</mml:mi></mml:math></inline-formula> through the <inline-formula><mml:math id="inf570"><mml:mi mathsize="90%" mathvariant="bold">W</mml:mi></mml:math></inline-formula>’.</p><p>Approximation for <inline-formula><mml:math id="inf571"><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>We begin by deriving an expression for <inline-formula><mml:math id="inf572"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in terms of all previous inputs, <inline-formula><mml:math id="inf573"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">x</mml:mi><mml:mi mathsize="90%">s</mml:mi></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="inf574"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the initial state <inline-formula><mml:math id="inf575"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Explicitly expanding the expression from our update equations, we have<disp-formula id="equ47"><label>(40)</label><mml:math id="m47"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>λ</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where, in reaching the last line, we have used the above Hadamard identity, <xref ref-type="disp-formula" rid="equ46">Equation 39</xref>. So far, this expression is exact, but is clearly becoming a mess quite quickly. We can see additional steps backward in time will make this expression even more complicated, so we look for some approximation.</p><p>After training the MPN, we find <inline-formula><mml:math id="inf576"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf577"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Additionally, <inline-formula><mml:math id="inf578"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, so passing the input vectors through the input layer does not significantly change their magnitude. In the last line of <xref ref-type="disp-formula" rid="equ47">Equation 40</xref>, note that we have a term of the form <inline-formula><mml:math id="inf579"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. As noted in the main text, for <inline-formula><mml:math id="inf580"><mml:mrow><mml:mi mathsize="90%">d</mml:mi><mml:mo mathsize="90%" stretchy="false">≫</mml:mo><mml:mn mathsize="90%">1</mml:mn></mml:mrow></mml:math></inline-formula>, these terms are small relative to terms with just <inline-formula><mml:math id="inf581"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, see <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>. Additionally, this term also has a Hadamard product in the hidden activity space. If we drop the <inline-formula><mml:math id="inf582"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> term since we know it will be small, and continue on this way, we arrive at<disp-formula id="equ48"><label>(41)</label><mml:math id="m48"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>λ</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≈</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>λ</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≈</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where in the each successive line we have dropped any <inline-formula><mml:math id="inf583"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> terms. Finally, taking <inline-formula><mml:math id="inf584"><mml:mrow><mml:msub><mml:mi mathsize="90%" mathvariant="bold">M</mml:mi><mml:mn mathsize="90%">0</mml:mn></mml:msub><mml:mo mathsize="90%" stretchy="false">=</mml:mo><mml:mn mathsize="90%">0</mml:mn></mml:mrow></mml:math></inline-formula>, we arrive at the expression in the main text.</p><p>Approximation for <inline-formula><mml:math id="inf585"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Again our goal is to write <inline-formula><mml:math id="inf586"><mml:msub><mml:mi mathsize="90%" mathvariant="bold">h</mml:mi><mml:mi mathsize="90%">t</mml:mi></mml:msub></mml:math></inline-formula> in terms of all previous inputs<disp-formula id="equ49"><label>(42)</label><mml:math id="m49"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where in the second line we have used the expansion for <inline-formula><mml:math id="inf587"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ26">Equation 19</xref>, and in the final line we have used <xref ref-type="disp-formula" rid="equ46">Equation 39</xref>. Again, note that we have many terms that contain a <inline-formula><mml:math id="inf588"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> factor and thus are small relative to any terms with just <inline-formula><mml:math id="inf589"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. All <inline-formula><mml:math id="inf590"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> dependence comes with a <inline-formula><mml:math id="inf591"><mml:mrow><mml:mi mathsize="90%" mathvariant="bold">x</mml:mi><mml:mo mathsize="90%" stretchy="false">⊙</mml:mo><mml:msup><mml:mi mathsize="90%" mathvariant="bold">x</mml:mi><mml:mo mathsize="90%" stretchy="false">′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> factor, and thus is already a sub-leading dependence. The leading-order approximation for a generic <inline-formula><mml:math id="inf592"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is then<disp-formula id="equ50"><label>(43)</label><mml:math id="m50"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where in the second line we have assumed <inline-formula><mml:math id="inf593"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Inserting this approximation into <xref ref-type="disp-formula" rid="equ49">Equation 42</xref> and taking <inline-formula><mml:math id="inf594"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we have<disp-formula id="equ51"><label>(44)</label><mml:math id="m51"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≈</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊙</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Note the leading-order contribution to a given <inline-formula><mml:math id="inf595"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is only dependent upon the current input, <inline-formula><mml:math id="inf596"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Any dependence on previous inputs only comes in at sub-leading order.</p><p>Above, our arguments relied on the fact that the weight modulations are small as a result of the Hadamard product between vectors with components <inline-formula><mml:math id="inf597"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. One could ask if the same dynamics are observed if the vector components were no longer <inline-formula><mml:math id="inf598"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. To test this, we trained the network with inputs such that <inline-formula><mml:math id="inf599"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e. input vectors with components of size <inline-formula><mml:math id="inf600"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In this setting, we again found that the network found a solution such that terms from weight modulations were small relative to unmodulated contributions. This again resulted in hidden activity dynamics that were primarily driven by the most recent input to the network. We leave a further analysis of MPNs trained in settings that might increase the relative size of weight modulations for future work.</p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83035.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mongillo</surname><given-names>Gianluigi</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f82e368</institution-id><institution>Université Paris Descartes</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.06.27.497776" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.27.497776"/></front-stub><body><p>The study shows that fast and transient modifications of the synaptic efficacies, alone, can support the storage and processing of information over time. Convincing evidence is provided by showing that feed-forward networks, when equipped with such short-term synaptic modulations, perform a wide variety of tasks at a performance level comparable with that of recurrent networks. The results of the study are valuable to both neuroscientists and researchers in machine learning.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83035.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mongillo</surname><given-names>Gianluigi</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f82e368</institution-id><institution>Université Paris Descartes</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Barak</surname><given-names>Omri</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03qryx823</institution-id><institution>Technion- Israel Institute of Technology</institution></institution-wrap><country>Israel</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.27.497776">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.06.27.497776v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neural Population Dynamics of Computing with Synaptic Modulations&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Omri Barak (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The key points consistently raised by all the reviewers in their reports are the following:</p><p>1) The main motivation of the study is unclear. Is the study proposing a specific synaptic plasticity mechanism or the goal is to expose the computational advantage(s) of transient, slow-decaying synaptic modulation? In either case, the modeling choices should be motivated more explicitly and, where possible, the link with the existing experimental observations should be made more precise.</p><p>2) It is important to test the network's performance on at least one non-integration task.</p><p>3) The most biologically-implausible aspects of the synaptic rule, such as the unbounded growth of the amplitude of the synaptic modulations, should be removed. In this context, it would be useful to relate the synaptic plasticity rule to the biology more quantitatively, at least in terms of the underlying time scales and of the required magnitude of the changes in the efficacies.</p><p>4) The study should be better contextualized in the existing literature, in order to highlight the elements of novelty in the present approach and in the results.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) The Introductory paragraphs should be reworked to clarify what sort of plasticity mechanisms are being modeled.</p><p>2) Short-term plasticity mechanisms are often non-Hebbian, depending only on presynaptic activity. If I am correct that the specific plasticity rule used is understood to be a short-term Hebbian mechanism, for the authors' conclusions is it necessary that the short-term plasticity mechanism be Hebbian, or would more traditional STP mechanisms work?</p><p>3) Pg. 4, &quot;where λ and η are parameters learned over training&quot;. This adds an element of metaplasticity, in that one typically does not think that such synaptic hyperparameters are optimized during learning a task. Does this make the network less biologically realistic? Is the selection of these during training meant simply as a route to find effective hyperparameters (that apply across tasks) but not necessarily to imply that they are actually learned? And finally, is it even necessary to learn these parameters? The reservoir computing argument in Figure 6 suggests not, though in that case the input weights are not learned either. And if learning these parameters is important for good performance on single tasks then probably not completely accurate to call M unsupervised.</p><p>4) I wondered about the robustness of the mechanism by which the MPN performs the task-the readout is based on the variation within the Go cluster and thus seems like it needs to distinguish smaller differences. Does this also make the network more sensitive to noise?</p><p>5) Related to the above, the MPN state grows quite dramatically with time (&quot;several orders of magnitude&quot;). Does this growth affect the robustness of the readout and does the mechanism work if the short-term plasticity saturates (and saturation seems reasonable given that synaptic efficacies likely cannot change by several orders of magnitude especially on short timescales)? How important is this large dynamic range to the network capacity?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The paper is well-written and the figures are carefully chosen and informative.</p><p>As I mentioned in my <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.27.497776">Public Review</ext-link>, one would like to understand what are the non-obvious limitations (if any) of the general principle, that is, any transient, stimulus-dependent modification of neural/synaptic properties is a memory buffer. For instance, what happens if instead of using Equation (2), you use short-term synaptic plasticity a la Tsodyks-Markram with heterogeneous parameters (e.g., different U, tau_D, and tau_F), and you only learn the synaptic weights (STP parameters are fixed)? It seems to me that this will help clarify some interesting issues, such as, do you really need the synaptic modulation to be associative (i.e., to jointly depend on the pre- and post-synaptic activity)?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The paper could be strengthened by tasks that are not integration-based. This could be in the context of RNNs, but then it should be clear what is the added benefit of short-term plasticity.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83035.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The key points consistently raised by all the reviewers in their reports are the following:</p><p>1) The main motivation of the study is unclear. Is the study proposing a specific synaptic plasticity mechanism or the goal is to expose the computational advantage(s) of transient, slow-decaying synaptic modulation? In either case, the modeling choices should be motivated more explicitly and, where possible, the link with the existing experimental observations should be made more precise.</p></disp-quote><p>The goal of this work is to explore how well networks with transient slowly-decaying synaptic modulation perform, and if they compute in a similar manner to RNNs. One of our main findings is that networks with synaptic modulation compute in a qualitatively different manner than networks with recurrence. We have modified both the abstract and introduction to better highlight this goal. Since several reviewers commented on the potential biological relevance for modulation that is only presynaptic dependent, characteristic of short-term synaptic plasticity (STSP), we have extended our results to include a modulation mechanism that is only presynaptic dependent as well (see below for more details).</p><p>Biologically, the MPN's associative modulations represent a general synapse-specific change in strength that occurs at faster relative time scales to some slow weight adjustment that in the MPN is represented by training/backpropagation. Although we do not aim to model one specific biological mechanism, we have added an additional paragraph to the Discussion section describing an example where the MPNs various plasticities could potentially map to early/late phase LTP. Similarly, the newly added presynaptic-dependent modulations could represent STSP and the slow underlying weight changes could come from LTP/LTD.</p><disp-quote content-type="editor-comment"><p>2) It is important to test the network's performance on at least one non-integration task.</p></disp-quote><p>We have trained the MPN on 16 additional supervised learning tasks that are relevant to neuroscience and added these results to the main text (Sec. 3.3, Figure 7). A few examples are: contextual decision making, delay match sample, interval discrimination, motor timing, etc. This was done using the NeuroGym package (Molano-Mazon et al., 2022). Across all tasks we investigate, the MPN achieves comparable performance to its RNN counterparts and even outperforms them on certain tasks.</p><disp-quote content-type="editor-comment"><p>3) The most biologically-implausible aspects of the synaptic rule, such as the unbounded growth of the amplitude of the synaptic modulations, should be removed. In this context, it would be useful to relate the synaptic plasticity rule to the biology more quantitatively, at least in terms of the underlying time scales and of the required magnitude of the changes in the efficacies.</p></disp-quote><p>We note that although the modulations are not explicitly bounded, the constant decay of modulations effectively limits their size (see saturation for λ &lt; 1 in Figure 4c). Also note the saturation did not seem to affect the accuracy of the networks considerably (Figure 4d). Additionally, we found that effects of the modulations on the hidden activity were generally small compared to that of the fixed weights (see Equation (20), which yields analytical approximations shown in Figure 5c and 5d).</p><p>However, to be complete, we have now also tested how adding explicit bounds to the modulations affected the MPN’s performance and behavior. We have added these results to a footnote in the main text, additional figures in the supplement, and a short discussion in the methods section. In short, unless bounds are prohibitively restrictive (beyond what we believe is biologically reasonable), we do not find a significant change in either the performance or behavior of the network. For example, limiting the modulations to only change by 10% of the weight’s original value (quite a bit more restrictive than experiment, see below), we found only a few percentage drop in accuracy on integration tasks. When we introduced explicit bounds, we found that the network adapted its learning rate to compensate for the loss of information that could occur when a modulation could no longer be changed. That is, more restrictive bounds on modulations caused the network to use smaller learning rates (i.e. smaller eta) so that it did not run into the modulation bounds as quickly.</p><p>Biologically, STDP that potentiates synapses up to ~200% of their original values and depresses by up to ~30% of original values at timescales of 10s of minutes has been seen in a wide variety of studies in various parts of the brain (Sjöström and Häusser, 2006; McFarlan et al., 2022). Other specific studies sometimes find values that are a bit more extreme, for example depression as low as 20% in Cho, K. et al., 2000. Over shorter time scales, STSP has been shown to facilitate to a similar relative magnitude and seems to be capable of almost complete depression (Campagnola et al., 2022).</p><p>McFarlan, Amanda R., et al. &quot;The plasticitome of cortical interneurons.&quot; Nature Reviews Neuroscience (2022): 1-18.</p><p>Sjöström PJ, Häusser M. “A cooperative switch determines the sign of synaptic plasticity in distal dendrites of neocortical pyramidal neurons.” Neuron 51: 227–238, 2006.</p><p>Cho, K. et al. “A new form of long-term depression in the perirhinal cortex.” Nat. Neurosci. 3, 150–156, 2000.</p><disp-quote content-type="editor-comment"><p>4) The study should be better contextualized in the existing literature, in order to highlight the elements of novelty in the present approach and in the results.</p></disp-quote><p>We have added several comparisons to references suggested by the reviewers in the related work and Discussion sections as well as throughout the work where relevant.</p><p>As mentioned above, to address several reviewers’ inquiry into whether an associative mechanism was necessary (and to extend this work to model STSP-like mechanisms), we have introduced and analyzed a special case of the modulation updates that is only presynaptic dependent. That is, it is the same modulation rule as the MPN, except the hidden state dependence has been removed. We test this new modulation setting in an otherwise identical network/training setup as the MPN. We call this network the “MPNpre” and have added results throughout our paper analyzing the performance and behavior of this non-associative mechanism relative to the MPN. In short, due to the similarity of their operation, the MPNpre dynamics are largely the same as the MPN – dynamics are driven foremost by the current input and information about the history of the phrase is stored in the modulation. Additionally, for the majority of tasks we consider in this work (including the 16 additional tasks), the MPNpre performs at levels comparable to the MPN, often only slightly worse due to its simpler updates.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) The Introductory paragraphs should be reworked to clarify what sort of plasticity mechanisms are being modeled.</p></disp-quote><p>See <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.27.497776">our response</ext-link> to your public review.</p><disp-quote content-type="editor-comment"><p>2) Short-term plasticity mechanisms are often non-Hebbian, depending only on presynaptic activity. If I am correct that the specific plasticity rule used is understood to be a short-term Hebbian mechanism, for the authors' conclusions is it necessary that the short-term plasticity mechanism be Hebbian, or would more traditional STP mechanisms work?</p></disp-quote><p>We have added several results to our work for a modulation update rule only dependent upon presynaptic firing rates, see our response to Essential Revisions for more details.</p><disp-quote content-type="editor-comment"><p>3) Pg. 4, &quot;where λ and η are parameters learned over training&quot;. This adds an element of metaplasticity, in that one typically does not think that such synaptic hyperparameters are optimized during learning a task. Does this make the network less biologically realistic? Is the selection of these during training meant simply as a route to find effective hyperparameters (that apply across tasks) but not necessarily to imply that they are actually learned? And finally, is it even necessary to learn these parameters? The reservoir computing argument in Figure 6 suggests not, though in that case the input weights are not learned either. And if learning these parameters is important for good performance on single tasks then probably not completely accurate to call M unsupervised.</p></disp-quote><p>Indeed, this adds an element of meta-learning to the network setup. The ability for eta and λ to be adjustable mostly came from a desire to allow for a flexible learning rule and see if there were trends to what the network preferred in order to learn certain computations. After training the MPN across several types of tasks, we learned that allowing the network to adjust said parameters during learning was not absolutely necessary. In practice, we found that λ always saturated its upper bound (so that it retained information for as long as possible) and the adjustment of the rest of the weights in the network could compensate for choosing different fixed eta (both for magnitude and sign). For instance, for smaller eta, the smaller relative effects of modulations on the hidden activity could be magnified by the network learning larger readouts.</p><disp-quote content-type="editor-comment"><p>4) I wondered about the robustness of the mechanism by which the MPN performs the task-the readout is based on the variation within the Go cluster and thus seems like it needs to distinguish smaller differences. Does this also make the network more sensitive to noise?</p></disp-quote><p>We did find that the MPN was slightly less robust to noise than the RNN counterparts (Figure 6c), but we more attributed it to the lack of a task-dependent attractor structure that provides some robustness to perturbations away from the attractor. This difference in perturbation behavior would certainly manifest in the hidden activity of both networks. Note that the output neuron values are dependent upon both the readout vector size and the size of variations of the hidden activity, so although the encoding of the MPN’s accumulated evidence may vary more rapidly in hidden activity space than its RNN counterparts, it could be compensated with larger readouts. Notably, the network performs well even in the new tests we ran where modulation size was severely restricted, making the variation due to accumulated evidence even smaller (Figure 3-figure supplement 2h).</p><disp-quote content-type="editor-comment"><p>5) Related to the above, the MPN state grows quite dramatically with time (&quot;several orders of magnitude&quot;). Does this growth affect the robustness of the readout and does the mechanism work if the short-term plasticity saturates (and saturation seems reasonable given that synaptic efficacies likely cannot change by several orders of magnitude especially on short timescales)? How important is this large dynamic range to the network capacity?</p></disp-quote><p>This was a big question we had as well, and we believe the plots in Figures 4c and 4d support the fact that the saturation of the SM matrix does not seem to affect the network’s ability to store information significantly. Note that for λ &lt; 1, the final state magnitude begins to saturate (Figure 4c), yet the accuracies do not drop significantly once it has saturated (Figure 4d).</p><p>We also tested this more explicitly by introducing bounds on the modulation growth size and found that the network continued to perform quite well (see response to Essential Revisions above).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The paper is well-written and the figures are carefully chosen and informative.</p><p>As I mentioned in my <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.27.497776">Public Review</ext-link>, one would like to understand what are the non-obvious limitations (if any) of the general principle, that is, any transient, stimulus-dependent modification of neural/synaptic properties is a memory buffer. For instance, what happens if instead of using Equation (2), you use short-term synaptic plasticity a la Tsodyks-Markram with heterogeneous parameters (e.g., different U, tau_D, and tau_F), and you only learn the synaptic weights (STP parameters are fixed)? It seems to me that this will help clarify some interesting issues, such as, do you really need the synaptic modulation to be associative (i.e., to jointly depend on the pre- and post-synaptic activity)?</p></disp-quote><p>As mentioned in our public response, we explored a pre-synaptic only rule and found that it did not change the results significantly. It continues to perform well in the reservoir computing setting, and so indeed the training of the eta and λ parameters is not strictly necessary for it to perform well if they are set to reasonable values.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The paper could be strengthened by tasks that are not integration-based. This could be in the context of RNNs, but then it should be clear what is the added benefit of short-term plasticity.</p></disp-quote><p>See <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.27.497776">our response</ext-link> to your public review.</p></body></sub-article></article>