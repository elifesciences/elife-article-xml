<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98148</article-id><article-id pub-id-type="doi">10.7554/eLife.98148</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98148.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The transformation of sensory to perceptual braille letter representations in the visually deprived brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Haupt</surname><given-names>Marleen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1683-8679</contrib-id><email>marleen.haupt@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Graumann</surname><given-names>Monika</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Teng</surname><given-names>Santani</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kaltenbach</surname><given-names>Carina</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name><email>rmcichy@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046ak2485</institution-id><institution>Department of Education and Psychology, Freie Universität Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Berlin School of Mind and Brain, Faculty of Philosophy, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05783y657</institution-id><institution>Smith-Kettlewell Eye Research Institute</institution></institution-wrap><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>SP</surname><given-names>Arun</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP98148</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-10"><day>10</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-03-22"><day>22</day><month>03</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.12.579923"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-28"><day>28</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98148.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-19"><day>19</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98148.2"/></event></pub-history><permissions><copyright-statement>© 2024, Haupt, Graumann et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Haupt, Graumann et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98148-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98148-figures-v1.pdf"/><abstract><p>Experience-based plasticity of the human cortex mediates the influence of individual experience on cognition and behavior. The complete loss of a sensory modality is among the most extreme such experiences. Investigating such a selective, yet extreme change in experience allows for the characterization of experience-based plasticity at its boundaries. Here, we investigated information processing in individuals who lost vision at birth or early in life by probing the processing of braille letter information. We characterized the transformation of braille letter information from sensory representations depending on the reading hand to perceptual representations that are independent of the reading hand. Using a multivariate analysis framework in combination with functional magnetic resonance imaging (fMRI), electroencephalography (EEG), and behavioral assessment, we tracked cortical braille representations in space and time, and probed their behavioral relevance. We located sensory representations in tactile processing areas and perceptual representations in sighted reading areas, with the lateral occipital complex as a connecting ‘hinge’ region. This elucidates the plasticity of the visually deprived brain in terms of information processing. Regarding information processing in time, we found that sensory representations emerge before perceptual representations. This indicates that even extreme cases of brain plasticity adhere to a common temporal scheme in the progression from sensory to perceptual transformations. Ascertaining behavioral relevance through perceived similarity ratings, we found that perceptual representations in sighted reading areas, but not sensory representations in tactile processing areas are suitably formatted to guide behavior. Together, our results reveal a nuanced picture of both the potentials and limits of experience-dependent plasticity in the visually deprived brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>congenital blindness</kwd><kwd>brain plasticity</kwd><kwd>letter representations</kwd><kwd>braille</kwd><kwd>MVPA</kwd><kwd>EEG</kwd><kwd>fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/1-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/3-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>CI241/7-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>INST 272/297-1</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-StG-2018-803370</award-id><principal-award-recipient><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The visually deprived brain processes sensory letter information in tactile areas but perceptual letter information in sighted reading areas, revealing experience-dependent brain plasticity.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Human brains vary due to individual experiences. This so-called experience-based plasticity of the human cortex mediates cognitive and behavioral adaptation to changes in the environment (<xref ref-type="bibr" rid="bib70">Pascual-Leone et al., 2005</xref>). Typically, plasticity reflects learning from species-typical experiences. However, plasticity also results from species-atypical changes to experience like the loss of a sensory modality.</p><p>Sensory loss constitutes a selective, yet large-scale change in experience that offers a unique experimental opportunity to study cortical plasticity at its boundaries (<xref ref-type="bibr" rid="bib76">Ricciardi et al., 2020</xref>). One deeply investigated case of sensory loss is blindness, that is, the lack of visual input to the brain. Previous research has shown that cortical structures most strongly activated by visual input in sighted brains are activated by a plethora of other cognitive functions in visually deprived brains (<xref ref-type="bibr" rid="bib7">Bedny, 2017</xref>), including braille reading (<xref ref-type="bibr" rid="bib16">Büchel et al., 1998</xref>; <xref ref-type="bibr" rid="bib21">Burton et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Rączy et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib80">Sadato et al., 1998</xref>; <xref ref-type="bibr" rid="bib88">Uhl et al., 1991</xref>; <xref ref-type="bibr" rid="bib79">Sadato, 1996</xref>). However, overlapping functional responses alone cannot inform us about the nature of the observed activations, that is, what kind of information they represent and thus what role they play in cognitive processing.</p><p>To elucidate the nature of information processing in the visually deprived brain, we investigate the tactile braille system in individuals who lost vision at birth or early in life (hereafter blind participants). Braille readers commonly use both hands, requiring their brain to transform sensory tactile input into a hand-independent perceptual format. We made use of this practical everyday requirement to experimentally characterize the transformation of sensory to perceptual braille letter representations. We operationalize sensory braille letter representations as representations coding information specific to the hand that was reading (hand-dependent). In contrast, we operationalize perceptual braille letter representations as representations coding information independent of which hand was reading (hand-independent).</p><p>Combining this operationalization with fMRI and EEG in a multivariate analysis framework (<xref ref-type="bibr" rid="bib29">Cichy et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Cichy et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Carlson et al., 2011a</xref>; <xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>), we determine the cortical location and temporal emergence of sensory and perceptual representations. Lastly, to ascertain the functional role of the identified representations, we relate them to behavioral similarity ratings (<xref ref-type="bibr" rid="bib31">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Mur et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Charest et al., 2014</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We recorded fMRI (<italic>N</italic> = 15) and EEG (<italic>N</italic> = 11) data while blind participants (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>) read braille letters with their left or right index finger. We delivered the braille stimuli using single piezo-electric refreshable cells. This allowed participants to read braille letters without moving their finger, thus avoiding finger motion artifacts in the brain signal and analyses.</p><p>We used a common experimental paradigm for fMRI and EEG that was adapted to the specifics of each imaging modality. The common stimulus set consisted of 10 different braille letters (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Eight letters entered the main analysis. Two letters (E and O) served as vigilance targets to which participants responded with their foot; these trials were excluded from all analyses. The stimuli were presented in random order, with each trial consisting of a 500-ms stimulus presentation to either the right or left hand. In fMRI, all trials were followed by an inter-stimulus interval (ISI) of 2500 ms to account for the sluggishness of the BOLD response (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In EEG, standard trials had an ISI of 500 ms while catch trials had an ISI of 1100 ms in order to avoid movement contaminations.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli, experimental design, pattern extraction, and multivariate analysis framework.</title><p>(<bold>A</bold>) We presented eight braille letters (B, C, D, L, M, N, V, and Z) to participants on braille cells. Two additional letters (E and O) served as catch trials and were excluded from all analyses. (<bold>B</bold>) Top: During the fMRI session, braille letters were presented for 500 ms with an inter-stimulus interval (ISI) of 2500 ms. Participants were instructed to respond to catch trials by pressing a button with their foot. Bottom: During a separate EEG session, braille letters were presented for 500 ms. The ISI following regular trials lasted 500 ms, the ISI following catch trials lasted 1100 ms to avoid movement confounds. Participants were instructed to respond to catch trials by pressing a foot pedal. (<bold>C</bold>) In fMRI, we extracted voxel-wise activations for every region of interest (ROI). In EEG, we extracted channel-wise activations for every time point. In both cases, this resulted in one response vector per letter and per experimental run. (<bold>D</bold>) For both fMRI and EEG, we divided pattern vectors into training (four pseudo-runs) and test (one pseudo-run) sets. For every pair of braille letters (e.g., B and V), we trained a support vector machine (SVM) to classify between pattern vectors related to the presentation of both letters read with the same hand. We then tested the SVM on the left-out pattern vectors related to the presentation of the same two letters read with the same hand (within-hand classification) or with the other hand (across-hand classification). The resulting pairwise decoding accuracies were aggregated in a decoding accuracy matrix that is symmetric along the diagonal, with the diagonal itself being undefined. We interpret the within-hand matrix (black) as a measure of sensory and perceptual braille letter representations. We interpret the across-hand matrix (green) as a measure of perceptual braille letter representations. We derive the measure of sensory braille letter representations (blue) by subtracting one matrix from the other.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Confusion matrices for region of interest (ROI) decoding within-hand.</title><p>Values represent pairwise decoding accuracies minus chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Confusion matrices for region of interest (ROI) decoding across-hand.</title><p>Values represent pairwise decoding accuracies minus chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Confusion matrices for EEG time decoding at indicated time points within-hand.</title><p>Values represent pairwise decoding accuracies minus chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Confusion matrices for EEG time decoding at indicated time points across-hand.</title><p>Values represent pairwise decoding accuracies minus chance level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig1-figsupp4-v1.tif"/></fig></fig-group><p>The common experimental paradigm for fMRI and EEG allowed us to use an equivalent multivariate classification scheme to track the transformation of sensory to perceptual representations. We assessed fMRI voxel patterns to reveal where sensory braille letter representations are located in the cortex. Likewise, we assessed EEG electrode patterns to reveal the temporal dynamics of braille letter representations (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>We operationalized sensory versus perceptual representations as hand-dependent versus hand-independent braille letter representations, respectively. To measure perceptual representations, we trained classifiers on brain data recorded during stimulation of one hand and tested the classifiers on data recorded during the stimulation of the other hand (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). We refer to this analysis as across-hand classification. It reveals perceptual braille letter representations that are independent of the specific hand being stimulated.</p><p>To assess sensory representations, we used a two-step procedure. In a first step, we trained and tested classifiers on brain data recorded during stimulation of the same hand (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). We refer to this analysis as within-hand classification. It reveals both sensory and perceptual braille letter representations. Thus, to further isolate sensory representations from perceptual representations, in a second step, we subtracted across-hand classification from within-hand classification results.</p><sec id="s2-1"><title>Spatial dynamics of braille letter representations</title><p>We started the analyses by determining the locations of sensory and perceptual braille letter representations in the visually deprived brain using fMRI. We focused our investigation on two sets of cortical regions based on previous literature: tactile processing areas and sighted reading areas (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Given the tactile nature of braille, we expected braille letters to be represented in the tactile processing stream encompassing somatosensory cortices (S1 and S2), intra-parietal cortex (IPS), and insula (<xref ref-type="bibr" rid="bib40">Dijkerman and de Haan, 2007</xref>). Given that visual reading information is processed in a ventral processing stream (<xref ref-type="bibr" rid="bib37">DiCarlo and Cox, 2007</xref>; <xref ref-type="bibr" rid="bib38">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Goodale and Milner, 1992</xref>), and that braille reading has been observed to elicit activations along those nodes (<xref ref-type="bibr" rid="bib16">Büchel et al., 1998</xref>; <xref ref-type="bibr" rid="bib21">Burton et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Rączy et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib80">Sadato et al., 1998</xref>), we investigated the sighted processing stream ranging from early visual cortex (EVC) (<xref ref-type="bibr" rid="bib33">Dehaene et al., 2005</xref>) over V4 (<xref ref-type="bibr" rid="bib32">Cohen et al., 2000</xref>) and the lateral occipital complex (LOC) (<xref ref-type="bibr" rid="bib68">Neudorf et al., 2022</xref>) to the letter form area (LFA) (<xref ref-type="bibr" rid="bib86">Thesen et al., 2012</xref>) and visual word form area (VWFA) (<xref ref-type="bibr" rid="bib32">Cohen et al., 2000</xref>; <xref ref-type="bibr" rid="bib68">Neudorf et al., 2022</xref>; <xref ref-type="bibr" rid="bib62">McCandliss et al., 2003</xref>; <xref ref-type="bibr" rid="bib34">Dehaene and Cohen, 2011</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Spatial dynamics of braille letter representations.</title><p>(<bold>A</bold>) Rendering of regions of interests associated with tactile processing (S1: primary somatosensory cortex, S2: secondary somatosensory cortex, aIPS: anterior intra-parietal sulcus, pIPS: posterior intra-parietal sulcus, insula) and sighted reading (EVC: early visual cortex, V4: visual area 4, LOC: lateral occipital complex, LFA: letter form area, VWFA: visual word form area). (<bold>B</bold>) Sensory and perceptual (left), sensory (middle), and perceptual (right) braille letters representations in tactile processing and sighted reading regions of interest (ROIs) <italic>N</italic> = 15, two-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate (FDR) corrected; stars below bars indicate significance above chance. Error bars represent 95% confidence intervals. Dots represent single subject data. (<bold>C</bold>) fMRI searchlight results for sensory (blue) and perceptual (green) braille letter representations (<italic>N</italic> = 15, height threshold p &lt; 0.001, cluster-level family-wise error (FWE) corrected p &lt; 0.05, colored voxels indicate significance). Results for combined sensory and perceptual representations are in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>fMRI searchlight results for within-hand braille letter classification.</title><p>Results index mixed sensory and perceptual braille letter representations (<italic>N</italic> = 15, height threshold p &lt; 0.001, cluster-level family-wise error (FWE) corrected p &lt; 0.05, colored voxels indicate significance).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig2-figsupp1-v1.tif"/></fig></fig-group><p>We hypothesized (H1) that sensory braille letter information is represented in tactile processing areas (H1.1), while perceptual braille letter representations are located in sighted reading areas (H1.2). To test H1, we conducted within- and across-hand classification of braille letters in the above-mentioned areas for tactile processing and sighted reading (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) in a region-of-interest (ROI) analysis.</p><p>We found that within-hand classification of braille letters was significantly above chance in regions associated with both tactile processing (S1, S2, anterior intra-parietal sulcus [aIPS], posterior intra-parietal sulcus [pIPS], and insula) and sighted reading (EVC, LOC, LFA, and VWFA) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left; <italic>N</italic> = 15, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected). As expected, this reveals both tactile and sighted reading areas as potential candidate regions housing sensory and perceptual braille letter representations.</p><p>To pinpoint the loci of sensory representations we subtracted the results of the across-hand classification from the within-hand classification. We found the difference to be significant in tactile processing areas (S1, S2, aIPS, and pIPS) and in LOC, but not elsewhere (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, middle). This confirms H1.1 in that sensory braille letter representations are located in tactile areas.</p><p>To determine the location of perceptual representations, we assessed the results of across-hand classification of braille letters. We found significant information (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right) only in sighted reading areas (EVC, LOC, and VWFA), with the exception of the insula. This confirms H1.2 in that perceptual braille letter representations emerge predominantly in sighted reading areas. The surprising finding of the insula can possibly be explained by the insula’s heterogeneous functions (<xref ref-type="bibr" rid="bib87">Uddin et al., 2017</xref>) beyond tactile processing.</p><p>Notably, LOC is the only region that contained both sensory and perceptual braille letter representations. This suggests a ‘hinge’ function in the transformation from sensory to perceptual braille letter representations.</p><p>To ascertain whether other areas beyond our hypothesized ROIs contained braille letter representations, we conducted a spatially unbiased fMRI searchlight classification analysis. The results confirmed that braille letter representations are located in the assessed ROIs without revealing any additional regions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <italic>N</italic> = 15, height threshold p &lt; 0.001, cluster-level family-wise error [FWE] corrected p &lt; 0.05).</p><p>Together, our fMRI results revealed sensory braille letter representations in tactile processing areas and perceptual braille letter representations in sighted reading areas, with LOC serving as a ‘hinge’ region between them.</p></sec><sec id="s2-2"><title>Temporal dynamics of braille letter representations</title><p>We next determined the temporal dynamics with which braille letter representations emerge using EEG.</p><p>We hypothesized (H2) that sensory braille letter representations emerge in time before perceptual braille letter representations, analogous to the sequential processing of sensory representations before perceptual representations in the visual (<xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib30">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Carlson et al., 2011b</xref>) and auditory (<xref ref-type="bibr" rid="bib59">Lowe et al., 2021</xref>) domain.</p><p>To test H2, we conducted time-resolved within- and across-hand classification on EEG data. We determined the time point at which representations emerge by finding the first time point with respect to the onset of the braille stimulation where the classification effects are significant (50 consecutive significant time point criteria, 95% confidence intervals reported in brackets).</p><p>The EEG classification analyses revealed significant and reliable results for both within- and across-hand classification of braille letters, as well as their difference (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; <italic>N</italic> = 11, 1000 bootstraps, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, FDR corrected). We found that within-hand classification became significant at 62 ms (29–111 ms) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, blue curve). To isolate sensory representations, we subtracted the results of the across-hand classification from the within-hand classification. This difference became significant at 77 ms (45–138 ms) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, black curve). In contrast, the across-hand classification, indicating perceptual representations, became significant later at 184 ms (127–230 ms) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, green curve).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Temporal dynamics of braille letter representations.</title><p>(<bold>A</bold>) EEG results for sensory and perceptual (black), sensory (blue), and perceptual (green) braille letter representations in time. Shaded areas around curves indicate standard error of the mean. Significance onsets and their 95% confidence intervals are indicated by dots and horizontal lines below curves (color-coded as curves, <italic>N</italic> = 11, 1000 bootstraps). Significant time points are indicated by x below curves (<italic>N</italic> = 11, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected). (<bold>B</bold>) EEG searchlight results for sensory and perceptual (top), sensory (middle), and perceptual (bottom) braille letter representations in EEG channel space (sampled down to 10 ms resolution) in 100-ms intervals. Significant electrodes are marked with black circles (<italic>N</italic> = 11, one-tailed Wilcoxon signed-rank test, p &lt;0 .05, FDR corrected across electrodes and time points).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>EEG time decoding results with empirical baseline (see <xref ref-type="fig" rid="fig3">Figure 3A</xref> for comparison).</title><p>For within- and across-hand decoding, we subtracted the chance level (50%) from the data. Shaded areas around the curves indicate standard error of the mean. Significance onset and its 95% confidence interval are indicated by a dot and horizontal line below curve (<italic>N</italic> = 11, 1000 bootstraps). Significant time points are indicated by x below curves (<italic>N</italic> = 11, sign permutation test, 10,000 permutations, p &lt; 0.05, false discovery rate [FDR] corrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>EEG results for decoding of stimulated hand in time.</title><p>The shaded area around the curve indicates standard error of the mean. Significance onset and its 95% confidence interval are indicated by a dot and horizontal line below curve (<italic>N</italic> = 11, 1000 bootstraps, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>EEG time decoding results when only using occipital and parieto-occipital electrodes.</title><p>Analysis is limited to eight electrodes instead of all electrodes (<italic>n</italic> = 63, see <xref ref-type="fig" rid="fig3">Figure 3A</xref> for comparison). For within- and across-hand decoding, we subtracted the chance level (50%) from the data. The area around curve indicates standard error of the mean. Significance onset and its 95% confidence interval are indicated by a dot and horizontal line below curve (<italic>N</italic> = 11, 1000 bootstraps). Significant time points are indicated by x below curves (<italic>N</italic> = 11, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig3-figsupp3-v1.tif"/></fig></fig-group><p>Importantly, the temporal dynamics of sensory and perceptual representations differed significantly. Compared to sensory representations, the significance onset of perceptual representations was delayed by 107 ms (21–167 ms) (<italic>N</italic> = 11, 1000 bootstraps, one-tailed bootstrap test against zero, p = 0.012). This results pattern was consistent when defining the analysis baseline empirically (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>To approximate the sources of the temporal signals, we complemented the EEG classification analysis with a searchlight classification analysis in EEG sensor space. In the time window of highest decodability (~200–300 ms), sensory braille letter information was decodable from widespread electrodes across the scalp. In contrast, perceptual braille letter information was decodable later and from overall fewer electrodes which were located over right frontal, central, left parietal, and left temporal areas (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; <italic>N</italic> = 11, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, FDR corrected across electrodes and time points). These additional results reinforce that sensory braille letter information is represented in more widespread brain areas than perceptual braille letter information, corroborating our ROI classification results.</p><p>In sum, our EEG results characterized the temporal dynamics with which braille letter representations emerge as a temporal sequence of sensory before perceptual representations.</p></sec><sec id="s2-3"><title>Relating representations of braille letters to behavior</title><p>The ultimate goal of perception is to provide an organism with representations enabling adaptive behavior. The analyses across space and time described above identified potential candidates for such braille letter representations in the sense that the representations distinguished between braille letters. However, not all of these representations have to be used by the brain to guide behavior; some of these representations might be epiphenomenal and only available to the experimenter (<xref ref-type="bibr" rid="bib36">deWit et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Reddy and Kanwisher, 2007</xref>; <xref ref-type="bibr" rid="bib92">Williams et al., 2007</xref>).</p><p>Therefore, we tested the hypothesis (H3) that the sensory and perceptual braille letter representations identified in space (H1) and time (H2) are in a suitable format to be behaviorally relevant. We used perceived similarity as a proxy for behavioral relevance. The idea is that if two stimuli are perceived to be similar, they will also elicit similar actions (<xref ref-type="bibr" rid="bib31">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Mur et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Charest et al., 2014</xref>). For this, we acquired perceived similarity ratings from blind participants (<italic>N</italic> = 19) in a separate behavioral experiment (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, middle) in which participants verbally rated the similarity of each pair of braille letters from the stimulus set on a scale.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Representational similarity of braille letters in neural and behavioral measures.</title><p>(<bold>A</bold>) For fMRI (top) and EEG (bottom), we formed representational dissimilarity matrices (RDMs) from pairwise decoding accuracies. In the behavioral experiment (middle), we presented two braille letters (e.g., B and V) on two braille cells. For all combinations, we asked participants to read both braille letters with the same hand (e.g., right) and verbally rate their similarity on a scale from 1 to 7. We formed the behavioral RDM from those perceptual similarity judgements for every letter pair. We then correlated the behavioral RDM (averaged over participants) with neural RDMs (subject specific) using Spearman’s <italic>R</italic>. (<bold>B</bold>) Results of representational similarity analysis (RSA) relating fMRI and behavior in regions of interest (ROIs) that showed significant sensory (left) and perceptual (right) braille letter representations in fMRI (see <xref ref-type="fig" rid="fig2">Figure 2</xref>) (<italic>N</italic> = 15, two-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected). Stars below bars indicate significance above chance. Error bars represent 95% confidence intervals. Dots represent single subject data. (<bold>C</bold>) Results of RSA relating EEG and behavior for sensory (blue) and perceptual (green) representations. Shaded areas around curves indicate standard error of the mean. Significance onsets and their 95% confidence intervals are indicated by dots and lines below curves (<italic>N</italic> = 11, 1000 bootstraps, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, FDR corrected, color-coded as result curves).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Representational similarity of braille letters in neural and behavioral measures using Jaccard similarity.</title><p>(<bold>A</bold>) Results of representational similarity analysis (RSA) relating fMRI and behavior in regions of interest (ROIs) that showed significant sensory (left) and perceptual (right) braille letter representations in fMRI (see <xref ref-type="fig" rid="fig2">Figure 2</xref>) (<italic>N</italic> = 15, two-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected). Error bars represent 95% confidence intervals. Dots represent single subject data. (<bold>B</bold>) Results of RSA relating EEG and behavior for sensory (blue) and perceptual (green) representations. Shaded areas around the curves indicate standard error of the mean. Significance onsets and their 95% confidence intervals are indicated by dots and lines below curves (<italic>N</italic> = 11, 1000 bootstraps, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, FDR corrected, color-coded as result curves).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>EEG-behavior representational similarity analysis (RSA) decoding results when only using occipital and parieto-occipital electrodes.</title><p>Analysis is limited to eight electrodes instead of all electrodes (<italic>n</italic> = 63, see <xref ref-type="fig" rid="fig4">Figure 4C</xref> for comparison). Shaded areas around the curves indicate standard error of the mean. Significance onset and its 95% confidence interval are indicated by a dot and horizontal line below curve (<italic>N</italic> = 11, 1000 bootstraps, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, false discovery rate [FDR] corrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98148-fig4-figsupp2-v1.tif"/></fig></fig-group><p>To relate perceived similarity ratings to neural representations, we used representational similarity analysis (RSA) (<xref ref-type="bibr" rid="bib55">Kriegeskorte et al., 2008</xref>). RSA relates different measurement spaces (such as EEG, fMRI, and behavior) by abstracting them into a common representational similarity space. We sorted behavioral similarity ratings into representational dissimilarity matrices (RDMs) indexed in rows and columns by the eight braille letters used in experimental conditions (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). For neural data, we sorted decoding accuracies from the previous analyses as a dissimilarity measure (<xref ref-type="bibr" rid="bib30">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Guggenmos et al., 2018</xref>). Thus, by arranging the classification results of the EEG and fMRI results we obtained EEG RDMs for every time point and fMRI RDMs for each ROI (for the within- and the across-hand analyses separately). To finally relate behavioral and neural data in the common RDM space, we correlated the behavioral RDM with each fMRI ROI RDM and each EEG time point RDM.</p><p>Considering braille letter representations in space through fMRI (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <italic>N</italic> = 15, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, FDR corrected), we found that previously identified perceptual representations (i.e., identified by the across-hand analysis) in EVC, LOC, VWFA, and insula showed significant correlations with behavior. In contrast, sensory representations (i.e., identified by the differences between within- and across-hand analyses) in S1, S2, aIPS, pIPS, and LOC were not significantly correlated with behavior. This indicates that perceptual braille letter representations in sighted reading areas are suitably formatted to guide behavior.</p><p>Considering braille letter representations in time through EEG (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; <italic>N</italic> = 11, 1000 bootstraps, one-tailed Wilcoxon signed-rank test, p &lt; 0.05, FDR corrected), we found significant relationships with behavior for both sensory and perceptual representations. The temporal dynamics mirrored those of the EEG classification analysis (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), in that the results related to sensory representations emerged earlier at 220 ms (167–567 ms) than the results related to perceptual representations at 466 ms (249–877 ms). The onset latency differences of 240 ms (−81 to 636 ms) (<italic>N</italic> = 11, 1000 bootstrap, one-tailed bootstrap test against zero, p = 0.046) was significant. This indicates that both earlier sensory representations and later perceptual representations of braille letters are suitable formatted to guide behavior.</p><p>In sum, our RSA results highlighted that perceptual representations in sighted reading areas, as well as initial sensory and later perceptual representations in time, are suitably formatted to guide behavior.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We assessed experience-based brain plasticity at its boundaries by investigating the nature of braille information processing in the visually deprived brain. For this, we assessed the transformation of sensory to perceptual braille letter representations in blind participants. Our experimental strategy combining fMRI, EEG, and behavioral assessment yielded three key findings about spatial distribution, temporal emergence, and behavioral relevance. First, concerning the spatial distribution of braille letter representations, we found that sensory braille letter representations are located in tactile processing areas while perceptual braille letter representations are located in sighted reading areas. Second, concerning the temporal emergence of braille letter representations, we found that sensory braille letter representations emerge before perceptual braille letter representations. Third, concerning the behavioral relevance of representations, we found that perceptual representations identified in sighted reading areas, as well as sensory and perceptual representations identified in time, are suitably formatted to guide behavior.</p><sec id="s3-1"><title>The topography of sensory and perceptual braille letter representations</title><p>Previous research has identified the regions activated during braille reading in high detail (<xref ref-type="bibr" rid="bib16">Büchel et al., 1998</xref>; <xref ref-type="bibr" rid="bib21">Burton et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Rączy et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib80">Sadato et al., 1998</xref>; <xref ref-type="bibr" rid="bib88">Uhl et al., 1991</xref>; <xref ref-type="bibr" rid="bib79">Sadato, 1996</xref>). However, activation in a brain region alone does not indicate its functional role or the kind of information it represents (<xref ref-type="bibr" rid="bib48">Haynes and Rees, 2006</xref>). Here, we characterize the information represented in a region by distinguishing between sensory and perceptual representations of single braille letters. Our findings extend our understanding of the cortical regions processing braille letters in the visually deprived brain in five ways.</p><p>First, we clarified the role of EVC activations in braille reading (<xref ref-type="bibr" rid="bib16">Büchel et al., 1998</xref>; <xref ref-type="bibr" rid="bib21">Burton et al., 2012</xref>; <xref ref-type="bibr" rid="bib72">Rączy et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib80">Sadato et al., 1998</xref>; <xref ref-type="bibr" rid="bib88">Uhl et al., 1991</xref>; <xref ref-type="bibr" rid="bib79">Sadato, 1996</xref>) by showing that EVC harbors representations of single braille letters. More specifically, our finding that EVC represents perceptual rather than sensory braille letter information indicates that EVC representations are formatted at a higher perceptual level rather than a tactile input level. Previous studies also found that EVC of blind participants processes other higher-level information such as natural sounds (<xref ref-type="bibr" rid="bib89">Vetter et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Mattioni et al., 2020</xref>) and language (<xref ref-type="bibr" rid="bib18">Burton et al., 2002</xref>; <xref ref-type="bibr" rid="bib20">Burton et al., 2003b</xref>; <xref ref-type="bibr" rid="bib78">Röder et al., 2002</xref>; <xref ref-type="bibr" rid="bib6">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Lane et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Dietrich et al., 2013</xref>; <xref ref-type="bibr" rid="bib91">Watkins et al., 2022</xref>; <xref ref-type="bibr" rid="bib1">Abboud et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Amedi et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Burton, 2003a</xref>). A parsimonious view is that EVC in the visually deprived brain engages in higher-level computations shared across domains, rather than performing multiple distinct lower-level sensory computations. Importantly, higher-level computations are not limited to the EVC in visually deprived brains. Natural sound representations (<xref ref-type="bibr" rid="bib89">Vetter et al., 2020</xref>) and language activations (<xref ref-type="bibr" rid="bib82">Seydell-Greenwald et al., 2023</xref>) are also located in EVC of sighted participants. This suggests that EVC, in general, has the capacity to process higher-level information (<xref ref-type="bibr" rid="bib69">Pascual-Leone and Hamilton, 2001</xref>). Thus, EVC in the visually deprived brain might not be undergoing fundamental changes in brain organization (<xref ref-type="bibr" rid="bib82">Seydell-Greenwald et al., 2023</xref>). This promotes a view of brain plasticity in which the cortex is capable of dynamic adjustments within pre-existing computational capacity limits (<xref ref-type="bibr" rid="bib7">Bedny, 2017</xref>; <xref ref-type="bibr" rid="bib82">Seydell-Greenwald et al., 2023</xref>; <xref ref-type="bibr" rid="bib69">Pascual-Leone and Hamilton, 2001</xref>; <xref ref-type="bibr" rid="bib60">Makin and Krakauer, 2023</xref>).</p><p>We note that our findings contribute additional evidence but cannot conclusively distinguish between the competing hypotheses that visually deprived brains dynamically adjust to the environmental constraints versus that they undergo a profound cortical reorganization. Resolving this debate would require an analogous experiment in sighted people which taps into the same mechanisms as the present study. Defining a suitable control experiment is, however, difficult. Any other type of reading would likely tap into different mechanism than braille reading. Furthermore, whenever sighted participants are asked to perform a haptic reading task, outcomes can be confounded by visual imagery driving visual cortex (<xref ref-type="bibr" rid="bib41">Dijkstra et al., 2019</xref>). Thus, the results would remain ambiguous as to whether observed differences between the groups index different mechanisms or plastic changes in the same mechanisms. Last, matching groups of sighted readers and braille readers such that they only differ with regard to their input modality seems practically unfeasible: There are vast differences within the blind population in general (for example, etiologies, onset, and severity), and the subsample of congenitally blind braille readers more specifically (for example, the quality and quantity of their braille education) as well as across braille and print readers (for example, different passive exposure to braille versus written letters during childhood <xref ref-type="bibr" rid="bib43">Englebretson et al., 2023</xref>; <xref ref-type="bibr" rid="bib63">Merabet and Pascual-Leone, 2010</xref>).</p><p>Second, we found that VWFA contains perceptual but not sensory braille letter representations. By clarifying the representational format of language representations in VWFA, our results support previous findings of the VWFA being functionally selective for letter and word stimuli in the visually deprived brain (<xref ref-type="bibr" rid="bib75">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="bib84">Striem-Amit et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">Liu et al., 2023</xref>). Together, these findings suggest that the functional organization of the VWFA is modality-independent (<xref ref-type="bibr" rid="bib75">Reich et al., 2011</xref>) depicting an important contribution to the ongoing debate on how visual experience shapes representations along the ventral stream (<xref ref-type="bibr" rid="bib8">Bedny, 2021</xref>).</p><p>Third, LOC represented hand-dependent and -independent braille letter information, suggesting a ‘hinge’ function between sensory and perceptual braille letter representations. We stipulate that shape serves as an intermediate level representational format in between lower-level properties such as specific location of tactile stimulation or dot number in braille letters and higher-level perceptual letter features (<xref ref-type="bibr" rid="bib2">Amedi et al., 2001</xref>).</p><p>Fourth, the finding of letter representations of tactile origin in both VWFA and LOC indicate that the functional organization of both regions is multimodal, contributing to the debate on how experience from vision or other sensory modalities shapes representations along the ventral stream (<xref ref-type="bibr" rid="bib8">Bedny, 2021</xref>; <xref ref-type="bibr" rid="bib2">Amedi et al., 2001</xref>).</p><p>Fifth, we observed that the somatosensory cortices and intra-parietal sulci represent hand-dependent but not hand-independent braille letter representations. This is consistent with previous studies reporting that the primary somatosensory cortex represents the location of tactile stimulation (<xref ref-type="bibr" rid="bib81">Sanchez-Panchuelo et al., 2012</xref>), but not the identity of braille words (<xref ref-type="bibr" rid="bib58">Liu et al., 2023</xref>). Taken together, these findings suggest that these tactile processing areas represent sensory rather than higher-level features of tactile inputs in visually deprived brains.</p><p>The involvement of the insula in processing braille letter information is more difficult to interpret. Based on previous studies in the sighted brain, the insula plays a role in tactile memory (<xref ref-type="bibr" rid="bib13">Bonda et al., 1996</xref>; <xref ref-type="bibr" rid="bib17">Burton and Sinclair, 2000</xref>; <xref ref-type="bibr" rid="bib74">Reed et al., 2004</xref>) and multisensory integration (<xref ref-type="bibr" rid="bib4">Banati et al., 2000</xref>; <xref ref-type="bibr" rid="bib46">Hadjikhani and Roland, 1998</xref>; <xref ref-type="bibr" rid="bib71">Prather et al., 2004</xref>; <xref ref-type="bibr" rid="bib67">Naghavi et al., 2007</xref>). Both aspects could have contributed to our findings as braille letters are retrievable from long-term memory but are also inherently nameable and linked to auditory experiences. A future study could disambiguate the contributions of tactile memory and multisensory integration by presenting meaningless dot arrays, that are either unnamable or paired with invented names. Insular representations of trained, unnamable stimuli but not novel, unnamable stimuli would align with memory requirements. Insular representations of trained, namable stimuli but not trained, unnamable stimuli would favor audio-tactile integration.</p><p>Overall, identifying braille letter representations in widespread brain areas raises the question of how information flow is organized in the visually deprived brain. Functional connectivity studies report deprivation-driven changes of thalamo-cortical connections which could explain both arrival of information to and further flow of information beyond EVC. First, the coexistence of early thalamic connections to both S1 and V1 (<xref ref-type="bibr" rid="bib65">Müller et al., 2019</xref>) would enable EVC to receive from different sources and at different time points. Second, potentially overlapping connections from both sensory cortices to other visual or parietal areas (<xref ref-type="bibr" rid="bib50">Ioannides et al., 2013</xref>) could enable the visually deprived brain to process information in a widespread and interconnected array of brain areas. In such a network architecture, several brain areas receive and forward information at the same time. In contrast to information discretely traveling from one processing unit to the next in the sighted brain’s processing cascade, we can rather picture information flowing in a spatially and functionally more distributed and overlapping fashion.</p></sec><sec id="s3-2"><title>Sensory representations emerge before perceptual representations</title><p>Using time-resolved multivariate analysis of EEG data (<xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib23">Carlson et al., 2011b</xref>), we showed that hand-dependent, sensory braille letter representations emerge in time before hand-independent, perceptual representations. Such sequential multi-step processing in time is a general principle of information processing in the human brain, also known in the visual (<xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib30">Cichy et al., 2014</xref>) and auditory (<xref ref-type="bibr" rid="bib59">Lowe et al., 2021</xref>) domain. Together, these findings suggest that the human brain, even in extreme instances of species-atypical cortical plasticity, honors this principle.</p><p>While braille letter reading follows the same temporal processing sequence as its visual counterpart, it operates on a different time scale. Our results indicate that braille letter classification peaks substantially later in time (~200 ms for hand-dependent and ~390 ms for hand-independent representations) than previously reported classification of visually presented words, letters, objects, and object categories (e.g., ~125 ms for location-dependent and ~175 ms for location-independent representations) (<xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib30">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib57">Ling et al., 2019</xref>; <xref ref-type="bibr" rid="bib85">Teng et al., 2015</xref>). This discrepancy raises the question which factors could limit the speed of processing braille letters. Importantly, this delay is not a consequence of slower cortical processing in the tactile domain (<xref ref-type="bibr" rid="bib65">Müller et al., 2019</xref>). We find that tactile information reaches the cortex fast: we can classify which hand was stimulated as early as 35 ms after stimulation onset (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Thus, the delay relates directly to the identification of braille letters.</p><p>A compelling explanation for the temporal processing properties of braille letter information are the underlying reading mechanics. Braille reading is slower than print reading (<xref ref-type="bibr" rid="bib11">Bola et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Brysbaert, 2019</xref>) even if participants are fluent braille readers. This slowing is specific to braille reading and does not translate to other types of information intake in the visually deprived brain, for example, auditory information. Blind participants have higher listening rates (<xref ref-type="bibr" rid="bib14">Bragg et al., 2018</xref>) and better auditory discrimination skills (<xref ref-type="bibr" rid="bib49">Hötting and Röder, 2009</xref>) than sighted participants, indicating more efficient auditory processing (<xref ref-type="bibr" rid="bib77">Röder et al., 1996</xref>; <xref ref-type="bibr" rid="bib64">Muchnik et al., 1991</xref>). Together, this result pattern suggests that the temporal dynamics with which braille letter representations emerge are limited by the particular efficiency of the braille letter system, rather than the capacity of the brain. To test this idea, future studies could compare the temporal dynamics of braille letter and haptic object representations: a temporal processing delay specific to braille letters would support the hypothesis.</p></sec><sec id="s3-3"><title>Representations identified in space and time guide behavior</title><p>Our results clarified that perceptual rather than sensory braille letter representations identified in space are suitably formatted to guide behavior. However, we only use one specific task to assess behavior and, therefore, acknowledge that this finding is task-dependent. Arguably, general similarity ratings of braille letters depend more on intake-independent (e.g., dot arrangements or linguistic similarities such as pronunciation) than intake-dependent features (similarities in stimulation location on finger). Future behavioral assessments could ask participants to assess similarity separately based on only stimulation location or linguistic features. We would predict that similarity ratings based on stimulation location are related to sensory representations while similarity ratings based on linguistic features are related to perceptual representations of braille letters.</p><p>Concerning temporal dynamics, our results reveal that sensory representations of braille letters are relevant for behavior earlier than perceptual ones. Interestingly, the similarity between perceptual braille letter representations and behavioral similarity ratings emerges in a time window in which transcranial magnetic stimulation (TMS) over the VWFA affects braille letter reading in sighted braille readers (<xref ref-type="bibr" rid="bib12">Bola et al., 2019</xref>). This implies that around 320–420 ms after the onset of reading braille, visually deprived and sighted brains utilize braille letter representations for performing tasks such as letter identification. Applying a comparable TMS protocol not only to sighted but also non-sighted braille readers would elucidate whether this time window of behavioral relevance can be generalized to braille reading, independent of visual experience.</p><p>Similarity ratings and sensory representations as captured by EEG are correlated, and so are similarity ratings and representations in perceptual ROIs, but not sensory ROIs. This might be interpreted as suggesting a link between the sensory representations captured in EEG and the representations in perceptual ROIs. However, we do not have any evidence towards this idea. Differing signal-to-noise ratios (SNRs) for the different ROIs and sensory versus perceptual analysis could be an alternative explanation.</p></sec><sec id="s3-4"><title>Conclusions</title><p>Our investigation of experience-based plasticity at its boundaries due to the loss of the visual modality reveals a nuanced picture of its potential and limits. On the one hand, our findings emphasize how plastic the brain is by showing that regions typically processing visual information adapt to represent perceptual braille letter information. On the other hand, our findings illustrate inherent limits of brain plasticity. Brain areas represent information from atypical inputs within the boundaries of their pre-existing computational capacity and the progression from sensory to perceptual transformations adheres to a common temporal scheme and functional role.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>We conducted three separate experiments with partially overlapping participants: an fMRI, an EEG, and a behavioral experiment. All experiments were approved by the ethics committee of the Department of Education and Psychology of the Freie Universität Berlin and were conducted in accordance with the Declaration of Helsinki. Sixteen participants completed the fMRI experiment. One person was excluded due to technical problems during the recording, leaving a total of 15 participants in the fMRI experiment (mean age 39 years, SD = 10, 9 females). Eleven participants participated in the EEG experiment (<italic>N</italic> = 11, mean age 44 years, SD = 10, 8 females). The participant pools of the EEG and fMRI experiments overlapped by five participants. Out of a total of 21 participants, 19 participants (excluding one fMRI and one EEG participant) completed an additional behavioral task in which they rated the perceived similarity of braille letter pairs. All participants were blind since birth or early childhood (≤3 years, for details see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). All participants provided informed consent prior to the studies and received a monetary reward for their participation.</p></sec><sec id="s4-2"><title>Experimental stimuli and design</title><p>In all experiments, we presented braille letters (B, C, D, L, M, N, V, and Z; <xref ref-type="fig" rid="fig2">Figure 2A</xref>) to the left and right index fingers of participants using piezo-actuated refreshable braille cells (<ext-link ext-link-type="uri" xlink:href="https://metec-ag.de/index.php">https://metec-ag.de/index.php</ext-link>) with two modules of eight pins each. We only used the top 6 pins from each module to present letters from the braille alphabet. The modules were taped to the clothes of a participant for the fMRI experiment and on the table for the EEG and behavioral experiment. This way, participants could read in a comfortable position with their index fingers resting on the braille cells to avoid motion confounds. Importantly, our experimental setup did not involve tactile exploration or sliding motions. We instructed participants to read letters regardless of whether the pins passively stimulated their right or left index finger. We presented all eight letters to both hands, resulting in 16 experimental conditions (8 letters × 2 hands). In addition, two braille letters (E and O) were included as catch stimuli and participants were instructed to respond to them by pressing a button (fMRI) or pedal (EEG) with their foot. Catch trials were excluded from further analysis due to confounding motor and sensory signals.</p></sec><sec id="s4-3"><title>Experimental procedures</title><sec id="s4-3-1"><title>fMRI experiment</title><p>The fMRI experiment consisted of two sessions. Fifteen participants completed the first fMRI session, during which we recorded a structural image (~4 min), a localizer run (7 min), and 10 runs of the main experiment (56 min). The total duration of the first session was 67 min excluding breaks. Eight of these 15 subjects completed a second fMRI recording session, in which we recorded an additional 15 runs of the main experiment (85 min). We did not record any structural images or localizer runs in the second session, resulting in a total duration of 85 min excluding breaks.</p><sec id="s4-3-1-1"><title>fMRI main experiment</title><p>During the fMRI main experiment, we presented participants with letters on braille cells and asked them to respond to occasionally appearing catch letters. We presented letters for 500 ms, with a 2500-ms ISI (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>, top). Each regular trial – belonging to one of the 16 experimental conditions – was repeated 5 times per run (run duration: 337 s) in random order. Regular trials were interspersed every ~20 trials with a catch trial, such that a catch trial occurred about once per minute. In addition, every third to fifth trial (equally probable) was a null trial where no stimulation was given. In total, one run consisted of 80 regular trials, 5 catch trials, and 22 null trials, amounting to a total of 107 trials per run.</p><p>To ensure that participants were able to read letters with both hands and understood the task instructions, participants first completed an experimental run outside the scanner.</p></sec><sec id="s4-3-1-2"><title>fMRI localizer experiment</title><p>To define ROIs, we performed a separate localizer experiment prior to the main fMRI experiment with tactile stimuli in four experimental conditions: braille letters read with the left hand, braille letters read with the right hand, fake letters read with the left hand, and fake letters read with the right hand. The letters presented in the braille conditions were 16 letters from the alphabet excluding the letters used in the main experiment. The stimuli in the fake letter conditions were 16 tactile stimuli that were each composed of 8 dots, deviating from the standard 6-dot configuration in the braille alphabet.</p><p>The localizer experiment consisted of a single run lasting 432 s, comprising five blocks of presentation of braille letters left, braille letters right, fake letters left, fake letters right, and blank blocks as baseline. Each stimulation block was 14.4 s long, consisting of 18 different letters presentations (500 ms on, 300 ms off) including two one-back repetitions that participants were instructed to respond to by pressing a button with their foot. We presented stimulation blocks in random order and regularly interspersed them with blank blocks.</p></sec></sec></sec><sec id="s4-4"><title>EEG experiment</title><p>The EEG experiment consisted of two sessions. Eleven participants completed the first session and eight of these participants completed a second session. The total duration of each session was 59 min excluding breaks.</p><p>The experimental setup was similar to that for fMRI but adapted to the specifics of EEG. We presented braille letters for 500 ms with a 500-ms ISI on regular trials. In catch trials, the letters were presented for 500 ms with a 1100-ms ISI to avoid contamination of movement on subsequent trials (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>, bottom). Each of the 16 experimental conditions was presented 170 times per session. Regular trials were interspersed every fifth to seventh trial (equally probable) with a catch trial. In total, one EEG recording session consisted of 2720 regular trials and 541 catch trials, amounting to a total of 3261 trials. Two participants completed additional trials due to technical problems leading to a total of 190 and 180 repetitions per stimulus, accordingly.</p></sec><sec id="s4-5"><title>Braille screening task</title><p>Prior to the experiment, participants completed a short screening task during which each letter of the alphabet was presented for 500 ms to each hand in random order. Participants were asked to verbally report the letter they had perceived to assess their reading capabilities with both hands using the same presentation time as in the experiment. The average performance for the left hand was 89% correct (SD = 10) and for the right hand it was 88% correct (SD = 13).</p></sec><sec id="s4-6"><title>Behavioral letter similarity ratings</title><p>In a separate behavioral experiment, participants judged the perceived similarity of the braille letters used in the neuroimaging experiments. For this task, participants sat at a desk and were presented with two braille cells next to each other. Each pair of letters was presented once, and participants compared them with the same finger. We instructed participants to freely compare the similarity of pairs of Braille letters without specifying which parameters they should use for the similarity assessment. The rating was without time constraints, meaning participants decided when they rated the stimuli. Participants were asked to verbally rate the similarity of each pair of braille letters on a scale from 1 = very similar to 7 = very different and the experimenter noted down their responses.</p></sec><sec id="s4-7"><title>fMRI data acquisition, preprocessing, and preparation</title><sec id="s4-7-1"><title>fMRI acquisition</title><p>We acquired MRI data on a 3 T Siemens Tim Trio scanner with a 12-channel head coil. We obtained structural images using a T1-weighted sequence (magnetization-prepared rapid gradient-echo, 1 mm<sup>3</sup> voxel size). For the main experiment and the localizer run, we obtained functional images covering the entire brain using a T2*-weighted gradient-echo planar sequence (repetition time TR = 2 s, echo time TE = 30 ms, flip angle = 70°, 3 mm<sup>3</sup> voxel size, 37 slices, field of view FOV = 192 mm, matrix size = 64 × 64, interleaved acquisition).</p></sec><sec id="s4-7-2"><title>fMRI preprocessing</title><p>For fMRI preprocessing, we used tools from FMRIB’s Software Library (FSL, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">http://www.fmrib.ox.ac.uk/fsl</ext-link>). We excluded non-brain tissue from analysis using the Brain Extraction Tool (BET) (<xref ref-type="bibr" rid="bib83">Smith, 2002</xref>) and motion corrected the data using MCFLIRT (<xref ref-type="bibr" rid="bib53">Jenkinson et al., 2002</xref>). We did not apply high- or low-pass temporal filters and did not perform slice time correction. We spatially smoothed fMRI localizer data with an 8-mm FWHM Gaussian kernel. We registered functional images to the high-resolution structural scans and to the MNI standard template using FLIRT (<xref ref-type="bibr" rid="bib52">Jenkinson and Smith, 2001</xref>). We carried out all further fMRI analyses in MATLAB R2021a (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">https://www.mathworks.com/</ext-link>).</p></sec><sec id="s4-7-3"><title>Univariate fMRI analysis</title><p>For all univariate fMRI analyses, we used SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). For the main experiment, we modeled the fMRI responses to the 16 experimental conditions for each run using a general linear model (GLM). The onsets and durations of each image presentation entered the GLM as regressors and were convolved with a hemodynamic response function (hrf). Six movement parameters (pitch, yaw, roll, <italic>x</italic>-, <italic>y</italic>-, <italic>z</italic>-translation) entered the GLM as nuisance regressors. For each of the 16 conditions we converted GLM parameter estimates into <italic>t</italic>-values by contrasting each parameter estimate against the implicit baseline. This resulted in 16-condition-specific <italic>t</italic>-value maps per run and participant.</p><p>For the localizer experiment, we modeled the fMRI response to the five experimental conditions entering block onsets and durations as regressors of interest and movement parameters as nuisance regressors before convolving with the hrf. From the resulting three parameter estimates we generated two contrasts. The first contrast served to localize activations in primary (S1) and secondary (S2) somatosensory cortex and was defined as letters and fake letters &gt; baseline. The second contrast served to localize activations in EVC, V4, LOC, LFA, VWFA, aIPS, pIPS, and insula and was defined as letters &gt; fake letters. In sum, this resulted in two <italic>t</italic>-value maps for the localizer run per participant.</p></sec><sec id="s4-7-4"><title>Definition of ROIs</title><p>To identify regions along the sighted reading and tactile processing pathway, we defined ROIs in a two-step procedure. We first constrained ROIs by anatomical masks using brain atlases, in each case combining regions across both hemispheres. We included five ROIs from the sighted reading pathway: EVC (merging the anatomical masks of V1, V2, and V3), V4, LOC, LFA, and the VWFA. We also included five ROIs from the tactile processing pathway: S1, S2, aIPS (merging the anatomical masks of IPS3, IPS4, and IPS5), pIPS (merging the anatomical masks of IPS0, IPS1, and IPS2), and the insula. For EVC, V4, LOC, aIPS, and pIPS, we used masks from the probabilistic Wang atlas (<xref ref-type="bibr" rid="bib90">Wang et al., 2015</xref>). For LFA, we defined the mask using the MarsBaR Toolbox (<ext-link ext-link-type="uri" xlink:href="https://marsbar-toolbox.github.io/">https://marsbar-toolbox.github.io/</ext-link>) with a 10-mm radius around the center voxel at MNI coordinates <italic>X</italic> = −40, <italic>Y</italic> = −78, and <italic>Z</italic> = −18 (<xref ref-type="bibr" rid="bib86">Thesen et al., 2012</xref>). We also defined the VWFA mask using the MarsBaR Toolbox with a 10-mm radius around the center voxel at MNI coordinates <italic>X</italic> = −44, <italic>Y</italic> = −57, and <italic>Z</italic> = −13 (<xref ref-type="bibr" rid="bib32">Cohen et al., 2000</xref>) and converted from Talairach to MNI space using the MNI&lt;-&gt;Talairach Tool (<ext-link ext-link-type="uri" xlink:href="https://bioimagesuiteweb.github.io/bisweb-manual/tools/mni2tal.html">https://bioimagesuiteweb.github.io/bisweb-manual/tools/mni2tal.html</ext-link>). We created the mask for S1 by merging the sub-masks of BA1, BA2, and BA3 from the WFU PickAtlas (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/wfu_pickatlas/">https://www.nitrc.org/projects/wfu_pickatlas/</ext-link>) and the mask for S2 by merging the sub-masks operculum 1–4 from the Anatomy Toolbox (<xref ref-type="bibr" rid="bib42">Eickhoff et al., 2005</xref>). Lastly, we extracted the mask for the insula from the WFU PickAtlas. The smallest mask was V4 which included 321 voxels. Therefore, in a second step, we selected the 321 most activated voxels of the participant-specific localizer results within each of the masks, using the letters and fake letters &gt; baseline contrast for S1 and S2 and the letters &gt; fake letters for the remaining ROIs. This yielded participant-specific definitions for all ROIs.</p></sec></sec><sec id="s4-8"><title>EEG data acquisition and preprocessing</title><p>We recorded EEG data using an EASYCAP 64-channel system and a Brainvision actiCHamp amplifier at a sampling rate of 1000 Hz. The electrodes were placed according to the standard 10-10 system. The data were filtered online between 0.03 and 100 Hz and re-referenced online to FCz.</p><p>We preprocessed data offline using the EEGLAB toolbox version 14 (<xref ref-type="bibr" rid="bib35">Delorme and Makeig, 2004</xref>). We incorporated a low-pass filter with a cut-off at 50 Hz and epoched trials between −100 and 999 ms with respect to stimulus onset, resulting in 1100 1-ms data points per epoch. We baseline corrected the epochs by subtracting the mean of the 100-ms prestimulus time window from the epoch. We re-referenced the data offline to the average reference. To clean the data from artifacts such as eye blinks, eye movements, and muscular contractions, we used independent component analysis as implemented in the EEGLAB toolbox. We used SASICA (<xref ref-type="bibr" rid="bib27">Chaumon et al., 2015</xref>) to guide the visual inspection of components for removal. We identified components related to horizontal eye movements using two lateral frontal electrodes (F7–F8). During five recordings (one participant first session, four participants second session), additional external electrodes were available that allowed for the direct recording of the horizontal electro-oculogram to identify and remove components related to horizontal eye movements. For blink artifact detection based on the vertical electro-oculogram, we used two frontal electrodes (Fp1 and Fp2). As a final step, we applied multivariate noise normalization to improve the SNR and reliability of the data (<xref ref-type="bibr" rid="bib45">Guggenmos et al., 2018</xref>), resulting in subject-specific trial-based time courses of electrode activity.</p></sec><sec id="s4-9"><title>Braille letter classification from brain measurements</title><p>To determine the amount of information about braille letter identity present in brain measurements, we used a multivariate classification scheme (<xref ref-type="bibr" rid="bib29">Cichy et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Cichy et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Carlson et al., 2011a</xref>; <xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>). We conducted subject-specific braille letter classification in two ways. First, we classified between letter pairs presented to one reading hand, that is, we trained and tested a classifier on brain data recorded during the presentation of braille stimuli to the same hand (either the right or the left hand). This yields a measure of hand-dependent braille letter information in neural measurements. We refer to this analysis as within-hand classification. Second, we classified between letter pairs presented to different hands in that we trained a classifier on brain data recorded during the presentation of stimuli to one hand (e.g., right), and tested it on data related to the other hand (e.g., left). This yields a measure of hand-independent braille letter information in neural measurements. We refer to this analysis as across-hand classification. We tested both within- and across-hand pairwise classification accuracies against a chance level of 50%. We also calculated a within- to across-hand classification score which we compared against 0.</p><p>All classification analyses were carried out in MATLAB R2021a (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">https://www.mathworks.com/</ext-link>) and relied on binary c-support vector classification (C-SVC) with a linear kernel as implemented in the libsvm toolbox (<xref ref-type="bibr" rid="bib25">Chang and Lin, 2011</xref>; <ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/cjlin/libsvm">https://www.csie.ntu.edu.tw/cjlin/libsvm</ext-link>). Furthermore, all analyses were conducted in a participant specific manner. The next section describes the multivariate fMRI and EEG analyses in more detail.</p><sec id="s4-9-1"><title>Spatially resolved multivariate fMRI analysis</title><p>We conducted both an ROI-based and a spatially unbiased volumetric searchlight procedure (<xref ref-type="bibr" rid="bib54">Kriegeskorte et al., 2006</xref>). For each ROI included in the ROI-based analysis, we extracted and arranged <italic>t</italic>-values into pattern vectors for each of the 16 conditions and experimental runs. If participants completed only one session, the analysis was conducted on 10 runs. If participants completed both sessions, the 10 runs from session 1 and 15 runs from session 2 were pooled and the analysis was conducted across 25 runs. To increase the SNR, we randomly assigned run-wise pattern vectors into bins and averaged them into pseudo-runs. For participants with one session, the bin size was two runs, resulting in five pseudo-runs. If participants completed 2 sessions and thus had 25 runs, the bin size was 5 runs resulting in 5 pseudo-runs. Thus, in both cases, each participant ended up with five pseudo-run pattern vectors that entered the classification analysis. We then performed fivefold leave-one-pseudo-run-out-cross validation, training on four and testing on one pseudo-trial per classification iteration.</p><p>We will first describe the classification procedure for braille letters within-hand and then for the classification of braille letters across-hand.</p></sec><sec id="s4-9-2"><title>fMRI ROI-based classification of braille letters within-hand</title><p>For the classification of braille letters within-hand, we assigned four pseudo-trials corresponding to the data from two braille letters of the same hand (e.g., right) to the training set. We then tested the support vector machine (SVM) on the remaining, fifth pseudo-trial corresponding to data from the same two braille letters of the same hand (e.g., right) as in the training set but using held-out data for the testing set. This yielded percent classification accuracy (50% chance level) as output. Equivalent SVM training and testing were repeated for all combinations of letter pairs within each hand.</p><p>With 8 letters that were all classified pairwise once per hand, this resulted in 28 pairwise classification accuracies per hand. We averaged accuracies across condition pairs and hands, yielding a measure of hand-dependent braille letter information for each ROI and participant separately.</p></sec><sec id="s4-9-3"><title>fMRI ROI-based classification of braille letters across-hand</title><p>The classification procedure of braille letters across reading hands was identical to the classification procedure within-hand with the important difference that the training data always came from one hand (e.g., right) and the testing data from the other hand (e.g., left).</p><p>With 8 letters that were all classified pairwise once across two hands, this resulted again in 28 pairwise classification accuracies across-hand per training–testing direction (i.e., train left, test right, and vice versa). We averaged accuracies across condition pairs and training–testing directions, yielding a measure of hand-independent braille letter information for each ROI and participant separately.</p></sec><sec id="s4-9-4"><title>fMRI searchlight classification of braille letters</title><p>The searchlight procedure was conceptually equivalent to the ROI-based analysis. For each voxel v<sub>i</sub> in the 3D <italic>t</italic>-value maps, we defined a sphere with a radius of four voxels centered around voxel v<sub>i</sub>. For each condition and run, we extracted and arranged the <italic>t</italic>-values for each voxel of the sphere into pattern vectors. Classification of braille letters across-hand proceeded as described above. This resulted in one average classification accuracy for voxel v<sub>i</sub>. Iterated across all voxels this yielded a 3D volume of classification accuracies across the brain for each participant separately.</p></sec><sec id="s4-9-5"><title>Time-resolved classification of braille letters within-hand from EEG data</title><p>To determine the timing with which braille letter information emerges in the brain, we conducted time-resolved EEG classification (<xref ref-type="bibr" rid="bib51">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Carlson et al., 2011b</xref>). This procedure was conceptually equivalent to the fMRI braille letter classification in that it classified letter pairs either within- or across-hand and was conducted separately for each participant.</p><p>For each time point of the epoched EEG data, we extracted 63 EEG channel activations and arranged them into pattern vectors for each of the 16 conditions. Participants who completed one session had 170 trials per condition and participants who completed two sessions had 340 trials per condition. To increase the SNR, we randomly assigned the trials into bins and averaged them into new pseudo-trials. For participants with one session, the bin size was 34 trials, resulting in 5 pseudo-trials. If participants completed 2 sessions and thus had 340 trials, the bin size was 68 trials resulting in 5 pseudo-trials. In both cases, each participant ended up with five pseudo-run pattern vectors that entered the classification analysis. We then performed fivefold leave-one-pseudo-run-out-cross validation, training on four and testing on one pseudo-trial per classification iteration. This procedure was repeated 100 times with random assignment of trials to pseudo-trials, and across all combinations of letter pairs and hands. We averaged results across condition pairs, folds, iterations, and hands, yielding a decoding accuracy time course reflecting how much hand-dependent braille letter information was present at each time point in each participant.</p></sec><sec id="s4-9-6"><title>Time-resolved classification of braille letters across-hand from EEG data</title><p>The classification procedure for braille letters across-hand was identical to the classification of braille letters within-hand with the crucial difference that training and testing data always came from separate hands and results were averaged across condition pairs, folds, iterations, and training–testing directions. Averaging results yielded a decoding accuracy time course reflecting how much hand-independent braille letter information was present at each time point in each participant.</p></sec><sec id="s4-9-7"><title>Time-resolved EEG searchlight in sensor space</title><p>We conducted an EEG searchlight analysis resolved in time and sensor space (i.e., across 63 EEG channels) to gain insights into which EEG channels contributed to the results of the time-resolved analysis described above. For the EEG searchlight, we conducted the time-resolved EEG classification as described above with the following difference: For each EEG channel c<sub>i</sub>, we conducted the classification procedure on the four closest channels surrounding c. The classification accuracy was stored at the position of c. After iterating across all channels and down-sampling the time points to a 10-ms resolution, this yielded a classification accuracy map across all channels and time points in 10 ms steps for each participant.</p></sec></sec><sec id="s4-10"><title>RSA of brain data and behavioral letter similarity ratings</title><p>To determine the subset of neural braille letter representations identified that is relevant for behavior (<xref ref-type="bibr" rid="bib31">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Mur et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Charest et al., 2014</xref>), we compared perceptual letter similarity ratings to braille letter representations identified from EEG and fMRI signals using RSA (<xref ref-type="bibr" rid="bib55">Kriegeskorte et al., 2008</xref>). RSA characterizes the representational space of a measurement space (e.g., fMRI or EEG data) with an RDM. RDMs aggregate pairwise distances between responses to all experimental conditions, thereby abstracting from the activity patterns of measurement units (e.g., fMRI voxels or EEG channels) to between-condition dissimilarities. The rationale of the approach is that neural measures and behavior are linked if their RDMs are similar.</p><p>We constructed RDMs for behavior, fMRI and EEG as follows.</p><p>For behavior, we arranged the perceptual similarity judgments averaged across participants (indicated by participants on a scale from 1 = very similar to 7 = very different) into an RDM format. All RDMs were averaged over both hands and had the dimensions 8 letters × 8 letters.</p><p>For both fMRI and EEG, we used the classification results from the conducted within- and across-hand classifications as a measure of (dis-)similarity relations between braille letters. Classification accuracies can be interpreted as a measure of dissimilarity because two conditions have a higher classification accuracy when they are more dissimilar (<xref ref-type="bibr" rid="bib30">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Guggenmos et al., 2018</xref>). Thus, we assembled participant-specific RDMs for each fMRI ROI and EEG time point in the time course from decoding accuracies.</p><p>In a final step we correlated (Spearman’s <italic>R</italic>) the lower triangular part of the respective RDMs (without the diagonal; <xref ref-type="bibr" rid="bib52">Jenkinson and Smith, 2001</xref>). For fMRI, this resulted in one correlation value per participant and ROI. For EEG, this analysis resulted in one correlation time course per participant.</p></sec><sec id="s4-11"><title>Statistical testing</title><sec id="s4-11-1"><title>Wilcoxon signed-rank test</title><p>We performed non-parametric one-tailed Wilcoxon signed-rank tests to test for above-chance classification accuracy for ROIs in the fMRI classification, for time points in the EEG classification, for time points and channels in the EEG searchlight, and for ROI and time courses in the RSA. In each case, the null hypothesis was that the observed parameter (i.e., classification accuracy, correlation) came from a distribution with a median of chance level performance (i.e., 50% for pairwise classification; 0 correlation). The resulting p-values were corrected for multiple comparisons using the FDR (<xref ref-type="bibr" rid="bib9">Benjamini and Hochberg, 1995</xref>) at 5% level if more than one test was conducted. This was done (1) across ROIs in the ROI classification and fMRI-behavior RSA, (2) across time points in the EEG classification and EEG-behavior RSA, and (3) across time points and channels in the EEG searchlight.</p></sec><sec id="s4-11-2"><title>Bootstrap tests</title><p>We used bootstrapping to compute 95% confidence intervals for onset latencies (the first 50 consecutive significant time points after trial onset) of EEG time courses as well as for determining the significance of onset latencies. In each case, we sampled the participant pool 1000 times with replacement calculated the statistic of interest for each sample.</p><p>For the EEG onset latency differences, we bootstrapped the latency difference between the onsets of the time courses of hand-dependent or -independent letter representations. This yielded an empirical distribution that could be compared to zero. To determine whether onset latencies differences in the EEG time courses were significantly different from zero, we computed the proportion of values that were equal to or smaller than zero and corrected them for multiple comparisons using FDR at p = 0.05.</p></sec><sec id="s4-11-3"><title>Other statistical tests</title><p>For the fMRI searchlight classification results, we applied a voxel-wise height threshold of p = 0.001. The resulting p-values were corrected for multiple comparisons using the FWE at the 5% level.</p></sec></sec><sec id="s4-12"><title>Code availability</title><p>The code used in this study is available on Github via <ext-link ext-link-type="uri" xlink:href="https://github.com/marleenhaupt/BrailleLetterRepresentations/">https://github.com/marleenhaupt/BrailleLetterRepresentations/</ext-link> (copy archived at <xref ref-type="bibr" rid="bib47">Haupt, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Methodology, Project administration</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the ethics committee of the Department of Education and Psychology of the Freie Universität Berlin and were conducted in accordance with the Declaration of Helsinki. All participants provided informed consent prior to the studies and received a monetary reward for their participation.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Participant information.</title></caption><media xlink:href="elife-98148-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98148-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The raw fMRI and EEG data are available on OpenNeuro via <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004956">https://openneuro.org/datasets/ds004956</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004951/">https://openneuro.org/datasets/ds004951/</ext-link>. The preprocessed fMRI, EEG, and behavioral data as well as the results of the ROI classification, time classification, and RSA can be accessed on OSF via <ext-link ext-link-type="uri" xlink:href="https://osf.io/a64hp/">https://osf.io/a64hp/</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Haupt</surname><given-names>M</given-names></name><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Teng</surname><given-names>S</given-names></name><name><surname>Kaltenbach</surname><given-names>C</given-names></name><name><surname>MCichy</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Braille letters - fMRI</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds004956.v1.0.1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Haupt</surname><given-names>M</given-names></name><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Teng</surname><given-names>S</given-names></name><name><surname>MCichy</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Braille letters - EEG</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds004951.v1.0.0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Haupt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>The transformation of sensory to perceptual braille letter representations in the visually deprived brain</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/A64HP</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank all of our participants for taking part in our experiments. We also thank Agnessa Karapetian, Johannes Singer, and Siying Xie for their valuable comments on the manuscript. We acquired EEG and fMRI data at the Center for Cognitive Neuroscience (CCNB), Freie Universität Berlin, and we thank the HPC Service of ZEDAT, Freie Universität Berlin, for computing time (<xref ref-type="bibr" rid="bib10">Bennett et al., 2020</xref>).The study was supported by German Research Council grants (CI241/1-1, CI241/3-1, CI241/7-1) and European Research Council grants (ERC-StG-2018-803370) to R.M.C. The funders had no role in study design, data collection, and analysis, decision to publish or preparation of the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abboud</surname><given-names>S</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Semantic coding in the occipital cortex of early blind individuals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/539437</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Peled</surname><given-names>S</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visuo-haptic object-related activation in the ventral visual pathway</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>324</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1038/85201</pub-id><pub-id pub-id-type="pmid">11224551</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Raz</surname><given-names>N</given-names></name><name><surname>Pianka</surname><given-names>P</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Early “visual” cortex activation correlates with superior verbal memory performance in the blind</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>758</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1038/nn1072</pub-id><pub-id pub-id-type="pmid">12808458</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banati</surname><given-names>RB</given-names></name><name><surname>Goerres</surname><given-names>GW</given-names></name><name><surname>Tjoa</surname><given-names>C</given-names></name><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Grasby</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The functional anatomy of visual-tactile integration in man: a study using positron emission tomography</article-title><source>Neuropsychologia</source><volume>38</volume><fpage>115</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/s0028-3932(99)00074-3</pub-id><pub-id pub-id-type="pmid">10660224</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title><source>NeuroImage</source><volume>178</volume><fpage>172</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.037</pub-id><pub-id pub-id-type="pmid">29777825</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname><given-names>M</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Dodell-Feder</surname><given-names>D</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Language processing in the occipital cortex of congenitally blind adults</article-title><source>PNAS</source><volume>108</volume><fpage>4429</fpage><lpage>4434</lpage><pub-id pub-id-type="doi">10.1073/pnas.1014818108</pub-id><pub-id pub-id-type="pmid">21368161</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence from blindness for a cognitively pluripotent cortex</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>637</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.06.003</pub-id><pub-id pub-id-type="pmid">28821345</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>How Does Visual Experience Shape Representations and Transformations along the Ventral Stream?</source><publisher-name>CCN Gener Advers</publisher-name></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: A practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bennett</surname><given-names>L</given-names></name><name><surname>Melchers</surname><given-names>B</given-names></name><name><surname>Proppe</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curta: a general-purpose high-performance 822 computer at ZEDAT</article-title><source>Berlin</source><volume>1</volume><elocation-id>e26754</elocation-id><pub-id pub-id-type="doi">10.17169/refubium-26754</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bola</surname><given-names>Ł</given-names></name><name><surname>Siuda-Krzywicka</surname><given-names>K</given-names></name><name><surname>Paplińska</surname><given-names>M</given-names></name><name><surname>Sumera</surname><given-names>E</given-names></name><name><surname>Hańczur</surname><given-names>P</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Braille in the sighted: Teaching tactile reading to sighted adults</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0155394</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0155394</pub-id><pub-id pub-id-type="pmid">27187496</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bola</surname><given-names>Ł</given-names></name><name><surname>Matuszewski</surname><given-names>J</given-names></name><name><surname>Szczepanik</surname><given-names>M</given-names></name><name><surname>Droździel</surname><given-names>D</given-names></name><name><surname>Sliwinska</surname><given-names>MW</given-names></name><name><surname>Paplińska</surname><given-names>M</given-names></name><name><surname>Jednoróg</surname><given-names>K</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name><name><surname>Marchewka</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional hierarchy for tactile processing in the visual cortex of sighted adults</article-title><source>NeuroImage</source><volume>202</volume><elocation-id>116084</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116084</pub-id><pub-id pub-id-type="pmid">31400530</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonda</surname><given-names>E</given-names></name><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural systems for tactual memories</article-title><source>Journal of Neurophysiology</source><volume>75</volume><fpage>1730</fpage><lpage>1737</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.75.4.1730</pub-id><pub-id pub-id-type="pmid">8727409</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bragg</surname><given-names>D</given-names></name><name><surname>Bennett</surname><given-names>C</given-names></name><name><surname>Reinecke</surname><given-names>K</given-names></name><name><surname>Ladner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A large inclusive study of human listening rates</article-title><conf-name>CHI ’18</conf-name><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3173574.3174018</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How many words do we read per minute? A review and meta-analysis of reading rate</article-title><source>Journal of Memory and Language</source><volume>109</volume><elocation-id>104047</elocation-id><pub-id pub-id-type="doi">10.1016/j.jml.2019.104047</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Büchel</surname><given-names>C</given-names></name><name><surname>Price</surname><given-names>C</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A multimodal language region in the ventral visual pathway</article-title><source>Nature</source><volume>394</volume><fpage>274</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1038/28389</pub-id><pub-id pub-id-type="pmid">9685156</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>H</given-names></name><name><surname>Sinclair</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attending to and remembering tactile stimuli</article-title><source>Journal of Clinical Neurophysiology</source><volume>17</volume><fpage>575</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1097/00004691-200011000-00004</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>H</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Diamond</surname><given-names>JB</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Adaptive changes in early and late blind: a FMRI study of verb generation to heard nouns</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>3359</fpage><lpage>3371</lpage><pub-id pub-id-type="doi">10.1152/jn.00129.2002</pub-id><pub-id pub-id-type="pmid">12466452</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003a</year><article-title>Visual cortex activity in early and late blind people</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>4005</fpage><lpage>4011</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-10-04005.2003</pub-id><pub-id pub-id-type="pmid">12764085</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>H</given-names></name><name><surname>Diamond</surname><given-names>JB</given-names></name><name><surname>McDermott</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="2003">2003b</year><article-title>Dissociating cortical regions activated by semantic and phonological tasks: a FMRI study in blind and sighted people</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>1965</fpage><lpage>1982</lpage><pub-id pub-id-type="doi">10.1152/jn.00279.2003</pub-id><pub-id pub-id-type="pmid">12789013</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>H</given-names></name><name><surname>Sinclair</surname><given-names>RJ</given-names></name><name><surname>Agato</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Recognition memory for Braille or spoken words: an fMRI study in early blind</article-title><source>Brain Research</source><volume>1438</volume><fpage>22</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2011.12.032</pub-id><pub-id pub-id-type="pmid">22251836</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name><name><surname>Fonteijn</surname><given-names>H</given-names></name><name><surname>Verstraten</surname><given-names>FAJ</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Spatial coding and invariance in object-selective cortex</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>47</volume><fpage>14</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2009.08.015</pub-id><pub-id pub-id-type="pmid">19833329</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name><name><surname>Kanai</surname><given-names>R</given-names></name><name><surname>Mesik</surname><given-names>J</given-names></name><name><surname>Turret</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>High temporal resolution decoding of object position and category</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/11.10.9</pub-id><pub-id pub-id-type="pmid">21920851</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Libsvm: a library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name><name><surname>Schmitz</surname><given-names>TW</given-names></name><name><surname>Deca</surname><given-names>D</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Unique semantic space in the brain of each beholder predicts perceived similarity</article-title><source>PNAS</source><volume>111</volume><fpage>14565</fpage><lpage>14570</lpage><pub-id pub-id-type="doi">10.1073/pnas.1402594111</pub-id><pub-id pub-id-type="pmid">25246586</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaumon</surname><given-names>M</given-names></name><name><surname>Bishop</surname><given-names>DVM</given-names></name><name><surname>Busch</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A practical guide to the selection of independent components of the electroencephalogram for artifact correction</article-title><source>Journal of Neuroscience Methods</source><volume>250</volume><fpage>47</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.02.025</pub-id><pub-id pub-id-type="pmid">25791012</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Encoding the identity and location of objects in human LOC</article-title><source>NeuroImage</source><volume>54</volume><fpage>2297</fpage><lpage>2307</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.044</pub-id><pub-id pub-id-type="pmid">20869451</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Heinzle</surname><given-names>J</given-names></name><name><surname>Elliott</surname><given-names>LT</given-names></name><name><surname>Ramirez</surname><given-names>F</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probing principles of large-scale object representation: category preference and location encoding</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>1636</fpage><lpage>1651</lpage><pub-id pub-id-type="doi">10.1002/hbm.22020</pub-id><pub-id pub-id-type="pmid">22371355</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The spatiotemporal neural dynamics underlying perceived similarity for real-world objects</article-title><source>NeuroImage</source><volume>194</volume><fpage>12</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.031</pub-id><pub-id pub-id-type="pmid">30894333</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Lehéricy</surname><given-names>S</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Hénaff</surname><given-names>MA</given-names></name><name><surname>Michel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients</article-title><source>Brain</source><volume>123 ( Pt 2)</volume><fpage>291</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1093/brain/123.2.291</pub-id><pub-id pub-id-type="pmid">10648437</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Vinckier</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The neural code for written words: a proposal</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>335</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.05.004</pub-id><pub-id pub-id-type="pmid">15951224</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The unique role of the visual word form area in reading</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>254</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.04.003</pub-id><pub-id pub-id-type="pmid">21592844</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deWit</surname><given-names>L</given-names></name><name><surname>Alexander</surname><given-names>D</given-names></name><name><surname>Ekroll</surname><given-names>V</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Is neuroimaging measuring information in the brain?</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1415</fpage><lpage>1428</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1002-0</pub-id><pub-id pub-id-type="pmid">26833316</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietrich</surname><given-names>S</given-names></name><name><surname>Hertrich</surname><given-names>I</given-names></name><name><surname>Ackermann</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ultra-fast speech comprehension in blind subjects engages primary visual cortex, fusiform gyrus, and pulvinar - a functional magnetic resonance imaging (fMRI) study</article-title><source>BMC Neuroscience</source><volume>14</volume><elocation-id>74</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-14-74</pub-id><pub-id pub-id-type="pmid">23879896</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Haan</surname><given-names>EHF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Somatosensory processes subserving perception and action</article-title><source>The Behavioral and Brain Sciences</source><volume>30</volume><fpage>189</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1017/S0140525X07001392</pub-id><pub-id pub-id-type="pmid">17705910</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Shared neural mechanisms of visual perception and imagery</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>423</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.02.004</pub-id><pub-id pub-id-type="pmid">30876729</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Grefkes</surname><given-names>C</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title><source>NeuroImage</source><volume>25</volume><fpage>1325</fpage><lpage>1335</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.034</pub-id><pub-id pub-id-type="pmid">15850749</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Englebretson</surname><given-names>R</given-names></name><name><surname>Holbrook</surname><given-names>MC</given-names></name><name><surname>Fischer-Baum</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A position paper on researching braille in the cognitive sciences: decentering the sighted norm</article-title><source>Applied Psycholinguistics</source><volume>44</volume><fpage>400</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1017/S0142716423000061</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Separate visual pathways for perception and action</article-title><source>Trends in Neurosciences</source><volume>15</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90344-8</pub-id><pub-id pub-id-type="pmid">1374953</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname><given-names>M</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multivariate pattern analysis for MEG: a comparison of dissimilarity measures</article-title><source>NeuroImage</source><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.02.044</pub-id><pub-id pub-id-type="pmid">29499313</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hadjikhani</surname><given-names>N</given-names></name><name><surname>Roland</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cross-modal transfer of information between the tactile and the visual representations in the human brain: a positron emission tomographic study</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>1072</fpage><lpage>1084</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-03-01072.1998</pub-id><pub-id pub-id-type="pmid">9437027</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Haupt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>BrailleLetterRepresentations</data-title><version designator="swh:1:rev:7f7610c2a06a2442897996bcc4fafaa4e46d52cd">swh:1:rev:7f7610c2a06a2442897996bcc4fafaa4e46d52cd</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:c3ab67c84a196847be9b245dec1a605d51ac70f9;origin=https://github.com/marleenhaupt/BrailleLetterRepresentations;visit=swh:1:snp:34941b1e2d4921191d23fa87123c74665982a0f5;anchor=swh:1:rev:7f7610c2a06a2442897996bcc4fafaa4e46d52cd">https://archive.softwareheritage.org/swh:1:dir:c3ab67c84a196847be9b245dec1a605d51ac70f9;origin=https://github.com/marleenhaupt/BrailleLetterRepresentations;visit=swh:1:snp:34941b1e2d4921191d23fa87123c74665982a0f5;anchor=swh:1:rev:7f7610c2a06a2442897996bcc4fafaa4e46d52cd</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>JD</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Decoding mental states from brain activity in humans</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>523</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1038/nrn1931</pub-id><pub-id pub-id-type="pmid">16791142</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hötting</surname><given-names>K</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Auditory and auditory-tactile processing in congenitally blind humans</article-title><source>Hearing Research</source><volume>258</volume><fpage>165</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2009.07.012</pub-id><pub-id pub-id-type="pmid">19651199</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannides</surname><given-names>AA</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Poghosyan</surname><given-names>V</given-names></name><name><surname>Saridis</surname><given-names>GA</given-names></name><name><surname>Gjedde</surname><given-names>A</given-names></name><name><surname>Ptito</surname><given-names>M</given-names></name><name><surname>Kupers</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG reveals a fast pathway from somatosensory cortex to occipital areas via posterior parietal cortex in a blind subject</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>429</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00429</pub-id><pub-id pub-id-type="pmid">23935576</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A global optimisation method for robust affine registration of brain images</article-title><source>Medical Image Analysis</source><volume>5</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/s1361-8415(01)00036-6</pub-id><pub-id pub-id-type="pmid">11516708</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lane</surname><given-names>C</given-names></name><name><surname>Kanjlia</surname><given-names>S</given-names></name><name><surname>Omaki</surname><given-names>A</given-names></name><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>“Visual” cortex of congenitally blind adults responds to syntactic movement</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>12859</fpage><lpage>12868</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1256-15.2015</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ling</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>ACH</given-names></name><name><surname>Armstrong</surname><given-names>BC</given-names></name><name><surname>Nestor</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How are visual words represented? Insights from EEG-based visual word decoding, feature derivation and image reconstruction</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>5056</fpage><lpage>5068</lpage><pub-id pub-id-type="doi">10.1002/hbm.24757</pub-id><pub-id pub-id-type="pmid">31403749</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>YF</given-names></name><name><surname>Rapp</surname><given-names>B</given-names></name><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Reading braille by touch recruits posterior parietal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>35</volume><fpage>1593</fpage><lpage>1616</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_02041</pub-id><pub-id pub-id-type="pmid">37584592</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>MX</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Lahner</surname><given-names>B</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Teng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cochlea to categories: the spatiotemporal dynamics of semantic auditory representations</article-title><source>Cognitive Neuropsychology</source><volume>38</volume><fpage>468</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1080/02643294.2022.2085085</pub-id><pub-id pub-id-type="pmid">35729704</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname><given-names>TR</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Against cortical reorganisation</article-title><source>eLife</source><volume>12</volume><elocation-id>e84716</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.84716</pub-id><pub-id pub-id-type="pmid">37986628</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattioni</surname><given-names>S</given-names></name><name><surname>Rezk</surname><given-names>M</given-names></name><name><surname>Battal</surname><given-names>C</given-names></name><name><surname>Bottini</surname><given-names>R</given-names></name><name><surname>Cuculiza Mendoza</surname><given-names>KE</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Categorical representation from sound and sight in the ventral occipito-temporal cortex of sighted and blind</article-title><source>eLife</source><volume>9</volume><elocation-id>e50732</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50732</pub-id><pub-id pub-id-type="pmid">32108572</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCandliss</surname><given-names>BD</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The visual word form area: expertise for reading in the fusiform gyrus</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>293</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00134-7</pub-id><pub-id pub-id-type="pmid">12860187</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural reorganization following sensory loss: the opportunity of change</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>44</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1038/nrn2758</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muchnik</surname><given-names>C</given-names></name><name><surname>Efrati</surname><given-names>M</given-names></name><name><surname>Nemeth</surname><given-names>E</given-names></name><name><surname>Malin</surname><given-names>M</given-names></name><name><surname>Hildesheimer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Central auditory skills in blind and sighted subjects</article-title><source>Scandinavian Audiology</source><volume>20</volume><fpage>19</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.3109/01050399109070785</pub-id><pub-id pub-id-type="pmid">1842264</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>F</given-names></name><name><surname>Niso</surname><given-names>G</given-names></name><name><surname>Samiee</surname><given-names>S</given-names></name><name><surname>Ptito</surname><given-names>M</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Kupers</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A thalamocortical pathway for fast rerouting of tactile information to occipital cortex in congenital blindness</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5154</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13173-7</pub-id><pub-id pub-id-type="pmid">31727882</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Meys</surname><given-names>M</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>128</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00128</pub-id><pub-id pub-id-type="pmid">23525516</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naghavi</surname><given-names>HR</given-names></name><name><surname>Eriksson</surname><given-names>J</given-names></name><name><surname>Larsson</surname><given-names>A</given-names></name><name><surname>Nyberg</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The claustrum/insula region integrates conceptually related sounds and pictures</article-title><source>Neuroscience Letters</source><volume>422</volume><fpage>77</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2007.06.009</pub-id><pub-id pub-id-type="pmid">17597301</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neudorf</surname><given-names>J</given-names></name><name><surname>Gould</surname><given-names>L</given-names></name><name><surname>Mickleborough</surname><given-names>MJS</given-names></name><name><surname>Ekstrand</surname><given-names>C</given-names></name><name><surname>Borowsky</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Unique, shared, and dominant brain activation in visual word form area and lateral occipital complex during reading and picture naming</article-title><source>Neuroscience</source><volume>481</volume><fpage>178</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2021.11.022</pub-id><pub-id pub-id-type="pmid">34800577</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Hamilton</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The metamodal organization of the brain</article-title><source>Progress in Brain Research</source><volume>134</volume><fpage>427</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1016/s0079-6123(01)34028-1</pub-id><pub-id pub-id-type="pmid">11702559</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Fregni</surname><given-names>F</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The plastic human brain cortex</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>377</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144216</pub-id><pub-id pub-id-type="pmid">16022601</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prather</surname><given-names>SC</given-names></name><name><surname>Votaw</surname><given-names>JR</given-names></name><name><surname>Sathian</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Task-specific recruitment of dorsal and ventral visual areas during tactile perception</article-title><source>Neuropsychologia</source><volume>42</volume><fpage>1079</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2003.12.013</pub-id><pub-id pub-id-type="pmid">15093147</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rączy</surname><given-names>K</given-names></name><name><surname>Urbańczyk</surname><given-names>A</given-names></name><name><surname>Korczyk</surname><given-names>M</given-names></name><name><surname>Szewczyk</surname><given-names>JM</given-names></name><name><surname>Sumera</surname><given-names>E</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Orthographic priming in braille reading as evidence for task-specific reorganization in the ventral visual cortex of the congenitally blind</article-title><source>Journal of Cognitive Neuroscience</source><volume>31</volume><fpage>1065</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01407</pub-id><pub-id pub-id-type="pmid">30938589</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Category selectivity in the ventral visual pathway confers robustness to clutter and diverted attention</article-title><source>Current Biology</source><volume>17</volume><fpage>2067</fpage><lpage>2072</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.10.043</pub-id><pub-id pub-id-type="pmid">17997310</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reed</surname><given-names>CL</given-names></name><name><surname>Shoham</surname><given-names>S</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neural substrates of tactile object recognition: an fMRI study</article-title><source>Human Brain Mapping</source><volume>21</volume><fpage>236</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1002/hbm.10162</pub-id><pub-id pub-id-type="pmid">15038005</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname><given-names>L</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A ventral visual stream reading center independent of visual experience</article-title><source>Current Biology</source><volume>21</volume><fpage>363</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.01.040</pub-id><pub-id pub-id-type="pmid">21333539</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Bottari</surname><given-names>D</given-names></name><name><surname>Ptito</surname><given-names>M</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The sensory-deprived brain as a unique tool to understand brain development and function</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>108</volume><fpage>78</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.10.017</pub-id><pub-id pub-id-type="pmid">31672616</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Rösler</surname><given-names>F</given-names></name><name><surname>Hennighausen</surname><given-names>E</given-names></name><name><surname>Näcker</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Event-related potentials during auditory and somatosensory discrimination in sighted and blind human subjects</article-title><source>Brain Research. Cognitive Brain Research</source><volume>4</volume><fpage>77</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/0926-6410(96)00024-9</pub-id><pub-id pub-id-type="pmid">8883921</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Stock</surname><given-names>O</given-names></name><name><surname>Bien</surname><given-names>S</given-names></name><name><surname>Neville</surname><given-names>H</given-names></name><name><surname>Rösler</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Speech processing activates visual cortex in congenitally blind humans</article-title><source>The European Journal of Neuroscience</source><volume>16</volume><fpage>930</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.02147.x</pub-id><pub-id pub-id-type="pmid">12372029</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Activation of V1 by Braille reading in blind subjects</article-title><source>Nature</source><volume>380</volume><fpage>526</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1038/380526a0</pub-id><pub-id pub-id-type="pmid">8606771</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Grafman</surname><given-names>J</given-names></name><name><surname>Deiber</surname><given-names>MP</given-names></name><name><surname>Ibañez</surname><given-names>V</given-names></name><name><surname>Hallett</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural networks for Braille reading by the blind</article-title><source>Brain</source><volume>121 ( Pt 7)</volume><fpage>1213</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1093/brain/121.7.1213</pub-id><pub-id pub-id-type="pmid">9679774</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez-Panchuelo</surname><given-names>RM</given-names></name><name><surname>Besle</surname><given-names>J</given-names></name><name><surname>Beckett</surname><given-names>A</given-names></name><name><surname>Bowtell</surname><given-names>R</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Francis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Within-digit functional parcellation of Brodmann areas of the human primary somatosensory cortex using functional magnetic resonance imaging at 7 tesla</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>15815</fpage><lpage>15822</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2501-12.2012</pub-id><pub-id pub-id-type="pmid">23136420</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seydell-Greenwald</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name><name><surname>Bi</surname><given-names>Y</given-names></name><name><surname>Striem-Amit</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spoken language processing activates the primary visual cortex</article-title><source>PLOS ONE</source><volume>18</volume><elocation-id>e0289671</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0289671</pub-id><pub-id pub-id-type="pmid">37566582</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fast robust automated brain extraction</article-title><source>Human Brain Mapping</source><volume>17</volume><fpage>143</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1002/hbm.10062</pub-id><pub-id pub-id-type="pmid">12391568</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname><given-names>E</given-names></name><name><surname>Dakwar</surname><given-names>O</given-names></name><name><surname>Reich</surname><given-names>L</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The large-scale organization of “visual” streams emerges without visual experience</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>1698</fpage><lpage>1709</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr253</pub-id><pub-id pub-id-type="pmid">21940707</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>S</given-names></name><name><surname>Cichy</surname><given-names>R</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural dynamics of letter perception in blind and sighted readers</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>126</elocation-id><pub-id pub-id-type="doi">10.1167/15.12.126</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>McDonald</surname><given-names>CR</given-names></name><name><surname>Carlson</surname><given-names>C</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Cash</surname><given-names>S</given-names></name><name><surname>Sherfey</surname><given-names>J</given-names></name><name><surname>Felsovalyi</surname><given-names>O</given-names></name><name><surname>Girard</surname><given-names>H</given-names></name><name><surname>Barr</surname><given-names>W</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Kuzniecky</surname><given-names>R</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sequential then interactive processing of letters and words in the left fusiform gyrus</article-title><source>Nature Communications</source><volume>3</volume><elocation-id>1284</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2220</pub-id><pub-id pub-id-type="pmid">23250414</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uddin</surname><given-names>LQ</given-names></name><name><surname>Nomi</surname><given-names>JS</given-names></name><name><surname>Hébert-Seropian</surname><given-names>B</given-names></name><name><surname>Ghaziri</surname><given-names>J</given-names></name><name><surname>Boucher</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Structure and function of the human insula</article-title><source>Journal of Clinical Neurophysiology</source><volume>34</volume><fpage>300</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1097/WNP.0000000000000377</pub-id><pub-id pub-id-type="pmid">28644199</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhl</surname><given-names>F</given-names></name><name><surname>Franzen</surname><given-names>P</given-names></name><name><surname>Lindinger</surname><given-names>G</given-names></name><name><surname>Lang</surname><given-names>W</given-names></name><name><surname>Deecke</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>On the functionality of the visually deprived occipital cortex in early blind persons</article-title><source>Neuroscience Letters</source><volume>124</volume><fpage>256</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1016/0304-3940(91)90107-5</pub-id><pub-id pub-id-type="pmid">2067724</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>P</given-names></name><name><surname>Bola</surname><given-names>Ł</given-names></name><name><surname>Reich</surname><given-names>L</given-names></name><name><surname>Bennett</surname><given-names>M</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Decoding natural sounds in early “visual” cortex of congenitally blind individuals</article-title><source>Current Biology</source><volume>30</volume><fpage>3039</fpage><lpage>3044</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.05.071</pub-id><pub-id pub-id-type="pmid">32559449</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Mruczek</surname><given-names>REB</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>KE</given-names></name><name><surname>Coullon</surname><given-names>GSL</given-names></name><name><surname>Bridge</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Language and nonverbal auditory processing in the occipital cortex of individuals who are congenitally blind due to anophthalmia</article-title><source>Neuropsychologia</source><volume>173</volume><elocation-id>108304</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2022.108304</pub-id><pub-id pub-id-type="pmid">35716797</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Dang</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Only some spatial patterns of fMRI response are read out in task performance</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>685</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1038/nn1900</pub-id><pub-id pub-id-type="pmid">17486103</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98148.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>SP</surname><given-names>Arun</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study investigates the brain representations of Braille letters in blind participants and provides evidence using EEG and fMRI that the decoding of letter identity across the reading hand takes place in the visual cortex. The evidence supporting the claims of the authors is <bold>convincing</bold> and the work will be of interest to neuroscientists working on brain plasticity.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98148.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The researchers examined how individuals who were born blind or lost their vision early in life process information, specifically focusing on the decoding of Braille characters. They explored the transition of Braille character information from tactile sensory inputs, based on which hand was used for reading, to perceptual representations that are not dependent on the reading hand.</p><p>They identified tactile sensory representations in areas responsible for touch processing and perceptual representations in brain regions typically involved in visual reading, with the lateral occipital complex serving as a pivotal &quot;hinge&quot; region between them.</p><p>In terms of temporal information processing, they discovered that tactile sensory representations occur prior to cognitive perceptual representations. The researchers suggest that this pattern indicates that even in situations of significant brain adaptability, there is a consistent chronological progression from sensory to cognitive processing.</p><p>Strengths:</p><p>By combining fMRI and EEG, and focusing on the diagnostic case of Braille reading, the paper provides an integrated view of the transformation processing from sensation to perception in the visually deprived brain. Such a multimodal approach is still rare in the study of human brain plasticity and allows to discern the nature of information processing in blind people early visual cortex, as well as the timecourse of information processing in a situation of significant brain adaptability.</p><p>Weaknesses:</p><p>ROI and searchlight analyses are not completely overlapping, although this might be due to the specific limits and strengths of each approach. Moreover, the conclusions regarding the behavioral relevance of the sensory and perceptual representations in the putatively reorganized brain, although important, are limited due to the behavioral measurements adopted.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98148.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Haupt and colleagues performed a well-designed study to test the spatial and temporal gradient of perceiving braille letters in blind individuals. Using cross-hand decoding of the read letters, and comparing it to the decoding of the read letter for each hand, they defined perceptual and sensory responses. Then they compared where (using fMRI) and when (using EEG) these were decodable. Using fMRI, they showed that low-level tactile responses specific to each hand are decodable from the primary and secondary somatosensory cortex as well as from IPS subregions, the insula and LOC. In contrast, more abstract representations of the braille letter independent from the reading hand were decodable from several visual ROIs, LOC, VWFA and surprisingly also EVC. Using a parallel EEG design, they showed that sensory hand-specific responses emerge in time before perceptual braille letter representations. Last, they used RSA to show that the behavioral similarity of the letter pairs correlates to the neural signal of both fMRI (for the perceptual decoding, in visual and ventral ROIs) and EEG (for both sensory and perceptual decoding).</p><p>Strengths:</p><p>This is a very well-designed study and it is analyzed well. The writing clearly describes the analyses and results. Overall, the study provides convincing evidence from EEG and fMRI that the decoding of letter identity across the reading hand occurs in the visual cortex in blindness. Further, it addresses important questions about the visual cortex hierarchy in blindness (whether it parallels that of the sighted brain or is inverted) and its link to braille reading.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98148.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Haupt</surname><given-names>Marleen</given-names></name><role specific-use="author">Author</role><aff><institution>Department of Education and Psychology, Freie Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Graumann</surname><given-names>Monika</given-names></name><role specific-use="author">Author</role><aff><institution>Freie Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Teng</surname><given-names>Santani</given-names></name><role specific-use="author">Author</role><aff><institution>Smith-Kettlewell Eye Research Institute</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Kaltenbach</surname><given-names>Carina</given-names></name><role specific-use="author">Author</role><aff><institution>Department of Education and Psychology, Freie Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name><role specific-use="author">Author</role><aff><institution>Freie Universitaet Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>We thank Reviewer #1 for the relevant and insightful comments on our paper. Please find our detailed answers below in the Recommendations to the Authors section.</p><p>Summary:</p><p>The researchers examined how individuals who were born blind or lost their vision early in life process information, specifically focusing on the decoding of Braille characters. They explored the transition of Braille character information from tactile sensory inputs, based on which hand was used for reading, to perceptual representations that are not dependent on the reading hand.</p><p>They identified tactile sensory representations in areas responsible for touch processing and perceptual representations in brain regions typically involved in visual reading, with the lateral occipital complex serving as a pivotal &quot;hinge&quot; region between them.</p><p>In terms of temporal information processing, they discovered that tactile sensory representations occur prior to cognitive-perceptual representations. The researchers suggest that this pattern indicates that even in situations of significant brain adaptability, there is a consistent chronological progression from sensory to cognitive processing.</p><p>Strengths:</p><p>By combining fMRI and EEG, and focusing on the diagnostic case of Braille reading, the paper provides an integrated view of the transformation processing from sensation to perception in the visually deprived brain. Such a multimodal approach is still rare in the study of human brain plasticity and allows us to discern the nature of information processing in blind people's early visual cortex, as well as the time course of information processing in a situation of significant brain adaptability.</p><p>Weaknesses:</p><p>The lack of a sighted control group limits the interpretations of the results in terms of profound cortical reorganization, or simple unmasking of the architectural potentials already present in the normally developing brain.</p></disp-quote><p>We thank the reviewer for raising this important point! We acknowledge that our claims regarding the unmasking of architectural potentials in both the normally developing and visually deprived brain are limited by the study design we employed. However, we note that defining an appropriate control group and assessing non-visual reading in sighted participants is far from straightforward. We discuss these issues in our response to the Public Review of Reviewer 2.</p><disp-quote content-type="editor-comment"><p>Moreover, the conclusions regarding the behavioral relevance of the sensory and perceptual representations in the putatively reorganized brain are limited due to the behavioral measurements adopted.</p></disp-quote><p>We agree with the reviewer that the relation between behavior and neural representations as established via perceived similarity judgments are task-dependent, and that a richer assessment of behavior would be valuable. Please note, however, that this limitation pertains to any experimental task used to assess behavior in the laboratory. Our major goal was to assess whether the identified neural representations are suitably formatted to be used by the brain for at least one behavior rather than being epiphenomenal. We found that the representations are suitably formatted for similarity judgments, thus establishing that they are relevant for at least this behavior. We also argue that judging similarity is a complex task that may underlie many other relevant behaviors. We discuss this point further in response to the Recommendations to the Authors.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p></disp-quote><p>We thank the reviewer for the considerate and thoughtful suggestions. Please find a detailed description of the implemented changes below.</p><disp-quote content-type="editor-comment"><p>Summary:</p><p>Haupt and colleagues performed a well-designed study to test the spatial and temporal gradient of perceiving braille letters in blind individuals. Using cross-hand decoding of the read letters, and comparing it to the decoding of the read letter for each hand, they defined perceptual and sensory responses. Then they compared where (using fMRI) and when (using EEG) these were decodable. Using fMRI, they showed that low-level tactile responses specific to each hand are decodable from the primary and secondary somatosensory cortex as well as from IPS subregions, the insula, and LOC. In contrast, more abstract representations of the braille letter independent from the reading hand were decodable from several visual ROIs, LOC, VWFA, and surprisingly also EVC. Using a parallel EEG design, they showed that sensory hand-specific responses emerge in time before perceptual braille letter representations. Last, they used RSA to show that the behavioral similarity of the letter pairs correlates to the neural signal of both fMRI (for the perceptual decoding, in visual and ventral ROIs) and EEG (for both sensory and perceptual decoding).</p><p>Strengths:</p><p>This is a very well-designed study and it is analyzed well. The writing clearly describes the analyses and results. Overall, the study provides convincing evidence from EEG and fMRI that the decoding of letter identity across the reading hand occurs in the visual cortex in blindness. Further, it addresses important questions about the visual cortex hierarchy in blindness (whether it parallels that of the sighted brain or is inverted) and its link to braille reading.</p><p>Weaknesses:</p><p>Although I have some comments and requests for clarification about the details of the methods, my main comment is that the manuscript could benefit from expanding its discussion. Specifically, I'd appreciate the authors drawing clearer theoretical conclusions about what this data suggests about the direction of information flow in the reorganized visual system in blindness, the role VWFA plays in blindness (revised from the original sighted role or similar to it?), how information arrives to the visual cortex, and what the authors' predictions would be if a parallel experiment would be carried out in sighted people (is this a multisensory recruitment or reorganization?). The data has the potential to speak to a lot of questions about the scope of brain plasticity, and that would interest broad audiences.</p></disp-quote><p>We thank the reviewer for the opportunity to provide clearer theoretical conclusions from our data. We elaborate on each of the points raised by the reviewer in the discussion section.</p><p>Concerning the direction of information flow in the reorganized visual system in blindness, we focus on information arrival to EVC and information flow beyond EVC.</p><p>p. 11, ll. 376-386, Discussion 4.1:</p><p>“Overall, identifying braille letter representations in widespread brain areas raises the question of how information flow is organized in the visually deprived brain. Functional connectivity studies report deprivation-driven changes of thalamo-cortical connections which could explain both arrival of information to and further flow of information beyond EVC. First, the coexistence of early thalamic connections to both S1 and V1 (Müller et al., 2019) would enable EVC to receive from different sources and at different timepoints. Second, potentially overlapping connections from both sensory cortices to other visual or parietal areas (Ioannides et al., 2013) could enable the visually deprived brain to process information in a widespread and interconnected array of brain areas. In such a network architecture, several brain areas receive and forward information at the same time. In contrast to information discretely traveling from one processing unit to the next in the sighted brain’s processing cascade, we can rather picture information flowing in a spatially and functionally more distributed and overlapping fashion.”</p><p>Regarding the role of VWFA, we propose that the functional organization of VWFA is modality-independent.</p><p>p. 10, ll. 346-348, Discussion 4.1:</p><p>“Second, we found that VWFA contains perceptual but not sensory braille letter representations. By clarifying the representational format of language representations in VWFA, our results support previous findings of the VWFA being functionally selective for letter and word stimuli in the visually deprived brain (Reich et al., 2011; Striem-Amit et al., 2012; Liu et al., 2023). Together, these findings suggest that the functional organization of the VWFA is modality-independent (Reich et al., 2011), depicting an important contribution to the ongoing debate on how visual experience shapes representations along the ventral stream (Bedny et al., 2021).” Lastly, we would like to share our thoughts about carrying out a parallel experiment in sighted people.</p><p>In general, we agree that it seems insightful to conduct a parallel, analogous experiment in sighted participants with the aim to disentangle whether the effects seen in blind participants are due to multisensory recruitment or reorganization. However, before making predictions regarding the outcome, we would have to define an analogous experiment in sighted participants that taps into the same mechanisms. This, however, is difficult to do as it is unclear what counts as analogous. For example, if we compare braille reading to reading visually presented braille dot arrays or Roman letters, we will assess visual object processing, a different mechanism from that involved in braille reading. Alternatively, if we compare braille reading to sighted participants reading embossed Roman letters haptically or ideally even reading Braille after extensive training, we still face the inherent problem that sighted participants have visual experiences and could use visual imagery strategies in these nonvisual tasks. As we cannot experimentally ensure that sighted participants do not use visual strategies to solve a task, this would always complicate drawing conclusions about the underlying processes. More specifically, we could never pinpoint whether differences between sighted and blind participants are due to measuring different mechanisms or measuring the same mechanism and unravelling underlying changes (i.e., multisensory recruitment or reorganization). Finally, apart from potential confounds due to visual imagery, considering populations of sighted readers and Braille readers as only differing with regard to their input modality and otherwise being comparable is problematic: In general, blind populations are more heterogenous than most typical samples due to various factors such as aetiologies, onset and severity (Merabet &amp; Pascual-Leone, 2010). Even when carrying out studies in highly specific population subsamples, such as in congenitally blind braille readers, vast within-group differences remain, e.g., the quality and quantity of their braille education, as well as across braille and print readers, e.g., different passive exposure to braille versus written letters during childhood (Englebretson et al., 2023). Hence, to fully match the groups in terms of learning experience we would, for example, have to teach sighted infants braille reading in childhood and follow them up until a comparable age. This approach does not seem feasible.</p><p>p. 10, ll. 328-341, Discussion 4.1:</p><p>“We note that our findings contribute additional evidence but cannot conclusively distinguish between the competing hypotheses that visually deprived brains dynamically adjust to the environmental constraints versus that they undergo a profound cortical reorganization. Resolving this debate would require an analogous experiment in sighted people which taps into the same mechanisms as the present study. Defining a suitable control experiment is, however, difficult. Any other type of reading would likely tap into different mechanism than braille reading. Further, whenever sighted participants are asked to perform a haptic reading task, outcomes can be confounded by visual imagery driving visual cortex (Dijkstra et al., 2019). Thus, the results would remain ambiguous as to whether observed differences between the groups index different mechanisms or plastic changes in the same mechanisms. Last, matching groups of sighted readers and braille readers such that they only differ with regard to their input modality seems practically unfeasible: There are vast differences within the blind population in general, e.g., aetiologies, onset and severity, and the subsample of congenitally blind braille readers more specifically, e.g., the quality and quantity of their braille education, as well as across braille and print readers, e.g., different passive exposure to braille versus written letters during childhood (Englebretson et al., 2023; Merabet &amp; Pascual-Leone, 2010).”</p><p>While we appreciate that the conclusions we can draw from our results are limited by our sample and defining an appropriate parallel experiment in sighted participants is difficult for the reasons discussed above, we would still like to share our speculations regarding the process underlying our result pattern. We think that our results, taken together with results of previous studies, suggest that EVC does not undergo fundamental reorganization in the case of visual deprivation. Rather, it can flexibly adjust to given processing requirements. This flexibility is not infinite; adjustments are limited by the area’s architectural and computational capacity. Importantly, we think that this claim refers to an unmasking of preexisting potential rather than multisensory recruitment.</p><disp-quote content-type="editor-comment"><p>To aid in drawing even more concrete conclusions about the flow of information, I suggest that the authors also add at least another early visual ROI to plot more clearly whether EVC's response to braille letters arrives there through an inverted cortical hierarchy, intermediate stages from VWFA, or directly, as found in the sighted brain for spoken language.</p></disp-quote><p>We thank the reviewer for this comment. However, EVC here consists of V1 to V3, and we already also assess V4, LOC, VWFA and LFA. Thus, we assess regions at all levels of processing from mid- over low- to high-level and cannot add a further interim ROI. Our results using this ROI set do not allow us to arbitrate between the hypotheses raised by the reviewer.</p><disp-quote content-type="editor-comment"><p>Similarly, it may be informative to look specifically at the occipital electrodes' time differences between decoding for the different parameters and their correlation to behavior.</p></disp-quote><p>We thank the reviewer for this suggestion. However, the spatial resolution of EEG measurements is limited, and we cannot convincingly determine the neural source of signals being recorded from specific electrodes, i.e., occipital. When we reduce the number of electrodes before analysis, we primarily see comparable qualitative trends in the data albeit with a reduction in signal-to-noise-ratio.</p><p>To illustrate, we repeated the EEG time decoding and the EEG-behavior RSA with only occipital and parieto-occipital electrodes (n=8) instead of all electrodes (n=63) and added the results to the Supplementary Material (see Supplementary Figure 3 and 4). Overall, we observe a reduction in signal-to-noise-ratio. This is not surprising given that the EEG searchlight decoding results (Figure 3b) reveal sources of the decoding signals extend beyond occipital and parieto-occipital electrodes.</p><p>In the EEG time decoding analysis, we see a comparable trend to the whole brain EEG analysis but do not find a significant difference in onsets of sensory and perceptual representation.</p><p>In the behavior-EEG RSA, we do find that the correlations between behavior and sensory representations emerge significantly earlier than correlations between behavior and perceptual representations. (N = 11, 1,000 bootstraps, one-tailed bootstrap test against zero, P&lt; 0.001). This result is in line with the whole brain EEG analysis.</p><disp-quote content-type="editor-comment"><p>Regarding the methods, further detail on the ability to read with both hands equally and any residual vision of the participants would be helpful.</p></disp-quote><p>We thank the reviewer for raising this point. We assessed participants’ letter reading capabilities in a short screening task prior to the experiment. Participants read letters with both hands separately and we used the same presentation time as in the experiment. As the result showed that average performance for recognizing letters with the left hand (89%) and right hand (88%) were comparable. We did not measure continuous reading in the present study, and we did not assess further information about participants’ ability to read equally well with both hands.</p><p>While the information about the screening task was previously included in Methods section 5.3.2 EEG experiment, we now moved it into a separate section 5.3.3 Braille screening task to make the information better accessible.</p><p>p. 14, ll. 529-533, Methods 5.3.3:</p><p>“Prior to the experiment, participants completed a short screening task during which each letter of the alphabet was presented for 500ms to each hand in random order. Participants were asked to verbally report the letter they had perceived to assess their reading capabilities with both hands using the same presentation time as in the experiment. The average performance for the left hand was 89% correct (SD = 10) and for the right hand it was 88% correct (SD = 13).”</p><p>We thank the reviewer for the suggestion to include information regarding participant’s residual vision. We now added information about participants’ residual light perception to Supplementary Table 1.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) ROI vs Searchlight Results: Figures 2 b and c do not seem to match. The ROI results (b) should be somehow consistent with the whole brain results (c), but &quot;perceptual&quot; decoding in the searchlight (in green) seems localized in sensorimotor areas while for the same classification, no sensorimotor ROI is significant. can the authors clarify this difference?</p><p>Similarly, perceptual decoding does not emerge in EVC with the searchlight analysis, whereas is quite strong in ROI analysis.</p></disp-quote><p>We agree that the results of the ROI and searchlight decoding do not show a direct match. We think that this difference is due to methodological reasons. For example, ROI decoding can be more sensitive when ROIs follow functionally relevant boundaries in the brain, in comparison to spheres used in searchlight decoding that do not. In turn, searchlight decoding may be more sensitive when information is distributed across functional boundaries that would be captured in different ROIs rather than combined, or when ROI definition is difficult (such as here in the visual system of blind participants).</p><p>However, we point out that the primary goal of our searchlight decoding was to show that no other areas beyond our hypothesized ROIs contained braille letter representations, rather than reproducing the ROI results.</p><disp-quote content-type="editor-comment"><p>Decoding accuracies are tested against chance (50% for pairwise classifications) according to methods. In the case of &quot;sensory and perceptual&quot; and &quot;perceptual&quot; classification, this is straightforward. In the case of the analysis that isolates &quot;sensory&quot; representations though the difference is computed between &quot;sensory and perceptual&quot; and &quot;perceptual&quot; decoding accuracies, the accuracies resulting from this difference should thus be centered around 0.</p><p>Are the accuracies tested against 0 in this case? This is not specified in the methods. Furthermore, the data reported in Figure 2 and Figure 3. seem to have 0% as a baseline and the label states &quot;decoding accuracy&quot;. Can the authors clarify whether the reported data are the difference in accuracy with an estimated empirical baseline or an expected baseline of 50%?</p></disp-quote><p>The reviewer is correct in stating that we tested “sensory and perceptual” and “perceptual” against chance level and the difference score “sensory” against 0 and that this information was missing in the methods section.</p><p>We now specify in the methods that we are testing the accuracies for the “sensory” analysis against 0.</p><p>p. 16, ll. 625-627, Methods 5.6:</p><p>“We conducted subject-specific braille letter classification in two ways. First, we classified between letter pairs presented to one reading hand, i.e., we trained and tested a classifier on brain data recorded during the presentation of braille stimuli to the same hand (either the right or the left hand). This yields a measure of hand-dependent braille letter information in neural measurements. We refer to this analysis as within-hand classification. Second, we classified between letter pairs presented to different hands in that we trained a classifier on brain data recorded during the presentation of stimuli to one hand (e.g., right), and tested it on data related to the other hand (e.g., left). This yields a measure of hand-independent braille letter information in neural measurements. We refer to this analysis as across-hand classification. We tested both within-hand and across-hand pairwise classification accuracies against a chance level of 50%. We also calculated a within-across hand classification score which we compared against 0.”</p><p>Regarding Figures 2 and 3, we plot the results as decoding accuracies minus chance level to standardize the y-axes for all three analyses, i.e., compare them to 0. We have corrected the y-axis labels accordingly.</p><p>In our analyses, we assumed an expected baseline of 50%. But in the response below we provide evidence that our results remain stable whether using an expected or empirical baseline.</p><disp-quote content-type="editor-comment"><p>If my understanding is correct, a potential problem persists. The different analyses may not be comparable, because in the &quot;sensory&quot; analysis the baseline is empirically defined, being the classification accuracies of the &quot;perceptual&quot; decoding, while in the other two analyses, the baseline is set at 50%. There are suggestions in the literature to derive empirically defined baselines by randomly shuffling the trial labels and repeating the classification accuracies [grootswagers 2017]. In the context of the present work, its use will make the different statistical analyses more comparable. I would thus suggest the authors define the baseline empirically for all their analyses or, given the high computational demand of this analysis, provide evidence that the results are not affected by this difference in the baseline.</p></disp-quote><p>We thank the reviewer for raising this point. As the reviewer correctly stated, the “sensory” analysis has an empirically defined baseline because it is a difference score while in the other two analyses the baseline is set at 50%.</p><p>To provide evidence that our results are not affected by this difference in baseline, we now re-ran the EEG time decoding. We derived null distributions from the empirical data for all three analyses, following the guidelines from Grootswagers 2017 (page 688, section “Evaluation of Classifier Performance and Group Level Statistical Testing Statistical”):</p><p>“Another popular alternative is the permutation test, which entails repeatedly shuffling the data and recomputing classifier performance on the shuffled data to obtain a null distribution, which is then compared against observed classifier performance on the original set to assess statistical significance (see, e.g., Kaiser et al., 2016; Cichy et al., 2014; Isik et al., 2014). Permutation tests are especially useful when no assumptions about the null distribution can be made (e.g., in the case of biased classifiers or unbalanced data), but they take much longer to run (e.g., repeating the analysis 10,000 times).”</p><p>Running a sign permutation test with 10,000 repetitions, we show that the results are comparable to the previously reported results based on one-sided Wilcoxon signed rank tests. We are, therefore, confident that our reported results are not affected by this difference in baseline. We now added this control analysis to the results section and supplementary material (see Supplementary Figure 5).</p><p>p. 7-8, ll. 213-215, Results 3.2:</p><p>“Importantly, the temporal dynamics of sensory and perceptual representations differed significantly. Compared to sensory representations, the significance onset of perceptual representations was delayed by 107ms (21-167ms) (N = 11, 1,000 bootstraps, one-tailed bootstrap test against zero, P = 0.012). This results pattern was consistent when defining the analysis baseline empirically (see Supplementary Figure 5).”</p><disp-quote content-type="editor-comment"><p>(2) According to the authors, perceptual rather than sensory braille letter representations identified in space are suitably formatted to guide behavior. However, they acknowledge that this finding is likely to be task-dependent because it is based on subject similarity ratings.</p><p>Maybe they could use a more objective similarity measurement of Braille letters similarity?</p><p>For instance, they can compare letters using Jaccard similarity (See for instance: Bottini et al. 2022).</p></disp-quote><p>We thank the reviewer for the opportunity to clarify. We acknowledge that our findings regarding the behavioral relevance of the identified neural representations are task-dependent. But, importantly, this is not because we use perceived similarity ratings as a measurement, but because we only use one measurement while there are infinitely many other potential tasks to assess behavior. This means that the same limitation holds when using another similarity measure like Jaccard similarity. We now clarify this in the Discussion section:</p><p>p. 12, ll. 419-420, Discussion 4.3:</p><p>“Our results clarified that perceptual rather than sensory braille letter representations identified in space are suitably formatted to guide behavior. However, we only use one specific task to assess behavior and, therefore, acknowledge that this finding is taskdependent.”</p><p>Nevertheless, we calculated Jaccard similarity based on the definition used in Bottini et. al. There are no significant correlations for the EEG-behavior or fMRI-behavior RSA when we use the Jaccard matrix and subject-specific EEG or fMRI RDMs (see Supplementary Figure 6).</p><p>This demonstrates that braille letter similarity ratings are significantly correlated with neural representations in space and time but Jaccard similarity of braille dot overlaps is not.</p><disp-quote content-type="editor-comment"><p>(3) If the primacy of perceptual similarity holds also with more objective measures of letter similarity, I think the authors should spend a few more words characterizing the results in fMRI and EEG that are rather divergent (concerning this analysis). Indeed, EEG analysis shows a significant correlation between similarity ratings and within-hand classification accuracy, although this correlation does not emerge in the &quot;sensory&quot; ROIs. I think these findings can be put together, hypothesizing that sensory-based similarity correlates with behavior but only in perceptual ROIs. However, why so? Can the authors provide a more mechanistic explanation? Am I missing something?</p></disp-quote><p>We thank the reviewer for this intriguing idea. We now speculate about how we could harmonize the results from the behavior-EEG and behavior-fMRI RSAs in the discussion section.</p><p>p. 12, ll. 438-442, Discussion 4.3:</p><p>“Similarity ratings and sensory representations as captured by EEG are correlated, and so are similarity ratings and representations in perceptual ROIs, but not sensory ROIs. This might be interpreted as suggesting a link between the sensory representations captured in EEG and the representations in perceptual ROIs. However, we do not have any evidence towards this idea. Differing signalto-noise ratios for the different ROIs and sensory versus perceptual analysis could be an alternative explanation.“</p><disp-quote content-type="editor-comment"><p>(4) In the methods they state that EEG decoding is tested against chance at each time point but these results are not reported, only latency analysis is reported. Can the authors report the significant time points of the EEG time series decoding?</p></disp-quote><p>We thank the reviewer for catching this inconsistency! We have now added this information to Figure 3a.</p><disp-quote content-type="editor-comment"><p>(5) In fMRI ROI definition procedure, the top 321 voxels of each anatomical ROI that had the highest functional activation were selected. The number of voxels is based on the smaller ROI, which to my understanding means that for this ROI all the voxels were selected potentially introducing noise and impacting the comparison between ROIs. Can the authors clarify which ROI was the smallest?</p></disp-quote><p>Thank you for the question! The smallest ROI was V4. This indeed means that for this ROI all voxels were selected. This could have led to our results being noisy in V4 but should not influence the results in other ROIs. We now added this information to the methods section. p. 15, ll. 592, Methods 5.4.4:</p><p>“The smallest mask was V4 which included 321 voxels.”</p><disp-quote content-type="editor-comment"><p>(6) Finally, the author suggests that: &quot;Importantly, higher-level computations are not limited to the EVC in visually deprived brains. Natural sound representations 41 and language activations 53 are also located in EVC of sighted participants. This suggests that EVC, in general, has the capacity to process higher-level information 54. Thus, EVC in the visually deprived brain might not be undergoing fundamental changes in brain organization 53. This promotes a view of brain plasticity in which the cortex is capable of dynamic adjustments within pre-existing computational capacity limits 4,53-55.&quot; - The presence of a sighted control group would have strengthened this claim.</p></disp-quote><p>We agree with the reviewer and now discuss the limitations of our approach in the discussion section (see response to weaknesses raised by Reviewer 2 in the Public Review above).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>(1) Can the authors comment on the reaction time of the two reading hands? Completely ambidextrous reading is not necessarily common, so any differences in ability or response time across the hands may affect the EEG results. Alternatively, do the authors have any additional behavioral data about the participants' ability to read well with both hands?</p></disp-quote><p>We thank the reviewer for these questions! We did not assess reaction times and acknowledge this as a limitation. We did, however, measure accuracies and would have expected to see a speed-accuracy-trade off if reaction times would differ between hands, i.e., we would have expected lower accuracy for the hand with higher RTs. But this was not the case: our participants had comparable accuracy values when reading letters with both hands (see methods section 5.3.3 and answer to Public Review above). This measure indicated that participants recognized Braille letters presented for 500ms equally well with both index fingers.</p><disp-quote content-type="editor-comment"><p>(2) Please add information about any residual sight in the blind participants (or are they all without light perception?)</p></disp-quote><p>We have now added information about residual light perception in Supplementary Table 1 (see above in response to Public Review).</p><disp-quote content-type="editor-comment"><p>(3) Is active tactile exploration involved, or are the participants not moving their fingers at all over the piezo-actuators? Can the authors elaborate more on how the participants used this passive input?</p></disp-quote><p>We thank the reviewer for the opportunity to clarify. Our experimental setup does not involve tactile exploration or sliding motions. Instead, participants rest their index fingers on the piezo-actuators and feel the static sensation of dots pushing up against their fingertips. We assume that participants used the passive input of specific dot stimulation location on fingers to perceive a dot array which, in turn, led to the percept of a braille letter.</p><p>We now specify this information in the methods section.</p><p>p. 13, ll. 474-475, Methods 5.2:</p><p>“The modules were taped to the clothes of a participant for the fMRI experiment and on the table for the EEG and behavioral experiment. This way, participants could read in a comfortable position with their index fingers resting on the braille cells to avoid motion confounds. Importantly, our experimental setup did not involve tactile exploration or sliding motions. We instructed participants to read letters regardless of whether the pins passively stimulated their immobile right or left index finger.”</p><disp-quote content-type="editor-comment"><p>(4) I appreciated the RSA analysis, but remain curious about what the ratings were based on.</p><p>Do the authors know what parameters participants used to rate for? Were these consistent across participants? That would aid in interpreting the results.</p></disp-quote><p>We thank the reviewer for the interest in our representational similarity analyses linking the neural representations to behavior.</p><p>We do not know which parameters participants explicitly used to rate the similarity between letters. We instructed participants to freely compare the similarity of pairs of braille letters without specifying which parameters they should use for the similarity assessment. We speculate that participants used a mixture of low-level features such as stimulation location on fingers and higher-level features such as linguistic similarity between letters. We now clarify the free comparison of braille letter pairs in the methods section:</p><p>p. 14, ll. 538-539, Methods 5.3.4:</p><p>“Each pair of letters was presented once, and participants compared them with the same finger. We instructed participants to freely compare the similarity of pairs of Braille letters without specifying which parameters they should use for the similarity assessment. The rating was without time constraints, meaning participants decided when they rated the stimuli. Participants were asked to verbally rate the similarity of each pair of braille letters on a scale from 1 = very similar to 7 = very different and the experimenter noted down their responses.”</p><disp-quote content-type="editor-comment"><p>(5) Can the authors provide confusion matrices for the decoding analyses in the supplementary materials? This could be informative in understanding what pairs of letters are most discernable and where.</p></disp-quote><p>We have added confusion matrices for within- and between-hand decoding for all ROIs and for the time points 100ms, 200ms, 300ms and 400ms to the Supplementary Material (see Supplementary Figures 7-10).</p><disp-quote content-type="editor-comment"><p>(6) Was slice time correction done for the fMRI data? This is not reported.</p></disp-quote><p>We now added this information to the methods section - our fMRI preprocessing pipeline did not include slice timing correction.</p><p>p. 14, ll. 554, Methods 5.4.2:</p><p>“We did not apply high or low-pass temporal filters and did not perform slice time correction.”</p></body></sub-article></article>