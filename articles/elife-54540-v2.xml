<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54540</article-id><article-id pub-id-type="doi">10.7554/eLife.54540</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A Matlab-based toolbox for characterizing behavior of rodents engaged in string-pulling</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-127285"><name><surname>Inayat</surname><given-names>Samsoon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1966-7967</contrib-id><email>samsoon.inayat@gmail.com</email><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-169672"><name><surname>Singh</surname><given-names>Surjeet</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4646-2089</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-169671"><name><surname>Ghasroddashti</surname><given-names>Arashk</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-169675"><name><surname>Qandeel</surname></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-169673"><name><surname>Egodage</surname><given-names>Pramuka</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-169674"><name><surname>Whishaw</surname><given-names>Ian Q</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-96140"><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0964-2977</contrib-id><email>mohajerani@uleth.ca</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution>Canadian Centre for Behavioural Neuroscience, University of Lethbridge</institution><addr-line><named-content content-type="city">Lethbridge</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kleinfeld</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, San Diego</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>26</day><month>06</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54540</elocation-id><history><date date-type="received" iso-8601-date="2019-12-18"><day>18</day><month>12</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-06-26"><day>26</day><month>06</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Inayat et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Inayat et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54540-v2.pdf"/><abstract><p>String-pulling by rodents is a behavior in which animals make rhythmical body, head, and bilateral forearm as well as skilled hand movements to spontaneously reel in a string. Typical analysis includes kinematic assessment of hand movements done by manually annotating frames. Here, we describe a Matlab-based software that allows whole-body motion characterization using optical flow estimation, descriptive statistics, principal component, and independent component analyses as well as temporal measures of Fano factor, entropy, and Higuchi fractal dimension. Based on image-segmentation and heuristic algorithms for object tracking, the software also allows tracking of body, ears, nose, and forehands for estimation of kinematic parameters such as body length, body angle, head roll, head yaw, head pitch, and path and speed of hand movements. The utility of the task and software is demonstrated by characterizing postural and hand kinematic differences in string-pulling behavior of two strains of mice, C57BL/6 and Swiss Webster.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>string-pulling</kwd><kwd>biomechanics</kwd><kwd>sensorimotor integration</kwd><kwd>hand kinematics</kwd><kwd>matlab toolbox</kwd><kwd>swiss webster c57bl/6</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000024</institution-id><institution>Canadian Institutes of Health Research</institution></institution-wrap></funding-source><award-id>390930</award-id><principal-award-recipient><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>40352</award-id><principal-award-recipient><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009192</institution-id><institution>Alberta Innovates</institution></institution-wrap></funding-source><award-id>43568</award-id><principal-award-recipient><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Alberta Alzheimer Research Program Grant</institution></institution-wrap></funding-source><award-id>PAZ15010</award-id><principal-award-recipient><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Alberta Alzheimer Research Program Grant</institution></institution-wrap></funding-source><award-id>PAZ17010</award-id><principal-award-recipient><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000143</institution-id><institution>Alzheimer Society of Canada</institution></institution-wrap></funding-source><award-id>43674</award-id><principal-award-recipient><name><surname>Mohajerani</surname><given-names>Majid H</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Analysis methods for video data of rodents engaged in string-pulling behavior have been developed and used to highlight differences in motion profiles of Swiss Webster and C57Bl/6 mice.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>String-pulling is a proto-tool behavior in which an animal pulls on a string to obtain a reward. Variations of the behavior are observed in over 160 species of animals including birds, bees, rodents and primates and humans (<xref ref-type="bibr" rid="bib17">Jacobs and Osvath, 2015</xref>; <xref ref-type="bibr" rid="bib29">Singh et al., 2019</xref>). The task has been used to assess cognitive function, motivation, brain and spinal cord injury and disease consequences and work/reward relations in animals and as a therapy and exercise tool in humans. If animals are presented with an overhanging string, they adopt a standing or sitting posture and use hand-over-hand movements to reel in the string (<xref ref-type="bibr" rid="bib4">Blackwell et al., 2018a</xref>; <xref ref-type="bibr" rid="bib5">Blackwell et al., 2018b</xref>; <xref ref-type="bibr" rid="bib6">Blackwell et al., 2018c</xref>). Mice will even spontaneously reel in a string, but the behavior becomes more reliable when reinforced by a food reward attached to the string (<xref ref-type="bibr" rid="bib20">Laidre, 2008</xref>). The movement is an on-line act, guided by sensory information from snout vibrissea, and features four hand shaping movements for grasping and releasing the string, and five arm movements for retreiving and advancing the string that are similar in mice, rats, and humans (<xref ref-type="bibr" rid="bib4">Blackwell et al., 2018a</xref>; <xref ref-type="bibr" rid="bib29">Singh et al., 2019</xref>).</p><p>The reaching/grasping movements of string-pulling resemble the movements of reach-to-eat movements displayed by rodents in other tasks, including the staircase (<xref ref-type="bibr" rid="bib8">Dunnett, 2010</xref>) and the single-pellet reaching task (<xref ref-type="bibr" rid="bib32">Whishaw, 1996</xref>) but additionally both hands can be assessed concurrently and many reach movements can be collected rapidly. The organized structure of string-pulling makes the task useful for investigating sensory, motor, cognitive and social learning, as well as studying pathophysiology of motor deficits in animal models of motor disorders (<xref ref-type="bibr" rid="bib6">Blackwell et al., 2018c</xref>). The string-pulling task for mice has promise as an unconditioned motor task for phenotyping the growing number of transgenic mouse models of neurological and other diseases but requires an analytical method for its systematic analysis. Although most of the work of advancing the string by a mouse is done with the arms and hands, the movements rely on online sensory control and appropriate postural support. Associated with strain differences or neurological impairment, any of these aspects of behavior may be impaired and compensated for by a variety of sensorimotor adjustments in not just the hands but by the entire body. Thus, the analysis of complex changes in behavior require sophisticated whole-body behavioral analysis in addition to the analysis of skilled hand movements. Currently, most of the analysis is carried out with manual annotation of frames leading to low yield of experiments. Automated analysis of mouse string-pulling is useful way for obtaining an objective kinematic assessment of movements, for identifying the contributions of different body parts to string-advancement, and for gaining insights into inter hand coordination (<xref ref-type="bibr" rid="bib32">Whishaw, 1996</xref>; <xref ref-type="bibr" rid="bib9">Dunnett and Brooks, 2018</xref>). The objective of the present paper is to describe a state-of-the-art method for analyzing mouse string-pulling.</p><p>We present a Matlab-based toolbox to facilitate the analyses of video data of different strains of intact or neurologically compromised mice engaged in string pulling. The toolbox features a graphical user interface for video data analysis and follows the analysis framework with two major components, a global assessment of whole-body position and speed and fine kinematic assessment of movements of body, ears, nose, and hands (<xref ref-type="fig" rid="fig1">Figure 1</xref>). As a first step, the user can run optical flow analysis on video data (image sequence) to estimate velocity vector fields and speed frames. Next using descriptive statistics and temporal measures of Fano factor, entropy, and Higuchi fractal dimension, the user can characterize whole-body position, speeds, and their temporal variability. With principal and independent component analyses of the image sequence and speed frames, the user can assess global position and speed patterns to observe regions of significant motion. Additional spatial measures of entropy, sharpness, and Hausdorff fractal dimension allow the user to quantify randomness, edges, and geometrical complexity respectively and use these quantifications to statistically compare groups of animals. We demonstrate that with the whole-body measures mentioned above, we can detect overall differences in position and motion patterns of two different strains of mice, C57BL/6 (Black) and Swiss Webster Albino (White). Furthermore, using color-based image segmentation and heuristic algorithms, the software allows detailed automatic tracking of body, ears, nose, and hands for kinematic analyses (<xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video2">Video 2</xref>). The software also allows the user to manually curate and correct automatic tagging of body parts. Finally, the user can either utilize Matlab scripts provided with the software for comparison of groups of mice or export data to Microsoft Excel for further flexible data analyses. A modular approach has been used to build the toolbox, allowing users to add additional analytical probes.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>String-pulling video analysis framework.</title><p>Characterization of whole-body position and speed and their temporal variability is assessed with descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension. Speed is estimated with optical flow estimation. Principal and independent component analysis of frames of image sequence and speed frames provide overall assessment of position and speed patterns. Measures of spatial entropy, sharpness, and Hausdorff fractal dimension on frames obtained in earlier steps are used to statistically compare global position and motion patterns of groups of animals. For fine kinematic assessment of the motion of body, ears, nose, and hands, they are tracked using image segmentation and heuristic algorithms. Shaded boxes represent methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig1-v2.tif"/></fig><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-54540-video1.mp4"><label>Video 1.</label><caption><title>Video of the representative.</title><p>Black mouse pulling string overlayed with tracked body, ears, nose, and hands also showing frame number and time of frames. Temporal progression of the measured parameters, body length, body angle, head roll angle, head yaw, the X and Y positions of hands from the lower left corner (origin) in the original frames.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-54540-video2.mp4"><label>Video 2.</label><caption><title>Same as <xref ref-type="video" rid="video1">Video 1</xref> but for White mouse.</title></caption></media></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Characterization of position with central-tendency descriptive statistics of image sequence shows larger number of distinct positions for white compared to black mice</title><p>Position characterization of a mouse during the string-pulling behavior (representative frames shown in <xref ref-type="fig" rid="fig2">Figure 2A and C</xref> for Black and White mice, respectively) can be grossly done using central-tendency descriptive statistics that is mean, median, and mode, of the image sequence. By observing the mean, median, and mode frames (<xref ref-type="fig" rid="fig2">Figure 2B and D</xref>), one can infer the average to most frequent positions of the mouse during string-pulling. These frames of the representative Black and White mice show sitting and standing positions, respectively. For statistical comparison of these frames between groups of Black (N = 5) and White (N = 5) mice, spatial entropy, sharpness, and Hausdorff fractal dimension (HFD) measures were calculated and compared using Student’s t-tests. Spatial entropy is measured from 1D projection of the frames while sharpness and HFD operates directly on a 2D frame. Spatial entropy, sharpness, and HFD quantify randomness, edges, and complexity of a frame, respectively. The measures were similar for both groups except that the spatial entropy of the mean frames was larger for White compared to Black mice (p=0.0092, effect size = 2.157, <xref ref-type="fig" rid="fig2">Figure 2E</xref>). This suggests that the mean frame of White mice had more random values as compared to Black mice owing to larger number of distinct positions.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Characterization of whole-body position using central-tendency descriptive statistics.</title><p>(<bold>A</bold>) Representative subset of image frames of a string-pulling epoch of a Black mouse. (<bold>B</bold>) Central-tendency descriptive statistics: mean, median, and mode of string-pulling image sequence for a Black mouse. Mean, median, and mode frames represent average to most frequent position of mouse. Shown below each frame are the respective values of measures of spatial entropy, sharpness, and Hausdorff fractal dimension separated by hyphens. The black line in the mean frame shows the boundary of a mask generated by selecting all pixels with values above the average value of the mean frame. (<bold>C</bold>) and (<bold>D</bold>) same as (<bold>A</bold>) and (<bold>B</bold>) respectively but for a White mouse. Descriptive statistics of representative Black mouse show sitting as most frequent posture while that of the White mouse is upright standing. (<bold>E</bold>) Mean ± SEM spatial entropy, sharpness, and Hausdorff fractal dimension shown for frames of descriptive statistics from 5 Black and 5 White mice.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig2-v2.tif"/></fig></sec><sec id="s2-2"><title>Characterization of temporal changes in position with variability descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension of image sequence depicts larger variability of positions of White compared to Black mice</title><p>To assess the temporal changes in the position of the mouse and the string, we found measures of the time series of individual pixels including variability descriptive statistics (standard deviation, skewness, and kurtosis), Fano factor, temporal entropy, and Higuchi fractal dimension (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The standard deviation frame shows a summary of spatial variation of the mouse position over time for example how the body of the representative Black mouse was in a sitting as well as standing position (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Conversely, the White mouse remained in a standing position and moved its head left and right that is yaw motion (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The skewness and kurtosis frames conveyed similar information and captured rare events such as positions of the string in relation to the mouse body. Both skewness and kurtosis had large values around the mouse body. Like the standard deviation frame, the Fano factor frame also showed change in position over time but with respect to the mean position. Fano factor frame highlighted the changes more than the standard deviation frame; for example for both representative Black and White mice, there is large variation at the top of the head region where the ears are. The temporal entropy of the image sequence quantified randomness of the time series of individual pixels and hence could also be translated to the change in position that is quantity of motion at the individual pixel location. For the representative Black mouse, it is largest around the boundary of the mouse (note the thin red layer near body edge in the Entropy frame). For the lower half of the body, the thickness of the red band, signifying value, is thin whereas it is large in the upper half of the body where it captured the change in mouse’s sitting to standing posture (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For the White mouse, the temporal entropy value was quite large within the mouse’s body depicting large motion of the body. In the upper half of the body, it also revealed a yaw motion of the head. The Higuchi fractal dimension (HiFD) quantified the complexity of the time series of individual pixels in the image sequence. For the representative Black and White mice, HiFD captured changing positions of the string with larger values (hot regions) outside the mouse body. Within the mouse body, it also captured the position of hands which were colocalized with the string in most of the frames (at least one of the hands). The smaller HiFD values (colder regions) for the Black mouse depict the gradual transition in mouse positions from sitting to standing while for the White mouse, they depict its head’s yaw motion.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Characterization of changes in position with variability descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension.</title><p>(<bold>A</bold>) Representative variability descriptive statistics – standard deviation (Std. Dev.), skewness, and kurtosis, Fano factor, Entropy, and Higuchi fractal dimension of string-pulling image sequence of a Black mouse. Standard deviation depicts summary of spatial variation over time (mouse posture from sitting to standing). Skewness and kurtosis capture regions of rare events around the mouse body such as position of string. Shown below each frame are the respective values of spatial entropy (SE), sharpness (<bold>S</bold>), and Hausdorff fractal dimension (HFD) separated with a hyphen. (<bold>B</bold>) same as (<bold>A</bold>) but for White mouse. The White mouse has greater yaw and pitch head motion as seen in the standard deviation. (<bold>C</bold>) Mean ± SEM cumulative distribution function (CDF) of the standard deviation values within a mask obtained for each individual mouse from their respective mean frames. For the representative Black and White mice, outline of these masks is shown in <xref ref-type="fig" rid="fig2">Figure 2B and D</xref>, respectively. (<bold>D–H</bold>) Same as (<bold>C</bold>) but for other parameters. (<bold>I</bold>) Mean ± SEM SE, S, and HFD shown for frames of parameters mentioned above.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig3-v2.tif"/></fig><p>To assess temporal changes in positions of Black versus White mice, we compared the distributions of values of the above-mentioned parameters. The values of parameters were obtained from their respective frames (e.g. values of HiFD from the HiFD frame) using a mask obtained from the mean frame (see Materials and methods) depicting average location of the mouse. The outline of this mask for the representative Black mouse is shown in the mean frame in <xref ref-type="fig" rid="fig2">Figure 2B</xref> (for White mouse in <xref ref-type="fig" rid="fig2">Figure 2D</xref>). For each animal, the probability density function (PDF) of the values of a parameter were estimated using Gaussian Kernel density estimation. The cumulative density function (CDF) was then determined from PDF. The Mean ± SEM CDFs from 5 Black and 5 White mice are shown in <xref ref-type="fig" rid="fig3">Figure 3C–H</xref>: standard deviation, skewness, kurtosis, Fano factor, temporal entropy, and HiFD respectively. Alongside the CDF curves, means over animals are also shown that were obtained by first finding a mean (single value for each animal) of the distribution of a parameter. The means of skewness, temporal entropy, and HiFD were significantly larger for White compared to Black mice (individual Student’s t-tests, p&lt;0.001, p=0.008, and p=0.007 with the following effect sizes: 4.035, 2.219, and 2.737, respectively). The opposite was the case for Kurtosis values (p=0.009, effect size = 2.138). Together, these significant differences highlight the differences underlying dynamics of pixel intensities that depict changes in positions of Black and White mice for example more negative skewness and smaller temporal entropy and HiFD values for Black mice show more stable and gradual changes in positions.</p><p>We also calculated the spatial measures of entropy, sharpness, and Hausdorff fractal dimension for all the individual frames depicting variability and then used individual Student’s t-tests (<xref ref-type="fig" rid="fig3">Figure 3I</xref>). Note that these measures are for the whole frames in contrast with above-mentioned analysis where values of parameters for example temporal entropy and HiFD were extracted using a mask representing average position of mouse. Thus, these measures take into account motion of string along with that of mouse. The measures of spatial entropy and sharpness for the standard deviation frames were significantly larger for the White compared to Black mice (p=0.023 and p=0.017 with the following effect sizes: 1.825 and 1.910, respectively <xref ref-type="fig" rid="fig3">Figure 3I</xref>). The spatial entropy of HiFD frame was significantly larger for the Black compared to White mice (p=0.023, effect size = 1.776). Finally, the sharpness of the Fano factor, temporal entropy, and HiFD frames was significantly larger for the Black compared to White mice (p=0.007, p&lt;0.001, and p=0.007 with the following effect size: 2.261, 3.305, 2.580, respectively). Combined these results suggest larger changes in positions of White compared to Black mice.</p></sec><sec id="s2-3"><title>Characterization of speeds with central-tendency descriptive statistics reveals larger speeds of White compared to Black mice</title><p>By using the CLG optical flow algorithm, velocity vector fields were determined for each consecutive pair of image frames and then speed frames were obtained by finding the magnitude of velocity vectors. The algorithm faithfully captured the motion dynamics of video frames (<xref ref-type="fig" rid="fig4">Figure 4A and C</xref>). The mean, median, and mode speed frames exhibited the spatial distribution of average to most frequent speeds (<xref ref-type="fig" rid="fig4">Figure 4B and D</xref>). Observation of the respective frames for representative Black and White mice show clear similarities as well as differences. For example, the peak values of mean and median frames for the White mouse are larger than those for the Black mouse but peak location is similar for both, and close to the center of mouse’s body where the hands release the string. Similar conclusions can be derived from the median and the mode frames.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Characterization of speed with central-tendency descriptive statistics reveals White mice have larger speeds compared to Black mice.</title><p>(<bold>A</bold>) Output of the optical flow analysis overlayed on three representative image frames. Velocity vector is shown for every fifth pixel (along both dimensions). For a pixel, the magnitude of the velocity vector shows instantaneous speed (cm/s). For the same image frames, respective speed frames are shown on the right. (<bold>B</bold>) Descriptive statistics of the speed frames. Shown below each frame are the respective values of spatial entropy (SE), sharpness (<bold>S</bold>), and Hausdorff fractal dimension (HFD) separated with a hyphen. The unit of color code is cm/sec. (<bold>C</bold>) and (<bold>D</bold>) same as (<bold>A</bold>) and (<bold>B</bold>) respectively but for White mouse. (<bold>E</bold>) Average cumulative distributions of speeds from the mean speed frames of Black (N = 5) and White (N = 5) mice. Shaded regions show SEM. Bar graph show Mean ± SEM over animals of mean speeds. (<bold>F</bold>) Mean ± SEM SE, S, and HFD shown for frames of descriptive statistics from 5 Black and 5 White mice.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig4-v2.tif"/></fig><p>To compare the distributions of speeds for Black and White mice, speed values were chosen from the mean speed frames using a mask (depicting the average position of mouse, see outline in the mean frames in <xref ref-type="fig" rid="fig2">Figure 2</xref>). The mean speed frame was used because the number of frames in a string-pulling epoch can be different from mouse to mouse. Therefore, normalization is done for comparison purpose. The mask was used to get a better estimate of mouse speeds rather than those of spurious speeds of background pixels. The average cumulative distribution of speeds for White and Black mice is shown in <xref ref-type="fig" rid="fig4">Figure 4E</xref>. The mean speed for White mice was significantly larger than that for Black mice (Student’s t-test, p=0.003, effect size = 2.607). Comparison of White and Black mice was also done with measures of spatial entropy, sharpness, and Hausdorff fractal dimension for the mean, median, and mode frames. These measures were not different for groups of Black and White mice except that the spatial entropy of mode frames was significantly larger for Black mice compared to White (Student’s t-test, p=0.017 and effect size = 1.894) highlighting that for White mice most frequent speeds are those of hands positioned around the center of body. Overall, these results suggest that White mice had faster movements than Black mice more localized on their body.</p></sec><sec id="s2-4"><title>Characterization of temporal changes in speed with variability descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension of speed frames shows larger variability of speeds of White compared to Black mice</title><p>Changes in speed were characterized by using variability descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension (HiFD) of speed Frames (<xref ref-type="fig" rid="fig5">Figure 5</xref>) in a similar manner as shown above for image sequence. For both representative Black and White mice, the standard deviation frame showed the largest variation in speeds around the center of body where the hands release the string. For the Black mouse, variation in speeds can also be seen in the upper left corner where the string appeared and moved in a subset of frames. For the White mouse, large variation can also be seen around the head depicting its yaw motion. The skewness and kurtosis frames show large values around the mouse body showing the distribution of speeds is more non-normal around the mouse (same as for the image sequence). The Fano factor frames measured dispersion of speeds in time relative to the mean or noise-to-signal ratio. For both White and Black mice, the dispersion appears to be greater around the mouse body. The temporal entropy was greater within the mouse body as compared to outside and vice versa for HiFD.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Characterization of changes in speed with variability descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension.</title><p>(<bold>A</bold>) Representative variability descriptive statistics – standard deviation (Std. Dev.), skewness, and kurtosis, Fano factor, Entropy, and Higuchi fractal dimension of speed frames of a Black Mouse. Shown below each frame are the respective values of spatial entropy (SE), sharpness (<bold>S</bold>), and Hausdorff fractal dimension (HFD) separated with a hyphen. (<bold>B</bold>) Same as (<bold>A</bold>) but for a White mouse. (<bold>C</bold>) Mean ± SEM cumulative distributions (CDF) of standard deviation values within a mask obtained for each individual mouse from their respective mean frames. For the representative Black and White mice, outline of these masks is shown in <xref ref-type="fig" rid="fig2">Figure 2B and D</xref>, respectively. (<bold>D–H</bold>) Same as (<bold>C</bold>) but for other parameters. (<bold>I</bold>) Mean ± SEM SE, S, and HFD shown for frames of parameters mentioned above.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig5-v2.tif"/></fig><p>To assess the temporal changes in speed for groups of Black and White mice, we compared the values of above-mentioned parameters using distributions of values obtained from each animal using its respective mask calculated from the mean frame of image sequence (see outline in mean frame in <xref ref-type="fig" rid="fig2">Figure 2B and D</xref> for representative Black and White mice). The Mean ± SEM cumulative distributions are shown in <xref ref-type="fig" rid="fig5">Figure 5C–H</xref> along with Mean ± SEM over animals for standard deviation, skewness, kurtosis, Fano factor, temporal entropy, and HiFD. The mean skewness (positive) and Fano factor were larger for Black compared to White mice (Student’s t-test, p=0.028 and p=0.023 with the following effect sizes: 1.686 and 1.781, respectively) owing to smaller means (see representative mean frames in <xref ref-type="fig" rid="fig4">Figure 4</xref>). The opposite was true for temporal entropy and HiFD measures (p=0.003 and p=0.008 with the following effect sizes: 2.700 and 2.495, respecctively). The spatial measures of entropy (spatial), sharpness, and Hausdorff fractal dimension for the frames of above-mentioned parameters were not significantly different between the Black and the White mice (<xref ref-type="fig" rid="fig5">Figure 5I</xref>); however, they have been included here for the sake of demonstration of analysis. Some of the measures likely would become significant with larger sample size. Taken together, these results suggest larger variability, temporal randomness, and temporal complexity of speeds within the mouse body for White vs Black mice.</p></sec><sec id="s2-5"><title>Principal component analysis of image sequence and speed frames for assessment of spatial patterns of position and speed</title><p>Principal component analysis of the image sequence and speed frames revealed orthogonal components representing regions with the largest variance in intensity and speeds, respectively (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The first principal component (PC1) of the image sequence showed the most dominant position of the mouse and was similar to frames obtained by first-order descriptive statistics. For the Black mouse, PC2 mostly captured the difference between sitting and standing posture of the mouse. Higher order components captured other dominant features of images for example, positions of head and string. PC1 of the speed frames mostly represented motion of the hands, which constituted significant motion. Motion of the string body, head, and ears appeared in higher principal components. For the Black mouse, PC1 of speed also captured dominant speed of the string in the upper left-hand corner. Since, PC1 captures the largest variance, we compared the characteristics of PC1 for groups of Black and White mice. For PC1s of both image sequence and speed frames, the spatial entropy, sharpness, and Hausdorff fractal dimensions of Black and White mice were similar (Student’s t-test, <xref ref-type="fig" rid="fig6">Figure 6E</xref>), except the spatial entropy of PC1 of image sequence was larger for White compared to Black mice (p=0.011, effect size = 2.095). However, the mean explained variance of PC1 of the image sequence for Black mice was significantly larger than that of White mice compared with a Student’s t-test (p&lt;0.0001, effect size = 6.363), whereas the explained variance of PC1 of the speed frames was smaller for Black mice compared to White mice (p=0.032, effect size = 1.636). These findings corroborate results reported above that White mice have larger variability of positions and higher speeds compared to Black mice.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Principal component analysis of image sequence and speed frames for assessment of patterns of positions and speeds.</title><p>(<bold>A</bold>) The first four principal components (PC) of the image sequence of the representative Black mouse. The number in parentheses indicates the value of explained variance. Shown below each frame are the respective values of spatial entropy (SE), sharpness (<bold>S</bold>), and Hausdorff fractal dimension (HFD) separated with a hyphen. (<bold>B</bold>) First four PCs of speed frames for the Black mouse. (<bold>C</bold>) and (<bold>D</bold>) same as (<bold>A</bold>) and (<bold>B</bold>) respectively but for the representative White mouse. (<bold>E</bold>) Mean ± SEM values of explained variance (Var.), SE, S, and HFD for PC1 of positions and speeds.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig6-v2.tif"/></fig></sec><sec id="s2-6"><title>Independent component analysis of image sequence and speed frames for detecting events with high spatiotemporal contrast</title><p>Independent component analysis (ICA) allows extraction of statistically independent information within a data set by decomposing it into non-Gaussian components. For the string-pulling motion sequence, the most dynamic movements are those of the hands and the string and they contribute to the non-gaussianity of the data set. When applied directly on the image sequence, ICA extracts events that have high spatiotemporal contrast that is where there are sudden changes in an image. Sudden changes include the appearance and disappearance of a hand or string in a frame. <xref ref-type="fig" rid="fig7">Figure 7A and C</xref> show representative independent components (ICs) of image sequences for the representative Black and White mice, respectively. The heat maps show small regions with red and dark blue colors identifying regions of high spatiotemporal contrast (red and blue colors for positive and negative values respectively indicate appearance and disappearance of objects in frames). The ICs of speed frames were similar in appearance to the ICs of image sequence as they picked events with high spatiotemporal changes in speeds that is mostly movements of the hands and the string. In order to obtain a global picture of the independent components over time, maximum (Max) and minimum (Min) projections were determined (<xref ref-type="fig" rid="fig7">Figure 7B and D</xref>) for ICs of both image sequence and speed frames. For both White and Black mice, Max and Min projections of independent components show regions where the hands and string were located. In order to validate these observations, ICA was applied to a synthetic mouse data set in which only the hands moved, and this revealed the prominent appearance of hand locations in Max and Min projections of independent components (<xref ref-type="fig" rid="fig7">Figure 7E and F</xref>). ICA was also done on the principal components of image sequence and speed frames obtained earlier. These analyses yielded results and conclusions similar to those for ICA on the image sequence and speed frames. For the Max and Min projections of ICs, spatial entropy, sharpness, and Hausdorff fractal dimension measures were determined and compared for groups of Black and White mice. In comparisons where differences were significant (with Student’s t-test) are shown in <xref ref-type="fig" rid="fig7">Figure 7G</xref>. The spatial entropy as well as sharpness was larger for White mice compared to Black supporting results reported above (p-values for comparison of spatial entropy of Max-IC-Position and Max-IC-Speed are 0.012 and 0.011 with the following effect sizes: 2.043 and 2.091, respectively; p-value for comparison of sharpness of Max-IC-Speed is 0.034 with effect size = 1.618, for IC frames of PCs, p-values for comparison of spatial entropy of Max-IC-Position and Max-IC-Speed are 0.030 and 0.008 with the following effect sizes:1.661 and 2.204, respectively; p-value for comparison of sharpness of Max-IC-Speed is 0.025 with effect size = 1.741).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Independent component analysis captures regions with high spatiotemporal contrast.</title><p>(<bold>A</bold>) Representative independent components for Black mouse. Individual independent components mostly represent snapshots of position of the hands and strings, as they are most dynamic in the image sequence. (<bold>B</bold>) Maximum (Max) and minimum (Min) value projections of all independent components of image sequence and speed frames. Max and Min projections grossly represent locations of sudden changes happening in an image sequence for example, hand appearing or disappearing at a new location. Shown below each frame are the respective values of spatial entropy, sharpness, and Hausdorff fractal dimension separated with a hyphen. (<bold>C</bold>) and (<bold>D</bold>) similar to (<bold>A</bold>) and (<bold>B</bold>) respectively but for White mouse. (<bold>E</bold>) Representative image frames of synthetic mouse data in which only hands moved. (<bold>F</bold>) Similar to (<bold>B</bold>) but for synthetic data. (<bold>G</bold>) Mean ± SEM of spatial entropy and sharpness of Max projections of ICs for image sequence, speed frames and their respective PCs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig7-v2.tif"/></fig></sec><sec id="s2-7"><title>Descriptive statistics on image sequences of masks of body, ears, nose, and hands</title><p>Descriptive statistics on the masks’ image sequence can reveal useful information about the extent of motion of individual body objects, for example, the standard deviation of body, ears, nose, and hand masks as shown in <xref ref-type="fig" rid="fig8">Figure 8A and B</xref> for representative Black and White mice provides the variability in the position of respective objects. One can easily see individual differences between the representative Black and White mice for example the body positions of a Black mouse are distinct as seen by crisp standard deviation frame compared to a White mouse which has diffuse appearance. Comparing the spatial measures between groups of Black and White mice revealed significantly higher spatial entropy for mean, standard deviation, skewness, and kurtosis frames for White mice (p=0.006, p=0.008, p=0.006, p=0.009 respectively with the following respective effect sizes; 2.356, 2.198, 2.370, and 2.132, <xref ref-type="fig" rid="fig8">Figure 8C</xref>). The sharpness of skewness was also larger for White mice (p=0.022, effect size = 1.785). The spatial measures were not different for ears, nose, and hands.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Image segmentation based on object colors for finding object masks and gross assessment from descriptive statistics of masks.</title><p>(<bold>A–B</bold>) Standard deviation of the masks image sequence of body, ears, nose, and hands for overall assessment of respective motion patterns of Black and White mice, respectively. Shown below each frame are the respective values of spatial entropy (SE), sharpness (<bold>S</bold>), and Hausdorff fractal dimension (HFD) separated with a hyphen. (<bold>C</bold>) Mean ± SEM SE, S, and HFD shown for Body masks.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig8-v2.tif"/></fig></sec><sec id="s2-8"><title>Identification of fur, ears, and nose for quantification of body and head posture of Black mice</title><p>Body length, body tilt, head roll, head yaw, and head pitch are estimated from the masks for fur, ears, and nose (<xref ref-type="fig" rid="fig9">Figure 9</xref>). From the mask for fur, the mouse’s body centroid and body fit are estimated. Since in the fur mask, segregated regions may be segmented because of an overlapping object on the body such as the string in <xref ref-type="fig" rid="fig9">Figure 9</xref> A1, a convex hull (Matlab function bwconvhull) operation is applied to combine regions and find a cumulative area representing fur (<xref ref-type="fig" rid="fig9">Figure 9</xref> A2, also see <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). Matlab function ‘regionprops’ is then used to identify regions in the cumulative mask and their properties for example area, centroid, orientation, major and minor axis length of an ellipse fitted on the region (<xref ref-type="fig" rid="fig9">Figure 9</xref> A7, green). Major axis length and orientation of the ellipse are then used as estimates of body length and tilt, respectively. In the example string pulling epoch in the sample video (lasting about eight secs), the starting body length is about 40 mm and increases by ~1.5% as the mouse stands and elongates its body to virtually walk up on the string (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). The body tilt however remains vertical (between 80°−90°) throughout the epoch (<xref ref-type="fig" rid="fig9">Figure 9C</xref>).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Quantification of body and head posture.</title><p>(<bold>A</bold>) Tagging of body, ears, and nose. From masks found with fur color (<bold>A1</bold>), body region (<bold>A2</bold>) is identified and fitted with an ellipse (A7, green). From the ears’ mask (<bold>A3</bold>), ear regions are identified by using body centroid position found earlier (A4, cyan and magenta dots in A7). Nose mask (<bold>A5</bold>) is used to identify the nose region (<bold>A6</bold>), which is fitted with an ellipse to identify nose position (A7, yellow dot). (<bold>B</bold>) Body length vs time. Body length is the length of major axis of the body fitted ellipse. (<bold>C</bold>) Body Angle vs time. Body angle is that of the major axis of the body fitted ellipse from horizontal. (<bold>D</bold>) Head Pitch vs time. Head pitch is estimated as the perpendicular distance between the nose and a straight line joining the ears (small red line in A7). Positive and negative values correspond to the nose above or below the line respectively. (<bold>E</bold>) Head Yaw vs time. Head yaw is estimated by finding the log of the ratio of distance of right ear from the nose to the distance of left ear from the nose (ratio of lengths of White lines in A7). A value of 0 indicates 0 yaw angle. (<bold>F</bold>) Head Roll Angle vs time. Head roll is the angle of the line joining ear centroids from horizontal using the right ear as the origin. (<bold>G</bold>). Shortest distance of the nose from the string vs time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Algorithm to reduce the number of regions by spatial clustering.</title><p>Close by regions are combined provided that the combined region’s area is less than the threshold and the length of the major axis of fitted ellipse is less than 7 mm. For ears and nose, only the length of major axis of fitted ellipse should be less than 9 and 8 mm respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig9-figsupp1-v2.tif"/></fig></fig-group><p>To estimate head yaw, roll, and pitch, regions representing the right and left ears and the nose are first identified using the respective masks and the elliptical fit for the body found earlier. An elliptical mask is generated for the body using the elliptical fit and the intersection of an inverse of the elliptical mask (converting 0’s to 1’s and vice versa) with that of ears to provide regions that are outside the ellipse for body (<xref ref-type="fig" rid="fig9">Figure 9</xref> A4). If segmented regions are close, they are combined (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>) and very small regions are ignored. In data for the Black mouse, only two regions remained after the end of this procedure and they were easily labeled as the right and left ears by finding their centroid location relative to body centroid that is regions left and right to the body centroid would be right and left ears respectively. However, if multiple regions remain after the intersection procedure, regions overlapping (or closest) to ear regions in a previous frame are chosen. Spatial clustering is also applied to combine fragmented regions that are close together. The centroids of the identified ear regions are used to represent their location (<xref ref-type="fig" rid="fig9">Figure 9</xref> A7, cyan and magenta dots). Next the nose is identified from the nose mask (<xref ref-type="fig" rid="fig9">Figure 9</xref> A5). After identifying spatial regions, clustering is first used to combine regions that are close together (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>) and then the region closest to the top of the body ellipse is labeled as the nose region (<xref ref-type="fig" rid="fig9">Figure 9</xref> A6). An ellipse is fitted onto the nose region and the top of this ellipse is used as a single point to represent nose location (<xref ref-type="fig" rid="fig9">Figure 9</xref> A7, yellow dot). Head yaw is then estimated as the log of the ratio of distance between the nose and the right ear to the distance between nose and left ear. A head yaw value of 0 indicates 0 head yaw angle which means that the face of mouse is facing the camera. In the representative string-pulling epoch of the Black mouse, head yaw values are initially greater than 0 and gradually reduce to values less than 0, indicating that the mouse is moving its head from left to right (<xref ref-type="fig" rid="fig9">Figure 9E</xref>). Head roll angle is estimated by finding the angle of line joining the centroids of the regions representing the right and left ears from the horizontal (<xref ref-type="fig" rid="fig9">Figure 9</xref> A7, long red line). While the mouse pulls the string, the head rolls at a frequency of about three rolls/s (<xref ref-type="fig" rid="fig9">Figure 9F</xref>) in the sample video (<xref ref-type="video" rid="video1">Video 1</xref>). However, there seems to be a bias for this mouse to keep its head titled such that the right ear is slightly higher than the left ear. Head pitch is estimated as the perpendicular distance between the nose and line joining the two ear centroids (<xref ref-type="fig" rid="fig9">Figure 9</xref> A7, small red line). Positive and negative values of head pitch correspond to nose position above or below the line joining the ears, respectively. In the sample video, the head pitch stays positive most of the time, as the mouse is moving its nose up for sniffing or using its whiskers to track the string (<xref ref-type="fig" rid="fig9">Figure 9D</xref>). Furthermore, as has been previously shown (<xref ref-type="bibr" rid="bib4">Blackwell et al., 2018a</xref>), the nose stays close to the string (<xref ref-type="fig" rid="fig9">Figure 9G</xref>).</p></sec><sec id="s2-9"><title>Identification of right and left hands and kinematic measures of hand movement of Black mice</title><p>Since both hands move rapidly and have similar colors, it is a challenge to distinctly identify the right and left hands without using a heuristic-based approach. To make automatic identification simpler, we let the user initialize the location of hands in the starting frame, the first frame of an epoch, where a user manually marks two regions representing right and left hands, respectively. In the subsequent frames, automatic identification of the hands in a frame is achieved by using information of the hand location in the previous frame. Automatic identification uses masks for the hands and finds regions using ‘regionprops’ function of Matlab. As a first step, the number of regions is reduced such that regions that most likely would represent the hands are kept; for example regions with very small areas (less than 20 mm<sup>2</sup>) are neglected (<xref ref-type="fig" rid="fig10">Figure 10</xref> A2 represent selected regions after extra regions were neglected). Close by regions are also combined using spatial clustering (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). Once a set of reduced number of regions is obtained, the algorithm (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>) identifies the left and right hands. If only one region remains, the algorithm assumes that the hands are touching each other. It divides the region into two equal regions and labels the subregions as left and right hands based on their distances from left- and right-hand regions in the previous frame (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>, <xref ref-type="fig" rid="fig10">Figure 10</xref> A3and A4). If two distinct regions remain, the same algorithm is used. If more than two regions are identified, regions are further reduced based on their distances from (and overlaps between) left- and right-hand regions in the previous frame and the regions are fed to the same algorithm in a recursive manner.</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Identification of hands and kinematic measures.</title><p>(<bold>A</bold>) Representative frame (127) with overlayed regions in hand masks (<bold>A1</bold>). Regions in A1 are reduced to find regions that would most likely represent the hands (<bold>A2</bold>). Note that it includes a region in the nose area. Using the position of hands in the previous frame (blue and red outlines in A3), the hands are identified in the current frame (<bold>A4</bold>). (<bold>B</bold>) same as in A but masks are first found by identifying maximally stable extremal regions in the original image (<bold>B1</bold>). (<bold>C</bold>), (<bold>D</bold>) Vertical (Vert) and Horizontal (Horz) positions vs time of right (blue) and left (red) hands, respectively. Positions are measured from the lower left corner of frame as the origin. Where horizontal movements are synchronized, vertical movements of right and left hands are alternating. (<bold>E</bold>), (<bold>F</bold>) Vertical and horizontal speed vs time of right and left hands. Maximum vertical speed is about three times larger than maximum horizontal speed. (<bold>G</bold>) Distance vs. time for vertical motion plotted from all reach cycles (lift-advance-grasp-pull-push-release). Red for left hand and blue for right hand motion. (<bold>H</bold>) Mean ± SEM rise and fall times for vertical motion reaches. (<bold>I</bold>) and (<bold>J</bold>) similar to G) and H) respectively but for horizontal motion. Also see <xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video2">Video 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig10-v2.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>Algorithm to label right- and left-hand regions from two unlabeled regions.</title><p>The algorithm primarily uses distances of unlabelled regions from the right- and left-hand regions in the previous frame to label them as right and left hands.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig10-figsupp1-v2.tif"/></fig></fig-group><p>The regions representing hands were also determined by first finding maximally stable extremal regions (MSER) in the original frame (<xref ref-type="bibr" rid="bib26">Obdrzalek and Mikulik, 2009</xref>). Using the Matlab function ‘detectMSERFeatures’ MSERs were found (<xref ref-type="fig" rid="fig10">Figure 10B1</xref>). Color definition for the hands was then used to find MSERs that contained hand colors and a mask was thus generated (<xref ref-type="fig" rid="fig10">Figure 10B2</xref>). For the representative Black mouse video, hand masks generated this way were in general better than those created with the ‘rangesearch’ function (see Materials and methods); that is they had lesser number of spurious regions. These masks were then treated as described above to find regions representing the left and right hands (<xref ref-type="fig" rid="fig10">Figure 10B3 and B4</xref>). Both methods of finding hand regions are made available in the graphical user interface using choices that can be made with check boxes.</p><p>Since the automatic algorithm is not deterministic but rather makes estimates using heuristics, it is possible that the algorithm incorrectly labels regions as either the right or left hand. In this case, the user can stop the automatic process and manually choose the appropriate identification in an individual frame. The automatic identification of hands in a frame depends upon the position of hands in the previous frame, therefore the user might have to reprocess all subsequent frames.</p><p>Once the positions of the hands are identified in all frames in behavioral epochs of interest that is the x and y coordinates of centroids are found. They can be used to calculate kinematic measures. For example, paths of hand motions can be visualized or the vertical and horizontal positions of each hand from an origin point (lower left corner of the original frame) can be determined and plotted (<xref ref-type="fig" rid="fig10">Figure 10C and D</xref>). In the representative epoch of string pulling for the Black mouse, the stroke length of pulling is initially shorter and gradually grows for both right and left hands. With respect to the common origin, for the vertical motion, the two hands are alternating, whereas for the horizontal motion, the two hands move to the right and left together. From the position plots, the speed of the hands can be found by finding a time derivative. The vertical speed of the hands is about two to three times faster than the horizontal speed. During the whole epoch, there are two bouts of rapid pulling each about two secs long (from 2 to 4 s and from 6 to 8 s). During these bouts, the path shapes of strokes of one hand were similar to each other and to the strokes of the other hand (<xref ref-type="fig" rid="fig10">Figure 10G</xref>). For each stroke, the rise time that is motion from point of release of the string (near the bottom) to the point of the grasp (near the top) is smaller than the fall time which is the time mouse is pulling the string downwards (Student’s t-test, p&lt;0.001, N = 21 cycles, <xref ref-type="fig" rid="fig10">Figure 10H</xref>). For the horizontal motion, however, the strokes are bimodal (<xref ref-type="fig" rid="fig10">Figure 10I</xref>), and the rise time is larger than the fall time (Student’s t-test, p&lt;0.001, N = 22 cycles, <xref ref-type="fig" rid="fig10">Figure 10J</xref>).</p></sec><sec id="s2-10"><title>Quantification of body and head posture and kinematic analyses of White mice and comparison with Black mice</title><p>Distinguishing between the fur and other body parts (hands and ears) was difficult for the White mice, resulting in the generation of poor-quality masks. This was because of lower contrast between body parts and not due to other factors such as lighting conditions. Therefore, using sharpie markers, we painted the hands and ears of the mice with blue and green colors respectively (<xref ref-type="fig" rid="fig11">Figure 11</xref> A1). After painting, image segmentation for identifying the ears and the hands was easier (<xref ref-type="fig" rid="fig11">Figure 11</xref> A3 and A6 respectively). All topographical and kinematic parameters for head posture and motion of the hands were determined (<xref ref-type="fig" rid="fig11">Figure 11B–E</xref>) in a similar fashion as described above. Whereas the Black mouse gradually stood up, increasing its body length while pulling, the White mouse moved the whole body in a rhythmic fashion up and down (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). The White mouse occasionally used its mouth to hold the string (<xref ref-type="video" rid="video2">Video 2</xref>).</p><fig-group><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Identification of body parts and quantification of kinematic measures in White mice.</title><p>(<bold>A</bold>) Procedure for tagging of body, ears, nose, and hands was like that used for Black mice except that the ears and hands were colored green and blue respectively. From masks found using fur color (<bold>A2</bold>), the body region is identified and fitted with an ellipse. Ear and nose regions (A3 and A4, respectively) are identified and used to measure head posture (<bold>A5</bold>). Hand regions (<bold>A6</bold>) are identified based on the position of the hands in the previous frame (A6, red and blue boundaries), the position of the hands in the current frame are identified (<bold>A7</bold>). (<bold>B</bold>) Body length vs time. (<bold>C</bold>) Head Pitch vs time. In frames where both ears are not visible, y-distance between nose and ear is measured as head pitch. <bold>D</bold>) Vertical position of the hands (from bottom of frame) vs time. (<bold>E</bold>) Shortest distance of the nose from the string vs time. (<bold>F</bold>) Mean ± SEM of body linear and angular speeds from five Black and five White mice. (<bold>G</bold>) Mean ± SEM amplitude, vertical speed, and time period of reach cycle of right hand.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig11-v2.tif"/></fig><fig id="fig11s1" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 1.</label><caption><title>Temporal profiling of reach cycle for black and white mice with Dynamic Time Warping.</title><p>Light gray and light blue lines indicate time-warped average reach cycles for individual Black and White mice respectively. Darker lines (black and blue) indicate time-warped group average reach cycles. ‘Time-warped average reach cycle’ for each animal was found by the following steps: (1) individual reach cycles were zero padded to make their lengths equal, (2) average reach cycle was obtained from zero padded cycles, (3) time warping of original cycles was done with average reach cycle obtained in step two using Matlab’s ‘dtw’ function, (4) averaging of individually time-warped reach cycles obtained in previous step provided time-warped average reach cycle for an animal (shown by light gray and light blue lines). By following similar steps, time-warped group average reach cycle was found (darker lines, black and blue).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig11-figsupp1-v2.tif"/></fig></fig-group><p>There was no significant difference between maximum body length, minimum body length, and change in body length (max – min) between White and Black mice, Student’s t-test, p&gt;0.05. However, body linear and angular speeds were significantly larger for White compared to Black mice (p=0.0198 and p=0.0199 with the following effect sizes: 1.837 and 1.834, respectively, <xref ref-type="fig" rid="fig11">Figure 11F</xref>). The dominant frequency in body length oscillations (determined with Fourier transform of the time series of body length) was not different between Black and White mice. Similarly, the dominant frequency of motion for hands was not different. The mean amplitude of both hands was smaller for White mice while mean vertical speed was larger, but neither was statistically significant (<xref ref-type="fig" rid="fig11">Figure 11G</xref>, shown for right hand only). The peak left-hand vertical speed, however, was slightly significantly larger for White mice (p=0.0445, effect size = 1.506). Average time of right-hand reach cycles (see examples in <xref ref-type="fig" rid="fig10">Figure 10G</xref>) characterized as lift-advance-grasp-pull-push-release (<xref ref-type="bibr" rid="bib4">Blackwell et al., 2018a</xref>) was significantly smaller for White compared to Black mice (p=0.005, effect size = 2.46) corroborating previous finding that white mice have higher speeds (<xref ref-type="fig" rid="fig11">Figure 11G</xref>). Time warping was also used to find an average reach cycle individually for each animal followed by another time warping operation to find group average (<xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>). The amplitudes of time-warped reach cycles were similar for Black and White mice.</p></sec><sec id="s2-11"><title>Benchmarking of algorithms for finding body parts</title><p>To determine the accuracy of heuristic algorithms presented here, the percentage of frames in which body parts were correctly detected automatically (verified with manual observation) is calculated. To display the accuracy of tagging of body parts for the data set being analyzed, a push button in the GUI has been provided. <xref ref-type="table" rid="table1">Table 1</xref> shows accuracy results for data from five black and five white mice. The overall average accuracy for all body parts is ~96%, with higher accuracies for fur and ears compared to those for nose and hands. This is because the nose is sometimes hidden behind the string, making detection difficult for the algorithm while the hands are moving at a rapid rate and the algorithm cannot differentiate between regions in the mask that belong to hands versus those that belong to hand-like body parts for example nose.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Accuracy of algorithms.</title><p>Percentage of frames in which a body part was detected automatically. N indicates the number of frames of the string-pulling epoch that was analyzed. (N/A indicates not applicable and values were not used for calculating the overall accuracy. For these situations, either color of a body part was not separable in masks or due to limitation of the camera, hands were blurry in most of the frames).</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Mouse/Body Part</th><th valign="top">Body</th><th valign="top">Ears</th><th valign="top">Nose</th><th valign="top">Hands</th></tr></thead><tbody><tr><td valign="top">Black Mouse 1 (N = 477 of 630)</td><td valign="top">100</td><td valign="top">100</td><td valign="top">98.32</td><td valign="top">98.74</td></tr><tr><td valign="top">Black Mouse 2 (N = 132 of 245)</td><td valign="top">100</td><td valign="top">100</td><td valign="top">96.12</td><td valign="top">87.12</td></tr><tr><td valign="top">Black Mouse 3 (N = 351 of 1435)</td><td valign="top">100</td><td valign="top">97.14</td><td valign="top">92.88</td><td valign="top">81.48</td></tr><tr><td valign="top">Black Mouse 4 (N = 191 of 629)</td><td valign="top">100</td><td valign="top">97.38</td><td valign="top">100</td><td valign="top">89.53</td></tr><tr><td valign="top">Black Mouse 5 (N = 171 of 808)</td><td valign="top">100</td><td valign="top">100</td><td valign="top">98.83</td><td valign="top">93.57</td></tr><tr><td valign="top">Simulated Black Mouse (N = 300 of 477)</td><td valign="top">100</td><td valign="top">100</td><td valign="top">100</td><td valign="top">99.67</td></tr><tr><td valign="top"><italic>Mean (Black Mice)</italic></td><td valign="top"><bold><italic>100</italic></bold></td><td valign="top"><bold><italic>99.08</italic></bold></td><td valign="top"><bold><italic>97.17</italic></bold></td><td valign="top"><bold><italic>91.70</italic></bold></td></tr><tr><td valign="top">White Mouse 1 (N = 275 of 990)</td><td valign="top">100</td><td valign="top">96.36</td><td valign="top">82.91</td><td valign="top">94.91</td></tr><tr><td valign="top">White Mouse 2 (N = 300 of 594)</td><td valign="top">99.67</td><td valign="top">99</td><td valign="top">100</td><td valign="top">91</td></tr><tr><td valign="top">White Mouse 3 (N = 251 of 453)</td><td valign="top">100</td><td valign="top">94.02</td><td valign="top">N/A</td><td valign="top">60.16 (N/A)</td></tr><tr><td valign="top">White Mouse 4 (N = 166 of 421)</td><td valign="top">99.40</td><td valign="top">90.96</td><td valign="top">N/A</td><td valign="top">30.12 (N/A)</td></tr><tr><td valign="top">White Mouse 5 (N = 216 of 930)</td><td valign="top">100</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">0 (N/A)</td></tr><tr><td valign="top"><italic>Mean (White Mice)</italic></td><td valign="top"><bold><italic>99.80</italic></bold></td><td valign="top"><bold><italic>95.10</italic></bold></td><td valign="top"><bold><italic>91.45</italic></bold></td><td valign="top"><bold><italic>92.95</italic></bold></td></tr></tbody></table></table-wrap></sec><sec id="s2-12"><title>Neural networks for identifying ears, nose, and hands</title><p>We also trained neural networks to analyze string-pulling videos to detect ears, nose, and hands in Black mice. We used the Python based frame work of Deeplabcut toolbox (<xref ref-type="bibr" rid="bib23">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Nath et al., 2018</xref>) to train the ResNet50 network for identifying ears, nose, and hands (<xref ref-type="bibr" rid="bib11">He et al., 2016</xref>). Four separate networks were trained; three for the individual recognition of the ears, nose, and hands and one for the combined recognition of all three in a single step. Since the first three networks for individual recognition had better performance (visually observed), they were used in subsequent analysis. Details of training the network and using it to analyze videos can be found in <xref ref-type="bibr" rid="bib23">Mathis et al., 2018</xref>. For training each neuronal network, 54 frames were labelled (18 from three different videos) and used to create the training dataset. For each network, training took about 36 hr on a GPU (NVIDIA GeForce GTX 1080 with 8 GB memory) and on a 64-bit Windows 10 computer with two processors (Intel Xeon CPU E5-2620 v4 @ 2.10 Gz) and 256 GB RAM. Training iterations were 1030000 and the train and test errors were as follows in pixels. For network trained to identify; 1) only hands, errors were 3.3 and 3.94, 2) only nose, errors were 4.19 and 8.74, 3) only ears, errors were 4.76 and 11.85, and 4) all objects, errors were 5.07 and 10.46.</p><p>For the epoch shown in previous figures for the Black mouse, the networks (separate for ears, nose, and hands) faithfully identified the ears, nose, and hands, although with some errors (<xref ref-type="fig" rid="fig12">Figure 12</xref>). Errors were mostly negligible for the identified position of ears and nose (<xref ref-type="fig" rid="fig12">Figure 12B,C and F</xref>). However, errors were up to ~100 pixels (pix) (10 mm) for the identification of hands in frames where the speed was large (<xref ref-type="fig" rid="fig12">Figure 12A,D and E</xref>). These errors can be reduced if further training of previously trained networks is done with more frames. For now, we offer an option to correct for these errors using the provided graphical user interface in which Deeplabcut results can be loaded and manually curated. Since our procedure (and software) for tagging requires manual validation of results, it can be used to provide ground truth data for validating the results of neural networks-based tagging of body parts.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Comparison of identification of ears, nose, and hands with heuristic algorithms and machine-learning-based neural networks.</title><p>(<bold>A</bold>) Representative frames highlighting the differences in position of ears, nose, and hands identified with the two methods. Dots for heuristics and plus for neural networks. (<bold>B–F</bold>) Differences in x and y coordinates of ears, nose, and hands identified with the two methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig12-v2.tif"/></fig></sec><sec id="s2-13"><title>Graphical user interface of toolbox and procedure for processing video data</title><p>The details of the graphical user interface (GUI, <xref ref-type="fig" rid="fig13">Figure 13</xref>) of the toolbox and procedure for processing video data are presented in the online wiki of GitHub. The software can be downloaded from the following GitHub link along with two sample videos from Black and White mice whose analysis is presented in this paper (<xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video2">Video 2</xref>). <ext-link ext-link-type="uri" xlink:href="https://github.com/samsoon-inayat/string_pulling_mouse_matlab">https://github.com/samsoon-inayat/string_pulling_mouse_matlab</ext-link> (<xref ref-type="bibr" rid="bib14">Inayat et al., 2019</xref>; <xref ref-type="bibr" rid="bib15">Inayat, 2020a</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/string_pulling_mouse_matlab">https://github.com/elifesciences-publications/string_pulling_mouse_matlab</ext-link>).</p><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Graphical user interface of the toolbox.</title><p>Individual steps for processing data are separated into panel blocks. The video frames can be viewed in the display window using the ‘Control Display of Frames’ panel. Step 0, the user has to define a zoom window for digital magnification of the video and scale for finding pixels/mm. Step 1, the user specifies epochs in which the mouse pulls the string. Step 2, the user defines colors. Step 3, the user finds zoom window, a sub region of the frames for which masks are calculated. Step 4, the user processes the epoch for whole-body analysis through ‘Whole-Body Assessment’ panel and also finds masks and later objects of interest. Finally, the user can generate plots using the ‘Post-Processing’ panel.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig13-v2.tif"/></fig><p>The tagged videos for the Black and White mice are shown above (<xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="video" rid="video2">Video 2</xref> respectively) and Youtube links are provided on the online wiki page. <ext-link ext-link-type="uri" xlink:href="https://github.com/samsoon-inayat/string_pulling_mouse_matlab/wiki">https://github.com/samsoon-inayat/string_pulling_mouse_matlab/wiki.</ext-link></p></sec><sec id="s2-14"><title>General applicability of the software (whole-body and kinematic analysis)</title><p>To demonstrate the general applicability of the whole-body analysis and heuristic algorithms for tracking body parts, two videos not related to string-pulling were analyzed. In the first video, a human in a sitting posture is catching and throwing a ball where the face, hands, and ball were tracked with 100, 99, and 95 percent accuracy respectively (<xref ref-type="fig" rid="fig14">Figure 14</xref>). In the second video, a mouse is tail clasped to freely hang and observed for the motion of hind paws. Here, body and hind paws were tracked with 100% and 88% accuracy, respectively (<xref ref-type="fig" rid="fig14s1">Figure 14—figure supplement 1</xref>). Links to both videos tagged with identified objects can be found on the following GitHub Wiki page. <ext-link ext-link-type="uri" xlink:href="https://github.com/samsoon-inayat/string_pulling_mouse_matlab/wiki">https://github.com/samsoon-inayat/string_pulling_mouse_matlab/wiki.</ext-link></p><fig-group><fig id="fig14" position="float"><label>Figure 14.</label><caption><title>General applicability of the software for tracking objects shown in a video of a human catching and throwing a ball.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig14-v2.tif"/></fig><fig id="fig14s1" position="float" specific-use="child-fig"><label>Figure 14—figure supplement 1.</label><caption><title>General applicability of the software for tracking objects shown in a video of a tail-clasped mouse where hind paws and body are tracked.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig14-figsupp1-v2.tif"/></fig></fig-group><p>Processed data sets including whole-body analysis for both above-mentioned videos can be downloaded from the online folder ‘General Applicability of Software’ on OSF website (please see link above).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We present a Matlab based toolbox for seamlessly processing mouse string-pulling video data to perform whole-body assessment of position and speed as well as automatically identify the body, ears, nose, and hands in individual video frames. Quantification of the body and head postures is presented using estimates of body length and tilt, and head yaw, roll, and pitch angles while kinematic measures are determined for the hands. Because mice track the string with snout sinus hairs, changes in head posture relative to string motion provides an estimate of sensorimotor integration. Kinematic measures for body and head can also be calculated from the quantifications provided. This toolbox is provided under the GNU General Public Licence v3.0, and is open source, free, and any modifications are allowed as long as this paper is cited when publishing or distributing a modified version. Here, we discuss phenotypic differences in behavior in exemplar mouse strains used for the analysis, characteristics and limitations of the toolbox as well as comparison with other similar software.</p><p>The description of behavior is central to understanding neural organization and function and accordingly behavior is frequently fractionated to describe the contributions of different body parts. The description of motor abnormalities emphasizes the allied contributions in which one body part compensates for impairment of other body parts (<xref ref-type="bibr" rid="bib2">Alaverdashvili and Whishaw, 2013</xref>; <xref ref-type="bibr" rid="bib18">Jones, 2017</xref>). For example, in using a hand to reach, detailed descriptions of hand movement also require a description of the contribution of postural adjustments of the body. Optimal string pulling can be envisioned as an act featuring individual arm/hand movements made with respect to a stable base of support featuring little body movement. Any variation of this relationship between the hands and body may be an expression of compensatory adjustment. The present procedure provides an integrated view of a prototool use behavior in which skilled hand movements are described along with the contributions of body movement, posture, and orientation.</p><p>We demonstrate phenotyping differences in the motor behavior of C57BL/6 (Back) and Swiss Webster (White) mice and present findings that White mice have larger variation in positions of the body, larger body speed but smaller reach-cycle time of string-pulling motion as compared to Black mice. The interpretation of this result is that the White mice are displaying an impairment in arm/hand movements relative to Black mice and so use upper arm and body motion for compensation. It is recognized that albino mice display anatomical abnormalities in the visual system (<xref ref-type="bibr" rid="bib21">Lee et al., 2019</xref>), other physiological processes for example fatty acid uptake and trafficking (<xref ref-type="bibr" rid="bib28">Seeger and Murphy, 2016</xref>), and recovery after spinal cord injury (<xref ref-type="bibr" rid="bib25">Noristani et al., 2018</xref>). There is also evidence that albino strains of laboratory animals display motor system abnormalities (<xref ref-type="bibr" rid="bib31">VandenBerg et al., 2002</xref>; <xref ref-type="bibr" rid="bib33">Whishaw et al., 2003</xref>). Thus, it is possible to speculate that abnormalities in motor systems, such as the corticospinal tracts that control the arms and hands, lead to compensatory whole-body movements of string pulling in the White mice, as is reported after motor cortex stroke in mice (<xref ref-type="bibr" rid="bib10">Farr and Whishaw, 2002</xref>).</p><p>That the string-pulling procedure captures strain differences in the two exemplar strains of mice that are examined, in which Swiss Wister mice display more body motion relative to C57BL/6 mice, shows that the procedures can be used for phenotyping strain differences more generally. In addition, the behavior of string-pulling is homologous to string-pulling described in many other animal species including humans, suggesting that our method can be exploited for species comparisons and for developing other animal models of human neurological conditions (<xref ref-type="bibr" rid="bib27">Ryait et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Singh et al., 2019</xref>).</p><sec id="s3-1"><title>The whole-body analysis approach</title><p>The whole-body analysis approach developed and presented here provides a generic method for analyzing animal behavior. With this approach, rather than analyzing kinematics of body movements, overall characteristics of the image sequence and derived spatiotemporal signals such as speed frames are assessed within a spatially confined region representing the average position/location of the animal. Thus, the need for pose estimation or tracking of body parts is eliminated. Intuitively, spatiotemporal signals with two spatial dimensions and a temporal dimension can be assessed with spatial and temporal measures and could also be examined using factorization to decompose into simpler components using for example principal and independent component analysis. To obtain a global and spatial summary over time, calculations were first performed in the temporal dimension to obtain single 2D frames for example with temporal measures of central and variability descriptive statistics, Fano factor, entropy, and Higuchi fractal dimension, for quantifying the regularity and variation, dispersion, randomness/disorder, and complexity respectively of the time series of pixel values. After these quantifications, a consistent method is required to compare them across animals and groups of animals.</p><p>The spatial mean values of the above-mentioned parameters over the average position or location of the animal is a reasonable overall spatiotemporal estimation, and as demonstrated above, allowed highlighting statistically significant differences between White and Black mice. Further differences between White and Black mice were evident with spatial measures of entropy, sharpness, and Hausdorff fractal dimension applied on previously obtained single 2D frames for quantifying their randomness/disorder, edges/blurriness, and complexity, respectively. One can envision mouse string pulling as a modification of climbing a string and accordingly, the more erect posture of the White mice uncovers a different strategy relative to strategy of the Black mice. Note that an expert eye might observe speed differences between Black and White mice from raw videos, but the spatiotemporal measurements demonstrated here provide statistical evidence not only for speed differences but also for gauging randomness and complexity of signals. Some of the measurements shown here were not statistically different between Black and White mice. However, for a different set of videos or an increase in sample number the differences might become significant statistically. Therefore, all the measures are included in the toolbox for users to apply and explore in the data at hand.</p></sec><sec id="s3-2"><title>Robustness and limitations of image segmentation and automatic detection of body parts</title><p>The success of the heuristic approach to detect body parts automatically is crucially dependent upon the quality of masks generated with image segmentation. Therefore, four strategies have been devised to improve image segmentation. First, an interface is provided to select colors of objects where the user can interactively select a region of interest, select a candidate set of colors, and then inspect the mask for accuracy (see methods). This way a narrow range of colors can be selected to represent an object. Second, a ‘zoom window’ is used to select a region of frame which encompasses the whole behavioral epoch of interest and therefore allows for fast and improved image segmentation (only look for colors where the user knows they are). An automatic method for detecting zoom window has been implemented where in each frame, the boundaries of the animal are found based on its body set of colors (defined previously). The zoom window found automatically can be inspected and modified as well as can also be defined manually all together. Third, similar to the zoom window, a ‘head box’ is found to define a window containing only the head. Head box is also found automatically, can be later inspected and modified. The use of head box improves the efficiency of detection of ears as the search region is restricted.</p><p>Finally, three different methods for image segmentation (<xref ref-type="fig" rid="fig15">Figure 15</xref>) have been provided (KNN, rangesearch, and a custom algorithm) to allow the user to choose what might work best for the data set at hand. KNN and rangesearch algorithms work similarly to find colors of interest that are closest to the colors of choice based on ‘Euclidian distance’. However, KNN searches for the nearest neighbors while in rangesearch the user predefines the Euclidian distance. For videos with excellent contrast that is clear separation of colors of body parts from background such as for the illustrative Black mouse shown here (see figures above), rangesearch with a Euclidian distance of 1.5 (default in GUI) works very well. However, KNN works better for frames where more rigorous search of closest colors is required. For poor videos where background has colors like those of body parts, the custom algorithm works best. This algorithm divides the screen using a grid and then uses rangesearch to find colors within each grid cell. Using a manually set threshold, this method eliminates spurious pixels and one gets sparser segments depending upon the size of the grid. Sparser segments allow for more accurate detection of islands that represent a certain body part. An illustrative example of finding masks with three methods is presented in <xref ref-type="fig" rid="fig15s1">Figure 15—figure supplement 1</xref> for the tail-clasped mouse to demonstrate the sparsity of segments found with the custom method. An example of finding all the body parts in a video collected with different illumination conditions, camera angle, and brown-colored table-top is presented in <xref ref-type="fig" rid="fig15s2">Figure 15—figure supplement 2</xref> which illustrates the effectiveness of the above-mentioned strategies of image segmentation.</p><p>Automatic identification of body parts features a heuristics approach which requires consistent input data, that is the videos be recorded in a consistent manner. The number of pixels in a frame, frame rate, illumination conditions, and overall position of the mouse in video frames, if consistent from video to video, will make processing effective. If these parameters are changed, image segmentation options (see above) and thresholds that are set in the software can be readjusted for improving accuracy. These thresholds are mentioned in the ‘selectAppropriateRegions’ function in the toolbox and in individual functions to identify body parts.</p><p>Due to the use of a heuristic approach, sometimes the algorithm fails to correctly identify regions for hands. In cases where a Matlab error occurs while identifying the hands, the user is given the option for manual tagging. Nevertheless, in case of no Matlab error, the algorithm can still incorrectly label a region as a nose rather than a hand. In such a case, the user has to manually observe frames for accurate detection and in case of erroneous tagging, use the manual tagging option provided in the GUI. Furthermore, the erroneous tagging in one frame can continue into subsequent frames because identification of the hands in a frame depends on correct identification in a previous frame. Therefore, the recommendation is to process the 20 frames that are displayed along with GUI together. In the sample data of the Black mouse, less than 2% of frames (6/477) were incorrectly labeled for hands and had to be tagged manually. The other case in which erroneous tagging might happen includes videos in which a mouse is pulling a string when turned sideways. In this case only one ear is visible. Then, when both ears appear again, the algorithm still detects one ear. The user has to again manually initialize the detection of the other non-detected ear for the software to automatically detect the ear in subsequent frames.</p></sec><sec id="s3-3"><title>Video acquisition strategy to improve accuracy of automatic detection of body parts</title><p>Since, image segmentation and the heuristic algorithms rely on high-contrast image sequence, the choice of colors for background and string can affect the outcome for example if for a White mouse, white background or string is chosen, the software would not be able to distinguish between string and body (fur of the animal). Similarly, if shutter speed of the camera is not high enough, frames in the image sequence will have blurry objects particularly hands as they have high-speed motion. The software will then have difficulty in resolving colors of hands from blurry images.</p><p>Illumination conditions should also be such as to enhance the contrast of acquired images. We recommend using front facing cold light (on the same side as the camera), that is light is in front of the animal. High resolution of the video is also preferred for better pixel resolution for resolving body parts (HD 1920 x 1080 is recommended). Note that these requirements are important for identifying and tracking body parts in videos but whole-body assessment can still be done on low- to medium-quality videos.</p></sec><sec id="s3-4"><title>Comparison with artificially intelligent machine learning based software</title><p>Machine learning (ML) based software for pose estimation in videos such as ‘DeepLabCut’ (<xref ref-type="bibr" rid="bib23">Mathis et al., 2018</xref>) and ‘Animal Part Tracker’ (<xref ref-type="bibr" rid="bib19">Kabra et al., 2013</xref>) and for object recognition such as ‘DeepBehavior’ (<xref ref-type="bibr" rid="bib3">Arac et al., 2019</xref>) can provide a seamless work flow, but they are computationally intensive initially when training is required. For identifying a new/different object, or for improving the accuracy of detection (<xref ref-type="fig" rid="fig12">Figure 12</xref>), usually retraining must be done. The algorithms presented here are simpler, require smaller computations, and can be implemented on any computer with reasonable computing power and temporary storage for example 2 GHz processor and 32 GB of RAM. In comparison, ML-based software require high-end computers ideally with graphical processing units.</p><p>In contrast with DeepLabCut and AnimalPartTracker, our approach for characterizing string-pulling behavior has several advantages. Instead of just pose estimation, it provides direct measurements of relevant parameters that might be used to identify similarities and differences between control and experimental groups of animals for example whole-body analysis, use of optical flow to identify speeds, identification of body length and angle, head posture, and kinematic analysis of body and hand movements. To identify body length with ML based software, one would have to label many points around the body and track them in frames to identify the body boundary. In short, the method imposes a priori an interpretation of the behavior, whereas our segmentation methods is neutral and so is more objective in this respect.</p><p>With body color definition and image segmentation, identification of the body is a breeze, and features almost 100% accuracy. For other body parts as well, ML-based pose estimation would track points, whereas with image segmentation one can track regions/areas which with the former method must be done in a secondary analysis. The approach presented here also has a faster lead time. The user can obtain results the same day after data acquisition as our software does not need any prior training of a neural network (which can take several days), rather one can open videos and start analyzing right away. Depending upon the quality of videos, 2–4 hr might be required to analyze 500 frames. Although, after training a neural network, individual videos might take smaller amount of time for pose estimation, post manual validation and subsequent analysis must be done for characterizing behavior. In the provided toolbox, pdfs of final high-quality figures are made and stored for immediate use and scripts are provided to compare groups of animals. The faster lead time with the current approach would also apply were experimental conditions changed for example animal species or strain, whereas retraining of the neural network would be required.</p></sec><sec id="s3-5"><title>Data compression for efficient storage of masks</title><p>While writing this software, data compression techniques were used to minimize storage of calculated masks in addition to the provision provided to select a zoom window which uses a subset of frames to process. Since each pixel of a mask is either 1 or a 0, it can occupy a single bit in a 1-byte number. This way individual bits of a byte represent a single pixel of an individual mask while the whole byte can be used to store one pixel for eight masks. Using this methodology, five masks are stored in an array with each element one byte long.</p></sec><sec id="s3-6"><title>Support and software maintenance, future work, and possible extensions of the toolbox and the string-pulling behavioral task</title><p>A user manual of the software has been provided in the online wiki at the GitHub page along with a video tutorial. Support for issues with the software will be provided via email or through the GitHub support page where other people will be able to view issues and their solutions. The software will be maintained as per need and any further improvements and changes will be updated on the GitHub ‘readme’ and ‘revisions’ page. In case major revisions are made to the software regarding GUI or improvements in detection algorithms, a completely newer version will be provided on GitHub and the users will be redirected to the newer GitHub page. An undisclosed email list serv of the users will be kept by the corresponding authors. Users will be encouraged to contribute toward future development and will be added as contributors to the GitHub page.</p><p>Future development regarding whole-body analysis might include assessing spatiotemporal properties of the phase of velocity vectors which would add directional information to motion measurements. For the tracking of body parts, additional functionality could be added regarding detection of phases of string-pulling movements for example when the hands grasp and release the string. String masks are available in the toolbox to add this functionality. The heuristic algorithms can also be easily modified by changing the appropriate functions. The string-pulling task can also be enriched with sensory, motor, and behavioral complexity. For example, sensory complexity would involve strings with different cross-sections, texture, or coated with odors. Motor complexity would involve adding tension or variable load to the string. Behavioral complexity would involve a paradigm in which the animal has to learn to associate string-pulling with a reward for example water.</p><p>Social competition or social altruism can also be studied with string-pulling. For example, in a competitive strategy, two animals will pull the string from two ends to obtain reward attached in the middle. The scenario could be presented to study motivation before and after stress. In experiments to study altruism, string pulling of one animal could give the reward to another animal. Motor memory formation and the role of sleep in its consolidation could also be studied (<xref ref-type="bibr" rid="bib16">Inayat et al., 2020b</xref>). We anticipate that this toolbox holds promise for facilitating high-throughput platform for the behavioral screening of drugs that may help in the treatment of Parkinson, Huntington, Alzheimer and similar diseases in addition to brain injuries (i.e. ischemic stroke, spinal cord injury, etc.). In short, the method can reveal the extent to which the hands can be used independently of the body, the asymmetry of hand use, the sensory control of reach guidance, the speed/accuracy of hand use, and compensatory adjustments between the individual measures. Exemplar assessments are relevant to estimating the effects of brain injury and disease, recovery and treatment effects, as well as strain and species differences in string pulling behavior.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental animals</title><p>Adult mice 3–6 months old, five male C57 Bl/6J (Black) and five male Swiss Webster Albino (White), were used in this study. No explicit power analysis was used to estimate sample size because in this work our goal was to establish analysis methods. N = 5 was chosen for both strains of mice due to limited availability of White mice. We observed a statistically significant effect of genotype for many parameters (see results) with the chosen sample size. To examine magnitude of effect where statistical significance was observed, we also report effect size.</p><p>Animals were kept in a controlled temperature (22°C), humidity, and light with a 12:12 light/dark cycle. They were singly housed as they were food restricted to increase the yield of string-pulling behavior. All testing and training were performed during the light phase of the cycle. All experiments were performed in accordance with the Canadian Council of Animal Care and were approved by the University of Lethbridge Animal Welfare Committee.</p></sec><sec id="s4-2"><title>String-pulling task</title><p>The apparatus and training procedure were as described previously (<xref ref-type="bibr" rid="bib4">Blackwell et al., 2018a</xref>). Briefly, the test box consisted of a clear Plexiglas cage (20 cm long, 9 cm wide, and 20 cm high) in which a string was thrown over the center of the front wall so that its end could be reached by the mouse. A video camera was placed in front of the cage and behavior was filmed with a Panasonic camcorder at 60 f/s with a 1/1000 shutter speed and frame resolution of 1920 × 1080 pixels. For three White mice, the data was collected with another camera, a Sony Camcorder with 1/750 shutter speed and 60 f/s. Mice would spontaneously pull the string into the cage but were also lightly food restricted so that they were motivated to pull the string to obtain a food reward (pieces of a cashew nut or cheerios) attached to the end of the string.</p></sec><sec id="s4-3"><title>Analysis platform and preliminary processing</title><p>Matlab R2016 and Microsoft Windows platform was used for the development of the toolbox and analysis of the video record. Later, it was tested and upgraded to work in Matlab R2018b and Matlab 2019b. From the videos, a subset of frames representing string-pulling epoch was selected. The start of the epoch was when the mouse touched the string and started pulling it down while the end of epoch was when the mouse stopped the bout of string-pulling. Furthermore, a subregion of frames was selected for analysis to reduce computation time. In the toolbox, this subregion is called ‘auto zoom window’ and is found automatically but is also subject to adjustment.</p><p>In order to do a fair comparison between the performances of Black and White mice with whole-body analysis, the image frames of Black mice were complemented using Matlab’s ‘imcomplement’ function. To accelerate the processing of whole-body analyses, red, green, and blue (RGB) frames were first resized to one-fourth their original size using Matlab’s ‘imresize’ function and then converted to gray scale frames using Matlab’s ‘rgb2gray’ function. The user can, however, set the image resize value using the provided graphical user interface (see below). For kinematic assessment of movements, body, ears, nose, and hands were tracked in the original frames using color-based image segmentation and heuristic algorithms.</p></sec><sec id="s4-4"><title>Optical flow analysis of image sequence for estimation of velocity vector fields and speed frames</title><p>The Combined Local-Global (CLG) method was used to estimate optical flow in string pulling videos (<xref ref-type="bibr" rid="bib7">Bruhn et al., 2005</xref>; <xref ref-type="bibr" rid="bib1">Afrashteh et al., 2017</xref>). The Matlab implementation of the CLG method provided by <xref ref-type="bibr" rid="bib22">Liu, 2009</xref> was used with the following default parameters which the user can modify if required (alpha = 0.01, ratio = 0.5, minWidth = 20, nOuterFPIterations = 7, nInnerFPIterations = 1, nSORIterations = 30). This method was chosen because it provides a better optical flow estimate compared to Horn-Schunck and Temporospatial methods (<xref ref-type="bibr" rid="bib1">Afrashteh et al., 2017</xref>). The chosen parameter values faithfully estimated optical flow, as was validated with visual observation of moving objects in frames. To reduce computational time, image frames in videos were reduced by four times but resizing of image frames did not affect the quality of images or the calculations of optical flow as confirmed with visual observation of velocity vector fields. As the optical flow computes one velocity vector field frame from two video frames, the number of optical flow frames that is velocity vector fields, is one less than that of video frames.</p></sec><sec id="s4-5"><title>Descriptive statistics</title><p>The mean, median, mode, standard deviation, skewness, and kurtosis were calculated for image sequence and speed frames by using respective Matlab functions. For example, the mean frame was determined by finding the mean of the time series of all individual pixels in a frame sequence that is <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> where μ<sub>x,y</sub> denotes mean value of intensity at x and y location, t is frame number, N is the total number of frames, and I is the average gray scale intensity determined from red, green, and blue frame sequences. Other parameters were similarly calculated.</p><sec id="s4-5-1"><title>Finding a mask for mean mouse position from mean frame</title><p>A mask was generated to estimate the average position of a mouse by using a threshold equal to average of all pixels in the mean frame. Matlab’s ‘imbinarize’ function was used to generate the mask, which makes all pixels zero whose value is below the threshold and sets other pixels to a value of one.</p></sec></sec><sec id="s4-6"><title>Temporal analysis of image sequence with Fano factor, entropy, and Higuchi fractal dimension</title><p>To assess the temporal changes in the position and speed from image sequence and speed frames respectively, three measures, Fano factor, entropy, and Higuchi fractal dimension (HiFD) were used. A Fano factor frame for the image sequence was calculated to find the Fano factor value for each pixel from its time series using the following equation; <inline-formula><mml:math id="inf2"><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi> <mml:mi/><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>, where σ is the standard deviation and μ is the mean of time series. Similarly, an entropy frame for the image sequence was calculated by finding the entropy of the intensity time series of individual pixels by using the 'entropy' function of Matlab which uses the following relationship; <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:mo>-</mml:mo><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> where <italic>E<sub>x,y</sub></italic> is the entropy of the time series at x,y location and <italic>p<sub>x,y</sub></italic> is the normalized histogram counts of intensity values for the same location returned by the 'imhist' Matlab function. Prior to finding this measure, 'mat2gray' Matlab function was applied on time series to scale maximum and minimum values as 1 and 0, respectively. The Higuchi fractal dimension (<xref ref-type="bibr" rid="bib12">Higuchi, 1988</xref>) frame was also calculated from the time series of individual pixels in a similar way using a function downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/50290-higuchi-and-katz-fractal-dimension-measures">https://www.mathworks.com/matlabcentral/fileexchange/50290-higuchi-and-katz-fractal-dimension-measures</ext-link>. The value of Kmax parameter used to find HiFD was arbitrarily chosen to be 10 (<xref ref-type="bibr" rid="bib30">Spasić et al., 2005</xref>). The measures of Fano factor, entropy, and HiFD were also calculated in the same way for speed frames.</p><p>To statistically compare the values of the above-mentioned parameters between groups of Black and White mice, values of parameters were extracted from respective frames using the mean position mask (see above). For example, for comparing entropy, values of entropy were selected from the entropy frame from pixels that had a corresponding value of 1 in the mean position mask. The mean of all these entropy values was chosen as a representative value for the animal. These means were statistically compared between groups using Student’s t-test.</p></sec><sec id="s4-7"><title>Principal component analysis of image sequence and speed frames</title><p>In Matlab, the ‘pca’ function was used to determine principal components (PCs) of the image sequence and motion profile (speed frames) determined with optical flow analysis. For a frame sequence, the 3D matrix HxWxN (H, W, and N being height, width, and number of frames) was converted to a 2D matrix with the number of rows equal to HxW and columns equal to N. The function pca was then executed to determine N principal components. The individual component scores were then reshaped from 1D (HxW) to 2D H rows and W columns. PCs of speed frames were similarly calculated.</p></sec><sec id="s4-8"><title>Independent component analysis of image sequence and speed frames</title><p>A fast independent component analysis (fastICA) algorithm (<xref ref-type="bibr" rid="bib13">Hyvärinen and Oja, 2000</xref>) was used to find independent components (ICs) of the image sequence as well as speed frames. The function was downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/38300-pca-and-ica-package">https://www.mathworks.com/matlabcentral/fileexchange/38300-pca-and-ica-package</ext-link>. To apply ‘fastICA’ on the image sequence or speed frames, the 3D matrix of frames was converted to a 2D matrix with the number of rows equal to HxW and columns equal to N where H and W are the number of height and width pixels while N is the number of frames. Here N was equal to 50% of the original number of frames in the image sequence or speed frames that is number of independent components was chosen to be 50% of the number of frames in the input sequence. This was done to reduce computational time. After applying the fastICA function, individual component frames were determined by reshaping from 1D scores (HxW) to H rows and W columns. Since, the algorithm to find ICs is iterative, the minimum tolerance set to stop iterations was 5e-5 and the maximum number of iterations was set to 1000. ICs were found before reaching the maximum number of iterations that is the error went below tolerance.</p></sec><sec id="s4-9"><title>Calculation of spatial measures of entropy, sharpness, and Hausdorff fractal dimension</title><p>For various frames obtained in the previous sets of whole-body analyses; for example mean frame of image sequence or the first principal component frame, spatial entropy, sharpness, and Hausdorff fractal dimension (HFD) were calculated. The same entropy function described earlier was used for finding spatial entropy after scaling the frame values between 0 and 1 with ‘mat2gray’ Matlab function. Sharpness was calculated using the following equation:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:msqrt><mml:mtext> </mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where ∇<italic>I<sub>x,y</sub></italic> represents gradient of intensity image and H and W denote height and width of the image respectively. Function to calculate sharpness was downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/32397-sharpness-estimation-from-image-gradients">https://www.mathworks.com/matlabcentral/fileexchange/32397-sharpness-estimation-from-image-gradients</ext-link>. The Hausdorff fractal dimension was obtained using the ‘boxCountfracDim’ function downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/58148-hausdorff-box-counting-fractal-dimension-with-multi-resolution-calculation">https://www.mathworks.com/matlabcentral/fileexchange/58148-hausdorff-box-counting-fractal-dimension-with-multi-resolution-calculation</ext-link>. This fractal dimension uses the following relationship; HFD = -log (N(ε))/log(ε) where N is the number of pixels above zeros in a box and ε is the box size. In the box counting method, an image is first binarized using an appropriate threshold and then non-overlapping boxes of different sizes (different ε values) are overlayed on the image and the number of pixels above zero value are counted to obtain N(ε). A line is then fitted for a scatter plot of log(N(ε)) and log(ε) and the slope of that line provides the fractal dimension (FD). Here, the threshold used to binarize an image was the mean value of all its pixels. For the Black mice, the frames were complemented before the calculation of HFD for comparison with White mice.</p></sec><sec id="s4-10"><title>Synthetic data for testing and validation of algorithms</title><p>To test the efficacy of the software, two different types of data sets were generated, 1) empirical: based on experimental values of mouse body position and those of hands, nose, and ears, and 2) modelled: where the movement of hands were calculated by a sinusoidal function. For both data sets, color of fur/body, ears, nose, and hand were chosen from the range of observed values to create elliptical patches of body parts on a rectangular background patch. For empirical data, animal movements were created using the centroid, major- and minor- axis of the ellipse calculated from the real experiments. For modeled data, we observed that x- and y- motion of hands can be modelled using two 180° out of phase sinusoidal oscillators, and body tilt is in-phase with the reaching-hand. In general, we observed that one complete reach-grasp-release cycle was completed in ~12 frames (0.2 s). Thus, the 360° x- and y- motion was divided into 12 parts to mimic the hand movements. Further, hand movements were linked to the body centre to mimic the changes in hand movements associated with body movements.</p></sec><sec id="s4-11"><title>Color-based image segmentation for gross motion assessment and tracking of body, ears, nose, and hands</title><p>In computer vision, image segmentation is a basic process in which an image is divided into regions based on distinguishing features, such as color and texture. In the current video data, we utilized color-based segmentation of frames for finding regions containing fur, ears, nose, hands, and the string. To define a color, a user first selects a rectangular region around the object of interest (<xref ref-type="fig" rid="fig15">Figure 15</xref>) for example, hands. K-means clustering is then used to separate colors in the rectangular region into three clusters. With trial and error, it was found that three clusters were sufficient to separate colors of interest for body and hands while seven clusters were sufficient for separating colors of interest for the string and nose. The number of clusters can be changed with the user interface, however. The user is then allowed to choose the cluster that represents the object of interest for example, cyan cluster in <xref ref-type="fig" rid="fig15">Figure 15</xref> A3 for hands. Using these colors, a mask is then generated and shown to the user for visual inspection. Through the supplied interface, the user can then add or subtract colors until a reasonable mask is generated. The user can then move to the next frame and repeat the process for the new frame. This way, with five to ten frames within an epoch chosen randomly, an appropriate selection of colors for an object of interest is made. All color values in a cluster are stored and later used to find masks. In a similar fashion, the user selects colors for other objects of interest.</p><fig-group><fig id="fig15" position="float"><label>Figure 15.</label><caption><title>Image segmentation based on object colors for finding object masks and gross assessment from descriptive statistics of masks.</title><p>(<bold>A</bold>) Color definitions in original frames are semi-automatically made with the user’s assistance for fur, ears, nose, hands, and string. The user first selects a rectangular region around the object of interest (<bold>A1</bold>). K-means clustering is then applied to segregate different colors within the selected region (<bold>A2</bold>). The user then selects a color that corresponds to the object of interest (A3, example shown for selecting color for the hands). Using color selections for the fur and ears, quick segmentation is used to identify the fur and ears in every frame in an epoch of interest and projected onto a single frame after edge detection (<bold>A4</bold>). A rectangular window around the blob is drawn to define a sub region of frames for which masks are then found (A5, representative masks). The determination of rectangular window can also be done automatically and modified later if required. Note that masks are not always clean and include regions other than objects of interest for example, hand masks include regions identified in ears and feet because of color similarity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig15-v2.tif"/></fig><fig id="fig15s1" position="float" specific-use="child-fig"><label>Figure 15—figure supplement 1.</label><caption><title>Masks found with KNN, rangesearch, and custrom (grid-way) algorithm.</title><p>The Grid-Way produces cleaner masks (eliminating noisy pixels) with sparser identified regions for more efficient subsequent automatic identification of body parts. Masks found for hind paws with three methods. Text on top of each mask shows the respective parameter value used to find the mask and text on the side indicates time taken in seconds to find the mask. Computational time is the largest for the Grid-Way method, but masks are cleaner and easier to operate for heuristic algorithms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig15-figsupp1-v2.tif"/></fig><fig id="fig15s2" position="float" specific-use="child-fig"><label>Figure 15—figure supplement 2.</label><caption><title>Robustness of image segmentation shown in a video with different camera angle with a mouse on a table with brown base.</title><p>Identification of body, ears, nose, and hands was done with 100, 100, 100, and 95 percent accuracy in the 40 frames analyzed for testing image segmentation and tracking algorithms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54540-fig15-figsupp2-v2.tif"/></fig></fig-group><p>Once colors are defined, masks are generated in which colors matching the color of interest are found (<xref ref-type="fig" rid="fig15">Figure 15</xref> A5). Three different methods can be used to generate masks; 1) K-Nearest Neighbor (Matlab’s KNNsearch function), 2) a Range search algorithm (Matlab’s rangesearch function), and 3) a custom algorithm (custom Matlab function find_mask_gridway) which divides a frame into a grid and uses rangesearch for individual grid blocks. To save computational time and memory storage, masks are not found for the whole frame, rather for a subset of each frame. The region of a frame for which masks are determined is found such that it includes the mouse body in all frames of an epoch. First, the range of motion of the whole mouse body within an epoch is estimated by finding the edges of a mouse’s body in each frame of an epoch and plotting them together (<xref ref-type="fig" rid="fig15">Figure 15</xref> A4). The user then draws a window around the blob of plotted edges, which is then stored as a parameter in ‘auto zoom window’ and used to find masks. On a computer with Intel Core i7-4820K with two CPUs (each 3.7 GHz) and 32 GB RAM, it takes ~1 hr to process 477 frames (each frame being 457 × 895 pixels) for finding five sets of masks for fur, ears, nose, hands, and string. With an option to reduce image size by a factor adjustable with the user interface, the calculation of masks can be accelerated by several orders of magnitude.</p></sec><sec id="s4-12"><title>Statistical analysis</title><p>Statistical analysis was done using Matlab R2016b. Data are presented as mean ± the standard error of mean (Mean ± SEM). Two-sample Student’s t-test was used for statistical comparisons and an alpha value of &lt;0.05 was used to determine significance. Effect size was measured using Cohen’s d and reported for substantive results.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Di Shao for animal husbandry and Behroo Mirzagha for her help in collecting video data.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis, Validation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation, Methodology</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All experiments were performed in strict accordance with the Canadian Council of Animal Care and were approved by the University of Lethbridge Animal Welfare Committee (Protocol 1812).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-54540-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The software is available to download from <ext-link ext-link-type="uri" xlink:href="https://github.com/samsoon-inayat/string_pulling_mouse_matlab">https://github.com/samsoon-inayat/string_pulling_mouse_matlab</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/string_pulling_mouse_matlab">https://github.com/elifesciences-publications/string_pulling_mouse_matlab</ext-link>. All video source and processed data is made available at the following website <ext-link ext-link-type="uri" xlink:href="https://osf.io/gmk9y/">https://osf.io/gmk9y/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Inayat</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Mouse String-Pulling Data</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/GMK9Y</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afrashteh</surname> <given-names>N</given-names></name><name><surname>Inayat</surname> <given-names>S</given-names></name><name><surname>Mohsenvand</surname> <given-names>M</given-names></name><name><surname>Mohajerani</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optical-flow analysis toolbox for characterization of spatiotemporal dynamics in mesoscale optical imaging of brain activity</article-title><source>NeuroImage</source><volume>153</volume><fpage>58</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.034</pub-id><pub-id pub-id-type="pmid">28351691</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alaverdashvili</surname> <given-names>M</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A behavioral method for identifying recovery and compensation: hand use in a preclinical stroke model using the single pellet reaching task</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>37</volume><fpage>950</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.03.026</pub-id><pub-id pub-id-type="pmid">23583614</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arac</surname> <given-names>A</given-names></name><name><surname>Zhao</surname> <given-names>P</given-names></name><name><surname>Dobkin</surname> <given-names>BH</given-names></name><name><surname>Carmichael</surname> <given-names>ST</given-names></name><name><surname>Golshani</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepBehavior: a deep learning toolbox for automated analysis of animal and human behavior imaging data</article-title><source>Frontiers in Systems Neuroscience</source><volume>13</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2019.00020</pub-id><pub-id pub-id-type="pmid">31133826</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blackwell</surname> <given-names>AA</given-names></name><name><surname>Banovetz</surname> <given-names>MT</given-names></name><name><surname>Qandeel</surname></name> <name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Wallace</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>The structure of arm and hand movements in a spontaneous and food rewarded on-line string-pulling task by the mouse</article-title><source>Behavioural Brain Research</source><volume>345</volume><fpage>49</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2018.02.025</pub-id><pub-id pub-id-type="pmid">29474809</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blackwell</surname> <given-names>AA</given-names></name><name><surname>Köppen</surname> <given-names>JR</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Wallace</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>String-pulling for food by the rat: assessment of movement, topography and kinematics of a bilaterally skilled forelimb act</article-title><source>Learning and Motivation</source><volume>61</volume><fpage>63</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.lmot.2017.03.010</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blackwell</surname> <given-names>AA</given-names></name><name><surname>Widick</surname> <given-names>WL</given-names></name><name><surname>Cheatwood</surname> <given-names>JL</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Wallace</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="2018">2018c</year><article-title>Unilateral forelimb sensorimotor cortex devascularization disrupts the topographic and kinematic characteristics of hand movements while string-pulling for food in the rat</article-title><source>Behavioural Brain Research</source><volume>338</volume><fpage>88</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2017.10.014</pub-id><pub-id pub-id-type="pmid">29037663</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruhn</surname> <given-names>A</given-names></name><name><surname>Weickert</surname> <given-names>J</given-names></name><name><surname>Schnörr</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Lucas/Kanade meets Horn/Schunck: combining local and global optic flow methods</article-title><source>International Journal of Computer Vision</source><volume>61</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000045324.43199.43</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dunnett</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Staircase (Skilled Reaching) Test</chapter-title><person-group person-group-type="editor"><name><surname>Metman</surname> <given-names>L. V</given-names></name></person-group><source>Encyclopedia of Movement Disorders</source><publisher-name>Academic Press</publisher-name><fpage>156</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-374105-9.00284-7</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunnett</surname> <given-names>SB</given-names></name><name><surname>Brooks</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor assessment in Huntington's Disease Mice</article-title><source>Methods in Molecular Biology</source><volume>1780</volume><fpage>121</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-7825-0_7</pub-id><pub-id pub-id-type="pmid">29856017</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farr</surname> <given-names>TD</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Quantitative and qualitative impairments in skilled reaching in the mouse (<italic>Mus musculus</italic>) after a focal motor cortex stroke</article-title><source>Stroke</source><volume>33</volume><fpage>1869</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1161/01.STR.0000020714.48349.4E</pub-id><pub-id pub-id-type="pmid">12105368</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higuchi</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Approach to an irregular time series on the basis of the fractal theory</article-title><source>Physica D: Nonlinear Phenomena</source><volume>31</volume><fpage>277</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(88)90081-4</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname> <given-names>A</given-names></name><name><surname>Oja</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Independent component analysis: algorithms and applications</article-title><source>Neural Networks</source><volume>13</volume><fpage>411</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(00)00026-5</pub-id><pub-id pub-id-type="pmid">10946390</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Inayat</surname> <given-names>S</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>Ghasroddashti</surname> <given-names>A</given-names></name><name><surname>Qandeel</surname></name><name><surname>Egodage</surname> <given-names>P</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Mohajerani</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A toolbox for automated video analysis of rodents engaged in string-pulling: phenotyping motor behavior of mice for sensory, whole-body and bimanual skilled hand function</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2019.12.18.881342</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Inayat</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020a</year><data-title>string_pulling_mouse_matlab</data-title><source>GitHub: String Pulling Behavioral Analytics, a Matlab-Based Toolbox for Characterizing Behavior of Rodents Engaged in String-Pulling</source><version designator="4.0">v4.0</version><ext-link ext-link-type="uri" xlink:href="https://github.com/samsoon-inayat/string_pulling_mouse_matlab">https://github.com/samsoon-inayat/string_pulling_mouse_matlab</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inayat</surname> <given-names>S</given-names></name><name><surname>Nazariahangarkolaee</surname> <given-names>M</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Mohajerani</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Low acetylcholine during early sleep is important for motor memory consolidation</article-title><source>Sleep</source><volume>43</volume><elocation-id>zsz297</elocation-id><pub-id pub-id-type="doi">10.1093/sleep/zsz297</pub-id><pub-id pub-id-type="pmid">31825510</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname> <given-names>IF</given-names></name><name><surname>Osvath</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The string-pulling paradigm in comparative psychology</article-title><source>Journal of Comparative Psychology</source><volume>129</volume><fpage>89</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1037/a0038746</pub-id><pub-id pub-id-type="pmid">25984937</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Motor compensation and its effects on neural reorganization after stroke</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>267</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.26</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabra</surname> <given-names>M</given-names></name><name><surname>Robie</surname> <given-names>AA</given-names></name><name><surname>Rivera-Alba</surname> <given-names>M</given-names></name><name><surname>Branson</surname> <given-names>S</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title><source>Nature Methods</source><volume>10</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id><pub-id pub-id-type="pmid">23202433</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laidre</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spontaneous performance of wild baboons on three novel food-access puzzles</article-title><source>Animal Cognition</source><volume>11</volume><fpage>223</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1007/s10071-007-0104-5</pub-id><pub-id pub-id-type="pmid">17710453</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>KS</given-names></name><name><surname>Vandemark</surname> <given-names>K</given-names></name><name><surname>Mezey</surname> <given-names>D</given-names></name><name><surname>Shultz</surname> <given-names>N</given-names></name><name><surname>Fitzpatrick</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Functional synaptic architecture of callosal inputs in mouse primary visual cortex</article-title><source>Neuron</source><volume>101</volume><fpage>421</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.12.005</pub-id><pub-id pub-id-type="pmid">30658859</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Beyond Pixels: Exploring New Representations and Applications for Motion Analysis</article-title><publisher-name>Massachusetts Institute of Technology</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>T</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>A</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/476531</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noristani</surname> <given-names>HN</given-names></name><name><surname>They</surname> <given-names>L</given-names></name><name><surname>Perrin</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>C57BL/6 and swiss webster mice display differences in mobility, gliosis, microcavity formation and lesion volume after severe spinal cord injury</article-title><source>Frontiers in Cellular Neuroscience</source><volume>12</volume><elocation-id>173</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2018.00173</pub-id><pub-id pub-id-type="pmid">29977191</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obdrzalek</surname> <given-names>D</given-names></name><name><surname>Mikulik</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Detecting scene elements using maximally stable colour regions</article-title><source>Communications in Computer and Information Science</source><volume>82</volume><fpage>107</fpage><lpage>115</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryait</surname> <given-names>H</given-names></name><name><surname>Bermudez-Contreras</surname> <given-names>E</given-names></name><name><surname>Harvey</surname> <given-names>M</given-names></name><name><surname>Faraji</surname> <given-names>J</given-names></name><name><surname>Mirza Agha</surname> <given-names>B</given-names></name><name><surname>Gomez-Palacio Schjetnan</surname> <given-names>A</given-names></name><name><surname>Gruber</surname> <given-names>A</given-names></name><name><surname>Doan</surname> <given-names>J</given-names></name><name><surname>Mohajerani</surname> <given-names>M</given-names></name><name><surname>Metz</surname> <given-names>GAS</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Luczak</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Data-driven analyses of motor impairments in animal models of neurological disorders</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000516</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000516</pub-id><pub-id pub-id-type="pmid">31751328</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeger</surname> <given-names>DR</given-names></name><name><surname>Murphy</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mouse strain impacts fatty acid uptake and trafficking in liver, heart, and brain: a comparison of C57BL/6 and swiss webster mice</article-title><source>Lipids</source><volume>51</volume><fpage>549</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1007/s11745-015-4117-6</pub-id><pub-id pub-id-type="pmid">26797754</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>Mandziak</surname> <given-names>A</given-names></name><name><surname>Barr</surname> <given-names>K</given-names></name><name><surname>Blackwell</surname> <given-names>AA</given-names></name><name><surname>Mohajerani</surname> <given-names>MH</given-names></name><name><surname>Wallace</surname> <given-names>DG</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human string-pulling with and without a string: movement, sensory control, and memory</article-title><source>Experimental Brain Research</source><volume>237</volume><fpage>3431</fpage><lpage>3447</lpage><pub-id pub-id-type="doi">10.1007/s00221-019-05684-y</pub-id><pub-id pub-id-type="pmid">31734786</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spasić</surname> <given-names>S</given-names></name><name><surname>Kalauzi</surname> <given-names>A</given-names></name><name><surname>Culić</surname> <given-names>M</given-names></name><name><surname>Grbić</surname> <given-names>G</given-names></name><name><surname>Martać</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Estimation of parameter kmax in fractal analysis of rat brain activity</article-title><source>Annals of the New York Academy of Sciences</source><volume>1048</volume><fpage>427</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1196/annals.1342.054</pub-id><pub-id pub-id-type="pmid">16154967</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VandenBerg</surname> <given-names>PM</given-names></name><name><surname>Hogg</surname> <given-names>TM</given-names></name><name><surname>Kleim</surname> <given-names>JA</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Long-Evans rats have a larger cortical topographic representation of movement than Fischer-344 rats: a microstimulation study of motor cortex in naïve and skilled reaching-trained rats</article-title><source>Brain Research Bulletin</source><volume>59</volume><fpage>197</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/S0361-9230(02)00865-1</pub-id><pub-id pub-id-type="pmid">12431749</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whishaw</surname> <given-names>IQ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>An endpoint, descriptive, and kinematic comparison of skilled reaching in mice (<italic>Mus musculus</italic>) with rats (Rattus norvegicus)</article-title><source>Behavioural Brain Research</source><volume>78</volume><fpage>101</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(95)00236-7</pub-id><pub-id pub-id-type="pmid">8864042</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whishaw</surname> <given-names>IQ</given-names></name><name><surname>Gorny</surname> <given-names>B</given-names></name><name><surname>Foroud</surname> <given-names>A</given-names></name><name><surname>Kleim</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Long-Evans and Sprague-Dawley rats have similar skilled reaching success and limb representations in motor cortex but different movements: some cautionary insights into the selection of rat strains for neurobiological motor research</article-title><source>Behavioural Brain Research</source><volume>145</volume><fpage>221</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(03)00143-8</pub-id><pub-id pub-id-type="pmid">14529819</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54540.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kleinfeld</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, San Diego</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>String-pulling by rodents is a canonical task in the assessment of spinal cord function in normal and neurologically compromised mice. The laboratories of Whishaw and Mohajerani demonstrate algorithms to automate the study of postural and hand kinematics in string-pulling behavior. This adds a new dimension to the routines available for high-throughput, high-volume studies of behavior.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A Matlab-based toolbox for characterizing behavior of rodents engaged in string-pulling&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two expert and respected peer reviewers, and the evaluation has been overseen by David Kleinfeld as Reviewing Editor and Kate Wassum as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and with the Reviewing Editor, and they find considerable merit in your Resources contribution, which addresses quantification of a behavioral paradigm of increasing importance and use in the spinal cord circuit community. However, the reviewers raise a number of strong and legitimate concerns that we ask be addressed. The Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Coordination of limb movement is a hallmark of spinal cord motor control. Rope pulling, which involves such coordination in addition to proper posture and grasping, is a critical task in the study of spinal circuits. This work describes a set of software tools for the automated characterization of this behavior, and thus provides a valuable tool for the behavioral characterization of spinal circuits and experimental modification of those circuits.</p><p>Essential revisions:</p><p>Some of these steps will require new data analysis.</p><p>Please clarify the specifics of your tracking accuracy. &quot;The paper states that &quot;our methods can provide group truth data for validating the results of neural network-based tagging of body parts.&quot; It is not clear how this conclusion is derived.&quot;</p><p>Please address the robust nature of your approach. &quot;It is unclear how robust the segmentation is to variations in the video, such as camera angle relative to the mouse, illumination conditions, and color of the mice. For example, the paws of the white mice could not be reliably tracked, and the authors had to use markers to paint the paws. This suggest variations in the video could strongly affect the outcome.&quot;</p><p>Please simplify the statistics and justify all measures. &quot;… while central-tendency results show large difference in speed between Black and White mice, it is unclear how spatial entropy, sharpness, Hausdorff fractal dimension, Higuichi fractal dimension, Fano Factor, and Entropy support this effect&quot;. Nor is the necessity of bringing these statistical measures made clear.</p><p>Please bring trial-averaged data. As string-pulling is a highly repetitive near rhythmic behavior, it would be useful to see the average cycle of the behavior to understand the stereotypical hand movements. Such response could be computed from the data of Figure 10 and 11 by warping each pull cycle to a common time base. Then &quot;compare descriptive statistics to quantify differences in hand movement trajectories across White and Black mouse groups.&quot;</p><p>Please discuss the generality of your software. It would be useful to see if the software for string pulling could be adapted to &quot;… analyze limb movements during pasta handling, thereby showing general applicability of the software.&quot;</p><p>Please discuss the support and maintenance. Further, the poor documentation must be improved and the associated GitHub must be organized</p><p>Please explicitly spell out the advantages of your approach over related methods. In this regard, address the black-box method of &quot;DeepLabCut&quot; and the &quot;Janelia Animal Part Tracker&quot;.</p><p>Lastly, please make the toolbox available to be downloaded in Matlab 2019b (reviewers were unable to successfully download).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;A Matlab-based toolbox for characterizing behavior of rodents engaged in string-pulling&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Kate Wassum as the Senior Editor, and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The reviewers would like you to please provide more discussion of the significance of these findings for an average audience. The data show how whole-body measurements could be used to reveal differences in motor kinematics between mouse strains. These analyses suggest that Swiss Webster Albino (white) mice exhibit faster movements that are more localized to the forelimb compared to black 6 mice. What is the significance of these differences? For example, the authors interpret these differences as the white mice using a &quot;more primitive strategy&quot; than the black mice. In addition, the white mice were considered to have &quot;exaggerated compensatory adjustment&quot; and &quot;impaired individualized arm/hand movements&quot;. Please elaborate on how these conclusions were derived and what are the implications for future studies?</p><p>We also noted a number of typos and errors in figure references. Here are some examples:</p><p>– In the first paragraph of the subsection “Comparison with artificially intelligent machine learning based software”, Figure 2.11 is likely referring to Figure 12.</p><p>– At the end of the first paragraph of the subsection “Robustness and limitations of image segmentation and automatic detection of body parts”, the figure is incorrectly referenced. It is likely referring to Figure 15—figure supplement 1.</p><p>–The figure on Time Warping should be Figure 11—figure supplement 1, not Figure 15.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54540.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Some of these steps will require new data analysis.</p><p>Please clarify the specifics of your tracking accuracy. &quot;The paper states that &quot;our methods can provide group truth data for validating the results of neural network-based tagging of body parts.&quot; It is not clear how this conclusion is derived.&quot;</p></disp-quote><p>We have updated the section “Benchmarking of algorithms for finding body parts” to clarify how tracking accuracy is calculated. We describe that with manual validation, frames are found where the algorithm “correctly” identified a body part and hence accuracy is the percentage of these frames. Furthermore, in the section “Neural networks for identifying ears, nose, and hands”, we describe that because manual validation is part of the procedure we have developed, our software can be used to generate “ground truth” data.</p><disp-quote content-type="editor-comment"><p>Please address the robust nature of your approach. &quot;It is unclear how robust the segmentation is to variations in the video, such as camera angle relative to the mouse, illumination conditions, and color of the mice. For example, the paws of the white mice could not be reliably tracked, and the authors had to use markers to paint the paws. This suggest variations in the video could strongly affect the outcome.&quot;</p></disp-quote><p>We have added a paragraph discussing the robustness of the segmentation procedure in the updated Discussion section <bold>“</bold>Robustness and limitations of image segmentation and automatic detection of body parts”.</p><p>Mainly, we discuss how we improve image segmentation by 1) using an interface with which colors representing an object can be narrowly selected, 2) setting of a zoom window to select a region of frames containing behavioral epoch of interest, 3) setting a head box for individual frames containing head of the animal, and 4) three different methods for finding masks or regions of interest (illustrated in added Figure 15—figure supplement 1).</p><p>Regarding the coloring of paws of White mice, they were not detectable due to the presence of white hair on the dorsal side (almost fully covered thus low contrast) and not because of illumination conditions or camera angle. We also demonstrate robustness of image segmentation with mouse tail clasping video which we added to demonstrate the general applicability of the software (please see our fifth response below and Figure 14—figure supplement 1). Furthermore, in another test video with a camera angle such that the surface of a brown table was visible and shadows of mouse could be seen in the plexiglass box (please see Figure 15—figure supplement 2), we identified body, ears, nose, and hands with 100, 100, 100, and 95 percent accuracy in the 40 frames that we analyzed for testing.</p><p>It is important to note that with this program, investigators are not prisoners of filming constraints, targets can be colored as with the White mice to enhance filming contrasts.</p><disp-quote content-type="editor-comment"><p>Please simplify the statistics and justify all measures. &quot;… while central-tendency results show large difference in speed between Black and White mice, it is unclear how spatial entropy, sharpness, Hausdorff fractal dimension, Higuichi fractal dimension, Fano Factor, and Entropy support this effect&quot;. Nor is the necessity of bringing these statistical measures made clear.</p></disp-quote><p>We thank the reviewers for this comment. We have updated sections 2.2, 2.3, and 2.4 where more interpretation of results has been provided e.g., how statistical differences between parameters highlight differences in dynamics of pixel intensities and speed signals depicting changes in positions and speeds of Black vs. White mice respectively. We have also added a new Discussion section “The whole-body Analysis Approach” where spatial and temporal measures that have been used are explained and justified. We argue that although in the current data set, some of the measures do not show a statistically significant difference between the two groups of mice, Black and White, in different data sets or with increased sample number, one might find significant differences. Therefore, for the sake of completion, we have included all the measures in the toolbox as part of the “whole-body analysis approach” which we highlight in Figure 1 summarizing analysis framework.</p><p>We also note that by using the excel data files, an investigator can explore any of a number of task relevant analyses including, independent movement of the hand relative to the body, asymmetry in hand movements, missed grasp vs. successful grasps, compensatory movements of the body relative to hand movement, sensory monitoring of the string using comparisons of snout movement vs. string movement, and so on.</p><disp-quote content-type="editor-comment"><p>Please bring trial-averaged data. As string-pulling is a highly repetitive near rhythmic behavior, it would be useful to see the average cycle of the behavior to understand the stereotypical hand movements. Such response could be computed from the data of Figures 10 and 11 by warping each pull cycle to a common time base. Then &quot;compare descriptive statistics to quantify differences in hand movement trajectories across White and Black mouse groups.&quot;</p></disp-quote><p>Please see updated Figure 11 where trial averaged data is now shown and compared between Black and White mice. Although the mean amplitude of movement of hands is smaller for White compared to Black mice, it is not statistically significant. We have also performed dynamic time warping and included average cycles of Black and White mice in Figure 11—figure supplement 1. The amplitudes of Black and White mice were not statistically different.</p><disp-quote content-type="editor-comment"><p>Please discuss the generality of your software. It would be useful to see if the software for string pulling could be adapted to &quot;… analyze limb movements during pasta handling, thereby showing general applicability of the software.&quot;</p></disp-quote><p>Please see new Results section “General applicability of the software (whole-body and kinematic analysis)”. We have analyzed two videos unrelated to string pulling, 1) in which a human is throwing and catching a ball, and 2) a mouse is clasped from tail and motion of body and hind paws is observed. Raw and tagged videos are included on the online GitHub wiki sample videos section. We have made raw and analyzed data available through the OSF website. In the manuscript Figure 14 and its supplement are included as illustrations.</p><disp-quote content-type="editor-comment"><p>Please discuss the support and maintenance. Further, the poor documentation must be improved and the associated GitHub must be organized</p></disp-quote><p>Please see new paragraph in retitled and updated Discussion section “Support and software maintenance, future work, and possible extensions of the toolbox and the string-pulling behavioral task”. We have updated “readme” and “Support” file on GitHub delineating installation procedure and added a wiki user manual demonstrating usage with illustrations (screen clips) of the software windows and interactions. Please visit links below.</p><p>https://github.com/samsoon-inayat/string_pulling_mouse_matlab</p><p>https://github.com/samsoon-inayat/string_pulling_mouse_matlab/wiki</p><p>https://github.com/samsoon-inayat/string_pulling_mouse_matlab/wiki/Detailed-Usage-Instructions</p><p>A Video tutorial has also been added through Youtube video.</p><disp-quote content-type="editor-comment"><p>Please explicitly spell out the advantages of your approach over related methods. In this regard, address the black-box method of &quot;DeepLabCut&quot; and the &quot;Janelia Animal Part Tracker&quot;.</p></disp-quote><p>Please see renumbered and updated Discussion section “Comparison with artificially intelligent machine learning based software”. We discuss that our approach provides direct measures of relevant parameters to analyze string pulling behavior. Instead of just pose estimation where points are tracked, with image segmentation, regions can be tracked e.g., body whose length and tilt can be measured directly from the identified region. Subsequent analyses show White mice have larger body linear and angular speed. Our approach doesn’t require large computation power and lead time is faster because no neural network training is required which can take several days. If experimental conditions are changed, retraining of networks is usually required.</p><disp-quote content-type="editor-comment"><p>Lastly, please make the toolbox available to be downloaded in Matlab 2019b (reviewers were unable to successfully download).</p></disp-quote><p>We tested the software with Matlab 2018b and Matlab 2019b and have updated the Materials and methods section.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>The reviewers would like you to please provide more discussion of the significance of these findings for an average audience. The data show how whole-body measurements could be used to reveal differences in motor kinematics between mouse strains. These analyses suggest that Swiss Webster Albino (white) mice exhibit faster movements that are more localized to the forelimb compared to black 6 mice. What is the significance of these differences? For example, the authors interpret these differences as the white mice using a &quot;more primitive strategy&quot; than the black mice. In addition, the white mice were considered to have &quot;exaggerated compensatory adjustment&quot; and &quot;impaired individualized arm/hand movements&quot;. Please elaborate on how these conclusions were derived and what are the implications for future studies?</p></disp-quote><p>We have added and rearranged text (as well as added more references) in the Discussion section to improve readability and highlight the significance of findings related to the differences between White and Black mice. We have replaced the indirect terms highlighted by the reviewers with more direct terms that are backed by conclusions from quantitative analysis. For example, we have replaced “more primitive” with “different”, “exaggerated compensatory adjustment” with “more body motion”, and “impaired individualized arm/hand movements” with “are displaying an impairment in arm/hand movements”. Please see changes in the Discussion section.</p><disp-quote content-type="editor-comment"><p>We also noted a number of typos and errors in figure references. Here are some examples:</p><p>– In the first paragraph of the subsection “Comparison with artificially intelligent machine learning based software”, Figure 2.11 is likely referring to Figure 12.</p><p>– At the end of the first paragraph of the subsection “Robustness and limitations of image segmentation and automatic detection of body parts”, the figure is incorrectly referenced. It is likely referring to Figure 15—figure supplement 1.</p><p>– The figure on Time Warping should be Figure 11—figure supplement 1, not Figure 15.</p></disp-quote><p>We apologize for these typos as they resulted due to a major format change of the manuscript (rearrangement of the Materials and methods, Results, and Discussion sections and different figure numbering) requested by the editorial staff at <italic>eLife</italic>. We have now corrected these and similar typos.</p></body></sub-article></article>