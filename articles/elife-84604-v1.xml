<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">84604</article-id><article-id pub-id-type="doi">10.7554/eLife.84604</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.84604.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A stable, distributed code for cue value in mouse cortex during reward learning</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-112645"><name><surname>Ottenheimer</surname><given-names>David J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4882-1898</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-298425"><name><surname>Hjort</surname><given-names>Madelyn M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9932-2349</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-191857"><name><surname>Bowen</surname><given-names>Anna J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8911-2572</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-191464"><name><surname>Steinmetz</surname><given-names>Nicholas A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7029-2908</contrib-id><email>nick.steinmetz@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-142006"><name><surname>Stuber</surname><given-names>Garret D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1730-4855</contrib-id><email>gstuber@uw.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Center for the Neurobiology of Addiction, Pain and Emotion, University of Washington</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Anesthesiology and Pain Medicine, University of Washington</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Department of Biological Structure, University of Washington</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Department of Pharmacology, University of Washington</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Eisen</surname><given-names>Michael B</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an7q238</institution-id><institution>University of California, Berkeley</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Eisen</surname><given-names>Michael B</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01an7q238</institution-id><institution>University of California, Berkeley</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>30</day><month>06</month><year>2023</year></pub-date><volume>12</volume><elocation-id>RP84604</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2022-11-10"><day>10</day><month>11</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2022-11-11"><day>11</day><month>11</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.13.499930"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-02-24"><day>24</day><month>02</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.84604.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-06-07"><day>07</day><month>06</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.84604.1"/></event></pub-history><permissions><copyright-statement>© 2023, Ottenheimer, Hjort, Bowen et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ottenheimer, Hjort, Bowen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-84604-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-84604-figures-v1.pdf"/><abstract><p>The ability to associate reward-predicting stimuli with adaptive behavior is frequently attributed to the prefrontal cortex, but the stimulus-specificity, spatial distribution, and stability of prefrontal cue-reward associations are unresolved. We trained head-fixed mice on an olfactory Pavlovian conditioning task and measured the coding properties of individual neurons across space (prefrontal, olfactory, and motor cortices) and time (multiple days). Neurons encoding cues or licks were most common in the olfactory and motor cortex, respectively. By quantifying the responses of cue-encoding neurons to six cues with varying probabilities of reward, we unexpectedly found value coding in all regions we sampled, with some enrichment in the prefrontal cortex. We further found that prefrontal cue and lick codes were preserved across days. Our results demonstrate that individual prefrontal neurons stably encode components of cue-reward learning within a larger spatial gradient of coding properties.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain</kwd><kwd>electrophysiology</kwd><kwd>learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id><institution>National Institute on Drug Abuse</institution></institution-wrap></funding-source><award-id>DA053714</award-id><principal-award-recipient><name><surname>Ottenheimer</surname><given-names>David J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id><institution>National Institute on Drug Abuse</institution></institution-wrap></funding-source><award-id>DA053706</award-id><principal-award-recipient><name><surname>Hjort</surname><given-names>Madelyn M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000062</institution-id><institution>National Institute of Diabetes and Digestive and Kidney Diseases</institution></institution-wrap></funding-source><award-id>DK007247</award-id><principal-award-recipient><name><surname>Bowen</surname><given-names>Anna J</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id><institution>National Institute on Drug Abuse</institution></institution-wrap></funding-source><award-id>DA032750</award-id><principal-award-recipient><name><surname>Stuber</surname><given-names>Garret D</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id><institution>National Institute on Drug Abuse</institution></institution-wrap></funding-source><award-id>DA048736</award-id><principal-award-recipient><name><surname>Stuber</surname><given-names>Garret D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Individual neurons in a surprising number of frontal cortical regions encode the value of reward-predicting olfactory cues across distinct stimulus sets and experimental sessions.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Association of environmental stimuli with rewards and the subsequent orchestration of value-guided reward-seeking behavior are crucial functions of the nervous system linked to the prefrontal cortex (PFC) (<xref ref-type="bibr" rid="bib42">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib33">Klein-Flügge et al., 2022</xref>). PFC is heterogeneous, with many studies noting subregional differences in both neural coding <xref ref-type="bibr" rid="bib31">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib72">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Hunt et al., 2018</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref> and functional impact on <xref ref-type="bibr" rid="bib11">Dalley et al., 2004</xref>; <xref ref-type="bibr" rid="bib57">Rudebeck et al., 2008</xref>; <xref ref-type="bibr" rid="bib6">Buckley et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Kesner and Churchwell, 2011</xref> value-based reward seeking in primates and rodents. Furthermore, functional manipulations of PFC subregions exhibiting robust value signals do not always cause a discernible impact on reward-guided behavior (<xref ref-type="bibr" rid="bib10">Chudasama and Robbins, 2003</xref>; <xref ref-type="bibr" rid="bib63">St Onge and Floresco, 2010</xref>; <xref ref-type="bibr" rid="bib12">Dalton et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Verharen et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>), encouraging investigation of differences between value signals across PFC. Within individual PFC subregions, multiple studies have observed evolving neural representations across time, calling into question the stability of PFC signaling (<xref ref-type="bibr" rid="bib27">Hyman et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Malagon-Vina et al., 2018</xref>). A systematic comparison of coding properties across rodent PFC and related motor and sensory regions, as well as across days and stimulus sets, is necessary to provide a full context for the contributions of PFC subregions to reward processing.</p><p>Identifying neural signals for value requires a number of considerations. One issue is that other task features can vary either meaningfully or spuriously with value. In particular, action coding is difficult to parse from value signaling, given the high correlations between behavior and task events (<xref ref-type="bibr" rid="bib44">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib78">Zagha et al., 2022</xref>) and widespread neural coding of reward-seeking actions (<xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>). Additionally, without a sufficiently rich value axis, it is possible to misidentify neurons as ‘value’ coding even though they do not generalize to valuations in other contexts (<xref ref-type="bibr" rid="bib65">Stalnaker et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Hayden and Niv, 2021</xref>; <xref ref-type="bibr" rid="bib79">Zhou et al., 2021</xref>). Because reports of the value have come from different experiments across different species, it is difficult to compare the presence of value signaling even across regions within the prefrontal cortex (<xref ref-type="bibr" rid="bib31">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib72">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib65">Stalnaker et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Otis et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Hunt et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Namboodiri et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>; <xref ref-type="bibr" rid="bib23">Hayden and Niv, 2021</xref>; <xref ref-type="bibr" rid="bib79">Zhou et al., 2021</xref>).</p><p>In this work, we sought to address the existing ambiguity in the distribution and stability of value signaling. We implemented an olfactory Pavlovian conditioning task that permitted the identification of value correlates within the domain of reward probability across two separate stimulus sets. With acute in vivo electrophysiology recordings, we were able to assess the coding of this task across 11 brain regions, including five PFC subregions, as well as olfactory and motor cortex, in a single group of mice, permitting a well-controlled comparison of coding patterns across a large group of the task-relevant regions in the same subjects. Unexpectedly, in contrast to the graded cue and lick coding across these regions, the proportion of neurons encoding cue value was more consistent across regions, with a slight enrichment in PFC but with similar value decoding performance across all regions. To assess coding stability, we performed 2-photon calcium imaging of neurons in the PFC for multiple days and determined that the cue and lick codes we identified were stable over time. Our data demonstrate the universality and stability of cue-reward coding in the mouse cortex.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Distributed neural activity during an olfactory Pavlovian conditioning task</title><p>We trained mice on an olfactory Pavlovian conditioning task with three cue (conditioned stimulus) types that predicted reward on 100% (‘CS+’), 50% (‘CS50’), or 0% (‘CS−’) of trials (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each mouse learned two odor sets (odor sets A and B), trained and imaged on separate days and then, for electrophysiology experiments, presented in six alternating blocks of 51 trials during the recording sessions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Mice developed anticipatory licking (<xref ref-type="fig" rid="fig1">Figure 1C–D</xref>), and the rate of this licking correlated with reward probability (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), indicating that subjects successfully learned the meaning of all six odors.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Electrophysiology and calcium imaging during olfactory Pavlovian conditioning.</title><p>(<bold>A</bold>) Trial structure in Pavlovian conditioning task. (<bold>B</bold>) Timeline for mouse training. (<bold>C</bold>) Mean (+/− standard error of the mean (SEM)) lick rate across mice (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) on each trial type for each odor set during electrophysiology sessions. CS50(r) and CS50(u) are rewarded and unrewarded trials, respectively. Inset: mean anticipatory licks (change from baseline) for the CS+ and CS50 cues for every session, color-coded by mouse. <inline-formula><mml:math id="inf2"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>36.6</mml:mn></mml:mrow></mml:math></inline-formula> for a main effect of cue in a two-way ANOVA including an effect of subject. (<bold>D</bold>) Same as (<bold>C </bold>), for the third session of each odor set (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mice). <inline-formula><mml:math id="inf4"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>5.4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for a t-test comparing anticipatory licks on CS+ and CS50 trials. (<bold>E</bold>) Neuropixels probe tracks labeled with fluorescent dye (red) in cleared brain (autofluorescence, green). AP, anterior/posterior; ML, medial/lateral; DV, dorsal/ventral. Allen common-coordinate framework (CCF) regions delineated in gray. Outline of prelimbic area in purple (<bold>F</bold>) Reconstructed recording sites from all tracked probe insertions (<inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>44</mml:mn></mml:mrow></mml:math></inline-formula> insertions, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mice), colored by mouse. (<bold>G</bold>) Sample histology image of lens placement. Visualization includes DAPI (blue) and GCaMP (green) signal with lines indicating cortical regions from Allen Mouse Brain Common Coordinate Framework. (<bold>H</bold>) Location of all lenses from experimental animals registered to Allen Mouse Brain Common Coordinate Framework. Blue line indicates location of lens in (<bold>A</bold>). The dotted black line represents approximate location of tissue that was too damaged to reconstruct an accurate lens track. The white dotted line indicates prelimbic area (PL) borders.(<bold>I</bold>) ML and DV coordinates of all neurons recorded in one example session, colored by region, and spike raster from example PL neurons. (<bold>J</bold>) ROI masks for identified neurons and fluorescence traces from five example neurons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Anticipatory licking during the electrophysiology sessions.</title><p>(<bold>A</bold>) Mean anticipatory licks (change from baseline) for the CS+ and CS50 from odor set A (left) and B (right) for every session, color-coded by mouse. <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>32.07</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>26.93</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> in each odor set for a main effect of cue in a two-way ANOVA including an effect of subject. (<bold>B</bold>) As above, for the CS+ and CS− from odor set A (left) and B (right). <inline-formula><mml:math id="inf9"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>433.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>574.6</mml:mn></mml:mrow></mml:math></inline-formula> in each odor set for a main effect of cue in a two-way ANOVA including an effect of subject. (<bold>C</bold>) As above, for the CS50 and CS− from odor set A (left) and B (right). <inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>252.3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>66</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>450.1</mml:mn></mml:mrow></mml:math></inline-formula> in each odor set for a main effect of cue in a two-way ANOVA including an effect of subject.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Similar neural activity in prelimbic area using electrophysiology and calcium imaging.</title><p>(<bold>A</bold>) Heatmap of the normalized activity of each neuron recorded with electrophysiology in prelimbic area (PL), aligned to each of the six odors. All columns sorted by mean firing 0 - 1.5s following odor onset for odor set A CS+ trials. (<bold>B</bold>) As in (<bold>A</bold>), for all neurons imaged in PL on day 3 of each odor set. (<bold>C</bold>) The score from the first four principal components of the normalized activity presented in (<bold>A</bold>), with variance explained in parentheses. (<bold>D</bold>) As in (<bold>C</bold>), for the activity in (<bold>B</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig1-figsupp2-v1.tif"/></fig></fig-group><p>Using Neuropixels 1.0 and 2.0 probes (<xref ref-type="bibr" rid="bib30">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Steinmetz et al., 2021</xref>), we recorded the activity of individual neurons in PFC, including anterior cingulate area (ACA), frontal pole (FRP), prelimbic area (PL), infralimbic area (ILA), and orbital area (ORB) (<xref ref-type="bibr" rid="bib76">Wang et al., 2020b</xref>; <xref ref-type="bibr" rid="bib37">Laubach et al., 2018</xref>). We also recorded from: secondary motor cortex (MOs), including anterolateral motor cotex (ALM), which has a well-characterized role in licking <xref ref-type="bibr" rid="bib8">Chen et al., 2017</xref>; olfactory cortex (OLF), including dorsal peduncular area (DP), dorsal taenia tecta (TTd), and anterior olfactory nucleus (AON), which receive input from the olfactory bulb (<xref ref-type="bibr" rid="bib28">Igarashi et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Mori and Sakano, 2021</xref>); and striatum, including caudoputamen (CP) and nucleus accumbens (ACB), which are major outputs of PFC (<xref ref-type="bibr" rid="bib24">Heilbronner et al., 2016</xref>, <xref ref-type="fig" rid="fig1">Figure 1E–F</xref>). In a separate group of mice, we performed longitudinal 2-photon calcium imaging through a Gradient Refractive Index (GRIN) lens to track the activity of individual neurons in PL across several days of behavioral training (<xref ref-type="fig" rid="fig1">Figure 1G–H</xref>). Both techniques permitted robust measurement of the activity of neurons of interest and generated complementary results (<xref ref-type="fig" rid="fig1">Figure 1I–J</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p></sec><sec id="s2-2"><title>Graded cue and lick coding across the recorded regions</title><p>In the electrophysiology experiment, we isolated the spiking activity of 5332 individual neurons in regions of interest across 5 mice (449-1550 neurons per mouse, <xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). The activity of neurons in all regions exhibited varying degrees of modulation in response to the six cues (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Broadly, there was strong modulation on CS+ and CS50 trials that appeared to be common to both odor sets (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). Across regions, there was heterogeneity in both the magnitude and the timing of the neural modulation relative to odor onset (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Graded cue and lick coding across the recorded regions.</title><p>(<bold>A</bold>) Location of each recorded neuron relative to bregma, projected onto one hemisphere. Each neuron is colored by common-coordinate framework (CCF) region. Numbers indicate total neurons passing quality control from each region. (<bold>B</bold>) Mean normalized activity of all neurons from each region, aligned to odor onset, grouped by whether peak cue activity (0–2.5 s) was above (top) or below (bottom) baseline in held out trials. Number of neurons noted for each plot. (<bold>C</bold>) Example kernel regression prediction of an individual neuron’s normalized activity on an example trial. (<bold>D</bold>) CS+ trial activity from an example neuron and predictions with full model and with cues, licks, and reward removed. Numbers in parentheses are model performance (fraction of variance explained). (<bold>E</bold>) Coordinates relative to bregma of every neuron encoding only cues or only licks, projected onto one hemisphere. (<bold>F</bold>) Fraction of neurons in each region and region group classified as coding cues, licks, reward, or all combinations of the three. (<bold>G</bold>) Additional cue (left) or lick (right) neurons in region on Y-axis compared to region on x-axis as a fraction of all neurons, for regions with statistically different proportions (see Methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Task-related neural activity across brain regions.</title><p>(<bold>A</bold>) For each of the 5 mice in the electrophysiology experiment, the number of neurons recorded in each region. (<bold>B</bold>) Heatmap of the normalized activity of each neuron (<inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>51</mml:mn></mml:mrow></mml:math></inline-formula> trials per cue). All columns sorted by region and then by mean firing 0–1.5s following odor onset for odor set A CS+ trials. (<bold>C</bold>) Mean (+/− SEM) activity of neurons from four regions aligned to each cue type, grouped by whether peak cue activity (0–2.5 s) was above (top) or below (bottom) baseline in held out trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Identification of cue and lick cells with GLM.</title><p>(<bold>A</bold>) Mean variance explained (fraction) by linear models in each region for each session (x) and the mean (+/− SEM) across those sessions. (<bold>B</bold>) Mean (+/− SEM) activity of neurons encoding cues, licks, both, or neither aligned to each cue type, grouped by whether peak cue activity (0–2.5s) was above (top) or below (bottom) baseline in held out trials. (<bold>C</bold>) Normalized activity of every neuron encoding cues, licks, or both, aligned to CS+ onset, sorted by mean firing 0–1.5s following odor onset. (<bold>D</bold>) Mean (+/− SEM) activity of neurons encoding cues or licks, grouped as in (<bold>B</bold>), on CS50 trials, divided into rewarded (lighter colors) or unrewarded (darker colors) trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Validation of variance cutoff for variable coding.</title><p>(<bold>A</bold>) Fraction of neurons encoding cues, licks, and rewards in each region when varying the unique variance cutoff used (how much model performance drops when removing that variable). (<bold>B</bold>) As in (<bold>A</bold>), for models where the reduced ranks are fit to neural activity with shuffled cue onset times. (<bold>C</bold>) The fraction of cue cells (neurons with unique cue variance but no other variables) in each region when varying the variance cutoff. (<bold>D</bold>) Pairwise region comparisons with each variance cutoff. (<bold>E</bold>) Normalized activity of every neuron encoding cues, sorted by mean firing 0 - 1.5s following odor onset. Left: neurons only passing as cue only with a 2% cutoff but a 5% cutoff. Right: neurons passing as cue only with either a 2% or a 5% cutoff.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Comparing proportions of cue and lick neurons across regions.</title><p>(<bold>A</bold>) Fraction of neurons in each region classified as coding cues (left), licks (middle), or both (right), as well as estimated fraction(± 95% CI) with random effect of session (see Methods). Data also shown in <xref ref-type="fig" rid="fig2">Figure 2F</xref>. (<bold>B</bold>) Additional cue/lick/both cells in region on y-axis compared to region on x-axis as a fraction of all neurons, for regions with significantly different proportions. Pairwise comparisons in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. Data also shown in <xref ref-type="fig" rid="fig2">Figure 2G</xref>. (<bold>C</bold>) As in (<bold>A</bold>), for region groups. (<bold>D</bold>) As in (<bold>B</bold>), for region groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig2-figsupp4-v1.tif"/></fig></fig-group><p>To quantify the relative contribution of cues and conditioned responding (licking) to the activity of neurons in each region, we implemented reduced rank kernel regression (<xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>), using cues, licks, and rewards to predict neurons’ activity on held-out trials (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). To determine the contribution of cues, licks, and rewards to each neuron’s activity, we calculated unique variance explained by individually removing each predictor from the model and calculating the reduction in model performance (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>We identified individual neurons encoding cues, licks, or rewards as those for which that predictor uniquely contributed to 2% or more of their variance (a cutoff permitting no false positives and identifying neurons with robust task modulation, see Methods and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Neurons encoding cues (24% of all neurons), licks (11%), or both (16%) were most common. Neurons with any response to reward (independent of licking) were rare (5%) (<xref ref-type="bibr" rid="bib25">Horst and Laubach, 2013</xref>). Cue neurons were characterized by sharp responses aligned to odor onset; in contrast, lick neurons’ responses were delayed and peaked around reward delivery (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B–C</xref>), consistent with the timing of licks (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The activity of cue neurons on rewarded and unrewarded CS50 trials validated our successful isolation of neurons with cue but not lick responses (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2D</xref>). The spatial distributions of cue and lick cells were noticeably different (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). The differences could be described as graded across regions, with the most lick neurons in ALM, and the most cue neurons in olfactory cortex and ORB, though each type of neuron was observed in every region (<xref ref-type="fig" rid="fig2">Figure 2F–G</xref>, <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). Thus, our quantification of task encoding revealed varying proportions of cue and lick signaling across all regions.</p></sec><sec id="s2-3"><title>Cue value coding is present in all regions</title><p>To expand upon our analysis identifying cue-responsive neurons, we next assessed the presence of cue value coding in this population. The three cue types (CS+, CS50, or CS−) in our behavioral tasks varied in relative value according to the predicted probability of reward (<xref ref-type="bibr" rid="bib19">Fiorillo et al., 2003</xref>; <xref ref-type="bibr" rid="bib17">Eshel et al., 2016</xref>; <xref ref-type="bibr" rid="bib77">Winkelmeier et al., 2022</xref>). We reasoned that a neuron encoding cue value should have activities that scaled with the relative value of the cues (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We modeled this relationship on a per-neuron basis by scaling a single cue kernel by its reward probability (0, 0.5, or 1, see Methods, <xref ref-type="fig" rid="fig3">Figure 3B</xref>). This model describes cue activity as similar across odors of the same value, and scaling in magnitude according to each odor’s value. To consider alternative cue coding patterns, we also fit each neuron with 152 additional models containing all possible permutations of these values across the six cues, as well as models with selective responses for 1, 2, 3, 4, 5, or 6 cues, and determined which model best fit each neuron (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). If cue responses were exclusively sensory and followed known olfactory coding properties (<xref ref-type="bibr" rid="bib69">Stettler and Axel, 2009</xref>; <xref ref-type="bibr" rid="bib52">Pashkovski et al., 2020</xref>), there would be no bias toward the ranked value model (CS+&gt;CS50&gt;CS−). We found, however, that this model was the most frequent best model, accounting for 14% of cue neurons (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). We refer to these neurons as value cells. There were two additional patterns that emerged across the population of cue neurons. First, there was a large fraction best explained by the model with equivalent responses to all 6 cues, which we term untuned cells (14% of cue neurons). Second, many of the alternative models had coding patterns that were similar to the ranked value model, and these appeared to be overrepresented among cue neurons, as well. We quantified the similarity to ranked value by correlating the values assigned to each cue in each model with those assigned to the cues in the ranked value model; this approach revealed an enrichment in neurons best fit by models most similar to ranked value (35% of cue neurons, <xref ref-type="fig" rid="fig3">Figure 3C–D</xref>). We refer to neurons best fit by models most similar to the value model as value-like cells.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Robust value encoding and decoding among cue cells.</title><p>(<bold>A</bold>) Normalized activity of an example value cell with increasing modulation for cues with higher reward probability.(<bold>B</bold>) For the same neuron, model-fit cue kernel for the original value model and with one of the 152 alternatively-permuted cue coding models. (<bold>C</bold>) Distribution of best model fits across all cue neurons. Light blue is value model, purple is value-like models, gray is untuned model, and the remaining models are dark blue. Value-like models are shaded according to their correlation with ranked value, as illustrated in (<bold>D</bold>). Dashed line is chance proportion when assuming even distribution. (<bold>D</bold>) Schematic of value assigned to each of the six cues for many of the cue coding models (full schematic in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Value-like models are sorted by their correlation with the ranked value model. (<bold>E</bold>) Left: normalized activity of every value cell, sorted by mean firing 0–1.5s following odor set A CS+ onset. Right: mean normalized activity of all value cells, grouped by whether peak cue activity (0–2.5s) was above (top) or below (bottom) baseline in held out trials. Number of neurons noted for each plot. (<bold>F</bold>) As in (<bold>E</bold>), for value-like cells. (<bold>G</bold>) Accuracy (mean ± SEM across neurons) of decoded cue identity for single neurons of value (<inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>248</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), value-like (<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>606</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and untuned (<inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>238</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) neurons. * indicates where value, value-like, and untuned neurons significantly differed from each other and baseline (all <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, Bonferroni corrected). All pairwise comparisons in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. (<bold>H</bold>) Accuracy (mean ± SD across bootstrapped iterations) of decoded cue identity using different numbers of neurons. (<bold>I</bold>) Left: estimated value (mean ± SD across 1000 bootstrapped iterations) of held out CS+ (top) and CS− (bottom) trials using linear models trained on the activity of value, value-like, or untuned neurons. Right: accuracy (mean ± SD across bootstrapped iterations) of decoded cue value using these value estimates. * indicates where the accuracy of value neurons exceeded value-like and untuned neurons (all <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.016</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, bootstrapped). All pairwise comparisons in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Schematic of value model shuffles.</title><p>(<bold>A</bold>) For each of the 153 cue coding models, the value taken on by the variable cue kernel on trials corresponding to each of the six cue types. Values were 0, 0.5, or 1. Also, the fraction of cue neurons best fit by each model. Dashed line is chance proportion when assuming even distribution. (<bold>B</bold>) As in (<bold>A</bold>), sorted by correlation with the ranked value model. (<bold>C</bold>) Data from <xref ref-type="fig" rid="fig3">Figure 3C</xref> with color indicating models with the same values for cues of the same trial type across odor sets. Dashed line is chance proportion when assuming even distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Population analysis of value coding schemes.</title><p>(<bold>A</bold>) Projecting the activity (0 to 2.5s from odor onset) of all value and value-like cells onto the coding dimensions maximally separating CS− and CS+ (x-axis) and CS− and CS50 (y-axis). X marks baseline activity. (<bold>B</bold>) For the odor set A projection, distribution of 5000 bootstrapped angles between CS+ and CS50 vectors (baseline to peak). Value cells had a smaller angle than value-like cells, evidence of a linear value scale.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Value characteristics were particularly strong among the neurons we identified as value cells. In particular, there was strong modulation for the CS+ odors, moderate modulation for CS50 odors, and the least modulation for CS− odors (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). These characteristics were present to varying degrees in value-like cells, as well (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). A key characteristic of value cells, however, was the singular value axis on which the cues were encoded. This was evident when projecting population activity onto the dimensions separating CS+ trials from CS− trials and CS50 trials from CS− trials (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>); the trajectory of value neurons traveled the same angle in this space for CS+ and CS50 trials, but differed for value-like (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). We additionally characterized the coding properties of these populations with single-unit and pseudo ensemble decoding. For individual neurons decoding the six cue identities, performance was better using value cells than value-like or untuned cells (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). At the population level, however, all groups of neurons performed similarly (<xref ref-type="fig" rid="fig3">Figure 3H</xref>). A key feature of a value signal beyond decoding cue identity, though, is the ability to represent many distinct cues along a shared value axis. Therefore, the value cells should be able to decode the value of a cue never presented during the training of the model. With this approach, models trained on value cells had better predictions of held-out cue value, leading to higher decoding accuracy (CS+, CS50, or CS−), compared to value-like and untuned cells (<xref ref-type="fig" rid="fig3">Figure 3I</xref>). Therefore, we successfully identified a population of neurons strongly encoding key features of value.</p><p>Interestingly, the frequency of value cells was similar across the recorded regions (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Despite the regional variability in the number of cue cells broadly (<xref ref-type="fig" rid="fig2">Figure 2F–G</xref>), there were very few regions that statistically differed in their proportions of value cells (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Overall, there were slightly more value cells across all of PFC than in motor and olfactory cortex (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Although the olfactory cortex had the most cue cells, these were less likely to encode value than cue cells in other regions (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Value-like cells were also widespread; they were less frequent in the motor cortex as a fraction of all neurons, but they were equivalently distributed in all regions as a fraction of cue neurons (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Widespread cue value coding.</title><p>(<bold>A</bold>) Fraction of neurons in each region and region group classified as value cells (blue) and other cue neurons (gray), as well as fraction (± 95% CI) estimated from a linear mixed effects model with random effect of session (see Methods). Prefrontal cortex (PFC) has more value cells than motor (<inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula>) and olfactory (<inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.00005</mml:mn></mml:mrow></mml:math></inline-formula>) cortex. All pairwise comparisons in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>. (<bold>B</bold>) As in (<bold>A</bold>), for value-like cells. Motor cortex has fewer value-like cells than PFC (<inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>8</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) and olfactory cortex (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). All pairwise comparisons in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>. (<bold>C</bold>) First principal component value cells from all regions. (<bold>D</bold>) As in (<bold>C</bold>), for value-like cells. (<bold>E</bold>) Accuracy of decoded cue value (mean ± SD across 1000 bootstrapped iterations) as in <xref ref-type="fig" rid="fig3">Figure 3I</xref>, using five (with replacement) value cells from each region (left) and 25 value cells from each region group (right) using cue-evoked (blue) and baseline (black) activity. No regions or region groups significantly differed from each other (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.46</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, Bonferroni corrected). All pairwise comparisons in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Relative proportions of value and value-like cells across regions.</title><p>(<bold>A</bold>) Additional cue value (left) or value-like (right) neurons in region on y-axis compared to region on x-axis as a fraction of all neurons, for regions with non-overlapping 95% confidence intervals. (<bold>B</bold>) As in (<bold>A</bold>), for region groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Value coding as a proportion of cue cells.</title><p>(<bold>A</bold>) Fraction of cue neurons in each region classified as coding value (left) or value-like (right), as well as estimated fraction(± 95% CI) with random effect of session (see Methods). (<bold>B</bold>) Additional value/value-like cue neurons in region on y-axis compared to region on x-axis as a fraction of all cue neurons, for regions with significantly different fractions. Pairwise comparisons in <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>. (<bold>C</bold>) As in (<bold>A</bold>), for region groups. (<bold>D</bold>) As in (<bold>B</bold>), for region groups.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Comparing prefrontal cortex (PFC) and striatum.</title><p>(<bold>A</bold>) Fraction of neurons in each region and region group classified as coding cues (left), licks (middle), or both (right), as well as estimated fraction(± 95% CI) with random effect of session (see Methods). (<bold>B</bold>) Fraction of neurons in each region and region group classified as coding value (left) or value-like (right), as well as estimated fraction(± 95% CI) with random effect of session. Light gray bars are remaining cue neurons not in that category.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig4-figsupp3-v1.tif"/></fig></fig-group><p>We next investigated the robustness of the value representation in each of our recorded regions. Principal component analysis on value and value-like cells from each region revealed similarly strong value-related dynamics across motor, prefrontal, and olfactory regions (<xref ref-type="fig" rid="fig4">Figure 4C–D</xref>). We quantified the robustness of value coding in each region by decoding cue value using selections of value cells from each region and found similar performance across all regions (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Taken together, these data illustrate that, in contrast to cue and lick coding broadly, value coding is similarly represented across the regions we sampled. In fact, this observation extended to the striatal regions we sampled as well, indicating that such value coding is widespread even beyond cortex (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>).</p><p>Because cue valuations can be influenced by preceding reward outcomes, we next considered whether the cue value signaling we detected was sensitive to the history of reinforcement (<xref ref-type="bibr" rid="bib45">Nakahara et al., 2004</xref>; <xref ref-type="bibr" rid="bib49">Ottenheimer et al., 2020</xref>; <xref ref-type="bibr" rid="bib77">Winkelmeier et al., 2022</xref>). To estimate the subjects’ trial-by-trial cue valuation, we fit a linear model predicting the number of anticipatory licks on each trial using cue type, overall reward history, and cue type-specific reward history as predictors. We found a strong influence of cue type-specific reward history and a more modest influence of overall reward history (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We used the model prediction of licks per trial as our estimate of trial value; the effects of reward history on lick rate were apparent when grouping trials by the value estimates from the trial value model (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A subset of cue cells incorporate reward history.</title><p>(<bold>A</bold>) Coefficient weight (± standard error from model fit) for reward outcome on the previous 10 trials of any type (left) and on the previous 10 trials of the same cue type (right) for the ‘trial value’ model: a linear model predicting the number of anticipatory licks on every trial of every session. Lick rates were normalized so that the maximum lick rate for each session was equal to 1. Colored lines are models fit to each individual mouse. (<bold>B</bold>) Mean (± SEM) lick rate across mice (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mice) on trials binned according to value estimated from the trial value model. (<bold>C</bold>) Normalized activity of an example history value cell with increasing modulation for cues of higher value. (<bold>D</bold>) For the same neuron, model-predicted activity with the original value model (left) and with the history model, which uses trial-by-trial value estimates from the trial value model (right). (<bold>E</bold>) For the same neuron, model-predicted activity using licks. Inset: variance explained using licks versus history for history neurons. (<bold>F</bold>) The activity of all cells in each category projected onto the coding dimension maximally separating CS− and CS+ for trials binned by value estimated from the trial value model. (<bold>G</bold>) The mean (± SD across 5000 bootstrapped selections of neurons) activity (1–2.5s from odor onset) along the coding dimension maximally separating CS− and CS+ for trials binned by value estimated from the lick model. (<bold>H</bold>) The mean (± SD across 5000 bootstrapped selections of neurons) slope of the activity on CS50 trials regressed onto the trial value model estimate for those trials. History and lick cells had greater slopes than the other groups (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0003</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, see <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). (<bold>I</bold>) Fraction of neurons in each region and region group classified as history cells (light blue) and other cue neurons (gray), as well as estimated fraction (± 95% CI) with random effect of session (see Methods). Prefrontal cortex (PFC) had more history cells than motor (<inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0016</mml:mn></mml:mrow></mml:math></inline-formula>) and olfactory (<inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.00053</mml:mn></mml:mrow></mml:math></inline-formula>) cortex. All pairwise comparisons in <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig5-v1.tif"/></fig><p>We, therefore, investigated whether value cells showed similar trial-by-trial differences in their cue-evoked firing rates (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). To test this, we compared the fit of our original cue coding models (<xref ref-type="fig" rid="fig3">Figure 3B–D</xref>) with an alternative model in which the kernel scaled with the per-trial value estimates from our trial value model (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Overall, 5% of cue cells, including 15% of the value cells, were best fit by the history model. Although the number of anticipatory licks per trial was used to generate the trial value estimates, the precise licking pattern on those trials was a poorer predictor of neural responses than the trial value-scaled cue kernel model (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). To further evaluate the history component of these neurons, we calculated these neurons’ activity on CS50 trials of varying value estimates from the trial value model and projected it onto the population dimension maximizing the separation between CS+ and CS−. We hypothesized that high value CS50 trials would be closer to CS+ activity while low value CS50 trials would be closer to CS− activity. Indeed, history cells (and lick cells) demonstrated graded activity along this dimension, in contrast to non-history value, value-like, and untuned cells (<xref ref-type="fig" rid="fig5">Figure 5F–H</xref>). Finally, we examined the regional distribution of history cells and found low numbers across all regions, but with a higher prevalence overall in PFC than in motor and olfactory cortex (<xref ref-type="fig" rid="fig5">Figure 5I</xref>), lending additional support for slightly enhanced value coding in PFC.</p></sec><sec id="s2-4"><title>Cue coding emerges along with behavioral learning</title><p>To determine the timescales over which these coding schemes emerged and persisted, we performed longitudinal 2-photon calcium imaging and tracked the activity of individual neurons across several days of behavioral training (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We targeted a GRIN lens to PL, a location with robust cue and lick coding (<xref ref-type="fig" rid="fig2">Figure 2F</xref>) and where cue responses were predominantly value or value-like (<xref ref-type="fig" rid="fig4">Figure 4A–B</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Mice (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>) developed anticipatory licking during the first sessions of odor set A (A1) that differentiated CS+ trials from CS50 (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.015</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and CS− (<inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>7.0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0002</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) trials and CS50 trials from CS− (<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3.7</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) trials (<xref ref-type="fig" rid="fig6">Figure 6B–C</xref>). Visualizing the normalized activity across the imaging plane following CS+ presentation early and late in session A1 revealed a pronounced increase in modulation across this first session (<xref ref-type="fig" rid="fig6">Figure 6D–E</xref>). Individual neurons (<inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>705</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 41-165 per mouse) also displayed a notable increase in modulation in response to the CS+ after task learning (<xref ref-type="fig" rid="fig6">Figure 6F</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Acquisition of conditioned behavior and cue encoding in prefrontal cortex (PFC).</title><p>(<bold>A</bold>) Training schedule for five of the mice in the calcium imaging experiment. An additional three were trained only on odor set A. (<bold>B</bold>) Mean (± SEM) licking on early (first 60) and late (last 60) trials from day 1 of odor set A (<inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> mice). (<bold>C</bold>) Mean (± SEM) baseline-subtracted anticipatory licks for early and late trials from each day of odor set A. Thin lines are individual mice (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> mice). (<bold>D</bold>) Standard deviation of fluorescence from example imaging plane. (<bold>E</bold>) Normalized activity of each pixel following CS+ presentation on early and late trials of session A1. (<bold>F</bold>) Normalized deconvolved spike rate of all individual neurons on early and late trials of session A1. (<bold>G</bold>) Proportion of neurons classified as coding cues, licks, rewards, and all combinations for each third of session A1. (<bold>H</bold>) Mean(± SEM across mice) unique variance explained by cues, licks, and rewards for neurons from each mouse. Thin lines are individual mice. Unique variance was significantly different across session thirds for cues (<inline-formula><mml:math id="inf38"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.71</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></inline-formula>) but not licks (<inline-formula><mml:math id="inf40"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.37</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.69</mml:mn></mml:mrow></mml:math></inline-formula>) or reward (<inline-formula><mml:math id="inf42"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.65</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.53</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> mice, one-way ANOVA). (<bold>I</bold>) Mean (± SEM) normalized deconvolved spike rate for cells coding cues (<inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>84</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> above, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> below), licks (<inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>91</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> above, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> below), both (<inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>31</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> above, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> below), or neither (<inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>307</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> above, <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>153</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> below) on early and late trials, sorted by whether peak cue activity (0–2.5 s) was above (top) or below (bottom) baseline for late trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig6-v1.tif"/></fig><p>To determine whether this increase in activity was best explained by a cue-evoked response, licking, or both, we again used kernel regression to fit and predict the activity of each neuron for early, middle, and late trials in session A1. The number of individual neurons encoding cues more than doubled from early to late A1 trials (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). The unique variance cues increased across this first session, in contrast to licks and reward (<xref ref-type="fig" rid="fig6">Figure 6H</xref>). This stark change in cue coding was also noticeable when plotting neurons encoding cues, licks, or both, as defined at the end of the sessions, on both early and late trials (<xref ref-type="fig" rid="fig6">Figure 6I</xref>). These data indicated that PFC neural activity related to cues (but not licks) rapidly emerge during initial learning of the behavioral task.</p></sec><sec id="s2-5"><title>Cue and lick coding is stable across days</title><p>We next assessed whether cue and lick coding were stable across days. By revisiting the same imaging plane on each day of training, we were able to identify neurons that were present on all three days of odor set A training (<inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>371</mml:mn></mml:mrow></mml:math></inline-formula>, 20-65 per mouse) (<xref ref-type="fig" rid="fig7">Figure 7A–B</xref>). There was remarkable conservation of task responding across days, both on an individual neuron level (<xref ref-type="fig" rid="fig7">Figure 7C</xref>) and across all imaged neurons (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). In fact, neurons were much more correlated with their own activity on the subsequent day than would be expected by chance (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>). To further quantify coding stability, we fit our kernel regression to the activity of each neurons on session A3 (<xref ref-type="fig" rid="fig7">Figure 7F</xref>) and then used these models to predict activity in early, middle, and late trials on sessions A1-3. Session A3 model predictions were most highly correlated with true activity during A3, but they outperformed shuffle controls at all time points, demonstrating preservation of a learned coding scheme (<xref ref-type="fig" rid="fig7">Figure 7G</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1B</xref>). We then asked more specifically whether cells coding cues, licks, and both maintained their coding preferences across days. For each group of cells, we calculated their unique cue, lick, and reward variance at each time point. The preferred coding of each group, as defined in session A3, was preserved in earlier days (<xref ref-type="fig" rid="fig7">Figure 7H</xref>). Thus, cue and lick coding are stable properties of PFC neurons across multiple days of behavioral training.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Cue and lick coding is stable across days.</title><p>(<bold>A</bold>) Standard deviation fluorescence from example imaging plane. (<bold>B</bold>) Masks (randomly colored) for all tracked neurons from this imaging plane. (<bold>C</bold>) Deconvolved spike rate on every CS+ trial from all three sessions of odor set A for an example neuron. Vertical dashed line is reward delivery. Color axis as in (<bold>D</bold>). (<bold>D</bold>) Normalized deconvolved spike rate for all tracked neurons on all three sessions of odor set A. (<bold>E</bold>) Correlation between the activity of a given neuron in one session and its own activity in the subsequent session, quantified as a percentile out of correlations with the activity of all other neurons on the subsequent day. Plotted as the median for each subject and the mean (± SEM) across these values. Real data was more correlated than shuffled data (<inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0078</mml:mn></mml:mrow></mml:math></inline-formula> for both comparisons, Wilcoxon signed-rank test). (<bold>F</bold>) Fraction of tracked neurons coding cues, licks, rewards, and their combinations on day 3. (<bold>G</bold>) Model performance when using models from session A3 to predict the activity of individual neurons across session thirds of odor set A training, plotted as mean (± SEM) correlation between true and predicted activity across mice, normalized to the correlation between model and training data. Thin lines are individual mice. Performance was greater than shuffled data at all time points (<inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, Bonferroni-corrected, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> mice). Non-normalized data in <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>. (<bold>H</bold>) Mean (± SEM across mice) unique cue, lick, and reward variance for cells classified as coding cues, licks, both, or neither on session A3. A3 cue cells had increased cue variance in A2 (<inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, see Methods) and A1 (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) relative to lick and reward variance. Same pattern for A3 lick cells in A2 (<inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and A1 (<inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Correlation across days in prelimbic area (PL).</title><p>(<bold>A</bold>) Cumulative distribution of percentile of correlation for the activity of a given neuron with its own activity on the subsequent day compared to its correlation with the activity of all other neurons. True data (black) and shuffled data (gray), revealing strong enrichment of correlated activity for a tracked neuron across days. (<bold>B</bold>) Model performance when using models from session A3 to predict the activity of individual neurons across session thirds of odor set A training, plotted as mean (± SEM) correlation between true and predicted activity across mice. Thin lines are individual mice. Performance was greater than shuffled data at all time points (<inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) except early day 1 (<inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.21</mml:mn></mml:mrow></mml:math></inline-formula>, Bonferroni-corrected, <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> mice).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig7-figsupp1-v1.tif"/></fig></fig-group><p>A subset of mice (<inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) also learned a second odor set (odor set B), presented on separate days. Activity was very similar for both odor sets, evident across the entire imaging plane (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), for individual tracked neurons (<inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>594</mml:mn></mml:mrow></mml:math></inline-formula>, 81-153 per mouse) (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>), and for kernel regression classification of these neurons (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). Notably, odor set A models performed similarly well at predicting both odors set A and odor set B activity (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). Moreover, cue, lick, and both neurons maintained their unique variance preference across odor sets (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). Finally, to investigate the presence of value coding across odor sets over separate days, we fit tracked cue neurons with the value model and its shuffles. Even with odor sets imaged on separate days (days 5 and 6 of training, A3 and B3), we again found that the value and value-like models were the best models for sizable fractions (9% and 47%, respectively) of cue neurons, demonstrating that value coding is conserved across stimulus sets on consecutive days (<xref ref-type="fig" rid="fig8">Figure 8E–G</xref>). Given the prominence of value-like signals in this imaged population, we then assessed the stability of cue cells with preferential CS+ responses across the tracked A1-3 sessions and found conservation of a value-like coding pattern (<xref ref-type="fig" rid="fig8">Figure 8H</xref>) and, as with the whole population (<xref ref-type="fig" rid="fig7">Figure 7G</xref>), greater correlation in activity across days than expected by chance (<xref ref-type="fig" rid="fig8">Figure 8I</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Stable cue coding across separately trained odor sets.</title><p>(<bold>A</bold>) Normalized activity of all pixels in the imaging plane following CS+ presentation on the third day of each odor set (A3 and B3, days 5 and 6 of training). (<bold>B</bold>) Fraction of neurons coding for cues, licks, rewards, and their combinations in A3 and B3 (days 5 and 6). (<bold>C</bold>) Mean (± SEM, across mice) correlation between activity predicted by odor set A3 models and its training data (A3, cross-validated) or activity in B3, for true (black) and trial shuffled (gray) activity. Thin lines are individual mice. <inline-formula><mml:math id="inf66"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>3.2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></inline-formula> for main effect of odor set, <inline-formula><mml:math id="inf68"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>135</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for main effect of shuffle, <inline-formula><mml:math id="inf70"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.16</mml:mn></mml:mrow></mml:math></inline-formula> for interaction, <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mice, two-way ANOVA. (<bold>D</bold>) Mean (± SEM, across mice) unique cue, lick, and reward variance for cells classified as coding cues, licks, both, or neither for odor set A. For each category, odor set A unique variance preference was maintained for odor set B (<inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) except for both cells, for which lick and reward variance were not different in odor set B (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.22</mml:mn></mml:mrow></mml:math></inline-formula>, Bonferroni-corrected, <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mice). (<bold>E</bold>) Distribution of best model fits across all cue cells, with colors from <xref ref-type="fig" rid="fig3">Figure 3C</xref>. Dashed line is chance proportion when assuming even distribution. (<bold>F</bold>) Left: normalized activity of every value cell, sorted by mean firing 0–1.5s following odor set A CS+ onset. Right: mean normalized activity of all value cells, grouped by whether peak cue activity (0–2.5 s) was above (top) or below (bottom) baseline in held out trials. Number of neurons noted for each plot. (<bold>G</bold>) As in (<bold>E</bold>), for value-like cells. (<bold>H</bold>) Mean (± SEM, across neurons) activity of cue cells tracked across A1, A2, and A3 with preferential CS+ firing, defined on half of A3 trials and plotted for the other half of A3 trials and all of A1 and A2 trials. (<bold>I</bold>) For neurons in (<bold>H</bold>), correlation between a neuron’s activity in one session and its own activity in the subsequent session, quantified as a percentile out of correlations with the activity of all other neurons on the subsequent day. Plotted as the median for each subject (<inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> with CS+ preferring cue cells) and the mean (± SEM) across these values. Real data was more correlated than shuffled data (<inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.016</mml:mn></mml:mrow></mml:math></inline-formula> A1:A2, <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.031</mml:mn></mml:mrow></mml:math></inline-formula> A2:A3, Wilcoxon signed-rank test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84604-fig8-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our experiments assessed how coding for reward-predicting cues and reward-seeking actions differed across brain regions and across multiple days of training. We found coding for cues and licks in all regions we sampled, but their proportions varied in a graded way across those regions. In contrast to regional differences in the proportion of cue-responsive neurons, cue-value cells were present in all regions and value could be decoded from them with similar accuracy regardless of the region. Coding for cue value was greatly overrepresented compared to alternative cue coding schemes and, in a subset of neurons, incorporated the recent reward history. Cue coding was established within the first day of training and neurons encoding cues or licks maintained their coding preference across multiple days of the task; the value characteristics of cue cells were also maintained across days. These results demonstrate widespread value coding and stability of cue and lick codes in PFC.</p><sec id="s3-1"><title>Graded cue and lick coding across regions</title><p>We found robust and separable coding for licks and cues (and combined coding of both) in all regions using electrophysiology and in PL using calcium imaging. The widespread presence of lick coding is consistent with recent reports of distributed movement and action coding (<xref ref-type="bibr" rid="bib70">Stringer et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>); however, we saw sizable differences in the amount of lick coding across recorded regions. Notably, ALM had the greatest number of lick neurons, as well as the fewest cue neurons, perhaps reflecting its specialized role in the preparation and execution of licking behavior (<xref ref-type="bibr" rid="bib8">Chen et al., 2017</xref>). Conversely, the olfactory cortical regions DP, TTd, and AON had the most cue neurons (especially non-value coding cue neurons), suggesting a role in early odor identification and processing (<xref ref-type="bibr" rid="bib43">Mori and Sakano, 2021</xref>). PFC subregions balanced lick and cue coding, consistent with their proposed roles as association areas (<xref ref-type="bibr" rid="bib42">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib33">Klein-Flügge et al., 2022</xref>), but there was variability within PFC as well. In particular, ORB had a greater fraction of cue cells than any other subregions, consistent with its known dense inputs from the olfactory system (<xref ref-type="bibr" rid="bib53">Price, 1985</xref>; <xref ref-type="bibr" rid="bib54">Price et al., 1991</xref>; <xref ref-type="bibr" rid="bib15">Ekstrand et al., 2001</xref>). Thus, our results establish that the neural correlates of this Pavlovian conditioned behavior consist of a gradient of cue and response coding rather than segmentation of sensory and motor responses.</p></sec><sec id="s3-2"><title>Widespread value signaling</title><p>Value signals can take on many forms and occur throughout task epochs. In our experiments, we focused on the predicted value associated with each conditioned stimulus, which is crucial for understanding how predictive stimuli produce motivated behavior (<xref ref-type="bibr" rid="bib4">Berridge, 2004</xref>). Surveys of value coding in primate PFC have found individual neurons correlated with stimulus-predicted value in many subregions, with the strongest representations typically in ORB (<xref ref-type="bibr" rid="bib55">Roesch and Olson, 2004</xref>; <xref ref-type="bibr" rid="bib58">Sallet et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">Hunt et al., 2018</xref>). In rodents, there is also a rich literature on value signaling in ORB (<xref ref-type="bibr" rid="bib60">Schoenbaum et al., 2003</xref>; <xref ref-type="bibr" rid="bib73">van Duuren et al., 2009</xref>; <xref ref-type="bibr" rid="bib72">Sul et al., 2010</xref>; <xref ref-type="bibr" rid="bib64">Stalnaker et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Namboodiri et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Kuwabara et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>), but there have also been many reports of value-like signals in frontal cortical regions beyond ORB (<xref ref-type="bibr" rid="bib47">Otis et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>; <xref ref-type="bibr" rid="bib35">Kondo and Matsuzaki, 2021</xref>). In our present experiment, we sought to expand upon these rodent results by separating cue activity from licking, which tracks the value and may confound interpretation, by including more than two cue types, which provided a rich space to assess value coding, and by sampling from many frontal regions in the same experiment.</p><p>When considering the number of neurons responsive to cues rather than licks, our data confirmed the importance of ORB, which has more cue-responsive neurons than the motor and other prefrontal regions, but, beyond cue responsiveness, we were interested in identifying specific cue coding patterns pertaining to value. By analyzing the activity of cue-responsive neurons across all six odors predicting varying probabilities of reward, we were able to isolate neurons coding value, as well as those with value-like signals that could easily be misconstrued as value-coding in a task with fewer cues and value levels. Included in the value-like models are coding patterns that bias their activity for higher value odors without fitting our strict linear ranked value criteria; for instance, selective firing for one or two of the CS+ odors. The enrichment of these models among cue responsive neurons, even in the olfactory cortex, indicates the prevalence of value-biased coding schemes for odor-responsive neurons across brain regions. The question remains of where odor information is first shaped according to value. There have been multiple reports of some association-related modification of odor representations as early as the olfactory bulb (<xref ref-type="bibr" rid="bib14">Doucette et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Li et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Chu et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Koldaeva et al., 2019</xref>). Considering we detected value and especially value-like coding in AON, DP, and TTd, perhaps these regions are a crucial first step in processing and amplifying task-related input from the olfactory bulb. Because they provide input to PFC (<xref ref-type="bibr" rid="bib28">Igarashi et al., 2012</xref>; <xref ref-type="bibr" rid="bib5">Bhattarai et al., 2022</xref>), they may be an important source of the cue coding we observed there.</p><p>The distribution of cue cells with linear coding of value was mostly even across regions, with slight enrichment overall in PFC compared to the motor and olfactory cortex, but no subregional differences in PFC. Importantly, cue value could be decoded from value cells in each region with similar accuracy. One consequence of a widely distributed value signal is that manipulating only one subregion would be less likely to fully disrupt value representations, which is consistent with the results of studies comparing functional manipulations across PFC (<xref ref-type="bibr" rid="bib10">Chudasama and Robbins, 2003</xref>; <xref ref-type="bibr" rid="bib63">St Onge and Floresco, 2010</xref>; <xref ref-type="bibr" rid="bib12">Dalton et al., 2016</xref>; <xref ref-type="bibr" rid="bib74">Verharen et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>). Different subregional impacts on behavior may reveal biases in how the value signal in each region contributes to reward-related behaviors, for instance during learning or expression of a reward associations (<xref ref-type="bibr" rid="bib47">Otis et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Namboodiri et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>). A related interpretation is that, in this task, there may be other properties that correlate with cue value, and the homogeneous value representation we observed across regions masks regional differences in tuning to these other correlated features, such as motivation (<xref ref-type="bibr" rid="bib55">Roesch and Olson, 2004</xref>) and a host of related concepts, including salience, uncertainty, vigor, and arousal (<xref ref-type="bibr" rid="bib65">Stalnaker et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Hayden and Niv, 2021</xref>; <xref ref-type="bibr" rid="bib79">Zhou et al., 2021</xref>), which can have different contributions to behavior. This interpretation is consistent with broader views that observations of ‘value’ signals are often misconstrued (<xref ref-type="bibr" rid="bib79">Zhou et al., 2021</xref>) and that pure abstract value may not be encoded in the brain at all (<xref ref-type="bibr" rid="bib23">Hayden and Niv, 2021</xref>). Although the identification of value in our task was robust to three levels of reward probability across two stimulus sets, the fact that this signal was widespread contributes to the case for revisiting the definition and interpretation of value to better understand regional specialization.</p><p>In our analysis, we uncovered a distinction between neurons encoding the overall value of cues and those with value representations that incorporated the recent reward history. Neurons with history effects were rare and most frequent in PFC. These neurons may have a more direct impact on behavioral output in this task, because the lick rate also incorporated recent reward history. Notably, the impact of reward history on these neurons was noticeable even prior to cue onset, consistent with a previously proposed mechanism for persistent value representations encoded in the baseline firing rates of PFC neurons (<xref ref-type="bibr" rid="bib3">Bari et al., 2019</xref>).</p></sec><sec id="s3-3"><title>Stability of PFC codes</title><p>Previous reports have observed drifting representations in PFC across time (<xref ref-type="bibr" rid="bib27">Hyman et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Malagon-Vina et al., 2018</xref>), and there is compelling evidence that odor representations in piriform drift over weeks when odors are experienced infrequently (<xref ref-type="bibr" rid="bib61">Schoonover et al., 2021</xref>). On the other hand, it has been shown that coding for odor association is stable in ORB and PL, and that coding for odor identity is stable in piriform (<xref ref-type="bibr" rid="bib75">Wang et al., 2020a</xref>), with similar findings for auditory Pavlovian cue encoding in PL (<xref ref-type="bibr" rid="bib47">Otis et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Grant et al., 2021</xref>) and ORB (<xref ref-type="bibr" rid="bib46">Namboodiri et al., 2019</xref>). We were able to expand upon these data in PL by identifying both cue and lick coding and showing separable, stable coding of cues and licks across days and across sets of odors trained on separate days. We were also able to detect value coding common to two stimulus sets presented on separate days, and conserved value features across the three training sessions. Notably, the model with responses only to CS+ cues best fit a larger fraction of imaged PL neurons than the ranked value model, a departure from the electrophysiology results. It would be interesting to know if this is due to a bias introduced by the calcium imaging approach, the slightly reduced CS50 licking relative to CS+ licking in the imaging cohort, or the shorter imaging experimental timeline.</p><p>The consistency in cue and lick representations we observed indicates that PL serves as a reliable source of information about cue associations and licking during reward-seeking tasks, perhaps contrasting with other representations in PFC (<xref ref-type="bibr" rid="bib27">Hyman et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Malagon-Vina et al., 2018</xref>). Interestingly, the presence of lick, but not cue coding at the very beginning of the first session of training suggests that lick cells in PL are not specific to the task but that cue cells are specific to the learned cue-reward associations. Future work could expand upon these findings by examining stimulus-independent within session value coding across many consecutive days.</p><p>Overall, our work emphasizes the importance of evaluating the regional specialization of neural encoding with systematic recordings in many regions using the same task. Future work will clarify whether cue value is similarly widely represented in other reward-seeking settings and whether there are regional differences in the function of the value signal.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Subjects (<inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> for electrophysiology, <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> for calcium imaging) were male and female C57BL/6 mice single-housed on a 12 hr light/dark cycle and aged 12–28 weeks at the time of recordings. Imaging experiments were performed during the dark cycle, electrophysiology during the light cycle. Mice were given free access to food in their home cages for the duration of the experiment. Mice were water restricted for the duration of the experiments and maintained at around 85% of their baseline weight (<xref ref-type="bibr" rid="bib21">Guo et al., 2014a</xref>). All experimental procedures were performed in strict accordance with protocols 4450–01 and 4461–01 approved by the Animal Care and Use Committee at the University of Washington.</p></sec><sec id="s4-2"><title>Surgical procedures</title><p>Mice were anesthetized with isoflurane (5%) and maintained under anesthesia for the duration of the surgery (1–2%). Mice received injections of carprofen (5 mg/kg) prior to incision.</p><sec id="s4-2-1"><title>Electrophysiology</title><p>A brief (1 hr) initial surgery was performed, as previously described (<xref ref-type="bibr" rid="bib22">Guo et al., 2014b</xref>; <xref ref-type="bibr" rid="bib66">Steinmetz et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>), to implant a steel headbar (approximately 15 × 3 × 0.5 mm, 1 g) for head fixation and a 3D-printed recording chamber exposing the skull for subsequent craniotomies. Briefly, an oval incision was made extending from the interparietal bone to the frontonasal suture, skirting the ocular area. The skin and periosteum were removed to expose the entire dorsal surface of the skull. Skull yaw, pitch, and roll were leveled, and exposed bone was texturized with a brief application of green activator (Super-Bond C&amp;B, Sun Medical). The incision was secured to the skull with the application of cyanoacrylate (VetBond; World Precision Instruments), and the 3D-printed recording chamber was attached to the skull with L-type radiopaque polymer (Super-Bond C&amp;B). A thin layer of cyanoacrylate was applied to the skull inside the chamber and allowed to dry. Multiple (2-4) thin layers of UV-curing optical glue (Norland Optical Adhesives #81, Norland Products) were applied to the skull inside the chamber and cured with UV light to protect the exposed bone. The headbar was attached to the skull over the interparietal bone posterior to the chamber with Super-Bond polymer, and more polymer was applied around the headbar and chamber. Following recovery, a second brief (15–30 min) surgery was conducted to perform craniotomies for Neuropixels probe insertion. Briefly, following induction of anesthesia a small (2 × 1.5 mm (w × h)) craniotomy was made over the frontal cortex (+2.5–1 mm AP, ± 2.5–0.3 mm ML) with a handheld dental drill. The craniotomy was covered with a soft silicone gel (DOWSIL 3–4680) and the recording chamber was covered with a 3D-printed lid sealed with Kwik-Cast elastomer to protects craniotomy from dust.</p></sec><sec id="s4-2-2"><title>Calcium imaging</title><p>A Gradient-Refractive Index (GRIN) lens and metal headcap were implanted following previously described procedures (<xref ref-type="bibr" rid="bib46">Namboodiri et al., 2019</xref>) with the following modifications. In most mice, once the dura was removed from the craniotomy, we performed two injections of 0.5 <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> of virus (1 <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> total) containing the GCaMP gene construct (AAVDJ-CamKIIa-GCaMP6s, <inline-formula><mml:math id="inf83"><mml:mrow><mml:mn>5.3</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>12</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> viral particles/mL from UNC Vector core lot AV6364) using a glass pipette microinjector (Nanoject II) at Bregma +1.94 mm AP, 0.3, and 1.2 mm ML, –2 mm DV. Ten minutes elapsed before the microinjector withdrawal to allow the virus to diffuse away from each infusion site. Then, mice were implanted with a 1 × 4 mm GRIN lens (Inscopix) aimed at +1.94 mm AP, 0.6 mm ML, and –1.8 mm DV. A subset of mice did not receive viral injections; instead, a lens with the imaging face coated 1 <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> of the GCaMP6s virus mixed with 5% aqueous silk fibroin solution (<xref ref-type="bibr" rid="bib29">Jackman et al., 2018</xref>) was implanted at the same coordinate. GCaMP expression and transients were similar in both preparations. Mice were allowed to recover for at least 5 weeks before experiments began.</p></sec></sec><sec id="s4-3"><title>Behavioral training</title><p>Mice were headfixed during training and recording sessions using either a headring (imaging experiments) or headbar (electrophysiology experiments). After initial habituation to head fixation, mice were first trained to lick for <inline-formula><mml:math id="inf85"><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> rewards of 10% sucrose solution, delivered every 8–12 s through a miniature inert liquid valve (Parker 003-0257-900). After 4–5 days of lick training, mice experienced their first odor exposure (without reward delivery). Odors were delivered for a total of 1.5 s using a 4-channel olfactometer (Aurora 206 A) with 10% odor flow rate and 800 SCCM overall flow rate of medical air. Odors were randomly assigned to sets and cue identities, counterbalanced across mice. Odors were -carvone, -limonene, alpha-pinene, butanol, benzaldehyde, and geranyl acetate (Sigma Aldrich 124931, 218367, 147524, 281549, 418099, 173495, respectively), selected because of they are of neutral valence to naive mice (<xref ref-type="bibr" rid="bib13">Devore et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Saraiva et al., 2016</xref>). Odors were diluted 1:10 in mineral oil and 10 μL was pipetted onto filter paper within the odor delivery vials (Thermo Fisher SS246-0040) prior to each session. Airflow was constant onto the mouse’s nose throughout the session and switched from clean air to scented air for the 1.5 s duration odor delivery on each trial.</p><p>On days 1–2 of Pavlovian conditioning, mice received 50–75 trials each of three odor cues (odor set A), followed by reward on 100% (CS+), 50% (CS50), or 0% (CS−) of trials, 2.5 s following the odor onset, with 8–12 s between odor presentations. On days 3–4 mice then received training for 2 days with a second odor set (odor set B) with three new odors. For electrophysiology experiments, the odors were subsequently presented in the same sessions in six blocks of 51 trials. Odor set order alternated and was counterbalanced across days. For imaging experiments, mice received the third day of odor set A on day 5 and the third day of odor set B on day 6 of conditioning. An additional three imaging mice were only trained on one odor set.</p></sec><sec id="s4-4"><title>Electrophysiological recording and spike sorting</title><p>During recording sessions, mice were headfixed. Recordings were made using either Neuropixels 1.0 or Neuropixels 2.0 electrode arrays (<xref ref-type="bibr" rid="bib30">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Steinmetz et al., 2021</xref>), which have 384 selectable recording sites. Recordings were made with either 1.0 (1 shank, 960 sites), 2.1 (1 shank, 1280 sites),, or 2.4 (4 shanks, 5120 sites) probes, depending on the regions of interest. Probes were mounted to a dovetail and affixed to a steel rod held by a micromanipulator (uMP-4, Sensapex Inc). For later electrode track localization within the brain, probes were coated with a fluorescent dye (DiI, ThermoFisher Vybrant V22888) by holding 2 <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> in a droplet on the end of a micropipette and painting the probe shank. In each session, one or two probes were advanced through the silicone gel covering the craniotomy over the frontal cortex, then advanced to their final position at approximately 3 <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> s<sup>-1</sup>. Electrodes were allowed to settle for around 15 min before starting recording. Recordings were made in internal reference mode using the ‘tip’ reference site, with a 30 kHz sampling rate. Recordings were repeated at different locations on each of multiple subsequent days, performing an additional craniotomy over the contralateral frontal cortex. The resulting data were automatically spike sorted with Kilosort2.5 and Kilosort3 (<ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016422">SCR_016422</ext-link>; <xref ref-type="bibr" rid="bib51">Pachitariu et al., 2023</xref>), v2.5 and 3.0. Extracellular voltage traces were preprocessed with common-average referencing by subtracting each channel’s median to remove baseline offsets, then subtracting the median across all channels at each time point to remove common electrical artifacts. Sorted units were curated using automated quality control (<xref ref-type="bibr" rid="bib2">Banga, et al., 2022</xref>): exclusions were based on spike floor violations (the estimated proportion of spikes that were missed because they fell below the noise level of the recording, estimated false negative rate), and refractory period violations (the estimated proportion of spikes arising from the non-primary neuron, the estimated false positive rate due to contamination, with a 10% cutoff). Quality control accuracy was assessed by manually reviewing a subset of the data using the phy GUI (<ext-link ext-link-type="uri" xlink:href="https://github.com/kwikteam/phy">https://github.com/kwikteam/phy</ext-link>; <xref ref-type="bibr" rid="bib56">Rossant et al., 2021</xref>). Because Kilosort2.5 and Kilosort3 use different clustering algorithms that can be advantageous for different types of recordings (stability, region, number of channels), for each session, we used units sorted with either Kilosort2.5 or Kilosort3 depending on which yielded the greatest number of high-quality units for that session. Brain regions were only included for subsequent analysis if there were recordings from at least three subjects and a total of over 100 neurons in the region. When we analyzed all of the motor cortex together, we included ALM and MOs neurons. When we analyzed all of the olfactory cortex, we included DP, TTd, AON, and other neurons in PIR, EPd, and OLF. We relabeled PIR and EPd as OLF because there were not enough neurons to analyze them as separate regions.</p></sec><sec id="s4-5"><title>Imaging and ROI extraction</title><p>During imaging sessions, mice were headfixed and positioned under the 2-photon microscope (Bruker Ultima2P Plus) using a 20 x air objective (Olympus LCPLN20XIR). A Spectra-Physics InSight X3 tuned to 920 nm was used to excite GCaMP6s through the GRIN lens. Synchronization of odor and 10% sucrose delivery, lick behavior recordings, and 2-photon recordings were achieved with custom Arduino code. After recording, raw TIF files were imported into suite2p (<ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/suite2p">https://github.com/MouseLand/suite2p</ext-link>; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016434">SCR_016434</ext-link>; <xref ref-type="bibr" rid="bib71">Stringer et al., 2023</xref>), v0.13.0. We used their registration, region-of-interest (ROI) extraction, and spike deconvolution algorithms, inputting a decay factor of <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1.3</mml:mn></mml:mrow></mml:math></inline-formula> to reflect the dynamics of GCaMP6s, and manually reviewed putative neuron ROIs for appropriate morphology and dynamics. To find changes in activity across the entire imaging plane, found the mean pixel intensity for frames in the time of interest (2–2.5 s from CS+), subtracted the mean intensity of each pixel prior to cue onset (−2–0 s from all cues), and divided by the standard deviation for each pixel across those frames prior to cue onset.</p></sec><sec id="s4-6"><title>Histology</title><p>Animals were anesthetized with pentobarbital or isoflurane. Mice were perfused intracardially with 0.9% saline followed by 4% paraformaldehyde (PFA).</p><sec id="s4-6-1"><title>Electrophysiology</title><p>Brains were extracted immediately following perfusion and post-fixed in 4% paraformaldehyde for 24 h. In preparation for light sheet imaging brains were cleared using organic solvents following the 3DISCO protocol (<xref ref-type="bibr" rid="bib16">Ertürk et al., 2012</xref>) (<ext-link ext-link-type="uri" xlink:href="https://idisco.info/">https://idisco.info/</ext-link>), with some modification. Briefly, on day 1 brains were washed 3 X in PBS and dehydrated in a series of increasing MeOH concentrations (20%, 40%, 60%, 80%, 100%, 100%; 1 hr each) then incubated overnight for lipid extraction in 66% dichloromethane (DCM) in MeOH. On day 2 brains were washed 2 X twice in 100% MeOH for 1 hr each, then bleached overnight in 5% H<sub>2</sub>O<sub>2</sub> in MeOH at 4 °C. On day 3 brains were washed 2 X in 100% MeOH, then final lipid extraction was accomplished in a series of DCM incubations (3 hr in 66% DCM in MeOH, 2X 100% DCM for 15 min each) before immersion in dibenzyl ether (DBE) for refractive index matching. Brains were imaged on a light sheet microscope (LaVIsion Biotec UltraScope II) 2–7 days after clearing. Brains were immersed in DBE in the imaging well secured in the horizontal position, and illuminated by a single light sheet (100% width, 4 <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> thick) from the right. Images were collected through the 2 X objective at 1 X magnification, from the dorsal surface of the brain to the ventral surface in 10 <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> steps in 488 <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> (autofluorescence, 30% power) and 594 <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> (DiI, 2–10% power) excitation channels. The 1000 raw TIF images were compiled into a single multi-image file with 10 <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> voxels, then spatially downsampled to 25 <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> voxels for transformation to the Allen common-coordinate framework (CCF) volume (<xref ref-type="bibr" rid="bib76">Wang et al., 2020b</xref>) using the Elastix algorithm (<xref ref-type="bibr" rid="bib62">Shamonin et al., 2013</xref>). CCF-transformed volumes were used to generate CCF fluorescent probe tract locations (pixel coordinates along the probe tract) using Lasagna (<ext-link ext-link-type="uri" xlink:href="https://github.com/SainsburyWellcomeCentre/lasagna">https://github.com/SainsburyWellcomeCentre/lasagna</ext-link>; <xref ref-type="bibr" rid="bib7">Campbell et al., 2020</xref>). Probe tract CCF pixel coordinates (origin front, top, left) were transformed to bregma coordinates (origin bregma, x==ML, y==AP, and z==DV) in preparation for final integration with electrophysiology recordings using the International Brain Lab electrophysiology GUI (Faulkner M, Ephys Atlas GUI; 2020. <ext-link ext-link-type="uri" xlink:href="https://github.com/int-brain-lab/iblapps/tree/master/atlaselectrophysiology">https://github.com/int-brain-lab/iblapps/tree/master/atlaselectrophysiology</ext-link>; <xref ref-type="bibr" rid="bib18">Faulkner, 2020</xref>). For recording alignment, sorted spikes and RMS voltage on each channel were displayed spatially in relation to the estimated channel locations in Atlas space from the tracked probe. The recording sites were then aligned to the Atlas by manually identifying a warping such that recording sites were best fit to the electrophysiological characteristics of the brain regions (e.g. matching location of ventricles or white matter tracts with low firing activity bands). This procedure has been estimated to have a 70 µm error (<xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Liu et al., 2021</xref>). Individual neuron locations were determined using the recording channel brain coordinates of each unit’s maximum-amplitude waveform. We additionally assigned MOs neurons to the anterolateral motor cortex (ALM) if they were within a 0.75 mm radius of 2.5 mm AP, and 1.5 mm ML (<xref ref-type="bibr" rid="bib8">Chen et al., 2017</xref>).</p></sec><sec id="s4-6-2"><title>Calcium imaging</title><p>Following perfusion, intact heads were left in PFA for an additional week before brain extraction. Brains were then sliced on a Leica Vibratome (VT1000S) at 70 <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> before mounting and nuclear staining via Fluoroshield with DAPI (Sigma-Aldrich F6057-20ML). Slices with GRIN lens tracks were then imaged on a Zeiss Axio Imager M2 Upright Trinocular Phase Contrast Fluorescence Microscope with ApoTome. The resulting images were manually aligned to the Allen Brain Atlas to reconstruct the location of each GRIN lens.</p></sec></sec><sec id="s4-7"><title>Neuron tracking</title><p>To identify the same neurons across imaging sessions, we used two approaches. To track neurons across the two odor sets on days 5 and 6, we concatenated the TIF files from each session and extracted ROIs simultaneously. To track neurons across training days 1–3 for a single odor set, we manually identified ROIs from the ROI masks outputted by suite2p. We linked the ROIs using a custom Python script that permitted the selection of the same ROI across the three imaging planes using OpenCV and saved the coordinates on each day. The tracking results across days 1–3 from one subject is displayed in <xref ref-type="fig" rid="fig7">Figure 7B</xref>.</p></sec><sec id="s4-8"><title>Behavioral analysis</title><p>For electrophysiology experiments, the subject was illuminated with infrared light (850 nm, CMVision IR30) and eye and face movements were monitored. The right eye was monitored with a camera (FLIR CM3-U3-13Y3M-CS) fitted with a zoom lens (Thorlabs MVL7000) and long-pass filter (Thorlabs FEL0750), recording at 70 fps. Face movements were monitored with another camera (FLIR CM3-U3-13Y3M-CS, zoom lens Thorlabs MVL16M23, long-pass filter Thorlabs FEL0750) directed at a 2 × 3 cm mirror reflecting the left side of the face, recording at 70 fps. Licks were detected from the face video by thresholding the average intensity of an ROI centered between the lips and the lick spout, calculated for every frame. Interlick intervals were thresholded at 0.083 s for a maximum lick rate of 12 licks s<sup>-1</sup>. For calcium imaging experiments, eye and face movements were not monitored, and licks were detected with a capacitance sensor (MPR121, Adafruit Industries) connected to an Arduino board. To determine the impact of cues and previous outcomes on anticipatory licking, we fit a linear model on all electrophysiology sessions simultaneously (and for each mouse). We predicted the number of licks 0–2.5 s from odor onset using cue identity, outcomes on the previous 10 trials, outcomes on the previous 10 of that cue type, and the total number of presentations of that cue type so far (to account for cue-specific satiety) using ‘fitlm’ in MATLAB. When dividing sessions into ‘early’ and ‘late,’ we used the first 60 and last 60 trials of the session. When dividing sessions into thirds for the GLM (‘early,’ ‘middle,’ ‘late’), we used even splits of trials into thirds.</p></sec><sec id="s4-9"><title>PSTH creation</title><p>Peri-stimulus time histograms (PSTHs) were constructed using 0.1 s bins surrounding cue onset.</p><sec id="s4-9-1"><title>Electrophysiology</title><p>Neuron spike times were first binned into 0.02 s bins and smoothed with a half-normal causal filter (<inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mrow></mml:math></inline-formula> ms) across 50 bins. PSTHs were then constructed in 0.1 s bins surrounding each cue onset. Each bin of the PSTH was z-scored by subtracting the mean firing rate and dividing the standard deviation across the 0.1 s bins in the 2 s before all trials. When splitting responses by polarity (above/below baseline, <xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig3">3E–F</xref>–<xref ref-type="fig" rid="fig8">8H</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>), we used even trials to determine polarity and plotted the mean across odd trials for cross-validation.</p></sec><sec id="s4-9-2"><title>Calcium imaging</title><p>Frames were collected at 30 Hz with 2-frame averaging, so the fluorescence for each neuron and the estimated deconvolved spiking was collected at 15 Hz. We interpolated the smoothing filter from the electrophysiology analysis (which was calculated at 50 Hz) and applied it to the deconvolved spiking traces. We then constructed PSTHs in 0.1 s bins surrounding each cue onset and z-scored (same as electrophysiology).</p></sec><sec id="s4-9-3"><title>Licks</title><p>Licking PSTHS were constructed in 0.1 s bins surrounding cue onset. Each trial was then smoothed with a half-normal causal filter (<inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:math></inline-formula> ms). For the GLM, the lick rate was calculated across the whole session by first counting licks in either the 0.02 s (electrophysiology) or 15 Hz (imaging) bins, smoothed with a half-normal causal filter over 25 bins, and then converted to 0.1 s bins relative to each cue.</p></sec></sec><sec id="s4-10"><title>Kernel regression</title><p>To identify coding for cues, licks, and rewards in individual neurons, we fit reduced rank kernel-based linear model (<xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>).</p><sec id="s4-10-1"><title>Data preparation</title><p>The discretized firing rates <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>f</mml:mi><mml:mmultiscripts><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mprescripts/><mml:mi>n</mml:mi><mml:none/></mml:mmultiscripts></mml:mrow></mml:math></inline-formula> for each neuron <inline-formula><mml:math id="inf99"><mml:mi>n</mml:mi></mml:math></inline-formula> were calculated as described above for PSTH creation. We used the activity –1–6.5 s from each cue onset on every trial for our GLM analysis.</p></sec><sec id="s4-10-2"><title>Predictor matrix</title><p>The model included predictor kernels for cues (CS+, CS50, and CS− for each odor set, as relevant), licks (individual licks, lick bout start, and lick rate), and reward (initiation of consummatory bout). The cue kernels were supported over the window 0–5 s relative to the stimulus onset. The lick predictor kernels were supported from –0.3–0.3 s relative to each lick, from –0.3–2 s relative to lick bout start, and lick rate was shifted from –0.4–0.6 s in 0.2 s increments from original rate. The reward kernel was supported 0–4 s relative to first lick following reward delivery. For electrophysiology experiments, the model also included six constants that identified the block number, accounting for tonic changes in firing rate across blocks. Because not all cues were present in every block, this strategy prevented the cue kernels from being used to explain the baseline changes across blocks. For each kernel to be fit we constructed a Toeplitz predictor matrix of size <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>, in which <inline-formula><mml:math id="inf101"><mml:mi>T</mml:mi></mml:math></inline-formula> is the total number of time bins and <inline-formula><mml:math id="inf102"><mml:mi>l</mml:mi></mml:math></inline-formula> is the number of lags required for the kernel. The predictor matrix contains diagonal stripes starting each time an event occurs and 0 otherwise. The predictor matrices were horizontally concatenated to yield a global prediction matrix <inline-formula><mml:math id="inf103"><mml:mi mathvariant="bold">P</mml:mi></mml:math></inline-formula> of size <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> containing all predictor kernels. Rate vectors of all <inline-formula><mml:math id="inf105"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons were horizontally concatenated to form <inline-formula><mml:math id="inf106"><mml:mi mathvariant="bold">F</mml:mi></mml:math></inline-formula>, a <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix.</p></sec><sec id="s4-10-3"><title>Reduced-rank regression</title><p>To prevent noisy and overfit kernels we implemented reduced-rank regression (<xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>), which allows regularized estimation by factorizing the kernel matrix <inline-formula><mml:math id="inf108"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> into the product of a <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf110"><mml:mi mathvariant="bold">B</mml:mi></mml:math></inline-formula> and a <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf112"><mml:mi mathvariant="bold">W</mml:mi></mml:math></inline-formula>, minimizing the total error: <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mi mathvariant="bold">B</mml:mi><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf115"><mml:mi mathvariant="bold">PB</mml:mi></mml:math></inline-formula> consists of a set of ordered temporal basis functions that can be linearly combined to estimate the neuron’s firing rate over the whole training set and which results in the best possible prediction from any rank <inline-formula><mml:math id="inf116"><mml:mi>r</mml:mi></mml:math></inline-formula> matrix. To estimate each neuron’s kernel functions we generated the reduced rank predictor matrix <inline-formula><mml:math id="inf117"><mml:mi mathvariant="bold">PB</mml:mi></mml:math></inline-formula> for <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>, and estimated the weights <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:msub><mml:mi/><mml:mi mathvariant="bold">n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to minimize the squared error <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mi mathvariant="bold">B</mml:mi><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo mathvariant="bold" stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with elastic net regularization (using the MATLAB function ‘lassoglm’) with parameters <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:mn>0.005</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.02</mml:mn><mml:mo>,</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> , using fourfold cross-validations to determine the optimal value for <inline-formula><mml:math id="inf123"><mml:mi>λ</mml:mi></mml:math></inline-formula> for each neuron. The kernel functions for neuron <inline-formula><mml:math id="inf124"><mml:mi>n</mml:mi></mml:math></inline-formula> was then unpacked from the L-length vector obtained by multiplying the first <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> columns of <inline-formula><mml:math id="inf126"><mml:mi mathvariant="bold">B</mml:mi></mml:math></inline-formula> by <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:msub><mml:mi/><mml:mi mathvariant="bold">n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (i.e. <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi mathvariant="bold">k</mml:mi><mml:mmultiscripts><mml:mo>=</mml:mo><mml:mprescripts/><mml:mi mathvariant="bold">n</mml:mi><mml:none/></mml:mmultiscripts><mml:msub><mml:mi mathvariant="bold">B</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>:</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:msub><mml:mi/><mml:mi mathvariant="bold">n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). Predictor unique contributions. To assess the importance of each group of kernels for predicting a neuron’s activity we first fit the activity of each neuron using the full reduced-rank regression procedure, then fit a reduced model (with fourfold cross-validation), holding out the kernels of the predictor to be tested (cues, licks, or rewards). If the difference in variance explained between the full and held-out model was <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the total variance explained by the full model was <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the neuron was deemed selective for those predictors (<xref ref-type="bibr" rid="bib67">Steinmetz et al., 2019</xref>). We validated this cutoff by comparing our results when adjusting the cutoff from 0.5–0.5% (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). The pattern of results was similar regardless of the cutoff. When we refit the reduced ranks to neural activity with the onset time of each trial shuffled, the 2% cutoff was the smallest that allowed no false positive identification of any neurons uniquely coding any variable (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3B</xref>). Using a higher cutoff led to mislabeling neurons with clear cue responses as non-coding (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3E</xref>).</p></sec></sec><sec id="s4-11"><title>Cue coding models</title><p>To assess cue coding schemes, we fit a new set of models focusing on a more restricted time window (−1–2.5 s from cue onset) using only cues and licks as predictors. Cue and lick neurons were identified as before, and subsequent cue characterization was performed on neurons with only a unique contribution of cues. To identify value coding among cue neurons, we fit a new kernel models with a single cue kernel that scaled according to the cue as well as six block constants (as above) with full rank. We inputted cue values as 1, 0.5, and 0 for each CS+, CS50, and CS−, respectively, ranked according to their reward probability. We fit 152 additional models with alternative configurations of cue value: all permutations of 1, 1, 0.5, 0.5, 0, 0 across the six cues, as well as all permutations of high responses (1) for 6 (we call this the ‘untuned’ model), 5, 4, 3, 2, or 1 cues, with other cues set to 0. Among the 153 total models, some were more similar to the ranked value model, which we quantified by correlating the six cue values of each alternative model with the ranked model. We termed all models with a correlation greater than 0.5 as ‘value-like.’ For each neuron, we found the model that best explained its activity. The models, their correlation with value, and the proportion of cue neurons best fit by each model are illustrated in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. To verify the robustness of value coding in the neurons best fit by the ranked value model, we fit each of those neurons with 1000 iterations of the cue value model with shuffled cue order to create a null distribution. The fits of the original value model exceeded the 98<sup>th</sup> percentile of the null for all value neurons.</p></sec><sec id="s4-12"><title>History model</title><p>For a more nuanced estimation of the value of the cue on each trial, we generated per trial value predictions using the lick linear model (described in section ‘Behavioral analysis’) with cue type, 10 previous outcomes, and 10 previous cue outcomes as predictors. These values were used to scale the height of cue kernel on each trial and were, on average, 0.05, 0.35, and 0.5 for CS−, CS50, and CS+, respectively, but varied on each trial according to the specific reward history. We compared the performance of this model to all the other cue coding models for value and value-like neurons to find neurons better explained by the history model. For neurons better fit by the history model, we also fit 1000 additional models with shuffled trial values within each cue (disrupting the trial history effect). Neurons exceeding the 95th percentile of this null distribution were deemed history neurons.</p></sec><sec id="s4-13"><title>Coding stability</title><p>In the calcium imaging experiments, we used a number of approaches to assess the stability of neural codings. First, for neurons tracked across the first three sessions of odor set A, we took the trial-averaged activity of a given neuron for CS+, CS50, and CS− trials in one session and correlated it with the same neuron’s trial-averaged activity from the subsequent session. We quantified the strength of the correlation as its percentile among correlations between that neuron in the first sessions and every other neuron on the subsequent session and compared this value to shuffle control (neuron identity shuffled) (<xref ref-type="fig" rid="fig7">Figure 7E</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>). To assess coding stability of these neurons, we used the kernels resulting from fitting the full model on day 3 and the predictors from each session third to predict neural activity at those time points. We assessed the accuracy of the prediction by correlating it with the true activity versus the correlation with the trial-shuffled data and present this data in original form (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1B</xref>) and normalize to model performance when predicting the (cross-validated) data from the entire training session (<xref ref-type="fig" rid="fig7">Figure 7G</xref>). This shuffle maintains the temporal dynamics of each trial but shuffles the link between predictors on a given trial and the neural activity for that trial; correlation of predictors (like licks) across trials preserves some prediction accuracy even with this shuffle. We also trained models with data from the third day of odor set A training (A3, day 5) and tested on training days A3 and B3 (days 5 and 6). To determine how unique variable contributions (cues, licks, rewards) evolved over times, we fit our kernel regression model independently to each session third (early, middle, late) of sessions 1–3 for neurons tracked across the three odor set A sessions (<xref ref-type="fig" rid="fig7">Figure 7H</xref>). To assess value coding across the third sessions of odor set A and B (A3 and B3, days 5 and 6) we fit the 153 cues coding models described in Cue coding models to the neurons imaged on separate days (<xref ref-type="fig" rid="fig8">Figure 8E</xref>), concatenating the data from each odor set and adding a constant for each day to account for day differences found the model with the best fit for each neuron. We also looked at the stability of value-like signals across the three days of odor set A training by identifying CS+− preferring cue cells using half of the trials in session A3 and plotting the activity of those neurons for the remaining A3 trials and all trials from A1 and A2 (<xref ref-type="fig" rid="fig8">Figure 8H</xref>).</p></sec><sec id="s4-14"><title>Principal component analysis</title><p>To visualize the dominant firing pattern of PL neurons (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), and of value and value-like cells (<xref ref-type="fig" rid="fig4">Figure 4</xref>), irrespective of direction (excitation or inhibition), we performed principal component analysis (‘PCA’ in MATLAB) on the concatenated PSTHs across all six cues for the neurons of interest, with each neuron’s activity normalized by peak modulation so that each neuron’s concatenated PSTH peaked at –1 or 1. We then plotted the score of the top components.</p></sec><sec id="s4-15"><title>Decoding cue identity</title><p>We adapted the approach in <xref ref-type="bibr" rid="bib48">Ottenheimer et al., 2018</xref> to use single units as well as random selections of pseudo ensembles to decode cue identity (out of six options) (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). First, we binned the activity of each neuron into 0.25 s bins spanning –0.5–2.5 s from the onset of every cue in the sessions. These bins were labeled as 1–6 corresponding to the 6 different cues. For all decoding, we randomly selected 50 trials of each cue to use (most sessions had 51 of each cue, but a few had only 50). For single unit decoding of cue identity, we used fivefold cross-validation to train a linear discriminant model (’fitcdiscr’ in MATLAB) on 80% of the data and tested correct classification of the six cues on the remaining 20%. We plotted the mean ± SEM performance over time for value, value-like, and untuned neurons, and compared their performance using an ANOVA with fixed effects of neuron type and time point and random effect of the session, making pairwise comparisons with Bonferroni correction. For population decoding, we pooled the activity between 1 and 2.5 s from cue onset (a period with stable decoding in the single unit analysis) and randomly selected groups of 1, 5, 10, 25, 50, 75, 100, or 200 value, value-like, or untuned neurons from all regions. We used the same linear discriminant model (with regularization <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in ’fitcdiscr’) and fivefold cross-validations. We performed 1000 selections of neurons at each level, plotted the mean and standard deviations of classification accuracy across these iterations, and made pairwise comparisons across groups by calculating the number of iterations where the second group was greater or equal to the first; we repeated this one-way test for both directions of all pairs of groups and used a Bonferroni corrected <inline-formula><mml:math id="inf132"><mml:mi>α</mml:mi></mml:math></inline-formula>. Pattern of results was unchanged when population activity was standardized with PCA. Pattern of results was also unchanged when we trained on one odor set and tested on the other.</p></sec><sec id="s4-16"><title>Decoding cue value</title><p>Data were prepared for population decoding of cue identity, but with cues labeled as 0, 0.5, or 1 for CS−, CS50, and CS+ trials, respectively. Instead of a linear discriminant model, we used a linear model (elastic net, <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>) to regress cue value onto the activity of pseudo ensembles of neurons. To balance our model, we used 50 of each cue type for training and tested on 50 held out trials for a cue never seen by the model; this setup thoroughly tested the idea that value is encoded on a linear scale and thus should be able to generalize to a new cue in same value domain. For example to predict the value of 50 CS+ in odor set B trials, we used for training 50 trials of CS+ A, 0 trials of CS+ B, 25 trials of CS50A, 25 trials of CS50B, 25 trials of CS-A, and 25 trials of CS-B, maximizing coverage of the data while maintaining a balanced design. These models produced predicted values for each cue. We plot the predicted value for CS+ and CS− cues on the left in <xref ref-type="fig" rid="fig3">Figure 3H</xref>. To convert these predictions to an accuracy score, we labeled values from –0.25–0.25 as CS−, 0.25–0.75 as CS50, and 0.75–1.25 as CS+ (values outside this range were automatically labeled incorrect). We performed this analysis on random groups of 1, 5, 10, 25, 50, 75, 100, or 200 value, value-like, or untuned neurons (<xref ref-type="fig" rid="fig3">Figure 3H</xref>), as well as random groups of five neurons (with replacement) from each region and 25 neurons (with replacement) from each region group (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). We compared region decoding to decoding using a baseline window of –0.5–0 s from odor onset using neurons from each region. We performed 1000 selections of neurons at each level, plotted the mean and standard deviation of classification accuracy across these iterations, and made pairwise comparisons for cue identity.</p></sec><sec id="s4-17"><title>Cue coding dimension</title><p>To project population activity onto the coding dimensions separating CS− activity from CS+ and CS50 activity, respectively, we adapted an approach from <xref ref-type="bibr" rid="bib39">Li et al., 2016</xref>. We first max normalized the odor set A PSTH activity of each neuron to prevent neurons with particularly large z-score values from dominating the dimension. We then defined coding dimensions from the even trials of odor set A. To find the ‘consensus’ cue-difference coding dimension across the group defined by each neuron’s maximal difference across cue responses, we found the 0.5 s bin in the range 0–2.5 s from cue onset with the peak difference between CS− and CS+ activity or CS- and CS50 activity, for each neuron. This comprised a difference vector of length <inline-formula><mml:math id="inf134"><mml:mi>N</mml:mi></mml:math></inline-formula> defining the maximum cue difference coding across the neuron group. This difference vector was then multiplied by the original z-score values of each neuron’s peak difference bin to find the values of peak CS+ vs CS− coding; these values were used to transform the data onto a 0 (CS−) to 1 (CS+) relative cue coding scale. To transform population activity onto the CS− to CS+ dimension at each moment in CS+, CS50, and CS− trials, we multiplied the activity of all neurons in each 0.1 s bin of remaining odd odor set A trials (z-score) by the difference vector and used the same 0–1 scale conversion (‘same odor set’). We also multiplied the activity of neurons for cues in odor set B by the difference vector (‘other odor set’). We repeated the same process for CS− and CS50 activity. To find the angle between the CS+ and CS50 projections, we bootstrapped the vectors that connected baseline activity to peak activity of CS50 and CS+ along the CS-/CS+ and CS−/CS50 axes and found the angle between these vectors. To find population activity along the CS+/CS− dimension at each moment for CS50 trials of various values, we multiplied the activity (z-score) of all neurons in each 0.1 s bin of the CS50 PSTHs (grouped by value estimated from the lick linear model) by the difference vector and used the same conversion to 0–1 scale. To estimate the distribution of values along the CS+/CS− dimension for each CS50 value condition, we bootstrapped (5000 iterations, with replacement) the population projection and took the mean 1–2.5 s from odor onset. We calculated the slope of the activity on CS50 trials by linearly regressing the estimated position of the population on the CS+/CS− dimension against the value from the lick linear model used to group those trials (5000 iterations, with replacement). To compare slopes across cell groups, we generated a p-value by calculating the number of iterations where the second group was greater or equal to the first; we repeated in this one-way test for both directions of all pairs of groups and used a Bonferroni corrected <inline-formula><mml:math id="inf135"><mml:mi>α</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-18"><title>Statistics</title><p>All statistical tests were performed in MATLAB (MathWorks). To compare the fraction of neurons of a specific coding type across regions, we fit a generalized linear mixed-effects model (‘fitglme’ in MATLAB) with logit link function and with fixed effects of intercept and region and a random effect of the session and then found the estimated mean and 95% confidence interval for each region. For pairwise comparisons across regions, we used a specific contrast for each pair of regions (’coefTest’ in MATLAB) to find the p-value that these regions differed from each other and used a Bonferroni-corrected <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> for significance. To compare the number of anticipatory licks on different trial types, we found the mean number of anticipatory licks for each cue in each session, and then performed a two-way ANOVA with effects of cue and subject and session as our n (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). To compare the variance explained during each third of the first session, we found the mean value across neurons from each mouse and then performed a one-way ANOVA on those means with mouse as our n (<xref ref-type="fig" rid="fig6">Figure 6H</xref>). To compare day 3 model performance on true and shuffled data across each time point (<xref ref-type="fig" rid="fig7">Figure 7F</xref>), we found the mean value across neurons from each mouse at each time point and then performed a two-way ANOVA with main effects of shuffle and time point, with mouse as our n. We then calculated pairwise statistics using ‘multcompare’ in MATLAB with Bonferroni correction. To compare cue, lick, and reward unique variance at each time point for each cell category (determined on day 3, <xref ref-type="fig" rid="fig7">Figure 7G</xref>), we found the mean from the cells in that category in each mouse at each time point and performed a two-way ANOVA with main effects of variable and day, with mouse as our n. We then calculated pairwise statistics using ‘multcompare’ in MATLAB with Bonferroni correction.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Validation, Investigation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Validation, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures were performed in strict accordance with protocols 4450-01 and 4461-01 approved by the Animal Care and Use Committee at the University of Washington.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Statistics related to <xref ref-type="fig" rid="fig2">Figure 2G</xref> and <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>.</title><p>Bonferroni-corrected p-values from region contrast in generalized linear mixed-effects model.</p></caption><media xlink:href="elife-84604-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Statistics related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Top: Bonferroni-corrected p-values from pairwise comparisons between the decoding accuracy of each group of neurons at each time point with their performance at baseline and with the other neuron groups at that time point. Middle, Bottom: Bonferroni-corrected p-values for pairwise comparisons of bootstrapped distributions (1000 samples) of decoding performance using increasing numbers of neurons in each group.</p></caption><media xlink:href="elife-84604-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Statistics related to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>Top, Middle: Bonferroni-corrected p-values from region contrasts in generalized linear mixed-effects model. Bottom: Bonferroni-corrected p-values for pairwise comparisons of bootstrapped distributions (1000 samples) of decoding performance using value cells from each region.</p></caption><media xlink:href="elife-84604-supp3-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Statistics related to <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title><p>Top: Bonferroni-corrected p-values for pairwise comparisons of bootstrapped distributions (5000 samples) of the slope of population activity of each group of neurons across CS50 trials of increasing value. Bottom: Bonferroni-corrected p-values from region contrasts in generalized linear mixed-effects model.</p></caption><media xlink:href="elife-84604-supp4-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Statistics related to <xref ref-type="fig" rid="fig2">Figure 2G</xref> and <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>.</title><p>Bonferroni-corrected p-values from region contrast in generalized linear mixed-effects model.</p></caption><media xlink:href="elife-84604-supp5-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-84604-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and code for this manuscript are publicly available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6686927">https://doi.org/10.5281/zenodo.6686927</ext-link> (<xref ref-type="bibr" rid="bib50">Ottenheimer et al., 2022</xref>).</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ottenheimer</surname><given-names>D</given-names></name><name><surname>Hjort</surname><given-names>M</given-names></name><name><surname>Bowen</surname><given-names>A</given-names></name><name><surname>Stuber</surname><given-names>G</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Electrophysiology and two-photon imaging data from an olfactory Pavlovian conditioning task</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21365598</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Ottenheimer</surname><given-names>DJ</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>SteinmetzLab/ottenheimer-et-al-2022: Revised manuscript</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7718542</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Thank you to Vijay Namboodiri and Charles Zhou for assistance with the imaging. Thank you to Noam Roth for the spike sorting quality control metrics and feedback on the decoding analysis. This work was supported by National Institutes of Health grants F32DA053714 (DJO), F31DA053706 (MMH), T32DK007247 (AJB), R37DA032750 (GDS), and P30DA048736 (GDS), a UW Center for the Neurobiology of Addiction, Pain, and Emotion 2-photon pilot project grant (DJO), a Klingenstein-Simons Fellowship in Neuroscience (NAS), and the Pew Biomedical Scholars Program (NAS).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>WE</given-names></name><name><surname>Chen</surname><given-names>MZ</given-names></name><name><surname>Pichamoorthy</surname><given-names>N</given-names></name><name><surname>Tien</surname><given-names>RH</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Thirst regulates motivated behavior through modulation of Brainwide neural population Dynamics</article-title><source>Science</source><volume>364</volume><elocation-id>253</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav3932</pub-id><pub-id pub-id-type="pmid">30948440</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Banga,</surname><given-names>K</given-names></name><name><surname>Benson,</surname><given-names>J</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Bruijns</surname><given-names>SA</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Chapuis</surname><given-names>GA</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Davatolhagh</surname><given-names>MF</given-names></name><name><surname>Lee</surname><given-names>HD</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><collab>International-Brain-Laboratory</collab></person-group><year iso-8601-date="2022">2022</year><article-title>Reproducibility of In-Vivo Electrophysiological Measurements in Mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.09.491042</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Grossman</surname><given-names>CD</given-names></name><name><surname>Lubin</surname><given-names>EE</given-names></name><name><surname>Rajagopalan</surname><given-names>AE</given-names></name><name><surname>Cressy</surname><given-names>JI</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stable representations of decision variables for flexible behavior</article-title><source>Neuron</source><volume>103</volume><fpage>922</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.001</pub-id><pub-id pub-id-type="pmid">31280924</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berridge</surname><given-names>KC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Motivation concepts in behavioral Neuroscience</article-title><source>Physiology &amp; Behavior</source><volume>81</volume><fpage>179</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2004.02.004</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhattarai</surname><given-names>JP</given-names></name><name><surname>Etyemez</surname><given-names>S</given-names></name><name><surname>Jaaro-Peled</surname><given-names>H</given-names></name><name><surname>Janke</surname><given-names>E</given-names></name><name><surname>Leon Tolosa</surname><given-names>UD</given-names></name><name><surname>Kamiya</surname><given-names>A</given-names></name><name><surname>Gottfried</surname><given-names>JA</given-names></name><name><surname>Sawa</surname><given-names>A</given-names></name><name><surname>Ma</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Olfactory modulation of the medial Prefrontal cortex circuitry: implications for social cognition</article-title><source>Seminars in Cell &amp; Developmental Biology</source><volume>129</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.semcdb.2021.03.022</pub-id><pub-id pub-id-type="pmid">33975755</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Mansouri</surname><given-names>FA</given-names></name><name><surname>Hoda</surname><given-names>H</given-names></name><name><surname>Mahboubi</surname><given-names>M</given-names></name><name><surname>Browning</surname><given-names>PGF</given-names></name><name><surname>Kwok</surname><given-names>SC</given-names></name><name><surname>Phillips</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dissociable components of rule-guided behavior depend on distinct medial and Prefrontal regions</article-title><source>Science</source><volume>325</volume><fpage>52</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1126/science.1172377</pub-id><pub-id pub-id-type="pmid">19574382</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Blot</surname><given-names>A</given-names></name><name><surname>Rousseau</surname><given-names>C</given-names></name><name><surname>Fabbri</surname><given-names>M</given-names></name><name><surname>West</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Lasagna</data-title><version designator="v.1.0">v.1.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/SainsburyWellcomeCentre/lasagna">https://github.com/SainsburyWellcomeCentre/lasagna</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>TW</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A map of anticipatory activity in mouse motor cortex</article-title><source>Neuron</source><volume>94</volume><fpage>866</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.005</pub-id><pub-id pub-id-type="pmid">28521137</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>MW</given-names></name><name><surname>Li</surname><given-names>WL</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Balancing the robustness and efficiency of odor representations during learning</article-title><source>Neuron</source><volume>92</volume><fpage>174</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.004</pub-id><pub-id pub-id-type="pmid">27667005</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chudasama</surname><given-names>Y</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dissociable contributions of the Orbitofrontal and Infralimbic cortex to Pavlovian Autoshaping and discrimination reversal learning: further evidence for the functional heterogeneity of the rodent frontal cortex</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>8771</fpage><lpage>8780</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-25-08771.2003</pub-id><pub-id pub-id-type="pmid">14507977</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalley</surname><given-names>JW</given-names></name><name><surname>Cardinal</surname><given-names>RN</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Prefrontal executive and cognitive functions in rodents: neural and neurochemical substrates</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>28</volume><fpage>771</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2004.09.006</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalton</surname><given-names>GL</given-names></name><name><surname>Wang</surname><given-names>NY</given-names></name><name><surname>Phillips</surname><given-names>AG</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multifaceted contributions by different regions of the Orbitofrontal and medial Prefrontal cortex to probabilistic reversal learning</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1996</fpage><lpage>2006</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3366-15.2016</pub-id><pub-id pub-id-type="pmid">26865622</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devore</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Linster</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Odor preferences shape discrimination learning in rats</article-title><source>Behavioral Neuroscience</source><volume>127</volume><fpage>498</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1037/a0033329</pub-id><pub-id pub-id-type="pmid">23895061</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doucette</surname><given-names>W</given-names></name><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Whitesell</surname><given-names>J</given-names></name><name><surname>Carmean</surname><given-names>V</given-names></name><name><surname>Lucero</surname><given-names>MT</given-names></name><name><surname>Restrepo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Associative cortex features in the first olfactory brain relay station</article-title><source>Neuron</source><volume>69</volume><fpage>1176</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.024</pub-id><pub-id pub-id-type="pmid">21435561</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekstrand</surname><given-names>JJ</given-names></name><name><surname>Domroese</surname><given-names>ME</given-names></name><name><surname>Johnson</surname><given-names>DM</given-names></name><name><surname>Feig</surname><given-names>SL</given-names></name><name><surname>Knodel</surname><given-names>SM</given-names></name><name><surname>Behan</surname><given-names>M</given-names></name><name><surname>Haberly</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A new subdivision of anterior Piriform cortex and associated deep nucleus with novel features of interest for Olfaction and epilepsy</article-title><source>The Journal of Comparative Neurology</source><volume>434</volume><fpage>289</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1002/cne.1178</pub-id><pub-id pub-id-type="pmid">11331530</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ertürk</surname><given-names>A</given-names></name><name><surname>Becker</surname><given-names>K</given-names></name><name><surname>Jährling</surname><given-names>N</given-names></name><name><surname>Mauch</surname><given-names>CP</given-names></name><name><surname>Hojer</surname><given-names>CD</given-names></name><name><surname>Egen</surname><given-names>JG</given-names></name><name><surname>Hellal</surname><given-names>F</given-names></name><name><surname>Bradke</surname><given-names>F</given-names></name><name><surname>Sheng</surname><given-names>M</given-names></name><name><surname>Dodt</surname><given-names>H-U</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Three-dimensional imaging of solvent-cleared organs using 3Disco</article-title><source>Nature Protocols</source><volume>7</volume><fpage>1983</fpage><lpage>1995</lpage><pub-id pub-id-type="doi">10.1038/nprot.2012.119</pub-id><pub-id pub-id-type="pmid">23060243</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Bukwich</surname><given-names>M</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine neurons share common response function for reward prediction error</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>479</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1038/nn.4239</pub-id><pub-id pub-id-type="pmid">26854803</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Faulkner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Iblapps</data-title><version designator="bde9a6e">bde9a6e</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/int-brain-lab/iblapps/tree/master/atlaselectrophysiology">https://github.com/int-brain-lab/iblapps/tree/master/atlaselectrophysiology</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Discrete coding of reward probability and uncertainty by dopamine neurons</article-title><source>Science</source><volume>299</volume><fpage>1898</fpage><lpage>1902</lpage><pub-id pub-id-type="doi">10.1126/science.1077349</pub-id><pub-id pub-id-type="pmid">12649484</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>RI</given-names></name><name><surname>Doncheck</surname><given-names>EM</given-names></name><name><surname>Vollmer</surname><given-names>KM</given-names></name><name><surname>Winston</surname><given-names>KT</given-names></name><name><surname>Romanova</surname><given-names>EV</given-names></name><name><surname>Siegler</surname><given-names>PN</given-names></name><name><surname>Holman</surname><given-names>H</given-names></name><name><surname>Bowen</surname><given-names>CW</given-names></name><name><surname>Otis</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Specialized coding patterns among Dorsomedial Prefrontal neuronal ensembles predict conditioned reward seeking</article-title><source>eLife</source><volume>10</volume><elocation-id>e65764</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65764</pub-id><pub-id pub-id-type="pmid">34184635</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Hires</surname><given-names>SA</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Bonardi</surname><given-names>C</given-names></name><name><surname>Morandell</surname><given-names>K</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Peron</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>N</given-names></name><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Procedures for behavioral experiments in head-fixed mice</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e88678</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0088678</pub-id><pub-id pub-id-type="pmid">24520413</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><volume>81</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id><pub-id pub-id-type="pmid">24361077</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The case against economic values in the Orbitofrontal cortex (or anywhere else in the brain)</article-title><source>Behavioral Neuroscience</source><volume>135</volume><fpage>192</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1037/bne0000448</pub-id><pub-id pub-id-type="pmid">34060875</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbronner</surname><given-names>SR</given-names></name><name><surname>Rodriguez-Romaguera</surname><given-names>J</given-names></name><name><surname>Quirk</surname><given-names>GJ</given-names></name><name><surname>Groenewegen</surname><given-names>HJ</given-names></name><name><surname>Haber</surname><given-names>SN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Circuit-based corticostriatal homologies between rat and primate</article-title><source>Biological Psychiatry</source><volume>80</volume><fpage>509</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2016.05.012</pub-id><pub-id pub-id-type="pmid">27450032</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horst</surname><given-names>NK</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reward-related activity in the medial Prefrontal cortex is driven by consumption</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>56</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00056</pub-id><pub-id pub-id-type="pmid">23596384</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Malalasekera</surname><given-names>WMN</given-names></name><name><surname>de Berker</surname><given-names>AO</given-names></name><name><surname>Miranda</surname><given-names>B</given-names></name><name><surname>Farmer</surname><given-names>SF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Triple dissociation of attention and decision computations across Prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1471</fpage><lpage>1481</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0239-5</pub-id><pub-id pub-id-type="pmid">30258238</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyman</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Balaguer-Ballester</surname><given-names>E</given-names></name><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Contextual Encoding by ensembles of medial Prefrontal cortex neurons</article-title><source>PNAS</source><volume>109</volume><fpage>5086</fpage><lpage>5091</lpage><pub-id pub-id-type="doi">10.1073/pnas.1114415109</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Igarashi</surname><given-names>KM</given-names></name><name><surname>Ieki</surname><given-names>N</given-names></name><name><surname>An</surname><given-names>M</given-names></name><name><surname>Yamaguchi</surname><given-names>Y</given-names></name><name><surname>Nagayama</surname><given-names>S</given-names></name><name><surname>Kobayakawa</surname><given-names>K</given-names></name><name><surname>Kobayakawa</surname><given-names>R</given-names></name><name><surname>Tanifuji</surname><given-names>M</given-names></name><name><surname>Sakano</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>WR</given-names></name><name><surname>Mori</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Parallel mitral and Tufted cell pathways route distinct odor information to different targets in the olfactory cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>7970</fpage><lpage>7985</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0154-12.2012</pub-id><pub-id pub-id-type="pmid">22674272</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackman</surname><given-names>SL</given-names></name><name><surname>Chen</surname><given-names>CH</given-names></name><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Neufeld</surname><given-names>SQ</given-names></name><name><surname>Drew</surname><given-names>IR</given-names></name><name><surname>Agba</surname><given-names>CK</given-names></name><name><surname>Flaquer</surname><given-names>I</given-names></name><name><surname>Stefano</surname><given-names>AN</given-names></name><name><surname>Kennedy</surname><given-names>TJ</given-names></name><name><surname>Belinsky</surname><given-names>JE</given-names></name><name><surname>Roberston</surname><given-names>K</given-names></name><name><surname>Beron</surname><given-names>CC</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Regehr</surname><given-names>WG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Silk Fibroin films facilitate single-step targeted expression of Optogenetic proteins</article-title><source>Cell Reports</source><volume>22</volume><fpage>3351</fpage><lpage>3361</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.02.081</pub-id><pub-id pub-id-type="pmid">29562189</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated Silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennerley</surname><given-names>SW</given-names></name><name><surname>Dahmubed</surname><given-names>AF</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neurons in the frontal lobe Encode the value of multiple decision variables</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1162</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21100</pub-id><pub-id pub-id-type="pmid">18752411</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kesner</surname><given-names>RP</given-names></name><name><surname>Churchwell</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>An analysis of rat Prefrontal cortex in mediating executive function</article-title><source>Neurobiology of Learning and Memory</source><volume>96</volume><fpage>417</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2011.07.002</pub-id><pub-id pub-id-type="pmid">21855643</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein-Flügge</surname><given-names>MC</given-names></name><name><surname>Bongioanni</surname><given-names>A</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Medial and orbital frontal cortex in decision-making and flexible behavior</article-title><source>Neuron</source><volume>110</volume><fpage>2743</fpage><lpage>2770</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.05.022</pub-id><pub-id pub-id-type="pmid">35705077</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koldaeva</surname><given-names>A</given-names></name><name><surname>Schaefer</surname><given-names>AT</given-names></name><name><surname>Fukunaga</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Rapid task-dependent tuning of the mouse olfactory bulb</article-title><source>eLife</source><volume>8</volume><elocation-id>e43558</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43558</pub-id><pub-id pub-id-type="pmid">30724732</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kondo</surname><given-names>M</given-names></name><name><surname>Matsuzaki</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuronal representations of reward-predicting cues and outcome history with movement in the frontal cortex</article-title><source>Cell Reports</source><volume>34</volume><elocation-id>108704</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.108704</pub-id><pub-id pub-id-type="pmid">33535051</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuwabara</surname><given-names>M</given-names></name><name><surname>Kang</surname><given-names>N</given-names></name><name><surname>Holy</surname><given-names>TE</given-names></name><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural mechanisms of economic choices in mice</article-title><source>eLife</source><volume>9</volume><elocation-id>e49669</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49669</pub-id><pub-id pub-id-type="pmid">32096761</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laubach</surname><given-names>M</given-names></name><name><surname>Amarante</surname><given-names>LM</given-names></name><name><surname>Swanson</surname><given-names>K</given-names></name><name><surname>White</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What, if anything, is rodent Prefrontal cortex</article-title><source>ENeuro</source><volume>5</volume><elocation-id>ENEURO.0315-18.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0315-18.2018</pub-id><pub-id pub-id-type="pmid">30406193</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>A</given-names></name><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Restrepo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spike-field coherence in a population of olfactory bulb neurons Differentiates between odors irrespective of associated outcome</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>5808</fpage><lpage>5822</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4003-14.2015</pub-id><pub-id pub-id-type="pmid">25855190</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Robust neuronal Dynamics in Premotor cortex during motor planning</article-title><source>Nature</source><volume>532</volume><fpage>459</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1038/nature17643</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>LD</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Hou</surname><given-names>H</given-names></name><name><surname>West</surname><given-names>SJ</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Laboratory</surname><given-names>TIB</given-names></name><name><surname>Economo</surname><given-names>MN</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Accurate localization of linear probe electrode Arrays across multiple brains</article-title><source>ENeuro</source><volume>8</volume><elocation-id>ENEURO.0241-21.2021</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0241-21.2021</pub-id><pub-id pub-id-type="pmid">34697075</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malagon-Vina</surname><given-names>H</given-names></name><name><surname>Ciocchi</surname><given-names>S</given-names></name><name><surname>Passecker</surname><given-names>J</given-names></name><name><surname>Dorffner</surname><given-names>G</given-names></name><name><surname>Klausberger</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fluid network Dynamics in the Prefrontal cortex during multiple strategy switching</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>309</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02764-x</pub-id><pub-id pub-id-type="pmid">29358717</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of Prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mori</surname><given-names>K</given-names></name><name><surname>Sakano</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Olfactory circuitry and behavioral decisions</article-title><source>Annual Review of Physiology</source><volume>83</volume><fpage>231</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1146/annurev-physiol-031820-092824</pub-id><pub-id pub-id-type="pmid">33228453</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural Dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakahara</surname><given-names>H</given-names></name><name><surname>Itoh</surname><given-names>H</given-names></name><name><surname>Kawagoe</surname><given-names>R</given-names></name><name><surname>Takikawa</surname><given-names>Y</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dopamine neurons can represent context-dependent prediction error</article-title><source>Neuron</source><volume>41</volume><fpage>269</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00869-9</pub-id><pub-id pub-id-type="pmid">14741107</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Namboodiri</surname><given-names>VMK</given-names></name><name><surname>Otis</surname><given-names>JM</given-names></name><name><surname>van Heeswijk</surname><given-names>K</given-names></name><name><surname>Voets</surname><given-names>ES</given-names></name><name><surname>Alghorazi</surname><given-names>RA</given-names></name><name><surname>Rodriguez-Romaguera</surname><given-names>J</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-cell activity tracking reveals that Orbitofrontal neurons acquire and maintain a long-term memory to guide behavioral adaptation</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1110</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0408-1</pub-id><pub-id pub-id-type="pmid">31160741</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otis</surname><given-names>JM</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name><name><surname>Matan</surname><given-names>AM</given-names></name><name><surname>Voets</surname><given-names>ES</given-names></name><name><surname>Mohorn</surname><given-names>EP</given-names></name><name><surname>Kosyk</surname><given-names>O</given-names></name><name><surname>McHenry</surname><given-names>JA</given-names></name><name><surname>Robinson</surname><given-names>JE</given-names></name><name><surname>Resendez</surname><given-names>SL</given-names></name><name><surname>Rossi</surname><given-names>MA</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prefrontal cortex output circuits guide reward seeking through divergent cue Encoding</article-title><source>Nature</source><volume>543</volume><fpage>103</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nature21376</pub-id><pub-id pub-id-type="pmid">28225752</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ottenheimer</surname><given-names>D</given-names></name><name><surname>Richard</surname><given-names>JM</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ventral Pallidum Encodes relative reward value earlier and more Robustly than nucleus accumbens</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4350</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06849-z</pub-id><pub-id pub-id-type="pmid">30341305</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ottenheimer</surname><given-names>DJ</given-names></name><name><surname>Bari</surname><given-names>BA</given-names></name><name><surname>Sutlief</surname><given-names>E</given-names></name><name><surname>Fraser</surname><given-names>KM</given-names></name><name><surname>Kim</surname><given-names>TH</given-names></name><name><surname>Richard</surname><given-names>JM</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A quantitative reward prediction error signal in the ventral Pallidum</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1267</fpage><lpage>1276</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0688-5</pub-id><pub-id pub-id-type="pmid">32778791</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ottenheimer</surname><given-names>DJ</given-names></name><name><surname>Hjort</surname><given-names>MM</given-names></name><name><surname>Bowen</surname><given-names>AJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data and code for Ottenheimer et al 2022</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6686927">https://doi.org/10.5281/zenodo.6686927</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Bondy</surname><given-names>AG</given-names></name><name><surname>Winter</surname><given-names>O</given-names></name><name><surname>Guzman</surname><given-names>J</given-names></name><collab>kushbanga</collab><name><surname>Janke</surname><given-names>A</given-names></name><name><surname>Czuba</surname><given-names>T</given-names></name><name><surname>Bhagat</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Kilosort</data-title><version designator="d55179f">d55179f</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Brann</surname><given-names>D</given-names></name><name><surname>Chicharro</surname><given-names>D</given-names></name><name><surname>Drummey</surname><given-names>K</given-names></name><name><surname>Franks</surname><given-names>KM</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structure and flexibility in cortical representations of odour space</article-title><source>Nature</source><volume>583</volume><fpage>253</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2451-1</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Beyond the primary olfactory cortex: olfactory-related areas in the neocortex, thalamus and hypothalamus</article-title><source>Chemical Senses</source><volume>10</volume><fpage>239</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1093/chemse/10.2.239</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Price</surname><given-names>JL</given-names></name><name><surname>Carmichael</surname><given-names>ST</given-names></name><name><surname>Carnes</surname><given-names>K</given-names></name><name><surname>Clugnet</surname><given-names>M</given-names></name><name><surname>Kuroda</surname><given-names>M</given-names></name><name><surname>Ray</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Olfactory Input to the Prefrontal Cortex. Olfaction: A Model System for Computational Neuroscience</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal activity related to reward value and motivation in Primate frontal cortex</article-title><source>Science</source><volume>304</volume><fpage>307</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1126/science.1093223</pub-id><pub-id pub-id-type="pmid">15073380</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Harris</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2021">2021</year><data-title>Phy</data-title><version designator="v. 2.0">v. 2.0</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/kwikteam/phy">https://github.com/kwikteam/phy</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Kennerley</surname><given-names>SW</given-names></name><name><surname>Baxter</surname><given-names>MG</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Frontal cortex Subregions play distinct roles in choices between actions and stimuli</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>13775</fpage><lpage>13785</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3541-08.2008</pub-id><pub-id pub-id-type="pmid">19091968</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Quilodran</surname><given-names>R</given-names></name><name><surname>Rothe</surname><given-names>M</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>Joseph</surname><given-names>J-P</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Expectations, gains, and losses in the anterior cingulate cortex</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>7</volume><fpage>327</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.3758/CABN.7.4.327</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saraiva</surname><given-names>LR</given-names></name><name><surname>Kondoh</surname><given-names>K</given-names></name><name><surname>Ye</surname><given-names>X</given-names></name><name><surname>Yoon</surname><given-names>K</given-names></name><name><surname>Hernandez</surname><given-names>M</given-names></name><name><surname>Buck</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Combinatorial effects of Odorants on mouse behavior</article-title><source>PNAS</source><volume>113</volume><fpage>E3300</fpage><lpage>E3306</lpage><pub-id pub-id-type="doi">10.1073/pnas.1605973113</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Setlow</surname><given-names>B</given-names></name><name><surname>Saddoris</surname><given-names>MP</given-names></name><name><surname>Gallagher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Encoding predicted outcome and acquired value in Orbitofrontal cortex during cue sampling depends upon input from Basolateral amygdala</article-title><source>Neuron</source><volume>39</volume><fpage>855</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00474-4</pub-id><pub-id pub-id-type="pmid">12948451</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoonover</surname><given-names>CE</given-names></name><name><surname>Ohashi</surname><given-names>SN</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Fink</surname><given-names>AJP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Representational drift in primary olfactory cortex</article-title><source>Nature</source><volume>594</volume><fpage>541</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03628-7</pub-id><pub-id pub-id-type="pmid">34108681</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamonin</surname><given-names>DP</given-names></name><name><surname>Bron</surname><given-names>EE</given-names></name><name><surname>Lelieveldt</surname><given-names>BPF</given-names></name><name><surname>Smits</surname><given-names>M</given-names></name><name><surname>Klein</surname><given-names>S</given-names></name><name><surname>Staring</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast parallel image registration on Cpu and GPU for diagnostic classification of Alzheimer’s disease</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>50</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00050</pub-id><pub-id pub-id-type="pmid">24474917</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>St Onge</surname><given-names>JR</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Prefrontal cortical contribution to risk-based decision making</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>1816</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp250</pub-id><pub-id pub-id-type="pmid">19892787</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Cooch</surname><given-names>NK</given-names></name><name><surname>McDannald</surname><given-names>MA</given-names></name><name><surname>Liu</surname><given-names>TL</given-names></name><name><surname>Wied</surname><given-names>H</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal neurons infer the value and identity of predicted outcomes</article-title><source>Nature Communications</source><volume>5</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/ncomms4926</pub-id><pub-id pub-id-type="pmid">24894805</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Cooch</surname><given-names>NK</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What the Orbitofrontal cortex does not do</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>620</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1038/nn.3982</pub-id><pub-id pub-id-type="pmid">25919962</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Buetfering</surname><given-names>C</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>CR</given-names></name><name><surname>Peters</surname><given-names>AJ</given-names></name><name><surname>Jacobs</surname><given-names>EAK</given-names></name><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Ollerenshaw</surname><given-names>DR</given-names></name><name><surname>Valley</surname><given-names>MT</given-names></name><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Manavi</surname><given-names>S</given-names></name><name><surname>Miles</surname><given-names>J</given-names></name><name><surname>White</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Griffin</surname><given-names>F</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Nguyen</surname><given-names>TV</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Pendergraft</surname><given-names>J</given-names></name><name><surname>Daigle</surname><given-names>T</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Thompson</surname><given-names>CL</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Olsen</surname><given-names>S</given-names></name><name><surname>Margolis</surname><given-names>DJ</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Hausser</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Aberrant cortical activity in multiple Gcamp6-expressing transgenic mouse lines</article-title><source>ENeuro</source><volume>4</volume><elocation-id>ENEURO.0207-17.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0207-17.2017</pub-id><pub-id pub-id-type="pmid">28932809</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title><source>Nature</source><volume>576</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1787-x</pub-id><pub-id pub-id-type="pmid">31776518</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Aydin</surname><given-names>C</given-names></name><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Beau</surname><given-names>M</given-names></name><name><surname>Bhagat</surname><given-names>J</given-names></name><name><surname>Böhm</surname><given-names>C</given-names></name><name><surname>Broux</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Colonell</surname><given-names>J</given-names></name><name><surname>Gardner</surname><given-names>RJ</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Kostadinov</surname><given-names>D</given-names></name><name><surname>Mora-Lopez</surname><given-names>C</given-names></name><name><surname>O’Callaghan</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Sauerbrei</surname><given-names>B</given-names></name><name><surname>van Daal</surname><given-names>RJJ</given-names></name><name><surname>Vollan</surname><given-names>AZ</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Welkenhuysen</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Hantman</surname><given-names>AW</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neuropixels 2.0: A Miniaturized high-density probe for stable, long-term brain recordings</article-title><source>Science</source><volume>372</volume><elocation-id>eabf4588</elocation-id><pub-id pub-id-type="doi">10.1126/science.abf4588</pub-id><pub-id pub-id-type="pmid">33859006</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stettler</surname><given-names>DD</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representations of odor in the Piriform cortex</article-title><source>Neuron</source><volume>63</volume><fpage>854</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.005</pub-id><pub-id pub-id-type="pmid">19778513</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, Brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Del Grosso</surname><given-names>NA</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Ki</surname><given-names>C</given-names></name><name><surname>Pierre</surname><given-names>A</given-names></name><name><surname>Dincer</surname><given-names>T</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Willem de Gee</surname><given-names>J</given-names></name><name><surname>Amsalem</surname><given-names>O</given-names></name><collab>winnubstj</collab><collab>bjeon90</collab></person-group><year iso-8601-date="2023">2023</year><data-title>Suite2P</data-title><version designator="9808eca">9808eca</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/suite2p">https://github.com/MouseLand/suite2p</ext-link></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sul</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Huh</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Distinct roles of rodent Orbitofrontal and medial Prefrontal cortex in decision making</article-title><source>Neuron</source><volume>66</volume><fpage>449</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.033</pub-id><pub-id pub-id-type="pmid">20471357</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Duuren</surname><given-names>E</given-names></name><name><surname>van der Plasse</surname><given-names>G</given-names></name><name><surname>Lankelma</surname><given-names>J</given-names></name><name><surname>Joosten</surname><given-names>RN</given-names></name><name><surname>Feenstra</surname><given-names>MGP</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Single-cell and population coding of expected reward probability in the Orbitofrontal cortex of the rat</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>8965</fpage><lpage>8976</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0005-09.2009</pub-id><pub-id pub-id-type="pmid">19605634</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verharen</surname><given-names>JPH</given-names></name><name><surname>den Ouden</surname><given-names>HEM</given-names></name><name><surname>Adan</surname><given-names>RAH</given-names></name><name><surname>Vanderschuren</surname><given-names>LJMJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modulation of value-based decision making behavior by Subregions of the rat Prefrontal cortex</article-title><source>Psychopharmacology</source><volume>237</volume><fpage>1267</fpage><lpage>1280</lpage><pub-id pub-id-type="doi">10.1007/s00213-020-05454-7</pub-id><pub-id pub-id-type="pmid">32025777</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>PY</given-names></name><name><surname>Boboila</surname><given-names>C</given-names></name><name><surname>Chin</surname><given-names>M</given-names></name><name><surname>Higashi-Howard</surname><given-names>A</given-names></name><name><surname>Shamash</surname><given-names>P</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Stein</surname><given-names>NP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Transient and persistent representations of odor value in Prefrontal cortex</article-title><source>Neuron</source><volume>108</volume><fpage>209</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.033</pub-id><pub-id pub-id-type="pmid">32827456</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Ding</surname><given-names>SL</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Royall</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Lesnar</surname><given-names>P</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Naeemi</surname><given-names>M</given-names></name><name><surname>Facer</surname><given-names>B</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Dee</surname><given-names>N</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Szafer</surname><given-names>A</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Oh</surname><given-names>SW</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Hawrylycz</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The Allen mouse brain common coordinate framework: a 3D reference Atlas</article-title><source>Cell</source><volume>181</volume><fpage>936</fpage><lpage>953</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.04.007</pub-id><pub-id pub-id-type="pmid">32386544</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkelmeier</surname><given-names>L</given-names></name><name><surname>Filosa</surname><given-names>C</given-names></name><name><surname>Hartig</surname><given-names>R</given-names></name><name><surname>Scheller</surname><given-names>M</given-names></name><name><surname>Sack</surname><given-names>M</given-names></name><name><surname>Reinwald</surname><given-names>JR</given-names></name><name><surname>Becker</surname><given-names>R</given-names></name><name><surname>Wolf</surname><given-names>D</given-names></name><name><surname>Gerchen</surname><given-names>MF</given-names></name><name><surname>Sartorius</surname><given-names>A</given-names></name><name><surname>Meyer-Lindenberg</surname><given-names>A</given-names></name><name><surname>Weber-Fahr</surname><given-names>W</given-names></name><name><surname>Clemm von Hohenberg</surname><given-names>C</given-names></name><name><surname>Russo</surname><given-names>E</given-names></name><name><surname>Kelsch</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Striatal Hub of dynamic and stabilized prediction coding in forebrain networks for olfactory reinforcement learning</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>3305</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-30978-1</pub-id><pub-id pub-id-type="pmid">35676281</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zagha</surname><given-names>E</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Lur</surname><given-names>G</given-names></name><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The importance of accounting for movement when relating neuronal activity to sensory and cognitive processes</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>1375</fpage><lpage>1382</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1919-21.2021</pub-id><pub-id pub-id-type="pmid">35027407</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Is the core function of Orbitofrontal cortex to signal values or make predictions</article-title><source>Current Opinion in Behavioral Sciences</source><volume>41</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.02.011</pub-id><pub-id pub-id-type="pmid">33869678</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84604.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Eisen</surname><given-names>Michael B</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This study makes <bold>valuable</bold> observations about the representation of &quot;value&quot; in the mouse brain, by using a nice task design and recording from an impressive number of brain regions. The combination of state-of-the-art imaging and electrophysiology data offer <bold>solid</bold> support for the authors' conclusions. The paper will be of interest to a broad audience of neuroscientists interested in reward processing in the brain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84604.3.sa1</article-id><title-group><article-title>Consensus Public Review:</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Ottenheimer et al., present an interesting study looking at the neural representation of value in mice performing a pavlovian association task. The task is repeated in the same animals using two odor sets, allowing a distinction between odor identity coding and value coding. The authors use state-of-the-art electrophysiological techniques to record thousands of neurons from 11 frontal cortical regions to conclude that (1) licking is represented more strongly in dorsal frontal regions, (2) odor cues are represented more strongly in ventral frontal regions, (3) cue values are evenly distributed across regions. They separately perform a calcium imaging study to track coding across days and conclude that the representation of task features increments with learning and remains stable thereafter.Overall, these conclusions are interesting and well supported by the data.</p><p>The authors use reduced-rank kernel regression to characterize the 5332 recorded neurons on a cell-by-cell basis in terms of their responses to cues, licks, and reward, with a cell characterized as encoding one of these parameters if it accounts for at least 2% of the observed variance (while at first this seemed overly lenient, the authors present analyses demonstrating low false-positives at this threshold and that the results are robust to different cutoffs).</p><p>Having identified lick, reward, and cue cells, the authors next select the 24% of &quot;cue-only&quot; neurons and look for cells that specifically encode cue value. Because the animal's perception of stimulus value can't be measured directly, the authors created a linear model that predicts the amount of anticipatory licking in the interval between odor cue and reward presentations. The session-average-predicted lick rate by this model is used as an estimate of cue value and is used in the regression analysis that identified value cells. (Hence, the authors' definition of value is dependent on the average amount of anticipatory behavior ahead of a reward, which indicates that compared to the CS+, mice licked around 70% as much to the CS50 and 10% as much to the CS-.) The claim that this is an encoding of value is strengthened by the fact that cells show similar scaling of responses to two odor sets tested. Whereas the authors found more &quot;lick&quot; cells in motor regions and more &quot;cue&quot; cells in sensory regions, they find a consistent percentage of &quot;value&quot; cells (that is, cells found to be cue-only in the initial round of analysis that is subsequently found to encode anticipatory lick rate) across all 11 recorded regions, leading to their claim of a distributed code of value.</p><p>In subsequent sections, the authors expand their model of anticipatory-licking-as-value by incorporating trial and stimulus history terms into the model, allowing them to predict the anticipatory lick rate on individual trials within a session. They also use 2-photon imaging in PFC to demonstrate that neural coding of cue and lick are stable across three days of imaging, supported by two lines of evidence. First, they show that the correlation between cell responses on all periods except for the start of day 1 is more correlated with day 3 responses than expected by chance (although the correlation is low, the authors attribute this to inherent limitations of the data), and that response for a given neuron is substantially better correlated with its own activity across time than random neurons. Second, they show that cue identity is able to capture the highest unique fraction of variance (around 8%) in day 3 cue cells across three days of imaging, and similarly for lick behavior in lick cells and cue+lick in cue+lick cells. Nonetheless, their sample rasters for all imaged cells also indicate that representations are not perfectly stable, and it will be interesting to see what *does* change across the three days of imaging.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84604.3.sa2</article-id><title-group><article-title>Author Response:</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ottenheimer</surname><given-names>David J</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Washington</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hjort</surname><given-names>Madelyn</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Bowen</surname><given-names>Anna J</given-names></name><role specific-use="author">Author</role><aff><institution>Howard Hughes Medical Institute, University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Steinmetz</surname><given-names>Nicholas</given-names></name><role specific-use="author">Author</role></contrib><contrib contrib-type="author"><name><surname>Stuber</surname><given-names>Garret D</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors' response to the original reviews.</p><p>We thank the editor and reviewers for their careful consideration of our manuscript and very helpful feedback, which guided us in improving our manuscript. We would like to highlight three main areas of improvement in this version:</p><list list-type="bullet"><list-item><p>Statistical rigor: we have added more detail to justify our 2% cutoff for GLM variable coding, implemented stricter shuffling and cutoffs for value and history coding, and provided more information on the statistical significance of our pairwise comparisons across regions and groups. These go well beyond the field standard for identifying and comparing neural encoding of task features.</p></list-item><list-item><p>Identification of value coding: we have implemented reviewer suggestions about kernel regression and value coding shuffles, providing even stronger evidence that value signaling among cue neurons is more prevalent than expected by chance, more prevalent than any other cue coding patterns, and present in all recorded regions. The rigor of this analysis is only possible due to our unique task design with 6 cues across two stimulus sets, and our consideration of 153 possible coding models exceeds standard practice for identifying value signals. We now implement population decoding, as well, providing additional support for a robust and widely-distributed value code.</p></list-item><list-item><p>Stability of value code: we have updated our terminology to better highlight that the value signals in our imaging dataset are indeed identified across days, and we add new analysis to show conservation of value-like signals across training days.</p></list-item></list><p>Thanks to the reviewers’ suggestions, our manuscript now has substantially stronger support for the presence of stable and distributed cue value signaling. We address the specific points below.</p><disp-quote content-type="editor-comment"><p><bold>Excerpts from the Consensus Public Reviews:</bold></p><p>One limitation is the lack of focus on population-level dynamics from the perspective of decoding, with the analysis focusing primarily on encoding analyses within individual neurons.</p></disp-quote><p>To address this limitation, we now include population-level decoding analysis (new panels, Figs. 3G-H, 4E). This new analysis reveals that, although value neurons can be used to decode cue identity on par with other cue cells, value neurons are more accurate at predicting the <italic>value</italic> of held out cues (never seen by the model), highlighting the utility of a value signal as a way to consistently represent the value of different stimulus sets.</p><p>Moreover, we find comparable value prediction performance when using value neurons from each region (Fig. 4E), adding more support for the similarity of this signal across regions:</p><disp-quote content-type="editor-comment"><p>The authors use reduced-rank kernel regression to characterize the 5332 recorded neurons on a cell-by-cell basis in terms of their responses to cues, licks, and reward, with a cell characterized as encoding one of these parameters if it accounts for at least 2% of the observed variance. At least 50% of cells met this inclusion criterion in each recorded area. 2% feels like a lenient cutoff, and it is unclear how sensitive the results are to this cutoff, though the authors argue that this cutoff should still only allow a false positive rate of 0.02% (determined by randomly shuffling the onset time of each trial.)</p></disp-quote><p>We have provided more information about the 2% cutoff in a new figure, Figure 2-figure supplement 3. We reanalyzed the false positive rate and found that at a cutoff of 2% (but not 0.5% or 1%) there were no false positives (Figure 2-figure supplement 3B). Thus, we are confident that all neurons contain true task-related signals. Moreover, we found that the pattern of results remains largely unchanged as we change the cutoff over a range from 0.5% to 5%. With more stringent cutoffs, we begin to lose neurons with robust task-related responses (Figure 2-figure supplement 3E), so we continue to use the 2% cutoff in this version of the manuscript.</p><disp-quote content-type="editor-comment"><p>First, they show that the correlation between cell responses on all periods except for the start of day 1 is more correlated with day 3 responses than expected by chance (although the correlation is still quite low, for example, 0.2 on day 2).</p></disp-quote><p>We agree that a correlation of 0.2 does not seem like a large effect, however the variability in neuronal responses and noise level of the measurement enforce a ceiling that we can estimate by predicting data from the same session that it was trained on. We have replotted these data (new panel Fig. 7G) with the correlation normalized to the cross-validated performance on the training day’s data. This shows that the models do about half as well in session 1 and session 2 compared to session 3. The original plot is in a new supplementary figure, Figure 7-figure supplement 1B.</p><p>To further emphasize the similarity across days, we have added new panels (Fig. 7E and Figure 7-figure supplement 1A) showing that, across mice, a typical neuron was more correlated with its own activity on the subsequent day than with ~90% of the other neurons (shuffle controls, 50%).</p><disp-quote content-type="editor-comment"><p>Second, they show that cue identity is able to capture the highest unique fraction of variance (around 8%) in day 3 cue cells across three days of imaging, and similarly for lick behavior in lick cells and cue+lick in cue+lick cells. Nonetheless, their sample rasters for all imaged cells also indicate that representations are not perfectly stable, and it will be interesting to see what *does* change across the three days of imaging.</p></disp-quote><p>We agree that the representations are not perfectly stable and that is an interesting point of further investigation. One difference we did observe is increased cue coding across training (Figs. 6H, 7H).</p><disp-quote content-type="editor-comment"><p>Importantly, the authors do not present evidence that value itself is stably encoded across days, despite the paper's title. The more conservative in its claims in the Discussion seems more appropriate: &quot;these results demonstrate a lack of regional specialization in value coding and the stability of cue and lick [(not value)] codes in PFC.&quot;</p></disp-quote><p>Due to confusing terminology on our part, the reviewers were mistaken about the timing of the experiment where we assess the stability of value coding. In the imaging sessions, odor sets were always presented on separate days. Thus, when we identify value coding in our imaged population, it is across two consecutive days with different odor sets, which is in itself evidence of a stable value code. We have updated our terminology and the text to make this clearer. We also added a new set of plots (Fig. 8H-I) showing the conservation of value-like signaling in cells we tracked across the first three sessions of odor set A, and, as above, that the correlation of these neurons across days is greater than expected by chance. These analyses lend further support to the stability of the value signal.</p><disp-quote content-type="editor-comment"><p><bold>Additional technical comments:</bold></p><p>1. The &quot;shuffle #33&quot; in figure 3B is confusing. The fit kernel in this shuffle shows that the &quot;high&quot; and &quot;medium&quot; responses increase above the pre-stimulus baseline. The &quot;high&quot; response is a combination of set 2 CS+ and set 1 CS50, both of which strongly suppressed the cell's firing over the 2.5-second window shown. Why then does the cue kernel fit these two trials predict an increase in firing rate above baseline at the 2.5-second time point? Is it a consequence of the reduced rank regression process, and if so, how? This strange-looking fit that does not well capture the response of the original cell makes me worry that the high fraction of identified &quot;value&quot; cells may be due to some constraint on the shuffle fits that leads them to often perform poorly.</p></disp-quote><p>To address this concern, we refit the value shuffle and its models using a full kernel regression model (rather than reduced ranks). It does improve the appearance of the kernel fits (updated Fig. 3B), and we now use this new approach when fitting cue coding models in the revised manuscript. The regularization inherent in reduced rank constrains the shape of the cue kernel somewhat, which contributed to the shape of the fits (although this did not negatively impact the variance explained); however, because of the importance of the shape of these alternative cue coding models to the interpretation of the analysis, we agree with the reviewers that this was worth improving. The main constraint on the value model and its shuffles, however, is that all cues must use the same template, scaled according to particular values assigned to each cue in each shuffle, which will doubtless lead to compromised (and strange-looking) fits when the shuffled values do not match the ranking of neuron’s cue activity. Critically, this constraint is applied equally to the value model and all the shuffles and would not bias the fits of any one model.</p><disp-quote content-type="editor-comment"><p>2. The &quot;shuffle&quot; condition when testing for value cells always assumes two high responses, two medium responses, and two low responses. This strategy doesn't account for cells that respond to only a subset of cues, as one might expect in a sparse-coding olfactory region. We suggest adding a set of shuffles where responses are split into two groups, with either 3 conditions per group or 2 in one group and 4 in the other.</p></disp-quote><p>We appreciate this valuable suggestion. We added all permutations of models with high responses to 6, 5, 4, 3, 2, or 1 odor cue to the analysis. We still find that the value model is the most frequent best model, displayed in new panels Fig. 3C-D and Figure 3-figure supplement 1A-B. The additional models allowed us to identify other neurons with cue activity best fit by models highly correlated with the ranked value model, which we term “value-like” neurons, including most neurons previously described as “trial-type” neurons. All 153 models and the fraction of neurons best fit by each one are depicted in Figure 3-figure supplement 1.</p><p>After implementing the changes to both the method of model fitting (full kernel regression, as noted above) and the possible alternative models, the distribution of value cells has changed slightly. All regions contain value cells, supporting our original conclusion that the value signal is distributed, but there is slight enrichment in PFC when combining these five regions together (Fig. 4A).</p><p>We have updated the conclusions of the paper accordingly:</p><p>Introduction: <italic>“Unexpectedly, in contrast to the graded cue and lick coding across these regions, the proportion of neurons encoding cue value was more consistent across regions, with a slight enrichment in PFC but with similar value decoding performance across all regions.”</italic></p><p>Results: <italic>“Interestingly, the frequency of value cells was similar across the recorded regions (Fig. 4A). Indeed, despite the regional variability in number of cue cells broadly (Fig. 2F-G), there were very few regions that statistically differed in their proportions of value cells (Fig. 4A, Figure 4-figure supplement 1). Overall, though, there were slightly more value cells across all of PFC than in motor and olfactory cortex (Figs. 4A, Figure 4-figure supplement 1). Although there were the most cue neurons in olfactory cortex, these were less likely to encode value than cue neurons in other regions (Figure 4-figure supplement 2). Value-like cells were also widespread; they were less frequent in motor cortex as a fraction of all neurons, but they were equivalently distributed in all regions as a fraction of cue neurons (Fig. 4B, Figure 4-figure supplement 1, Figure 4-figure supplement 2).”</italic></p><p>Discussion: <italic>“In contrast to regional differences in the proportion of cue-responsive neurons, cue value cells were present in all regions and could be used to decode value with similar accuracy regardless of region.”</italic> AND <italic>“The distribution of cue cells with linear coding of value was mostly even across regions, with slight enrichment overall in PFC compared to motor and olfactory cortex, but no subregional differences in PFC. Importantly, cue value could be decoded from the value cells in all regions with similar accuracy.”</italic></p><disp-quote content-type="editor-comment"><p>3. On pages 11-12, the authors write &quot;value coding is similarly represented across the regions we sampled.&quot; I feel this isn't quite what was shown: the authors have shown that all recorded regions contain a roughly comparable number of individual cells that are modulated by value, i.e. &quot;value cells&quot;. However, the authors also showed that some recorded cells have mixed selectivity for value and other factors- it is possible that these mixed selectivity cells do vary between brain regions in their quantity or degree of value coding. Regions could potentially also vary in the dynamics of their value response, or in the trial-to-trial variability in the activity of value cells. I suggest the authors revise their original statement, for example by writing &quot;we find a similar proportion of value-specific cells across the regions we sampled.&quot;</p></disp-quote><p>We thank the reviewer for carefully reviewing our claims. In addition to showing similar proportions of value cells, we also show that the value-related activity is similar (by plotting the first principal component of value and value-like cells, Fig. 4C-D) and that cue value could be decoded from the value cells in all regions with similar accuracy (new panel, Fig. 4E). We have updated the text to more accurately reflect these observations:</p><p><italic>“In contrast to regional differences in the proportion of cue-responsive neurons, cue value cells were present in all regions and value could be decoded from them with similar accuracy regardless of region.”</italic></p><disp-quote content-type="editor-comment"><p>4. We appreciate the authors' idea to introduce a history term to their value cell model but worry that the distinction between history-dependent value cells and lick/cue+lick cells in Figure 4 has gotten fuzzy. At this point, history-dependent value cells are the product of a set of steps: (1) they are identified as &quot;cue&quot; neurons because the cue type accounts for at least 2% of the variance, while the lick rate does not, then (2) among the cue neurons, a subset are identified as &quot;value&quot; neurons because their activity scales with the cue type across both odor sets, and then (3) among value neurons, the &quot;history-dependent&quot; value neurons show a response rate that scales with a model that predicts anticipatory licking. Our concern comes down to this: your conclusion that these cells are not licking cells hinges on the initial point that licking does not account for 2% of the observed variance in cell activity. But if you had dedicated an equal number of model parameters and selection steps to your licking model, might it still not turn out that a licking model predicts their activity as well as the history-dependent cue value model?</p><p>What would bolster our confidence here would be a comparison of variance explained: if you compare the predictions of the history-dependent value-encoding cue neuron model to the predictions of a simple lick neuron model, how much better does the former predict what the cells are doing? Are all those extra parameters and selection steps really contributing to an improved description of how neurons will respond?</p></disp-quote><p>First, we would like to emphasize that “cue” neurons, as a population, have no discernible modulation by licks, which can be seen when comparing their activity on CS50 trials with and without reward, when licking clearly varies (Figure 2-figure supplement 2D). A new panel, Figure 5E now depicts the improvement in variance explained by the history model over a lick only model. The improvement is robust and universal. This is because even though the number of anticipatory licks per trial is used to fit the weights of our trial value model, these cue neurons have temporal dynamics that are more consistent with cue presentation than the presence of licks. We explain more below in our response to point 7.</p><disp-quote content-type="editor-comment"><p>5. The paper's title claims that the coding of cue value is both stable and distributed. While the point for value coding being distributed is well supported with analysis, the claim that cue value coding is &quot;stable&quot; is weaker. The authors show in Figure 6 that cue identity best accounts for unique variance among cue cells across three days of imaging, but it does not follow that cue value is similarly stable. Figure 7 shows that on day 3 of imaging, the two odor sets have similar encoding- but this analysis is only performed within day 3, not across days. Why not examine unique variance among value cells over days, as was done for a cue, lick, and both cells in Figure 6G? That seems to be an important missing piece and a logical next step. The Discussion is more conservative in its claims- &quot;these results demonstrate a lack of regional specialization in value coding and the stability of cue and lick [(not value)] codes in PFC.&quot; But this subtlety is missing from the paper's title and introduction.</p></disp-quote><p>First, an important correction. “This analysis is only performed within day 3, not across days,” is a misunderstanding of our experiment brought on by our confusing terminology, which we have updated. This figure (now Figure 8) analyzes two sessions performed on consecutive days: Odor Set A day 3 (A3) and Odor Set B day 3 (B3), which constitute days 5 and 6 of our experiment (see updated panels Fig. 1B, 6A). This is why identifying value signaling across both of these sessions is justification for a stable code; by definition, it was present on two consecutive days.</p><p>A limitation of our imaging experiment prevents us from evaluating value signaling in each individual session (like we did for cues and licks). For the imaging, we only presented one odor set per session (unlike the electrophysiology, where odor sets were presented in blocks). Our method of identifying value signals relies on two odor sets, so we cannot quantify it on a per session basis in the imaging. However, to address this as best we could, we identified CS+-preferring cue cells in session A3 (odor set A day 3) and plotted them for sessions A1-A3 (Fig. 8H), which reveals a conserved value-like signal across days. We also found that the correlation of the activity of these neurons across days was higher than expected by chance (Fig. 8I).</p><p>We have edited the discussion text about coding stability, adding in more detail and caveats:</p><p><italic>“Previous reports have observed drifting representations in PFC across time (Hyman et al., 2012; Malagon-Vina et al., 2018), and there is compelling evidence that odor representations in piriform drift over weeks when odors are experienced infrequently (Schoonover et al., 2021). On the other hand, it has been shown that coding for odor association is stable in ORB and PL, and that coding for odor identity is stable in piriform (Wang et al., 2020a), with similar findings for auditory Pavlovian cue encoding in PL (Grant et al., 2021; Otis et al., 2017) and ORB (Namboodiri et al., 2019). We were able to expand upon these data in PL by identifying both cue and lick coding and showing separable, stable coding of cues and licks across days and across sets of odors trained on separate days. We were also able to detect value coding common to two stimulus sets presented on separate days, and conserved value features across the three training sessions. Notably, the model with responses only to CS+ cues best fit a larger fraction of imaged PL neurons than the ranked value model, a departure from the electrophysiology results. It would be interesting to know if this is due to a bias introduced by the imaging approach, the slightly reduced CS50 licking relative to CS+ licking in the imaging cohort, or the shorter imaging experimental timeline.</italic></p><p><italic>The consistency in cue and lick representations we observed indicates that PL serves as a reliable source of information about cue associations and licking during reward seeking tasks, perhaps contrasting with other representations in PFC (Hyman et al., 2012; Malagon-Vina et al., 2018). Interestingly, the presence of lick, but not cue coding at the very beginning of the first session of training suggests that lick cells in PL are not specific to the task but that cue cells are specific to the learned cue-reward associations. Future work could expand upon these findings by examining stimulus-independent value coding within session across many consecutive days.”</italic></p><disp-quote content-type="editor-comment"><p>6. Considering licking as the readout of value has pros and cons. Anticipatory licking may be correlated with subjective value, but certainly nonlinearly. After all, licking has a ceiling and floor (bounded rate from 0-&gt;10 Hz). Are results consistent with the objective value of the cues (which are 0, .5, 1)? Which measure better explained the data?</p></disp-quote><p>Thanks to this important suggestion, we tried fitting another set of models with 0, 0.5, 1 as the cue values. We found the same pattern of results. Overall, the fits were slightly better with 0, 0.5, 1, with 50.6% of potential value neurons (found with either version of the model) better fit by 0, 0.5, 1, and with mean variance explained of 0.265 with 0, 0.5, 1 (compared to 0.264 with the anticipatory lick values). Without strong evidence to choose one model over the other, we decided to use 0, 0.5, 1 because it exactly reflects reward probability, and is more objective as the reviewer notes, whereas before we relied on a noisier estimate of subjective value. We have changed the text accordingly.</p><disp-quote content-type="editor-comment"><p>7. How can a neuron encode &quot;Cue&quot; in a value-dependent manner and not also encode licking, given they are correlated? If the kernel window includes anticipatory licking, and anticipatory licking is by definition related to value, then how could a licking kernel not at least explain some of that neuron's variance?</p></disp-quote><p>The trial estimates of value from the lick linear regression are derived from typical licking patterns across all sessions and do not incorporate the particular number of licks on a given trial <italic>or the latency of licking relative to cue onset</italic>. Although the trial value model is predicting the number of licks on each trial, it only uses cue identity and reward history to make its prediction, so it is not tightly correlated with the stochastic licks on a given trial. And, importantly, we input the trial value as a cue kernel spanning the entire cue period, whereas lick kernels, per our definition, are restricted to a window around when licking occurs, which generously encompasses neural signals relating to both lick initiation and feedback. Licking <italic>can</italic> explain some of value and (history) neurons’ variance, which you can see in our new panel Fig. 5E, but it does not contribute any <italic>unique</italic> variance to the model. That is, with or without licks, the model performs just as well, so the activity of the neuron does not track any of the unique features of licks over cues (like whether or not the mouse licked on trial, when the mouse started licking on a given trial). Without cues, however, the model does worse, which means that the neuron’s activity is modulated by cues separately from when the mouse is licking. Thus, we can conclude the neuron encodes cues, but we have no evidence the neuron encodes licks (beyond the extent to which licks are correlated with cues). In our example fit in 5E, you can see how, although licks track value, they cannot recapitulate the temporal dynamics of this cue neuron. We added more description of this distinction in the manuscript.</p><disp-quote content-type="editor-comment"><p>8. The ordering analysis with the 89 permutations is very nice for showing across the population the &quot;value ordered&quot; gains are the best explanation of the neural activity. However, it doesn't tell you that any one neuron significantly encodes value, or the strength of this effect if they do. For the former, they could compare to a null distribution of shuffled order of neural vs CS data, and consider neurons for which model is better than chance ( a .05 FDR on a null distribution would be appropriate). This is important for supporting their conclusion of the fraction of neurons encoding value for each region.</p></disp-quote><p>In fact, with so many alternative models, the probability of a neuron being best fit by the value model but not encoding value above chance is extremely low. To confirm this, we ran the reviewer’s suggested shuffle analysis, and found that 100% of value neurons performed above the 0.05 FDR. We have added this result to the methods:</p><p><italic>“To verify the robustness of value coding in the neurons best fit by the ranked value model, we fit each of those neurons with 1000 iterations of the cue value model with shuffled cue order to create a null distribution. The fits of the original value model exceeded the 98th percentile of the null for all value neurons.”</italic></p><disp-quote content-type="editor-comment"><p>9. Similarly the 65% cutoff for trial history relative to shuffled is unusually low and therefore not convincing these neurons significantly encode the value. Usually, 95% or 99% is selected to give you a more standard significance criterion (FDR).</p></disp-quote><p>We have changed the cutoff to 95%. We originally selected 65% because neurons in the 65% to 95% range had clear history effects, especially at the population level, but we appreciate the importance of rigorous selection. Note this shuffle is very strict, preserving CS+, CS50, CS- ranking but shuffling within-cue fluctuations in value due to trial history. With the stricter value and history shuffling, we now observe fewer history neurons, and they are most prevalent in PFC (Fig. 5I)</p><disp-quote content-type="editor-comment"><p>10. &quot;Regions with non-overlapping CIs were considered to have significantly different fractions of neurons of that coding type.&quot; This isn't a statistical test. Confidence intervals are not the same as significance.</p></disp-quote><p>We now perform Bonferroni-corrected pairwise contrasts between all regions in the generalized linear mixed effects model. We added the p-values for all the comparisons that previously relied on non-overlapping confidence intervals in supplementary tables.</p><disp-quote content-type="editor-comment"><p><bold>Minor comments:</bold></p><p>The methods are hard to read. Most of the information seems to be there but in general, paragraphs need to be read over multiple times for meaning to emerge.</p></disp-quote><p>We have edited for clarity, and if there are particular sections that remain unclear, we would be happy to know which ones.</p><disp-quote content-type="editor-comment"><p>Why is there a block predictor in the encoding model?</p></disp-quote><p>Because not every odor is present in every block, we did not want our models to use the specific cue predictors to try to account for differences in baseline activity that naturally occur across the session. Thus, each of the six blocks has its own predictor that serves as a constant that can adjust for changing baseline firing rate. Importantly, the block predictor simply marks the passage of blocks and contains no information about the odors present. We added more information about this to the methods:</p><p><italic>“For electrophysiology experiments, the model also included 6 constants that identified the block number, accounting for tonic changes in firing rate across blocks. Because not all cues were present in every block, this strategy prevented the cue kernels from being used to explain baseline changes across blocks.”</italic></p><disp-quote content-type="editor-comment"><p>Did you use an elastic net rather than a lasso? What is the alpha parameter for lasso?</p></disp-quote><p>We used an elastic net with alpha = 0.5. We added this information to the methods.</p><disp-quote content-type="editor-comment"><p>Figure 3F legend doesn't seem to match the figure.</p></disp-quote><p>Corrected.</p></body></sub-article></article>