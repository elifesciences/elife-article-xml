<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">52599</article-id><article-id pub-id-type="doi">10.7554/eLife.52599</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Hierarchical temporal prediction captures motion processing along the visual pathway</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-98202"><name><surname>Singer</surname><given-names>Yosef</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4480-0574</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-328401"><name><surname>Taylor</surname><given-names>Luke CL</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38220"><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2969-7572</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-14601"><name><surname>King</surname><given-names>Andrew J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5180-7179</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-19854"><name><surname>Harper</surname><given-names>Nicol S</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7851-4840</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor3">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Physiology, Anatomy and Genetics</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-68611"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing editor</role><aff><institution>University of Chicago</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>yosef.singer@stcatz.ox.ac.uk</email> (YS);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>andrew.king@dpag.ox.ac.uk</email> (AK);</corresp><corresp id="cor3"><label>*</label>For correspondence: <email>nicol.harper@dpag.ox.ac.uk</email> (NH);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>16</day><month>10</month><year>2023</year></pub-date><volume>12</volume><elocation-id>e52599</elocation-id><history><date date-type="received"><day>14</day><month>10</month><year>2019</year></date><date date-type="accepted"><day>04</day><month>10</month><year>2023</year></date></history><permissions><copyright-statement>Â© 2023, Singer et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Singer et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-52599-v1.pdf"/><abstract><p>Visual neurons respond selectively to features that become increasingly complex from the eyes to the cortex. Retinal neurons prefer flashing spots of light, primary visual cortical (V1) neurons prefer moving bars, and those in higher cortical areas favor complex features like moving textures. Previously, we showed that V1 simple cell tuning can be accounted for by a basic model implementing temporal prediction - representing features that predict future sensory input from past input (Singer et al., 2018). Here we show that hierarchical application of temporal prediction can capture how tuning properties change across at least two levels of the visual system. This suggests that the brain does not efficiently represent all incoming information; instead, it selectively represents sensory inputs that help in predicting the future. When applied hierarchically, temporal prediction extracts time-varying features that depend on increasingly high-level statistics of the sensory input.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><name><surname>King</surname><given-names>Andrew J</given-names></name><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>University of Oxford Clarendon Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Singer</surname><given-names>Yosef</given-names></name><name><surname>Taylor</surname><given-names>Luke CL</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All custom code used in this study was implemented in Python. The code for the models and analyses shown in Figures 1-8 and associated sections can be found at https://bitbucket.org/ox-ang/hierarchical_temporal_prediction/src/master/. The V1 neural response data (Ringach et al., 2002) used for comparison with the temporal prediction model in Figure 6 came from http://ringachlab.net/ (&quot;Data &amp; Code&quot;, &quot;Orientation tuning in Macaque V1&quot;). The V1 image response data used to test the models included in Figure 9 were downloaded with permission from https://github.com/sacadena/Cadena2019PlosCB (Cadena et al., 2019). The V1 movie response data used to test these models were collected in the Laboratory of Dario Ringach at UCLA and downloaded from https://crcns.org/data-sets/vc/pvc-1 (Nahaus and Ringach, 2007; Ringach and Nahaus, 2009). The code for the models and analyses shown in Figure 9 and the associated section can be found at https://github.com/webstorms/StackTP and https://github.com/webstorms/NeuralPred. The movies used for training the models in Figure 9 are available at https://figshare.com/articles/dataset/Natural_movies/24265498.</p></sec><supplementary-material><ext-link xlink:href="elife-52599-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>