<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="article-commentary" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">104965</article-id><article-id pub-id-type="doi">10.7554/eLife.104965</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Insight</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Locomotion</subject></subj-group></article-categories><title-group><article-title>How vision shapes the paths we choose</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kuo</surname><given-names>Arthur D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5233-9709</contrib-id><email>arthurdkuo@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/><bio><p><bold>Arthur D Kuo</bold> is in the Faculty of Kinesiology and Department of Biomedical Engineering, University of Calgary, Calgary, Canada</p></bio></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03yjb2x39</institution-id><institution>Faculty of Kinesiology and Department of Biomedical Engineering, University of Calgary</institution></institution-wrap><addr-line><named-content content-type="city">Calgary</named-content></addr-line><country>Canada</country></aff></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>09</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e104965</elocation-id><permissions><copyright-statement>© 2024, Kuo</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Kuo</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-104965-v1.pdf"/><related-article related-article-type="commentary-article" ext-link-type="doi" xlink:href="10.7554/eLife.91243" id="ra1"/><abstract><p>A mathematical model can predict the path walkers take through a rugged landscape, including the tendency of people to avoid paths that are too steep, even if it means going farther.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>natural locomotion</kwd><kwd>visual control of locomotion</kwd><kwd>photogrammetry</kwd><kwd>visual decisions</kwd><kwd>locomotion</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A mathematical model can predict the path walkers take through a rugged landscape, including the tendency of people to avoid paths that are too steep, even if it means going farther.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></article-meta></front><body><boxed-text><p><bold>Related research article</bold> Muller KS, Panfili D, Shields S, Matthis JS, Bonnen K, Hayhoe MM. 2024. Foothold selection during locomotion in uneven terrain: Results from the integration of eye tracking, motion capture, and photogrammetry. <italic>eLife</italic> <bold>12</bold>:RP91243. doi: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.7554/eLife.91243">10.7554/eLife.91243</ext-link>.</p></boxed-text><p>When walking, our brain needs to make decisions, such as where to step and which path to follow. Visual inputs are central to this process, as they allow us to continuously estimate our motion and orientation, as well as regulate our direction and speed (<xref ref-type="bibr" rid="bib8">Warren et al., 2001</xref>).</p><p>This continuous regulation can be explained using mathematical models that rely on vector calculus (<xref ref-type="bibr" rid="bib6">Matthis et al., 2022</xref>). However, walking also involves discrete choices – such as selecting which available foothold to use, or turning left or right – that are guided by visual features of the path ahead, such as the rockiness and steepness of the terrain (<xref ref-type="bibr" rid="bib5">Matthis et al., 2018</xref>). While it may seem obvious that people avoid rugged paths when easier alternatives exist, there is no mathematical model to explain this behavior. Now, in eLife, Mary Hayhoe and colleagues – including Karl Muller as first author – report the results of a study that sheds new light on how these decisions are made (<xref ref-type="bibr" rid="bib7">Muller et al., 2024</xref>).</p><p>The researchers analyzed data from previous studies in which they had tracked the eye and body movements of nine individuals as they walked across natural terrains (<xref ref-type="bibr" rid="bib6">Matthis et al., 2022</xref>; <xref ref-type="bibr" rid="bib1">Bonnen et al., 2021</xref>). Participants also wore eye-tracking cameras that captured visual information about where they were looking within the surrounding physical landscape. Muller et al. used this data to create a three-dimensional computational reconstruction of the terrains, which they aligned to the body and eye movements of the participants. This allowed them to develop egocentric visual depth images (which estimated how the retinas of individuals detect the depth of the terrain), and to determine where participants placed their feet.</p><p>The researchers – who are based at the University of Texas at Austin, Indiana University and Northeastern University – then used these visual depth images and foothold locations to train a neural network where to step in the reconstructed terrain. The trained model successfully predicted where the human participants placed their feet better than chance, suggesting that decisions about where to step are influenced by visual depth cues. This agrees with prior work showing reduced depth cues cause walkers to shift their gaze to closer footholds, perhaps so they can devote more attention to resolving depth ambiguities (<xref ref-type="bibr" rid="bib1">Bonnen et al., 2021</xref>).</p><p>Next, Muller et al. used their model to investigate whether walkers chose flatter, more winding paths over direct, steeper alternatives that require more energy to cross due to the uneven terrain (<xref ref-type="bibr" rid="bib3">Darici and Kuo, 2023</xref>; <xref ref-type="bibr" rid="bib4">Kowalsky et al., 2021</xref>). The team compared two metrics: tortuosity, which is the ratio between the length of a path and the direct distance between its start and end; and a dimensionless measure of ruggedness, equal to the average angle of the up and down steps within a path. These metrics were recorded for the actual path the walkers took, and also for simulated, alternative routes they might have taken through the same terrain.</p><p>Muller et al. found that the tortuosity of the paths chosen by the human walkers correlated with the ruggedness of the most direct simulated path through the terrain: the steeper and more rugged the direct path, the more likely a walker is to detour and take a longer, flatter route. This suggests a trade-off whereby walkers assess paths with high tortuosity to be less energetically costly than ones which are straighter but more rugged. Muller et al. also discovered that this trade-off was stronger for shorter individuals, possibly due to greater difficulty with steep steps, though this effect warrants further study (as discussed in the <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/reviewed-preprints/91243v2/reviews#peer-review-1">public reviews of the article</ext-link>).</p><p>Studying locomotion in natural settings is challenging, as terrains and visual features are harder to manipulate or measure compared to controlled laboratory environments. The methods created by Muller et al. open up new avenues for understanding decision-making during locomotion. However, there is much about the trade-off proposed in the study that remains unknown.</p><p>For example, controlled experiments are needed to confirm whether the ruggedness of the simulated direct paths match those experienced in real life. In addition, the human eye likely perceives other aspects of the terrain beyond just depth, and it remains to be seen whether other visual cues help evaluate different structural properties of the landscape, such as friction and the hardness of footholds. Related issues in natural locomotion, such as energy expenditure, also merit further investigation. For instance, the energetic costs of tortuous versus rugged paths still need quantification, as do temporal costs, which are also suspected to govern ecological behavior (<xref ref-type="bibr" rid="bib2">Carlisle and Kuo, 2023</xref>).</p><p>The approach taken by Muller et al. could also be used to study people in human-designed environments, such as pedestrians crossing busy roads (do people jaywalk rather than use pedestrian crossings to save energy, time, or other costs?). Whether navigating natural landscapes or cityscapes, humans constantly create their own paths according to a variety of physical and perceptual criteria.</p></body><back><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Gibaldi</surname><given-names>A</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Levi</surname><given-names>DM</given-names></name><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Binocular vision and the control of foot placement during walking in natural terrain</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>20881</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-99846-0</pub-id><pub-id pub-id-type="pmid">34686759</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlisle</surname><given-names>RE</given-names></name><name><surname>Kuo</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Optimization of energy and time predicts dynamic speeds for human walking</article-title><source>eLife</source><volume>12</volume><elocation-id>e81939</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.81939</pub-id><pub-id pub-id-type="pmid">36779697</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darici</surname><given-names>O</given-names></name><name><surname>Kuo</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Humans plan for the near future to walk economically on uneven terrain</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2211405120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2211405120</pub-id><pub-id pub-id-type="pmid">37126717</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kowalsky</surname><given-names>DB</given-names></name><name><surname>Rebula</surname><given-names>JR</given-names></name><name><surname>Ojeda</surname><given-names>LV</given-names></name><name><surname>Adamczyk</surname><given-names>PG</given-names></name><name><surname>Kuo</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Human walking in the real world: Interactions between terrain type, gait parameters, and energy expenditure</article-title><source>PLOS ONE</source><volume>16</volume><elocation-id>e0228682</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0228682</pub-id><pub-id pub-id-type="pmid">33439858</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gaze and the control of foot placement when walking in natural terrain</article-title><source>Current Biology</source><volume>28</volume><fpage>1224</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.008</pub-id><pub-id pub-id-type="pmid">29657116</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Muller</surname><given-names>KS</given-names></name><name><surname>Bonnen</surname><given-names>KL</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Retinal optic flow during natural locomotion</article-title><source>PLOS Computational Biology</source><volume>18</volume><elocation-id>e1009575</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009575</pub-id><pub-id pub-id-type="pmid">35192614</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>KS</given-names></name><name><surname>Panfili</surname><given-names>D</given-names></name><name><surname>Shields</surname><given-names>S</given-names></name><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Foothold selection during locomotion in uneven terrain: Results from the integration of eye tracking, motion capture, and photogrammetry</article-title><source>eLife</source><volume>12</volume><elocation-id>RP91243</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.91243</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name><name><surname>Kay</surname><given-names>BA</given-names></name><name><surname>Zosh</surname><given-names>WD</given-names></name><name><surname>Duchon</surname><given-names>AP</given-names></name><name><surname>Sahuc</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Optic flow is used to control human walking</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>213</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1038/84054</pub-id><pub-id pub-id-type="pmid">11175884</pub-id></element-citation></ref></ref-list></back></article>