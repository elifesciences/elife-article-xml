<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78344</article-id><article-id pub-id-type="doi">10.7554/eLife.78344</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A hardware system for real-time decoding of in vivo calcium imaging data</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-273113"><name><surname>Chen</surname><given-names>Zhe</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-281541"><name><surname>Blair</surname><given-names>Garrett J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2724-8914</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-255591"><name><surname>Guo</surname><given-names>Changliang</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273115"><name><surname>Zhou</surname><given-names>Jim</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-302440"><name><surname>Romero-Sosa</surname><given-names>Juan-Luis</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-49902"><name><surname>Izquierdo</surname><given-names>Alicia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9897-2091</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-40842"><name><surname>Golshani</surname><given-names>Peyman</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273116"><name><surname>Cong</surname><given-names>Jason</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-273117"><name><surname>Aharoni</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4931-8514</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-272137"><name><surname>Blair</surname><given-names>Hugh T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8256-5109</contrib-id><email>tadblair@g.ucla.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>Department of Electrical and Computer Engineering, University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>Department of Psychology, University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>David Geffen School of Medicine, University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>Department of Neurology, David Geffen School of Medicine, University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>Integrative Center for Learning and Memory, University of California, Los Angeles</institution></institution-wrap><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>24</day><month>01</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e78344</elocation-id><history><date date-type="received" iso-8601-date="2022-03-03"><day>03</day><month>03</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-23"><day>23</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-02-02"><day>02</day><month>02</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.01.31.478424"/></event></pub-history><permissions><copyright-statement>© 2023, Chen et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Chen et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78344-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-78344-figures-v2.pdf"/><abstract><p>Epifluorescence miniature microscopes (‘miniscopes’) are widely used for in vivo calcium imaging of neural population activity. Imaging data are typically collected during a behavioral task and stored for later offline analysis, but emerging techniques for online imaging can support novel closed-loop experiments in which neural population activity is decoded in real time to trigger neurostimulation or sensory feedback. To achieve short feedback latencies, online imaging systems must be optimally designed to maximize computational speed and efficiency while minimizing errors in population decoding. Here we introduce <italic>DeCalciOn</italic>, an open-source device for real-time imaging and population decoding of in vivo calcium signals that is hardware compatible with all miniscopes that use the UCLA Data Acquisition (DAQ) interface. DeCalciOn performs online motion stabilization, neural enhancement, calcium trace extraction, and decoding of up to 1024 traces per frame at latencies of &lt;50 ms after fluorescence photons arrive at the miniscope image sensor. We show that DeCalciOn can accurately decode the position of rats (<italic>n</italic> = 12) running on a linear track from calcium fluorescence in the hippocampal CA1 layer, and can categorically classify behaviors performed by rats (<italic>n</italic> = 2) during an instrumental task from calcium fluorescence in orbitofrontal cortex. DeCalciOn achieves high decoding accuracy at short latencies using innovations such as field-programmable gate array hardware for real-time image processing and contour-free methods to efficiently extract calcium traces from sensor images. In summary, our system offers an affordable plug-and-play solution for real-time calcium imaging experiments in behaving animals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>closed-loop</kwd><kwd>neural decoding</kwd><kwd>calcium imaging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>NSF NeuroNex</institution></institution-wrap></funding-source><award-id>1707408</award-id><principal-award-recipient><name><surname>Golshani</surname><given-names>Peyman</given-names></name><name><surname>Cong</surname><given-names>Jason</given-names></name><name><surname>Aharoni</surname><given-names>Daniel</given-names></name><name><surname>Blair</surname><given-names>Hugh T</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>DeCalciOn is a low-cost open-source hardware system for real-time in vivo calcium imaging that offers capabilities for online decoding of neural population activity and delivery of short latency closed-loop feedback in freely behaving animals.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Miniature epifluorescence microscopes (‘miniscopes’) can be worn on the head of an unrestrained animal to perform in vivo calcium imaging of neural population activity during free behavior (<xref ref-type="bibr" rid="bib17">Ghosh et al., 2011</xref>; <xref ref-type="bibr" rid="bib33">Ziv et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Cai et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Aharoni et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Hart et al., 2020</xref>). Imaging data are usually collected while subjects are engaged in a task and stored for later offline analysis. Popular offline analysis packages such as CaImAn (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>) and MIN1PIPE (<xref ref-type="bibr" rid="bib14">Deng et al., 2015</xref>) employ algorithms (<xref ref-type="bibr" rid="bib27">Pnevmatikakis et al., 2016</xref>) for demixing crossover fluorescence between multiple sources to extract calcium traces from single neurons, but these algorithms cannot be implemented in real time because they incur significant computing delays and rely on acausal computations. Emerging techniques for online trace extraction (<xref ref-type="bibr" rid="bib15">Friedrich et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Mitani and Komiyama, 2018</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2022a</xref>; <xref ref-type="bibr" rid="bib16">Friedrich et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Taniguchi et al., 2021</xref>) offer potential for carrying out real-time imaging experiments in which closed-loop neurostimulation or sensory feedback are triggered at short latencies in response to neural activity decoded from calcium fluorescence (<xref ref-type="bibr" rid="bib1">Aharoni and Hoogland, 2019</xref>; <xref ref-type="bibr" rid="bib32">Zhang et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Liu et al., 2021</xref>). Such experiments could open new avenues for investigating the neural basis of behavior, developing brain–machine interface devices, and preclinical testing of neurofeedback-based therapies for neurological disorders (<xref ref-type="bibr" rid="bib19">Grosenick et al., 2015</xref>). To advance these novel lines of research, it is necessary to develop and disseminate accessible tools for online calcium imaging and neural population decoding.</p><p>Here we introduce <italic>DeCalciOn</italic>, a plug-and-play hardware device for <italic>De</italic>coding <italic>Calci</italic>um Images <italic>On</italic>line that is compatible with existing miniscope devices which utilize the UCLA Miniscope DAQ interface board (<xref ref-type="bibr" rid="bib5">Cai et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Scott et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">de Groot et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Blair et al., 2021</xref>). We show that this system can decode hippocampal CA1 calcium activity in unrestrained rats (<italic>n</italic> = 12) running on a linear track, and can also categorically classify behaviors performed by rats (<italic>n</italic> = 2) during an instrumental touchscreen task from orbitofrontal cortex (OFC) calcium activity. Performance tests show that the system can accurately decode up to 1024 calcium traces to trigger TTL outputs within &lt;50 ms of fluorescence detection by the miniscope’s image sensor. To achieve these short real-time decoding latencies, DeCalciOn relies upon customized hardware for online image processing (<xref ref-type="bibr" rid="bib7">Chen et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2022a</xref>) running in the fabric of a field-programmable gate array (FPGA); the system also utilizes efficient software for trace decoding (<xref ref-type="bibr" rid="bib10">Chen et al., 2022b</xref>) that achieves microsecond variability in decoding latencies by running on an ARM core under the FreeRTOS real-time operating system. We show that the time required for training the real-time decoder from example data (prior to the initiation of online imaging) can be minimized by employing contour-free methods for defining regions of interest (ROIs) from which calcium traces are extracted.</p><p>In summary, DeCalciOn provides the research community with a low-cost, open-source, easy-to-use hardware platform for short latency real-time decoding of calcium trace population activity. All hardware, software, and firmware are openly available through <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link>.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The DeCalciOn hardware system is implemented on an Avnet Ultra96 development board featuring a Xilinx Zynq Ultrascale + multiprocessor system-on-a-chip (MPSoC) with 2-GB DRAM (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). A custom interface board mated to the Ultra96 receives real-time image data from a modified version of the UCLA Miniscope Data Acquisition (DAQ) interface, which has 14 flywires soldered to the printed circuit board (PCB) for transmitting deserialized video to the Ultra96 (pre-modified DAQ boards can be obtained through <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link>). Throughout online imaging sessions, the DeCalciOn system is controlled from a host PC running standard Miniscope DAQ software (to focus the lens and adjust the LED light source) alongside newly developed real-time interface (RTI) software which communicates with our custom ACTEV (Accelerator for Calcium Trace Extraction from Video) hardware accelerator, which runs in the fabric of the MPSoC’s FPGA. Raw video data (including real-time motion correction vectors) and calcium trace values are transmitted via Ethernet from the Ultra96 to a host PC for storage, so that additional analyses of calcium data can be performed offline. ACTEV accelerator for the two versions of the UCLA Miniscope used here (MiniLFOV and V4, both sampling at a frame rate of ~20 Hz), as well as RTI software resources for the host PC, can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>, (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Real-time imaging protocol and system hardware.</title><p>(<bold>A</bold>) Sequence of steps for a real-time imaging and decoding session. (<bold>B</bold>) Miniscope connects to DAQ via coax cable, DAQ connects to Ultra96 via flywires, host PC connects to Ultra96 via Ethernet and to DAQ via USB 3.0; TTL pinouts from Ultra96 can drive closed-loop feedback stimulation from external devices.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig1-v2.tif"/></fig><sec id="s2-1"><title>Steps of a real-time imaging session</title><p>DeCalcion is designed for conducting real-time calcium imaging experiments in freely behaving animals. Although the animal species, targeted brain regions, and behavioral tasks will necessarily differ depending on the goals of a given experiment, real-time imaging sessions conducted with DeCalciOn will generally be carried out by performing a sequence of five common steps (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>Step 1 is to mount the miniscope on the subject’s head and collect 1000 frames of data to derive a reference alignment image for online motion correction. It is preferable for the subject to be in a quiescent state during this initial data collection period, to minimize brain motion while the reference image data are being stored. Imaging and behavioral data are transmitted via Ethernet from the Ultra96 for storage on the host PC. After 1000 frames of data have been stored to the host PC (~50 s after the start of acquisition at the ~20 Hz frame rates used here), imaging acquisition automatically pauses and the user is prompted to select a 128 × 128 pixel subregion of the image to use for motion correction. The host PC derives a reference image for motion correction by computing the mean image within the selected area over the 1000 frames of stored data. The reference image is then uploaded to the Ultra96 and data collection resumes with online motion correction enabled. Collecting the alignment data and uploading the reference frame takes ~3 min.</p><p>Step 2 is to collect an initial set of motion-corrected data for training the decoder. Online motion correction vectors are derived in real time for each frame and uploaded to the host PC for storage along with imaging and behavioral data. The time required to collect initial training data depends upon how much data is needed to train the decoder, which in turn depends upon the quality of calcium signals and what behaviors the decoder will be trained to predict. In performance tests presented here (see below), asymptotic accuracy for decoding a rat’s position on a linear track was achieved with as little as ~3 min of training data whereas asymptotic accuracy for classifying categorical behaviors during an instrumental task required 20–40 min of training data (~100 experimental trials). The size of the training dataset required for a given experiment can be determined in advance (prior to real-time imaging sessions) by storing offline imaging data from the brain ROI while subjects are engaged in the experimental task, then carrying out offline simulations of real-time decoder performance with training sets of varying size to determine the minimum amount of training data needed for accurate decoding.</p><p>Step 3 is to pause data acquisition so that the host PC can perform analyses to extract calcium traces from each frame of the initial training dataset. Calcium traces can be extracted by two methods: a contour-based method that requires 30–60 min of processing time on the host PC during Step 3 (for reasons explained below), or a contour-free method that requires no processing time during Step 3 because this method allows calcium traces to be extracted online and stored to the host PC concurrently with collection of the initial training dataset, thereby obviating the need to devote additional time or effort to calcium trace extraction during Step 3. It is preferable for the miniscope to remain attached to the animal’s head throughout Step 3 (and Step 4), because removing and remounting the miniscope can introduce shifts in the focus or alignment of the brain image and thereby degrade the accuracy of subsequent real-time decoding. Since the contour-based method of calcium trace extraction consumes considerable processing time to perform offline identification of cell contours, it also incurs the inconvenience of monitoring the subject during this extended time period while the miniscope remains attached to the head.</p><p>Step 4 is to use extracted calcium traces for training a decoder to predict behavior from vectors of calcium trace values. The time required for this depends upon the complexity of the user-defined decoder that is being trained (the decoder is trained on the host PC and runs in real time on the Ultra96 ARM core; see ‘Stage 4: population decoding’). Thus, a multilayer deep network decoder would take longer to train than a single-layer linear classifier of the kind used here for performance testing. Training consumed &lt;60 s for the linear classifiers used in performance tests presented below. Trained decoder weights (along with derived cell contour pixel masks if contour-based decoding was used) are uploaded from the host PC to the Ultra96 (a process which takes a maximum of 5 min to complete). Once the decoder weights have been uploaded, DeCalciOn is fully configured for real-time decoding of calcium traces.</p><p>Step 5 is to resume the behavior experiment while DeCalciOn performs real-time imaging and decoding of calcium traces to generate feedback output at TTL pins.</p></sec><sec id="s2-2"><title>Real-time image processing pipeline</title><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows a schematic diagram of DeCalciOn’s real-time image processing pipeline, which performs four sequential stages of image processing and calcium trace decoding: (1) online motion stabilization, (2) image enhancement via background removal, (3) calcium trace extraction, and (4) fluorescence vector decoding. As explained below, Stages 1–3 are performed by our custom ACTEV hardware accelerator (<xref ref-type="bibr" rid="bib7">Chen et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2022a</xref>) running in the programmable logic fabric of the FPGA, whereas Stage 4 is performed by a C++ program running under the FreeRTOS operating system on the MPSoC’s embedded ARM core.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Online imaging and control pipelines.</title><p>Serialized video data from the MiniLFOV are transmitted through a coaxial tether cable to the DAQ, where it is deserialized and transmitted via the flywire bus to ACTEV (Accelerator for Calcium Trace Extraction from Video) firmware programmed on the field-programmable gate array (FPGA) of the Ultra96. ACTEV crops the incoming image from its original size (1296 × 972 for the LFOV sensor shown here) down to a 512 × 512 subwindow (manually selected to contain the richest area of fluorescing neurons in each animal) before storing video frames to a BRAM buffer on the FPGA. Subsequent motion correction and calcium trace extraction steps are performed by the FPGA fabric as described in the main text. Extracted calcium traces are stored to DRAM from which they can be read as inputs to a decoder algorithm running on the Ultra96 ARM core. Decoder output is fed to a logic mapper that can trigger TTL output signals from the Ultra96, which in turn can control external devices for generating closed-loop feedback.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig2-v2.tif"/></fig><sec id="s2-2-1"><title>Stage 1: motion stabilization</title><p>Incoming frames arriving to the Ultra96 from the Miniscope DAQ are cropped to a 512 × 512 subregion, manually selected to contain the richest area of fluorescing neurons in each animal. Pixel data from the cropped subregion are stored to a BRAM buffer on the Ultra96 where it can be accessed by ACTEV accelerator running in the FPGA logic fabric. To correct for translational movement of brain tissue, a 128 × 128 pixel area with distinct anatomical features is manually selected during Stage 1 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) from within the 512 × 512 imaging subwindow to serve as a motion stabilization window. ACTEV’s image stabilization algorithm (<xref ref-type="bibr" rid="bib7">Chen et al., 2019</xref>) rigidly corrects for translational movement of brain tissue by convolving the 128 × 128 stabilization window in each frame with a 17 × 17 contrast filter kernel, and then applying a fast 2D FFT/IFFT-based algorithm to correlate the stabilization window contents with a stored reference template (derived by averaging 1000 contrast-filtered frames from the beginning of each experimental session) to obtain a 2D motion displacement vector for the current frame. <xref ref-type="video" rid="video1">Video 1</xref> demonstrates online performance of ACTEV’s real-time motion stabilization algorithm.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-78344-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Real-time motion correction.</title><p>The left and right windows show sensor video data before and after motion correction, respectively. The line graphs at bottom show <italic>x</italic> (yellow) and <italic>y</italic> (green) components of the image displacement between frames before (left) and after (right) motion correction.</p></caption></media></sec><sec id="s2-2-2"><title>Stage 2: background removal</title><p>After motion stabilization, ACTEV removes background fluorescence from the 512 × 512 image by performing a sequence of three enhancement operations (<xref ref-type="bibr" rid="bib8">Chen et al., 2020</xref>): smoothing via convolution with a 3 × 3 mean filtering kernel, estimating the frame background via erosion and dilation with a 19 × 19 structuring element (<xref ref-type="bibr" rid="bib24">Lu et al., 2018</xref>), and subtracting the estimated background from the smoothed image. These operations produce an enhanced image in which fluorescing neurons stand out in contrast against the background (see <xref ref-type="video" rid="video2">Video 2</xref>; ‘Enhanced image’ in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Calcium traces are then extracted (by methods described under ‘Stage 3’) from the motion-corrected (Stage 1) and background-removed (Stage 2) image.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-78344-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Real-time interface (RTI) view of contour-based calcium trace extraction.</title><p>The online image display (left window) shows the motion corrected and enhanced (i.e., background subtracted) sensor image data as it arrives in real time from the Ultra96 board. The right window shows a scrolling display of 63 selected calcium traces from regions outlined by colored borders in the image display window. These traces are derived on the Ultra96 by summing fluorescence within their respective contour regions, and the resulting trace values are transmitted (along with sensor image data) via ethernet to the host PC for display in the RTI window. For demonstration purposes, traces shown in the window on the right are normalized within the ranges of their own individual minimum and maximum values.</p></caption></media></sec><sec id="s2-2-3"><title>Stage 3: trace extraction</title><p>To derive calcium trace values, the enhanced image is filtered through a library of up to 1024 binary pixel masks, each of which defines a unique ROI for extracting a calcium trace. Each mask is defined as a selected subset of pixels within a 25 × 25 square region that can be centered anywhere in the 512 × 512 imaging subwindow (<xref ref-type="fig" rid="fig2">Figure 2</xref>, upper left). In performance test results presented below, decoding accuracy was compared for two different methods of selecting ROI pixels: <italic>contour-based</italic> versus <italic>contour-free</italic>. For contour-based ROI selection, the CaImAn (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>) algorithm was used during the intermission period to identify contours of individual neurons in the training dataset, and pixel mask ROIs were then aligned to the identified regions where neurons were located. For contour-free ROI selection, the 512 × 512 imaging subwindow pixel was uniformly covered with a 32 × 32 array of square tiles, each measuring 16 × 16 pixels, which served as ROIs for calcium trace extraction. This contour-free approach did not align ROIs with individual neurons, so each extracted calcium trace could detect fluorescence originating from more than one neuron. It is shown below that this method of contour-free ROI selection can improve the computational efficiency of online decoding at little or no cost to prediction accuracy. After ROIs had been defined by the contour-based or contour-free method, calcium traces were derived by summing pixel intensities within each ROI. Demixing was not performed during real-time calcium trace extraction; it is shown below that similar prediction accuracy can be achieved with offline (demixed) versus online (non-demixed) decoding using identical contour-based ROIs.</p></sec><sec id="s2-2-4"><title>Stage 4: population vector decoding</title><p>The vector of calcium trace values extracted from each frame is stored to the Ultra96 DRAM, from which trace values are then read into the MPSoC’s ARM core. The ARM core executes user-defined decoding algorithms running as a C++ program under the FreeRTOS operating system (which affords highly consistent execution times and therefore minimizes frame-to-frame variability in decoding latency). In performance tests reported below, we used simple linear classifier algorithms to decode calcium population vector activity of neurons imaged in CA1 and OFC of freely behaving rats. However, the C++ programmable ARM core is capable of implementing a wide range of different decoder architectures, including deep convolutional neural networks and long short-term memory networks (<xref ref-type="bibr" rid="bib10">Chen et al., 2022b</xref>).</p></sec></sec><sec id="s2-3"><title>Performance testing</title><p>Results presented below were obtained using two different versions of the UCLA Miniscope: CA1 position decoding tests were conducted using the Large Field-of-View (<xref ref-type="bibr" rid="bib3">Blair et al., 2021</xref>) (MiniLFOV) version featuring 1296 × 972 pixel resolution sampled at 22.8 fps, whereas OFC instrumental behavior decoding tests were conducted using the V4 version featuring 600 × 600 pixel resolution sampled at 19.8 fps. For performance tests, we used a virtual sensor (see Methods) that was capable of feeding stored image data to the ACTEV accelerator at exactly the same frame rate (19.8 fps for V4, 22.8 fps for MiniLFOV) as raw video data arriving in real time. This allowed different real-time image processing algorithm implementations (e.g., contour-based vs. contour-free decoding) to be compared and benchmarked on the same stored datasets. Calcium traces obtained with the virtual sensor were verified to be identical with those obtained during real-time imaging sessions. ACTEV accelerators for virtual sensor testing and for real-time (non-virtual) imaging with both versions of the UCLA Miniscope (MiniLFOV and V4) can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>, (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>).</p><sec id="s2-3-1"><title>Position decoding from CA1 cells</title><p>For the first set of performance tests, the MiniLFOV scope was used to image neurons in the hippocampal CA1 region while Long-Evans rats (<italic>n</italic> = 12) ran back and forth on a 250-cm linear track (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Each linear track session was ~7 min in duration, yielding 8K–9K frames of calcium trace data per rat. CA1 pyramidal neurons are known to behave as ‘place cells’ that fire selectively when an animal traverses specific locations in space (<xref ref-type="bibr" rid="bib26">O’Keefe and Dostrovsky, 1971</xref>), so a rodent’s position can be reliably decoded from CA1 population activity (<xref ref-type="bibr" rid="bib33">Ziv et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Cai et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Wilson and McNaughton, 1993</xref>; <xref ref-type="bibr" rid="bib22">Kinsky et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Center for Brains Minds Machines, 2017</xref>). As expected, many of the neurons we imaged in rat CA1 were spatially tuned on the linear track (see example tuning curves in <xref ref-type="fig" rid="fig3">Figure 3C, D</xref>; summary data in panels D, E of Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>). For real-time decoding, we did not define a decision boundary for classifying CA1 traces as coming from ‘place’ versus ‘non-place’ cells; calcium traces were decoded from all ROIs regardless of their spatial tuning properties.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Position decoding from CA1 cells.</title><p>(<bold>A</bold>) The CA1 layer of the hippocampus was imaged while rats ran laps on a 250-cm linear track; for position decoding, the rat’s path was circularized and subdivided into 24 position bins, each ~20 cm wide. (<bold>B</bold>) <italic>N</italic> linear classifer inputs (one per calcium trace) were mapped to 12 outputs using a Gray code representing the 24 track positions (open/filled circles show units with target outputs of −1/+1 at each position). (<bold>C</bold>) Regions of interest (ROIs) from which contour-based (CB, left), contour-free (CF, middle), and expanded contour-based (CB+) calcium traces were extracted in Rat #6; ROI shading intensity shows the similarity score, <italic>S</italic>, for traces extracted from the ROI. (<bold>D</bold>) Tuning curve heatmaps from training and testing epochs for CB (left), CF (middle), and CB+ traces extracted from Rat #6; rows are sorted by location of peak activity in the testing epoch. (<bold>E</bold>) Decoding performance did not differ for CB versus offline (Off) traces (left graphs), but was significantly better for sessions with larger numbers of calcium traces (right graphs). (<bold>F</bold>) Decoding performance averaged over track positions within each session (left graphs) and over sessions at each circularized track position (right graphs). (<bold>G</bold>) Decoding performance as a function of training set size; asterisks mark significant improvement with the addition of 500 more frames to the training set. For all panels: *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Decoding position from calcium traces on the linear track.</title><p>(<bold>A</bold>) For contour-free (left) and contour-based (right) trace extraction methods, hit-rate accuracy (±<italic>D</italic>, <italic>y</italic>-axis) of position decoding was higher when classifier was trained and tested on raw calcium trace values than inferred spike events; ±<italic>D</italic> is the percentage of predicted position bins falling within a radius of ±<italic>D</italic> of true position bin. (<bold>B</bold>) Performance comparison between linear classifier (LC, same as <xref ref-type="fig" rid="fig3">Figure 3F</xref>) versus naive Bayes (NB) decoder; NB performs poorly and was thus not implemented for real-time decoding of calcium traces. (<bold>C</bold>) Decoder predictions are far more accurate after training on 3K frames of calcium traces aligned with position data (‘align’) than circularly shifted against position data (‘shift’). (<bold>D</bold>) Graphs plot each rat’s mean running speed (top graph), mean acceleration (middle graph), and total number of image frames during which the rat was sitting still (running speed &lt;5 cm/s) in each of the 24 position bins (<italic>x</italic>-axis for all three graphs); black lines show averages over all rats in each graph. (<bold>E</bold>) Colored lines plot cumulative distributions of tuning curve similarity scores (<italic>S</italic>) for offline (Off), contour-based (CB), contour-free (CF), and expanded contour-based (CB+) calcium traces in each rat. The tuning curve similarity score (<italic>S</italic>) was computed for each trace as <italic>S</italic> = −log<sub>10</sub>(<italic>P</italic>) × sign(<italic>R</italic>), where <italic>R</italic> is the correlation between the trace’s tuning curves from the training versus testing epochs, and <italic>P</italic> is the significance level for <italic>R</italic>; the p &lt; 0.01 level for the similarity score is <italic>S</italic> &gt; 2 (dashed lines in each graph) since log<sub>10</sub>(0.01) = 2. Bar graph (lower right) shows mean <italic>S</italic> scores for individual rats (black lines) and averaged over rats (colored bars). The mean value of <italic>S</italic> over sets of traces imaged in each rat was significantly greater than 2 for every rat, indicating that on average calcium traces in CA1 exhibited stable spatial tuning between the training and testing epochs, as required for accurate decoding. A one-way repeated measures analysis of variance (ANOVA) found that the mean <italic>S</italic> value differed significantly by trace type (<italic>F</italic><sub>3,44</sub> = 16.2, p &lt; 0.00001); colored asterisks over each bar show significance of an uncorrected paired <italic>t</italic>-test comparison between that bar versus bar matching the color of the asterisk. (<bold>F</bold>) Colored lines plot distributions of calcium trace activity ranges (CTARs) for all traces from each rat; CTAR is defined as the total distance (in cm) over which a calcium trace’s tuning curve exceeds 67% of the tuning curve’s peak value. Bar graph (lower right) shows mean CTAR per rat (black lines) and averaged over rats (colored bars). A one-way repeated measures ANOVA found that the mean CTAR value differed significantly by trace type (<italic>F</italic><sub>3,44</sub> = 9.1, p &lt; 0.001); colored asterisks over each bar show significance of an uncorrected paired <italic>t</italic>-test comparison between that bar versus bar matching the color of the asterisk. *p &lt; 0.05; **p &lt; 0.01; ***p &lt; 0.001; ****p &lt; 0.0001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig3-figsupp1-v2.tif"/></fig></fig-group><p>When hippocampal activity is analyzed or decoded offline (rather than online), it is common practice to perform <italic>speed filtering</italic> that omits time periods when the rat is sitting still. This is done because during stillness, the hippocampus enters a characteristic ‘sharp wave’ EEG state during which place cell activity is less reliably tuned for the animal’s current location (<xref ref-type="bibr" rid="bib4">Buzsáki, 2015</xref>). Here, speed filtering was not implemented during online decoding of CA1 fluorescence because the linear classifier received only real-time calcium trace data and not position tracking data that would be needed for speed filtering. As shown below, DeCalciOn achieved accurate decoding of the rat’s position from calcium traces without speed filtering.</p><p>ACTEV can utilize a real-time spike inference engine to convert each frame’s calcium trace value into an inferred spike count per frame. However, when decoding information from spike events (e.g., from single-unit recordings of neural population activity), it is standard practice to perform a temporal integration step by counting spikes in time bins of a specified width. In calcium imaging experiments, the GCamp molecule itself serves as a temporal integrator of spike activity. The GCamp7s indicator used in our CA1 imaging experiments has a decay constant of several hundred ms (<xref ref-type="bibr" rid="bib12">Dana et al., 2019</xref>), which is similar to the width of time bins typically used for counting spikes when decoding a rat’s position from single-unit recordings of place cells (<xref ref-type="bibr" rid="bib33">Ziv et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Cai et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Wilson and McNaughton, 1993</xref>; <xref ref-type="bibr" rid="bib22">Kinsky et al., 2018</xref>; <xref ref-type="bibr" rid="bib6">Center for Brains Minds Machines, 2017</xref>). In accordance with prior studies of position decoding from CA1 calcium fluorescence (<xref ref-type="bibr" rid="bib30">Tu et al., 2020</xref>), we found (see panel A of Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>) that the linear classifier was more accurate at decoding the rat’s position from raw calcium trace amplitudes (which chemically time-integrated spikes on a time scale of the several hundred milliseconds, corresponding to the GCaMP7s decay rate) than from inferred spike counts (which computationally time-integrated spike counts on a much shorter time scale of ~50 ms, corresponding to the miniscope’s frame interval). Hence, we decoded the rat’s position from raw calcium traces rather than inferred spike counts in performance tests reported below.</p><p>The linear classifier was trained on calcium trace and position data from the first half of each session, then tested on data from the second half of the session. The classifier’s output vector consisted of 12 units that used a Gray coding scheme (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) to represent 24 position bins (each ~20 cm wide) along a circularized representation of the track. After the linear classifier had been trained on data from the first portion of the session, learned weights were uploaded from the host PC to the Ultra96 and image data from the second half of the session (test data) was fed through the virtual sensor to mimic real-time decoding. To verify that decoders predicted position with far better than chance accuracy, error performance was compared for decoders trained on calcium traces that were aligned versus misaligned with position tracking data (panel C, Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>). Position decoding performance was compared for different methods of extracting online fluorescence traces including <italic>contour-based</italic> (CB) versus <italic>contour-free</italic> (CF) methods as explained below.</p><sec id="s2-3-1-1"><title>CB trace decoding</title><p>CB trace extraction derived calcium traces by summing fluorescence within pixel masks defining ROIs overlapping with identified neurons (<xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="video" rid="video2">Video 2</xref>). Neurons were identified during the intermission period by using the CaImAn pipeline (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>) to perform constrained non-negative matrix factorization (CNMF) (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Pnevmatikakis et al., 2016</xref>) on the training dataset and extract ROIs for demixed calcium traces. Running CaImAn on 4K–5K frames of training data required 30–60 min of computing time on the host PC, making this the slowest step performed during the intermission period (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). After contour ROIs had been identified, an additional ~5 min of computing time was required to feed all of the training data frames through a simulator to reconstruct online calcium traces that would have been extracted in real time during the first half of the session using the extracted contour masks. The simulated online traces were then aligned frame-by-frame with position data and used to train a linear classifier for decoding the rat’s position on the linear track from calcium activity; training the decoder required a maximum ~60 s of computing time.</p><p>Accurate position decoding requires that calcium trace inputs to the decoder must retain stable spatial tuning across the training and testing epochs of the session. To verify that this was the case, spatial tuning curves were derived for CB calcium traces during the first (training) versus second (testing) half of each session (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, left). Analyses of spatial selectivity and within-session spatial tuning stability (panels D, E in Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>) indicated that large proportions of CB traces in CA1 exhibited stable spatial tuning between the training and testing epochs, as required for accurate decoding. The real-time decoder was trained and tested on CB traces from all ROIs identified by CaImAn in each session; we did not restrict the decoder’s training set to a subset of traces that met a spatial selectivity criterion.</p><p>Two measures were used to quantify accuracy of position predictions decoded from calcium traces in each frame: <italic>Distance error</italic> <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was the absolute distance (in cm) between the true versus decoded position in frame <inline-formula><mml:math id="inf2"><mml:mi>t</mml:mi></mml:math></inline-formula>; the mean distance error over frames in the testing epoch shall be denoted <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> . <italic>Hit rate</italic> <inline-formula><mml:math id="inf4"><mml:mi>ρ</mml:mi></mml:math></inline-formula> was the percentage of all frames in the test epoch for which the predicted position was within ±30 cm of the true position. <xref ref-type="fig" rid="fig3">Figure 3E</xref> shows that when the decoder was trained and tested with DeCalciOn’s online (non-demixed) CB traces, there was no significant difference in mean decoding accuracy (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : <italic>t</italic><sub>11</sub> = −1.32, p = 0.21; <inline-formula><mml:math id="inf6"><mml:mi>ρ</mml:mi></mml:math></inline-formula>: <italic>t</italic><sub>11</sub> = 1.32, p = 0.21) from when the same decoder was trained on demixed offline (‘Off’) traces extracted by CaImAn from the same set of ROIs. Hence, decoding from online (real-time) CB traces versus demixed offline traces yielded similarly accurate position predictions. Unsurprisingly, the accuracy of decoding position from calcium traces was proportional to the population size of detected neurons (<inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : <italic>R</italic> = −0.6, p = 0.0375; <inline-formula><mml:math id="inf8"><mml:mi>ρ</mml:mi></mml:math></inline-formula>: <italic>R</italic><sub>1</sub> = 0.59, p = 0.0426; <xref ref-type="fig" rid="fig4">Figure 4E</xref>, right). Hence, the accuracy of position predictions improves with the number of calcium traces from which predictions are generated.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Improvement of place cell decoding by online motion correction.</title><p>(<bold>A</bold>) Session-averaged performance of decoding from CB and CF traces is compared with (mc+) versus without (mc−) online motion correction; filled symbols ‘●’ and ‘▲’ mark data from example sessions shown in panel ‘C’. (<bold>B</bold>) Scatter plots compare motion artifact scores with (Wmc+) versus without (Wmc−) online motion correction (see main text for explanation of shaded regions); insets show that points lie significantly nearer to the vertical (<italic>D</italic><sub><italic>V</italic></sub>) than horizontal (<italic>D</italic><sub><italic>H</italic></sub>) midline indicating a significant benefit of online motion correction. (<bold>C</bold>) Displacement of sensor image against reference alignment plotted over all frames in the session (<italic>x</italic>-axis) for Rat #1 (‘●’) which accumulated a large (~10 pixels) shift alignment error over the session, and Rat #12 (‘▲’) which exhibited transient jitter error but did not accumulate a large shift error; bottom graphs show signed horizontal (Δ<italic>x</italic>) and vertical (Δ<italic>y</italic>) displacement from the reference alignment, middle graphs show distance (<italic>d</italic>) from reference alignment. And top graphs show absolute value of time derivative of distance (|Δ<italic>d</italic>|) from the reference alignment. *p&lt;.05; **p&lt;.01.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig4-v2.tif"/></fig></sec><sec id="s2-3-1-2"><title>CF decoding</title><p>In contrast with CB extraction, CF extraction derived calcium traces by summing fluorescence within pixel masks that did not overlap with identified neurons. Instead, ROIs were obtained by partitioning the 512 × 512 image into a 32 × 32 sheet of tiles, each tile measuring 16 × 16 pixels in area (<xref ref-type="video" rid="video3">Video 3</xref>); 124 tiles bordering the frame (shaded light blue in middle plot of <xref ref-type="fig" rid="fig3">Figure 3C</xref>) were excluded to eliminate edge contamination by motion artifacts. The CF extraction method always yielded a total of 900 calcium traces per rat (one for each square tile). By contrast, the CB method (see above) yielded a variable number of traces depending upon how many neuron ROIs were detected in the training dataset by CaImAn.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-78344-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Real-time decoding of contour-free calcium traces.</title><p>The online image display (left window) shows the motion corrected and enhanced mosaic of contour-free pixel mask tiles. The right window shows a scrolling heatmap display of 103 (out of the total 900) calcium traces with the highest tuning curve similarity scores, <italic>S</italic> (see main text). The calcium trace rows in the heatmap are sorted by the peak activity location of each trace, so that calcium activity can be seen to propagate through the population as the rat runs laps on the linear track. The line graph at the top shows the rat’s true position (blue line) together with its predicted position (orange line) decoded in real time by the linear classifier. For demonstration purposes, trace intensities shown in the scrolling heatmap are normalized within the ranges of their own individual minimum and maximum values.</p></caption></media><p>Unlike CB trace extraction, CF extraction does not require any computing time for contour identification or trace simulation during the intermission period, because the decoder is trained directly on 900 traces of CF data stored to memory during the training epoch; this training consumed &lt;60 s of computing time on the host PC for 5K frames of training data. Hence, CF trace extraction required much less computing time during the intermission period than the CB trace extraction (which requires 30–60 min of computing time during intermission to identify trace ROIs, as explained above). Since CF calcium traces were derived from square tiles that were not aligned with locations of individual neurons in the sensor image, each CF trace could be modulated by fluorescence from multiple neurons within the ROI of its assigned tile. CF traces are thus analogous to multiunit spike recordings in neurophysiology that combine signals from multiple neurons. Similarly, CB traces can be regarded as analogous with single-unit recordings, since they are better at isolating signals from single neurons.</p><p>Like CB traces, CF traces in CA1 also exhibited stable spatially selective tuning (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, middle). However, individual CF traces showed less within-session stability and fired over wider regions of the track (panels D, E in Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>) than individual CB traces, as should be expected if CF traces contain signals from multiple place cells and CB traces isolate signals from individual place cells. But at the population level, decoding the rat’s position from CF traces was as or more accurate than decoding from CB traces. As in the case of CB traces, the real-time decoder was trained and tested on all CF traces (<italic>N</italic> = 900 per rat in the tile mosaic) regardless of their spatial selectivity. When the accuracy of CF versus CB decoding was compared by performing paired <italic>t</italic>-tests on the <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mi>ρ</mml:mi></mml:math></inline-formula> measures defined above (<xref ref-type="fig" rid="fig3">Figure 3F</xref>, left), CF decoding was more accurate than CB decoding (<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : <italic>t</italic><sub>11</sub> = −3.43, p = 0.0056; <inline-formula><mml:math id="inf12"><mml:mi>ρ</mml:mi></mml:math></inline-formula>: <italic>t</italic><sub>11</sub> = 4.6, p = 7.49e<sup>−4</sup>). Analogous results have been reported in electrophysiology, where it has been found that decoding from multiunit spike trains can be more accurate than decoding from sorted spikes sourced to single units (<xref ref-type="bibr" rid="bib14">Deng et al., 2015</xref>).</p><p>Since decoding accuracy for CB traces was proportional to population size (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, right), it is possible that the reason CF traces provided better decoding accuracy was because they were greater in number (in all rats, there were more CF than CB traces). To test this, CaImAn’s sensitivity parameters were adjusted to detect more CB traces in each rat, yielding an expanded population (CB+) of CB traces that was larger than the original population of CB traces in each rat (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, right). <xref ref-type="fig" rid="fig4">Figure 4E</xref> shows that CB+ decoding was more accurate than CB decoding (<inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : <italic>t</italic><sub>11</sub> = −3.09, p = 0.0013; <inline-formula><mml:math id="inf14"><mml:mi>ρ</mml:mi></mml:math></inline-formula>: <italic>t</italic><sub>11</sub> = 3.7, p = 0.0035), but did not differ significantly in accuracy from CF decoding (<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> : <italic>t</italic><sub>11</sub> = −1.08, p = 0.3; <inline-formula><mml:math id="inf16"><mml:mi>ρ</mml:mi></mml:math></inline-formula>: <italic>t</italic><sub>11</sub> = 0.83, p = 0.42). Hence, CF and CB+ traces yielded similar decoding accuracy, suggesting that the number of calcium trace predictors was a strong determinant of decoding accuracy. Deriving CB+ traces required about the same amount of computing time (30–60 min) during the intermission period as CB traces to identify contours and simulate online traces (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Therefore, despite the similar accuracy of CF and CB+ traces, it was much more efficient to use CF traces since they did not require extra computing time to identify ROIs during the intermission period.</p></sec><sec id="s2-3-1-3"><title>Duration of the training epoch</title><p>An initial dataset must be collected at the beginning of each session to train the decoder before real-time imaging begins. The training epoch should be long enough to obtain sufficient data for training an accurate decoder, but short enough to leave sufficient remaining time in the session for real-time imaging. To analyze how position decoding accuracy on the linear track depended upon the duration of the training epoch, we varied the length of the training epoch in increments of 500 frames (see Methods). As expected, position decoding accuracy (quantified by <inline-formula><mml:math id="inf17"><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mi>ρ</mml:mi></mml:math></inline-formula>) improved with the duration of the training epoch (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). On average, asymptotic accuracy occurred when the training epoch reached ~3000 frames, beyond which increasing the duration of the training epoch yielded diminishing benefits for decoding accuracy.</p></sec><sec id="s2-3-1-4"><title>Online motion correction</title><p>Online motion stabilization (Stage 1) is the most computationally expensive stage in the real-time image processing pipeline, consuming more MPSoC hardware resources than any other stage (<xref ref-type="bibr" rid="bib9">Chen et al., 2022a</xref>). To quantify the benefits of these high resource costs, we compared decoder performance with (mc+) and without (mc−) motion correction using CB or CF methods for CA1 calcium trace extraction (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For these analyses, the decoder was trained on mc+ or mc− traces from the first 3000 frames in each session, and then tested on the corresponding mc+ or mc− traces from the remainder of the session.</p><p>Effects of motion correction upon session-averaged distance error and hit rate were computed using the formulas Δ <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> = <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><sub>mc−mc+</sub> and <inline-formula><mml:math id="inf21"><mml:mi>Δ</mml:mi><mml:mi>ρ</mml:mi></mml:math></inline-formula> = <inline-formula><mml:math id="inf22"><mml:mi>ρ</mml:mi></mml:math></inline-formula><sub>mc+</sub><inline-formula><mml:math id="inf23"><mml:mo>-</mml:mo><mml:mi>ρ</mml:mi></mml:math></inline-formula><sub>mc−</sub>, respectively (note that both formulas yield a positive result for cases where motion correction improves accuracy, and a negative result for cases where motion correction degrades accuracy). Paired <italic>t</italic>-tests found no difference between mc+ versus mc− conditions for Δ <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf25"><mml:mi>Δ</mml:mi><mml:mi>ρ</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), but session-averaged decoding accuracy may not be the most appropriate measure for motion correction’s benefits because image stabilization is only beneficial when the image is translationally displaced from its reference alignment with the sensor, and this only occurs during a subset of frames within a session. To further assess the benefits of online motion correction, we analyzed how image stabilization reduced two distinct types of alignment error that can arise from translational displacement of the image on the sensor: <italic>shift error</italic> and <italic>jitter error</italic>.</p><sec id="s2-3-1-4-1"><title>Shift error</title><p>Shift error occurs when the image permanently changes its steady-state alignment with the miniscope sensor (either suddenly or gradually) during the course of a session. For example, this can occur if the miniscope shifts its seating in the baseplate during a session. Significant shift error occurred in only 1 of the 12 linear track sessions analyzed for our performance tests (Rat #1 in <xref ref-type="fig" rid="fig4">Figure 4C</xref>); this session is marked by filled circles in panels of <xref ref-type="fig" rid="fig4">Figure 4</xref>. For this one session (but no others), benefits of motion correction were apparent even in the session-averaged data (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Hence, online motion stabilization was effective at correcting shift errors in the one session where they occurred; but given that shift errors occurred so rarely, it might be argued that correcting these errors does not adequately justify resource costs of online motion stabilization, especially if shift errors can be effectively prevented by other means such as rigidly attaching the miniscope to the baseplate.</p></sec><sec id="s2-3-1-4-2"><title>Jitter error</title><p>Jitter error occurs when the animal’s inertial head acceleration causes transient motion of the brain inside the skull cavity, resulting in phasic translation of the brain image across the sensor. By definition, jitter error is temporary and lasts only until the brain image returns to its original alignment with the sensor after inertial motion stops. Jitter error can occur when the animal jerks or angles its head in such a way that the brain moves within the skull. Motion of the brain inside the skull is a natural phenomenon during free behavior, and for this reason, jitter error cannot easily be prevented by means other than online motion stabilization. To analyze whether online stabilization was effective at correcting for jitter error, we used the distance formula to compute displacement of each frame’s non-motion-corrected image against the reference alignment: <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mi>Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>Δ</mml:mi><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:math></inline-formula> . Two Pearson correlation coefficients (<italic>R</italic><sub>mc+</sub> and <italic>R</italic><sub>mc−</sub>) were derived for each session by correlating the distance error vectors (<inline-formula><mml:math id="inf27"><mml:mi>λ</mml:mi></mml:math></inline-formula><sub>mc+</sub> and <inline-formula><mml:math id="inf28"><mml:mi>λ</mml:mi></mml:math></inline-formula><sub>mc−</sub>) with the displacement vector <bold>d</bold> over all frames in the testing epoch for the session. Positive <italic>R</italic> values indicated that distance error was larger during frames that were misaligned on the sensor, as would be expected if decoding accuracy were impaired when jitter artifact corrupted the values of calcium traces. Conversely, negative <italic>R</italic> values indicated that decoding error was <italic>smaller</italic> during frames that were misaligned on the sensor, which might occur if jitter error was correlated with the rat’s position on the track in such a way that the decoder artifactually learned to predict the rat’s location from image displacement rather than from neural calcium activity. <italic>R</italic> values from each session were converted into motion artifact error scores using the formula <italic>Ω</italic> = −log<sub>10</sub> (<italic>P</italic>) × sign(<italic>R</italic>), where <italic>R</italic> is the correlation between distance error and image displacement and <italic>P</italic> is the significance level for <italic>R</italic>; the p &lt; 0.01 significance level for the similarity score is <italic>S</italic> &gt; 2, since log<sub>10</sub>(0.01) = 2.</p><p><xref ref-type="fig" rid="fig4">Figure 4B</xref> shows scatterplots of <italic>Ω</italic><sub>mc+</sub> versus <italic>Ω</italic><sub>mc−</sub> derived from CB (top) or CF (bottom) traces for all 12 linear track sessions in the performance testing analysis. Sessions falling within the central gray square region (3/12 for CB traces, 6/12 for CF traces) had non-significant motion artifact error scores before as well as after online motion correction, indicating that for these sessions, there was little benefit from online motion stabilization. Sessions falling within the upper yellow region (6/12 for CB traces, 2/12 for CF traces) had significant positive motion artifact error scores in the mc− condition that resolved in the mc+ condition, indicating that in these sessions, online motion correction successfully prevented degradation of decoding accuracy by motion artifact. Sessions falling within the lower blue region (1/12 for CB traces, 0/12 for CF traces) had significant ‘negative’ motion artifact error scores in the mc− but not mc+ condition, suggesting that without motion stabilization, the decoder artifactually learned to predict position from location-dependent motion artifacts rather than from neural calcium activity; this ersatz decoding was reduced by online motion correction. Further examples of decoders learning to predict behavior from image motion are shown below (see ‘Decoding instrumental behavior from OFC neurons’).</p><p>In <xref ref-type="fig" rid="fig4">Figure 4B</xref> and a small number of plotted points fell outside any of the shaded regions indicating that significant motion artifact scores occurred in both the mc− and mc+ conditions. However, the mean distance of points from the vertical midline (<italic>D</italic><sub><italic>V</italic></sub>) was significantly smaller than their mean distance (<italic>D</italic><sub><italic>H</italic></sub>) from the horizontal midline in scatterplots of both CB (<italic>t</italic><sub>11</sub> = 2.63, p = 0.0232) and CF (<italic>t</italic><sub>11</sub> = 2.92, p = 0.0139) traces. This indicates that correlations (positive and negative) between distance error and motion artifact were significantly reduced by online motion correction of both CB and CF traces. Hence, online motion stabilization benefitted decoding accuracy by improving the accuracy of online predictions for frames with motion artifact.</p></sec></sec></sec></sec><sec id="s2-4"><title>Decoding instrumental behavior from OFC neurons</title><p>Another set of performance tests was carried out using the V4 UCLA Miniscope to image neurons in OFC while Long-Evans rats (<italic>n</italic> = 2) performed a 2-choice instrumental touchscreen task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Each trial of the task began with the appearance of a central fixation stimulus (circle) on the screen, which the rat was required to touch. After touching the center stimulus, two visually distinct choice stimuli (A and B) appeared on the left and right sides of the screen. Assignment of stimuli A/B to the left/right side of the screen was randomized on each trial, but the correct response was always to touch the stimulus on one side (left for JL66, right for JL63) regardless of its identity. A correct response immediately triggered audible release of a food pellet into a magazine located behind the rat; an incorrect response triggered a 5-s timeout period. After each rewarded or non-rewarded outcome, a 10-s intertrial interval was enforced before initiating the next trial. <xref ref-type="fig" rid="fig5">Figure 5B</xref> (top) summarizes behavioral performance during 45-m imaging sessions (one for each rat) conducted after training on the discrimination task. In both sessions, rats performed &gt;150 trials with &gt;95% correct choices.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Decoding instrumental behavior from orbitofrontal cortex (OFC) calcium activity.</title><p>(<bold>A</bold>) OFC neurons were imaged while rats performed a 2-choice touchscreen task (see main text). (<bold>B</bold>) Number of trials (top) and number of neurons identified by constrained non-negative matrix factorization (CNMF) (bottom) during the two analyzed sessions (JL66 and JL63). (<bold>C</bold>) Jitter error |Δ<italic>d</italic>| (gray traces, top) and real-time predictions of behavior labels (reward retrieval, correct choice, trial initiation) are shown for 90-s periods from the testing epoch of each rat (JL66 and JL63) after the binary tree decoder had been trained on 100 trials of offline (Off), contour-based (CB), or contour-free (CF) calcium traces; black traces show ground truth (GT) behavior labels, blue traces show predictions derived from motion vectors only (Mot). Sensitivity (shaded bars) and <italic>F</italic>-scores (horizontal lines) are shown for binary classification of reward retrieval (<bold>D</bold>), correct choice (<bold>E</bold>), and trial initiation (<bold>F</bold>) events using decoders trained on Off, CB, CF, or Mot predictors; for online CB and CF predictors, classifer performance is plotted separately for traces derived with versus without motion correction (mc+ vs. mc−), respectively. Violin plot inside each bar shows distribution of <italic>F</italic>-scores obtained from 1000 shuffles of event labels; asterisks over actual <italic>F</italic>-scores indicate significant difference from the shuffle distribution (***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig5-v2.tif"/></fig><sec id="s2-4-1"><title>Predicting behavior from calcium traces</title><p>Calcium traces were extracted from image data using each of the three methods described above: online contour-based (CB), online contour-free (CF), or offline contour-based (Off) extraction. <xref ref-type="fig" rid="fig5">Figure 5B</xref> (bottom left) shows that the number of orbitofrontal neurons (and thus the number of CB/Off traces) identified by CaImAn was much higher in one rat (JL66, <italic>N</italic> = 138 cells) than the other (JL63, <italic>N</italic> = 17 cells), whereas the number of CF traces was <italic>N</italic> = 900 in both rats (for reasons explained above).</p><p>To decode behavior from calcium traces, image frames were classified into five mutually exclusive categories: (1) <italic>pre-trial</italic> frames occurring within a 1-s window prior to each touch of the central fixation stimulus, (2) <italic>correct choice</italic> and (3) <italic>incorrect choice</italic> frames occurring between touches of the central fixation stimulus and touches of the correct or incorrect choice stimulus, respectively, (4) <italic>reward retrieval</italic> frames occurring within a 2-s window after reward magazine entry, and (5) <italic>intertrial</italic> frames not belonging to any of the previous four categories. A binary tree classifier was trained to predict behavior category labels from calcium trace vectors extracted using the CB, CF, or Off methods described above. As a control, the classifier was also trained to predict the behavior category from image motion displacement (Mot) rather than calcium traces (see Methods). For each session, the classifier was trained on data from the first 100 trials and then tested on the subsequent 50 trials. During testing, the predicted behavior label for each frame was taken to be the label that had been selected by the decoder most frequently within a five-frame window that included the current and previous four frames. Performance was assessed using two measures: <italic>sensitivity</italic>, the percentage of behavior events during the test epoch that were correctly predicted, and <italic>F</italic>-score, a standard measure of binary classification accuracy that weighs sensitivity against precision of the classifier (see Methods). <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows 90 s of example data from the testing epoch of each session (J66 and J63). In the figure, traces of ground truth behavior categories are aligned to real-time output from decoders trained on one of the four predictors (Off, CB, CF, and Mot).</p><p>Performance of the real-time decoder was assessed for predicting three of the five behavior categories: reward retrieval, correct choice, and trial initiation. Decoding performance was not analyzed for the ‘incorrect choice’ behavior category because very few incorrect choice trials occurred during the analyzed sessions, nor was it analyzed for the ‘intertrial’ category because rats exhibited variable behavior during intertrial intervals, reducing predictive validity.</p><sec id="s2-4-1-1"><title>Reward retrieval</title><p><xref ref-type="fig" rid="fig5">Figure 5D</xref> shows performance for real-time decoding of reward retrieval events from each of the four predictors (Off, CB, CF, and Mot). Sensitivity for detecting reward retrieval was higher for Rat JL66 than JL63, which is not surprising since predictions were derived from a larger number of detected neurons in Rat JL66. In both rats, sensitivity was similar when the decoder was trained on Off, CB, or CF traces. The <italic>F</italic>-score was higher for CF than Off or CB traces in Rat JL63 but not JL66, suggesting that in cases where a small number of neurons are detected (as in Rat JL63), decoder precision can be improved by using CF rather than CB traces. As discussed above for CA1 decoding, this benefit is likely to accrue from the larger number of CF than CB traces in cases where the detected neuron count is low.</p></sec><sec id="s2-4-1-2"><title>Correct choice</title><p><xref ref-type="fig" rid="fig5">Figure 5E</xref> shows performance for real-time decoding of correct choice events from each of the four predictors (Off, CB, CF, and Mot). Sensitivity was higher for Rat JL66 than JL63, which again is not surprising since predictions were derived from a larger number of neurons in Rat JL66. In rat JL66, sensitivity for detecting reward retrieval was similar regardless of whether the decoder was trained on Off, CB, or CF traces. However, in Rat JL63, sensitivity was higher for CF traces, again suggesting that in cases where a small number of neurons are detected (as in Rat JL63), decoder performance is better for CF than CB traces.</p></sec><sec id="s2-4-1-3"><title>Trial initiation</title><p><xref ref-type="fig" rid="fig5">Figure 5F</xref> shows that real-time decoding of trial initiation events was much less accurate than for reward retrieval (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) or correct choice (<xref ref-type="fig" rid="fig5">Figure 5E</xref>) events, even in Rat JL66 where there were a large number of detected OFC neurons. This result suggests that trial initiation events were not robustly encoded by OFC neurons; hence, decoding of trial initiation was not considered further.</p></sec></sec></sec><sec id="s2-5"><title>Online motion correction</title><p>In both rats, real-time decoding of reward retrieval and correct choice events was similarly accurate (from both CB and CF traces) regardless of whether online motion correction was used (mc+ vs. mc−; <xref ref-type="fig" rid="fig5">Figure 5D–F</xref>). This suggests that online motion correction did not benefit decoding accuracy. However, in both rats, jitter error (quantified by <inline-formula><mml:math id="inf29"><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula>) was significantly greater during reward retrieval than during other behaviors (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), indicating that retrieval of food from the magazine hopper caused the brain to move inside the skull. In rat JL63 (but not JL66), <inline-formula><mml:math id="inf30"><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> was also greater during correct choice events than during other behaviors (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), indicating that pressing the touchscreen may also have sometimes caused the brain to move inside the skull. Given that brain motion was significantly correlated with some behaviors, it is important to consider the possibility that in the absence of motion correction, the decoder may have learned to predict behavior from motion artifacts rather than from calcium activity. If so, then even though prediction accuracy remains high without motion correction, it would still be necessary to perform motion correction to assure that the real-time decoder derives its predictions from calcium activity rather than motion artifacts.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Correlation between motion artifact and decoding accuracy.</title><p>(<bold>A</bold>) Jitter error |Δ<italic>d</italic>| is significantly greater during reward retrieval events than other behaviors in both rats. (<bold>B</bold>) |Δ<italic>d</italic>| is significantly greater during correct choice events than other behaviors in rat J63 but not JL66. (<bold>C</bold>) When CB traces are extracted without online motion correction (mc−; top), |Δ<italic>d</italic>| is significantly greater during accurately classified (’predicted’) reward retrieval event frames than inaccurately rejected (’unpredicted’) reward event frames; this difference is abolished when online motion correction is applied before trace extraction (mc+; bottom). (<bold>D</bold>) Same as ‘C’ but for predictions of correct choice events. (<bold>E</bold>) Same as ‘C’ but forCF rather than CB traces; note that for rat JL63 online motion correction does not abolish the correlation between decoding accuracy and motion artifact. (<bold>F</bold>) Same as ‘D’ but for CF traces extracted without (top) versus with (bottom) online motion correction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig6-v2.tif"/></fig><p>To investigate this, we isolated all frames that occurred during a given type of behavior event (reward retrieval or correct choice), and then performed rank sum tests to compare the median value of <inline-formula><mml:math id="inf31"><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> from frames in which the behavior event was correctly classified (‘predicted’) versus frames in which it was not (‘unpredicted’). Without motion correction, the median jitter error <inline-formula><mml:math id="inf32"><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> in both rats was significantly higher for frames in which CB traces correctly predicted reward retrieval (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, top graphs) or correct choice (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, top graphs); this effect was abolished after motion correction (<xref ref-type="fig" rid="fig6">Figure 6C, D</xref>; bottom graphs). Similarly, without motion correction the median <inline-formula><mml:math id="inf33"><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> in both rats was significantly higher for frames in which CF traces correctly predicted reward retrieval (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, top graphs) or correct choice (<xref ref-type="fig" rid="fig6">Figure 6F</xref>, top graphs); this effect was also abolished after motion correction (<xref ref-type="fig" rid="fig6">Figure 6E, F</xref>; bottom graphs) in all but one case (reward retrieval predictions in JL63).</p></sec><sec id="s2-6"><title>Real-time decoding latency</title><p>DeCalciOn’s real-time feedback latency, <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , is equal to the time that elapses between photons hitting the miniscope’s image sensor and the rising edge of the TTL output signal that is triggered when the decoder detects a target pattern of neural activity. The total feedback latency is the sum of three delays that accrue sequentially: <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> + <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> + <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the light-gathering delay, <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the frame transmission delay, and <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the image processing delay.</p><sec id="s2-6-1"><title>Light-gathering delay (<inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>)</title><p>The first delay in the real-time decoding sequence arises from the time it takes for the sensor to gather light. Each image frame is formed on the sensor by summing light from photons that arrive during an exposure time window that is ~1 ms shorter than the frame interval, <italic>I</italic>. For example, at a frame rate of <italic>F</italic> ≈ 20 Hz (as in experiments presented here) the frame interval is <italic>I</italic> ≈ 50 ms, so the exposure time would be (50-1) ≈ 49 ms, which is slightly shorter than <italic>I</italic> because the sensor’s circuits require ~1 ms to perform analog-to-digital (A/D) conversion of pixel values at the end of each exposure. Hence, at each end-of-frame, the captured image integrates the intensity of light that has hit the sensor during the preceding frame interval. This binned averaging of light intensity is tantamount to convolving the ‘true’ brain fluorescence signal with a boxcar kernel of width <italic>I</italic>, effectively shifting the sampled fluorescence signal rightward in time from the true fluorescence signal by a delay of  <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = <italic>I</italic>/2 (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Thus, at the frame rate of 20 Hz used here, <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ≈ 50/2 = 25 ms.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Real-time decoding latency.</title><p>(<bold>A</bold>) Left graph illustrates how the light-gathering delay (<italic>τ<sub>L</sub></italic>) is incurred as the sensor time integrates the true fluorescence signal (black line) by summing light gathered during each frame exposure, yielding a time series of binned fluorescence values (red dots) that are delayed from the original fluorescence signal by 1/2 the frame interval (50-ms frame interval was used for this example); right graph shows temporal cross correlogram between the source versus binned fluorescence signals, peaking at 1/2 the frame interval (<italic>τ</italic><sub><italic>L</italic></sub> = 25 ms). (<bold>B</bold>) Transmission delay (<italic>τ</italic><sub><italic>T</italic></sub>) is proportional to the number of pixels, <italic>P</italic>, in the shaded red rectangular area between the upper left corner of the sensor image and the lower right corner of the 512 × 512 imaging subwindow. (<bold>C</bold>) Image processing delay (<italic>τ</italic><sub><italic>I</italic></sub>) for 1204 calcium traces (maximum supported) is proportional to the number of units in the linear classifier’s output layer; empirical distributions of <italic>τ</italic><sub><italic>I</italic></sub> are shown for linear classifiers with a single output unit (here <italic>τ<sub>I</sub></italic> is approximated as the minimum latency for a 5-node binary tree decoder), 12 output units, and 23 output units. (<bold>D</bold>) Linear fit to mean <italic>τ<sub>I</sub></italic> (<italic>y</italic>-axis) as function of the number of linear classifiers (<italic>x</italic>-axis); the <italic>y</italic>-intercept corresponds to the mean field-programmable gate array (FPGA) processing delay. (<bold>E</bold>) Blue line shows mean distance error (<italic>y</italic>-axis) for maximum likelihood decoding of a rat’s position on a linear track from spike counts of 53 place cells (recorded by Hector Penagos in Matt Wilson’s lab at MIT30) using time bins of differing size (<italic>x</italic>-axis) for spike counts; circles plot distance error for real-time decoding of CB and CF calcium traces from 3 rats in our dataset for which the number of CB traces was very close to 50 (thus matching the number of place cells in the single-unit vs. calcium trace decoding data). It can be seen that decoding from single-unit spikes was slightly more accurate than decoding from CB calcium traces and slightly less accurate than decoding from CF calcium traces in these three rats.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78344-fig7-v2.tif"/></fig></sec><sec id="s2-6-2"><title>Frame transmission delay (<inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>)</title><p>The second delay in the real-time decoding sequence arises from the time required to transmit frame data from the miniscope sensor to the DAQ, and then from the DAQ to the Ultra96 frame buffer. The miniscope sensor does not begin transmitting pixel data for a given frame until after A/D conversion is completed for all pixels in that frame, so there is no temporal overlap between <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Each data pixel is serially transmitted from the sensor to the DAQ via the coax cable, and then immediately (with a delay of only one pixel clock cycle) deserialized and retransmitted over the flywire bus from the DAQ to the Ultra96 (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Hence, the time required for a frame of data to travel from sensor to DAQ and then from DAQ to Ultra96 can be measured as a single transmission delay, <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . The transmission time for an image frame can be computed as  <inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = <italic>P</italic> × <italic>C</italic>, where <italic>C</italic> is the pixel clock period (60.24 ns for the V4 Miniscope, 16.6 ns for the LFOV Miniscope) and <italic>P</italic> is the number of pixels per frame. Only pixels within the 512 × 512 imaging subwindow are used for real-time decoding, the effective number of pixels per frame is <italic>P</italic> = <italic>W</italic>(<italic>B −</italic> 1) + <italic>R</italic>, where <italic>B</italic> and <italic>R</italic> are the bottom-most pixel row and right-most pixel column of the selected 512 × 512 imaging subwindow, and <italic>W</italic> is the width (in pixels) of the sensor image (red shaded region in <xref ref-type="fig" rid="fig7">Figure 7B</xref>). The size of the V4 UCLA Miniscope sensor is 608 × 608 pixels, so if the 512 × 512 subwindow is positioned in the center of the V4 sensor, the effective number of pixels per frame is p = 608 (608/2 + 512/2 – 1) + 608/2 + 512/2 = 340,432 and the transmission delay is  <inline-formula><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 340,432 × 60.24 = 20.5 ms. The size of the LFOV sensor is 1296 × 972 pixels, so if the 512 × 512 subwindow is positioned in the center of the LFOV sensor, then the effective number of pixels per frame is p = 1296 (1296/2 + 512/2 − 1) + 972/2 + 512/2 = 1,171,030 and the transmission delay is  <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 1,171,030 × 60.24 = 19.5 ms. Thus, with a frame rate of 20 Hz and an imaging subwindow located near the center of the sensor, the V4 and LFOV Miniscopes both incur transmission delays of ~20 ms.</p></sec><sec id="s2-6-3"><title>Image processing delay (<inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>)</title><p>The final delay in the real-time decoding sequence arises from the time it takes to execute the real-time image processing pipeline on the Ultra96 (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). As explained above, Stages 1–3 of image processing (online motion correction, background subtraction, and trace extraction) are performed in the fabric of the FPGA; after all three of these stages are completed, Stage 4 (linear classifier decoding) is performed by the MPSoC’s ARM core. Hence, the image processing delay can be decomposed into two sequential and non-overlapping latencies, <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> + <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the FPGA latency incurred during Steps 1–3 and <inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the ARM core latency incurred during Stage 4. Here, we estimated these latencies empirically by measuring the time elapsed between arrival of the last pixel of a frame in the Ultra96 image buffer (start of the <inline-formula><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> interval) and the rising edge of the TTL pulse triggered by ARM core decoder output (end of the <inline-formula><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> interval). It should be noted that the FPGA begins performing Step 1 of image processing (online motion correction) immediately after the lower right pixel of the 128 × 128 image processing subwindow (see yellow box in <xref ref-type="fig" rid="fig2">Figure 2</xref>) is received in the buffer, which is prior to arrival of the last pixel of the 512 × 512 subwindow. Consequently, Stage 1 is often finished by the time the last pixel of the 512 × 512 subwindow arrives (it may finish slightly later if the 128 × 128 window is located near the lower right corner of the 512 × 512 window). For this reason, our empirical estimate of <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> primarily measures the time required for the FPGA to perform Stages 2 and 3 after Stage 1 has been completed. <xref ref-type="fig" rid="fig7">Figure 7C</xref> plots empirically measured distributions of <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> using 1204 calcium traces (the maximum number of predictors supported by DeCalciOn). The TTL latency for a linear classifier with a single output unit is approximated as the minimum observed latency for a 5-node binary tree decoder (corresponding to the case where the shallowest tree node produces a positive output); distributions are also shown for classifiers with 12 and 23 output units. We computed a linear fit to the empirical function  <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = <italic>f</italic>(<italic>M</italic>), where <italic>M</italic> is the number of output units. The <italic>y</italic>-intercept of this function provides an estimate for the FPGA processing delay of <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> ≈ 0.888 ms; the ARM core processing delay then adds an additional ~47.3 μs per output unit to the FPGA delay.</p><p>Summarizing, DeCalcIOn’s real-time feedback latency is <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> &lt; <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. For the UCLA Miniscope sampling at a frame rate of 20 Hz, this equates to <inline-formula><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> &lt; 25 ms + 20 ms + 2.5 ms = 47.5 ms. Since the frame interval is 50 ms at the 20 Hz frame rate, the TTL output signal generated for each frame will occur at least 2.5 ms before the last pixel of the subsequent frame arrives in the Ultra96 frame buffer. To put this feedback latency into the context of the rat’s behavior, a freely behaving rat can travel ~7.5 cm in 47.5 ms if it is running at the maximum observed speed on the linear track of ~150 cm/s (see panel A in Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>). Hence, if we write <inline-formula><mml:math id="inf67"><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> to denote the position a rat occupies at the moment when a calcium trace vector arrives at the sensor, and <inline-formula><mml:math id="inf68"><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> to denote the position the rat later occupies at the moment when TTL feedback is generated from the decoded calcium trace vector, the maximum distance error that can accrue between detection and decoding of the trace vector is  <inline-formula><mml:math id="inf69"><mml:mi>Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> = 7.5 cm.</p><p><xref ref-type="fig" rid="fig7">Figure 7E</xref> shows that when a maximum likelihood decoder is used to predict a rat’s position on a linear track from single-unit spike trains of 53 place cells (<xref ref-type="bibr" rid="bib6">Center for Brains Minds Machines, 2017</xref>) at a 47.5-ms latency from the rat’s true position, the distance error asymptotes at a minimum of ~20 cm for spike count time windows longer than ~0.5 s. It is difficult to directly equate the exponential decay time of a genetically encoded calcium indicator with the width of a square time window used for counting single-unit spikes, but if we assume that the GCaMP7s indicator used here integrates spikes with a time constant that is comparable to counting spikes in 1-s time window, then the accuracy of real-time position decoding from calcium traces appears to be slightly worse than decoding from single-unit spikes when CB traces are used, and slightly better than decoding from single-unit spikes when CF traces are used (dots in <xref ref-type="fig" rid="fig7">Figure 7E</xref>). This comparison was made for a subset of calcium imaging rats (<italic>n</italic> = 3) in which the number of detected CB traces was very similar to the number of place cells (<italic>n</italic> = 53) in the single-unit dataset (<xref ref-type="bibr" rid="bib6">Center for Brains Minds Machines, 2017</xref>). Hence, real-time decoding of calcium traces with DeCalciOn achieves distance errors that are within a similar range as those seen in offline analysis of place cell spike trains.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we have demonstrated the DeCalciOn system’s capabilities for online image processing, calcium trace extraction and decoding, and short latency triggering of TTL output pulses for closed-loop feedback experiments.</p><sec id="s3-1"><title>Source identification</title><p>Most in vivo calcium imaging experiments utilize offline algorithms to analyze calcium trace data that has previously been collected and stored during a behavioral task. Popular offline analysis packages such as CaImAn (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>) and MIN1PIPE (<xref ref-type="bibr" rid="bib14">Deng et al., 2015</xref>) employ sophisticated algorithms like CNMF (<xref ref-type="bibr" rid="bib27">Pnevmatikakis et al., 2016</xref>) to demix crossover fluorescence and source calcium traces to single neurons. Accurate source identification is important for studies where investigating the tuning properties of individual neurons is a major aim of data analysis, but DeCalciOn is designed for real-time imaging experiments in which fast and accurate decoding of behavior from neural population activity takes precedence over characterizing single-neuron firing properties. For real-time decoding, the high computational costs of source identification may not yield proportional benefits to decoding accuracy. Supporting this, we found that linear classifiers trained on calcium traces extracted online (without demixing) predicted behavior just as accurately as those trained on traces extracted offline (with demixing) using the same set of ROIs (<xref ref-type="fig" rid="fig3">Figure 3E</xref>; <xref ref-type="fig" rid="fig5">Figure 5D–F</xref>). We also found that decoders trained on traces extracted from contour-free ROIs (which do not overlap with individual neurons) predicted behavior as or more accurately than traces extracted from contour-based ROIs that overlapped with identified neurons in the image frame (<xref ref-type="fig" rid="fig3">Figure 3F and G</xref>; <xref ref-type="fig" rid="fig4">Figure 4A and B</xref>; <xref ref-type="fig" rid="fig5">Figure 5D–F</xref>). Analogous results have been reported in electrophysiology, where ‘spike sorting’ algorithms for sourcing action potentials to single neurons incur high computational overhead costs that are comparable to the costs of sourcing calcium traces to single neurons with CNMF; decoding behavior from electrophysiological recordings of multiunit (unsorted) spikes is more computationally efficient than decoding from sorted spikes and often incurs no cost (or even a benefit) to decoding accuracy (<xref ref-type="bibr" rid="bib14">Deng et al., 2015</xref>). Results presented here indicate that as in electrophysiology studies, the high computing cost of source identification in calcium imaging studies must be weighed against the benefits. For real-time imaging studies, contour-free decoding approaches can offer a strategy for optimizing computing efficiency without compromising decoding accuracy.</p></sec><sec id="s3-2"><title>Online motion correction</title><p>After source identification, the most computationally expensive stage in the image processing pipeline is motion stabilization. DeCalciOn reduces motion stabilization costs by using a motion correction subwindow (128 × 128 pixels, see <xref ref-type="fig" rid="fig2">Figure 2</xref>) that is considerably smaller than the 512 × 512 pixel imaging window (yet still large enough to achieve reliable motion correction). Even so, deriving each frame’s motion displacement vector at millisecond latencies consumes more MPSoC hardware resources than any other stage within the image processing pipeline (<xref ref-type="bibr" rid="bib9">Chen et al., 2022a</xref>). To quantify the benefits of these high resource costs, we compared decoder performance with and without motion correction. We found that online motion stabilization was effective at preventing two problems that can arise from instability of the brain image on the sensor. First, when decoding a rat’s position on the linear track, online motion correction prevented decoding accuracy from becoming degraded during frames in which brain motion shifted the sensor image, as evidenced by the fact the positive correlations between decoding error and image displacement were reduced by online motion correction (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Second, in the instrumental touch screen task where brain motion was significantly correlated with specific behaviors, the decoder learned to predict behavior from image motion (rather than calcium fluorescence) in the absence of motion correction; the decoder was prevented from learning these ersatz predictions when the online motion correction algorithm was applied to the decoder’s training and testing datasets (<xref ref-type="fig" rid="fig5">Figure 5D–F</xref>; <xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>Motion correction is a transient phenomenon and is therefore not needed during every image frame. Illustrating this, session-averaged accuracy for decoding position on the linear track was not significantly improved by online motion correction (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). However, for real-time feedback experiments, the session-averaged prediction accuracy is not the most important measure of decoder performance. What matters most is the accuracy of decoding during specific frames when it is important for feedback stimulation to be delivered or withheld. If such frames happen to overlap with motion artifact that interferes with decoding accuracy, then failure to perform motion correction can have disastrous consequences for feedback experiments, regardless of whether or not motion correction improves the ‘average’ decoding accuracy. Based on these considerations and the results of our performance tests, we conclude that the high resource costs of online motion correction are well worth the benefits they accrue by increasing the reliability of online decoding of neural population activity.</p></sec><sec id="s3-3"><title>Comparison with other real-time imaging platforms</title><p>Several previous online calcium fluorescence trace extraction algorithms have been proposed; some have been validated only by offline experiments showing that they can generate online traces that rival the quality of offline traces, and have not yet been performance tested on hardware platforms to assess their performance at real-time imaging or decoding of calcium traces (<xref ref-type="bibr" rid="bib15">Friedrich et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Friedrich et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Taniguchi et al., 2021</xref>). Other systems have been performance tested in real time, as we have done here for DeCalciOn. For example, <xref ref-type="bibr" rid="bib23">Liu et al., 2021</xref> introduced a system for real-time decoding with the UCLA miniscope that, unlike DeCalciOn, requires no additional hardware components because it is implemented entirely by software running on the miniscope’s host PC. The system does not perform online motion stabilization, which is a significant drawback for reasons discussed above (see ‘Online motion correction’). <xref ref-type="bibr" rid="bib23">Liu et al., 2021</xref> showed that their system can implement a single binary classifier to decode calcium traces from 10 ROIs in mouse cortex at latencies &lt;3 ms; this latency measurement appears to only include the decoder’s execution time for generating predictions from calcium vectors (equivalent to Step 4 of our image processing pipeline) but not the light-gathering delay time (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) or data buffering time. <xref ref-type="bibr" rid="bib23">Liu et al., 2021</xref> did not report how their system’s latency scales with the number of classifiers or the size/number of calcium ROIs, but under the assumption that serial processing time on the host PC scales linearly with these variables, decoding latencies of several seconds or more would be incurred for the decoding stage alone if 12–24 classifiers were used to predict behavior from 1204 calcium traces, which is much longer then the millisecond latencies achieved by DeCalciOn in performance tests presented here (see <xref ref-type="fig" rid="fig7">Figure 7C, D</xref>).</p><p><xref ref-type="bibr" rid="bib32">Zhang et al., 2018</xref> performed 2-photon calcium imaging experiments that incorporated real-time image processing on a GPU to detect neural activity and trigger closed-loop optical feedback stimulation in mouse cortex. The time required for calcium trace extraction increased linearly with a slope of ~0.05 ms per ROI (from a <italic>y</italic>-intercept of 0.5 ms; see panel C of Supplementary Fig. 6 in <xref ref-type="bibr" rid="bib32">Zhang et al., 2018</xref>,), implying a total trace extraction time of ~52 ms for 1204 ROIs. Summing this latency with the light-gathering delay (~15 ms at the system’s default 30-Hz frame rate), data buffering delay (reported to be ~2.5 ms), online motion correction delay (reported to be ~3.5 ms), and decoder execution time (~2 ms), it can be estimated that when decoding 1024 traces, <xref ref-type="bibr" rid="bib32">Zhang et al., 2018</xref> system would incur a delay of ~75 ms between light arriving at the sensor and triggering of closed-loop feedback. This latency is only 25 ms (~50%) longer than DeCalciOn’s 47.5-ms decoding latency under similar conditions (<xref ref-type="fig" rid="fig7">Figure 7</xref>), and could possibly be improved using parallelization or other efficiency strategies. Although latency performance is similar for DeCalciOn’s FPGA-based pipeline and <xref ref-type="bibr" rid="bib32">Zhang et al., 2018</xref> GPU-based pipeline, the two systems have different hardware compatibilities: DeCalciOn is designed for compatibility with head-free miniscopes that use the UCLA DAQ interface, whereas <xref ref-type="bibr" rid="bib32">Zhang et al., 2018</xref> system is designed for compatibility with hardware and software interface standards used in benchtop multiphoton imaging systems.</p></sec><sec id="s3-4"><title>Summary and conclusions</title><p>DeCalciOn’s low-cost, ease of use, and latency performance compare favorably against other real-time imaging systems proposed in the literature. We hope that by making DeCalciOn widely available to the research community, we can provide a platform for real-time decoding of neural population activity that will facilitate novel closed-loop experiments and accelerate discovery in neuroscience and neuroengineering. All of DeCalciOn’s hardware, software, and firmware are openly available through <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link>.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>A total of 14 Long-Evans rats (6F, 6M for CA1 experiments, 2M for OFC experiments) were acquired from Charles River at 3 months of age. Subjects were singly housed within a temperature and humidity controlled vivarium on a 12-hr reverse light cycle. Surgical procedures began after a 1-week acclimation period in the vivarium, and recordings and behavioral experiments began around 5 months of age. All experimental protocols were approved by the Chancellor’s Animal Research Committee of the University of California, Los Angeles, in accordance with the US National Institutes of Health (NIH) guidelines.</p></sec><sec id="s4-2"><title>Surgical procedures</title><sec id="s4-2-1"><title>Hippocampal surgeries</title><p>Subjects were given two survival surgeries prior to behavior training in order to record fluorescent calcium activity from hippocampal CA1 cells. During the first surgery, rats were anesthetized with 5% isoflurane at 2.5 l/min of oxygen, then maintained at 2–2.5% isoflurane while a craniotomy was made above the dorsal hippocampus. Next, 1.2 μl of AAV9-Syn-GCamp7s (Addgene) was injected at 0.12 μl/min just below the dorsal CA1 pyramidal layer (−3.6 AP, 2.5 ML, 2.6 DV) via a 10-μl Nanofil syringe (World Precision Instruments) mounted in a Quintessential Stereotaxic Injector (Stoelting) controlled by a Motorized Lab Standard Stereotax (Harvard Apparatus). Left or right hemisphere was balanced across all animals. One week later, the rat was again induced under anesthesia and four skull screws were implanted to provide stable hold for the subsequent implant. The viral craniotomy was reopened to a diameter of 1.8 mm, and cortical tissue and corpus callosal fibers above the hippocampus were aspirated away using a 27- and 30-gauge blunt needle. Following this aspiration, and assuring no bleeding persisted in the craniotomy, a stack of two 1.8-mm diameter Gradient Refractive INdex (GRIN) lenses (Edmund Optics) was implanted over the hippocampus and cemented in place with methacrylate bone cement (Simplex-P, Stryker Orthopaedics). The dorsal surface of the skull and bone screws were cemented with the GRIN lens to ensure stability of the implant, while the dorsal surface of the implanted lens was left exposed. Two to three weeks later, rats were again placed under anesthesia in order to cement a 3D printed baseplate above the lens. First a second GRIN lens was optically glued (Norland Optical Adhesive 68, Edmund Optics) to the surface of the implanted lens and cured with UV light. The pitch of each GRIN lens was ≲0.25, so implanting two stacked lenses yielded a total pitch of ≲0.5. This half pitch provides translation of the image at the bottom surface of the lenses to the top while maintaining the focal point below the lens. This relay implant enables access to tissue deep below the skull surface. The miniscope was placed securely in the baseplate and then mounted to the stereotax to visualize the calcium fluorescence and tissue. The baseplate was then cemented in place above the relay lenses at the proper focal plane and allowed to cure.</p></sec><sec id="s4-2-2"><title>OFC surgeries</title><p>Infusion of AAV9-CaMKIIα-GCaMP6f (Addgene, #100834-AAV9) in OFC was performed using aseptic stereotaxic techniques under isoflurane gas (1–5% in O<sub>2</sub>) anesthesia prior to any behavioral testing. Before surgery animals were administered 5 mg/kg s.c. carprofen (NADA #141-199, Pfizer, Inc, Drug Labeler Code: 000069) and 1 cc saline. After being placed in the stereotaxic apparatus (David Kopf; model 306041), the scalp was removed. The skull was leveled +2 and −2 mm A-P and M-L to ensure that bregma and lambda were in the same horizontal plane. Small burr holes were drilled in the skull above the target. Rats were infused with GCaMP6f virus in OFC (AP = +3.7; ML = ±2.5; DV = −4.6) and afterwards, implanted with a GRIN lens and affixed with a custom 3D printed lens cover during the same surgery. A total of 0.05 μl of GCaMP6f virus was infused at a rate of 0.01 μl per minute in the target region. After each infusion, 5 min elapsed before exiting the brain. Two to three weeks after virus infusion and lens implantation, rats were again placed under anesthesia to remove the lens cover and check for signal. After confirming signal, a miniscope baseplate was affixed with dental cement which enabled quick placement and removal of the miniscope in a freely moving rat.</p></sec></sec><sec id="s4-3"><title>Linear track task</title><p>After rats had been baseplated, they were placed on food restriction to reach a goal weight of 85% <italic>ad lib</italic> weight and then began behavioral training. Time between the beginning of the surgical procedures and the start of behavior training was typically 6–8 weeks. Rats earned 20 mg chocolate pellets by alternating between two rewarded ends of a linear track (250 cm) during 15-min recordings beginning 5 days after baseplating. After receiving a reward at one end, the next reward had to be earned at the other end by crossing the center of the track. A webcam mounted in the behavior room tracked a red LED located on the top of the Miniscope and this video was saved alongside the calcium imaging via the Miniscope DAQ software with synchronized frame timestamps. These behavior video files were initially processed by custom python code, where all the session videos were concatenated together into one tiff stack, downsampled to the video imaging frame rate, the median of the stack was subtracted from each image, and finally they were all rescaled to the original 8-bit range to yield the same maximum and minimum values before subtraction. Background subtracted behavior videos were then processed in MATLAB. The rat’s position in each frame was determined using the location of the red LED on the camera. Extracted positions were then rescaled to remove the camera distortion and convert the pixel position to centimeters according to the maze size. Positional information was then interpolated to the timestamps of the calcium imaging video using a custom MATLAB script.</p></sec><sec id="s4-4"><title>Instrumental touchscreen task</title><p>Before pretraining, rats were acclimated to a gentle hold on the lap of the experimenter sitting adjacent to the test chamber. This acclimation took place over at least 1 week prior to an imaging session wherein the rat was acclimated to unscrewing the cover of the baseplate on the rat’s head, then placing the miniscope on the baseplate while on and imaging. After checking placement, the miniscope was screwed onto the baseplate. The rat was then placed in the operant chamber and the miniscope was plugged into the commutator. Pretraining schedules were then initiated that culminated in independent nosepokes to the touchscreen (<xref ref-type="bibr" rid="bib20">Harris et al., 2021</xref>). Following criterion of 60 committed trials or more during a 45-min session, rats were advanced to the main instrumental task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Rats were required to initiate a trial by touching the white graphic stimulus in the center screen (displayed for 40 s), and after initiation rats would be presented with two identical stimuli (i.e., fan or marble) on the left and right side of the screen (displayed for 60 s), that they were required to nosepoke as either the correct spatial side (<italic>p</italic><sub><italic>R</italic></sub>(<italic>B</italic>) = 1; rewarded with one sucrose pellet) or incorrect spatial side (<italic>p</italic><sub><italic>R</italic></sub>(<italic>W</italic>) = 0). Thus, rats were required to ignore the properties of the stimuli and determine the better-rewarded side.</p></sec><sec id="s4-5"><title>Histology</title><p>At the end of the experiment, rats were anesthetized with isoflurane, intraperitoneally injected with 1 ml of pentobarbital, then transcardially perfused with 100 ml of 0.01 M phosphate-buffered saline (PBS) followed by 200 ml of 4% paraformaldehyde in 0.01 M PBS to fix the brain tissue. Brains were sectioned at 40-µm thickness on a cryostat (Leica), mounted on gelatin prepared slides, then imaged on a confocal microscope (Zeiss) to confirm GFP expression and GRIN lens placement.</p></sec><sec id="s4-6"><title>Hardware</title><p>The DeCalciOn system is designed for use with UCLA Miniscope devices (<xref ref-type="bibr" rid="bib5">Cai et al., 2016</xref>) and other miniscopes (<xref ref-type="bibr" rid="bib28">Scott et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">de Groot et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Blair et al., 2021</xref>) that use the UCLA DAQ interface. The DAQ hardware requires modification by soldering flywires to the PCB (see <xref ref-type="fig" rid="fig1">Figure 1</xref>); instructions for doing this are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link> (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>) and pre-modified DAQ boards are available at <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link>. DeCalciOn is implemented on an Avnet Ultra96 development board (available from the manufacturer at <ext-link ext-link-type="uri" xlink:href="https://avnet.com/">Avnet.com</ext-link>), which must be mated to our custom interface board available at <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link> (the interface can also be ordered from a PCB manufacturer with design files available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>) (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>). ACTEV compiled hardware (for online image stabilization, image enhancement, calcium trace extraction) and embedded software (for real-time decoding and ethernet communication, which runs under the FreeRTOS operating system) can be programmed onto the Ultra96 board by copying a bootable bitstream file to a MicroSD card and then powering up the board with the card inserted. The V4 and MiniLFOV versions of the bitstream used for experiments reported here are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>., (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>) Bitstream files for other miniscope models and Vivado HLS C source code for generating new custom bitstream files are available on request.</p></sec><sec id="s4-7"><title>Software</title><p>Real-time imaging and decoding are controlled by custom RTI software (available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>) (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>) which runs on the host PC alongside standard DAQ software (available at <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link>) for adjusting the gain and focus of the Miniscope device. The DAQ software’s default operation mode is to receive and display raw Miniscope video data from the DAQ via a USB 3.0 port (and store these data if the storage option has been selected), and also to receive and display raw behavior tracking video from a webcam through a separate USB port. At the start of a real experimental session, data acquisition by both programs (DAQ and RTI software) is initiated simultaneously with a single button click in the RTI user interface, so that Miniscope video storage by the RTI software and behavior video storage by the Miniscope DAQ software are synchronized to begin at exactly the same time. This allows behavioral data stored by the DAQ software to be aligned with Miniscope video and calcium trace data stored by the RTI software. During the intermission period between initial data acquisition and real-time inference, data stored by DAQ and RTI software are used to train the linear classifier on the host PC. Trained classifier weights are then uploaded to the Ultra96 for real-time decoding.</p></sec><sec id="s4-8"><title>Contour-based pixel masks</title><p>Pixel masks for contour-based trace extraction were derived using the CaImAn (<xref ref-type="bibr" rid="bib18">Giovannucci et al., 2019</xref>) pipeline (implemented in python) to analyze motion-corrected sensor images from the training dataset (as noted in the main text, this took 30–60 min of computing time on the host PC). CaImAn is an offline algorithm that uses CNMF to isolate single neurons by demixing their calcium traces. The CNMF method is acausal so it cannot be used to extract traces in real time. But during offline trace extraction, CaImAn generates a set of spatial contours identifying pixel regions from which each demixed trace was derived, which ACTEV then uses as pixel masks for online extraction of contour-based traces. Once pixel masks were identified from the training data, motion-correct miniscope video from the training period was passed through an offline simulator that used ACTEV’s causal algorithm for extracting calcium traces from contour pixel masks. This yielded a set of calcium traces identical to those that would have been extracted from the training data in real time by ACTEV. These simulated traces were then used as input vectors to train the linear classifier. Since the online traces are not generated by CNMF (and are thus not demixed from one another), they are susceptible to contamination from fluorescence originating outside of contour boundaries. However, this crosstalk fluorescence did not impair decoding since similar accuracy results were obtained by training the linear classifier on online or offline contour-based traces (<xref ref-type="fig" rid="fig4">Figure 4F, G</xref>).</p></sec><sec id="s4-9"><title>Contour-free pixel masks</title><p>To implement contour-free trace extraction, we simply partitioned the 512 × 512 image frame into a 32 × 32 sheet of tiles, each measuring a square of 16 × 16 pixels (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). No traces were extracted from 124 tiles bordering the edge of the frame, to avoid noise artifacts that might arise from edge effects in the motion stabilization algorithm. Hence, a total of 1024–124 = 900 pixel mask tiles were used for contour-free calcium trace extraction. These traces were derived in real time and stored to the host PC throughout the initial data acquisition period, so they were immediately available for training the linear classifier at the start of the intermission. Consequently, an advantage of contour-free trace extraction is that the intermission period between training and testing is shortened to just a few minutes, because the lengthy process of contour identification is no longer required. A disadvantage of contour-free trace extraction is that contour tiles do not align with individual neurons in the sensor image. As reported in the main text, this lack of alignment between neurons and pixel masks did not impair (and often enhanced) position decoding; however, contour-free decoding does place limits upon what can be inferred about how single neurons represent information in imaged brain regions.</p></sec><sec id="s4-10"><title>Fluorescence summation</title><p>After pixel masks were created using one of the two methods (contour-based or contour-free) and uploaded to the FPGA, calculations for extracting calcium traces from the masks were the same. Each mask specified a set of pixels over which grayscale intensities were summed to obtain the fluorescence value of a single calcium trace:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>T</italic>(<italic>f</italic>) is the summed trace intensity for frame <italic>f</italic>, and <italic>p<sub>i</sub></italic>(<italic>f</italic>) is the intensity of the <italic>i</italic>th pixel in the mask for frame <italic>f</italic>, and <italic>P</italic> is the number of pixels in the mask. Each contour-free mask was a square tile containing 16 × 16 = 256 pixels (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), whereas the size of each contour-based mask depended upon where CaImAn identified neurons in the image (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p></sec><sec id="s4-11"><title>Drop filtering</title><p>The mean size of contour-based masks was 20–100 pixels (depending on the rat), which was an order of magnitude smaller than contour-free tiles. In a few rats, a small amount of jitter sometimes penetrated the online motion correction filter, causing the stabilized sensor image to slip by 1–2 pixels against stationary contour masks during 1–2 frames (see Video S1, line graph at lower right). This slippage misaligned pixels at the edges of each contour mask, producing intermittent noise in the calcium trace that was proportional to the fraction of misaligned pixels in the contour, which in turn was inversely proportional to the size of the contour, so that motion jitter caused more noise in traces derived from small contour masks than large contour masks. To filter out this occasional motion jitter noise from calcium traces, a <italic>drop filter</italic> was applied to traces derived from contour masks that contained fewer than 50 pixels. The drop filter exploited the fact that genetic calcium indicators have slow decay times, and therefore, sudden drops in trace fluorescence can be reliably attributed to jitter noise. For example, the GCamp7s indicator used here has a half decay time <xref ref-type="bibr" rid="bib26">O’Keefe and Dostrovsky, 1971</xref> of about 0.7 s, so at a frame rate of ~20 Hz, a fluorescence reduction of more than 5% between frames can only arise from jitter artifact. The drop filter defines a maximum permissible reduction in fluorescence between successive frames as follows:<disp-formula id="equ2"><mml:math id="m2"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mi>q</mml:mi></mml:math></disp-formula></p><p>where <italic>G</italic> is the maximum possible fluorescence intensity for any single pixel (255 for 8-bit grayscale depth), <italic>C</italic> is the number of pixels in the contour mask for the trace, and <italic>q</italic> is a user-specified sensitivity threshold, which was set to 0.9 for all results presented here. The drop-filtered calcium trace value for frame <italic>f</italic> was given by:<disp-formula id="equ3"><mml:math id="m3"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>It should be noted that while drop filtering protects against artifactual decreases in trace values, it offers no protection against artifactual increases that might masquerade as neural activity events. However, motion jitter almost never produced artifactual increases in fluorescence for the small contours to which drop filtering was applied, because small contours were ‘difficult targets’ for patches of stray fluorescence to wander into during jitter events that rarely exceeded 1–2 pixels of slippage. Larger contours were slightly more likely to experience artifactual fluorescence increases during jitter events, but in such cases, the artifact was diluted down to an inconsequential size because it only affected a tiny fraction of the large contour’s pixels. In summary, jitter artifact was highly asymmetric, producing artifactual decreases but not increases for small contours, and producing negligible artifact of any kind for large contours.</p></sec><sec id="s4-12"><title>Spike inference</title><p>Once trace values have been extracted (and drop filtered if necessary), ACTEV can apply a real-time spike inference engine to convert raw calcium trace values into inferred spike counts for each frame. Briefly, the spike inference engine measures how much the trace value in the current frame has changed from the prior frame, <italic>T</italic>(<italic>f</italic>) <italic>− T</italic>(<italic>f −</italic> 1), and compares this against a threshold value, <italic>Φ</italic>, which is equal to 2.5 times the standard deviation of difference values between successive frames of the same trace in the training dataset. For the CA1 data reported here, we found that the linear classifier was more accurate at decoding raw (unconverted) calcium traces than inferred spike counts (panel A of Supplementary to <xref ref-type="fig" rid="fig3">Figure 3</xref>). Decoding from raw calcium traces is not only more accurate but also more computationally efficient; therefore, the main text reports results obtained by decoding raw calcium trace values.</p></sec><sec id="s4-13"><title>Training classifiers</title><p>To train the linear classifier, data stored during the initial acquisition period by the DAQ and RTI software were analyzed during the intermission period by custom MATLAB scripts, available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>, (<xref ref-type="bibr" rid="bib11">Chen, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>). Different linear classifier architectures were used for position versus behavioral decoding, as explained below. Training vectors and target outputs were passed to MATLAB’s <italic>fitceoc</italic> function (from the machine learning toolbox) to compute the trained linear classifier weights, which were then uploaded from the host PC to the Ultra96 via ethernet, so that the real-time classifier running under FreeRTOS in the ARM core could decode calcium traces in real time.</p><sec id="s4-13-1"><title>Linear classifier for position decoding</title><p>The linear classifier had <italic>N</italic> inputs and <italic>M</italic> outputs, where <italic>N</italic> is the number of calcium traces. In performance tests reported here, the output layer utilized a Gray coding scheme to represent each of the 24 binned track locations (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Under this representation scheme, the total number of output units is equal to <italic>M</italic> = <italic>K</italic>/2, where <italic>K</italic> is the number of categories (position bins) to be encoded. Since the linear track was subdivided into <italic>K</italic> = 24 spatial bins, there were <italic>M</italic> = 24/2 = 12 binary classifier units in the decoder’s output layer. Training data consisted of calcium trace input vectors (derived by either contour-based or contour-free methods described above) and target position output vectors (encoding the rat’s true position) for each Miniscope frame. As a control condition, calcium traces and tracking data were circularly shifted against one another by 500, 1000, 1500, 2000, or 2500 frames before training, and the trained classifier was then tested on correctly aligned trace inputs and target position outputs. Accuracy results from all five shift values were averaged together to obtain the ‘shift’ decoding accuracies plotted in panel C of the Supplement to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></sec><sec id="s4-13-2"><title>Binary tree classifier for behavior decoding</title><p>The binary tree classifier had <italic>N</italic> inputs and <italic>M</italic> outputs, where <italic>M</italic> is the number of behavior categories to be decoded. In performance tests reported here, there were <italic>M</italic> = 5 behavior categories (<italic>pre-trial</italic>, <italic>correct choice</italic>, <italic>incorrect choice</italic>, <italic>reward retrieval</italic>, and <italic>intertrial</italic>) as defined in the main text. Calcium traces were extracted from each image frame using the CB, CF, or Off methods described above. As a control measure, the classifier was also trained to predict the behavior category from image motion displacement (Mot) rather than calcium traces; the Mot predictor was a vector comprised from five elements per frame: Δ<italic>x</italic>, Δ<italic>y</italic>, <italic>d</italic>, Δ<italic>d</italic>, and |Δ<italic>d</italic>| (see main text for definitions of symbols). For each session, the classifier was trained on data from the first 100 trials (40,998 frames for rat JL66, 44,877 frames for rat JL63) and then tested on the subsequent 50 trials (25,798 frames for rat JL66, 24,974 frames for rat JL63).</p></sec></sec><sec id="s4-14"><title>Decoding calcium traces</title><p>Once classifier weights had been uploaded to the Ultra96 from the host PC, it was then possible for the embedded ARM core in the Ultra96 MPSoC to read in calcium trace values from the DRAM in real time as each frame of data arrived from the Miniscope, and present these values as inputs to the linear classifier. The ARM core was programmed in C++ to perform calculations equivalent to MATLAB’s <italic>predict</italic> function (from the machine learning toolbox) and thereby generate the linear classifier output vector. For predicting the rat’s position on the track from CA1 activity, the decoded position bin was taken to be that represented by the Gray code vector that most closely matched the classifier output vector. For categorizing the rat’s behavior from OFC activity, the predicted behavior label was taken to be the label that had been most often selected by the decoder during a five-frame window that included the current and previous four frames. OFC decoding performance was assessed by computing <italic>sensitivity</italic> (the overall percentage of behavior events during the test epoch that were correctly predicted) and <italic>F</italic>-score (a standard measure of binary classification accuracy computed as 2<italic>PS</italic>/(<italic>P</italic> + <italic>S</italic>), where <italic>S</italic> is the sensitivity and <italic>P</italic> is the precision defined as the number of correct positive results divided by the number of all positive results). Decoder predictions were sent back to the host PC via ethernet for storage, and were also sent to a programmable <italic>logic mapper</italic> running on the ARM core, which converted the linear classifer’s output vector into a pattern of square pulses generated at on of 8 TTL pinouts from the Ultra96, capable of driving closed-loop neural or sensory feedback via external devices such as lasers, electrical stimulators, or audiovisual display equipment.</p></sec><sec id="s4-15"><title>Spatial tuning curves</title><p>To generate spatial tuning curves for fluorescence traces, each trace was first referenced to zero by subtracting its minimum observed fluorescence value (over all frames) from the value in every frame: <inline-formula><mml:math id="inf70"><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, where <italic>Z</italic>(<italic>f</italic>) is the zero-referenced trace value for frame <italic>f</italic>. The 250-cm linear track was subdivided into 24 evenly spaced spatial bins (12 each in the left-to-right and right-to-left directions; see <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Fluorescence activity, <italic>A</italic>, in each bin <italic>b</italic> was averaged by<disp-formula id="equ4"><mml:math id="m4"><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>B</italic> is the number of visits to bin <italic>b</italic> and <italic>f<sub>i</sub></italic> is the <italic>i</italic>th frame during which the rat was visiting bin <italic>b</italic>. The tuning curve vector [<italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>, …, <italic>A</italic><sub>24</sub>] was then normalized by dividing each of its elements by the value of the maximum element. Normalized tuning curve vectors were plotted as heatmaps in <xref ref-type="fig" rid="fig2">Figure 2C, D</xref>. Spatial similarity scores (<italic>S</italic>) were computed as described in the main text; <italic>S</italic> distributions for each rat are shown in Extended Data <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></sec><sec id="s4-16"><title>Correlation between decoding error and image displacement</title><p>Two Pearson correlation coefficients (<italic>R</italic><sub>mc+</sub> and R<sub>mc−</sub>) were derived for each session by correlating the distance error vectors (<inline-formula><mml:math id="inf71"><mml:mi>λ</mml:mi></mml:math></inline-formula><sub>mc+</sub> and <inline-formula><mml:math id="inf72"><mml:mi>λ</mml:mi></mml:math></inline-formula><sub>mc−</sub>) in each frame with the displacement vector <bold>d</bold> = {<italic>d</italic><sub>1</sub>, <italic>d</italic><sub>2</sub>, ..., <italic>d</italic><sub>F</sub>}, where <italic>F</italic> is the total number of frames in the session. A necessary condition for computing reliable <italic>R</italic> values was that <inline-formula><mml:math id="inf73"><mml:mi>λ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mi>d</mml:mi></mml:math></inline-formula> had to be sufficiently dispersed along their respective axes to detect a significant linear relationship between the variables. However, within any given session, there were often some position bins in which motion artifact either never occurred or was so small in magnitude that it was impossible to assess the impact of motion artifact upon decoding accuracy at that position. For this reason, the motion-accuracy correlation analysis was performed only on frames from selected position bins in which the maximum value of <inline-formula><mml:math id="inf75"><mml:mi>d</mml:mi></mml:math></inline-formula> exceeded a threshold of 4 pixels, which was a large enough displacement to alter the values of CB trace values extracted from images without motion correction. All frames from position bins meeting this criterion (and no frames from position bins that did not) were included in the correlation analysis. Hence, analysis was restricted to those locations on the track where trace values (and thus distance error of decoded positions) could potentially be affected by motion artifact and excluded locations where no influence was possible because of a lack of motion artifact.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Software, Validation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Formal analysis</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Software, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved Institutional Animal Care and Use Committee (IACUC) protocols (#2017-038) of the University of California Los Angeles. The protocol was approved by the Committee on the Ethics of Animal Experiments of UCLA. All surgery was performed under deep isoflurane anesthesia, and every effort was made to minimize suffering, including administration of pre- and post-surgical analgesia.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-78344-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All hardware, software, and firmware are openly available through <ext-link ext-link-type="uri" xlink:href="http://miniscope.org/index.php/Main_Page">miniscope.org</ext-link> and at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhe-ch/ACTEV">https://github.com/zhe-ch/ACTEV</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank Xilinx Corporation for donation of the Ultra96 board. We also thank Jeffrey Johnson for technical assistance in designing the custom interface board, and Shiyun Wang and Ryan Grgurich for helpful comments on the manuscript. This work was supported by NSF NeuroNex 1704708 (HTB, PG, DA, JC), 1UF1NS107668 (DA), and MH122800 (AI, HTB).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Hoogland</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Circuit investigations with open-source miniaturized microscopes: past, present and future</article-title><source>Frontiers in Cellular Neuroscience</source><volume>13</volume><elocation-id>141</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2019.00141</pub-id><pub-id pub-id-type="pmid">31024265</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Khakh</surname><given-names>BS</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>All the light that we can see: a new era in miniaturized microscopy</article-title><source>Nature Methods</source><volume>16</volume><fpage>11</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0266-x</pub-id><pub-id pub-id-type="pmid">30573833</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Blair</surname><given-names>C</given-names></name><name><surname>Sehgal</surname><given-names>M</given-names></name><name><surname>Jimka</surname><given-names>FNS</given-names></name><name><surname>Bellafard</surname><given-names>A</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name><name><surname>Basso</surname><given-names>MA</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Miniscope-LFOV: A Large Field of View, Single Cell Resolution, Miniature Microscope for Wired and Wire-Free Imaging of Neural Dynamics in Freely Behaving Animals (Submitted)</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.21.469394</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal sharp wave-ripple: a cognitive biomarker for episodic memory and planning</article-title><source>Hippocampus</source><volume>25</volume><fpage>1073</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1002/hipo.22488</pub-id><pub-id pub-id-type="pmid">26135716</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>DJ</given-names></name><name><surname>Aharoni</surname><given-names>D</given-names></name><name><surname>Shuman</surname><given-names>T</given-names></name><name><surname>Shobe</surname><given-names>J</given-names></name><name><surname>Biane</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wei</surname><given-names>B</given-names></name><name><surname>Veshkini</surname><given-names>M</given-names></name><name><surname>La-Vu</surname><given-names>M</given-names></name><name><surname>Lou</surname><given-names>J</given-names></name><name><surname>Flores</surname><given-names>SE</given-names></name><name><surname>Kim</surname><given-names>I</given-names></name><name><surname>Sano</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Baumgaertel</surname><given-names>K</given-names></name><name><surname>Lavi</surname><given-names>A</given-names></name><name><surname>Kamata</surname><given-names>M</given-names></name><name><surname>Tuszynski</surname><given-names>M</given-names></name><name><surname>Mayford</surname><given-names>M</given-names></name><name><surname>Golshani</surname><given-names>P</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A shared neural ensemble links distinct contextual memories encoded close in time</article-title><source>Nature</source><volume>534</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1038/nature17955</pub-id><pub-id pub-id-type="pmid">27251287</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Center for Brains Minds Machines</collab></person-group><year iso-8601-date="2017">2017</year><article-title>Single-unit spikes from rat running on a linear track are from a publicly available dataset collected by Hector Penagos in Matt Wilson’s lab at MIT</article-title><ext-link ext-link-type="uri" xlink:href="https://cbmm.mit.edu/learning-hub/tools-datasets/decoding-hippocampal-place-cell-data">https://cbmm.mit.edu/learning-hub/tools-datasets/decoding-hippocampal-place-cell-data</ext-link><date-in-citation iso-8601-date="2022-11-30">November 30, 2022</date-in-citation></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Cong</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>LA-NorRMCorre: LSTM-Assisted Non-Rigid Motion Correction on FGPA for Calcium Image Stabilization</article-title><conf-name>27th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA</conf-name></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Blair</surname><given-names>GJ</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Cong</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>BLINK: bit-sparse LSTM inference kernel enabling efficient calcium trace extraction for neurofeedback devices</article-title><conf-name>Proceedings of the ACM/IEEE International Symposium on Low Power Electronics</conf-name><pub-id pub-id-type="doi">10.1145/3370748.3406552</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Blair</surname><given-names>GJ</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Cong</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Efficient kernels for real-time position decoding from in vivo calcium images</article-title><conf-name>IEEE International Symposium on Circuits and Systems</conf-name><pub-id pub-id-type="doi">10.1109/ISCAS48785.2022.9937945</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Blair</surname><given-names>GJ</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Cong</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>FPGA-Based In-Vivo Calcium Image Decoding for Closed-Loop Feedback Applications</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2212.04736">https://arxiv.org/abs/2212.04736</ext-link><pub-id pub-id-type="doi">10.48550/arXiv.2212.04736</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Accelerator for calcium trace extraction from video (ACTEV)</data-title><version designator="swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82">https://archive.softwareheritage.org/swh:1:dir:52edbb3143496240abfe565f4e2262afcb445460;origin=https://github.com/zhe-ch/ACTEV;visit=swh:1:snp:ba0bd0b4439b682eb07220dc3c69e665abd285a5;anchor=swh:1:rev:aa6393d3bd2dd490aa5369e1f2677e85e8a64a82</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dana</surname><given-names>H</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Mohar</surname><given-names>B</given-names></name><name><surname>Hulse</surname><given-names>BK</given-names></name><name><surname>Kerlin</surname><given-names>AM</given-names></name><name><surname>Hasseman</surname><given-names>JP</given-names></name><name><surname>Tsegaye</surname><given-names>G</given-names></name><name><surname>Tsang</surname><given-names>A</given-names></name><name><surname>Wong</surname><given-names>A</given-names></name><name><surname>Patel</surname><given-names>R</given-names></name><name><surname>Macklin</surname><given-names>JJ</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Konnerth</surname><given-names>A</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Schreiter</surname><given-names>ER</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Kim</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>High-performance calcium sensors for imaging activity in neuronal populations and microcompartments</article-title><source>Nature Methods</source><volume>16</volume><fpage>649</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0435-6</pub-id><pub-id pub-id-type="pmid">31209382</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Groot</surname><given-names>A</given-names></name><name><surname>van den Boom</surname><given-names>BJ</given-names></name><name><surname>van Genderen</surname><given-names>RM</given-names></name><name><surname>Coppens</surname><given-names>J</given-names></name><name><surname>van Veldhuijzen</surname><given-names>J</given-names></name><name><surname>Bos</surname><given-names>J</given-names></name><name><surname>Hoedemaker</surname><given-names>H</given-names></name><name><surname>Negrello</surname><given-names>M</given-names></name><name><surname>Willuhn</surname><given-names>I</given-names></name><name><surname>De Zeeuw</surname><given-names>CI</given-names></name><name><surname>Hoogland</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>NINscope, a versatile miniscope for multi-region circuit investigations</article-title><source>eLife</source><volume>9</volume><elocation-id>e49987</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49987</pub-id><pub-id pub-id-type="pmid">31934857</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Clusterless decoding of position from multiunit activity using a marked point process filter</article-title><source>Neural Computation</source><volume>27</volume><fpage>1438</fpage><lpage>1460</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00744</pub-id><pub-id pub-id-type="pmid">25973549</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fast online deconvolution of calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005423</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005423</pub-id><pub-id pub-id-type="pmid">28291787</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Online analysis of microendoscopic 1-photon calcium imaging data streams</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008565</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008565</pub-id><pub-id pub-id-type="pmid">33507937</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>KK</given-names></name><name><surname>Burns</surname><given-names>LD</given-names></name><name><surname>Cocker</surname><given-names>ED</given-names></name><name><surname>Nimmerjahn</surname><given-names>A</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name><name><surname>Gamal</surname><given-names>AE</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Miniaturized integration of a fluorescence microscope</article-title><source>Nature Methods</source><volume>8</volume><fpage>871</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1694</pub-id><pub-id pub-id-type="pmid">21909102</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giovannucci</surname><given-names>A</given-names></name><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Gunn</surname><given-names>P</given-names></name><name><surname>Kalfon</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>BL</given-names></name><name><surname>Koay</surname><given-names>SA</given-names></name><name><surname>Taxidis</surname><given-names>J</given-names></name><name><surname>Najafi</surname><given-names>F</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Khakh</surname><given-names>BS</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Caiman an open source tool for scalable calcium imaging data analysis</article-title><source>eLife</source><volume>8</volume><elocation-id>e38173</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38173</pub-id><pub-id pub-id-type="pmid">30652683</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosenick</surname><given-names>L</given-names></name><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Closed-Loop and activity-guided optogenetic control</article-title><source>Neuron</source><volume>86</volume><fpage>106</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.034</pub-id><pub-id pub-id-type="pmid">25856490</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>C</given-names></name><name><surname>Aguirre</surname><given-names>C</given-names></name><name><surname>Kolli</surname><given-names>S</given-names></name><name><surname>Das</surname><given-names>K</given-names></name><name><surname>Izquierdo</surname><given-names>A</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unique features of stimulus-based probabilistic reversal learning</article-title><source>Behavioral Neuroscience</source><volume>135</volume><fpage>550</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1037/bne0000474</pub-id><pub-id pub-id-type="pmid">34460275</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>EE</given-names></name><name><surname>Blair</surname><given-names>GJ</given-names></name><name><surname>O’Dell</surname><given-names>TJ</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Izquierdo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Chemogenetic modulation and single-photon calcium imaging in anterior cingulate cortex reveal a mechanism for effort-based decisions</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5628</fpage><lpage>5643</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2548-19.2020</pub-id><pub-id pub-id-type="pmid">32527984</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinsky</surname><given-names>NR</given-names></name><name><surname>Sullivan</surname><given-names>DW</given-names></name><name><surname>Mau</surname><given-names>W</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name><name><surname>Eichenbaum</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hippocampal place fields maintain a coherent and flexible MAP across long timescales</article-title><source>Current Biology</source><volume>28</volume><fpage>3578</fpage><lpage>3588</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.09.037</pub-id><pub-id pub-id-type="pmid">30393037</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Cui</surname><given-names>X</given-names></name><name><surname>Jung</surname><given-names>H</given-names></name><name><surname>Halin</surname><given-names>K</given-names></name><name><surname>You</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Online decoding system with calcium image from mice primary motor cortex</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference</conf-name><fpage>6402</fpage><lpage>6405</lpage><pub-id pub-id-type="doi">10.1109/EMBC46164.2021.9630138</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Singh-Alvarado</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>ZC</given-names></name><name><surname>Fröhlich</surname><given-names>F</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>MIN1PIPE: a miniscope 1-photon-based calcium imaging signal extraction pipeline</article-title><source>Cell Reports</source><volume>23</volume><fpage>3673</fpage><lpage>3684</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.05.062</pub-id><pub-id pub-id-type="pmid">29925007</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitani</surname><given-names>A</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Real-time processing of two-photon calcium imaging data including lateral motion artifact correction</article-title><source>Frontiers in Neuroinformatics</source><volume>12</volume><elocation-id>98</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00098</pub-id><pub-id pub-id-type="pmid">30618703</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Soudry</surname><given-names>D</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Merel</surname><given-names>J</given-names></name><name><surname>Pfau</surname><given-names>D</given-names></name><name><surname>Reardon</surname><given-names>T</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Lacefield</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Ahrens</surname><given-names>M</given-names></name><name><surname>Bruno</surname><given-names>R</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Peterka</surname><given-names>DS</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Simultaneous denoising, deconvolution, and demixing of calcium imaging data</article-title><source>Neuron</source><volume>89</volume><fpage>285</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.037</pub-id><pub-id pub-id-type="pmid">26774160</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Thiberge</surname><given-names>SY</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Tervo</surname><given-names>DGR</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Karpova</surname><given-names>AY</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging cortical dynamics in GCaMP transgenic rats with a head-mounted widefield macroscope</article-title><source>Neuron</source><volume>100</volume><fpage>1045</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.050</pub-id><pub-id pub-id-type="pmid">30482694</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taniguchi</surname><given-names>M</given-names></name><name><surname>Tezuka</surname><given-names>T</given-names></name><name><surname>Vergara</surname><given-names>P</given-names></name><name><surname>Srinivasan</surname><given-names>S</given-names></name><name><surname>Hosokawa</surname><given-names>T</given-names></name><name><surname>Cherasse</surname><given-names>Y</given-names></name><name><surname>Naoi</surname><given-names>T</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Sakaguchi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Open-source software for real-time calcium imaging and synchronized neuron firing detection</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference</conf-name><fpage>2997</fpage><lpage>3003</lpage><pub-id pub-id-type="doi">10.1109/EMBC46164.2021.9629611</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tu</surname><given-names>M</given-names></name><name><surname>Zhao</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>A</given-names></name><name><surname>Gan</surname><given-names>WB</given-names></name><name><surname>Chen</surname><given-names>ZS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient position decoding methods based on fluorescence calcium imaging in the mouse hippocampus</article-title><source>Neural Computation</source><volume>32</volume><fpage>1144</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01281</pub-id><pub-id pub-id-type="pmid">32343646</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Dynamics of the hippocampal ensemble code for space</article-title><source>Science</source><volume>261</volume><fpage>1055</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1126/science.8351520</pub-id><pub-id pub-id-type="pmid">8351520</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Russell</surname><given-names>LE</given-names></name><name><surname>Packer</surname><given-names>AM</given-names></name><name><surname>Gauld</surname><given-names>OM</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Closed-Loop all-optical interrogation of neural circuits in vivo</article-title><source>Nature Methods</source><volume>15</volume><fpage>1037</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0183-z</pub-id><pub-id pub-id-type="pmid">30420686</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziv</surname><given-names>Y</given-names></name><name><surname>Burns</surname><given-names>LD</given-names></name><name><surname>Cocker</surname><given-names>ED</given-names></name><name><surname>Hamel</surname><given-names>EO</given-names></name><name><surname>Ghosh</surname><given-names>KK</given-names></name><name><surname>Kitch</surname><given-names>LJ</given-names></name><name><surname>El Gamal</surname><given-names>A</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Long-Term dynamics of CA1 hippocampal place codes</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>264</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nn.3329</pub-id><pub-id pub-id-type="pmid">23396101</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78344.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.01.31.478424" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.31.478424"/></front-stub><body><p>This article presents a novel realtime processing pipeline for miniscope imaging which enables accurate decoding of behavioral variables and generation of feedback commands within less than 50ms. The efficiency of this important tool for experiments requiring close-loop interaction with brain activity is demonstrated based on compelling measurements in two experimental contexts. This pipeline will be useful for a wide range of questions in system neuroscience.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78344.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.31.478424">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.01.31.478424v2">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A hardware system for real-time decoding of in-vivo calcium imaging data&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Brice Bathellier as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Laura Colgin as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Provide a clear estimate of position accuracy that can be achieved with a space metric and not with a classifier accuracy with space bins whose size is unclear. Compare position estimates to what would be obtained from electrophysiology based on the literature.</p><p>2) Please provide a spatial distribution of speed, acceleration, arrests, and a description of the number of place cells and of the size of the place fields. Related to that, it is crucial to provide a plot of the accuracy of spatial decoding against spatial location to appreciate the performance of the system in real conditions, knowing that in linear tracks place fields are not homogeneously distributed.</p><p>3) Improve the description of the methodology and of imaging speed with respect to displacement speed according to referee 2's comments. It is particularly important to explain how much exploration time is needed to train the real-time classifier.</p><p>4) Provide an evaluation of the localisation information available in corrected motion artefacts in order to rule out that residual motion artefacts do not provide spatial information.</p><p>5) Repeat the experiment in a more complex environment, such as a 2D arena, in order to demonstrate the versatility of the method.</p><p>6) Provide a direct comparison of real performances based DeCalciOn to two previous online calcium imaging algorithms. The technical innovations of this work would be better highlighted by directly testing all three of these algorithms, ideally on similar datasets.</p><p>7) Provide a quantification of the spatial error (across different locations, and running speeds) made in a virtual experiment in which the goal is to emit a TTL signal at a specific location of the animal. The difference with point 7 is that this quantification includes not only classifier inaccuracies but also the time delay of the real time device in relation to the speed of the animal.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>– Provide a clear estimate of position accuracy that can be achieved with a space metric and not with a classifier accuracy with space bins whose size is unclear.</p><p>– Provide measurements in 2D and estimates of both location and head direction. How does it compare with electrophysiology? Are the conclusions about the lack of importance of cell body identification still holding for these conditions?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– There is not enough information about the behavior. Please provide a spatial distribution of speed, acceleration, and arrests. Is there an influence on the decoding accuracy?</p><p>– There is not enough information about the place cells. How many place cells? What is the spatial information of the cells? What are the size of the place fields?</p><p>– Some details are lacking or are not explicitly described. For instance, I think the following information should be easier to obtain in the article. The 2.5m linear track is split into 13 regions, leading to regions of roughly 20cm. In linear tracks rats run with an average speed of 70cm.s<sup>-1</sup>, with maximum speed going up to 100cm.s<sup>-1</sup>. This means that running rats will stay between 280ms to 400ms in each zone. The video frames are down-sampled to 15 frames/s leading to 66ms between each frame. This implies that 4 to 6 frames will correspond to the same location.</p><p>– A clearer description of the methodology might help the reader:</p><p>Here is a short overview of the methodology:</p><p>Technological methodology:</p><p>Step 1: Correct for translational movement of brain tissue. Aa 128x128 pixel area with distinct anatomical features was selected within the 512x512 cropped subregion to serve as a motion stabilization window;</p><p>Step 2: ACTEV removes background fluorescence from the 512x512 image;</p><p>Step 3: ROI definition: The enhanced image then is filtered through a library of up to 1,024 binary pixel masks (each up to 25x25 in size) that define ROIs within fluorescence is summed to extract calcium traces;</p><p>Step 4: Decoding. Population vectors of extracted calcium trace values are stored in the DRAM, and then decoded by sending them as inputs to a linear classifier running on the MPSoC's ARM core.</p><p>Experimental methodology:</p><p>Step 1: collect an initial imaging dataset and store it on the host PC;</p><p>Step 2: pause for an intermission to identify cell contours if necessary (this is only required for contour-based decoding; see below) and train a linear classifier to decode behavior from the initial dataset;</p><p>Step 3: upload classifier weights from the host PC to the Ultra96;</p><p>Step 4: perform real-time decoding with the trained classifier.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78344.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Provide a clear estimate of position accuracy that can be achieved with a space metric and not with a classifier accuracy with space bins whose size is unclear.</p></disp-quote><p>The size of the position bins (~20 cm) is now clarified in the main text (page 13) as well as in the caption for Figure 3A. As requested, position decoding accuracy is now reported using a space metric (cm) in addition to the classifier hit rate (Figures 3E-F, Figure 4A). To accommodate this change, it was necessary to alter the decoder’s output representation from the ordinal scheme used in the original submission (which was discontinuous at the ends of the track) to a Gray code in the revised submission (which has no discontinuities and thus allows distance error to be measured for all position bins, including at the ends of the track). The Gray coding scheme is illustrated in Figure 3B and described on page 13 the main text.</p><disp-quote content-type="editor-comment"><p>Compare position estimates to what would be obtained from electrophysiology based on the literature.</p></disp-quote><p>To address this, we have used a publicly available dataset of single-unit spike trains from place cells (n=53) recorded on a linear track; the data is from Hector Penagos in Matt Wilson's lab at MIT (see ref #30 in the revised manuscript). Figure 7E shows that for a subset of our CA1 imaging rats (n=3) in which the number of detected CB (contour-based) traces was close to 50 (and thus similar to the number of single units in the Penagos/Wilson place cell dataset), the single unit spike decoder’s mean distance error is 15-25 cm better than the calcium decoder trained on ~50 CB traces, and 5-10 cm worse than the than the calcium decoder trained on 900 CF (contour-free) traces. These results show that the real-time calcium decoder is able to achieve distance errors within a similar range as those achievable with offline analysis of place cell spike trains.</p><disp-quote content-type="editor-comment"><p>2) Please provide a spatial distribution of speed, acceleration, arrests, and a description of the number of place cells and of the size of the place fields.</p></disp-quote><p>Spatial distributions of speed, acceleration, and arrests (time spent sitting still) are now plotted in panel C of the Supplement to Figure 3.</p><p>As for the “number of place cells,” we did not classify calcium traces as originating from place versus non-place cells (indeed, CF calcium traces can combine fluorescence from multiple neurons, and thus cannot be sourced to single cells at all). Instead, we decoded the rats’ position from all available calcium traces (that is, all ROIs) of a given type (Off, CB, CF, CB+) regardless of their spatial tuning properties. This was not adequately explained in the original manuscript, so we have made it more clear in the revised paper by explaining this on pages 12 and 14., and by revising language about “place cells” throughout.</p><p>As in the original paper, the revised paper reports how many calcium traces of each type were extracted during all imaging sessions in all rats (Figure 3E, Figure 5B, bottom). Again, we do not count or report the number of “place cells” since we did not define a decision boundary for classifying whether or not a given calcium trace originated from a “place cell.”</p><p>To compute the “size of the place fields” as requested by the referees, despite not classifying calcium traces into place versus non place cells, we derived a ‘calcium trace activity range’ (CTAR, defined as total distance over which tuning curve exceeds ⅔ of its peak value) measure for each calcium trace in the dataset (Panel E in Supplement to Figure 3). These distributions show that CF traces were active over a significantly larger region of the track than other trace types, consistent with the fact that CF traces could combine fluorescence from more than one CA1 cell.</p><disp-quote content-type="editor-comment"><p>Related to that, it is crucial to provide a plot of the accuracy of spatial decoding against spatial location to appreciate the performance of the system in real conditions, knowing that in linear tracks place fields are not homogeneously distributed.</p></disp-quote><p>We agree that this is very important, which is why we included such plots in both the original and revised manuscripts. In the revised manuscript, mean decoding accuracy in each of the 24 position bins is shown in the polar plots on the left side of Figure F. Separate plots are shown for decoders trained on offline, CB, CF, and CB+ traces.</p><disp-quote content-type="editor-comment"><p>3) Improve the description of the methodology and of imaging speed with respect to displacement speed according to referee 2's comments. It is particularly important to explain how much exploration time is needed to train the real-time classifier.</p></disp-quote><p>Concerning “imaging speed with respect to displacement speed” see reply to comment #7 below. Concerning “how much exploration time is needed to train the real time classifier” Figure 3G of the revised manuscript shows how position decoding accuracy improves as a function of the size of the training dataset; it can be seen that decoding accuracy asymptotes when the training set reaches about 300 frames in size. Additional discussion of this issue is now provided on page 5 of the main text in the paragraph on ‘Step 2’ of the real-time experiment.</p><disp-quote content-type="editor-comment"><p>4) Provide an evaluation of the localisation information available in corrected motion artefacts in order to rule out that residual motion artefacts do not provide spatial information.</p></disp-quote><p>Regardless of whether analysis is done offline or online, any calcium imaging and decoding experiment is vulnerable to two potential problems arising from motion artifact:</p><p>Problem #1. Image motion can generate noise in calcium signals that disrupts the accuracy of decoding.</p><p>Problem #2. Image motion that is correlated with behavior can convey uncontrolled information that allows the decoder to learn predictions from image motion rather than calcium signals.</p><p>Very few published in-vivo calcium imaging experiments provide adequate controls for these two possible sources of artifact (again, such controls are just as necessary for offline as for online experiments). In response to the referee comments, we have provided controls for these confounds in our performance tests of DeCalciOn’s online decoding capabilities.</p><p>Figure 4B of the revised paper shows that without online motion correction, several rats in the linear track experiment show a significant correlation between position error and motion artifact (indicated by positive values on the y-axis); hence, motion artifact impairs decoding of position on the linear track in these rats (problem #1 above). This correlation between motion artifact and decoding error is reduced or eliminated by online motion correction (as indicated by values near zero on the x-axis), demonstrating that online motion correction helps to prevent motion artifact from impairing the accuracy of decoding.</p><p>Figure 6 of the revised paper shows that during an operant touchscreen experiment, motion artifact occurs preferentially during specific behaviors such as visiting the food magazine (reward retrieval, Figure 6A) or touching the screen to make a response (correct choice, Figure 6B). When motion correction is not used (top graphs in Figures 6C-F), the average motion artifact is higher during frames when the decoder accurately predicts behavior than during frames when the decoder fails to predict behavior; hence, motion artifact appears to improve the accuracy of predicting these behaviors (problem #2 above). When motion correction is used, the average motion artifact no longer differs for correctly versus incorrectly decoded frames (except in one case, bottom right graph of Figure 6E), indicating that motion correction helps to prevent the decoder from learning to predict behavior from motion artifact.</p><disp-quote content-type="editor-comment"><p>5) Repeat the experiment in a more complex environment, such as a 2D arena, in order to demonstrate the versatility of the method.</p></disp-quote><p>We agree that versatility is important, because DeCalciOn is intended for use by a wide range of experimenters studying neural activity during various kinds of behaviors. That being the case, a shortcoming of the original manuscript was that it only reported performance testing for position decoding from CA1 neurons and no other behaviors, perhaps conveying the false impression that the system is specialized for position decoding.</p><p>Adding yet more position decoding examples (e.g., in a 2D environment as some referees requested) might only serve to reinforce inaccurate impressions that our system is specialized for position decoding. So rather than adding more examples of position decoding, we have diversified our performance testing by presenting results for decoding calcium activity from a different brain region (OFC rather than CA1) during a different kind of behavior (an instrumental touchscreen task rather than a linear track). These performance tests are reported in a new subsection of the Results titled “Decoding instrumental behavior from orbitofrontal cortex neurons” (pages 21-25) and in new figures (Figures 5,6).</p><disp-quote content-type="editor-comment"><p>6) Provide a direct comparison of real performances based DeCalciOn to two previous online calcium imaging algorithms. The technical innovations of this work would be better highlighted by directly testing all three of these algorithms, ideally on similar datasets.</p></disp-quote><p>We understand the logic of this request, but it is unfeasible for several reasons. One of the two cited online algorithms (ref #14) is supported by public resources on Github (similar to the public resources we have made available for our own system), but as noted in our revised Discussion section (pages 33-34), that system is designed for compatibility with benchtop 2P microscopes so we would have to engineer a solution for interfacing it with miniscopes to carry out the requested performance comparisons; we lack the time and resources for such an enterprise. The other cited online algorithm (ref #25) is supposed to be compatible with UCLA Miniscopes, but public resources for this algorithm are not available. Other cited online algorithms (refs #6,#11,#26) were not performance tested in hardware at all.</p><p>We very much appreciate and respect all of the referee’s critiques, and have made diligent efforts to address most of them in revision. We will respect the final decision to accept or reject our revised paper on its scientific merits. We ask that as the referees and editors consider their decision, please take into account our view that comment #6 asks us to meet standards not met by prior cited publications (namely, direct hardware performance comparisons with other systems that either have never been implemented in hardware, are incompatible with our hardware, or are not publicly available).</p><disp-quote content-type="editor-comment"><p>7) Provide a quantification of the spatial error (across different locations, and running speeds) made in a virtual experiment in which the goal is to emit a TTL signal at a specific location of the animal. The difference with point 7 is that this quantification includes not only classifier inaccuracies but also the time delay of the real time device in relation to the speed of the animal.</p></disp-quote><p>We have addressed this by providing a much more thorough accounting of the latency between detection of photons at the miniscope sensor and generating of the TTL output pulse from the decoder (Figure 7 and subsection titled “Real time decoding latency” on pages 25-30 of the revised manuscript). As explained at the top of page 30:</p><p>“if we write x(t) to denote the position a rat occupies at the moment when a calcium trace vector arrives at the sensor, and x(t+tau_F) to denote the position the rat later occupies at the moment when TTL feedback is generated from the decoded calcium trace vector, the maximum distance error that can accrue between detection and decoding of the trace vector is δ_x=|x(t)-x(t+tau_F)| = 7.5 cm.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>– Provide a clear estimate of position accuracy that can be achieved with a space metric and not with a classifier accuracy with space bins whose size is unclear.</p></disp-quote><p>This issue was addressed in response to essential revision #1a.</p><disp-quote content-type="editor-comment"><p>– Provide measurements in 2D and estimates of both location and head direction. How does it compare with electrophysiology? Are the conclusions about the lack of importance of cell body identification still holding for these conditions?</p></disp-quote><p>As noted in our reply to reviewer #1’s public comments, maximizing the system’s spatial position decoding capabilities was a lower priority design goal than rapidly disseminating a system with versatile capabilities for short latency decoding of various behaviors. To better demonstrate versatility, we have added a new section to the Results that shows categorical classification of behaviors during an instrumental touchscreen task (see above response to Essential Revision #5).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– There is not enough information about the behavior. Please provide a spatial distribution of speed, acceleration, and arrests. Is there an influence on the decoding accuracy?</p></disp-quote><p>Distributions of speed, acceleration, and arrests are now provided in panel C of the Supplement to Figure 3.</p><disp-quote content-type="editor-comment"><p>– There is not enough information about the place cells. How many place cells? What is the spatial information of the cells? What are the size of the place fields?</p></disp-quote><p>Distributions of spatial stability scores and calcium trace activity ranges (CTAR) are now provided in panels D,E of the Supplement to Figure 3.</p><disp-quote content-type="editor-comment"><p>– Some details are lacking or are not explicitly described. For instance, I think the following information should be easier to obtain in the article. The 2.5m linear track is split into 13 regions, leading to regions of roughly 20cm.</p></disp-quote><p>This information is now clearly stated in the main text and in the caption for Figure 3A.</p><disp-quote content-type="editor-comment"><p>In linear tracks rats run with an average speed of 70cm.s<sup>-1</sup>, with maximum speed going up to 100cm.s<sup>-1</sup>. This means that running rats will stay between 280ms to 400ms in each zone. The video frames are down-sampled to 15 frames/s leading to 66ms between each frame. This implies that 4 to 6 frames will correspond to the same location.</p></disp-quote><p>The original methods section erroneously stated that video frames were downsampled to 15 frames/s, due to a copy-paste error from a description of offline analysis methods we have used in other experiments. We thank the referee for catching this error. It has now been corrected, and a detailed description of how far the rat can travel in a single frame (~7.5 cm) is provided in the main text at the top of page 29.</p><disp-quote content-type="editor-comment"><p>– A clearer description of the methodology might help the reader:</p><p>Here is a short overview of the methodology:</p><p>Technological methodology:</p><p>Step 1: Correct for translational movement of brain tissue. Aa 128x128 pixel area with distinct anatomical features was selected within the 512x512 cropped subregion to serve as a motion stabilization window;</p><p>Step 2: ACTEV removes background fluorescence from the 512x512 image;</p><p>Step 3: ROI definition: The enhanced image then is filtered through a library of up to 1,024 binary pixel masks (each up to 25x25 in size) that define ROIs within fluorescence is summed to extract calcium traces;</p><p>Step 4: Decoding. Population vectors of extracted calcium trace values are stored in the DRAM, and then decoded by sending them as inputs to a linear classifier running on the MPSoC's ARM core.</p><p>Experimental methodology:</p><p>Step 1: collect an initial imaging dataset and store it on the host PC;</p><p>Step 2: pause for an intermission to identify cell contours if necessary (this is only required for contour-based decoding; see below) and train a linear classifier to decode behavior from the initial dataset;</p><p>Step 3: upload classifier weights from the host PC to the Ultra96;</p><p>Step 4: perform real-time decoding with the trained classifier.</p></disp-quote><p>We have followed the referee’s advice to structure the methodological description in this way, except the order of technological and experimental methodology reversed. The first subsection of the Results (titled ‘Steps of a real time imaging session’) and panel B of Figure 1 provide a detailed description of the experimental methodology (note that for additional clarity we now break the session down into 5 steps instead of 4), and the second subsection of the Results (titled ‘Real time image processing pipeline’) provides a detailed description of the technical methodology.</p></body></sub-article></article>