<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103512</article-id><article-id pub-id-type="doi">10.7554/eLife.103512</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103512.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Barcode activity in a recurrent network model of the hippocampus enables efficient memory binding</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Fang</surname><given-names>Ching</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3653-0057</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0930-7327</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa1">‡</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Abbott</surname><given-names>Larry F</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Aronov</surname><given-names>Dmitriy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3717-2477</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Chettih</surname><given-names>Selmaan N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2045-3747</contrib-id><email>selmaan.chettih@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute at Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">Chevy Chase</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>‡</label><p>Anthropic, New York, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>08</day><month>01</month><year>2026</year></pub-date><volume>14</volume><elocation-id>RP103512</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-09-25"><day>25</day><month>09</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-09-18"><day>18</day><month>09</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.09.612073"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-09"><day>09</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103512.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-12-12"><day>12</day><month>12</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103512.2"/></event></pub-history><permissions><copyright-statement>© 2025, Fang, Lindsey et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fang, Lindsey et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103512-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-103512-figures-v1.pdf"/><abstract><p>Forming an episodic memory requires binding together disparate elements that co-occur in a single experience. One model of this process is that neurons representing different components of a memory bind to an ‘index’ — a subset of neurons unique to that memory. Evidence for this model has recently been found in chickadees, which use hippocampal memory to store and recall locations of cached food. Chickadee hippocampus produces sparse, high-dimensional patterns (‘barcodes’) that uniquely specify each caching event. Unexpectedly, the same neurons that participate in barcodes also exhibit conventional place tuning. It is unknown how barcode activity is generated, and what role it plays in memory formation and retrieval. It is also unclear how a memory index (e.g. barcodes) could function in the same neural population that represents memory content (e.g. place). Here, we design a biologically plausible model that generates barcodes and uses them to bind experiential content. Our model generates barcodes from place inputs through the chaotic dynamics of a recurrent neural network and uses Hebbian plasticity to store barcodes as attractor states. The model matches experimental observations that memory indices (barcodes) and content signals (place tuning) are randomly intermixed in the activity of single neurons. We demonstrate that barcodes reduce memory interference between correlated experiences. We also show that place tuning plays a complementary role to barcodes, enabling flexible, contextually appropriate memory retrieval. Finally, our model is compatible with previous models of the hippocampus as generating a predictive map. Distinct predictive and indexing functions of the network are achieved via an adjustment of global recurrent gain. Our results suggest how the hippocampus may use barcodes to resolve fundamental tensions between memory specificity (pattern separation) and flexible recall (pattern completion) in general memory systems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>chickadee</kwd><kwd>recurrent neural network</kwd><kwd>chaos</kwd><kwd>memory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0290hax27</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Fang</surname><given-names>Ching</given-names></name><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Abbott</surname><given-names>Larry F</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00kztt736</institution-id><institution>The Kavli Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Fang</surname><given-names>Ching</given-names></name><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Abbott</surname><given-names>Larry F</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI-1707398</award-id><principal-award-recipient><name><surname>Fang</surname><given-names>Ching</given-names></name><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><name><surname>Abbott</surname><given-names>Larry F</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05h1kgg64</institution-id><institution>NIH Office of the Director</institution></institution-wrap></funding-source><award-id>DP2-AG071918</award-id><principal-award-recipient><name><surname>Aronov</surname><given-names>Dmitriy</given-names></name><name><surname>Chettih</surname><given-names>Selmaan N</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01s5ya894</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>1K99NS136846</award-id><principal-award-recipient><name><surname>Chettih</surname><given-names>Selmaan N</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01bj3aw27</institution-id><institution>U.S. Department of Energy</institution></institution-wrap></funding-source><award-id>DE-SC0020347</award-id><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The representation of individual memories in a recurrent neural network can be efficiently differentiated using chaotic recurrent dynamics.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans and other animals draw upon memories to shape their behaviors in the world. Memories of specific personal experiences — called episodic memories (<xref ref-type="bibr" rid="bib54">Tulving, 1972</xref>) — are particularly important for livelihood. In animals, episodic-like memory is operationally defined as the binding of the ‘where’, ‘what’, and ‘when’ components that comprise a single experience. This information can later be retrieved from memory to affect behavior flexibly, depending on context (<xref ref-type="bibr" rid="bib10">Clayton and Dickinson, 1998</xref>). The binding of memory contents into a discrete memory is thought to occur in the hippocampus. Previous work proposed that the hippocampus supports memory by generating an ‘index’, that is, a signal distinct from the contents of a memory (<xref ref-type="bibr" rid="bib51">Teyler and DiScenna, 1986</xref>; <xref ref-type="bibr" rid="bib52">Teyler and Rudy, 2007</xref>). In this scheme, during memory formation, plasticity links the neurons that represent memory contents with the neurons that generate this index. Later, re-experience of partial memory contents may reactivate the index, and reactivation of the index drives complete recall of the memory contents.</p><p>Indexing theory was originally articulated at an abstract level, without reference to particular neural representations (<xref ref-type="bibr" rid="bib51">Teyler and DiScenna, 1986</xref>). More recently, signatures of index signals were identified in neural activity through experiments in food-caching chickadees (<italic>Poecile atricapillus</italic>), an influential animal model of episodic memory (<xref ref-type="bibr" rid="bib46">Sherry, 1984</xref>). <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> identified ‘barcode’-like activity in the chickadee hippocampus during memory formation and suggested that barcodes function as memory indices. Barcodes are sparse, high-dimensional patterns of hippocampal activity that occur transiently during caching. They are unique to each cache and are uncorrelated between cache sites, even for nearby sites with similar place tuning. Barcodes are then reactivated when a bird retrieves the cached item. Chickadee hippocampus also encodes the bird’s location — as expected, given the presence of place cells — as well as the presence of a cached sunflower seed, irrespective of location. Thus, <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> found that hippocampal activity contains both putative memory indices (in the form of barcodes) and putative memory content (in the form of place and seed-related activity).</p><p>These findings raise several critical questions. How are barcodes generated and associated with place and seed-related activity during caching? How can hippocampal dynamics subsequently recall these same patterns during retrieval? Critically, neurons participate in both barcodes and place codes with near-random overlap, in contrast with theoretical models where content and indexing functions occur in separate neurons (<xref ref-type="bibr" rid="bib28">Krotov and Hopfield, 2016</xref>; <xref ref-type="bibr" rid="bib8">Bricken and Pehlevan, 2021</xref>; <xref ref-type="bibr" rid="bib55">Tyulmankov et al., 2021</xref>; <xref ref-type="bibr" rid="bib57">Whittington et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Kozachkov et al., 2023</xref>). It is unclear at a computational level how memory index and content signals can be functionally distinct when they coexist in the same network and even in the activities of single neurons.</p><p>In this paper, we use the findings of <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> to guide the design of a biologically plausible recurrent neural network (RNN) for cache memory formation and recall. The model generates barcodes and associates them with memory content in the same neural population. At recall time, the model can flexibly adjust the spatial scale of its memory retrieval, ranging from site-specific information to search of an extended area, depending on contextual demands. Using this model, we demonstrate the computational advantages of barcode-mediated memory by showing that it reduces interference between similar memories. We also show that place and barcode activities in the model play complementary roles in enabling memory specificity and flexible recall.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Barcode representations in the hippocampus are observed during caching</title><p>We first review the key experimental results in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>. Black-capped chickadees were placed in a laboratory arena comprised of a grid of identical sites. Each site had a perch to land on and a hidden compartment, covered by a flap, where a seed could be hidden. Chickadees were allowed to behave freely in this environment and collect sunflower seeds from feeders. A chickadee often visited sites without interacting with the hidden compartment (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) but, at other times, the chickadee cached a seed into the compartment, or retrieved a previously cached seed (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Previous experiments demonstrated that chickadees remember the precise sites of their caches in this behavioral paradigm (<xref ref-type="bibr" rid="bib2">Applegate and Aronov, 2022</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Hippocampal activity during food-caching reveals sparse ‘barcodes’ for each cache memory.</title><p>(<bold>A</bold>) A black-capped chickadee visits cache sites in a laboratory arena. The chickadee does not interact with cache sites during what we call a visit. (<bold>B</bold>) A chickadee caches or retrieves at a cache site by peeling back the rubber flap at the site. This reveals a hidden compartment where seeds can be stored. (<bold>C</bold>) Cartoon example of spiking activity during visits of four hippocampal neurons in a square arena. Spikes are in red. Gray diamonds indicate the location of sites with caches. (<bold>D</bold>) As in (<bold>C</bold>), for the same cells during caching and retrieval. Neurons fire sparsely and randomly during caches (activity clusters at sites) independent of their place tuning. (<bold>E</bold>) Correlation of population activity during visits to the same or different sites of increasing distance. Values are max-normalized. Reproduced with permission from <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>. (<bold>F</bold>) As in (<bold>E</bold>), but comparing activity from caches at a site to activity from retrievals at the same or different sites. Barcode activity shared between caches and retrievals at the same site produces a sharp deviation from smooth spatial tuning. Values are normalized by the same factor as in (<bold>E</bold>). Reproduced with permission from <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig1-v1.tif"/></fig><p><xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> recorded hippocampal population activity during these behaviors. When chickadees visited sites, place cells were observed, similar to those previously found in birds and mammals (<xref ref-type="bibr" rid="bib41">Payne et al., 2021</xref>; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Place cells did not change their spatial tuning after a cache. Instead, during caching and retrieval, neurons transiently displayed memory-related activity. During caching, neurons fired sparsely, with individual neurons producing large activity bursts for a small subset of caches at seemingly random locations (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). These bursts occurred in both place cells and non-place cells, with the location of cache bursts unrelated to a neuron’s place tuning. At the population level, activity during a cache consisted of both typical place activity and a cache-specific component orthogonal to the place code, termed a “barcode”. Strikingly, barcode activity for a particular cache reactivated during later retrieval of that cache. These findings were evident when examining the correlation between population activity vectors for visits, caches, and retrievals at different sites. When comparing two visits, the correlation profile decayed smoothly with distance, as expected from place tuning (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). When comparing caching with retrieval, a similar smooth decay was observed for most distances, indicating the presence of place tuning. However, there was a substantial boost in correlation for caches and retrievals at the exact same site (<xref ref-type="fig" rid="fig1">Figure 1F</xref>). This site-specific boost resulted from reactivation of the cache barcode during subsequent retrieval.</p><p>Barcode activity during caching and retrieval, which are moments of memory formation and recall, suggests a mechanism supporting episodic memory. We hypothesize that the hippocampus generates a sparse, high-dimensional pattern of activity transiently during the formation of each memory, and that this serves as a unique index to which the contents of the memory are bound. Reactivation of the barcode at a later time drives the recall of the associated memory contents. In the food caching behavior studied by <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>, the contents of memory include the location (‘where’) and presence of a seed (‘what’). Below, we implement this hypothesis in a computational model.</p></sec><sec id="s2-2"><title>Generating barcode activity with random recurrent dynamics</title><p>We model the hippocampus as a recurrent neural network (RNN; <xref ref-type="bibr" rid="bib1">Alvarez and Squire, 1994</xref>; <xref ref-type="bibr" rid="bib53">Tsodyks, 1999</xref>; <xref ref-type="bibr" rid="bib21">Hopfield, 1982</xref>) and propose that recurrent dynamics can generate barcodes from place inputs (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). As in experiments, the model’s population activity during a cache should exhibit both place and barcode activity components. The network receives place inputs termed <inline-formula><alternatives><mml:math id="inf1"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft1">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> which are spatially correlated as follows. Each neuron in the RNN receives inputs that are maximal at one location in the environment and decay with distance from that location, causing the neuron to have a single place field centered at that location. Across neurons, place fields uniformly span the environment. The firing rate activity of RNN units is denoted as <inline-formula><alternatives><mml:math id="inf2"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft2">\begin{document}$\vec{x}$\end{document}</tex-math></alternatives></inline-formula>. The recurrent weights of the model are given by <inline-formula><alternatives><mml:math id="inf3"><mml:mi>J</mml:mi></mml:math><tex-math id="inft3">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula>, and the RNN activity <inline-formula><alternatives><mml:math id="inf4"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft4">\begin{document}$\vec{x}$\end{document}</tex-math></alternatives></inline-formula> follows standard dynamics equations:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle \newcommand{\pat}[1]{} \vec{x} = ReLU(\vec{v})$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle \newcommand{\pat}[1]{} \frac{d\vec{v}}{dt} = -g(\vec{x})\vec{v} + J\vec{x} + \vec{p}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf5"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft5">\begin{document}$\vec{v}$\end{document}</tex-math></alternatives></inline-formula> is the voltage signal of the RNN units and <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$g(\cdot)$\end{document}</tex-math></alternatives></inline-formula> is a leak rate that depends on the average activity of the full network, representing a form of global shunting inhibition that normalizes network activity to prevent runaway excitation from recurrent dynamics. In our simulations, we simplify the task by using a 1D circular environment binned into 100 discrete spatial ‘states’. We set the spatial correlation length scale of place inputs such that the smallest distance between cache sites in the experiments of <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> is equal to 8 of these states (see Methods for more details).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A recurrent neural network generates barcode activity through recurrent dynamics.</title><p>(<bold>A</bold>) Diagram of a RNN; the activity of the network units is denoted as <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$\vec{x}$\end{document}</tex-math></alternatives></inline-formula>. Place information arrives from an input layer with activities <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula>. Recurrent weights are initialized randomly, that is, <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$J_{ij}\sim\mathcal{N}(\frac{\mu}{N_{x}},\frac{\sigma^{2}}{N_{x}})$\end{document}</tex-math></alternatives></inline-formula> for the synapse connecting neuron <inline-formula><alternatives><mml:math id="inf10"><mml:mi>j</mml:mi></mml:math><tex-math id="inft10">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula> to neuron <inline-formula><alternatives><mml:math id="inf11"><mml:mi>i</mml:mi></mml:math><tex-math id="inft11">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf12"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft12">\begin{document}$N_{x}$\end{document}</tex-math></alternatives></inline-formula> is the number of RNN neurons. (<bold>B</bold>) Correlation of activity vectors across different locations, when RNN weights are initialized with <inline-formula><alternatives><mml:math id="inf13"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft13">\begin{document}$\sigma=1$\end{document}</tex-math></alternatives></inline-formula> (left), <inline-formula><alternatives><mml:math id="inf14"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:math><tex-math id="inft14">\begin{document}$\sigma=7$\end{document}</tex-math></alternatives></inline-formula> (center), and <inline-formula><alternatives><mml:math id="inf15"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math><tex-math id="inft15">\begin{document}$\sigma=20$\end{document}</tex-math></alternatives></inline-formula> (right). We show correlation of place inputs (gray) and correlation of the RNN’s rate vector at <inline-formula><alternatives><mml:math id="inf16"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><tex-math id="inft16">\begin{document}$t=100$\end{document}</tex-math></alternatives></inline-formula> (black). X-axis is in units of site distance (see Methods for definition). (<bold>C</bold>) Response of RNN units when simulating a visit to a location halfway around the circular track (with <inline-formula><alternatives><mml:math id="inf17"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft17">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>; <xref ref-type="disp-formula" rid="equ3">equation 3</xref>). In gray is the activity of the RNN at <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$t=1$\end{document}</tex-math></alternatives></inline-formula>. In black is the activity at <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$t=100$\end{document}</tex-math></alternatives></inline-formula> RNN units that are uniformly subsampled and sorted by the tuning of their inputs for plotting purposes. (<bold>D</bold>) As in (<bold>C</bold>), but for <inline-formula><alternatives><mml:math id="inf20"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft20">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>. RNN activity is more sparsely distributed, including high activity in neurons without place tuning for the current location. For visualization purposes, 50 RNN units with nonzero activity and 50 RNN units with 0 activity are sampled at this time point (<inline-formula><alternatives><mml:math id="inf21"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><tex-math id="inft21">\begin{document}$t=100$\end{document}</tex-math></alternatives></inline-formula>) for display, and responses greater than 1 are clipped to &gt; 1. (<bold>E</bold>) RNN activity in response to state 1 compared to state 2, when <inline-formula><alternatives><mml:math id="inf22"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft22">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>. Each point corresponds to a single RNN unit. (<bold>F</bold>) As in (<bold>E</bold>), but for <inline-formula><alternatives><mml:math id="inf23"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft23">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>. RNN activity for these neighboring states is substantially decorrelated by recurrence. (<bold>G</bold>) Firing fields of three example units on the circular track displaying place-cell activity. Maximum value of each field is labeled and the colormap is max-normalized. (<bold>H</bold>) Simulated spike counts of a RNN population during visits to each location on the circular track. Spikes are generated by simulating Poisson-distributed counts from underlying unit rates. Place tuning results in a strong diagonal structure when units are sorted by their input’s preferred location. The maximum limit of the colormap is set to the 99th percentile value of spike counts (≥3 spikes). (<bold>I</bold>) Same neurons as (<bold>G</bold>), but for <inline-formula><alternatives><mml:math id="inf24"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft24">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula> with units now showing barcode activity. The location of the <inline-formula><alternatives><mml:math id="inf25"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft25">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula> place field peak of each unit (i.e. its corresponding peak in (<bold>G</bold>)) is marked by a red triangle. (<bold>J</bold>) As in (<bold>H</bold>), but for <inline-formula><alternatives><mml:math id="inf26"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft26">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>. The independence of barcode activity for neighboring sites results in a matrix with reduced diagonal structure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Convergence of RNN dynamics and additional unit activity examples.</title><p>(<bold>A</bold>) Difference between RNN activity across each step of recurrent dynamics. Specifically, at recurrent timestep <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>, we plot <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$||\vec{x}_{t}-\vec{x}_{t-1}||$\end{document}</tex-math></alternatives></inline-formula>. Each line corresponds to a different random seed and model initialization. (<bold>B</bold>) Difference between RNN activity at some step of recurrent dynamics and the initial RNN state. Specifically, at recurrent timestep <inline-formula><alternatives><mml:math id="inf29"><mml:mi>t</mml:mi></mml:math><tex-math id="inft29">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>, we plot <inline-formula><alternatives><mml:math id="inf30"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><tex-math id="inft30">\begin{document}$||\vec{x}_{t}-\vec{x}_{0}||$\end{document}</tex-math></alternatives></inline-formula>. Each line corresponds to a different random seed and model initialization. (<bold>C</bold>) As in <xref ref-type="fig" rid="fig2">Figure 2GI</xref>, but for 20 additional randomly sampled units.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Feedforward model comparison for barcode generation.</title><p>(<bold>A</bold>) Diagram of the feedforward model for barcode generation. Place inputs <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> are passed through an expansion layer <inline-formula><alternatives><mml:math id="inf32"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft32">\begin{document}$W_{h}$\end{document}</tex-math></alternatives></inline-formula> with a nonlinear activation function to get the hidden layer activity <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$x_{h}$\end{document}</tex-math></alternatives></inline-formula>. A subsequent compression layer <inline-formula><alternatives><mml:math id="inf34"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft34">\begin{document}$W_{o}$\end{document}</tex-math></alternatives></inline-formula> with a nonlinear activation function generates the final barcode activity <inline-formula><alternatives><mml:math id="inf35"><mml:mi>x</mml:mi></mml:math><tex-math id="inft35">\begin{document}$x$\end{document}</tex-math></alternatives></inline-formula>. (<bold>B</bold>) As in <xref ref-type="fig" rid="fig2">Figure 2B</xref> but adding in red the results for the feedforward model as sparsity is varied. (<bold>C</bold>) As in <xref ref-type="fig" rid="fig2">Figure 2GI</xref>, but for 16 randomly sampled units from the feedforward model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We initialize the recurrent weights from a random Gaussian distribution <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>J</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$J\sim\mathcal{N}(\frac{\mu}{N_{x}},\frac{\sigma^{2}}{N_{x}})$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf37"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft37">\begin{document}$N_{x}$\end{document}</tex-math></alternatives></inline-formula> is the number of RNN neurons and <inline-formula><alternatives><mml:math id="inf38"><mml:mi>μ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft38">\begin{document}$\mu \lt 0$\end{document}</tex-math></alternatives></inline-formula>, reflecting global subtractive inhibition that encourages sparse network activity to match experimental findings <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>. We first consider network activity before any learning-related changes. For the range of parameters we use, the network is in a chaotic state with a roughly constant overall level of activity but fluctuations in the activities of individual units. From an initial state of 0, we run recurrent dynamics until this steady-state level of overall activity has been achieved (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A, B</xref>). The chaotic recurrent dynamics induced by the random weights (<xref ref-type="bibr" rid="bib47">Sompolinsky et al., 1988</xref>) effectively scrambles incoming place inputs. We demonstrate this by measuring the correlation of RNN activity across different locations and plotting this correlation as a function of distance (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Place inputs show a smoothly decaying correlation curve. At low values of <inline-formula><alternatives><mml:math id="inf39"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft39">\begin{document}$\sigma$\end{document}</tex-math></alternatives></inline-formula>, the network is primarily input-driven, showing a smoothly decaying correlation matching inputs. At high values of <inline-formula><alternatives><mml:math id="inf40"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft40">\begin{document}$\sigma$\end{document}</tex-math></alternatives></inline-formula>, recurrence is so strong that it entirely eliminates the spatial correlation of nearby sites: activity at each state is decorrelated from activity at all other states.</p><p>Interestingly, at an intermediate level of <inline-formula><alternatives><mml:math id="inf41"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:math><tex-math id="inft41">\begin{document}$\sigma=7$\end{document}</tex-math></alternatives></inline-formula>, the network retains elements from both high and low recurrence regimes (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The network exhibits a smoothly decaying correlation curve reflecting its inputs, but each state’s activity also contains a strong decorrelated component, apparent in the large drop in correlations for nearby but non-identical sites. This intermediate network regime closely resembles spatial correlation profiles observed during caching (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>; <xref ref-type="fig" rid="fig1">Figure 1F</xref>). The smoothly decaying component is caused by the place code, whereas the sharp peak at zero distance – reflecting barcode activity – is caused by the recurrent dynamics. We thus construct networks using intermediate <inline-formula><alternatives><mml:math id="inf42"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft42">\begin{document}$\sigma$\end{document}</tex-math></alternatives></inline-formula> values, which allow for the coexistence of place code and barcode components in population activity.</p><p>A key result from <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> is that the hippocampus exhibits both place code and barcode activity during caching and retrieval, but only place activity during visits. We propose that this effect can result from a network in which recurrent strength is flexibly modulated. In a low-recurrence condition, the network produces the place code, whereas in a high-recurrence condition, the same network produces a combination of place code and barcode activity. To simulate this in our model, we modify <xref ref-type="disp-formula" rid="equ2">equation 2</xref> to<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>J</mml:mi><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle \newcommand{\pat}[1]{} \frac{d\vec{v}}{dt} = -g(\vec{x})\vec{v} + rJ\vec{x} + \vec{p}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the newly included <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$r\in\{0,1\}$\end{document}</tex-math></alternatives></inline-formula> scales recurrent strength such that the network may be in a feedforward (<inline-formula><alternatives><mml:math id="inf44"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft44">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>) or recurrent regime (<inline-formula><alternatives><mml:math id="inf45"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft45">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>). During visits, we assume the network is operating with low recurrent strength (<inline-formula><alternatives><mml:math id="inf46"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft46">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>). As a result, the activity in the RNN exhibits the spatial profile of its place inputs. We verify this is the case by visualizing early and late RNN activity given a place input (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). With low recurrence, early and late RNN activity have similar spatial profiles, and late activity patterns for nearby states are highly correlated (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). In contrast, when recurrence is enabled (<inline-formula><alternatives><mml:math id="inf47"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft47">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>), network activity is sparse with a heavy tail of a few strongly active neurons and is decorrelated between nearby states (<xref ref-type="fig" rid="fig2">Figure 2D and F</xref>). These changes match experimental observations of excitatory neurons during food caching (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>).</p><p>We visualized neural activity during visits (<inline-formula><alternatives><mml:math id="inf48"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft48">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>) and caches (<inline-formula><alternatives><mml:math id="inf49"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft49">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>). During visits, RNN units display ordinary place tuning (<xref ref-type="fig" rid="fig2">Figure 2G</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). We also simulated spikes for all units at each location and visualized this as a matrix where units were sorted by the preferred location of their inputs (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). This matrix exhibits a diagonal structure reflecting strong spatial correlations. During caches, single neurons develop sparse, scrambled spatial responses relative to their place tuning (<xref ref-type="fig" rid="fig2">Figure 2I</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). Accordingly, simulated spikes across the population have greater random, off-diagonal components during caching than during visits. Thus, recurrence in a randomly connected RNN is a simple and effective mechanism to generate barcode signals from spatially correlated inputs. We alternatively considered a feedforward mechanism to generate barcodes, in which barcodes are computed by a separate feedforward pathway (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). We found the feedforward mechanism required an unreasonably large number of neurons and sparsity levels to match the decorrelation level of the recurrent mechanism.</p></sec><sec id="s2-3"><title>Storing memories by binding content inputs with a barcode</title><p>Having suggested a mechanism for the generation of barcode representations, we next propose how such a representation can be leveraged for memory storage in a network model. In our food-caching task, we assume that the contents of a memory include the location of the cache and the identity of a food item within the cache. Thus, we add an additional input source besides spatial location into our model – an input <inline-formula><alternatives><mml:math id="inf50"><mml:mi>s</mml:mi></mml:math><tex-math id="inft50">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> representing the presence of a seed (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The ‘seed’ input projects randomly onto the RNN neurons. During caching, both place inputs and seed inputs arrive into the RNN, matching experimental findings (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>). This causes a mixed response in the network: one component of the response (place activity) is spatially tuned, another component (seed activity) indicates the presence of a seed and does not vary with location, and the third component (barcode activity) is generated by recurrent dynamics interacting with these inputs.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Barcode activity binds content to store memories in the RNN.</title><p>(<bold>A</bold>) Diagram of RNN activity during memory formation. Along with place inputs, a scalar seed input <inline-formula><alternatives><mml:math id="inf51"><mml:mi>s</mml:mi></mml:math><tex-math id="inft51">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> is provided to the model. <inline-formula><alternatives><mml:math id="inf52"><mml:mi>s</mml:mi></mml:math><tex-math id="inft52">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> connects to RNN units via 5000-dimensional weight vector <inline-formula><alternatives><mml:math id="inf53"><mml:msub><mml:mi>j</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft53">\begin{document}$j_{sx}\sim\mathcal{N}(0,1)$\end{document}</tex-math></alternatives></inline-formula>. During memory formation (i.e. when an animal caches a seed), place inputs representing the animal’s current location and a seed input are provided to the RNN. After recurrent dynamics are run for some time (<inline-formula><alternatives><mml:math id="inf54"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><tex-math id="inft54">\begin{document}$t=100$\end{document}</tex-math></alternatives></inline-formula>), the network undergoes Hebbian learning. (<bold>B</bold>) An example of memory recall in the network. The animal is at the same location as in (<bold>A</bold>). The place input encoding that location is provided to the model and results in the RNN recalling the stored attractor pattern seen in (<bold>A</bold>). C. Examples of RNN population activity during caching (left), retrieval (center), and visits (right) at three sites. During visits, the RNN has <inline-formula><alternatives><mml:math id="inf55"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft55">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>, while during caches and retrievals <inline-formula><alternatives><mml:math id="inf56"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft56">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>. For visualization purposes, 50 units are randomly sampled and displayed in the ‘Caching’ and the ‘Retrieval’ plot. (D) Correlation of Poisson-generated spikes simulated from RNN rate vectors at two sites, plotted as a function of the distance between the two sites. In black is the correlation in activity between two visits. In purple is the correlation between caching and retrieval activity. Experiments were simulated with 20 simulations where 5 sites were randomly chosen for caching and retrieval. 99% confidence intervals are calculated over the simulations and plotted over the markers. Compare to <xref ref-type="fig" rid="fig1">Figure 1E and F</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig3-v1.tif"/></fig><p>To store memories, we assume the RNN undergoes Hebbian learning after some fixed time point during the <inline-formula><alternatives><mml:math id="inf57"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft57">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula> recurrent dynamics (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). At this time, the synapse <inline-formula><alternatives><mml:math id="inf58"><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi></mml:math><tex-math id="inft58">\begin{document}$j\rightarrow i$\end{document}</tex-math></alternatives></inline-formula> changes by an amount<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle \newcommand{\pat}[1]{} \Delta J_{ij} \propto x_i x_j - \beta x_i$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$\beta \gt 0$\end{document}</tex-math></alternatives></inline-formula> provides an overall inhibition of the stored pattern. This term helps to prevent network activity from collapsing to previously stored attractors. Memory storage works as follows, following the experimentally observed sequence in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>: place inputs arrive into the RNN, recurrent dynamics generate a random barcode, seed inputs are activated, and then Hebbian learning binds a particular pattern of barcode activity to place- and seed-related activity. In this way, fixed-point attractors are formed corresponding to a conjunction of a barcode and memory content.</p><p>Memory recall in our network follows typical pattern completion dynamics, with recurrence strength set to <inline-formula><alternatives><mml:math id="inf60"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft60">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula> as for caches. As an example, consider a scenario in which an animal has already formed a memory at some location <inline-formula><alternatives><mml:math id="inf61"><mml:mi>l</mml:mi></mml:math><tex-math id="inft61">\begin{document}$l$\end{document}</tex-math></alternatives></inline-formula>, resulting in the storage of an attractor <inline-formula><alternatives><mml:math id="inf62"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft62">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula> into the RNN. The attractor <inline-formula><alternatives><mml:math id="inf63"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft63">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula> can be thought of as a linear combination of place input-driven activity <inline-formula><alternatives><mml:math id="inf64"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft64">\begin{document}$p(l)$\end{document}</tex-math></alternatives></inline-formula>, seed input-driven activity <inline-formula><alternatives><mml:math id="inf65"><mml:mi>s</mml:mi></mml:math><tex-math id="inft65">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, and a recurrent-driven barcode component <inline-formula><alternatives><mml:math id="inf66"><mml:mi>b</mml:mi></mml:math><tex-math id="inft66">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>. Later, the animal returns to the same location and attempts recall (i.e. sets <inline-formula><alternatives><mml:math id="inf67"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft67">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>, <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Place inputs for location <inline-formula><alternatives><mml:math id="inf68"><mml:mi>l</mml:mi></mml:math><tex-math id="inft68">\begin{document}$l$\end{document}</tex-math></alternatives></inline-formula> drive RNN activity towards <inline-formula><alternatives><mml:math id="inf69"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft69">\begin{document}$p(l)$\end{document}</tex-math></alternatives></inline-formula>, which is partially correlated with attractor <inline-formula><alternatives><mml:math id="inf70"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft70">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula>, and the recurrent dynamics cause network activity to converge onto attractor <inline-formula><alternatives><mml:math id="inf71"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft71">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula>. In this way, barcode activity <inline-formula><alternatives><mml:math id="inf72"><mml:mi>b</mml:mi></mml:math><tex-math id="inft72">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> is reactivated as part of attractor <inline-formula><alternatives><mml:math id="inf73"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft73">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula>, along with the place and seed components stored in the attractor state <inline-formula><alternatives><mml:math id="inf74"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft74">\begin{document}$p(l)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf75"><mml:mi>s</mml:mi></mml:math><tex-math id="inft75">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. The seed input can also affect recall, as discussed in the following section.</p><p>As an initial test of memory function in our model, we analyzed the activity patterns that are stored and recalled in the model. We simulated caching at different sites in the arena and extracted the population activity that is stored via Hebbian learning (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, left). We then simulated retrieval as an event in which the animal returns to the same site and runs memory recall dynamics (<inline-formula><alternatives><mml:math id="inf76"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft76">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>) in the RNN. Population activity during retrieval closely matches activity during caching and is substantially decorrelated from activity during visits (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). To compare our model with the empirical results reproduced in 1E,F, we ran in silico experiments with caches and retrievals at varying sites in the circular arena. We simulated Poisson-generated spikes drawn from our network’s underlying rates to match the intrinsic variability in empirical data (see Methods). We find that population activity in the RNN is more strongly correlated between caches and retrievals at the same site than two visits to a site that are not seed related (<inline-formula><alternatives><mml:math id="inf77"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft77">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>; <xref ref-type="fig" rid="fig3">Figure 3D</xref>). In addition, cache-retrieval correlations for non-identical sites rapidly drop to the level of visit-visit correlations because barcodes are orthogonal even for nearby sites. These correlation profiles closely resemble those observed in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref> (compare <xref ref-type="fig" rid="fig3">Figure 3D</xref> and <xref ref-type="fig" rid="fig1">Figure 1E, F</xref>).</p></sec><sec id="s2-4"><title>Barcode-mediated memory supports precise and flexible context-based memory recall</title><p>What are the computational benefits of barcode-mediated memory storage? We designed two behavioral tasks for our model that quantify complementary aspects of memory performance. In both tasks, we simulate a sequence of three caches in the circular arena. We then test the model’s performance during memory recall (i.e. <inline-formula><alternatives><mml:math id="inf78"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft78">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>) using two distinct tasks. The ‘Cache Presence’ task requires the network to identify the presence or absence of a cached seed at the current location (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). By evaluating the model on this task using different spacings between caches, we can measure the spatial precision of memory, that is how far apart two caches must be to prevent interference between their associated memories. The ‘Cache Location’ task requires the network to identify the cache nearest to the current location by reactivating place activity corresponding to that cache location (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). This task measures the robustness of recall and requires attractor dynamics to accurately retrieve memories from potentially distant locations. Together, these questions probe the ability of the memory system to be both specific (pattern separation) and searchable (pattern completion).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Precise and flexible recall of barcode-mediated cache memories.</title><p>(<bold>A</bold>) Cartoon of a chickadee at a site, trying to remember whether the site contains a seed. The chickadee cannot see inside the hidden compartment and must rely on memory to answer this question. (<bold>B</bold>) Cartoon of a chickadee at a site, trying to recall the location of the closest cache. In this case, the animal must use its memory to recall the location of a cache three sites away. (<bold>C</bold>) Seed output of the RNN at different locations along the circular track. Red dots above the line indicate locations where the value is greater than 0.5. Top, results after the first cache is made at a location 20% of the way through the track. The location of the first cache is marked on the x-axis as <inline-formula><alternatives><mml:math id="inf79"><mml:msub><mml:mi>C</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft79">\begin{document}$C_{1}$\end{document}</tex-math></alternatives></inline-formula>. Middle, same but after the second cache (<inline-formula><alternatives><mml:math id="inf80"><mml:msub><mml:mi>C</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft80">\begin{document}$C_{2}$\end{document}</tex-math></alternatives></inline-formula>) is made at a location 35% of the way through the track. Bottom, same but after the third cache (<inline-formula><alternatives><mml:math id="inf81"><mml:msub><mml:mi>C</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft81">\begin{document}$C_{3}$\end{document}</tex-math></alternatives></inline-formula>) is made at a location 70% of the way through the track. (<bold>D</bold>) The place output of the RNN at different locations along the track. In the heatmaps, each column shows the activities of all the output units when the animal is at a particular location (horizontal axis). The left side panels of the subfigure correspond to the model operating at low recall strength (<inline-formula><alternatives><mml:math id="inf82"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft82">\begin{document}$s=0$\end{document}</tex-math></alternatives></inline-formula>). The right side panels correspond to the model operating at high recall strength (<inline-formula><alternatives><mml:math id="inf83"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:math><tex-math id="inft83">\begin{document}$s=1.5$\end{document}</tex-math></alternatives></inline-formula>). As in (<bold>C</bold>), top, middle, and bottom plots correspond to RNN activity after caches are made at C1, C2, and C3. Place output correctly switches between the location of each cache depending on proximity to the current position. (<bold>E</bold>) Probability of correct reject rate in the model when the animal is at a location between caches 1 and 2, after all caches have been made. The X-axis shows the distance from the probed location to the surrounding caches, measured in site distance. The color of each line corresponds to the recall strength <inline-formula><alternatives><mml:math id="inf84"><mml:mi>s</mml:mi></mml:math><tex-math id="inft84">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. At low recall strengths, the model correctly identified the absence of a cache between C1 and C2, reflecting discrimination of the two cache locations. Experiments were simulated with 35 random seeds. 99% confidence intervals are calculated over the simulations and plotted as line shading. (<bold>F</bold>) Probability of recalling the location of the closest cache, given the distance of the animal from the cache. A recall is considered successful if the seed output exceeds 0.5 and if the peak of the output corresponds to the location of the nearest cache. Lines are colored by recall strength <inline-formula><alternatives><mml:math id="inf85"><mml:mi>s</mml:mi></mml:math><tex-math id="inft85">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. The model correctly recalls the locations of nearby caches, with search radius increasing as recall strength increases. Number of simulations and error shading as in (E).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Threshold selection and parameter sensitivity for seed output detection.</title><p>(<bold>A</bold>) True positive rate of the seed output in the 3-cache task of <xref ref-type="fig" rid="fig4">Figure 4</xref>, as the choice of binarized threshold is varied. The color of each line corresponds to the recall strength <inline-formula><alternatives><mml:math id="inf86"><mml:mi>s</mml:mi></mml:math><tex-math id="inft86">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. Here, caches are all spaced at least two sites apart. (<bold>B</bold>) As in (<bold>A</bold>), but for true negative rate. We will choose a threshold of 0.5 to binarize the seed output. (<bold>C-D</bold>) As in (<bold>A-B</bold>), but the order caches are made is random. (<bold>E</bold>) True positive rate of the seed output binarized at 0.5 when plasticity bias <inline-formula><alternatives><mml:math id="inf87"><mml:mi>β</mml:mi></mml:math><tex-math id="inft87">\begin{document}$\beta$\end{document}</tex-math></alternatives></inline-formula> and search strength <inline-formula><alternatives><mml:math id="inf88"><mml:mi>s</mml:mi></mml:math><tex-math id="inft88">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> is varied. (<bold>F</bold>) As in (<bold>E</bold>) but for true negative rate. (<bold>G</bold>) As in (<bold>E</bold>) but plotting the correct reject rate at a location between two caches. Here the two caches are 1.5 sites apart or closer. (<bold>H</bold>) As in (<bold>E</bold>) but plotting the probability of recalling the location of the closest cache when the animal is 2 sites away from it.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Robustness of recall to noise in dynamics and place inputs.</title><p>(<bold>A</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4F</xref>, with recall strength fixed to 0.4. We vary noise in the network dynamics, randomly drawing 0-centered Gaussian noise at every timestep and adding that to the dynamics. Lines are colored by the noise standard deviation. Note that the input signal has a variance of 1. (<bold>B</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4F</xref>, with recall strength fixed to 0.4. We vary noise in the place inputs into the model. We randomly draw 0-centered Gaussian noise, add this vector into the place inputs, and use this as input into the model. Lines are colored by the noise standard deviation. Again, the input signal has a variance of 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Model performance with realistic Gaussian place field inputs.</title><p>(<bold>A</bold>) Diagram showing how place inputs are modified to be more realistic. Place encoding <inline-formula><alternatives><mml:math id="inf89"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft89">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> is now drawn from a 0-mean Gaussian process with place-like covariance structure. This results in a more distributed representation in <inline-formula><alternatives><mml:math id="inf90"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft90">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> over spatial locations. Furthermore, <inline-formula><alternatives><mml:math id="inf91"><mml:msub><mml:mi>J</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft91">\begin{document}$J_{xp}$\end{document}</tex-math></alternatives></inline-formula> is now a random Gaussian matrix (instead of the identity matrix). (<bold>B</bold>) Correlations between place inputs at different locations, as a function of their distance. (<bold>C</bold>) As in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>, but for the Gaussian model. (<bold>D</bold>) As in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>, but for the Gaussian model. (<bold>E</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4E</xref>, but for the Gaussian model. (<bold>F</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4F</xref>, but for the Gaussian model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig4-figsupp3-v1.tif"/></fig></fig-group><p>To enable readout from the model, we add an output layer containing place and seed units. During memory formation, these units receive a copy of place and seed inputs, respectively, and undergo Hebbian plasticity (<xref ref-type="disp-formula" rid="equ4">equation 4</xref> with <inline-formula><alternatives><mml:math id="inf92"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft92">\begin{document}$\beta=0$\end{document}</tex-math></alternatives></inline-formula>) with RNN units. This enables the RNN to reactivate a copy of the inputs that were provided during memory formation. We measure the activity of the seed output to determine cache presence, and we measure place outputs to determine cache location. We note that other readout mechanisms would likely function similarly – for example, plastic feedback connectivity from recurrent onto input units. The output layer used here is not intended to correspond to a specific brain region, but simply to provide a window into what could be read out easily from network activity by downstream neural circuits.</p><p>We first show model performance on a single example of the Cache Presence and Cache Location tasks, where caches are made at the locations 20%, 35%, and 70% of the way around the circular track. For the Cache Presence task, we evaluate the model at each spatial location and plot the activity of the seed output (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). After the first cache, we see that the seed output is only high at states around the vicinity of the first cache (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, top). The false positive rate is not zero since the states around the cache also have high output. However, the width of this false positive area is less than the distance between two sites in the arena used in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>, indicating that the model is able to identify caches at the spatial resolution observed in chickadee behavior in these experiments. After a second cache, the seed output correctly reports seeds in the areas around the two caches (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, middle). Importantly, the network separates the caches from each other, correctly identifying the absence of a seed at a position between the two caches. This behavior is maintained after the addition of the final third cache (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom).</p><p>For the Cache Location task, we examine the place outputs (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, ‘Narrow Search’). When the current position is near a cache, the network correctly outputs a place field for the location of the nearest cache. However, an animal relying on memory to guide behavior may need to recall cache locations even when far away from a cache. To enable increased search radius, we make use of the seed input, which can bias network activity towards all memory patterns previously associated with this input. Activating the seed input greatly increases the range of current positions over which cache memories are retrieved (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, ‘Broad Search’). Critically, this does not cause interference between memories. For example, as the current position moves from the location of cache 1 to cache 2, place outputs discretely jump from one cache location to the other, corresponding to correct selection of the nearest cache location. The search radius during recall can thus be flexibly adjusted according to task demands, allowing the trade-off between pattern completion and pattern separation to be dynamically regulated by simple scaling of a network input.</p><p>We next systematically quantified model performance on these tasks. For these analyses, we binarized the seed output as being above or below a threshold (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A, B</xref>). When caches are sufficiently spaced, memory recall is near perfect (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A, B</xref>). This led us to evaluate the resolution of the model’s memory as caches become more closely spaced. We decreased the distance between cache 1 and 2 to identify the minimum distance where caches are separable, that is where the model correctly indicates the absence of a cache at a midpoint between caches 1 and 2. <xref ref-type="fig" rid="fig4">Figure 4E</xref> shows this correct reject rate as a function of the distance from each cache and the seed input strength. With low seed input (e.g. <inline-formula><alternatives><mml:math id="inf93"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft93">\begin{document}$s=0$\end{document}</tex-math></alternatives></inline-formula>), the correct reject rate is high when the current location is one site distance away from caches 1 and 2. In other words, if a cache is made at site 1 and site 3, our model recognizes that site 2 remains empty. This precision matches the single-site resolution measured by behavioral experiments in this arena (<xref ref-type="bibr" rid="bib2">Applegate and Aronov, 2022</xref>). As expected, performance on the Cache Presence task decreases with greater seed input strengths, as these are suited to searching over a broad spatial range.</p><p>For the Cache Location task, we measure the probability that the model outputs the location of the nearest cache. We plot this probability as a function of distance from the current location to the nearest cache (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). The model is able to correctly recall cache locations with almost perfect accuracy when it is near a cache. Performance drops sharply with distance when the seed input is low, but is substantially recovered by increasing the seed input strength. Critically, even when the search radius is broad enough to include multiple caches, attractor dynamics encourage selection of the single closest cache location rather than blending memories. Thus, the seed input strength provides a flexible search radius during the recall process. Low values of <inline-formula><alternatives><mml:math id="inf94"><mml:mi>s</mml:mi></mml:math><tex-math id="inft94">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> are more suitable for detecting the presence or absence of a seed near the current location, while high values of <inline-formula><alternatives><mml:math id="inf95"><mml:mi>s</mml:mi></mml:math><tex-math id="inft95">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> are more suitable for finding remote caches. The complementary demands of the Cache Presence and Cache Location tasks demonstrate the utility of a flexible search radius within one memory system.</p><p>We further examined how model hyperparameters affected performance on these tasks. We find that the plasticity bias <inline-formula><alternatives><mml:math id="inf96"><mml:mi>β</mml:mi></mml:math><tex-math id="inft96">\begin{document}$\beta$\end{document}</tex-math></alternatives></inline-formula> is needed to prevent erroneous memory recall at sites without caches (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E-H</xref>). Without this, recall specificity is poor and model performance suffers on the Cache Presence task. We also verified that our model is robust to the order in which caches were made (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C, D</xref>). We found that adding noise to the network’s temporal dynamics had little effect on memory recall performance (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>). However, large static noise vectors added to the network’s input and initial state decreased the overall probability of memory recall, but not its spatial profile (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2B</xref>). Finally, we extended our model to work with random, spatially correlated inputs, rather than receiving place cell-like inputs (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). This shows that our results apply regardless of the specific format of spatial inputs, which are experimentally undetermined.</p></sec><sec id="s2-5"><title>Place activity and barcode activity play complementary roles in memory</title><p>We have shown that a barcode-mediated memory system is precise yet allows flexible, content-based retrieval. Below we identify the specific contributions of place and of barcode activity by ablating either of these components in our model. To ablate barcodes (‘Place Code Only’ in <xref ref-type="fig" rid="fig5">Figure 5</xref>), we initialize our model without the random recurrent weights that produce chaotic dynamics. This is akin to running caching dynamics in ‘visit mode’, that is <inline-formula><alternatives><mml:math id="inf97"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft97">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2</xref>. To ablate place activity (‘Barcode Only’ in <xref ref-type="fig" rid="fig5">Figure 5</xref>), we eliminate spatial correlations in the network inputs. This is akin to having place fields with extremely narrow precision, rather than a spatially smooth place code. We test both ablated models on the three-cache tasks from above and compare their performance to our full model.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Ablations reveal complementary roles of place code and barcode activity in memory recall.</title><p>(<bold>A</bold>) Left, as in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom, but for the ‘Place Code Only’ model (barcode-ablated), which has recurrent weights initialized with <inline-formula><alternatives><mml:math id="inf98"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft98">\begin{document}$\mu=0,\sigma=0$\end{document}</tex-math></alternatives></inline-formula>. Right, same, but for the ‘Barcode Only’ model (place-ablated). The barcode-only model is the same as the full model, but place inputs are uncorrelated for nearby locations. (<bold>B</bold>) Left, as in <xref ref-type="fig" rid="fig4">Figure 4D</xref>, bottom, but for the place code only model. Right, the same but for the barcode-only model (for visual clarity, the matrix is downsampled to remove zero-value rows). The place code-only model has similar outputs for recall from all current locations. The barcode only model has no place output for recall from most locations. (<bold>C</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4E</xref>, but showing the full model (solid line), the place code only model (dotted line), and the barcode only model (dashed line). For all models, <inline-formula><alternatives><mml:math id="inf99"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft99">\begin{document}$s=0$\end{document}</tex-math></alternatives></inline-formula>. The place code only model is unable to discriminate between nearby caches. (<bold>D</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4F</xref>, with lines as in (<bold>C</bold>). For all models, <inline-formula><alternatives><mml:math id="inf100"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math><tex-math id="inft100">\begin{document}$s=0.4$\end{document}</tex-math></alternatives></inline-formula>. The barcode-only model recalls successfully only when the current location contains a cache. Only the full model reliably recalls remote cache locations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Performance comparison of place-code-only and barcode-only models.</title><p>(<bold>A-B</bold>) As in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A, B</xref>, but for the place code only model shown in 5 A. (<bold>C</bold>) Probability that the <italic>N</italic>th cache location is successfully recalled as site spacing is increased in the 3 cache task. Results are shown for the place code only model. (<bold>D</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4F</xref>, but for the place code only model. EFGH. As in (ABCD), but for the barcode only model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig5-figsupp1-v1.tif"/></fig></fig-group><p>The Place Code Only model directly binds memory contents of place and seed. This causes cache memories for nearby locations, where the place code is correlated, to interfere with each other. Interference is clearly visible in the performance on the Cache Presence task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left). The seed output of the place code only network is high across a wide area including caches 1 and 2 and the empty locations between them. Indeed, this network is unable to identify the absence of a cache at locations between caches 1 and 2, even with a substantial distance between them (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Furthermore, the memories for caches 1 and 2 sometimes appear to suppress memory for a distant cache 3. Further evidence of interference is apparent in the Cache Location task, where the network merges caches 1 and 2 and entirely fails to signal the cache at location 3 (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, left). This network has a low true negative rate, and often a single cache dominates the output (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A, D</xref>). Accordingly, the network is able to signal the location of a cache, but this location is often not the nearest cache (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Intuitively, without barcodes, the network is unable to distinguish individual memories at nearby spatial locations. Without this barcode-mediated competition, it forms a single agglomerated memory that is inflexibly recalled.</p><p>If correlations in inputs cause memory interference, why not do away with them entirely? The Barcode Only model shows that this is not a good option. This model performs well on the Cache Presence task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1E-G</xref>), correctly identifying the presence of all three caches and the absence of caches in other locations (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). However, the Barcode Only model fails on the Cache Location task. The model is unable to recall place fields that are not precisely at its current location (<xref ref-type="fig" rid="fig5">Figure 5B and D</xref>). With greater seed input, the model can sometimes recall memories at remote locations, but these are selected randomly with no preference for nearby caches (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1H</xref>). Intuitively, the model cannot distinguish nearby and distant caches because it lacks input correlations, which establish the measure of proximity.</p><p>In summary, place and barcode activity play complementary roles in our model. Barcode activity functions like an index for individual memories. This function supports discriminability of memories even when memory contents overlap – for example, for two nearby caches with correlated place activity. Barcodes also support selective recall of individual memories in our model via competitive attractor dynamics. However, the discriminability advantage of barcodes is only useful if they can be reactivated during memory recall. Spatially correlated place inputs (and the seed input) allow efficient memory retrieval by defining a measure of proximity in memory contents and support remote memory recall. The presence of both index and content signals in our model allows all of these functions to be performed by the same network, and for the trade-offs between them to be adjusted flexibly.</p></sec><sec id="s2-6"><title>Modulating recurrent strength allows the RNN to incorporate both predictive maps and barcode memory</title><p>We have constructed a model of a simple episodic memory, taking inspiration from hippocampal data (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>). However, the hippocampus has also been suggested to support functions beyond episodic memory. An especially influential line of prior work proposes that the hippocampus plays a role in generating predictive maps, with evidence from both experiments (including in food-caching birds; <xref ref-type="bibr" rid="bib37">Muller and Kubie, 1989</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib41">Payne et al., 2021</xref>; <xref ref-type="bibr" rid="bib3">Applegate et al., 2023</xref>) and theory (<xref ref-type="bibr" rid="bib7">Blum and Abbott, 1996</xref>; <xref ref-type="bibr" rid="bib48">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Whittington et al., 2020</xref>). This raises the question – is our model consistent with predictive map theories? And if so, how might predictive maps influence episodic memory recall?</p><p>Interestingly, prior work has shown that biologically realistic RNNs can generate predictive maps with structured recurrent weights (<xref ref-type="bibr" rid="bib12">Fang et al., 2023</xref>). Specifically, if recurrent weights encode the transition statistics of an animal’s experience, then predictive map-like activity will arise from the RNN dynamics and can be controlled by recurrent strength (analogous to the value of <inline-formula><alternatives><mml:math id="inf101"><mml:mi>r</mml:mi></mml:math><tex-math id="inft101">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="equ3">equation 3</xref>). This prediction via structured recurrent weights invites comparison to the use of random recurrent weights in our model to generate barcodes. Inspired by this connection, we considered whether a single model could generate both predictive and barcode activity via recurrent dynamics. We constructed a hybrid model, in which recurrent weights are a blend of random weights, as above, and structured, predictive weights as in <xref ref-type="bibr" rid="bib12">Fang et al., 2023</xref> (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1B</xref>). We hypothesize that network activity changes from a predictive to a chaotic regime (supporting barcodes) with increasing recurrent strength, that is as we adjust <inline-formula><alternatives><mml:math id="inf102"><mml:mi>r</mml:mi></mml:math><tex-math id="inft102">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> in the range <inline-formula><alternatives><mml:math id="inf103"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft103">\begin{document}$0 \lt r \lt 1$\end{document}</tex-math></alternatives></inline-formula>.</p><p>We use the same circular arena for our simulations, but with the assumption that animal behavior is biased, that is the animal only moves clockwise (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We visualize population activity at each site, with units sorted by their place field. When recurrent strength is at its lowest (<inline-formula><alternatives><mml:math id="inf104"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft104">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>, <xref ref-type="fig" rid="fig6">Figure 6B</xref>) network activity is place-like and reflects the current location of the animal provided in its inputs. When recurrent strength is slightly increased (<inline-formula><alternatives><mml:math id="inf105"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math><tex-math id="inft105">\begin{document}$r=0.3$\end{document}</tex-math></alternatives></inline-formula>, <xref ref-type="fig" rid="fig6">Figure 6C</xref>), the network exhibits predictive activity. That is, neural activity of recurrent units is shifted to reflect an expected future position, relative to the current spatial position encoded in inputs. This is consistent with observations from experimental data collected from the hippocampus of animals with biased movements in a linear track (<xref ref-type="bibr" rid="bib58">Wilson and McNaughton, 1993</xref>; <xref ref-type="bibr" rid="bib35">Mehta et al., 1997</xref>; <xref ref-type="bibr" rid="bib36">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib31">Lisman and Redish, 2009</xref>). Finally, when recurrent strength is maximal (<inline-formula><alternatives><mml:math id="inf106"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math><tex-math id="inft106">\begin{document}$r=1.0$\end{document}</tex-math></alternatives></inline-formula>, <xref ref-type="fig" rid="fig6">Figure 6D</xref>) we observe barcodes, i.e. sparse activity at random positions relative to inputs. We plot spatial tuning curves of a few example RNN units under low, intermediate, and high recurrent strengths (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref>). Individual units can display typical place fields (blue), skewed predictive place fields (orange), and barcode activity (purple).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Predictive coding and barcode generation are performed by the same RNN in different dynamical regimes.</title><p>(<bold>A</bold>) Cartoon of a chickadee in a circular track, running in a consistently clockwise direction. (<bold>B</bold>) Heatmap of RNN firing fields when <inline-formula><alternatives><mml:math id="inf107"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft107">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>, where each row corresponds to the tuning curve of one neuron across all locations. Red dashed line indicates the diagonal. (<bold>C</bold>) As in (<bold>B</bold>), but for <inline-formula><alternatives><mml:math id="inf108"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math><tex-math id="inft108">\begin{document}$r=0.3$\end{document}</tex-math></alternatives></inline-formula>. Here, clockwise movement corresponds to movement from site <inline-formula><alternatives><mml:math id="inf109"><mml:mi>i</mml:mi></mml:math><tex-math id="inft109">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> to site <inline-formula><alternatives><mml:math id="inf110"><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft110">\begin{document}$i+1$\end{document}</tex-math></alternatives></inline-formula>. Thus, predictive activity appears as a shift in RNN activity below the matrix diagonal. (<bold>D</bold>) As in (<bold>B</bold>), but for <inline-formula><alternatives><mml:math id="inf111"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math><tex-math id="inft111">\begin{document}$r=1.0$\end{document}</tex-math></alternatives></inline-formula>. Barcodes appear as random, off-diagonal structure in the activity matrix. (<bold>E</bold>) The firing fields of four example units across different recurrent strengths. That is, each unit’s row in (<bold>B</bold>) is in blue, its row in (<bold>C</bold>) is in orange, and its row in (<bold>D</bold>) is in purple. Each curve is max-normalized. (<bold>F</bold>) Average projection strength of RNN activity onto the place code, predictive code, and barcode vectors. The place code vector is defined as activity with <inline-formula><alternatives><mml:math id="inf112"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft112">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>. The predictive code vector is defined as the place field of the unit at the next clockwise site, minus the projection onto the place code. The barcode vector is defined as activity with <inline-formula><alternatives><mml:math id="inf113"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math><tex-math id="inft113">\begin{document}$r=1.0$\end{document}</tex-math></alternatives></inline-formula>, minus the projection onto the place code. Lines show the mean over the 100 units of the mode, with shading displaying 99% confidence interval. Each line is max-normalized. (<bold>G</bold>) Seed output of the model with predictive weights, given the animal’s location on the circular track. Here, there is only one cache, made at the halfway location of the circular track (labeled “C”).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Structure of predictive weights matrix and unit activity examples.</title><p>(<bold>A</bold>) The predictive weights <inline-formula><alternatives><mml:math id="inf114"><mml:mi>M</mml:mi></mml:math><tex-math id="inft114">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> that is added to <inline-formula><alternatives><mml:math id="inf115"><mml:mi>J</mml:mi></mml:math><tex-math id="inft115">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula>. (<bold>B</bold>) As in (<bold>A</bold>), but zooming in 10x into the matrix for clarity. (<bold>C</bold>) As in <xref ref-type="fig" rid="fig6">Figure 6E</xref>, but for 24 additional randomly sampled units.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Model performance with predictive weights.</title><p>(<bold>A-B</bold>) As in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>, but for the model with predictive weights added in. (<bold>C-D</bold>) As in <xref ref-type="fig" rid="fig4">Figure 4EF</xref>, but the model with predictive weights added in.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-fig6-figsupp2-v1.tif"/></fig></fig-group><p>These results suggest that the structured and random components of recurrent connectivity can act somewhat independently of each other, with their relative contributions determined by recurrent strength. We quantify this by measuring the magnitude of the projection of population activity onto the place code, the predictive code, and barcodes as a function of recurrent strength (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). At the lowest recurrent strength (<inline-formula><alternatives><mml:math id="inf116"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft116">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>), the population activity is solely concentrated on the place code. As recurrent strength is increased, place coding decreases and predictive activity increases with a peak around <inline-formula><alternatives><mml:math id="inf117"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math><tex-math id="inft117">\begin{document}$r=0.4$\end{document}</tex-math></alternatives></inline-formula>. Beyond this, both place and predictive activity decrease as barcode activity rises to a peak when <inline-formula><alternatives><mml:math id="inf118"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math><tex-math id="inft118">\begin{document}$r=1.0$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Finally, we explore the functional implications of including predictive maps into our memory model. We first verify that model performance in the previous three-cache tasks is not disrupted by including predictive weights in the model (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). We then examine memory recall of a single cache for a predictive model, assuming a clockwise behavioral bias (<xref ref-type="fig" rid="fig6">Figure 6G</xref>). The model exhibits a profound skew: the cache is recalled much earlier on the path leading up to the cache location, at further distance from the cache than on the path after the cache location. This indicates that knowledge of the environment and prior experience, as reflected in predictive place activity, can shape memory recall in our model.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have proposed a biologically realistic model for a simple form of episodic memory using barcodes. Our work is related to previous auto-associative memory models of the hippocampus such as Hopfield networks (<xref ref-type="bibr" rid="bib15">Gardner-Medwin, 1976</xref>; <xref ref-type="bibr" rid="bib34">McNaughton and Morris, 1987</xref>; <xref ref-type="bibr" rid="bib33">Marr et al., 1991</xref>; <xref ref-type="bibr" rid="bib1">Alvarez and Squire, 1994</xref>; <xref ref-type="bibr" rid="bib53">Tsodyks, 1999</xref>), but diverges in a few critical areas. Building on ideas from hippocampal indexing theory (<xref ref-type="bibr" rid="bib51">Teyler and DiScenna, 1986</xref>; <xref ref-type="bibr" rid="bib52">Teyler and Rudy, 2007</xref>), and following the discovery of barcodes (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>), we show how recurrent computation can implement memory indexing. Our model is further noteworthy in randomly intermixing representations of memory index and memory content in the activity of single neurons, matching experimental findings. This intermixing implies that single neurons cannot be definitively identified as ‘place cells’ or ‘barcode cells’, despite clear differentiation between the place code and the barcode at the population level. A further innovation of our model is the ability to control the trade-off between pattern completion and pattern separation during memory recall, by simply turning up or down the strength of a memory content input (‘search strength’ in <xref ref-type="fig" rid="fig4">Figure 4</xref>). In this work, we considered only place and a single ‘seed’ input, but it is straightforward to generalize this to naturalistic cases where different food types are stored, or to memory contents beyond cached food. In principle, our approach would allow independent control of search strength for each potential element of memory content.</p><p>To generate barcodes during caching and retrieval without affecting place activity during visits, our model changes recurrent strength in an RNN between different behaviors. A major question is how the brain could implement such gain changes in recurrence. One possible mechanism is a change in recurrent inhibition, which is consistent with dramatic changes in the activity of inhibitory neurons observed during caching (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>). Neuromodulators like acetylcholine have been shown to bidirectionally modulate different inhibitory neuron subtypes (<xref ref-type="bibr" rid="bib59">Xiang et al., 1998</xref>; <xref ref-type="bibr" rid="bib32">Lovett-Barron et al., 2014</xref>), and proposed to control the recurrent gain of hippocampal processing (<xref ref-type="bibr" rid="bib19">Hasselmo, 1999</xref>; <xref ref-type="bibr" rid="bib20">Hasselmo, 2006</xref>). However, our model uses generic RNN units, and it is unclear precisely how units in the model should be mapped to real excitatory and inhibitory hippocampal neurons in the brain. Our model predicts a state change in hippocampal activity during memory formation and recall, allowing recurrent computation to generate or reactivate memory barcodes. Detailed modeling of realistic E-I networks is needed to further clarify its specific biological implementation.</p><p>Alternatively, other mechanisms may be involved in generating barcodes. We demonstrated that conventional feed-forward sparsification (<xref ref-type="bibr" rid="bib4">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib60">Xie et al., 2023</xref>) was highly inefficient, but more specialized computations may improve this (<xref ref-type="bibr" rid="bib14">Földiák, 1990</xref>; <xref ref-type="bibr" rid="bib40">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib43">Sacouto and Wichert, 2023</xref>; <xref ref-type="bibr" rid="bib38">Muscinelli et al., 2023</xref>). Another possibility is that barcodes are generated in a separate recurrent network upstream of the recurrent network where memories are stored. In this two-network scenario, the downstream network receives both spatial tuning and barcodes as inputs. This would not obviate the need for modulating recurrent strength in the downstream network to switch between input-driven modes and attractor dynamics. We suspect separating barcode generation and memory storage in separate networks would not fundamentally affect our conclusions.</p><p>We showed that barcodes allow for precise memory retrieval despite the presence of other correlated memories. This sharpened memory retrieval is similar to mechanisms used in key-value memory structures that are often embedded in machine learning architectures (<xref ref-type="bibr" rid="bib17">Graves et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Graves et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Sukhbaatar et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Le et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Banino et al., 2020</xref>). At their simplest, these key-value memory structures consist of memory slots. Each slot consists of a memory that can be addressed via ‘keys’ such that their stored memory is returned as ‘values’. In machine learning, key-value memory has been connected to the dot-product attention mechanism used in transformers (<xref ref-type="bibr" rid="bib28">Krotov and Hopfield, 2016</xref>; <xref ref-type="bibr" rid="bib42">Ramsauer et al., 2020</xref>). Interestingly, prior theoretical work has suggested neural implementations for both key-value memory and attention mechanisms, arguing for their usefulness in neural systems such as long-term memory (<xref ref-type="bibr" rid="bib26">Kanerva, 1988</xref>; <xref ref-type="bibr" rid="bib55">Tyulmankov et al., 2021</xref>; <xref ref-type="bibr" rid="bib8">Bricken and Pehlevan, 2021</xref>; <xref ref-type="bibr" rid="bib57">Whittington et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Kozachkov et al., 2023</xref>; <xref ref-type="bibr" rid="bib29">Krotov and Hopfield, 2020</xref>; <xref ref-type="bibr" rid="bib16">Gershman et al., 2025</xref>). In this framework, the address where a memory is stored (the key) may be optimized independently of the value or content of the memory. In our model, barcodes improve memory performance by providing a content-independent scaffold that binds to memory content, preventing memories with overlapping content from blurring together. Thus, barcodes can be considered as a change in memory address, and our model suggests important connections between recurrent neural activity and key generation mechanisms. However, we note that barcodes should not be literally equated with keys in key-value systems as our model’s memory is ‘content-addressable’—it can be queried by place and seed inputs.</p><p>Episodic memory is often studied at a behavioral level in humans performing free or cued recall of remembered word lists (<xref ref-type="bibr" rid="bib25">Kahana, 2020</xref>; <xref ref-type="bibr" rid="bib39">Naim et al., 2020</xref>). Temporal context models (TCM) of episodic memory have been highly successful in accounting for the sequential order effects observed reliably in this experimental setting (<xref ref-type="bibr" rid="bib22">Howard and Kahana, 2002</xref>; <xref ref-type="bibr" rid="bib23">Howard et al., 2005</xref>; <xref ref-type="bibr" rid="bib45">Sederberg et al., 2008</xref>), and the idea of a ‘context vector’ in TCM is closely related to use of barcodes as a memory index in our model. However, experiments have shown that chickadee cache retrieval does not exhibit temporal order effects (<xref ref-type="bibr" rid="bib2">Applegate and Aronov, 2022</xref>), suggesting that caches at different locations are likely not linked by a temporal context as in TCM. Interestingly, caches at the same location were found to have distinct but correlated barcodes (<xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>), which could be related to caches sharing a ‘spatial context’ analogous to TCM. In the present study, we did not consider memory for different caches at the same location, since it requires a mechanism for forgetting or overwriting cache memory following retrieval. Although such ‘directed forgetting’ is observed in chickadee behavior (<xref ref-type="bibr" rid="bib46">Sherry, 1984</xref>), there is no definitive solution for Hopfield-like networks, and it is thus beyond the scope of our current work.</p><p>Our hippocampal model focused on the implementation of episodic memory. Importantly, the proposed barcode mechanism is capable of coexisting with other hippocampal functions, such as predictive coding as formalized by the successor representation (SR; <xref ref-type="bibr" rid="bib48">Stachenfeld et al., 2017</xref>). Surprisingly, we found that a hybrid network can switch between SR-generating and barcode-generating modes of operation by adjusting the gain of recurrent connectivity. Further work is needed to characterize the general conditions under which barcode and SR functions do or do not mutually interfere. It is also unclear if these are separate functions of the same circuit, or if they are complementary in certain scenarios (<xref ref-type="bibr" rid="bib44">Schapiro et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Barron et al., 2020</xref>). For example, we found that the SR could bias barcode-mediated memory recall. In a complex environment, the Euclidean distance between two points may not correspond to its proximity in a practical sense, which the SR better captures. In this case, experience-dependent biases in memory recall can be functionally advantageous (<xref ref-type="bibr" rid="bib11">Dasgupta and Gershman, 2021</xref>) and would be consistent with behavioral results (<xref ref-type="bibr" rid="bib24">Kahana, 1996</xref>; <xref ref-type="bibr" rid="bib50">Talmi and Moscovitch, 2004</xref>).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Caching task</title><p>We simulate a caching task in a circular track, similar to <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>. The track consists of <inline-formula><alternatives><mml:math id="inf119"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft119">\begin{document}$N_{s}$\end{document}</tex-math></alternatives></inline-formula> connected states. The goal of this task is to test how well an agent equipped with a memory model can precisely recall the locations where a cache (or memory) has been stored. Specifically, for each simulation, we first choose a set of states <inline-formula><alternatives><mml:math id="inf120"><mml:mi>C</mml:mi></mml:math><tex-math id="inft120">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> that will be the location where caches are made. For each state <inline-formula><alternatives><mml:math id="inf121"><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:math><tex-math id="inft121">\begin{document}$c\in C$\end{document}</tex-math></alternatives></inline-formula> we assume the agent is currently at state <inline-formula><alternatives><mml:math id="inf122"><mml:mi>c</mml:mi></mml:math><tex-math id="inft122">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> and allow the model to store a memory at that state. We then simulate what the output of the model is if the agent is at any of the other <inline-formula><alternatives><mml:math id="inf123"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft123">\begin{document}$N_{s}$\end{document}</tex-math></alternatives></inline-formula> states. We continue this procedure for the remaining states in the list.</p><p>In the three-cache task, <inline-formula><alternatives><mml:math id="inf124"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mn>0.7</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math><tex-math id="inft124">\begin{document}$C=\{0,m,N_{s}*0.7\}$\end{document}</tex-math></alternatives></inline-formula>. We sweep over different values of <inline-formula><alternatives><mml:math id="inf125"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mn>0.7</mml:mn></mml:math><tex-math id="inft125">\begin{document}$0 \lt m \lt N_{s}*0.7$\end{document}</tex-math></alternatives></inline-formula> to test the effects of site spacing. In the main figures, these three caches are made in increasing order of their location. However, we also randomly shuffle the order of caching in a supplementary figure (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C, D</xref>) and do not find any effects on model performance.</p></sec><sec id="s4-2"><title>Barcode model</title><sec id="s4-2-1"><title>Architecture</title><p>Place inputs into the model arrive from an input layer <inline-formula><alternatives><mml:math id="inf126"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft126">\begin{document}$\vec{p}\in\mathcal{R}^{N_{p}}$\end{document}</tex-math></alternatives></inline-formula>. The input layer feeds into a recurrent neural network with activity <inline-formula><alternatives><mml:math id="inf127"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft127">\begin{document}$\vec{x}\in\mathcal{R}^{N_{x}}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf128"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft128">\begin{document}$N_{x}=N_{p}$\end{document}</tex-math></alternatives></inline-formula>. Place input units connect to recurrent units with one-to-one connections (that is, the weight matrix <inline-formula><alternatives><mml:math id="inf129"><mml:msub><mml:mi>J</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft129">\begin{document}$J_{xi}$\end{document}</tex-math></alternatives></inline-formula> from the place input layer to the recurrent network is the size <inline-formula><alternatives><mml:math id="inf130"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft130">\begin{document}$N_{x}$\end{document}</tex-math></alternatives></inline-formula> identity matrix). Recurrent weights are encoded in the matrix <inline-formula><alternatives><mml:math id="inf131"><mml:mi>J</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft131">\begin{document}$J\in\mathcal{R}^{N_{x}\times N_{x}}$\end{document}</tex-math></alternatives></inline-formula>. At initialization, <inline-formula><alternatives><mml:math id="inf132"><mml:mi>J</mml:mi><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft132">\begin{document}$J\sim\mathcal{N}(\frac{\mu}{N_{x}},\frac{\sigma^{2}}{N_{x}})$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf133"><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:math><tex-math id="inft133">\begin{document}$\mu,\sigma$\end{document}</tex-math></alternatives></inline-formula> are tunable hyperparameters controlling the mean and standard deviation of the distribution.</p><p>The input representing seeds arrives from a single unit <inline-formula><alternatives><mml:math id="inf134"><mml:mi>s</mml:mi></mml:math><tex-math id="inft134">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. The connections from <inline-formula><alternatives><mml:math id="inf135"><mml:mi>s</mml:mi></mml:math><tex-math id="inft135">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> to recurrent units <inline-formula><alternatives><mml:math id="inf136"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft136">\begin{document}$\vec{x}$\end{document}</tex-math></alternatives></inline-formula> are encoded in the vector <inline-formula><alternatives><mml:math id="inf137"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft137">\begin{document}$\vec{j}_{sx}\in\mathcal{R}^{N_{x}}$\end{document}</tex-math></alternatives></inline-formula>. Each value of <inline-formula><alternatives><mml:math id="inf138"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft138">\begin{document}$\vec{j}_{sx}$\end{document}</tex-math></alternatives></inline-formula> is sampled from the standard normal distribution.</p></sec><sec id="s4-2-2"><title>Recurrent dynamics</title><p>Recurrent dynamics are run over <inline-formula><alternatives><mml:math id="inf139"><mml:mi>T</mml:mi></mml:math><tex-math id="inft139">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> timesteps. Let <inline-formula><alternatives><mml:math id="inf140"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft140">\begin{document}$\vec{v_{t}}$\end{document}</tex-math></alternatives></inline-formula> be the preactivations of the recurrent population at time <inline-formula><alternatives><mml:math id="inf141"><mml:mi>t</mml:mi></mml:math><tex-math id="inft141">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf142"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft142">\begin{document}$\vec{x_{t}}$\end{document}</tex-math></alternatives></inline-formula> be the activations of the recurrent population at time <inline-formula><alternatives><mml:math id="inf143"><mml:mi>t</mml:mi></mml:math><tex-math id="inft143">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>. That is, <inline-formula><alternatives><mml:math id="inf144"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft144">\begin{document}$\vec{x_{t}}=ReLU(\vec{v_{t}})$\end{document}</tex-math></alternatives></inline-formula>. At <inline-formula><alternatives><mml:math id="inf145"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft145">\begin{document}$t=0$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf146"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft146">\begin{document}$\vec{x_{0}}=\vec{v_{0}}=0$\end{document}</tex-math></alternatives></inline-formula>. The recurrent dynamics are defined over <inline-formula><alternatives><mml:math id="inf147"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft147">\begin{document}$\vec{v}$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ5"><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>∑</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>J</mml:mi><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  \frac{d\vec{v}}{dt}=\left(-\frac{\alpha}{N_{x}}\sum\vec{x}\right)\vec{v}+rJ\vec{x}+\vec{p}+s\vec{j}_{sx}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf148"><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math><tex-math id="inft148">\begin{document}$r\in\{0,1\}$\end{document}</tex-math></alternatives></inline-formula> is a modulatory factor that controls whether the network operates with recurrent dynamics or is purely feedforward driven. The first term in the equation corresponds to a voltage leak term where the leak at each neuron is proportional to global population activity. This effectively implements a form of divisive normalization and helps keep network activity stable even as weights are updated during the caching task. The second term in the equation represents recurrent inputs, while the last two terms represent feedforward inputs into the network.</p><sec id="s4-2-2-1"><title>Input structure and timing</title><p>Place is encoded in <inline-formula><alternatives><mml:math id="inf149"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft149">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> such that, at location <inline-formula><alternatives><mml:math id="inf150"><mml:mi>k</mml:mi></mml:math><tex-math id="inft150">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, each input neuron <inline-formula><alternatives><mml:math id="inf151"><mml:mi>l</mml:mi></mml:math><tex-math id="inft151">\begin{document}$l$\end{document}</tex-math></alternatives></inline-formula> has activity <inline-formula><alternatives><mml:math id="inf152"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mi>ν</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:math><tex-math id="inft152">\begin{document}$p_{l}=e^{\frac{-d}{\nu}}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf153"><mml:mi>d</mml:mi></mml:math><tex-math id="inft153">\begin{document}$d$\end{document}</tex-math></alternatives></inline-formula> is the shortest distance between <inline-formula><alternatives><mml:math id="inf154"><mml:mi>k</mml:mi></mml:math><tex-math id="inft154">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf155"><mml:mi>l</mml:mi></mml:math><tex-math id="inft155">\begin{document}$l$\end{document}</tex-math></alternatives></inline-formula> as a percentage of the circular arena size. We consider 100 evenly sampled states in this state space. Seed input <inline-formula><alternatives><mml:math id="inf156"><mml:mi>s</mml:mi></mml:math><tex-math id="inft156">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> may be any nonnegative scalar value.</p><list list-type="bullet" id="list1"><list-item><p>Place mode: Input <inline-formula><alternatives><mml:math id="inf157"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft157">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> is active over all <inline-formula><alternatives><mml:math id="inf158"><mml:mi>T</mml:mi></mml:math><tex-math id="inft158">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> timesteps. Recurrence is turned off (<inline-formula><alternatives><mml:math id="inf159"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft159">\begin{document}$r=0$\end{document}</tex-math></alternatives></inline-formula>), as is the seed input (<inline-formula><alternatives><mml:math id="inf160"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft160">\begin{document}$s=0$\end{document}</tex-math></alternatives></inline-formula>).</p></list-item><list-item><p>Caching mode: Input <inline-formula><alternatives><mml:math id="inf161"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft161">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> is active over all <inline-formula><alternatives><mml:math id="inf162"><mml:mi>T</mml:mi></mml:math><tex-math id="inft162">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> timesteps and input <inline-formula><alternatives><mml:math id="inf163"><mml:mi>s</mml:mi></mml:math><tex-math id="inft163">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> is active over timesteps <inline-formula><alternatives><mml:math id="inf164"><mml:mo stretchy="false">[</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:math><tex-math id="inft164">\begin{document}$[T-t_{s},T]$\end{document}</tex-math></alternatives></inline-formula> with strength <inline-formula><alternatives><mml:math id="inf165"><mml:mi>λ</mml:mi></mml:math><tex-math id="inft165">\begin{document}$\lambda$\end{document}</tex-math></alternatives></inline-formula>. Recurrence is on (<inline-formula><alternatives><mml:math id="inf166"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft166">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>).</p></list-item><list-item><p>Recall mode: Input <inline-formula><alternatives><mml:math id="inf167"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft167">\begin{document}$\vec{p}$\end{document}</tex-math></alternatives></inline-formula> and input <inline-formula><alternatives><mml:math id="inf168"><mml:mi>s</mml:mi></mml:math><tex-math id="inft168">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> are both active over all <inline-formula><alternatives><mml:math id="inf169"><mml:mi>T</mml:mi></mml:math><tex-math id="inft169">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> timesteps. Recurrence is on (<inline-formula><alternatives><mml:math id="inf170"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft170">\begin{document}$r=1$\end{document}</tex-math></alternatives></inline-formula>). The value of <inline-formula><alternatives><mml:math id="inf171"><mml:mi>s</mml:mi></mml:math><tex-math id="inft171">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> is flexibly modulated to adjust the search strength <inline-formula><alternatives><mml:math id="inf172"><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft172">\begin{document}$(s\geq 0)$\end{document}</tex-math></alternatives></inline-formula>.</p></list-item></list></sec><sec id="s4-2-2-2"><title>Update rule</title><p>Let <inline-formula><alternatives><mml:math id="inf173"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft173">\begin{document}$\vec{x}$\end{document}</tex-math></alternatives></inline-formula> be the activations of the recurrent network at the end of recurrent dynamics (in our case, at time <inline-formula><alternatives><mml:math id="inf174"><mml:mi>T</mml:mi></mml:math><tex-math id="inft174">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula>). At each cache event, the update rule carried out is<disp-formula id="equ6"><alternatives><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>η</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>⊺</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:msup><mml:mn>𝟏</mml:mn><mml:mo>⊺</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t6">\begin{document}$$\displaystyle \Delta J=\frac{\eta}{N_{x}}(\vec{x}\vec{x}^{\intercal}+\beta\vec{x}\mathbf{1}^{\intercal})$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf175"><mml:mi>η</mml:mi></mml:math><tex-math id="inft175">\begin{document}$\eta$\end{document}</tex-math></alternatives></inline-formula> controls the learning rate. The weight update contains a Hebbian update through <inline-formula><alternatives><mml:math id="inf176"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⊺</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft176">\begin{document}$\vec{x}\vec{x}^{\intercal}$\end{document}</tex-math></alternatives></inline-formula> and an inhibitory update through <inline-formula><alternatives><mml:math id="inf177"><mml:mi>β</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⊺</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft177">\begin{document}$\beta\vec{x}\mathbf{1}^{\intercal}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf178"><mml:mi>β</mml:mi></mml:math><tex-math id="inft178">\begin{document}$\beta$\end{document}</tex-math></alternatives></inline-formula> is a negative scalar. This inhibitory term causes the connections between neurons which are not co-active during caching to weaken. The update rule can also be stated in terms of the synapse <inline-formula><alternatives><mml:math id="inf179"><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi></mml:math><tex-math id="inft179">\begin{document}$j\rightarrow i$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ7"><alternatives><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t7">\begin{document}$$\displaystyle \Delta J_{ij}\sim x_{i}x_{j}+\beta x_{i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>This is the form shown in <xref ref-type="disp-formula" rid="equ4">equation 4</xref>.</p></sec><sec id="s4-2-2-3"><title>Network readout</title><p>To detect cache presence, we define a seed output signal. The output <inline-formula><alternatives><mml:math id="inf180"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft180">\begin{document}$y_{s}$\end{document}</tex-math></alternatives></inline-formula> is read out from the recurrent network activity through weights <inline-formula><alternatives><mml:math id="inf181"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft181">\begin{document}$\vec{j}_{sx}$\end{document}</tex-math></alternatives></inline-formula>. At initialization, <inline-formula><alternatives><mml:math id="inf182"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft182">\begin{document}$\vec{j}_{sx}=0$\end{document}</tex-math></alternatives></inline-formula>. Every time a cache is made, the following update rule is run: <inline-formula><alternatives><mml:math id="inf183"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft183">\begin{document}$\Delta\vec{j}_{sx}=\vec{x}$\end{document}</tex-math></alternatives></inline-formula>. Thus, seed output is read out as <inline-formula><alternatives><mml:math id="inf184"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>⊺</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft184">\begin{document}$y_{s}=\vec{j}_{sx}^{\intercal}\vec{x}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>To recall cache location, we define a place field readout layer. The output <inline-formula><alternatives><mml:math id="inf185"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft185">\begin{document}$\vec{y}_{p}\in\mathcal{R}^{N_{p}}$\end{document}</tex-math></alternatives></inline-formula> is read out from the recurrent network through weights <inline-formula><alternatives><mml:math id="inf186"><mml:msub><mml:mi>J</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft186">\begin{document}$J_{yx}\in\mathcal{R}^{N_{i}\times N_{x}}$\end{document}</tex-math></alternatives></inline-formula>. At initialization, <inline-formula><alternatives><mml:math id="inf187"><mml:msub><mml:mi>J</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft187">\begin{document}$J_{yx}=0$\end{document}</tex-math></alternatives></inline-formula>. Every time a cache is made, the following update rule is run: <inline-formula><alternatives><mml:math id="inf188"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>J</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⊺</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft188">\begin{document}$\Delta J_{yx}=\vec{p}\vec{x}^{\intercal}$\end{document}</tex-math></alternatives></inline-formula>. Thus, recalled cache locations are read out as <inline-formula><alternatives><mml:math id="inf189"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft189">\begin{document}$\vec{y}_{p}=J_{yx}\vec{x}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-2-2-4"><title>Ablations</title><p>To construct a place code-only model, we ablate barcode generation by setting <inline-formula><alternatives><mml:math id="inf190"><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft190">\begin{document}$J=0$\end{document}</tex-math></alternatives></inline-formula> at initialization. To construct a barcode-only model, we ablate the presence of place correlations in the input by setting the place input spatial scale parameter <inline-formula><alternatives><mml:math id="inf191"><mml:mi>ν</mml:mi></mml:math><tex-math id="inft191">\begin{document}$\nu$\end{document}</tex-math></alternatives></inline-formula> to a very small value <inline-formula><alternatives><mml:math id="inf192"><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft192">\begin{document}$10^{-3}$\end{document}</tex-math></alternatives></inline-formula> in the place encoding equation.</p></sec></sec></sec><sec id="s4-3"><title>Defining site distance in our simulation</title><p>To allow comparisons to data in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>, we first determine the number of states in our simulation that’s equivalent to the distance between adjacent cache sites in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>. To do so, we note that the spatial correlation between population activity at two adjacent cache sites is around 0.75 (when spatial correlation profile is normalized to [0, 1]). We identify the number of states in our simulation such that the normalized correlation between visit activity is also around 0.75. We find that this is around eight states. Thus, we define eight states in our simulation as equivalent to the distance between adjacent cache sites.</p></sec><sec id="s4-4"><title>Simulating and visualizing spikes</title><p>We simulate Poisson spikes from our rate network in <xref ref-type="fig" rid="fig2">Figure 2HJ</xref>. Specifically, for a unit with rate <inline-formula><alternatives><mml:math id="inf193"><mml:mi>r</mml:mi></mml:math><tex-math id="inft193">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>, we draw spikes from a Poisson distribution with mean and variance <inline-formula><alternatives><mml:math id="inf194"><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>K</mml:mi></mml:math><tex-math id="inft194">\begin{document}$r+K$\end{document}</tex-math></alternatives></inline-formula>. We set <inline-formula><alternatives><mml:math id="inf195"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math><tex-math id="inft195">\begin{document}$K=0.2$\end{document}</tex-math></alternatives></inline-formula> to visually match observations from data.</p></sec><sec id="s4-5"><title>Spatial correlation of RNN activity for cache-retrieval pairs at different sites</title><p>To calculate correlation values as in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, we simulated experiments where five sites were randomly chosen for caching and retrieval. To compare model results to the empirical data in 1E,F, which includes intrinsic neural variability, we sampled Poisson-generated spike counts from the rates output by our model. Specifically, for RNN activity <inline-formula><alternatives><mml:math id="inf196"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft196">\begin{document}$\vec{r_{i}}$\end{document}</tex-math></alternatives></inline-formula> at location <inline-formula><alternatives><mml:math id="inf197"><mml:mi>i</mml:mi></mml:math><tex-math id="inft197">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>, using the rates at <inline-formula><alternatives><mml:math id="inf198"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><tex-math id="inft198">\begin{document}$t=100$\end{document}</tex-math></alternatives></inline-formula> as elsewhere, we first generate a sample vector of spikes <inline-formula><alternatives><mml:math id="inf199"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>∼</mml:mo><mml:mtext>Poisson</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft199">\begin{document}$\vec{s_{i}}\sim\text{Poisson}(k\vec{r_{i}})$\end{document}</tex-math></alternatives></inline-formula>. We choose <inline-formula><alternatives><mml:math id="inf200"><mml:mi>k</mml:mi></mml:math><tex-math id="inft200">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> to be 0.2 to match experimental spatial correlation profiles in the ‘visit-visit’ condition. To get the correlation of population activity at locations <inline-formula><alternatives><mml:math id="inf201"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:math><tex-math id="inft201">\begin{document}$i,j$\end{document}</tex-math></alternatives></inline-formula>, we calculate the population Pearson correlation coefficient between <inline-formula><alternatives><mml:math id="inf202"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft202">\begin{document}$\vec{s_{i}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf203"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft203">\begin{document}$\vec{s_{j}}$\end{document}</tex-math></alternatives></inline-formula>. To generate <xref ref-type="fig" rid="fig3">Figure 3D</xref>, we repeated this simulated experiment 20 times with different random seeds and then pooled over experiments to average correlation values as a function of distance. All values are normalized by the ‘visit-visit’ correlation value at a site distance of 0, to match the analysis of experimental data in <xref ref-type="bibr" rid="bib9">Chettih et al., 2024</xref>. This is repeated over 20 random seeds.</p></sec><sec id="s4-6"><title>Barcode model with predictive map</title><p>The barcode model with prediction differs from the default model in the initialized weight matrix. Specifically, the weight matrix <inline-formula><alternatives><mml:math id="inf204"><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:math><tex-math id="inft204">\begin{document}$J=B+M$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf205"><mml:mi>B</mml:mi></mml:math><tex-math id="inft205">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> is the random Gaussian matrix of the default model. <inline-formula><alternatives><mml:math id="inf206"><mml:mi>M</mml:mi></mml:math><tex-math id="inft206">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> is a successor representation-like matrix (<xref ref-type="bibr" rid="bib48">Stachenfeld et al., 2017</xref>) defined as<disp-formula id="equ8"><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>D</mml:mi></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle \newcommand{\pat}[1]{} M = \rho(\sum_{d=0}^D \gamma^d T^t) + \delta$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf207"><mml:mi>T</mml:mi></mml:math><tex-math id="inft207">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> is the transition probability matrix and <inline-formula><alternatives><mml:math id="inf208"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:math><tex-math id="inft208">\begin{document}$\gamma=0.99$\end{document}</tex-math></alternatives></inline-formula> is a temporal discount factor. We also add a scaling factor <inline-formula><alternatives><mml:math id="inf209"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.075</mml:mn></mml:math><tex-math id="inft209">\begin{document}$\rho=0.075$\end{document}</tex-math></alternatives></inline-formula> and an inhibitory offset <inline-formula><alternatives><mml:math id="inf210"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.015</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft210">\begin{document}$\delta=-0.015$\end{document}</tex-math></alternatives></inline-formula> is a 5000-dimensional square matrix, and we truncate the summation at <inline-formula><alternatives><mml:math id="inf211"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:math><tex-math id="inft211">\begin{document}$D=300$\end{document}</tex-math></alternatives></inline-formula> steps. <inline-formula><alternatives><mml:math id="inf212"><mml:mi>T</mml:mi></mml:math><tex-math id="inft212">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> is defined as<disp-formula id="equ9"><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>T</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mtext/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle \newcommand{\pat}[1]{} T_{ij} = \begin{cases} 1, &amp; \text{if}\ j=i+1 \\ 0, &amp; \text{otherwise} \end{cases}$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-7"><title>Alternative model: feedforward barcode generation</title><p>We will construct a feedforward model to generate sparse, decorrelated barcodes (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). Place inputs to the model are fed through a hidden expansion layer before being compressed again by an output layer to generate the barcode. We first define these layers via the random matrices <inline-formula><alternatives><mml:math id="inf213"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="inft213">\begin{document}$W_{h}\in\mathcal{R}^{M,N_{p}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf214"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft214">\begin{document}$W_{o}\in\mathcal{R}^{N_{x},M}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf215"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft215">\begin{document}$W_{h}\sim\mathcal{N}(0,\frac{1}{N_{p}})$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf216"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="inft216">\begin{document}$W_{o}\sim\mathcal{N}(0,\frac{1}{M})$\end{document}</tex-math></alternatives></inline-formula>. We make the hidden layer very large: <inline-formula><alternatives><mml:math id="inf217"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>20000</mml:mn></mml:math><tex-math id="inft217">\begin{document}$M=20000$\end{document}</tex-math></alternatives></inline-formula>. The activity of the model in the hidden layer is described as:<disp-formula id="equ10"><alternatives><mml:math id="m10"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mtext>ReLU</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t10">\begin{document}$$\displaystyle \vec{x_{h}}=\text{ReLU}(W_{h}\vec{p}-C_{\theta})$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf218"><mml:msub><mml:mi>C</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft218">\begin{document}$C_{\theta}$\end{document}</tex-math></alternatives></inline-formula> is a constant chosen such that the proportion of units in <inline-formula><alternatives><mml:math id="inf219"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft219">\begin{document}$\vec{x_{h}}$\end{document}</tex-math></alternatives></inline-formula> that are active is <inline-formula><alternatives><mml:math id="inf220"><mml:mi>θ</mml:mi></mml:math><tex-math id="inft220">\begin{document}$\theta$\end{document}</tex-math></alternatives></inline-formula>. In other words, <inline-formula><alternatives><mml:math id="inf221"><mml:mi>θ</mml:mi></mml:math><tex-math id="inft221">\begin{document}$\theta$\end{document}</tex-math></alternatives></inline-formula> is a hyperparameter of the model that sets how sparse the hidden layer activity is.</p><p>The hidden layer activity is then passed through the output weights to form the barcode activity generated by the feedforward model:<disp-formula id="equ11"><alternatives><mml:math id="m11"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mtext>ReLU</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>-</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \vec{x}=\text{ReLU}(W_{o}\vec{x_{h}}-C)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf222"><mml:mi>C</mml:mi></mml:math><tex-math id="inft222">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> is a constant chosen such that the sparsity of <inline-formula><alternatives><mml:math id="inf223"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="inft223">\begin{document}$\vec{x}$\end{document}</tex-math></alternatives></inline-formula> matches the sparsity of barcodes generated in the default RNN barcode model.</p></sec><sec id="s4-8"><title>Alternative model: place encoding with Gaussian input weights</title><p>We simulate a version of the model with more realistic and complex place inputs. We generated the input currents to the RNN units according to a 0-mean multivariate Gaussian process. The statistics of the Gaussian process are chosen such that the covariance of the inputs to RNN unit <inline-formula><alternatives><mml:math id="inf224"><mml:mi>i</mml:mi></mml:math><tex-math id="inft224">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> and unit <inline-formula><alternatives><mml:math id="inf225"><mml:mi>j</mml:mi></mml:math><tex-math id="inft225">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula> is an exponentially decaying function of their minimum spatial distance <inline-formula><alternatives><mml:math id="inf226"><mml:mi>d</mml:mi></mml:math><tex-math id="inft226">\begin{document}$d$\end{document}</tex-math></alternatives></inline-formula> around the circular arena: <inline-formula><alternatives><mml:math id="inf227"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mn>0.4</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft227">\begin{document}$\Sigma_{ij}=e^{-\frac{d}{0.4}}$\end{document}</tex-math></alternatives></inline-formula>. All other details of this alternative model are the same as in the default.</p></sec><sec id="s4-9"><title>Code</title><p>Code is publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/chingf/barcodes">https://github.com/chingf/barcodes</ext-link> (copy archived at <xref ref-type="bibr" rid="bib13">Fang, 2024</xref>).</p></sec><sec id="s4-10"><title>Parameter values</title><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" colspan="2">Hyperparameter</th><th align="left" valign="bottom">Symbol</th><th align="left" valign="bottom">Value</th></tr></thead><tbody><tr><td align="left" valign="bottom">Place Inputs</td><td align="left" valign="bottom">Spatial scale parameter</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf228"><mml:mi>ν</mml:mi></mml:math><tex-math id="inft228">\begin{document}$\nu$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.2</td></tr><tr><td align="left" valign="bottom">Dimensionality</td><td align="left" valign="bottom">Number of place input neurons</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf229"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft229">\begin{document}$N_{i}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">5000</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Number of recurrent network neurons</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf230"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft230">\begin{document}$N_{x}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">5000</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Number of output neurons</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf231"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft231">\begin{document}$N_{y}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">5000</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Number of states</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf232"><mml:msub><mml:mi>N</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft232">\begin{document}$N_{s}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">RNN weight matrix</td><td align="left" valign="bottom">Scale of RNN initial weight mean</td><td align="left" valign="bottom"><italic>μ</italic></td><td align="left" valign="bottom">–40</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Scale of RNN initial weight standard deviation</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf233"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft233">\begin{document}$\sigma$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">7</td></tr><tr><td align="left" valign="bottom">RNN dynamics</td><td align="left" valign="bottom">Dynamics integration step</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf234"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:math><tex-math id="inft234">\begin{document}$\Delta t$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.1</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Dynamics divisive normalization strength</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf235"><mml:mi>α</mml:mi></mml:math><tex-math id="inft235">\begin{document}$\alpha$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">20</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Length of recurrent dynamics</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf236"><mml:mi>T</mml:mi></mml:math><tex-math id="inft236">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">100</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Length of seed input in caching mode</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf237"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft237">\begin{document}$t_{s}$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">5</td></tr><tr><td align="left" valign="bottom">Learning parameters</td><td align="left" valign="bottom">Strength of seed input during caching</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf238"><mml:mi>λ</mml:mi></mml:math><tex-math id="inft238">\begin{document}$\lambda$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">3.0</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Update learning rate</td><td align="left" valign="bottom"><italic>μ</italic></td><td align="left" valign="bottom">40</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">Update rule inhibition bias</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf239"><mml:mi>β</mml:mi></mml:math><tex-math id="inft239">\begin{document}$\beta$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">–0.35</td></tr><tr><td align="left" valign="bottom">Analysis parameters</td><td align="left" valign="bottom">Readout threshold of seed output</td><td align="left" valign="bottom"><inline-formula><alternatives><mml:math id="inf240"><mml:mi>κ</mml:mi></mml:math><tex-math id="inft240">\begin{document}$\kappa$\end{document}</tex-math></alternatives></inline-formula></td><td align="left" valign="bottom">0.5</td></tr></tbody></table></table-wrap></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Investigation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103512-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modeling code is publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/chingf/barcodes">https://github.com/chingf/barcodes</ext-link> (copy archived at <xref ref-type="bibr" rid="bib13">Fang, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Kim Stachenfeld, Ashok Litwin-Kumar, and members of the Aronov, Stachenfeld, and Abbott labs for feedback on this work. This research was supported by the Gatsby Charitable Foundation and the Kavli Foundation and by NSF award DBI-1707398, NIH Director’s New Innovator Award (DP2-AG071918), NIH Pathway to Independence Award (SC, <sup>1</sup>K99NS136846), NSF GRFP (CF), and DOE CSGF (JL, DE–SC0020347).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname><given-names>P</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Memory consolidation and the medial temporal lobe: a simple network model</article-title><source>PNAS</source><volume>91</volume><fpage>7041</fpage><lpage>7045</lpage><pub-id pub-id-type="doi">10.1073/pnas.91.15.7041</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Applegate</surname><given-names>MC</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible use of memory by food-caching birds</article-title><source>eLife</source><volume>11</volume><elocation-id>e70600</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.70600</pub-id><pub-id pub-id-type="pmid">35467526</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Applegate</surname><given-names>MC</given-names></name><name><surname>Gutnichenko</surname><given-names>KS</given-names></name><name><surname>Mackevicius</surname><given-names>EL</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>An entorhinal-like region in food-caching birds</article-title><source>Current Biology</source><volume>33</volume><fpage>2465</fpage><lpage>2477</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.05.031</pub-id><pub-id pub-id-type="pmid">37295426</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparseness and expansion in sensory representations</article-title><source>Neuron</source><volume>83</volume><fpage>1213</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id><pub-id pub-id-type="pmid">25155954</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Badia</surname><given-names>AP</given-names></name><name><surname>Köster</surname><given-names>R</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Zambaldi</surname><given-names>V</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Memo: A deep network for flexible combination of episodic memories</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2001.10913">https://doi.org/10.48550/arXiv.2001.10913</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prediction and memory: A predictive coding account</article-title><source>Progress in Neurobiology</source><volume>192</volume><elocation-id>101821</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101821</pub-id><pub-id pub-id-type="pmid">32446883</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>KI</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A model of spatial map formation in the hippocampus of the rat</article-title><source>Neural Computation</source><volume>8</volume><fpage>85</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.1.85</pub-id><pub-id pub-id-type="pmid">8564805</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bricken</surname><given-names>T</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attention Approximates Sparse Distributed Memory</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2111.05498">https://doi.org/10.48550/arXiv.2111.05498</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Mackevicius</surname><given-names>EL</given-names></name><name><surname>Hale</surname><given-names>S</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Barcoding of episodic memories in the hippocampus of a food-caching bird</article-title><source>Cell</source><volume>187</volume><fpage>1922</fpage><lpage>1935</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2024.02.032</pub-id><pub-id pub-id-type="pmid">38554707</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayton</surname><given-names>NS</given-names></name><name><surname>Dickinson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Episodic-like memory during cache recovery by scrub jays</article-title><source>Nature</source><volume>395</volume><fpage>272</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1038/26216</pub-id><pub-id pub-id-type="pmid">9751053</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dasgupta</surname><given-names>I</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Memory as a computational resource</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>240</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.12.008</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>C</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Mackevicius</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title><source>eLife</source><volume>12</volume><elocation-id>e80680</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.80680</pub-id><pub-id pub-id-type="pmid">36928104</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Barcodes</data-title><version designator="swh:1:rev:d1dff647e9ccd3cec14189a2479c729faa9748c2">swh:1:rev:d1dff647e9ccd3cec14189a2479c729faa9748c2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:88488b5d81d9fd10a849d3d024e69b34568f36cf;origin=https://github.com/chingf/barcodes;visit=swh:1:snp:69272d7a9c55768f8d12283b32f409663c51c631;anchor=swh:1:rev:d1dff647e9ccd3cec14189a2479c729faa9748c2">https://archive.softwareheritage.org/swh:1:dir:88488b5d81d9fd10a849d3d024e69b34568f36cf;origin=https://github.com/chingf/barcodes;visit=swh:1:snp:69272d7a9c55768f8d12283b32f409663c51c631;anchor=swh:1:rev:d1dff647e9ccd3cec14189a2479c729faa9748c2</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Földiák</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Forming sparse representations by local anti-Hebbian learning</article-title><source>Biological Cybernetics</source><volume>64</volume><fpage>165</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1007/BF02331346</pub-id><pub-id pub-id-type="pmid">2291903</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner-Medwin</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>The recall of events through the learning of associations between their parts</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>194</volume><fpage>375</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1098/rspb.1976.0084</pub-id><pub-id pub-id-type="pmid">11493</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name><name><surname>Irie</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Key-value memory in the brain</article-title><source>Neuron</source><volume>113</volume><fpage>1694</fpage><lpage>1707</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2025.02.029</pub-id><pub-id pub-id-type="pmid">40147436</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Danihelka</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural turing machines</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1410.5401">https://doi.org/10.48550/arXiv.1410.5401</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>Alex</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Reynolds</surname><given-names>M</given-names></name><name><surname>Harley</surname><given-names>T</given-names></name><name><surname>Danihelka</surname><given-names>I</given-names></name><name><surname>Grabska-Barwińska</surname><given-names>A</given-names></name><name><surname>Colmenarejo</surname><given-names>SG</given-names></name><name><surname>Grefenstette</surname><given-names>E</given-names></name><name><surname>Ramalho</surname><given-names>T</given-names></name><name><surname>Agapiou</surname><given-names>J</given-names></name><name><surname>Badia</surname><given-names>AP</given-names></name><name><surname>Hermann</surname><given-names>KM</given-names></name><name><surname>Zwols</surname><given-names>Y</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><name><surname>Cain</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>H</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Blunsom</surname><given-names>P</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hybrid computing using a neural network with dynamic external memory</article-title><source>Nature</source><volume>538</volume><fpage>471</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nature20101</pub-id><pub-id pub-id-type="pmid">27732574</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuromodulation: acetylcholine and memory consolidation</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>351</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01365-0</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of acetylcholine in learning and memory</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>710</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.09.002</pub-id><pub-id pub-id-type="pmid">17011181</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A distributed representation of temporal context</article-title><source>Journal of Mathematical Psychology</source><volume>46</volume><fpage>269</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1006/jmps.2001.1388</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Fotedar</surname><given-names>MS</given-names></name><name><surname>Datey</surname><given-names>AV</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The temporal context model in spatial navigation and relational learning: toward a common explanation of medial temporal lobe function across domains</article-title><source>Psychological Review</source><volume>112</volume><fpage>75</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.1.75</pub-id><pub-id pub-id-type="pmid">15631589</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Associative retrieval processes in free recall</article-title><source>Memory &amp; Cognition</source><volume>24</volume><fpage>103</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.3758/BF03197276</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computational models of memory search</article-title><source>Annual Review of Psychology</source><volume>71</volume><fpage>107</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010418-103358</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kanerva</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Sparse Distributed Memory</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozachkov</surname><given-names>L</given-names></name><name><surname>Kastanenka</surname><given-names>KV</given-names></name><name><surname>Krotov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Building transformers from neurons and astrocytes</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2219150120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2219150120</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Krotov</surname><given-names>D</given-names></name><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dense associative memory for pattern recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1606.01164">https://doi.org/10.48550/arXiv.1606.01164</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Krotov</surname><given-names>D</given-names></name><name><surname>Hopfield</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Large associative memory problem in neurobiology and machine learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2008.06996">https://doi.org/10.48550/arXiv.2008.06996</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Le</surname><given-names>H</given-names></name><name><surname>Tran</surname><given-names>T</given-names></name><name><surname>Venkatesh</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural stored-program memory</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1906.08862">https://doi.org/10.48550/arXiv.1906.08862</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>J</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Prediction, sequences and the hippocampus</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>364</volume><fpage>1193</fpage><lpage>1201</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0316</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lovett-Barron</surname><given-names>M</given-names></name><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Kheirbek</surname><given-names>MA</given-names></name><name><surname>Danielson</surname><given-names>N</given-names></name><name><surname>Zaremba</surname><given-names>JD</given-names></name><name><surname>Reardon</surname><given-names>TR</given-names></name><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Hen</surname><given-names>R</given-names></name><name><surname>Zemelman</surname><given-names>BV</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dendritic inhibition in the hippocampus supports fear learning</article-title><source>Science</source><volume>343</volume><fpage>857</fpage><lpage>863</lpage><pub-id pub-id-type="doi">10.1126/science.1247485</pub-id><pub-id pub-id-type="pmid">24558155</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name><name><surname>Willshaw</surname><given-names>D</given-names></name><name><surname>McNaughton</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1991">1991</year><chapter-title>Simple memory: a theory for archicortex</chapter-title><person-group person-group-type="editor"><name><surname>Vaina</surname><given-names>L</given-names></name></person-group><source>From the Retina to the Neocortex</source><publisher-name>Springer</publisher-name><fpage>59</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1007/978-1-4684-6775-8_5</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Morris</surname><given-names>RGM</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title><source>Trends in Neurosciences</source><volume>10</volume><fpage>408</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(87)90011-7</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>MR</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Experience-dependent, asymmetric expansion of hippocampal place fields</article-title><source>PNAS</source><volume>94</volume><fpage>8918</fpage><lpage>8921</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.16.8918</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>MR</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Experience-dependent asymmetric shape of hippocampal receptive fields</article-title><source>Neuron</source><volume>25</volume><fpage>707</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)81072-7</pub-id><pub-id pub-id-type="pmid">10774737</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Kubie</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The firing of hippocampal place cells predicts the future position of freely moving rats</article-title><source>The Journal of Neuroscience</source><volume>9</volume><fpage>4101</fpage><lpage>4110</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.09-12-04101.1989</pub-id><pub-id pub-id-type="pmid">2592993</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muscinelli</surname><given-names>SP</given-names></name><name><surname>Wagner</surname><given-names>MJ</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Optimal routing to cerebellum-like structures</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>1630</fpage><lpage>1641</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01403-7</pub-id><pub-id pub-id-type="pmid">37604889</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naim</surname><given-names>M</given-names></name><name><surname>Katkov</surname><given-names>M</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fundamental law of memory recall</article-title><source>Physical Review Letters</source><volume>124</volume><elocation-id>018101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.124.018101</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>HL</given-names></name><name><surname>Lynch</surname><given-names>GF</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural representations of space in the hippocampus of a food-caching bird</article-title><source>Science</source><volume>373</volume><fpage>343</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1126/science.abg2009</pub-id><pub-id pub-id-type="pmid">34437154</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ramsauer</surname><given-names>H</given-names></name><name><surname>Schäfl</surname><given-names>B</given-names></name><name><surname>Lehner</surname><given-names>J</given-names></name><name><surname>Seidl</surname><given-names>P</given-names></name><name><surname>Widrich</surname><given-names>M</given-names></name><name><surname>Adler</surname><given-names>T</given-names></name><name><surname>Gruber</surname><given-names>L</given-names></name><name><surname>Holzleitner</surname><given-names>M</given-names></name><name><surname>Pavlović</surname><given-names>M</given-names></name><name><surname>Sandve</surname><given-names>GK</given-names></name><name><surname>Greiff</surname><given-names>V</given-names></name><name><surname>Kreil</surname><given-names>D</given-names></name><name><surname>Kopp</surname><given-names>M</given-names></name><name><surname>Klambauer</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hopfield Networks Is All You Need</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2008.02217">https://doi.org/10.48550/arXiv.2008.02217</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacouto</surname><given-names>L</given-names></name><name><surname>Wichert</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Competitive learning to generate sparse representations for associative memory</article-title><source>Neural Networks</source><volume>168</volume><fpage>32</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2023.09.005</pub-id><pub-id pub-id-type="pmid">37734137</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>372</volume><elocation-id>20160049</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0049</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A context-based theory of recency and contiguity in free recall</article-title><source>Psychological Review</source><volume>115</volume><fpage>893</fpage><lpage>912</lpage><pub-id pub-id-type="doi">10.1037/a0013396</pub-id><pub-id pub-id-type="pmid">18954208</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherry</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Food storage by black-capped chickadees: Memory for the location and contents of caches</article-title><source>Animal Behaviour</source><volume>32</volume><fpage>451</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/S0003-3472(84)80281-X</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Crisanti</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Chaos in random neural networks</article-title><source>Physical Review Letters</source><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sukhbaatar</surname><given-names>S</given-names></name><name><surname>Weston</surname><given-names>J</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>End-to-end memory networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1503.08895">https://doi.org/10.48550/arXiv.1503.08895</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talmi</surname><given-names>D</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Can semantic relatedness explain the enhancement of memory for emotional words?</article-title><source>Memory &amp; Cognition</source><volume>32</volume><fpage>742</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.3758/BF03195864</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>TJ</given-names></name><name><surname>DiScenna</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The hippocampal memory indexing theory</article-title><source>Behavioral Neuroscience</source><volume>100</volume><fpage>147</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1037//0735-7044.100.2.147</pub-id><pub-id pub-id-type="pmid">3008780</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>TJ</given-names></name><name><surname>Rudy</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The hippocampal indexing theory and episodic memory: updating the index</article-title><source>Hippocampus</source><volume>17</volume><fpage>1158</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1002/hipo.20350</pub-id><pub-id pub-id-type="pmid">17696170</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attractor neural network models of spatial maps in hippocampus</article-title><source>Hippocampus</source><volume>9</volume><fpage>481</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1999)9:4&lt;481::AID-HIPO14&gt;3.0.CO;2-S</pub-id><pub-id pub-id-type="pmid">10495029</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>Episodic and semantic memory</chapter-title><person-group person-group-type="editor"><name><surname>Tulving</surname><given-names>E</given-names></name><name><surname>Donaldson</surname><given-names>W</given-names></name></person-group><source>Organization of Memory</source><publisher-name>Academic Press</publisher-name><fpage>381</fpage><lpage>403</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tyulmankov</surname><given-names>D</given-names></name><name><surname>Fang</surname><given-names>C</given-names></name><name><surname>Vadaparty</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biological learning in key-value memory networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2110.13976">https://doi.org/10.48550/arXiv.2110.13976</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Tolman-Eichenbaum Machine: Unifying space and relational memory through generalization in the hippocampal formation</article-title><source>Cell</source><volume>183</volume><fpage>1249</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.10.024</pub-id><pub-id pub-id-type="pmid">33181068</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JC</given-names></name><name><surname>Warren</surname><given-names>J</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Relating transformers to models and neural representations of the hippocampal formation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2112.04035">https://doi.org/10.48550/arXiv.2112.04035</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Dynamics of the hippocampal ensemble code for space</article-title><source>Science</source><volume>261</volume><fpage>1055</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1126/science.8351520</pub-id><pub-id pub-id-type="pmid">8351520</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiang</surname><given-names>Z</given-names></name><name><surname>Huguenard</surname><given-names>JR</given-names></name><name><surname>Prince</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cholinergic switching within neocortical inhibitory networks</article-title><source>Science</source><volume>281</volume><fpage>985</fpage><lpage>988</lpage><pub-id pub-id-type="doi">10.1126/science.281.5379.985</pub-id><pub-id pub-id-type="pmid">9703513</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>M</given-names></name><name><surname>Muscinelli</surname><given-names>SP</given-names></name><name><surname>Decker Harris</surname><given-names>K</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Task-dependent optimal representations for cerebellar learning</article-title><source>eLife</source><volume>12</volume><elocation-id>e82914</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.82914</pub-id><pub-id pub-id-type="pmid">37671785</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103512.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>École Normale Supérieure - PSL</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Fundamental</kwd></kwd-group></front-stub><body><p>This <bold>fundamental</bold> work substantially advances our understanding of episodic memory by proposing a biologically plausible mechanism through which hippocampal barcode activity enables efficient memory binding and flexible recall. The evidence supporting the conclusions is <bold>convincing</bold>, with rigorously validated computational models and alignment with experimental findings. The work will be of broad interest to neuroscientists and computational modelers studying memory and hippocampal function.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103512.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this paper, the authors develop a biologically plausible recurrent neural network model to explain how the hippocampus generates and uses barcode-like activity to support episodic memory. They address key questions raised by recent experimental findings: how barcodes are generated, how they interact with memory content (such as place and seed-related activity), and how the hippocampus balances memory specificity with flexible recall. The authors demonstrate that chaotic dynamics in a recurrent neural network can produce barcodes that reduce memory interference, complement place tuning, and enable context-dependent memory retrieval, while aligning their model with observed hippocampal activity during caching and retrieval in chickadees.</p><p>Strengths:</p><p>(1) The manuscript is well-written and structured.</p><p>(2) The paper provides a detailed and biologically plausible mechanism for generating and utilizing barcode activity through chaotic dynamics in a recurrent neural network. This mechanism effectively explains how barcodes reduce memory interference, complement place tuning, and enable flexible, context-dependent recall.</p><p>(3) The authors successfully reproduce key experimental findings on hippocampal barcode activity from chickadee studies, including the distinct correlations observed during caching, retrieval, and visits.</p><p>(4) Overall, the study addresses a somewhat puzzling question about how memory indices and content signals coexist and interact in the same hippocampal population. By proposing a unified model, it provides significant conceptual clarity.</p><p>Weaknesses:</p><p>The recurrent neural network model incorporates assumptions and mechanisms, such as the modulation of recurrent input strength, whose biological underpinnings remain unclear. The authors acknowledge some of these limitations thoughtfully, offering plausible mechanisms and discussing their implications in depth. It may be worth exploring the robustness of the results to certain modeling assumptions. For instance, the choice to run the network for a fixed amount of time and then use the activity at the end for plasticity could be relaxed.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103512.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Striking experimental results by Chettih et al 2024 have identified high-dimensional, sparse patterns of activity in the chickadee hippocampus when birds store or retrieve food at a given site. These barcode-like patterns were interpreted as &quot;indexes&quot; allowing the birds to retrieve from memory the locations of stored food.</p><p>The present manuscript proposes a recurrent network model that generates such barcode activity and uses it to form attractor-like memories that bind information about location and food. The manuscript then examines the computational role of barcode activity in the model by simulating two behavioral tasks, and by comparing the model with an alternate model in which barcode activity is ablated.</p><p>Strengths of the study:</p><p>proposes a potential neural implementation for the indexing theory of episodic memory\</p><p>Provides a mechanistic model of striking experimental findings: barcode-like, sparse patterns of activity when birds store a grain at a specific location</p><p>A particularly interesting aspect of the model is that it proposes a mechanism for binding discrete events to a continuous spatial map, and demonstrates the computational advantages of this mechanism</p><p>Weaknesses:</p><p>The importance of different modeling ingredients and dynamical mechanisms could be made more clear.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103512.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Ching</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Abbott</surname><given-names>Larry F</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Aronov</surname><given-names>Dmitriy</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Chettih</surname><given-names>Selmaan N</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>In this paper, the authors develop a biologically plausible recurrent neural network model to explain how the hippocampus generates and uses barcode-like activity to support episodic memory. They address key questions raised by recent experimental findings: how barcodes are generated, how they interact with memory content (such as place and seed-related activity), and how the hippocampus balances memory specificity with flexible recall. The authors demonstrate that chaotic dynamics in a recurrent neural network can produce barcodes that reduce memory interference, complement place tuning, and enable context-dependent memory retrieval, while aligning their model with observed hippocampal activity during caching and retrieval in chickadees.</p><p>Strengths:</p><p>(1) The manuscript is well-written and structured.</p><p>(2) The paper provides a detailed and biologically plausible mechanism for generating and utilizing barcode activity through chaotic dynamics in a recurrent neural network. This mechanism effectively explains how barcodes reduce memory interference, complement place tuning, and enable flexible, context-dependent recall.</p><p>(3) The authors successfully reproduce key experimental findings on hippocampal barcode activity from chickadee studies, including the distinct correlations observed during caching, retrieval, and visits.</p><p>(4) Overall, the study addresses a somewhat puzzling question about how memory indices and content signals coexist and interact in the same hippocampal population. By proposing a unified model, it provides significant conceptual clarity.</p><p>Weaknesses:</p><p>The recurrent neural network model incorporates assumptions and mechanisms, such as the modulation of recurrent input strength, whose biological underpinnings remain unclear. The authors acknowledge some of these limitations thoughtfully, offering plausible mechanisms and discussing their implications in depth.</p><p>One thread of questions that authors may want to further explore is related to the chaotic nature of activity that generates barcodes when recurrence is strong. Chaos inherently implies sensitivity to initial conditions and noise, which raises questions about its reliability as a mechanism for producing robust and repeatable barcode signals. How sensitive are the results to noise in both the dynamics and the input signals? Does this sensitivity affect the stability of the generated barcodes and place fields, potentially disrupting their functional roles? Moreover, does the implemented plasticity mitigate some of this chaos, or might it amplify it under certain conditions? Clarifying these aspects could strengthen the argument for the robustness of the proposed mechanism.</p></disp-quote><p>In our model, chaos is used to produce a random barcode when forming memories, but memory retrieval depends on attractor dynamics. Specifically, the plasticity update at the end of the cache creates an attractor state, and then afterwards for successful memory retrieval the network activity must settle into this attractor rather than remaining chaotic. This attractor state is a conjunction of memory content (place and seed activity) and memory index (barcode activity). Thus a barcode is ‘reactivated’ when network dynamics during retrieval settle into this cache attractor, or in other words chaotic dynamics do not need to generate the same barcode twice.</p><p>The reviewer raises an important point, which is how sensitivity to initial conditions and noise would affect the reliability of our proposed mechanism. The key question here is how noise will affect the network’s dynamics during retrieval. Would adding noise to the dynamics make memory retrieval more difficult? We thank the reviewer for suggesting we investigate this further, and below describe our experiments and changes to the manuscript to better address this topic.</p><p>We first experimented with adding independent gaussian distributed noise into each unit, drawn independently at each timestep. We analyzed recall accuracy using the same task and methods as Fig. 4F while varying the magnitude of noise. Memory recall was quite robust to this form of noise, even as the magnitude of noise approached half of the signal amplitude. This first experiment added noise into the temporal dynamics of the network. We subsequently examined adding static noise into the network inputs, which can also be thought of as introducing noise into initial conditions. Specifically, we added independent gaussian distributed noise into each unit, with the random value held constant for the extent of temporal dynamics. This perturbation decreased the likelihood of memory recall in a graded manner with noise magnitude, without dramatically changing the spatial profile. Examination of dynamics on individual trials revealed that the network failed to converge onto a cache attractor on some random fraction of trials, with other trials appearing nearly identical to noiseless results. We now include these results in the text and as a new supplementary figure, Figure S4AB.</p><p>To clarify the network dynamics and the purpose of chaos in our model, we make the following modifications in text:</p><p>Section 2.3, paragraph 2 (starting at “To store memories…”):</p><p>“…place inputs arrive into the RNN, recurrent dynamics generate an essentially random barcode, seed inputs are activated, and then Hebbian learning binds a particular pattern of barcode activity to place- and seed-related activity.”</p><p>Section 2.3, paragraph 3 (starting at “Memory recall in our network…”): As an example, consider a scenario in which an animal has already formed a memory at some location <italic>l</italic>, resulting in the storage of an attractor <inline-formula><alternatives><mml:math id="sa3m1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft241">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula> into the RNN. The attractor <inline-formula><alternatives><mml:math id="sa3m2"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft242">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula> can be thought of as a linear combination of place input-driven activity <inline-formula><alternatives><mml:math id="sa3m3"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft243">\begin{document}$p(1)$\end{document}</tex-math></alternatives></inline-formula>, seed input-driven activity <inline-formula><alternatives><mml:math id="sa3m4"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft244">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, and a recurrent-driven barcode component <inline-formula><alternatives><mml:math id="sa3m5"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>b</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft245">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>. Later, the animal returns to the same location and attempts recall (i.e. sets <italic>r</italic> = 1, Figure 3B). Place inputs for location <italic>l</italic> drive RNN activity towards <inline-formula><alternatives><mml:math id="sa3m6"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft246">\begin{document}$p(1)$\end{document}</tex-math></alternatives></inline-formula>, which is partially correlated with attractor <inline-formula><alternatives><mml:math id="sa3m7"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft247">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula>, and the recurrent dynamics cause network activity to converge onto attractor <inline-formula><alternatives><mml:math id="sa3m8"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft248">\begin{document}$\vec{a}$\end{document}</tex-math></alternatives></inline-formula>. In this way, barcode activity <inline-formula><alternatives><mml:math id="sa3m9"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>b</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft249">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> is reactivated, along with the place and seed components stored in the attractor state, <inline-formula><alternatives><mml:math id="sa3m10"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft250">\begin{document}$p(1)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="sa3m11"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft251">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. The seed input can also affect recall, as discussed in the following section.</p><p>Section 2.4, final paragraph (starting “We further examined how model hyperparameters affected performance on these tasks”), added the following describing new results on adding noise: We found that adding noise to the network's temporal dynamics had little effect on memory recall performance (Figure S4A). However, large static noise vectors added to the network's input and initial state decreased the overall probability of memory recall, but not its spatial profile (Figure S4B).</p><disp-quote content-type="editor-comment"><p>It may also be worth exploring the robustness of the results to certain modeling assumptions. For instance, the choice to run the network for a fixed amount of time and then use the activity at the end for plasticity could be relaxed.</p></disp-quote><p>As described above, chaotic dynamics are necessary to generate a barcode during a cache, but not to reactivate that barcode during retrieval. During a successful memory retrieval, network activity settles into an attractor state and thus does not depend on the duration of simulated dynamics. The choice of duration to run dynamics during caching <italic>is</italic> important, but only insofar as activity significantly decorrelates from the initial state. We show in Figure S1B that decorrelation saturates ~t=25, and thus any random time point t &gt; 25 would be similarly effective. We used a fixed duration runtime for caches only to avoid introducing unnecessary complication into our model.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>Striking experimental results by Chettih et al 2024 have identified high-dimensional, sparse patterns of activity in the chickadee hippocampus when birds store or retrieve food at a given site. These barcode-like patterns were interpreted as &quot;indexes&quot; allowing the birds to retrieve from memory the locations of stored food.</p><p>The present manuscript proposes a recurrent network model that generates such barcode activity and uses it to form attractor-like memories that bind information about location and food. The manuscript then examines the computational role of barcode activity in the model by simulating two behavioral tasks, and by comparing the model with an alternate model in which barcode activity is ablated.</p><p>Strengths of the study:</p><p>Proposes a potential neural implementation for the indexing theory of episodic memory - Provides a mechanistic model of striking experimental findings: barcode-like, sparse patterns of activity when birds store a grain at a specific location</p><p>A particularly interesting aspect of the model is that it proposes a mechanism for binding discrete events to a continuous spatial map, and demonstrates the computational advantages of this mechanism.</p><p>Weaknesses:</p><p>The relation between the model and experimentally recorded activity needs some clarification</p><p>The relation with indexing theory could be made more clear</p><p>The importance of different modeling ingredients and dynamical mechanisms could be made more clear</p><p>The paper would be strengthened by focusing on the most essential aspects</p><p>Comments:</p><p>The model distinguishes between &quot;barcode activity&quot; and &quot;attractors&quot;. Which of the two corresponds to experimentally-recorded barcodes? I would presume the attractors. A potential issue is that the attractors are, as explained in the text (l.137), conjunctions of place activity, barcode activity and &quot;seed&quot; inputs. The fact that the seed activity is shared across attractors seems to imply that they have a non-zero correlation independent of distance. Is that the case in the model? If I understand correctly, Fig 3D shows correlations between an attractor and barcodes at different locations, but correlations between attractors at different locations are not shown. Fig 1 F instead shows that correlations between recorded retrieval activities decay to zero with distance.</p><p>More generally, the fact that the expression &quot;barcode&quot; is apparently used with different meanings in the model and in the experiments is potentially confusing (in the model they correspond to activity generating during caching, and this activity is distinct from the memories; my understanding is that in the experiments barcodes correspond to both caching and retrieval, but perhaps I am mistaken?).</p></disp-quote><p>Our intent is to use the expression “barcode” as similarly as possible between model and experimental work. The reviewer points out that the connection between barcodes in experimental and modeling work is unclear, as well as the relation of “attractors” in our model to previous experimental results. The meaning of ‘barcode’ is absolutely critical—we clarify below our intended meaning, and then describe changes to the manuscript to highlight this.</p><p>In experiments, we observed that activity during caching looked different than ordinary hippocampal activity (i.e. typical “place activity” observed during visits). Empirically there were two major differences. First, there was a pattern of neural activity which was present during every cache . This pattern was also present when birds visually inspected sites containing a cached seed, but not when visually inspecting an empty site. This is what we refer to as “seed activity”. Second, there was a pattern of neural activity which was unique to each cache. This pattern re-occurred during retrieval, and was orthogonal to place activity (see Fig. 1E-F). This is what we refer to as “barcode activity”. In summary, activity during a cache (or retrieval) contains a combination of three components: place activity, seed activity, and barcode activity.</p><p>These experimental findings are recapitulated in our model, as activity during a cache contains a combination of three components: place activity driven by place inputs, seed activity driven by seed inputs, and barcode activity generated by recurrent dynamics. Cache activity in the model corresponds to cache activity in experiments, and barcodes in the model correspond to barcodes in experiments. Our model additionally has “attractors”, meaning that network connectivity changes so that the activity generated during a simulated cache becomes an attractor state of network dynamics. “Attractors” refers to a feature of network dynamics, not a distinct activity state, and we do not yet know if these attractors exist in experimental data.</p><p>Figure 3D, as described in the figure legend, is a correlation of activity during cache and retrieval (in purple), for cache-retrieval pairs at the same or at different sites. We believe this is what the reviewer asks to see: the correlation between attractor states for different cache locations. The reviewer makes an important point: seed activity is shared across all attractors, so then why are correlations not high for all locations? This is because attractors also have a place component, which is anti-correlated for distant locations. This is evident in Fig. 3D by noticing that visit-visit correlations (black line, corresponding to place activity only) are negative for distant locations, and the correlation between attractors (purple line, cache-retrieval pairs) is subtly shifted up relative to the black line (place code only) for these distant locations. The size of this shift is due to the relative magnitude of place and seed inputs. For example, if we increase the strength of the seed input during caching (blue line), we can further increase the correlation between attractors even for quite distant sites:</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103512-sa3-fig1-v1.tif"/></fig><p>To clarify the manuscript, we made the following modifications:</p><p>Section 2.2, first paragraph: We model the hippocampus as a recurrent neural network (RNN) (Alvarez and Squire, 1994; Tsodyks, 1999; Hopfield, 1982) and propose that recurrent dynamics can generate barcodes from place inputs. As in experiments, the model’s population activity during a cache should exhibit both place and barcode activity components.</p><p>Section 2.3, paragraph 3 (starting at “Memory recall in our network…”): As an example, consider a scenario in which an animal has already formed a memory at some location <italic>l</italic> , resulting in the storage of an attractor <inline-formula><alternatives><mml:math id="sa3m12"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft252">\begin{document}$\vec{a} $\end{document}</tex-math></alternatives></inline-formula> into the RNN . The attractor <inline-formula><alternatives><mml:math id="sa3m13"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft253">\begin{document}$\vec{a} $\end{document}</tex-math></alternatives></inline-formula> can be thought of as a linear combination of place input-driven activity <inline-formula><alternatives><mml:math id="sa3m14"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft254">\begin{document}$p(1)$\end{document}</tex-math></alternatives></inline-formula>, seed input-driven activity <inline-formula><alternatives><mml:math id="sa3m15"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft255">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>, and a recurrent-driven barcode component <inline-formula><alternatives><mml:math id="sa3m16"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>b</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft256">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula>. Later, the animal returns to the same location and attempts recall (i.e. sets <italic>r</italic> = 1, Figure 3B). Place inputs for <italic>l</italic> drive RNN activity towards <inline-formula><alternatives><mml:math id="sa3m17"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft257">\begin{document}$p(1)$\end{document}</tex-math></alternatives></inline-formula>, which is partially correlated with attractor <inline-formula><alternatives><mml:math id="sa3m18"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft258">\begin{document}$\vec{a} $\end{document}</tex-math></alternatives></inline-formula>, and the recurrent dynamics cause network activity to converge onto attractor <inline-formula><alternatives><mml:math id="sa3m19"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft259">\begin{document}$\vec{a} $\end{document}</tex-math></alternatives></inline-formula>. In this way, barcode activity <inline-formula><alternatives><mml:math id="sa3m20"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>b</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft260">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> is reactivated as part of attractor <inline-formula><alternatives><mml:math id="sa3m21"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft261">\begin{document}$\vec{a} $\end{document}</tex-math></alternatives></inline-formula>, along with the place and seed components stored in the attractor state, <inline-formula><alternatives><mml:math id="sa3m22"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft262">\begin{document}$p(1)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="sa3m23"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft263">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. The seed input can also affect recall, as discussed in the following section.</p><disp-quote content-type="editor-comment"><p>The insights obtained from the network model for the computational role of barcode activity could be explained more clearly. The introduction starts by laying out the indexing theory, which proposes that the hippocampus links an index with each memory so that the memory is reactivated when the index is presented. The experimental paper suggests that the barcode activations play the role of indexes. Yet, in the model reactivations of memories are driven not by presenting bar-code activity, but by presenting place activity (Cache Presence task) or seed activity (Cache Location task). So it seems that either place activity and seed activity play the role of indexes. Section 2.5 nicely shows that ultimately the role of barcode activity is to decorrelate attractors, which seems different from playing the role of indexes. I feel it would be useful that the Discussion reassess more critically the relationship between barcodes, indexing theory, and key-value architectures.</p></disp-quote><p>The reviewer highlights a failure on our part to clearly identify the connection between our findings on barcodes, indexing theory, and key-value architectures. This is another major component of the paper, and below we propose changes to the manuscript to clarify these concepts and their relationships. First, we will summarize the key points that were unclear in our original manuscript.</p><p>The reviewer equates the concept of an ‘index’ with that of a ‘query’: the signal that drives memory reactivation. This may be intuitive, but it is not how a memory index was defined in indexing theory (e.g. Teyler &amp; DiScenna 1986). In indexing theory, the index is a pattern of hippocampal activity that is (a) generated during memory formation, (b) separate from the activity encoding memory content, and (c) linked to memory content via associative plasticity. After memory formation, a memory might be queried by activating a partial set of the memory contents, which would then drive reactivation of the hippocampal index, leading to pattern completion of memory contents. See, for example, figure 1 of Teyler and DiScenna 1986. The ‘index’ is thus not the same as the ‘query’ that drives recall.</p><p>We propose in this work that barcode activity is such an index. Indexing theory originally posited that memory content was encoded by neocortex, and memory index was encoded by hippocampus. However the experiments of Chettih et al. 2024 revealed that the hippocampus contained both memory content and memory index signals, and furthermore there was no division of cells into ‘content’ and ‘index’ subtypes. Thus our model drops the assumption of earlier work that index and content signals correspond to different neurons in different brain areas—a significant advance of our work. Otherwise, the experimentally observed barcodes and the barcodes generated by our computational model play the role of indices as originally defined.</p><p>Our original manuscript was unclear on the relationship of indexing theory and key-value systems. Our work connects diverse areas of memory models, including attractor dynamics, key-value memory systems, and memory indexing. A full account of these literatures and their relationships may be beyond the scope of this manuscript, and we note that a recent review article (Gershman, Fiete, and Irie, 2025) further clarifies the relationship between key-value memory, indexing theory, and the hippocampus. We will cite this work in our discussion as a source for the interested reader.</p><p>Briefly, a key-value memory system distinguishes between the address where a memory is stored, the ‘key’, and the content of that memory, the ‘value’. An advantage of such systems is that keys can be optimized for purposes independent of the value of each memory. The use of barcodes in our model to decorrelate memories is related to this optimization of keys in key-value memory systems. By generating barcodes and adding this to the attractor state corresponding to a cache memory, the ‘address’ of the memory in population activity is differentiated from other memories. Our work is thus consistent with the idea that hippocampus generates keys and implements a key storage system. However it is not so straightforward to <italic>equate</italic> barcodes with keys, as they are defined in key-value memory. As the reviewer points out, memory recall can be driven by location and seed inputs, i.e. it is content-addressable. We think of the barcode as modifying the memory address to better separate similar memories, without changing memory content, and the resulting memory can be recalled by querying with either content or barcode. Given the complex and speculative nature of these relationships, we prefer to note the salient connection of our work with ongoing efforts applying the key-value framework to biological memory, and leave the precise details of this connection to future work.</p><p>We make the following changes in the manuscript to clarify these ideas:</p><p>Introduction, first paragraph: In this scheme, during memory formation the hippocampus generates an index of population activity, and the neurons representing this index are linked with the neurons representing memory content by associative plasticity . Later, re-experience of partial memory contents may reactivate the index, and reactivation of the index drives complete recall of the memory contents.</p><p>Discussion, 4th paragraph on key-value: Interestingly, prior theoretical work has suggested neural implementations for both key-value memory and attention mechanisms, arguing for their usefulness in neural systems such as long term memory (Kanerva, 1988; Tyulmankov et al., 2021; Bricken and Pehlevan, 2021; Whittington et al., 2021; Kozachkov et al., 2023; Krotov and Hopfield, 2020; Gershman 2025). In this framework, the address where a memory is stored (the key) may be optimized independently of the value or content of the memory. In our model, barcodes improve memory performance by providing a content-independent scaffold that binds to memory content, preventing memories with overlapping content from blurring together. Thus barcodes can be considered as a change in memory address, and our model suggests important connections between recurrent neural activity and key generation mechanisms. However we note that barcodes should not be literally equated with keys in key-value systems as our model’s memory is ‘content-addresable’—it can be queried by place and seed inputs.</p><disp-quote content-type="editor-comment"><p>The model includes a number of non-standard ingredients. It would be useful to explain which of these ingredients and which of the described mechanisms are essential for the studied phenomenon. In particular:</p><p>- the dynamics in Eq.2 include a shunting inhibition term. Is it essential and why?</p></disp-quote><p>The shunting inhibition is important as it acts to normalize the network activity to prevent runaway excitation. We hope to clarify this further by amending the following sentence in section 2.2: <italic>“g</italic> (·) is a leak rate that depends on the average activity of the full network, representing a form of global shunting inhibition that normalizes network activity to prevent runaway excitation from recurrent dynamics.”</p><disp-quote content-type="editor-comment"><p>- same question for the global inhibition included in the random connectivity;</p></disp-quote><p>The distribution from which connectivity strengths are drawn has a negative mean (global inhibition). This causes activity during caching (i.e. r = 1) to be sparser than activity during visits (i.e. r = 0), and was chosen to match experimental findings. In figures 2B and S2B we show that our model can transition between a mode with place code only, barcode only, or a mode containing both, by changing the variance of the weight distribution while holding the mean constant. We suggest clarifying this by editing the following in section 2.2, paragraph 2: “We initialize the recurrent weights from a random Gaussian distribution, <inline-formula><alternatives><mml:math id="sa3m24"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>J</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>η</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft264">\begin{document}$J \sim {N}\left(\frac{\eta}{N_X}, \frac{\sigma^{2}}{N_X}\right) $\end{document}</tex-math></alternatives></inline-formula>. where 𝑁<sub>𝑋</sub> is the number of RNN neurons and μ &lt; 0, reflecting global subtractive inhibition that encourages sparse network activity to match experimental findings (Chettih et al. 2024).”</p><disp-quote content-type="editor-comment"><p>- the model is fully rate-based, but for certain figures, spikes are randomly generated. This seems superfluous.</p></disp-quote><p>Spikes are simulated for one analysis and one visualization, where it is important to consider noise or variability in neural responses across trials. First, for Fig. 2H,J, we generated spikes to allow a visual comparison to figures that can be easily generated from experimental data. Second, and more significantly, for the analysis underlying Fig. 3D, it is essential to simulate variability in neural responses. Because our rate-based models are noiseless, the RNN’s rate vector at site distance = 0 will always be the same and result in a correlation of 1 for both visit-visit and cache-retrieval. However, we show that, if one interprets the rate as a noisy Poisson spiking process, the correlation at site distance = 0 between a cache-retrieval pair is higher than that of two visits. This is because under a Poisson spiking model, the signal-to-noise ratio is higher for cache-retrieval activity, where rates are higher in magnitude. The greater correlation for a cache-retrieval pair at the same site, relative to visits at the same site, is an experimental finding that was critical for our model to reproduce. We detail clarifications to the manuscript below in response to the reviewer’s following and related question.</p><disp-quote content-type="editor-comment"><p>How are the correlations determined in the model (e.g., Fig 2 B)? The methods explain that they are computed from Poisson-generated spikes, but over which time period? Presumably during steady-state responses, but are these responses time-averaged?</p></disp-quote><p>The reviewer points out a lack of clarity in our original manuscript. Correlations for events (caches, retrievals and visits) at different sites are calculated in two sections of the paper (2B, 3D), for different purposes and with slight differences in methods:</p><p>- For figure 2B, no spikes are simulated. Note that the methods mentioning poisson spike generation specify only Fig. 2H,J and Fig. 3D. We simply take the network’s rate vector at timestep t=100 (when the decorrelating effect of chaotic dynamics has saturated, S1A-B) and correlate this vector when generated at different locations. We now clarify this in the legend for Figure 2B: “We show correlation of place inputs (gray) and correlation of the RNN's rate vector at t = 100 (black).”</p><p>- For Figure 3D, we want to compare the model to empirical results from Chettih et al. 2024, and reproduced in this paper in Fig. 1E-F. These empirical results are derived from correlating vectors of spiking activity on pairs of single trials, and are thus affected by noise or variability in neural responses as described in our response to the reviewer’s previous question. We thus took the RNN’s rate vector at t=100 and simulated spiking data by drawing samples from a poisson distribution to get spike counts. Our original manuscript was unclear about this, and we suggest the following changes:</p><p>- Legend for Figure 3D: D. Correlation of Poisson-generated spikes simulated from RNN rate vectors at two sites, plotted as a function of the distance between the two sites.</p><p>- Section 2.3, last paragraph: Population activity during retrieval closely matches activity during caching, and is substantially decorrelated from activity during visits (Figure 3C). To compare our model with the empirical results reproduced in Figure 1E,F, we ran <italic>in silico</italic> experiments with caches and retrievals at varying sites in the circular arena. We simulated Poisson-generated spikes drawn from our network's underlying rates to match the intrinsic variability in empirical data (see Methods).</p><p>- Methods, subsection Spatial correlation of RNN activity for cache-retrieval pairs at different sites: To calculate correlation values as in Figure3D, we simulated experiments where 5 sites were randomly chosen for caching and retrieval. To compare model results to the empirical data in Fig. 1E,F, which includes intrinsic neural variability, we sampled Poisson-generated spike counts from the rates output by our model. Specifically, for RNN activity <inline-formula><alternatives><mml:math id="sa3m25"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft265">\begin{document}$\vec{r_i}$\end{document}</tex-math></alternatives></inline-formula> at location i, using the rates at t=100 as elsewhere, we first generate a sample vector of spikes…</p><disp-quote content-type="editor-comment"><p>I was confused by early and late responses in Fig 2 C. The text says that the activity is initialized at zero, so the response at t=0 should be flat (and zero). More generally, I am not sure I understand why the dynamics matter for the phenomenon at all, presumably the decorrelation shown in Fig 2B depends only on steady state activity (cf previous question).</p></disp-quote><p>Thanks for catching this mistake. The legend has been updated to indicate that the ‘early’ response is actually at t=1, when network activity reflects place inputs without the effects of dynamics. The reviewer is correct that we are primarily interested in the ‘late’ response of the network. All other results in the paper use this late response at t=100. As shown in Fig. S2A,B, this timepoint is not truly a steady state, as activity in the network continues to change, but the decorrelation of network activity with place-driven activity has saturated.</p><p>We include the early response in Fig. 2C for visual comparison of the purely place-driven early activity with the eventual network response. It is also relevant since, as the reviewer points out above, there is a shunting inhibition term in the dynamics that is present during both low and high recurrent strength simulations.</p><disp-quote content-type="editor-comment"><p>Related to the previous point, the discussion of decorrelation (l.79 - 97) is somewhat confusing. That paragraph focuses on chaotic activity, but chaos decorrelates responses across different time points. Here the main phenomenon is the decorrelation of responses across different spatial inputs (Fig 2B). This decorrelation is presumably due to the fact that different inputs lead to different non-trivial steady-state responses, but this requires some clarification. If that is correct, the temporal chaos adds fluctuations around these non-trivial steady-state responses, but that alone would not lead to the decorrelation shown in Fig 2B.</p></disp-quote><p>We agree with the reviewer that chaotic activity produces a decorrelation across time points. Because of chaotic dynamics, network activity does not settle into a trivial steady-state, and instead evolves from the initial state in an unpredictable way. The network does not settle into a steady-state pattern, but both the decorrelation of network state with initial state and the rate of change in the network state saturate after ~t=25 timesteps, as shown in Fig. S2A-B.</p><p>The initial activity for nearby states is similar, due to them receiving similar place inputs.</p><p>Because network activity is chaotically decorrelated from this initial state by temporal dynamics, ‘late stage’ network activity between nearby spatial states is less correlated than ‘early stage’ activity. Thus the temporal decorrelation produces a spatial decorrelation. We believe that the changes we have introduced to the manuscript in revision will make this point clearer in our resubmission.</p><disp-quote content-type="editor-comment"><p>A key ingredient of the model is that the recurrent interactions are switched on and off between &quot;caching&quot; and &quot;visits&quot;. The discussion argues that a possible mechanism for this is recurrent inhibition (l.320), which would need to be added. However two forms of inhibition are already included in the model. The text also says that it is unclear how units in the model should be mapped onto E and I neurons. However the model makes explicit assumptions about this, in particular by generating spikes from individual neurons. Altogether, I did not find that part of the Discussion convincing.</p></disp-quote><p>We agree with the reviewer that this section is a limitation of our current work, and in fact it is an ongoing area of future research. However we think the advances in this current work warrant publication despite this topic requiring further research. We attempted to discuss this limitation explicitly, and note that the other reviewer pointed this section out as particularly helpful. We do not think it is problematic for a realistic model of the brain to ultimately include 3, or even more forms of inhibition. We do not think that poisson-generated spikes commit us to interpreting network units as single neurons. Spikes are not a core part of our model’s mechanism, and were used only as a mechanism of introducing variability on top of deterministic rates for specific analyses. Furthermore one could still view network units as pools of both E and I spiking neurons. We would welcome further recommendations the reviewer believes are important to note in this section on our model’s limitations.</p><disp-quote content-type="editor-comment"><p>On lines 117-120 the text briefly mentions an alternate feed-forward model and promptly discards it. The discussion instead says that a &quot;separate possibility is that barcodes are generated in a circuit upstream of where memories are stored, and supplied as inputs to the hippocampal population&quot;, and that this possibility would lead to identical conclusions. The two statements seem a bit contradictory. It seems that the alternative possibility would replace the need for switching on and off recurrent interactions, with a mechanism where barcode inputs are switched on and off. This alternate scenario is perhaps more plausible, so it would be useful to discuss it more explicitly.</p></disp-quote><p>We apologize for the confusion here, which seems to be due to our phrasing in the discussion section. We do reject the idea that a simple feed-forward model could generate the spatial correlation profile observed in data, as mentioned in the text and included as Fig. S2. Our statement in the discussion may have seemed contradictory because here we intended to discuss the possibility that an upstream area generates barcodes, for example by the chaotic recurrent dynamics proposed in our work, while a downstream network receives these barcodes as inputs and undergoes plasticity to store memories as attractors. We did not intend to suggest any connection to the feedforward model of barcode generation, and apologize for the confusion. Our claim that this ‘2 network’ solution would lead to similar conclusions is because the upstream network would need an efficient means of barcode generation, and the downstream network would need an efficient means of storing memory attractors, and separating these functions into different networks is not likely to affect for example the advantage of partially decorrelating memory attractors. Moreover, the downstream network would still require some form of recurrent gating, so that during visits it exhibits place activity without activating stored memory attractors!</p><p>We thus chose a 1 network instead of a 2 network solution because it was simpler and, we believe, more interesting. It is challenging in the absence of more data to say which is more plausible, thus we wanted to mention the possibility of a 2 network solution. We suggest the following changes to the manuscript:</p><p>- Discussion, 3rd paragraph: “Alternatively, other mechanisms may be involved in generating barcodes. We demonstrated that conventional feed-forward sparsification (Babadi and Sompolinsky, 2014; Xie et al., 2023) was highly inefficient, but more specialized computations may improve this (Földiak, 1990; Olshausen and Field, 1996; Sacouto and Wichert, 2023; Muscinelli et al., 2023). Another possibility is that barcodes are generated in a separate recurrent network upstream of the recurrent network where memories are stored. In this 2-network scenario, the downstream network receives both spatial tuning and barcodes as inputs. This would not obviate the need for modulating recurrent strength in the downstream network to switch between input-driven modes and attractor dynamics. We suspect separating barcode generation and memory storage in separate networks would not fundamentally affect our conclusions.”</p><disp-quote content-type="editor-comment"><p>As a minor note, the beginning of the discussion states that the presented model is similar to previous recurrent network models of the hippocampus. It would be worth noting that several of the cited works assign a very different role to recurrent interactions: they generate place cell activity, while the present model assumes it is inherited from upstream inputs.</p></disp-quote><p>We are not sure how best to modify the paper to address this suggestion. As far as we know, all of the cited models which deal with spatial encoding do assume that the hippocampus receives a spatially-modulated or spatially-tuned input. For example, the Tsodyks 1999 paper cited in this paragraph uses exponentially-decaying place inputs to each neuron highly similar to our model. Furthermore we explore how our model would perform if we change the format of spatial inputs in Fig. S4, and find key results are unchanged. It is unclear how hippocampal place fields could emerge without inputs that differentiate between spatial locations. We think it is appropriate to highlight the similarity of our model to well known hopfield-type recurrent models, where memories are stored as attractor states of the network dynamics.</p><p>On the other hand, we agree that a common line of hippocampal modeling proposes that recurrent interactions reshape spatial inputs to produce place fields. This often arises in the context of hippocampus generating a predictive map, where inputs may be one-hot for a single spatial state, in a grid cell-like format, or a random projection of sensory features. We attempted to address this in section 2.6, using a model which superimposes the random connectivity needed for barcode generation with the structured connectivity needed for predictive map formation. We found that such a model was able to perform both predictive and barcode functions, suggesting a path forward to connecting different lines of hippocampal modeling in future work.</p></body></sub-article></article>