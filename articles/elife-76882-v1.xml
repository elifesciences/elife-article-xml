<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">76882</article-id><article-id pub-id-type="doi">10.7554/eLife.76882</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>State-dependent representations of mixtures by the olfactory bulb</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-255189"><name><surname>Adefuin</surname><given-names>Aliya Mari</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-255191"><name><surname>Lindeman</surname><given-names>Sander</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-255190"><name><surname>Reinert</surname><given-names>Janine K</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-54375"><name><surname>Fukunaga</surname><given-names>Izumi</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1860-5377</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Sensory and Behavioural Neuroscience Unit</institution>, <institution>Okinawa Institute of Science and Technology Graduate University</institution>, <addr-line><named-content content-type="city">Okinawa</named-content></addr-line>, <country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-10010"><name><surname>Uchida</surname><given-names>Naoshige</given-names></name><role>Reviewing editor</role><aff><institution>Harvard University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>izumi.fukunaga@oist.jp</email> (IF);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>07</day><month>03</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e76882</elocation-id><history><date date-type="received"><day>07</day><month>01</month><year>2022</year></date><date date-type="accepted"><day>05</day><month>03</month><year>2022</year></date></history><permissions><copyright-statement>© 2022, Adefuin et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Adefuin et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-76882-v1.pdf"/><abstract><p>Sensory systems are often tasked to analyse complex signals from the environment, separating relevant from irrelevant parts. This process of decomposing signals is challenging when a mixture of signals does not equal the sum of its parts, leading to an unpredictable corruption of signal patterns. In olfaction, nonlinear summation is prevalent at various stages of sensory processing. Here, we investigate how the olfactory system deals with binary mixtures of odours under different brain states, using two-photon imaging of olfactory bulb (OB) output neurons. Unlike previous studies using anaesthetised animals, we found that mixture summation is more linear in the early phase of evoked responses in awake, head-fixed mice performing an odour detection task, due to dampened responses. Despite this, and responses being more variable, decoding analyses indicated that the data from behaving mice was well discriminable. Curiously, the time course of decoding accuracy did not correlate strictly with the linearity of summation. Further, a comparison with naïve mice indicated that learning to accurately perform the mixture detection task is not accompanied by more linear mixture summation. Finally, using a simulation, we demonstrate that, while saturating sublinearity tends to degrade the discriminability, the extent of the impairment may depend on other factors, including pattern decorrelation. Altogether, our results demonstrate that the mixture representation in the primary olfactory area is state-dependent, but the analytical perception may not strictly correlate with linearity in summation.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004199</institution-id><institution>Okinawa Institute of Science and Technology Graduate University</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Adefuin</surname><given-names>Aliya Mari</given-names></name><name><surname>Lindeman</surname><given-names>Sander</given-names></name><name><surname>Reinert</surname><given-names>Janine K</given-names></name><name><surname>Fukunaga</surname><given-names>Izumi</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures described in this study have been approved by the OIST Graduate University's Animal Care and Use Committee (Protocol 2016-151 and 2020-310)</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The files consist of individual data to compare linear sum vs. observed mixture responses (300 - 1000 ms after odour onset).</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Fukunaga I</collab></person-group><year iso-8601-date="2022">2022</year><source>Figure 6C</source><ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.5061/dryad.p2ngf1vrh">https://dx.doi.org/10.5061/dryad.p2ngf1vrh</ext-link><comment>Dryad Digital Repository, doi:10.5061/dryad.p2ngf1vrh</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-76882-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>